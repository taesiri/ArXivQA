# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- Single-Image 3D Human Digitization with Shape-Guided Diffusion - [[Arxiv](https://arxiv.org/abs/2311.09221)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09221.md)]
- DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction
  Model - [[Arxiv](https://arxiv.org/abs/2311.09217)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09217.md)]
- GRIM: GRaph-based Interactive narrative visualization for gaMes - [[Arxiv](https://arxiv.org/abs/2311.09213)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09213.md)]
- Fusion-Eval: Integrating Evaluators with LLMs - [[Arxiv](https://arxiv.org/abs/2311.09204)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09204.md)]
- PEARL: Personalizing Large Language Model Writing Assistants with
  Generation-Calibrated Retrievers - [[Arxiv](https://arxiv.org/abs/2311.09180)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09180.md)]
- SiRA: Sparse Mixture of Low Rank Adaptation - [[Arxiv](https://arxiv.org/abs/2311.09179)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.09179.md)]
- Llamas Know What GPTs Don't Show: Surrogate Models for Confidence
  Estimation - [[Arxiv](https://arxiv.org/abs/2311.08877)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08877.md)]
- Thread of Thought Unraveling Chaotic Contexts - [[Arxiv](https://arxiv.org/abs/2311.08734)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08734.md)]
- Routing to the Expert: Efficient Reward-guided Ensemble of Large
  Language Models - [[Arxiv](https://arxiv.org/abs/2311.08692)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08692.md)]
- EDMSound: Spectrogram Based Diffusion Models for Efficient and
  High-Quality Audio Synthesis - [[Arxiv](https://arxiv.org/abs/2311.08667)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08667.md)]
- Drivable 3D Gaussian Avatars - [[Arxiv](https://arxiv.org/abs/2311.08581)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08581.md)]
- UT5: Pretraining Non autoregressive T5 with unrolled denoising - [[Arxiv](https://arxiv.org/abs/2311.08552)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08552.md)]
- UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations - [[Arxiv](https://arxiv.org/abs/2311.08469)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08469.md)]
- Instant3D: Instant Text-to-3D Generation - [[Arxiv](https://arxiv.org/abs/2311.08403)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08403.md)]
- Fine-tuning Language Models for Factuality - [[Arxiv](https://arxiv.org/abs/2311.08401)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08401.md)]
- Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads
  to Answers Faster - [[Arxiv](https://arxiv.org/abs/2311.08263)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08263.md)]
- DiLoCo: Distributed Low-Communication Training of Language Models - [[Arxiv](https://arxiv.org/abs/2311.08105)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08105.md)]
- A Survey on Language Models for Code - [[Arxiv](https://arxiv.org/abs/2311.07989)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07989.md)]
- The ART of LLM Refinement: Ask, Refine, and Trust - [[Arxiv](https://arxiv.org/abs/2311.07961)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07961.md)]
- Qwen-Audio: Advancing Universal Audio Understanding via Unified
  Large-Scale Audio-Language Models - [[Arxiv](https://arxiv.org/abs/2311.07919)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07919.md)]
- Instruction-Following Evaluation for Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.07911)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07911.md)]
- One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View
  Generation and 3D Diffusion - [[Arxiv](https://arxiv.org/abs/2311.07885)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07885.md)]
- MART: Improving LLM Safety with Multi-round Automatic Red-Teaming - [[Arxiv](https://arxiv.org/abs/2311.07689)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07689.md)]
- SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for
  Multi-modal Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.07575)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07575.md)]
- To See is to Believe: Prompting GPT-4V for Better Visual Instruction
  Tuning - [[Arxiv](https://arxiv.org/abs/2311.07574)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07574.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
