# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting - [[Arxiv](https://arxiv.org/abs/2311.17061)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17061.md)]
- Material Palette: Extraction of Materials from a Single Image - [[Arxiv](https://arxiv.org/abs/2311.17060)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17060.md)]
- Mission-driven Exploration for Accelerated Deep Reinforcement Learning
  with Temporal Logic Task Specifications - [[Arxiv](https://arxiv.org/abs/2311.17059)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17059.md)]
- Panoptic Video Scene Graph Generation - [[Arxiv](https://arxiv.org/abs/2311.17058)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17058.md)]
- ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions - [[Arxiv](https://arxiv.org/abs/2311.17057)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17057.md)]
- Self-Supervised Motion Magnification by Backpropagating Through Optical
  Flow - [[Arxiv](https://arxiv.org/abs/2311.17056)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17056.md)]
- Rethinking Directional Integration in Neural Radiance Fields - [[Arxiv](https://arxiv.org/abs/2311.16504)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.16504.md)]
- No Representation Rules Them All in Category Discovery - [[Arxiv](https://arxiv.org/abs/2311.17055)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17055.md)]
- DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative
  Diffusion Models - [[Arxiv](https://arxiv.org/abs/2311.17053)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17053.md)]
- Surf-D: High-Quality Surface Generation for Arbitrary Topologies using
  Diffusion Models - [[Arxiv](https://arxiv.org/abs/2311.17050)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17050.md)]
- MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced
  Training - [[Arxiv](https://arxiv.org/abs/2311.17049)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17049.md)]
- Zero-shot Referring Expression Comprehension via Structural Similarity
  Between Images and Captions - [[Arxiv](https://arxiv.org/abs/2311.17048)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17048.md)]
- LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.17043)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17043.md)]
- Adversarial Diffusion Distillation - [[Arxiv](https://arxiv.org/abs/2311.17042)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17042.md)]
- Efficient In-Context Learning in Vision-Language Models for Egocentric
  Videos - [[Arxiv](https://arxiv.org/abs/2311.17041)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17041.md)]
- Scalable Extraction of Training Data from (Production) Language Models - [[Arxiv](https://arxiv.org/abs/2311.17035)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17035.md)]
- Telling Left from Right: Identifying Geometry-Aware Semantic
  Correspondence - [[Arxiv](https://arxiv.org/abs/2311.17034)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17034.md)]
- Is This the Subspace You Are Looking for? An Interpretability Illusion
  for Subspace Activation Patching - [[Arxiv](https://arxiv.org/abs/2311.17030)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17030.md)]
- When the Few Outweigh the Many: Illicit Content Recognition with
  Few-Shot Learning - [[Arxiv](https://arxiv.org/abs/2311.17026)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17026.md)]
- Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with
  Distilled Semantic Features - [[Arxiv](https://arxiv.org/abs/2311.17024)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17024.md)]
- Foundational Moral Values for AI Alignment - [[Arxiv](https://arxiv.org/abs/2311.17017)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17017.md)]
- Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer - [[Arxiv](https://arxiv.org/abs/2311.17009)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17009.md)]
- Computational Hypergraph Discovery, a Gaussian Process framework for
  connecting the dots - [[Arxiv](https://arxiv.org/abs/2311.17007)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17007.md)]
- An Investigation of Time Reversal Symmetry in Reinforcement Learning - [[Arxiv](https://arxiv.org/abs/2311.17008)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17008.md)]
- On the Impact of Sampling on Deep Sequential State Estimation - [[Arxiv](https://arxiv.org/abs/2311.17006)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.17006.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.1` API.
