# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- Instant3D: Instant Text-to-3D Generation - [[Arxiv](https://arxiv.org/abs/2311.08403)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08403.md)]
- Fine-tuning Language Models for Factuality - [[Arxiv](https://arxiv.org/abs/2311.08401)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08401.md)]
- Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads
  to Answers Faster - [[Arxiv](https://arxiv.org/abs/2311.08263)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08263.md)]
- DiLoCo: Distributed Low-Communication Training of Language Models - [[Arxiv](https://arxiv.org/abs/2311.08105)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.08105.md)]
- A Survey on Language Models for Code - [[Arxiv](https://arxiv.org/abs/2311.07989)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07989.md)]
- The ART of LLM Refinement: Ask, Refine, and Trust - [[Arxiv](https://arxiv.org/abs/2311.07961)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07961.md)]
- Qwen-Audio: Advancing Universal Audio Understanding via Unified
  Large-Scale Audio-Language Models - [[Arxiv](https://arxiv.org/abs/2311.07919)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07919.md)]
- Instruction-Following Evaluation for Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.07911)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07911.md)]
- One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View
  Generation and 3D Diffusion - [[Arxiv](https://arxiv.org/abs/2311.07885)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07885.md)]
- MART: Improving LLM Safety with Multi-round Automatic Red-Teaming - [[Arxiv](https://arxiv.org/abs/2311.07689)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07689.md)]
- SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for
  Multi-modal Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.07575)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07575.md)]
- To See is to Believe: Prompting GPT-4V for Better Visual Instruction
  Tuning - [[Arxiv](https://arxiv.org/abs/2311.07574)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07574.md)]
- GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone
  GUI Navigation - [[Arxiv](https://arxiv.org/abs/2311.07562)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07562.md)]
- MEGAVERSE: Benchmarking Large Language Models Across Languages,
  Modalities, Models and Tasks - [[Arxiv](https://arxiv.org/abs/2311.07463)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07463.md)]
- Story-to-Motion: Synthesizing Infinite and Controllable Character
  Animation from Long Text - [[Arxiv](https://arxiv.org/abs/2311.07446)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07446.md)]
- Music ControlNet: Multiple Time-varying Controls for Music Generation - [[Arxiv](https://arxiv.org/abs/2311.07069)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07069.md)]
- Q-Instruct: Improving Low-level Visual Abilities for Multi-modality
  Foundation Models - [[Arxiv](https://arxiv.org/abs/2311.06783)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06783.md)]
- ChatAnything: Facetime Chat with LLM-Enhanced Personas - [[Arxiv](https://arxiv.org/abs/2311.06772)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06772.md)]
- Towards General-Purpose Speech Abilities for Large Language Models Using
  Unpaired Data - [[Arxiv](https://arxiv.org/abs/2311.06753)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06753.md)]
- Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small
  Scorer - [[Arxiv](https://arxiv.org/abs/2311.06720)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06720.md)]
- Trusted Source Alignment in Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.06697)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06697.md)]
- LayoutPrompter: Awaken the Design Ability of Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.06495)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06495.md)]
- GOAT: GO to Any Thing - [[Arxiv](https://arxiv.org/abs/2311.06430)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06430.md)]
- Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization - [[Arxiv](https://arxiv.org/abs/2311.06243)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06243.md)]
- Florence-2: Advancing a Unified Representation for a Variety of Vision
  Tasks - [[Arxiv](https://arxiv.org/abs/2311.06242)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06242.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
