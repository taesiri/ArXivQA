# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and
  reusing ModulEs - [[Arxiv](https://arxiv.org/abs/2311.04901)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04901.md)]
- TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.04589)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04589.md)]
- NExT-Chat: An LMM for Chat, Detection and Segmentation - [[Arxiv](https://arxiv.org/abs/2311.04498)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04498.md)]
- LRM: Large Reconstruction Model for Single Image to 3D - [[Arxiv](https://arxiv.org/abs/2311.04400)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04400.md)]
- 3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features - [[Arxiv](https://arxiv.org/abs/2311.04391)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04391.md)]
- Holistic Evaluation of Text-To-Image Models - [[Arxiv](https://arxiv.org/abs/2311.04287)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04287.md)]
- OtterHD: A High-Resolution Multi-modality Model - [[Arxiv](https://arxiv.org/abs/2311.04219)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04219.md)]
- Video Instance Matting - [[Arxiv](https://arxiv.org/abs/2311.04212)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04212.md)]
- I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion
  Models - [[Arxiv](https://arxiv.org/abs/2311.04145)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04145.md)]
- Unveiling Safety Vulnerabilities of Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.04124)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04124.md)]
- mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with
  Modality Collaboration - [[Arxiv](https://arxiv.org/abs/2311.04257)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04257.md)]
- Everything of Thoughts: Defying the Law of Penrose Triangle for Thought
  Generation - [[Arxiv](https://arxiv.org/abs/2311.04254)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04254.md)]
- Leveraging Large Language Models for Automated Proof Synthesis in Rust - [[Arxiv](https://arxiv.org/abs/2311.03739)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03739.md)]
- Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent
  Learning - [[Arxiv](https://arxiv.org/abs/2311.03736)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03736.md)]
- Random Field Augmentations for Self-Supervised Representation Learning - [[Arxiv](https://arxiv.org/abs/2311.03629)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03629.md)]
- SoundCam: A Dataset for Finding Humans Using Room Acoustics - [[Arxiv](https://arxiv.org/abs/2311.03517)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03517.md)]
- GLaMM: Pixel Grounding Large Multimodal Model - [[Arxiv](https://arxiv.org/abs/2311.03356)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03356.md)]
- CoVLM: Composing Visual Entities and Relationships in Large Language
  Models Via Communicative Decoding - [[Arxiv](https://arxiv.org/abs/2311.03354)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03354.md)]
- Ziya2: Data-centric Learning is All LLMs Need - [[Arxiv](https://arxiv.org/abs/2311.03301)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03301.md)]
- S-LoRA: Serving Thousands of Concurrent LoRA Adapters - [[Arxiv](https://arxiv.org/abs/2311.03285)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03285.md)]
- LDM3D-VR: Latent Diffusion Model for 3D VR - [[Arxiv](https://arxiv.org/abs/2311.03226)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03226.md)]
- CogVLM: Visual Expert for Pretrained Language Models - [[Arxiv](https://arxiv.org/abs/2311.03079)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03079.md)]
- Can LLMs Follow Simple Rules? - [[Arxiv](https://arxiv.org/abs/2311.04235)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04235.md)]
- Co-training and Co-distillation for Quality Improvement and Compression
  of Language Models - [[Arxiv](https://arxiv.org/abs/2311.02849)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02849.md)]
- Consistent4D: Consistent 360Â° Dynamic Object Generation from
  Monocular Video - [[Arxiv](https://arxiv.org/abs/2311.02848)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02848.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
