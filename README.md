# List of Papers
- (2023/08) - YaRN: Efficient Context Window Extension of Large Language Models - [[Arxiv](https://arxiv.org/abs/2309.00071)] [[QA](./papers/2309.00071.md)].
- (2023/07) - Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech
  Prompts - [[Arxiv](https://arxiv.org/abs/2307.07218)] [[QA](./papers/2307.07218.md)].
- (2023/07) - Learning to Retrieve In-Context Examples for Large Language Models - [[Arxiv](https://arxiv.org/abs/2307.07164)] [[QA](./papers/2307.07164.md)].
- (2023/07) - DIALGEN: Collaborative Human-LM Generated Dialogues for Improved
  Understanding of Human-Human Conversations - [[Arxiv](https://arxiv.org/abs/2307.07047)] [[QA](./papers/2307.07047.md)].
- (2023/07) - HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image
  Models - [[Arxiv](https://arxiv.org/abs/2307.06949)] [[QA](./papers/2307.06949.md)].
- (2023/07) - In-context Autoencoder for Context Compression in a Large Language Model - [[Arxiv](https://arxiv.org/abs/2307.06945)] [[QA](./papers/2307.06945.md)].
- (2023/07) - InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding
  and Generation - [[Arxiv](https://arxiv.org/abs/2307.06942)] [[QA](./papers/2307.06942.md)].
- (2023/07) - Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation - [[Arxiv](https://arxiv.org/abs/2307.06940)] [[QA](./papers/2307.06940.md)].
- (2023/07) - Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image
  Models - [[Arxiv](https://arxiv.org/abs/2307.06925)] [[QA](./papers/2307.06925.md)].
- (2023/07) - Generating Benchmarks for Factuality Evaluation of Language Models - [[Arxiv](https://arxiv.org/abs/2307.06908)] [[QA](./papers/2307.06908.md)].
- (2023/07) - Copy Is All You Need - [[Arxiv](https://arxiv.org/abs/2307.06962)] [[QA](./papers/2307.06962.md)].
- (2023/07) - Distilling Large Language Models for Biomedical Knowledge Extraction: A
  Case Study on Adverse Drug Events - [[Arxiv](https://arxiv.org/abs/2307.06439)] [[QA](./papers/2307.06439.md)].
- (2023/07) - T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional
  Text-to-image Generation - [[Arxiv](https://arxiv.org/abs/2307.06350)] [[QA](./papers/2307.06350.md)].
- (2023/07) - Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and
  Resolution - [[Arxiv](https://arxiv.org/abs/2307.06304)] [[QA](./papers/2307.06304.md)].
- (2023/07) - Instruction Mining: High-Quality Instruction Data Selection for Large
  Language Models - [[Arxiv](https://arxiv.org/abs/2307.06290)] [[QA](./papers/2307.06290.md)].
- (2023/07) - SayPlan: Grounding Large Language Models using 3D Scene Graphs for
  Scalable Task Planning - [[Arxiv](https://arxiv.org/abs/2307.06135)] [[QA](./papers/2307.06135.md)].
- (2023/07) - PolyLM: An Open Source Polyglot Large Language Model - [[Arxiv](https://arxiv.org/abs/2307.06018)] [[QA](./papers/2307.06018.md)].
- (2023/07) - VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models - [[Arxiv](https://arxiv.org/abs/2307.05973)] [[QA](./papers/2307.05973.md)].
- (2023/07) - Giving Robots a Hand: Learning Generalizable Manipulation with
  Eye-in-Hand Human Video Demonstrations - [[Arxiv](https://arxiv.org/abs/2307.05959)] [[QA](./papers/2307.05959.md)].
- (2023/07) - Towards Robust and Efficient Continual Language Learning - [[Arxiv](https://arxiv.org/abs/2307.05741)] [[QA](./papers/2307.05741.md)].
- (2023/07) - Stack More Layers Differently: High-Rank Training Through Low-Rank
  Updates - [[Arxiv](https://arxiv.org/abs/2307.05695)] [[QA](./papers/2307.05695.md)].
- (2023/07) - Differentiable Blocks World: Qualitative 3D Decomposition by Rendering
  Primitives - [[Arxiv](https://arxiv.org/abs/2307.05473)] [[QA](./papers/2307.05473.md)].
- (2023/07) - Self-consistency for open-ended generations - [[Arxiv](https://arxiv.org/abs/2307.06857)] [[QA](./papers/2307.06857.md)].
- (2023/07) - EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the
  Backbone - [[Arxiv](https://arxiv.org/abs/2307.05463)] [[QA](./papers/2307.05463.md)].
- (2023/07) - Efficient 3D Articulated Human Generation with Layered Surface Volumes - [[Arxiv](https://arxiv.org/abs/2307.05462)] [[QA](./papers/2307.05462.md)].
- (2023/07) - Empowering Cross-lingual Behavioral Testing of NLP Models with
  Typological Features - [[Arxiv](https://arxiv.org/abs/2307.05454)] [[QA](./papers/2307.05454.md)].
- (2023/07) - Self-Supervised Learning with Lie Symmetries for Partial Differential
  Equations - [[Arxiv](https://arxiv.org/abs/2307.05432)] [[QA](./papers/2307.05432.md)].
- (2023/07) - Unleashing Cognitive Synergy in Large Language Models: A Task-Solving
  Agent through Multi-Persona Self-Collaboration - [[Arxiv](https://arxiv.org/abs/2307.05300)] [[QA](./papers/2307.05300.md)].
- (2023/07) - Generative Pretraining in Multimodality - [[Arxiv](https://arxiv.org/abs/2307.05222)] [[QA](./papers/2307.05222.md)].
- (2023/07) - DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence
  Analysis Tasks - [[Arxiv](https://arxiv.org/abs/2307.05628)] [[QA](./papers/2307.05628.md)].
- (2023/07) - Test-Time Training on Video Streams - [[Arxiv](https://arxiv.org/abs/2307.05014)] [[QA](./papers/2307.05014.md)].
- (2023/07) - Secrets of RLHF in Large Language Models Part I: PPO - [[Arxiv](https://arxiv.org/abs/2307.04964)] [[QA](./papers/2307.04964.md)].
- (2023/07) - Semantic-SAM: Segment and Recognize Anything at Any Granularity - [[Arxiv](https://arxiv.org/abs/2307.04767)] [[QA](./papers/2307.04767.md)].
- (2023/07) - SITTA: A Semantic Image-Text Alignment for Image Captioning - [[Arxiv](https://arxiv.org/abs/2307.05591)] [[QA](./papers/2307.05591.md)].
- (2023/07) - Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal
  Rearrangement - [[Arxiv](https://arxiv.org/abs/2307.04751)] [[QA](./papers/2307.04751.md)].
- (2023/07) - AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models
  without Specific Tuning - [[Arxiv](https://arxiv.org/abs/2307.04725)] [[QA](./papers/2307.04725.md)].
- (2023/07) - Large Language Models as General Pattern Machines - [[Arxiv](https://arxiv.org/abs/2307.04721)] [[QA](./papers/2307.04721.md)].
- (2023/07) - International Institutions for Advanced AI - [[Arxiv](https://arxiv.org/abs/2307.04699)] [[QA](./papers/2307.04699.md)].
- (2023/07) - VampNet: Music Generation via Masked Acoustic Token Modeling - [[Arxiv](https://arxiv.org/abs/2307.04686)] [[QA](./papers/2307.04686.md)].
- (2023/07) - AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation
  System - [[Arxiv](https://arxiv.org/abs/2307.04577)] [[QA](./papers/2307.04577.md)].
- (2023/07) - RLTF: Reinforcement Learning from Unit Test Feedback - [[Arxiv](https://arxiv.org/abs/2307.04349)] [[QA](./papers/2307.04349.md)].
- (2023/07) - SVIT: Scaling up Visual Instruction Tuning - [[Arxiv](https://arxiv.org/abs/2307.04087)] [[QA](./papers/2307.04087.md)].
- (2023/07) - Toward Interactive Dictation - [[Arxiv](https://arxiv.org/abs/2307.04008)] [[QA](./papers/2307.04008.md)].
- (2023/07) - On decoder-only architecture for speech-to-text and large language model
  integration - [[Arxiv](https://arxiv.org/abs/2307.03917)] [[QA](./papers/2307.03917.md)].
- (2023/07) - Large Language Models for Supply Chain Optimization - [[Arxiv](https://arxiv.org/abs/2307.03875)] [[QA](./papers/2307.03875.md)].
- (2023/07) - Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation - [[Arxiv](https://arxiv.org/abs/2307.03869)] [[QA](./papers/2307.03869.md)].
- (2023/07) - AutoDecoding Latent 3D Diffusion Models - [[Arxiv](https://arxiv.org/abs/2307.05445)] [[QA](./papers/2307.05445.md)].
- (2023/07) - Solvent: A Framework for Protein Folding - [[Arxiv](https://arxiv.org/abs/2307.04603)] [[QA](./papers/2307.04603.md)].
- (2023/07) - Collaborative Score Distillation for Consistent Visual Synthesis - [[Arxiv](https://arxiv.org/abs/2307.04787)] [[QA](./papers/2307.04787.md)].
- (2023/06) - Kosmos-2: Grounding Multimodal Large Language Models to the World - [[Arxiv](https://arxiv.org/abs/2306.14824)] [[QA](./papers/2306.14824.md)].
- (2023/06) - DesCo: Learning Object Recognition with Rich Language Descriptions - [[Arxiv](https://arxiv.org/abs/2306.14060)] [[QA](./papers/2306.14060.md)].
- (2023/05) - Towards Generalist Robots: A Promising Paradigm via Generative
  Simulation - [[Arxiv](https://arxiv.org/abs/2305.10455)] [[QA](./papers/2305.10455.md)].
- (2023/05) - TidyBot: Personalized Robot Assistance with Large Language Models - [[Arxiv](https://arxiv.org/abs/2305.05658)] [[QA](./papers/2305.05658.md)].
- (2023/05) - Principle-Driven Self-Alignment of Language Models from Scratch with
  Minimal Human Supervision - [[Arxiv](https://arxiv.org/abs/2305.03047)] [[QA](./papers/2305.03047.md)].
- (2023/05) - Multimodal Procedural Planning via Dual Text-Image Prompting - [[Arxiv](https://arxiv.org/abs/2305.01795)] [[QA](./papers/2305.01795.md)].
- (2023/05) - ArK: Augmented Reality with Knowledge Interactive Emergent Ability - [[Arxiv](https://arxiv.org/abs/2305.00970)] [[QA](./papers/2305.00970.md)].
- (2023/04) - LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model - [[Arxiv](https://arxiv.org/abs/2304.15010)] [[QA](./papers/2304.15010.md)].
- (2023/04) - ChatVideo: A Tracklet-centric Multimodal and Versatile Video
  Understanding System - [[Arxiv](https://arxiv.org/abs/2304.14407)] [[QA](./papers/2304.14407.md)].
- (2023/04) - mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality - [[Arxiv](https://arxiv.org/abs/2304.14178)] [[QA](./papers/2304.14178.md)].
- (2023/04) - Multimodal Grounding for Embodied AI via Augmented Reality Headsets for
  Natural Language Driven Task Planning - [[Arxiv](https://arxiv.org/abs/2304.13676)] [[QA](./papers/2304.13676.md)].
- (2023/04) - ChatABL: Abductive Learning via Natural Language Interaction with
  ChatGPT - [[Arxiv](https://arxiv.org/abs/2304.11107)] [[QA](./papers/2304.11107.md)].
- (2023/04) - Can GPT-4 Perform Neural Architecture Search? - [[Arxiv](https://arxiv.org/abs/2304.10970)] [[QA](./papers/2304.10970.md)].
- (2023/04) - MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
  Language Models - [[Arxiv](https://arxiv.org/abs/2304.10592)] [[QA](./papers/2304.10592.md)].
- (2023/04) - SINC: Spatial Composition of 3D Human Motions for Simultaneous Action
  Generation - [[Arxiv](https://arxiv.org/abs/2304.10417)] [[QA](./papers/2304.10417.md)].
- (2023/04) - Chameleon: Plug-and-Play Compositional Reasoning with Large Language
  Models - [[Arxiv](https://arxiv.org/abs/2304.09842)] [[QA](./papers/2304.09842.md)].
- (2023/04) - Visual Instruction Tuning - [[Arxiv](https://arxiv.org/abs/2304.08485)] [[QA](./papers/2304.08485.md)].
- (2023/04) - Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with
  Text - [[Arxiv](https://arxiv.org/abs/2304.06939)] [[QA](./papers/2304.06939.md)].
- (2023/04) - RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment - [[Arxiv](https://arxiv.org/abs/2304.06767)] [[QA](./papers/2304.06767.md)].
- (2023/04) - Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
  Prompt Augmented by ChatGPT - [[Arxiv](https://arxiv.org/abs/2304.11116)] [[QA](./papers/2304.11116.md)].
- (2023/04) - OpenAGI: When LLM Meets Domain Experts - [[Arxiv](https://arxiv.org/abs/2304.04370)] [[QA](./papers/2304.04370.md)].
- (2023/04) - Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions - [[Arxiv](https://arxiv.org/abs/2304.04227)] [[QA](./papers/2304.04227.md)].
- (2023/04) - ChatGPT Empowered Long-Step Robot Control in Various Environments: A
  Case Application - [[Arxiv](https://arxiv.org/abs/2304.03893)] [[QA](./papers/2304.03893.md)].
- (2023/04) - ERRA: An Embodied Representation and Reasoning Architecture for
  Long-horizon Language-conditioned Manipulation Tasks - [[Arxiv](https://arxiv.org/abs/2304.02251)] [[QA](./papers/2304.02251.md)].
- (2023/03) - HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging
  Face - [[Arxiv](https://arxiv.org/abs/2303.17580)] [[QA](./papers/2303.17580.md)].
- (2023/03) - ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with
  GPT and Prototype Guidance - [[Arxiv](https://arxiv.org/abs/2303.16894)] [[QA](./papers/2303.16894.md)].
- (2023/03) - LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init
  Attention - [[Arxiv](https://arxiv.org/abs/2303.16199)] [[QA](./papers/2303.16199.md)].
- (2023/03) - Learning video embedding space with Natural Language Supervision - [[Arxiv](https://arxiv.org/abs/2303.14584)] [[QA](./papers/2303.14584.md)].
- (2023/03) - Errors are Useful Prompts: Instruction Guided Task Programming with
  Verifier-Assisted Iterative Prompting - [[Arxiv](https://arxiv.org/abs/2303.14100)] [[QA](./papers/2303.14100.md)].
- (2023/03) - eP-ALM: Efficient Perceptual Augmentation of Language Models - [[Arxiv](https://arxiv.org/abs/2303.11403)] [[QA](./papers/2303.11403.md)].
- (2023/03) - MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action - [[Arxiv](https://arxiv.org/abs/2303.11381)] [[QA](./papers/2303.11381.md)].
- (2023/03) - DialogPaint: A Dialog-based Image Editing Model - [[Arxiv](https://arxiv.org/abs/2303.10073)] [[QA](./papers/2303.10073.md)].
- (2023/03) - MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action
  Recognition with Language Knowledge - [[Arxiv](https://arxiv.org/abs/2303.08914)] [[QA](./papers/2303.08914.md)].
- (2023/03) - Can Large Language Models design a Robot? - [[Arxiv](https://arxiv.org/abs/2303.15324)] [[QA](./papers/2303.15324.md)].
- (2023/03) - Chat with the Environment: Interactive Multimodal Perception Using Large
  Language Models - [[Arxiv](https://arxiv.org/abs/2303.08268)] [[QA](./papers/2303.08268.md)].
- (2023/03) - ViperGPT: Visual Inference via Python Execution for Reasoning - [[Arxiv](https://arxiv.org/abs/2303.08128)] [[QA](./papers/2303.08128.md)].
- (2023/03) - RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via
  Language-Based Feedback - [[Arxiv](https://arxiv.org/abs/2303.07622)] [[QA](./papers/2303.07622.md)].
- (2023/03) - Audio Visual Language Maps for Robot Navigation - [[Arxiv](https://arxiv.org/abs/2303.07522)] [[QA](./papers/2303.07522.md)].
- (2023/03) - ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched
  Visual Descriptions - [[Arxiv](https://arxiv.org/abs/2303.06594)] [[QA](./papers/2303.06594.md)].
- (2023/03) - Task and Motion Planning with Large Language Models for Object
  Rearrangement - [[Arxiv](https://arxiv.org/abs/2303.06247)] [[QA](./papers/2303.06247.md)].
- (2023/03) - Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
  Models - [[Arxiv](https://arxiv.org/abs/2303.04671)] [[QA](./papers/2303.04671.md)].
- (2023/03) - Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Based Zero-Shot
  Object Navigation - [[Arxiv](https://arxiv.org/abs/2303.03480)] [[QA](./papers/2303.03480.md)].
- (2023/03) - Prompting Large Language Models with Answer Heuristics for
  Knowledge-based Visual Question Answering - [[Arxiv](https://arxiv.org/abs/2303.01903)] [[QA](./papers/2303.01903.md)].
- (2023/03) - Open-World Object Manipulation using Pre-trained Vision-Language Models - [[Arxiv](https://arxiv.org/abs/2303.00905)] [[QA](./papers/2303.00905.md)].
- (2023/03) - Grounded Decoding: Guiding Text Generation with Grounded Models for
  Robot Control - [[Arxiv](https://arxiv.org/abs/2303.00855)] [[QA](./papers/2303.00855.md)].
- (2023/02) - Internet Explorer: Targeted Representation Learning on the Open Web - [[Arxiv](https://arxiv.org/abs/2302.14051)] [[QA](./papers/2302.14051.md)].
- (2023/02) - Language Is Not All You Need: Aligning Perception with Language Models - [[Arxiv](https://arxiv.org/abs/2302.14045)] [[QA](./papers/2302.14045.md)].
- (2022/11) - Visual Programming: Compositional visual reasoning without training - [[Arxiv](https://arxiv.org/abs/2211.11559)] [[QA](./papers/2211.11559.md)].
- (2022/10) - VIMA: General Robot Manipulation with Multimodal Prompts - [[Arxiv](https://arxiv.org/abs/2210.03094)] [[QA](./papers/2210.03094.md)].
- (2022/10) - Multimodal Analogical Reasoning over Knowledge Graphs - [[Arxiv](https://arxiv.org/abs/2210.00312)] [[QA](./papers/2210.00312.md)].
- (2022/07) - LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,
  Vision, and Action - [[Arxiv](https://arxiv.org/abs/2207.04429)] [[QA](./papers/2207.04429.md)].
- (2022/06) - Language Models are General-Purpose Interfaces - [[Arxiv](https://arxiv.org/abs/2206.06336)] [[QA](./papers/2206.06336.md)].
