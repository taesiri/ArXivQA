# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- LLM-FP4: 4-Bit Floating-Point Quantized Transformers - [[Arxiv](https://arxiv.org/abs/2310.16836)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16836.md)]
- LightSpeed: Light and Fast Neural Light Fields on Mobile Devices - [[Arxiv](https://arxiv.org/abs/2310.16832)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16832.md)]
- DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion
  Prior - [[Arxiv](https://arxiv.org/abs/2310.16818)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16818.md)]
- QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models - [[Arxiv](https://arxiv.org/abs/2310.16795)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16795.md)]
- Detecting Pretraining Data from Large Language Models - [[Arxiv](https://arxiv.org/abs/2310.16789)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16789.md)]
- ConvNets Match Vision Transformers at Scale - [[Arxiv](https://arxiv.org/abs/2310.16764)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16764.md)]
- A Picture is Worth a Thousand Words: Principled Recaptioning Improves
  Image Generation - [[Arxiv](https://arxiv.org/abs/2310.16656)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16656.md)]
- An Early Evaluation of GPT-4V(ision) - [[Arxiv](https://arxiv.org/abs/2310.16534)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16534.md)]
- TiC-CLIP: Continual Training of CLIP Models - [[Arxiv](https://arxiv.org/abs/2310.16226)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16226.md)]
- Woodpecker: Hallucination Correction for Multimodal Large Language
  Models - [[Arxiv](https://arxiv.org/abs/2310.16045)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16045.md)]
- Dissecting In-Context Learning of Translations in GPTs - [[Arxiv](https://arxiv.org/abs/2310.15987)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15987.md)]
- In-Context Learning Creates Task Vectors - [[Arxiv](https://arxiv.org/abs/2310.15916)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15916.md)]
- KITAB: Evaluating LLMs on Constraint Satisfaction for Information
  Retrieval - [[Arxiv](https://arxiv.org/abs/2310.15511)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15511.md)]
- TRAMS: Training-free Memory Selection for Long-range Language Modeling - [[Arxiv](https://arxiv.org/abs/2310.15494)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15494.md)]
- Moral Foundations of Large Language Models - [[Arxiv](https://arxiv.org/abs/2310.15337)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15337.md)]
- SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial
  Understanding - [[Arxiv](https://arxiv.org/abs/2310.15308)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15308.md)]
- RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions - [[Arxiv](https://arxiv.org/abs/2310.15171)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15171.md)]
- FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling - [[Arxiv](https://arxiv.org/abs/2310.15169)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15169.md)]
- Large Language Models are Visual Reasoning Coordinators - [[Arxiv](https://arxiv.org/abs/2310.15166)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15166.md)]
- DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual
  Design - [[Arxiv](https://arxiv.org/abs/2310.15144)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15144.md)]
- Branch-Solve-Merge Improves Large Language Model Evaluation and
  Generation - [[Arxiv](https://arxiv.org/abs/2310.15123)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15123.md)]
- Matryoshka Diffusion Models - [[Arxiv](https://arxiv.org/abs/2310.15111)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15111.md)]
- Wonder3D: Single Image to 3D using Cross-Domain Diffusion - [[Arxiv](https://arxiv.org/abs/2310.15008)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15008.md)]
- ALCUNA: Large Language Models Meet New Knowledge - [[Arxiv](https://arxiv.org/abs/2310.14820)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.14820.md)]
- Inject Semantic Concepts into Image Tagging for Open-Set Recognition - [[Arxiv](https://arxiv.org/abs/2310.15200)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15200.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
