# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- An Early Evaluation of GPT-4V(ision) - [[Arxiv](https://arxiv.org/abs/2310.16534)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16534.md)]
- TiC-CLIP: Continual Training of CLIP Models - [[Arxiv](https://arxiv.org/abs/2310.16226)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16226.md)]
- Woodpecker: Hallucination Correction for Multimodal Large Language
  Models - [[Arxiv](https://arxiv.org/abs/2310.16045)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.16045.md)]
- Dissecting In-Context Learning of Translations in GPTs - [[Arxiv](https://arxiv.org/abs/2310.15987)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15987.md)]
- In-Context Learning Creates Task Vectors - [[Arxiv](https://arxiv.org/abs/2310.15916)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15916.md)]
- KITAB: Evaluating LLMs on Constraint Satisfaction for Information
  Retrieval - [[Arxiv](https://arxiv.org/abs/2310.15511)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15511.md)]
- TRAMS: Training-free Memory Selection for Long-range Language Modeling - [[Arxiv](https://arxiv.org/abs/2310.15494)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15494.md)]
- Moral Foundations of Large Language Models - [[Arxiv](https://arxiv.org/abs/2310.15337)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15337.md)]
- SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial
  Understanding - [[Arxiv](https://arxiv.org/abs/2310.15308)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15308.md)]
- RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions - [[Arxiv](https://arxiv.org/abs/2310.15171)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15171.md)]
- FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling - [[Arxiv](https://arxiv.org/abs/2310.15169)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15169.md)]
- Large Language Models are Visual Reasoning Coordinators - [[Arxiv](https://arxiv.org/abs/2310.15166)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15166.md)]
- DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual
  Design - [[Arxiv](https://arxiv.org/abs/2310.15144)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15144.md)]
- Branch-Solve-Merge Improves Large Language Model Evaluation and
  Generation - [[Arxiv](https://arxiv.org/abs/2310.15123)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15123.md)]
- Matryoshka Diffusion Models - [[Arxiv](https://arxiv.org/abs/2310.15111)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15111.md)]
- ALCUNA: Large Language Models Meet New Knowledge - [[Arxiv](https://arxiv.org/abs/2310.14820)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.14820.md)]
- Inject Semantic Concepts into Image Tagging for Open-Set Recognition - [[Arxiv](https://arxiv.org/abs/2310.15200)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.15200.md)]
- Exploring the Boundaries of GPT-4 in Radiology - [[Arxiv](https://arxiv.org/abs/2310.14573)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.14573.md)]
- HallusionBench: You See What You Think? Or You Think What You See? An
  Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5,
  and Other Multi-modality Models - [[Arxiv](https://arxiv.org/abs/2310.14566)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.14566.md)]
- InstructExcel: A Benchmark for Natural Language Instruction in Excel - [[Arxiv](https://arxiv.org/abs/2310.14495)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.14495.md)]
- Ensemble-Instruct: Generating Instruction-Tuning Data with a
  Heterogeneous Mixture of LMs - [[Arxiv](https://arxiv.org/abs/2310.13961)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.13961.md)]
- Specific versus General Principles for Constitutional AI - [[Arxiv](https://arxiv.org/abs/2310.13798)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.13798.md)]
- TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion
  Models - [[Arxiv](https://arxiv.org/abs/2310.13772)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.13772.md)]
- Localizing and Editing Knowledge in Text-to-Image Generative Models - [[Arxiv](https://arxiv.org/abs/2310.13730)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.13730.md)]
- Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large
  Language Models by Extrapolating Errors from Small Models - [[Arxiv](https://arxiv.org/abs/2310.13671)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2310.13671.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
