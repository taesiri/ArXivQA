# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- InterroGate: Learning to Share, Specialize, and Prune Representations
  for Multi-task Learning - [[Arxiv](https://arxiv.org/abs/2402.16848)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16848.md)]
- GROUNDHOG: Grounding Large Language Models to Holistic Segmentation - [[Arxiv](https://arxiv.org/abs/2402.16846)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16846.md)]
- Neural Operators with Localized Integral and Differential Kernels - [[Arxiv](https://arxiv.org/abs/2402.16845)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16845.md)]
- Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding - [[Arxiv](https://arxiv.org/abs/2402.16844)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16844.md)]
- Asymmetry in Low-Rank Adapters of Foundation Models - [[Arxiv](https://arxiv.org/abs/2402.16842)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16842.md)]
- MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT - [[Arxiv](https://arxiv.org/abs/2402.16840)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16840.md)]
- Do Large Language Models Latently Perform Multi-Hop Reasoning? - [[Arxiv](https://arxiv.org/abs/2402.16837)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16837.md)]
- PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large
  Multimodal Models - [[Arxiv](https://arxiv.org/abs/2402.16836)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16836.md)]
- Eight Methods to Evaluate Robust Unlearning in LLMs - [[Arxiv](https://arxiv.org/abs/2402.16835)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16835.md)]
- Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual
  Capabilities Without Richer Cross-Modal Projections - [[Arxiv](https://arxiv.org/abs/2402.16832)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16832.md)]
- SKILL: Similarity-aware Knowledge distILLation for Speech
  Self-Supervised Learning - [[Arxiv](https://arxiv.org/abs/2402.16830)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16830.md)]
- GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning - [[Arxiv](https://arxiv.org/abs/2402.16829)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16829.md)]
- Training Neural Networks from Scratch with Parallel Low-Rank Adapters - [[Arxiv](https://arxiv.org/abs/2402.16828)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16828.md)]
- A Survey on Data Selection for Language Models - [[Arxiv](https://arxiv.org/abs/2402.16827)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16827.md)]
- Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional
  layers for 3D abdominal organ segmentation - [[Arxiv](https://arxiv.org/abs/2402.16825)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16825.md)]
- Language Agents as Optimizable Graphs - [[Arxiv](https://arxiv.org/abs/2402.16823)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16823.md)]
- Nemotron-4 15B Technical Report - [[Arxiv](https://arxiv.org/abs/2402.16819)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16819.md)]
- Investigating the Effectiveness of HyperTuning via Gisting - [[Arxiv](https://arxiv.org/abs/2402.16817)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16817.md)]
- Multi-Human Mesh Recovery with Transformers - [[Arxiv](https://arxiv.org/abs/2402.16806)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16806.md)]
- Set the Clock: Temporal Alignment of Pretrained Language Models - [[Arxiv](https://arxiv.org/abs/2402.16797)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16797.md)]
- Why Transformers Need Adam: A Hessian Perspective - [[Arxiv](https://arxiv.org/abs/2402.16788)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16788.md)]
- Political Compass or Spinning Arrow? Towards More Meaningful Evaluations
  for Values and Opinions in Large Language Models - [[Arxiv](https://arxiv.org/abs/2402.16786)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16786.md)]
- CARTE: pretraining and transfer for tabular learning - [[Arxiv](https://arxiv.org/abs/2402.16785)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16785.md)]
- On the Growth of Mistakes in Differentially Private Online Learning: A
  Lower Bound Perspective - [[Arxiv](https://arxiv.org/abs/2402.16778)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16778.md)]
- A Comprehensive Evaluation of Quantization Strategies for Large Language
  Models - [[Arxiv](https://arxiv.org/abs/2402.16775)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2402.16775.md)]

## List of Papers by Year
- [Papers for 2024](https://github.com/taesiri/ArXivQA/blob/main/Papers-2024.md)
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.1` API.
