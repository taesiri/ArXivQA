# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- Dense Optical Tracking: Connecting the Dots - [[Arxiv](https://arxiv.org/abs/2312.00786)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00786.md)]
- Sequential Modeling Enables Scalable Learning for Large Vision Models - [[Arxiv](https://arxiv.org/abs/2312.00785)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00785.md)]
- Making Large Multimodal Models Understand Arbitrary Visual Prompts - [[Arxiv](https://arxiv.org/abs/2312.00784)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00784.md)]
- MorpheuS: Neural Dynamic 360Â° Surface Reconstruction from Monocular
  RGB-D Video - [[Arxiv](https://arxiv.org/abs/2312.00778)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00778.md)]
- VideoBooth: Diffusion-based Video Generation with Image Prompts - [[Arxiv](https://arxiv.org/abs/2312.00777)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00777.md)]
- Towards Generalizable Zero-Shot Manipulation via Translating Human
  Interaction Plans - [[Arxiv](https://arxiv.org/abs/2312.00775)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00775.md)]
- Context Retrieval via Normalized Contextual Latent Interaction for
  Conversational Agent - [[Arxiv](https://arxiv.org/abs/2312.00774)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00774.md)]
- Automated Material Properties Extraction For Enhanced Beauty Product
  Discovery and Makeup Virtual Try-on - [[Arxiv](https://arxiv.org/abs/2312.00766)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00766.md)]
- Explaining Knock-on Effects of Bias Mitigation - [[Arxiv](https://arxiv.org/abs/2312.00765)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00765.md)]
- Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized
  Model Responses - [[Arxiv](https://arxiv.org/abs/2312.00763)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00763.md)]
- Deep Unlearning: Fast and Efficient Training-free Approach to Controlled
  Forgetting - [[Arxiv](https://arxiv.org/abs/2312.00761)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00761.md)]
- Mitigating Over-smoothing in Transformers via Regularized Nonlocal
  Functionals - [[Arxiv](https://arxiv.org/abs/2312.00751)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00751.md)]
- Deciphering Digital Detectives: Understanding LLM Behaviors and
  Capabilities in Multi-Agent Mystery Games - [[Arxiv](https://arxiv.org/abs/2312.00746)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00746.md)]
- Scalable Meta-Learning with Gaussian Processes - [[Arxiv](https://arxiv.org/abs/2312.00742)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00742.md)]
- Adversarial Score Distillation: When score distillation meets GAN - [[Arxiv](https://arxiv.org/abs/2312.00739)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00739.md)]
- Gaussian Grouping: Segment and Edit Anything in 3D Scenes - [[Arxiv](https://arxiv.org/abs/2312.00732)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00732.md)]
- Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space - [[Arxiv](https://arxiv.org/abs/2312.00727)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00727.md)]
- Removing Biases from Molecular Representations via Information
  Maximization - [[Arxiv](https://arxiv.org/abs/2312.00718)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00718.md)]
- SpaCE: The Spatial Confounding Environment - [[Arxiv](https://arxiv.org/abs/2312.00710)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00710.md)]
- PointBeV: A Sparse Approach to BeV Predictions - [[Arxiv](https://arxiv.org/abs/2312.00703)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00703.md)]
- GIFT: Generative Interpretable Fine-Tuning Transformers - [[Arxiv](https://arxiv.org/abs/2312.00700)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00700.md)]
- Rethinking Detection Based Table Structure Recognition for Visually Rich
  Documents - [[Arxiv](https://arxiv.org/abs/2312.00699)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00699.md)]
- Object Detector Differences when using Synthetic and Real Training Data - [[Arxiv](https://arxiv.org/abs/2312.00694)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00694.md)]
- VisionaryVR: An Optical Simulation Tool for Evaluating and Optimizing
  Vision Correction Solutions in Virtual Reality - [[Arxiv](https://arxiv.org/abs/2312.00692)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00692.md)]
- Open-vocabulary object 6D pose estimation - [[Arxiv](https://arxiv.org/abs/2312.00690)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.00690.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.1` API.
