# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- GroupContrast: Semantic-aware Self-supervised Representation Learning
  for 3D Understanding - [[Arxiv](https://arxiv.org/abs/2403.09639)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09639.md)]
- SCP-Diff: Photo-Realistic Semantic Image Synthesis with
  Spatial-Categorical Joint Prior - [[Arxiv](https://arxiv.org/abs/2403.09638)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09638.md)]
- GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary
  Robotic Grasping - [[Arxiv](https://arxiv.org/abs/2403.09637)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09637.md)]
- Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference - [[Arxiv](https://arxiv.org/abs/2403.09636)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09636.md)]
- Transformers Get Stable: An End-to-End Signal Propagation Theory for
  Language Models - [[Arxiv](https://arxiv.org/abs/2403.09635)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09635.md)]
- OneTracker: Unifying Visual Object Tracking with Foundation Models and
  Efficient Tuning - [[Arxiv](https://arxiv.org/abs/2403.09634)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09634.md)]
- Holo-Relighting: Controllable Volumetric Portrait Relighting from a
  Single Image - [[Arxiv](https://arxiv.org/abs/2403.09632)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09632.md)]
- 3D-VLA: A 3D Vision-Language-Action Generative World Model - [[Arxiv](https://arxiv.org/abs/2403.09631)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09631.md)]
- Generalized Predictive Model for Autonomous Driving - [[Arxiv](https://arxiv.org/abs/2403.09630)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09630.md)]
- Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking - [[Arxiv](https://arxiv.org/abs/2403.09629)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09629.md)]
- Video Mamba Suite: State Space Model as a Versatile Alternative for
  Video Understanding - [[Arxiv](https://arxiv.org/abs/2403.09626)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09626.md)]
- Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation - [[Arxiv](https://arxiv.org/abs/2403.09625)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09625.md)]
- Score-Guided Diffusion for 3D Human Recovery - [[Arxiv](https://arxiv.org/abs/2403.09623)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09623.md)]
- Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering - [[Arxiv](https://arxiv.org/abs/2403.09622)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09622.md)]
- Minimax Optimal and Computationally Efficient Algorithms for
  Distributionally Robust Offline Reinforcement Learning - [[Arxiv](https://arxiv.org/abs/2403.09621)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09621.md)]
- PosSAM: Panoptic Open-vocabulary Segment Anything - [[Arxiv](https://arxiv.org/abs/2403.09620)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09620.md)]
- Explore In-Context Segmentation via Latent Diffusion Models - [[Arxiv](https://arxiv.org/abs/2403.09616)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09616.md)]
- Reawakening knowledge: Anticipatory recovery from catastrophic
  interference via structured training - [[Arxiv](https://arxiv.org/abs/2403.09613)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09613.md)]
- Compute-first optical detection for noise-resilient visual perception - [[Arxiv](https://arxiv.org/abs/2403.09612)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09612.md)]
- MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training - [[Arxiv](https://arxiv.org/abs/2403.09611)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09611.md)]
- Large Language Models and Causal Inference in Collaboration: A
  Comprehensive Survey - [[Arxiv](https://arxiv.org/abs/2403.09606)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09606.md)]
- Counterfactual contrastive learning: robust representations via causal
  image synthesis - [[Arxiv](https://arxiv.org/abs/2403.09605)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09605.md)]
- Optimistic Verifiable Training by Controlling Hardware Nondeterminism - [[Arxiv](https://arxiv.org/abs/2403.09603)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09603.md)]
- Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds - [[Arxiv](https://arxiv.org/abs/2403.09598)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09598.md)]
- Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM
  and Dense Submaps Built without LiDAR - [[Arxiv](https://arxiv.org/abs/2403.09596)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2403.09596.md)]

## List of Papers by Year
- [Papers for 2024](https://github.com/taesiri/ArXivQA/blob/main/Papers-2024.md)
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.1` API.
