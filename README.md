# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- GLaMM: Pixel Grounding Large Multimodal Model - [[Arxiv](https://arxiv.org/abs/2311.03356)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03356.md)]
- CoVLM: Composing Visual Entities and Relationships in Large Language
  Models Via Communicative Decoding - [[Arxiv](https://arxiv.org/abs/2311.03354)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03354.md)]
- Ziya2: Data-centric Learning is All LLMs Need - [[Arxiv](https://arxiv.org/abs/2311.03301)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03301.md)]
- S-LoRA: Serving Thousands of Concurrent LoRA Adapters - [[Arxiv](https://arxiv.org/abs/2311.03285)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03285.md)]
- LDM3D-VR: Latent Diffusion Model for 3D VR - [[Arxiv](https://arxiv.org/abs/2311.03226)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03226.md)]
- CogVLM: Visual Expert for Pretrained Language Models - [[Arxiv](https://arxiv.org/abs/2311.03079)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03079.md)]
- Co-training and Co-distillation for Quality Improvement and Compression
  of Language Models - [[Arxiv](https://arxiv.org/abs/2311.02849)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02849.md)]
- Consistent4D: Consistent 360Â° Dynamic Object Generation from
  Monocular Video - [[Arxiv](https://arxiv.org/abs/2311.02848)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02848.md)]
- Tailoring Self-Rationalizers with Multi-Reward Distillation - [[Arxiv](https://arxiv.org/abs/2311.02805)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02805.md)]
- Attention or Convolution: Transformer Encoders in Audio Language Models
  for Inference Efficiency - [[Arxiv](https://arxiv.org/abs/2311.02772)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02772.md)]
- VR-NeRF: High-Fidelity Virtualized Walkable Spaces - [[Arxiv](https://arxiv.org/abs/2311.02542)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02542.md)]
- Levels of AGI: Operationalizing Progress on the Path to AGI - [[Arxiv](https://arxiv.org/abs/2311.02462)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02462.md)]
- Ultra-Long Sequence Distributed Transformer - [[Arxiv](https://arxiv.org/abs/2311.02382)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02382.md)]
- MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning - [[Arxiv](https://arxiv.org/abs/2311.02303)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02303.md)]
- Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs - [[Arxiv](https://arxiv.org/abs/2311.02262)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02262.md)]
- EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via
  Self-Supervision - [[Arxiv](https://arxiv.org/abs/2311.02077)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02077.md)]
- PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task
  Completion - [[Arxiv](https://arxiv.org/abs/2311.01767)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01767.md)]
- FLAP: Fast Language-Audio Pre-training - [[Arxiv](https://arxiv.org/abs/2311.01615)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01615.md)]
- Idempotent Generative Network - [[Arxiv](https://arxiv.org/abs/2311.01462)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01462.md)]
- RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning
  via Generative Simulation - [[Arxiv](https://arxiv.org/abs/2311.01455)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01455.md)]
- FlashDecoding++: Faster Large Language Model Inference on GPUs - [[Arxiv](https://arxiv.org/abs/2311.01282)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01282.md)]
- E3 TTS: Easy End-to-End Diffusion-based Text to Speech - [[Arxiv](https://arxiv.org/abs/2311.00945)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.00945.md)]
- RoboVQA: Multimodal Long-Horizon Reasoning for Robotics - [[Arxiv](https://arxiv.org/abs/2311.00899)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.00899.md)]
- In-Context Prompt Editing For Conditional Audio Generation - [[Arxiv](https://arxiv.org/abs/2311.00895)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.00895.md)]
- Relax: Composable Abstractions for End-to-End Dynamic Machine Learning - [[Arxiv](https://arxiv.org/abs/2311.02103)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02103.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
