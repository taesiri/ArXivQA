# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for
  Multi-modal Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.07575)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07575.md)]
- To See is to Believe: Prompting GPT-4V for Better Visual Instruction
  Tuning - [[Arxiv](https://arxiv.org/abs/2311.07574)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07574.md)]
- GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone
  GUI Navigation - [[Arxiv](https://arxiv.org/abs/2311.07562)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07562.md)]
- MEGAVERSE: Benchmarking Large Language Models Across Languages,
  Modalities, Models and Tasks - [[Arxiv](https://arxiv.org/abs/2311.07463)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07463.md)]
- Story-to-Motion: Synthesizing Infinite and Controllable Character
  Animation from Long Text - [[Arxiv](https://arxiv.org/abs/2311.07446)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07446.md)]
- Music ControlNet: Multiple Time-varying Controls for Music Generation - [[Arxiv](https://arxiv.org/abs/2311.07069)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.07069.md)]
- Q-Instruct: Improving Low-level Visual Abilities for Multi-modality
  Foundation Models - [[Arxiv](https://arxiv.org/abs/2311.06783)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06783.md)]
- ChatAnything: Facetime Chat with LLM-Enhanced Personas - [[Arxiv](https://arxiv.org/abs/2311.06772)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06772.md)]
- Towards General-Purpose Speech Abilities for Large Language Models Using
  Unpaired Data - [[Arxiv](https://arxiv.org/abs/2311.06753)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06753.md)]
- Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small
  Scorer - [[Arxiv](https://arxiv.org/abs/2311.06720)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06720.md)]
- Trusted Source Alignment in Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.06697)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06697.md)]
- LayoutPrompter: Awaken the Design Ability of Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.06495)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06495.md)]
- GOAT: GO to Any Thing - [[Arxiv](https://arxiv.org/abs/2311.06430)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06430.md)]
- Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization - [[Arxiv](https://arxiv.org/abs/2311.06243)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06243.md)]
- Florence-2: Advancing a Unified Representation for a Variety of Vision
  Tasks - [[Arxiv](https://arxiv.org/abs/2311.06242)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06242.md)]
- Instant3D: Fast Text-to-3D with Sparse-View Generation and Large
  Reconstruction Model - [[Arxiv](https://arxiv.org/abs/2311.06214)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06214.md)]
- Language Models can be Logical Solvers - [[Arxiv](https://arxiv.org/abs/2311.06158)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.06158.md)]
- JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal
  Language Models - [[Arxiv](https://arxiv.org/abs/2311.05997)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05997.md)]
- FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor
  Cores - [[Arxiv](https://arxiv.org/abs/2311.05908)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05908.md)]
- Hiformer: Heterogeneous Feature Interactions Learning with Transformers
  for Recommender Systems - [[Arxiv](https://arxiv.org/abs/2311.05884)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05884.md)]
- PolyMaX: General Dense Prediction with Mask Transformer - [[Arxiv](https://arxiv.org/abs/2311.05770)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05770.md)]
- FMViT: A multiple-frequency mixing Vision Transformer - [[Arxiv](https://arxiv.org/abs/2311.05707)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05707.md)]
- Mirasol3B: A Multimodal Autoregressive model for time-aligned and
  contextual modalities - [[Arxiv](https://arxiv.org/abs/2311.05698)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05698.md)]
- LCM-LoRA: A Universal Stable-Diffusion Acceleration Module - [[Arxiv](https://arxiv.org/abs/2311.05556)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05556.md)]
- LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents - [[Arxiv](https://arxiv.org/abs/2311.05437)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.05437.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
