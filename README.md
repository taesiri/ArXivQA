# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- OtterHD: A High-Resolution Multi-modality Model - [[Arxiv](https://arxiv.org/abs/2311.04219)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04219.md)]
- Video Instance Matting - [[Arxiv](https://arxiv.org/abs/2311.04212)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04212.md)]
- I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion
  Models - [[Arxiv](https://arxiv.org/abs/2311.04145)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04145.md)]
- Unveiling Safety Vulnerabilities of Large Language Models - [[Arxiv](https://arxiv.org/abs/2311.04124)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.04124.md)]
- Leveraging Large Language Models for Automated Proof Synthesis in Rust - [[Arxiv](https://arxiv.org/abs/2311.03739)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03739.md)]
- Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent
  Learning - [[Arxiv](https://arxiv.org/abs/2311.03736)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03736.md)]
- Random Field Augmentations for Self-Supervised Representation Learning - [[Arxiv](https://arxiv.org/abs/2311.03629)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03629.md)]
- SoundCam: A Dataset for Finding Humans Using Room Acoustics - [[Arxiv](https://arxiv.org/abs/2311.03517)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03517.md)]
- GLaMM: Pixel Grounding Large Multimodal Model - [[Arxiv](https://arxiv.org/abs/2311.03356)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03356.md)]
- CoVLM: Composing Visual Entities and Relationships in Large Language
  Models Via Communicative Decoding - [[Arxiv](https://arxiv.org/abs/2311.03354)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03354.md)]
- Ziya2: Data-centric Learning is All LLMs Need - [[Arxiv](https://arxiv.org/abs/2311.03301)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03301.md)]
- S-LoRA: Serving Thousands of Concurrent LoRA Adapters - [[Arxiv](https://arxiv.org/abs/2311.03285)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03285.md)]
- LDM3D-VR: Latent Diffusion Model for 3D VR - [[Arxiv](https://arxiv.org/abs/2311.03226)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03226.md)]
- CogVLM: Visual Expert for Pretrained Language Models - [[Arxiv](https://arxiv.org/abs/2311.03079)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.03079.md)]
- Co-training and Co-distillation for Quality Improvement and Compression
  of Language Models - [[Arxiv](https://arxiv.org/abs/2311.02849)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02849.md)]
- Consistent4D: Consistent 360Â° Dynamic Object Generation from
  Monocular Video - [[Arxiv](https://arxiv.org/abs/2311.02848)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02848.md)]
- Tailoring Self-Rationalizers with Multi-Reward Distillation - [[Arxiv](https://arxiv.org/abs/2311.02805)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02805.md)]
- Attention or Convolution: Transformer Encoders in Audio Language Models
  for Inference Efficiency - [[Arxiv](https://arxiv.org/abs/2311.02772)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02772.md)]
- VR-NeRF: High-Fidelity Virtualized Walkable Spaces - [[Arxiv](https://arxiv.org/abs/2311.02542)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02542.md)]
- Levels of AGI: Operationalizing Progress on the Path to AGI - [[Arxiv](https://arxiv.org/abs/2311.02462)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02462.md)]
- Ultra-Long Sequence Distributed Transformer - [[Arxiv](https://arxiv.org/abs/2311.02382)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02382.md)]
- MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning - [[Arxiv](https://arxiv.org/abs/2311.02303)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02303.md)]
- Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs - [[Arxiv](https://arxiv.org/abs/2311.02262)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02262.md)]
- EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via
  Self-Supervision - [[Arxiv](https://arxiv.org/abs/2311.02077)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.02077.md)]
- PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task
  Completion - [[Arxiv](https://arxiv.org/abs/2311.01767)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2311.01767.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.0` API.
