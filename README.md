# Automated Question Answering with ArXiv Papers

## Latest 25 Papers
- Do Androids Know They're Only Dreaming of Electric Sheep? - [[Arxiv](https://arxiv.org/abs/2312.17249)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17249.md)]
- The LLM Surgeon - [[Arxiv](https://arxiv.org/abs/2312.17244)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17244.md)]
- Learning to Generate Text in Arbitrary Writing Styles - [[Arxiv](https://arxiv.org/abs/2312.17242)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17242.md)]
- SparseProp: Efficient Event-Based Simulation and Training of Sparse
  Recurrent Spiking Neural Networks - [[Arxiv](https://arxiv.org/abs/2312.17216)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17216.md)]
- Control Barrier Function Based UAV Safety Controller in Autonomous
  Airborne Tracking and Following Systems - [[Arxiv](https://arxiv.org/abs/2312.17215)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17215.md)]
- Virtual Scientific Companion for Synchrotron Beamlines: A Prototype - [[Arxiv](https://arxiv.org/abs/2312.17180)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17180.md)]
- Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,
  Language, Audio, and Action - [[Arxiv](https://arxiv.org/abs/2312.17172)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17172.md)]
- InsActor: Instruction-driven Physics-based Characters - [[Arxiv](https://arxiv.org/abs/2312.17135)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17135.md)]
- Large Language Model for Causal Decision Making - [[Arxiv](https://arxiv.org/abs/2312.17122)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17122.md)]
- Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale
  Pretraining Corpus for Math - [[Arxiv](https://arxiv.org/abs/2312.17120)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17120.md)]
- Generalizable Visual Reinforcement Learning with Segment Anything Model - [[Arxiv](https://arxiv.org/abs/2312.17116)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17116.md)]
- How Far Are We from Believable AI Agents? A Framework for Evaluating the
  Believability of Human Behavior Simulation - [[Arxiv](https://arxiv.org/abs/2312.17115)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17115.md)]
- Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of
  Plants - [[Arxiv](https://arxiv.org/abs/2312.17110)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17110.md)]
- MIVC: Multiple Instance Visual Component for Visual-Language Models - [[Arxiv](https://arxiv.org/abs/2312.17109)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17109.md)]
- Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined
  Levels - [[Arxiv](https://arxiv.org/abs/2312.17090)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17090.md)]
- Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil
  Cognitive Depth in LLMs - [[Arxiv](https://arxiv.org/abs/2312.17080)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17080.md)]
- Minimally-intrusive Navigation in Dense Crowds with Integrated Macro and
  Micro-level Dynamics - [[Arxiv](https://arxiv.org/abs/2312.17076)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17076.md)]
- Improving In-context Learning via Bidirectional Alignment - [[Arxiv](https://arxiv.org/abs/2312.17055)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17055.md)]
- Length Extrapolation of Transformers: A Survey from the Perspective of
  Position Encoding - [[Arxiv](https://arxiv.org/abs/2312.17044)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17044.md)]
- Experiential Co-Learning of Software-Developing Agents - [[Arxiv](https://arxiv.org/abs/2312.17025)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.17025.md)]
- Unified Lattice Graph Fusion for Chinese Named Entity Recognition - [[Arxiv](https://arxiv.org/abs/2312.16917)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.16917.md)]
- Spike No More: Stabilizing the Pre-training of Large Language Models - [[Arxiv](https://arxiv.org/abs/2312.16903)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.16903.md)]
- BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence - [[Arxiv](https://arxiv.org/abs/2312.16893)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.16893.md)]
- OmniDialog: An Omnipotent Pre-training Model for Task-Oriented Dialogue
  System - [[Arxiv](https://arxiv.org/abs/2312.16864)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.16864.md)]
- TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones - [[Arxiv](https://arxiv.org/abs/2312.16862)] [[QA](https://github.com/taesiri/ArXivQA/blob/main/papers/2312.16862.md)]

## List of Papers by Year
- [Papers for 2023](https://github.com/taesiri/ArXivQA/blob/main/Papers-2023.md)
- [Papers for 2022](https://github.com/taesiri/ArXivQA/blob/main/Papers-2022.md)
- [Papers for 2021](https://github.com/taesiri/ArXivQA/blob/main/Papers-2021.md)
- [Papers for 2020](https://github.com/taesiri/ArXivQA/blob/main/Papers-2020.md)
- [Papers for 2019](https://github.com/taesiri/ArXivQA/blob/main/Papers-2019.md)
- [Papers for 2018](https://github.com/taesiri/ArXivQA/blob/main/Papers-2018.md)
- [Papers for 2017](https://github.com/taesiri/ArXivQA/blob/main/Papers-2017.md)
- [Papers for 2016](https://github.com/taesiri/ArXivQA/blob/main/Papers-2016.md)
- [Papers for 2015](https://github.com/taesiri/ArXivQA/blob/main/Papers-2015.md)
- [Papers for 2014](https://github.com/taesiri/ArXivQA/blob/main/Papers-2014.md)
- [Papers for 2013](https://github.com/taesiri/ArXivQA/blob/main/Papers-2013.md)
- [Papers for 2012](https://github.com/taesiri/ArXivQA/blob/main/Papers-2012.md)
- [Papers for 2010](https://github.com/taesiri/ArXivQA/blob/main/Papers-2010.md)
- [Papers for 2009](https://github.com/taesiri/ArXivQA/blob/main/Papers-2009.md)

## Acknowledgements
This project is made possible through the generous support of [Anthropic](https://www.anthropic.com/), who provided free access to the `Claude-2.1` API.
