# List of Papers

## 2023

### September 2023
- Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D   Understanding, Generation, and Instruction Following - [[ArXiv](https://arxiv.org/abs/2309.00615)] [[QA](./papers/2309.00615.md)].
- CityDreamer: Compositional Generative Model of Unbounded 3D Cities - [[ArXiv](https://arxiv.org/abs/2309.00610)] [[QA](./papers/2309.00610.md)].
- VideoGen: A Reference-Guided Latent Diffusion Approach for High   Definition Text-to-Video Generation - [[ArXiv](https://arxiv.org/abs/2309.00398)] [[QA](./papers/2309.00398.md)].
- Large Content And Behavior Models To Understand, Simulate, And Optimize   Content And Behavior - [[ArXiv](https://arxiv.org/abs/2309.00359)] [[QA](./papers/2309.00359.md)].
- RLAIF: Scaling Reinforcement Learning from Human Feedback with AI   Feedback - [[ArXiv](https://arxiv.org/abs/2309.00267)] [[QA](./papers/2309.00267.md)].

### August 2023
- YaRN: Efficient Context Window Extension of Large Language Models - [[ArXiv](https://arxiv.org/abs/2309.00071)] [[QA](./papers/2309.00071.md)].
- FACET: Fairness in Computer Vision Evaluation Benchmark - [[ArXiv](https://arxiv.org/abs/2309.00035)] [[QA](./papers/2309.00035.md)].
- GNFactor: Multi-Task Real Robot Learning with Generalizable Neural   Feature Fields - [[ArXiv](https://arxiv.org/abs/2308.16891)] [[QA](./papers/2308.16891.md)].
- The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122   Language Variants - [[ArXiv](https://arxiv.org/abs/2308.16884)] [[QA](./papers/2308.16884.md)].
- SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame   Interpolation - [[ArXiv](https://arxiv.org/abs/2308.16876)] [[QA](./papers/2308.16876.md)].
- Can Programming Languages Boost Each Other via Instruction Tuning? - [[ArXiv](https://arxiv.org/abs/2308.16824)] [[QA](./papers/2308.16824.md)].
- Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size   HD Images - [[ArXiv](https://arxiv.org/abs/2308.16582)] [[QA](./papers/2308.16582.md)].
- MVDream: Multi-view Diffusion for 3D Generation - [[ArXiv](https://arxiv.org/abs/2308.16512)] [[QA](./papers/2308.16512.md)].
- BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual   Pragmatic Knowledge - [[ArXiv](https://arxiv.org/abs/2308.16458)] [[QA](./papers/2308.16458.md)].
- Emergence of Segmentation with Minimalistic White-Box Transformers - [[ArXiv](https://arxiv.org/abs/2308.16271)] [[QA](./papers/2308.16271.md)].
- Active Neural Mapping - [[ArXiv](https://arxiv.org/abs/2308.16246)] [[QA](./papers/2308.16246.md)].
- Learning Vision-based Pursuit-Evasion Robot Policies - [[ArXiv](https://arxiv.org/abs/2308.16185)] [[QA](./papers/2308.16185.md)].
- LM-Infinite: Simple On-the-Fly Length Generalization for Large Language   Models - [[ArXiv](https://arxiv.org/abs/2308.16137)] [[QA](./papers/2308.16137.md)].
- RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation - [[ArXiv](https://arxiv.org/abs/2308.15975)] [[QA](./papers/2308.15975.md)].
- WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model - [[ArXiv](https://arxiv.org/abs/2308.15962)] [[QA](./papers/2308.15962.md)].
- LLaSM: Large Language and Speech Model - [[ArXiv](https://arxiv.org/abs/2308.15930)] [[QA](./papers/2308.15930.md)].
- WeatherBench 2: A benchmark for the next generation of data-driven   global weather models - [[ArXiv](https://arxiv.org/abs/2308.15560)] [[QA](./papers/2308.15560.md)].
- Evaluation and Analysis of Hallucination in Large Vision-Language Models - [[ArXiv](https://arxiv.org/abs/2308.15126)] [[QA](./papers/2308.15126.md)].
- LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks - [[ArXiv](https://arxiv.org/abs/2308.14972)] [[QA](./papers/2308.14972.md)].
- MedAlign: A Clinician-Generated Dataset for Instruction Following with   Electronic Medical Records - [[ArXiv](https://arxiv.org/abs/2308.14089)] [[QA](./papers/2308.14089.md)].
- ORES: Open-vocabulary Responsible Visual Synthesis - [[ArXiv](https://arxiv.org/abs/2308.13785)] [[QA](./papers/2308.13785.md)].
- ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon   Sequential Task Planning - [[ArXiv](https://arxiv.org/abs/2308.13724)] [[QA](./papers/2308.13724.md)].
- Eventful Transformers: Leveraging Temporal Redundancy in Vision   Transformers - [[ArXiv](https://arxiv.org/abs/2308.13494)] [[QA](./papers/2308.13494.md)].
- Position-Enhanced Visual Instruction Tuning for Multimodal Large   Language Models - [[ArXiv](https://arxiv.org/abs/2308.13437)] [[QA](./papers/2308.13437.md)].
- Nougat: Neural Optical Understanding for Academic Documents - [[ArXiv](https://arxiv.org/abs/2308.13418)] [[QA](./papers/2308.13418.md)].
- SoTaNa: The Open-Source Software Development Assistant - [[ArXiv](https://arxiv.org/abs/2308.13416)] [[QA](./papers/2308.13416.md)].
- Relighting Neural Radiance Fields with Shadow and Highlight Hints - [[ArXiv](https://arxiv.org/abs/2308.13404)] [[QA](./papers/2308.13404.md)].
- OmniQuant: Omnidirectionally Calibrated Quantization for Large Language   Models - [[ArXiv](https://arxiv.org/abs/2308.13137)] [[QA](./papers/2308.13137.md)].
- MLLM-DataEngine: An Iterative Refinement Approach for MLLM - [[ArXiv](https://arxiv.org/abs/2308.13566)] [[QA](./papers/2308.13566.md)].
- Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities - [[ArXiv](https://arxiv.org/abs/2308.12966)] [[QA](./papers/2308.12966.md)].
- Towards Realistic Zero-Shot Classification via Self Structural Semantic   Alignment - [[ArXiv](https://arxiv.org/abs/2308.12960)] [[QA](./papers/2308.12960.md)].
- Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language   Pretraining? - [[ArXiv](https://arxiv.org/abs/2308.12898)] [[QA](./papers/2308.12898.md)].
- VIGC: Visual Instruction Generation and Correction - [[ArXiv](https://arxiv.org/abs/2308.12714)] [[QA](./papers/2308.12714.md)].
- CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from   Unbounded Synthesized Images - [[ArXiv](https://arxiv.org/abs/2308.12288)] [[QA](./papers/2308.12288.md)].
- Diffusion Language Models Can Perform Many Tasks with Scaling and   Instruction-Finetuning - [[ArXiv](https://arxiv.org/abs/2308.12219)] [[QA](./papers/2308.12219.md)].
- StoryBench: A Multifaceted Benchmark for Continuous Story Visualization - [[ArXiv](https://arxiv.org/abs/2308.11606)] [[QA](./papers/2308.11606.md)].
- ProAgent: Building Proactive Cooperative AI with Large Language Models - [[ArXiv](https://arxiv.org/abs/2308.11339)] [[QA](./papers/2308.11339.md)].
- ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts - [[ArXiv](https://arxiv.org/abs/2308.11236)] [[QA](./papers/2308.11236.md)].
- WanJuan: A Comprehensive Multimodal Dataset for Advancing English and   Chinese Large Models - [[ArXiv](https://arxiv.org/abs/2308.10755)] [[QA](./papers/2308.10755.md)].
- On the Adversarial Robustness of Multi-Modal Foundation Models - [[ArXiv](https://arxiv.org/abs/2308.10741)] [[QA](./papers/2308.10741.md)].
- StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized   Image-Dialogue Data - [[ArXiv](https://arxiv.org/abs/2308.10253)] [[QA](./papers/2308.10253.md)].
- ViT-Lens: Towards Omni-modal Representations - [[ArXiv](https://arxiv.org/abs/2308.10185)] [[QA](./papers/2308.10185.md)].
- ExpeL: LLM Agents Are Experiential Learners - [[ArXiv](https://arxiv.org/abs/2308.10144)] [[QA](./papers/2308.10144.md)].
- March in Chat: Interactive Prompting for Remote Embodied Referring   Expression - [[ArXiv](https://arxiv.org/abs/2308.10141)] [[QA](./papers/2308.10141.md)].
- Tackling Vision Language Tasks Through Learning Inner Monologues - [[ArXiv](https://arxiv.org/abs/2308.09970)] [[QA](./papers/2308.09970.md)].
- BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual   Questions - [[ArXiv](https://arxiv.org/abs/2308.09936)] [[QA](./papers/2308.09936.md)].
- PUMGPT: A Large Vision-Language Model for Product Understanding - [[ArXiv](https://arxiv.org/abs/2308.09568)] [[QA](./papers/2308.09568.md)].
- TeCH: Text-guided Reconstruction of Lifelike Clothed Humans - [[ArXiv](https://arxiv.org/abs/2308.08545)] [[QA](./papers/2308.08545.md)].
- Dual-Stream Diffusion Net for Text-to-Video Generation - [[ArXiv](https://arxiv.org/abs/2308.08316)] [[QA](./papers/2308.08316.md)].
- SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes - [[ArXiv](https://arxiv.org/abs/2308.08258)] [[QA](./papers/2308.08258.md)].
- DragNUWA: Fine-grained Control in Video Generation by Integrating Text,   Image, and Trajectory - [[ArXiv](https://arxiv.org/abs/2308.08089)] [[QA](./papers/2308.08089.md)].
- Teach LLMs to Personalize -- An Approach inspired by Writing Education - [[ArXiv](https://arxiv.org/abs/2308.07968)] [[QA](./papers/2308.07968.md)].
- CoDeF: Content Deformation Fields for Temporally Consistent Video   Processing - [[ArXiv](https://arxiv.org/abs/2308.07926)] [[QA](./papers/2308.07926.md)].
- RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder   Language Models - [[ArXiv](https://arxiv.org/abs/2308.07922)] [[QA](./papers/2308.07922.md)].
- Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with   Code-based Self-Verification - [[ArXiv](https://arxiv.org/abs/2308.07921)] [[QA](./papers/2308.07921.md)].
- Relightable and Animatable Neural Avatar from Sparse-View Video - [[ArXiv](https://arxiv.org/abs/2308.07903)] [[QA](./papers/2308.07903.md)].
- Link-Context Learning for Multimodal LLMs - [[ArXiv](https://arxiv.org/abs/2308.07891)] [[QA](./papers/2308.07891.md)].
- Learning to Identify Critical States for Reinforcement Learning from   Videos - [[ArXiv](https://arxiv.org/abs/2308.07795)] [[QA](./papers/2308.07795.md)].
- Text Injection for Capitalization and Turn-Taking Prediction in Speech   Models - [[ArXiv](https://arxiv.org/abs/2308.07395)] [[QA](./papers/2308.07395.md)].
- Platypus: Quick, Cheap, and Powerful Refinement of LLMs - [[ArXiv](https://arxiv.org/abs/2308.07317)] [[QA](./papers/2308.07317.md)].
- Jurassic World Remake: Bringing Ancient Fossils Back to Life via   Zero-Shot Long Image-to-Image Translation - [[ArXiv](https://arxiv.org/abs/2308.07316)] [[QA](./papers/2308.07316.md)].
- The Devil is in the Errors: Leveraging Large Language Models for   Fine-grained Machine Translation Evaluation - [[ArXiv](https://arxiv.org/abs/2308.07286)] [[QA](./papers/2308.07286.md)].
- RestoreFormer++: Towards Real-World Blind Face Restoration from   Undegraded Key-Value Pairs - [[ArXiv](https://arxiv.org/abs/2308.07228)] [[QA](./papers/2308.07228.md)].
- OctoPack: Instruction Tuning Code Large Language Models - [[ArXiv](https://arxiv.org/abs/2308.07124)] [[QA](./papers/2308.07124.md)].
- CausalLM is not optimal for in-context learning - [[ArXiv](https://arxiv.org/abs/2308.06912)] [[QA](./papers/2308.06912.md)].
- SpeechX: Neural Codec Language Model as a Versatile Speech Transformer - [[ArXiv](https://arxiv.org/abs/2308.06873)] [[QA](./papers/2308.06873.md)].
- IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image   Diffusion Models - [[ArXiv](https://arxiv.org/abs/2308.06721)] [[QA](./papers/2308.06721.md)].
- VisIT-Bench: A Benchmark for Vision-Language Instruction Following   Inspired by Real-World Use - [[ArXiv](https://arxiv.org/abs/2308.06595)] [[QA](./papers/2308.06595.md)].
- Detecting and Preventing Hallucinations in Large Vision Language Models - [[ArXiv](https://arxiv.org/abs/2308.06394)] [[QA](./papers/2308.06394.md)].
- Enhancing Network Management Using Code Generated by Large Language   Models - [[ArXiv](https://arxiv.org/abs/2308.06261)] [[QA](./papers/2308.06261.md)].
- Self-Alignment with Instruction Backtranslation - [[ArXiv](https://arxiv.org/abs/2308.06259)] [[QA](./papers/2308.06259.md)].
- Improving Joint Speech-Text Representations Without Alignment - [[ArXiv](https://arxiv.org/abs/2308.06125)] [[QA](./papers/2308.06125.md)].
- Composable Function-preserving Expansions for Transformer Architectures - [[ArXiv](https://arxiv.org/abs/2308.06103)] [[QA](./papers/2308.06103.md)].
- BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents - [[ArXiv](https://arxiv.org/abs/2308.05960)] [[QA](./papers/2308.05960.md)].
- PIPPA: A Partially Synthetic Conversational Dataset - [[ArXiv](https://arxiv.org/abs/2308.05884)] [[QA](./papers/2308.05884.md)].
- Follow Anything: Open-set detection, tracking, and following in   real-time - [[ArXiv](https://arxiv.org/abs/2308.05737)] [[QA](./papers/2308.05737.md)].
- AudioLDM 2: Learning Holistic Audio Generation with Self-supervised   Pretraining - [[ArXiv](https://arxiv.org/abs/2308.05734)] [[QA](./papers/2308.05734.md)].
- PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers - [[ArXiv](https://arxiv.org/abs/2308.05732)] [[QA](./papers/2308.05732.md)].
- Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language   Models' Alignment - [[ArXiv](https://arxiv.org/abs/2308.05374)] [[QA](./papers/2308.05374.md)].
- Flexible Isosurface Extraction for Gradient-Based Mesh Optimization - [[ArXiv](https://arxiv.org/abs/2308.05371)] [[QA](./papers/2308.05371.md)].
- OpenProteinSet: Training data for structural biology at scale - [[ArXiv](https://arxiv.org/abs/2308.05326)] [[QA](./papers/2308.05326.md)].
- Alexa, play with robot: Introducing the First Alexa Prize SimBot   Challenge on Embodied AI - [[ArXiv](https://arxiv.org/abs/2308.05221)] [[QA](./papers/2308.05221.md)].
- LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image   Generation - [[ArXiv](https://arxiv.org/abs/2308.05095)] [[QA](./papers/2308.05095.md)].
- JEN-1: Text-Guided Universal Music Generation with Omnidirectional   Diffusion Models - [[ArXiv](https://arxiv.org/abs/2308.04729)] [[QA](./papers/2308.04729.md)].
- Accelerating LLM Inference with Staged Speculative Decoding - [[ArXiv](https://arxiv.org/abs/2308.04623)] [[QA](./papers/2308.04623.md)].
- Shepherd: A Critic for Language Model Generation - [[ArXiv](https://arxiv.org/abs/2308.04592)] [[QA](./papers/2308.04592.md)].
- FocalFormer3D : Focusing on Hard Instance for 3D Object Detection - [[ArXiv](https://arxiv.org/abs/2308.04556)] [[QA](./papers/2308.04556.md)].
- 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment - [[ArXiv](https://arxiv.org/abs/2308.04352)] [[QA](./papers/2308.04352.md)].
- Empowering Vision-Language Models to Follow Interleaved Vision-Language   Instructions - [[ArXiv](https://arxiv.org/abs/2308.04152)] [[QA](./papers/2308.04152.md)].
- OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion   and Infinite Data Generation - [[ArXiv](https://arxiv.org/abs/2308.04126)] [[QA](./papers/2308.04126.md)].
- Gentopia: A Collaborative Platform for Tool-Augmented LLMs - [[ArXiv](https://arxiv.org/abs/2308.04030)] [[QA](./papers/2308.04030.md)].
- Tiny LVLM-eHub: Early Multimodal Experiments with Bard - [[ArXiv](https://arxiv.org/abs/2308.03729)] [[QA](./papers/2308.03729.md)].
- AgentBench: Evaluating LLMs as Agents - [[ArXiv](https://arxiv.org/abs/2308.03688)] [[QA](./papers/2308.03688.md)].
- Learning Concise and Descriptive Attributes for Visual Recognition - [[ArXiv](https://arxiv.org/abs/2308.03685)] [[QA](./papers/2308.03685.md)].
- Foundation Model based Open Vocabulary Task Planning and Executive   System for General Purpose Service Robots - [[ArXiv](https://arxiv.org/abs/2308.03357)] [[QA](./papers/2308.03357.md)].
- MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities - [[ArXiv](https://arxiv.org/abs/2308.02490)] [[QA](./papers/2308.02490.md)].
- The All-Seeing Project: Towards Panoptic Visual Recognition and   Understanding of the Open World - [[ArXiv](https://arxiv.org/abs/2308.01907)] [[QA](./papers/2308.01907.md)].
- DETR Doesn't Need Multi-Scale or Locality Design - [[ArXiv](https://arxiv.org/abs/2308.01904)] [[QA](./papers/2308.01904.md)].
- Scaling Relationship on Learning Mathematical Reasoning with Large   Language Models - [[ArXiv](https://arxiv.org/abs/2308.01825)] [[QA](./papers/2308.01825.md)].
- RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic   and Regional Comprehension - [[ArXiv](https://arxiv.org/abs/2308.02299)] [[QA](./papers/2308.02299.md)].
- Ambient Adventures: Teaching ChatGPT on Developing Complex Stories - [[ArXiv](https://arxiv.org/abs/2308.01734)] [[QA](./papers/2308.01734.md)].
- MusicLDM: Enhancing Novelty in Text-to-Music Generation Using   Beat-Synchronous Mixup Strategies - [[ArXiv](https://arxiv.org/abs/2308.01546)] [[QA](./papers/2308.01546.md)].
- Multimodal Neurons in Pretrained Text-Only Transformers - [[ArXiv](https://arxiv.org/abs/2308.01544)] [[QA](./papers/2308.01544.md)].
- TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality   Explorations - [[ArXiv](https://arxiv.org/abs/2308.01499)] [[QA](./papers/2308.01499.md)].
- HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose   Annotations, Affordances, and Reconstructions - [[ArXiv](https://arxiv.org/abs/2308.01477)] [[QA](./papers/2308.01477.md)].
- OpenFlamingo: An Open-Source Framework for Training Large Autoregressive   Vision-Language Models - [[ArXiv](https://arxiv.org/abs/2308.01390)] [[QA](./papers/2308.01390.md)].
- DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like   Models at All Scales - [[ArXiv](https://arxiv.org/abs/2308.01320)] [[QA](./papers/2308.01320.md)].
- Computational Long Exposure Mobile Photography - [[ArXiv](https://arxiv.org/abs/2308.01379)] [[QA](./papers/2308.01379.md)].
- More Context, Less Distraction: Visual Classification by Inferring and   Conditioning on Contextual Attributes - [[ArXiv](https://arxiv.org/abs/2308.01313)] [[QA](./papers/2308.01313.md)].
- Revisiting DETR Pre-training for Object Detection - [[ArXiv](https://arxiv.org/abs/2308.01300)] [[QA](./papers/2308.01300.md)].
- From Sparse to Soft Mixtures of Experts - [[ArXiv](https://arxiv.org/abs/2308.00951)] [[QA](./papers/2308.00951.md)].
- ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based   Image Manipulation - [[ArXiv](https://arxiv.org/abs/2308.00906)] [[QA](./papers/2308.00906.md)].
- LISA: Reasoning Segmentation via Large Language Model - [[ArXiv](https://arxiv.org/abs/2308.00692)] [[QA](./papers/2308.00692.md)].
- Tool Documentation Enables Zero-Shot Tool-Usage with Large Language   Models - [[ArXiv](https://arxiv.org/abs/2308.00675)] [[QA](./papers/2308.00675.md)].
- SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step   Reasoning - [[ArXiv](https://arxiv.org/abs/2308.00436)] [[QA](./papers/2308.00436.md)].
- Skills-in-Context Prompting: Unlocking Compositionality in Large   Language Models - [[ArXiv](https://arxiv.org/abs/2308.00304)] [[QA](./papers/2308.00304.md)].

### July 2023
- Predicting masked tokens in stochastic locations improves masked image   modeling - [[ArXiv](https://arxiv.org/abs/2308.00566)] [[QA](./papers/2308.00566.md)].
- Learning to Model the World with Language - [[ArXiv](https://arxiv.org/abs/2308.01399)] [[QA](./papers/2308.01399.md)].
- Discovering Adaptable Symbolic Algorithms from Scratch - [[ArXiv](https://arxiv.org/abs/2307.16890)] [[QA](./papers/2307.16890.md)].
- Virtual Prompt Injection for Instruction-Tuned Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.16888)] [[QA](./papers/2307.16888.md)].
- ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world   APIs - [[ArXiv](https://arxiv.org/abs/2307.16789)] [[QA](./papers/2307.16789.md)].
- UniVTG: Towards Unified Video-Language Temporal Grounding - [[ArXiv](https://arxiv.org/abs/2307.16715)] [[QA](./papers/2307.16715.md)].
- Guiding Image Captioning Models Toward More Specific Captions - [[ArXiv](https://arxiv.org/abs/2307.16686)] [[QA](./papers/2307.16686.md)].
- MovieChat: From Dense Token to Sparse Memory for Long Video   Understanding - [[ArXiv](https://arxiv.org/abs/2307.16449)] [[QA](./papers/2307.16449.md)].
- Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for   Complex Visual Reasoning Tasks - [[ArXiv](https://arxiv.org/abs/2307.16395)] [[QA](./papers/2307.16395.md)].
- LP-MusicCaps: LLM-Based Pseudo Music Captioning - [[ArXiv](https://arxiv.org/abs/2307.16372)] [[QA](./papers/2307.16372.md)].
- AntGPT: Can Large Language Models Help Long-term Action Anticipation   from Videos? - [[ArXiv](https://arxiv.org/abs/2307.16368)] [[QA](./papers/2307.16368.md)].
- Evaluating ChatGPT and GPT-4 for Visual Programming - [[ArXiv](https://arxiv.org/abs/2308.02522)] [[QA](./papers/2308.02522.md)].
- Unified Model for Image, Video, Audio and Language Tasks - [[ArXiv](https://arxiv.org/abs/2307.16184)] [[QA](./papers/2307.16184.md)].
- SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension - [[ArXiv](https://arxiv.org/abs/2307.16125)] [[QA](./papers/2307.16125.md)].
- RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic   Control - [[ArXiv](https://arxiv.org/abs/2307.15818)] [[QA](./papers/2307.15818.md)].
- The Hydra Effect: Emergent Self-repair in Language Model Computations - [[ArXiv](https://arxiv.org/abs/2307.15771)] [[QA](./papers/2307.15771.md)].
- Robust Distortion-free Watermarks for Language Models - [[ArXiv](https://arxiv.org/abs/2307.15593)] [[QA](./papers/2307.15593.md)].
- Exploring Format Consistency for Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2307.15504)] [[QA](./papers/2307.15504.md)].
- Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding - [[ArXiv](https://arxiv.org/abs/2307.15337)] [[QA](./papers/2307.15337.md)].
- Open Problems and Fundamental Limitations of Reinforcement Learning from   Human Feedback - [[ArXiv](https://arxiv.org/abs/2307.15217)] [[QA](./papers/2307.15217.md)].
- PromptStyler: Prompt-driven Style Generation for Source-free Domain   Generalization - [[ArXiv](https://arxiv.org/abs/2307.15199)] [[QA](./papers/2307.15199.md)].
- Med-Flamingo: a Multimodal Medical Few-shot Learner - [[ArXiv](https://arxiv.org/abs/2307.15189)] [[QA](./papers/2307.15189.md)].
- Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields - [[ArXiv](https://arxiv.org/abs/2307.15131)] [[QA](./papers/2307.15131.md)].
- To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation - [[ArXiv](https://arxiv.org/abs/2307.15063)] [[QA](./papers/2307.15063.md)].
- Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation - [[ArXiv](https://arxiv.org/abs/2308.07931)] [[QA](./papers/2308.07931.md)].
- TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis - [[ArXiv](https://arxiv.org/abs/2307.15042)] [[QA](./papers/2307.15042.md)].
- How Good is Google Bard's Visual Understanding? An Empirical Study on   Open Challenges - [[ArXiv](https://arxiv.org/abs/2307.15016)] [[QA](./papers/2307.15016.md)].
- Scaling TransNormer to 175 Billion Parameters - [[ArXiv](https://arxiv.org/abs/2307.14995)] [[QA](./papers/2307.14995.md)].
- PanGu-Coder2: Boosting Large Language Models for Code with Ranking   Feedback - [[ArXiv](https://arxiv.org/abs/2307.14936)] [[QA](./papers/2307.14936.md)].
- NeRF-Det: Learning Geometry-Aware Volumetric Representation for   Multi-View 3D Object Detection - [[ArXiv](https://arxiv.org/abs/2307.14620)] [[QA](./papers/2307.14620.md)].
- Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition - [[ArXiv](https://arxiv.org/abs/2307.14535)] [[QA](./papers/2307.14535.md)].
- MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation - [[ArXiv](https://arxiv.org/abs/2307.14460)] [[QA](./papers/2307.14460.md)].
- Three Bricks to Consolidate Watermarks for Large Language Models - [[ArXiv](https://arxiv.org/abs/2308.00113)] [[QA](./papers/2308.00113.md)].
- WavJourney: Compositional Audio Creation with Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.14335)] [[QA](./papers/2307.14335.md)].
- Towards Generalist Biomedical AI - [[ArXiv](https://arxiv.org/abs/2307.14334)] [[QA](./papers/2307.14334.md)].
- Large Language Models are Competitive Near Cold-start Recommenders for   Language- and Item-based Preferences - [[ArXiv](https://arxiv.org/abs/2307.14225)] [[QA](./papers/2307.14225.md)].
- Leveraging Implicit Feedback from Deployment Data in Dialogue - [[ArXiv](https://arxiv.org/abs/2307.14117)] [[QA](./papers/2307.14117.md)].
- Adaptive Frequency Filters As Efficient Global Token Mixers - [[ArXiv](https://arxiv.org/abs/2307.14008)] [[QA](./papers/2307.14008.md)].
- Tracking Anything in High Quality - [[ArXiv](https://arxiv.org/abs/2307.13974)] [[QA](./papers/2307.13974.md)].
- trajdata: A Unified Interface to Multiple Human Trajectory Datasets - [[ArXiv](https://arxiv.org/abs/2307.13924)] [[QA](./papers/2307.13924.md)].
- Points-to-3D: Bridging the Gap between Sparse Points and   Shape-Controllable Text-to-3D Generation - [[ArXiv](https://arxiv.org/abs/2307.13908)] [[QA](./papers/2307.13908.md)].
- WebArena: A Realistic Web Environment for Building Autonomous Agents - [[ArXiv](https://arxiv.org/abs/2307.13854)] [[QA](./papers/2307.13854.md)].
- How to Scale Your EMA - [[ArXiv](https://arxiv.org/abs/2307.13813)] [[QA](./papers/2307.13813.md)].
- Composite Diffusion | whole &gt;= Î£parts - [[ArXiv](https://arxiv.org/abs/2307.13720)] [[QA](./papers/2307.13720.md)].
- ARB: Advanced Reasoning Benchmark for Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.13692)] [[QA](./papers/2307.13692.md)].
- Predicting Code Coverage without Execution - [[ArXiv](https://arxiv.org/abs/2307.13383)] [[QA](./papers/2307.13383.md)].
- LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA   Composition - [[ArXiv](https://arxiv.org/abs/2307.13269)] [[QA](./papers/2307.13269.md)].
- Strivec: Sparse Tri-Vector Radiance Fields - [[ArXiv](https://arxiv.org/abs/2307.13226)] [[QA](./papers/2307.13226.md)].
- GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for   Task-Oriented Grasping - [[ArXiv](https://arxiv.org/abs/2307.13204)] [[QA](./papers/2307.13204.md)].
- Contrastive Example-Based Control - [[ArXiv](https://arxiv.org/abs/2307.13101)] [[QA](./papers/2307.13101.md)].
- LLM-Rec: Personalized Recommendation via Prompting Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.15780)] [[QA](./papers/2307.15780.md)].
- 3D-LLM: Injecting the 3D World into Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.12981)] [[QA](./papers/2307.12981.md)].
- Evaluating the Ripple Effects of Knowledge Editing in Language Models - [[ArXiv](https://arxiv.org/abs/2307.12976)] [[QA](./papers/2307.12976.md)].
- RLCD: Reinforcement Learning from Contrast Distillation for Language   Model Alignment - [[ArXiv](https://arxiv.org/abs/2307.12950)] [[QA](./papers/2307.12950.md)].
- A Real-World WebAgent with Planning, Long Context Understanding, and   Program Synthesis - [[ArXiv](https://arxiv.org/abs/2307.12856)] [[QA](./papers/2307.12856.md)].
- Multiscale Video Pretraining for Long-Term Activity Forecasting - [[ArXiv](https://arxiv.org/abs/2307.12854)] [[QA](./papers/2307.12854.md)].
- MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised   Learning of Motion and Content Features - [[ArXiv](https://arxiv.org/abs/2307.12698)] [[QA](./papers/2307.12698.md)].
- Less is More: Focus Attention for Efficient DETR - [[ArXiv](https://arxiv.org/abs/2307.12612)] [[QA](./papers/2307.12612.md)].
- Interpolating between Images with Diffusion Models - [[ArXiv](https://arxiv.org/abs/2307.12560)] [[QA](./papers/2307.12560.md)].
- PUMA: Secure Inference of LLaMA-7B in Five Minutes - [[ArXiv](https://arxiv.org/abs/2307.12533)] [[QA](./papers/2307.12533.md)].
- Optimized Network Architectures for Large Language Model Training with   Billions of Parameters - [[ArXiv](https://arxiv.org/abs/2307.12169)] [[QA](./papers/2307.12169.md)].
- CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction   Execution for Robots - [[ArXiv](https://arxiv.org/abs/2307.11865)] [[QA](./papers/2307.11865.md)].
- Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts - [[ArXiv](https://arxiv.org/abs/2307.11661)] [[QA](./papers/2307.11661.md)].
- CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields - [[ArXiv](https://arxiv.org/abs/2307.11526)] [[QA](./papers/2307.11526.md)].
- Prompting Large Language Models with Speech Recognition Abilities - [[ArXiv](https://arxiv.org/abs/2307.11795)] [[QA](./papers/2307.11795.md)].
- FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural   Radiance Fields - [[ArXiv](https://arxiv.org/abs/2307.11418)] [[QA](./papers/2307.11418.md)].
- Subject-Diffusion:Open Domain Personalized Text-to-Image Generation   without Test-time Fine-tuning - [[ArXiv](https://arxiv.org/abs/2307.11410)] [[QA](./papers/2307.11410.md)].
- Brain2Music: Reconstructing Music from Human Brain Activity - [[ArXiv](https://arxiv.org/abs/2307.11078)] [[QA](./papers/2307.11078.md)].
- PASTA: Pretrained Action-State Transformer Agents - [[ArXiv](https://arxiv.org/abs/2307.10936)] [[QA](./papers/2307.10936.md)].
- FLASK: Fine-grained Language Model Evaluation based on Alignment Skill   Sets - [[ArXiv](https://arxiv.org/abs/2307.10928)] [[QA](./papers/2307.10928.md)].
- Diffusion Sampling with Momentum for Mitigating Divergence Artifacts - [[ArXiv](https://arxiv.org/abs/2307.11118)] [[QA](./papers/2307.11118.md)].
- The Role of Entropy and Reconstruction in Multi-View Self-Supervised   Learning - [[ArXiv](https://arxiv.org/abs/2307.10907)] [[QA](./papers/2307.10907.md)].
- Meta-Transformer: A Unified Framework for Multimodal Learning - [[ArXiv](https://arxiv.org/abs/2307.10802)] [[QA](./papers/2307.10802.md)].
- SciBench: Evaluating College-Level Scientific Problem-Solving Abilities   of Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.10635)] [[QA](./papers/2307.10635.md)].
- Instruction-following Evaluation through Verbalizer Manipulation - [[ArXiv](https://arxiv.org/abs/2307.10558)] [[QA](./papers/2307.10558.md)].
- TokenFlow: Consistent Diffusion Features for Consistent Video Editing - [[ArXiv](https://arxiv.org/abs/2307.10373)] [[QA](./papers/2307.10373.md)].
- DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity   Human-centric Rendering - [[ArXiv](https://arxiv.org/abs/2307.10173)] [[QA](./papers/2307.10173.md)].
- DialogStudio: Towards Richest and Most Diverse Unified Dataset   Collection for Conversational AI - [[ArXiv](https://arxiv.org/abs/2307.10172)] [[QA](./papers/2307.10172.md)].
- Challenges and Applications of Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.10169)] [[QA](./papers/2307.10169.md)].
- LLMs as Workers in Human-Computational Algorithms? Replicating   Crowdsourcing Pipelines with LLMs - [[ArXiv](https://arxiv.org/abs/2307.10168)] [[QA](./papers/2307.10168.md)].
- Improving Multimodal Datasets with Image Captioning - [[ArXiv](https://arxiv.org/abs/2307.10350)] [[QA](./papers/2307.10350.md)].
- FABRIC: Personalizing Diffusion Models with Iterative Feedback - [[ArXiv](https://arxiv.org/abs/2307.10159)] [[QA](./papers/2307.10159.md)].
- Android in the Wild: A Large-Scale Dataset for Android Device Control - [[ArXiv](https://arxiv.org/abs/2307.10088)] [[QA](./papers/2307.10088.md)].
- Text2Layer: Layered Image Generation using Latent Diffusion Model - [[ArXiv](https://arxiv.org/abs/2307.09781)] [[QA](./papers/2307.09781.md)].
- Towards A Unified Agent with Foundation Models - [[ArXiv](https://arxiv.org/abs/2307.09668)] [[QA](./papers/2307.09668.md)].
- Promoting Exploration in Memory-Augmented Adam using Critical Momenta - [[ArXiv](https://arxiv.org/abs/2307.09638)] [[QA](./papers/2307.09638.md)].
- ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring   Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2307.09474)] [[QA](./papers/2307.09474.md)].
- Does Circuit Analysis Interpretability Scale? Evidence from Multiple   Choice Capabilities in Chinchilla - [[ArXiv](https://arxiv.org/abs/2307.09458)] [[QA](./papers/2307.09458.md)].
- Biomaker CA: a Biome Maker project using Cellular Automata - [[ArXiv](https://arxiv.org/abs/2307.09320)] [[QA](./papers/2307.09320.md)].
- Llama 2: Open Foundation and Fine-Tuned Chat Models - [[ArXiv](https://arxiv.org/abs/2307.09288)] [[QA](./papers/2307.09288.md)].
- Augmenting CLIP with Improved Visio-Linguistic Reasoning - [[ArXiv](https://arxiv.org/abs/2307.09233)] [[QA](./papers/2307.09233.md)].
- NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and   Repulsive UDF - [[ArXiv](https://arxiv.org/abs/2307.09112)] [[QA](./papers/2307.09112.md)].
- How is ChatGPT's behavior changing over time? - [[ArXiv](https://arxiv.org/abs/2307.09009)] [[QA](./papers/2307.09009.md)].
- Diffusion Models Beat GANs on Image Classification - [[ArXiv](https://arxiv.org/abs/2307.08702)] [[QA](./papers/2307.08702.md)].
- AlpaGasus: Training A Better Alpaca with Fewer Data - [[ArXiv](https://arxiv.org/abs/2307.08701)] [[QA](./papers/2307.08701.md)].
- TableGPT: Towards Unifying Tables, Nature Language and Commands into One   GPT - [[ArXiv](https://arxiv.org/abs/2307.08674)] [[QA](./papers/2307.08674.md)].
- Retentive Network: A Successor to Transformer for Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.08621)] [[QA](./papers/2307.08621.md)].
- BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs - [[ArXiv](https://arxiv.org/abs/2307.08581)] [[QA](./papers/2307.08581.md)].
- Scale-Aware Modulation Meet Transformer - [[ArXiv](https://arxiv.org/abs/2307.08579)] [[QA](./papers/2307.08579.md)].
- Does Visual Pretraining Help End-to-End Reasoning? - [[ArXiv](https://arxiv.org/abs/2307.08506)] [[QA](./papers/2307.08506.md)].
- Measuring Faithfulness in Chain-of-Thought Reasoning - [[ArXiv](https://arxiv.org/abs/2307.13702)] [[QA](./papers/2307.13702.md)].
- Question Decomposition Improves the Faithfulness of Model-Generated   Reasoning - [[ArXiv](https://arxiv.org/abs/2307.11768)] [[QA](./papers/2307.11768.md)].
- Planting a SEED of Vision in Large Language Model - [[ArXiv](https://arxiv.org/abs/2307.08041)] [[QA](./papers/2307.08041.md)].
- Language Conditioned Traffic Generation - [[ArXiv](https://arxiv.org/abs/2307.07947)] [[QA](./papers/2307.07947.md)].
- INVE: Interactive Neural Video Editing - [[ArXiv](https://arxiv.org/abs/2307.07663)] [[QA](./papers/2307.07663.md)].
- CoTracker: It is Better to Track Together - [[ArXiv](https://arxiv.org/abs/2307.07635)] [[QA](./papers/2307.07635.md)].
- NIFTY: Neural Object Interaction Fields for Guided Human Motion   Synthesis - [[ArXiv](https://arxiv.org/abs/2307.07511)] [[QA](./papers/2307.07511.md)].
- DreamTeacher: Pretraining Image Backbones with Deep Generative Models - [[ArXiv](https://arxiv.org/abs/2307.07487)] [[QA](./papers/2307.07487.md)].
- Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech   Prompts - [[ArXiv](https://arxiv.org/abs/2307.07218)] [[QA](./papers/2307.07218.md)].
- Learning to Retrieve In-Context Examples for Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.07164)] [[QA](./papers/2307.07164.md)].
- Bootstrapping Vision-Language Learning with Decoupled Language   Pre-training - [[ArXiv](https://arxiv.org/abs/2307.07063)] [[QA](./papers/2307.07063.md)].
- DIALGEN: Collaborative Human-LM Generated Dialogues for Improved   Understanding of Human-Human Conversations - [[ArXiv](https://arxiv.org/abs/2307.07047)] [[QA](./papers/2307.07047.md)].
- HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image   Models - [[ArXiv](https://arxiv.org/abs/2307.06949)] [[QA](./papers/2307.06949.md)].
- In-context Autoencoder for Context Compression in a Large Language Model - [[ArXiv](https://arxiv.org/abs/2307.06945)] [[QA](./papers/2307.06945.md)].
- InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding   and Generation - [[ArXiv](https://arxiv.org/abs/2307.06942)] [[QA](./papers/2307.06942.md)].
- Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation - [[ArXiv](https://arxiv.org/abs/2307.06940)] [[QA](./papers/2307.06940.md)].
- mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs - [[ArXiv](https://arxiv.org/abs/2307.06930)] [[QA](./papers/2307.06930.md)].
- Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image   Models - [[ArXiv](https://arxiv.org/abs/2307.06925)] [[QA](./papers/2307.06925.md)].
- Generating Benchmarks for Factuality Evaluation of Language Models - [[ArXiv](https://arxiv.org/abs/2307.06908)] [[QA](./papers/2307.06908.md)].
- Copy Is All You Need - [[ArXiv](https://arxiv.org/abs/2307.06962)] [[QA](./papers/2307.06962.md)].
- Distilling Large Language Models for Biomedical Knowledge Extraction: A   Case Study on Adverse Drug Events - [[ArXiv](https://arxiv.org/abs/2307.06439)] [[QA](./papers/2307.06439.md)].
- T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional   Text-to-image Generation - [[ArXiv](https://arxiv.org/abs/2307.06350)] [[QA](./papers/2307.06350.md)].
- Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and   Resolution - [[ArXiv](https://arxiv.org/abs/2307.06304)] [[QA](./papers/2307.06304.md)].
- Instruction Mining: High-Quality Instruction Data Selection for Large   Language Models - [[ArXiv](https://arxiv.org/abs/2307.06290)] [[QA](./papers/2307.06290.md)].
- SayPlan: Grounding Large Language Models using 3D Scene Graphs for   Scalable Task Planning - [[ArXiv](https://arxiv.org/abs/2307.06135)] [[QA](./papers/2307.06135.md)].
- VELMA: Verbalization Embodiment of LLM Agents for Vision and Language   Navigation in Street View - [[ArXiv](https://arxiv.org/abs/2307.06082)] [[QA](./papers/2307.06082.md)].
- PolyLM: An Open Source Polyglot Large Language Model - [[ArXiv](https://arxiv.org/abs/2307.06018)] [[QA](./papers/2307.06018.md)].
- VoxPoser: Composable 3D Value Maps for Robotic Manipulation with   Language Models - [[ArXiv](https://arxiv.org/abs/2307.05973)] [[QA](./papers/2307.05973.md)].
- Giving Robots a Hand: Learning Generalizable Manipulation with   Eye-in-Hand Human Video Demonstrations - [[ArXiv](https://arxiv.org/abs/2307.05959)] [[QA](./papers/2307.05959.md)].
- Towards Robust and Efficient Continual Language Learning - [[ArXiv](https://arxiv.org/abs/2307.05741)] [[QA](./papers/2307.05741.md)].
- Stack More Layers Differently: High-Rank Training Through Low-Rank   Updates - [[ArXiv](https://arxiv.org/abs/2307.05695)] [[QA](./papers/2307.05695.md)].
- Differentiable Blocks World: Qualitative 3D Decomposition by Rendering   Primitives - [[ArXiv](https://arxiv.org/abs/2307.05473)] [[QA](./papers/2307.05473.md)].
- Self-consistency for open-ended generations - [[ArXiv](https://arxiv.org/abs/2307.06857)] [[QA](./papers/2307.06857.md)].
- EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the   Backbone - [[ArXiv](https://arxiv.org/abs/2307.05463)] [[QA](./papers/2307.05463.md)].
- Efficient 3D Articulated Human Generation with Layered Surface Volumes - [[ArXiv](https://arxiv.org/abs/2307.05462)] [[QA](./papers/2307.05462.md)].
- Empowering Cross-lingual Behavioral Testing of NLP Models with   Typological Features - [[ArXiv](https://arxiv.org/abs/2307.05454)] [[QA](./papers/2307.05454.md)].
- Self-Supervised Learning with Lie Symmetries for Partial Differential   Equations - [[ArXiv](https://arxiv.org/abs/2307.05432)] [[QA](./papers/2307.05432.md)].
- Unleashing Cognitive Synergy in Large Language Models: A Task-Solving   Agent through Multi-Persona Self-Collaboration - [[ArXiv](https://arxiv.org/abs/2307.05300)] [[QA](./papers/2307.05300.md)].
- Generative Pretraining in Multimodality - [[ArXiv](https://arxiv.org/abs/2307.05222)] [[QA](./papers/2307.05222.md)].
- DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence   Analysis Tasks - [[ArXiv](https://arxiv.org/abs/2307.05628)] [[QA](./papers/2307.05628.md)].
- Test-Time Training on Video Streams - [[ArXiv](https://arxiv.org/abs/2307.05014)] [[QA](./papers/2307.05014.md)].
- Secrets of RLHF in Large Language Models Part I: PPO - [[ArXiv](https://arxiv.org/abs/2307.04964)] [[QA](./papers/2307.04964.md)].
- Semantic-SAM: Segment and Recognize Anything at Any Granularity - [[ArXiv](https://arxiv.org/abs/2307.04767)] [[QA](./papers/2307.04767.md)].
- SITTA: A Semantic Image-Text Alignment for Image Captioning - [[ArXiv](https://arxiv.org/abs/2307.05591)] [[QA](./papers/2307.05591.md)].
- Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal   Rearrangement - [[ArXiv](https://arxiv.org/abs/2307.04751)] [[QA](./papers/2307.04751.md)].
- AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models   without Specific Tuning - [[ArXiv](https://arxiv.org/abs/2307.04725)] [[QA](./papers/2307.04725.md)].
- Large Language Models as General Pattern Machines - [[ArXiv](https://arxiv.org/abs/2307.04721)] [[QA](./papers/2307.04721.md)].
- International Institutions for Advanced AI - [[ArXiv](https://arxiv.org/abs/2307.04699)] [[QA](./papers/2307.04699.md)].
- VampNet: Music Generation via Masked Acoustic Token Modeling - [[ArXiv](https://arxiv.org/abs/2307.04686)] [[QA](./papers/2307.04686.md)].
- AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation   System - [[ArXiv](https://arxiv.org/abs/2307.04577)] [[QA](./papers/2307.04577.md)].
- RLTF: Reinforcement Learning from Unit Test Feedback - [[ArXiv](https://arxiv.org/abs/2307.04349)] [[QA](./papers/2307.04349.md)].
- SVIT: Scaling up Visual Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2307.04087)] [[QA](./papers/2307.04087.md)].
- Toward Interactive Dictation - [[ArXiv](https://arxiv.org/abs/2307.04008)] [[QA](./papers/2307.04008.md)].
- On decoder-only architecture for speech-to-text and large language model   integration - [[ArXiv](https://arxiv.org/abs/2307.03917)] [[QA](./papers/2307.03917.md)].
- Large Language Models for Supply Chain Optimization - [[ArXiv](https://arxiv.org/abs/2307.03875)] [[QA](./papers/2307.03875.md)].
- Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation - [[ArXiv](https://arxiv.org/abs/2307.03869)] [[QA](./papers/2307.03869.md)].
- AutoDecoding Latent 3D Diffusion Models - [[ArXiv](https://arxiv.org/abs/2307.05445)] [[QA](./papers/2307.05445.md)].
- GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest - [[ArXiv](https://arxiv.org/abs/2307.03601)] [[QA](./papers/2307.03601.md)].
- Solvent: A Framework for Protein Folding - [[ArXiv](https://arxiv.org/abs/2307.04603)] [[QA](./papers/2307.04603.md)].
- Building Cooperative Embodied Agents Modularly with Large Language   Models - [[ArXiv](https://arxiv.org/abs/2307.02485)] [[QA](./papers/2307.02485.md)].
- What Matters in Training a GPT4-Style Language Model with Multimodal   Inputs? - [[ArXiv](https://arxiv.org/abs/2307.02469)] [[QA](./papers/2307.02469.md)].
- Robots That Ask For Help: Uncertainty Alignment for Large Language Model   Planners - [[ArXiv](https://arxiv.org/abs/2307.01928)] [[QA](./papers/2307.01928.md)].
- Embodied Task Planning with Large Language Models - [[ArXiv](https://arxiv.org/abs/2307.01848)] [[QA](./papers/2307.01848.md)].
- Collaborative Score Distillation for Consistent Visual Synthesis - [[ArXiv](https://arxiv.org/abs/2307.04787)] [[QA](./papers/2307.04787.md)].
- SCITUNE: Aligning Large Language Models with Scientific Multimodal   Instructions - [[ArXiv](https://arxiv.org/abs/2307.01139)] [[QA](./papers/2307.01139.md)].
- Visual Instruction Tuning with Polite Flamingo - [[ArXiv](https://arxiv.org/abs/2307.01003)] [[QA](./papers/2307.01003.md)].
- Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset - [[ArXiv](https://arxiv.org/abs/2307.00818)] [[QA](./papers/2307.00818.md)].
- JourneyDB: A Benchmark for Generative Image Understanding - [[ArXiv](https://arxiv.org/abs/2307.00716)] [[QA](./papers/2307.00716.md)].
- DoReMi: Grounding Language Model by Detecting and Recovering from   Plan-Execution Misalignment - [[ArXiv](https://arxiv.org/abs/2307.00329)] [[QA](./papers/2307.00329.md)].

### June 2023
- SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen   LLMs - [[ArXiv](https://arxiv.org/abs/2306.17842)] [[QA](./papers/2306.17842.md)].
- Statler: State-Maintaining Language Models for Embodied Reasoning - [[ArXiv](https://arxiv.org/abs/2306.17840)] [[QA](./papers/2306.17840.md)].
- LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image   Understanding - [[ArXiv](https://arxiv.org/abs/2306.17107)] [[QA](./papers/2306.17107.md)].
- KITE: Keypoint-Conditioned Policies for Semantic Manipulation - [[ArXiv](https://arxiv.org/abs/2306.16605)] [[QA](./papers/2306.16605.md)].
- Towards Language Models That Can See: Computer Vision Through the LENS   of Natural Language - [[ArXiv](https://arxiv.org/abs/2306.16410)] [[QA](./papers/2306.16410.md)].
- Kosmos-2: Grounding Multimodal Large Language Models to the World - [[ArXiv](https://arxiv.org/abs/2306.14824)] [[QA](./papers/2306.14824.md)].
- Aligning Large Multi-Modal Model with Robust Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2306.14565)] [[QA](./papers/2306.14565.md)].
- DesCo: Learning Object Recognition with Rich Language Descriptions - [[ArXiv](https://arxiv.org/abs/2306.14060)] [[QA](./papers/2306.14060.md)].
- MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language   Models - [[ArXiv](https://arxiv.org/abs/2306.13394)] [[QA](./papers/2306.13394.md)].
- SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by   Generative Pre-trained Heterogeneous Graph Transformer - [[ArXiv](https://arxiv.org/abs/2306.12677)] [[QA](./papers/2306.12677.md)].
- Mass-Producing Failures of Multimodal Systems with Language Models - [[ArXiv](https://arxiv.org/abs/2306.12105)] [[QA](./papers/2306.12105.md)].
- SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling - [[ArXiv](https://arxiv.org/abs/2306.11886)] [[QA](./papers/2306.11886.md)].
- Improving Image Captioning Descriptiveness by Ranking and LLM-based   Fusion - [[ArXiv](https://arxiv.org/abs/2306.11593)] [[QA](./papers/2306.11593.md)].
- RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with   Progressive Reasoning Tasks - [[ArXiv](https://arxiv.org/abs/2306.11335)] [[QA](./papers/2306.11335.md)].
- MotionGPT: Finetuned LLMs are General-Purpose Motion Generators - [[ArXiv](https://arxiv.org/abs/2306.10900)] [[QA](./papers/2306.10900.md)].
- CLARA: Classifying and Disambiguating User Commands for Reliable   Interactive Robotic Agents - [[ArXiv](https://arxiv.org/abs/2306.10376)] [[QA](./papers/2306.10376.md)].
- Investigating Prompting Techniques for Zero- and Few-Shot Visual   Question Answering - [[ArXiv](https://arxiv.org/abs/2306.09996)] [[QA](./papers/2306.09996.md)].
- LVLM-eHub: A Comprehensive Evaluation Benchmark for Large   Vision-Language Models - [[ArXiv](https://arxiv.org/abs/2306.09265)] [[QA](./papers/2306.09265.md)].
- Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and   Text Integration - [[ArXiv](https://arxiv.org/abs/2306.09093)] [[QA](./papers/2306.09093.md)].
- Toward Grounded Social Reasoning - [[ArXiv](https://arxiv.org/abs/2306.08651)] [[QA](./papers/2306.08651.md)].
- Language to Rewards for Robotic Skill Synthesis - [[ArXiv](https://arxiv.org/abs/2306.08647)] [[QA](./papers/2306.08647.md)].
- Towards AGI in Computer Vision: Lessons Learned from GPT and Large   Language Models - [[ArXiv](https://arxiv.org/abs/2306.08641)] [[QA](./papers/2306.08641.md)].
- AssistGPT: A General Multi-modal Assistant that can Plan, Execute,   Inspect, and Learn - [[ArXiv](https://arxiv.org/abs/2306.08640)] [[QA](./papers/2306.08640.md)].
- MIMIC-IT: Multi-Modal In-Context Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2306.05425)] [[QA](./papers/2306.05425.md)].

### May 2023
- Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL   Models - [[ArXiv](https://arxiv.org/abs/2305.19595)] [[QA](./papers/2305.19595.md)].
- EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought - [[ArXiv](https://arxiv.org/abs/2305.15021)] [[QA](./papers/2305.15021.md)].
- Training Diffusion Models with Reinforcement Learning - [[ArXiv](https://arxiv.org/abs/2305.13301)] [[QA](./papers/2305.13301.md)].
- Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions   with Large Language Model - [[ArXiv](https://arxiv.org/abs/2305.11176)] [[QA](./papers/2305.11176.md)].
- VisionLLM: Large Language Model is also an Open-Ended Decoder for   Vision-Centric Tasks - [[ArXiv](https://arxiv.org/abs/2305.11175)] [[QA](./papers/2305.11175.md)].
- LLMScore: Unveiling the Power of Large Language Models in Text-to-Image   Synthesis Evaluation - [[ArXiv](https://arxiv.org/abs/2305.11116)] [[QA](./papers/2305.11116.md)].
- An Android Robot Head as Embodied Conversational Agent - [[ArXiv](https://arxiv.org/abs/2305.10945)] [[QA](./papers/2305.10945.md)].
- OpenShape: Scaling Up 3D Shape Representation Towards Open-World   Understanding - [[ArXiv](https://arxiv.org/abs/2305.10764)] [[QA](./papers/2305.10764.md)].
- Evaluating Object Hallucination in Large Vision-Language Models - [[ArXiv](https://arxiv.org/abs/2305.10355)] [[QA](./papers/2305.10355.md)].
- Towards Generalist Robots: A Promising Paradigm via Generative   Simulation - [[ArXiv](https://arxiv.org/abs/2305.10455)] [[QA](./papers/2305.10455.md)].
- ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced   MiniGPT-4 - [[ArXiv](https://arxiv.org/abs/2305.07490)] [[QA](./papers/2305.07490.md)].
- TidyBot: Personalized Robot Assistance with Large Language Models - [[ArXiv](https://arxiv.org/abs/2305.05658)] [[QA](./papers/2305.05658.md)].
- Principle-Driven Self-Alignment of Language Models from Scratch with   Minimal Human Supervision - [[ArXiv](https://arxiv.org/abs/2305.03047)] [[QA](./papers/2305.03047.md)].
- Multimodal Procedural Planning via Dual Text-Image Prompting - [[ArXiv](https://arxiv.org/abs/2305.01795)] [[QA](./papers/2305.01795.md)].
- ArK: Augmented Reality with Knowledge Interactive Emergent Ability - [[ArXiv](https://arxiv.org/abs/2305.00970)] [[QA](./papers/2305.00970.md)].

### April 2023
- LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model - [[ArXiv](https://arxiv.org/abs/2304.15010)] [[QA](./papers/2304.15010.md)].
- ChatVideo: A Tracklet-centric Multimodal and Versatile Video   Understanding System - [[ArXiv](https://arxiv.org/abs/2304.14407)] [[QA](./papers/2304.14407.md)].
- mPLUG-Owl: Modularization Empowers Large Language Models with   Multimodality - [[ArXiv](https://arxiv.org/abs/2304.14178)] [[QA](./papers/2304.14178.md)].
- Multimodal Grounding for Embodied AI via Augmented Reality Headsets for   Natural Language Driven Task Planning - [[ArXiv](https://arxiv.org/abs/2304.13676)] [[QA](./papers/2304.13676.md)].
- ChatABL: Abductive Learning via Natural Language Interaction with   ChatGPT - [[ArXiv](https://arxiv.org/abs/2304.11107)] [[QA](./papers/2304.11107.md)].
- Can GPT-4 Perform Neural Architecture Search? - [[ArXiv](https://arxiv.org/abs/2304.10970)] [[QA](./papers/2304.10970.md)].
- MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large   Language Models - [[ArXiv](https://arxiv.org/abs/2304.10592)] [[QA](./papers/2304.10592.md)].
- SINC: Spatial Composition of 3D Human Motions for Simultaneous Action   Generation - [[ArXiv](https://arxiv.org/abs/2304.10417)] [[QA](./papers/2304.10417.md)].
- Chameleon: Plug-and-Play Compositional Reasoning with Large Language   Models - [[ArXiv](https://arxiv.org/abs/2304.09842)] [[QA](./papers/2304.09842.md)].
- Visual Instruction Tuning - [[ArXiv](https://arxiv.org/abs/2304.08485)] [[QA](./papers/2304.08485.md)].
- Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with   Text - [[ArXiv](https://arxiv.org/abs/2304.06939)] [[QA](./papers/2304.06939.md)].
- RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment - [[ArXiv](https://arxiv.org/abs/2304.06767)] [[QA](./papers/2304.06767.md)].
- Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via   Prompt Augmented by ChatGPT - [[ArXiv](https://arxiv.org/abs/2304.11116)] [[QA](./papers/2304.11116.md)].
- OpenAGI: When LLM Meets Domain Experts - [[ArXiv](https://arxiv.org/abs/2304.04370)] [[QA](./papers/2304.04370.md)].
- Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions - [[ArXiv](https://arxiv.org/abs/2304.04227)] [[QA](./papers/2304.04227.md)].
- ChatGPT Empowered Long-Step Robot Control in Various Environments: A   Case Application - [[ArXiv](https://arxiv.org/abs/2304.03893)] [[QA](./papers/2304.03893.md)].
- ERRA: An Embodied Representation and Reasoning Architecture for   Long-horizon Language-conditioned Manipulation Tasks - [[ArXiv](https://arxiv.org/abs/2304.02251)] [[QA](./papers/2304.02251.md)].

### March 2023
- HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging   Face - [[ArXiv](https://arxiv.org/abs/2303.17580)] [[QA](./papers/2303.17580.md)].
- ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with   GPT and Prototype Guidance - [[ArXiv](https://arxiv.org/abs/2303.16894)] [[QA](./papers/2303.16894.md)].
- LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init   Attention - [[ArXiv](https://arxiv.org/abs/2303.16199)] [[QA](./papers/2303.16199.md)].
- Learning video embedding space with Natural Language Supervision - [[ArXiv](https://arxiv.org/abs/2303.14584)] [[QA](./papers/2303.14584.md)].
- Errors are Useful Prompts: Instruction Guided Task Programming with   Verifier-Assisted Iterative Prompting - [[ArXiv](https://arxiv.org/abs/2303.14100)] [[QA](./papers/2303.14100.md)].
- eP-ALM: Efficient Perceptual Augmentation of Language Models - [[ArXiv](https://arxiv.org/abs/2303.11403)] [[QA](./papers/2303.11403.md)].
- MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action - [[ArXiv](https://arxiv.org/abs/2303.11381)] [[QA](./papers/2303.11381.md)].
- DialogPaint: A Dialog-based Image Editing Model - [[ArXiv](https://arxiv.org/abs/2303.10073)] [[QA](./papers/2303.10073.md)].
- MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action   Recognition with Language Knowledge - [[ArXiv](https://arxiv.org/abs/2303.08914)] [[QA](./papers/2303.08914.md)].
- Can Large Language Models design a Robot? - [[ArXiv](https://arxiv.org/abs/2303.15324)] [[QA](./papers/2303.15324.md)].
- Chat with the Environment: Interactive Multimodal Perception Using Large   Language Models - [[ArXiv](https://arxiv.org/abs/2303.08268)] [[QA](./papers/2303.08268.md)].
- ViperGPT: Visual Inference via Python Execution for Reasoning - [[ArXiv](https://arxiv.org/abs/2303.08128)] [[QA](./papers/2303.08128.md)].
- RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via   Language-Based Feedback - [[ArXiv](https://arxiv.org/abs/2303.07622)] [[QA](./papers/2303.07622.md)].
- Audio Visual Language Maps for Robot Navigation - [[ArXiv](https://arxiv.org/abs/2303.07522)] [[QA](./papers/2303.07522.md)].
- ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched   Visual Descriptions - [[ArXiv](https://arxiv.org/abs/2303.06594)] [[QA](./papers/2303.06594.md)].
- Task and Motion Planning with Large Language Models for Object   Rearrangement - [[ArXiv](https://arxiv.org/abs/2303.06247)] [[QA](./papers/2303.06247.md)].
- Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation   Models - [[ArXiv](https://arxiv.org/abs/2303.04671)] [[QA](./papers/2303.04671.md)].
- Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Based Zero-Shot   Object Navigation - [[ArXiv](https://arxiv.org/abs/2303.03480)] [[QA](./papers/2303.03480.md)].
- Prompting Large Language Models with Answer Heuristics for   Knowledge-based Visual Question Answering - [[ArXiv](https://arxiv.org/abs/2303.01903)] [[QA](./papers/2303.01903.md)].
- Open-World Object Manipulation using Pre-trained Vision-Language Models - [[ArXiv](https://arxiv.org/abs/2303.00905)] [[QA](./papers/2303.00905.md)].
- Grounded Decoding: Guiding Text Generation with Grounded Models for   Robot Control - [[ArXiv](https://arxiv.org/abs/2303.00855)] [[QA](./papers/2303.00855.md)].

### February 2023
- Internet Explorer: Targeted Representation Learning on the Open Web - [[ArXiv](https://arxiv.org/abs/2302.14051)] [[QA](./papers/2302.14051.md)].
- Language Is Not All You Need: Aligning Perception with Language Models - [[ArXiv](https://arxiv.org/abs/2302.14045)] [[QA](./papers/2302.14045.md)].
- ChatGPT for Robotics: Design Principles and Model Abilities - [[ArXiv](https://arxiv.org/abs/2306.17582)] [[QA](./papers/2306.17582.md)].

## 2022

### November 2022
- Visual Programming: Compositional visual reasoning without training - [[ArXiv](https://arxiv.org/abs/2211.11559)] [[QA](./papers/2211.11559.md)].

### October 2022
- VIMA: General Robot Manipulation with Multimodal Prompts - [[ArXiv](https://arxiv.org/abs/2210.03094)] [[QA](./papers/2210.03094.md)].
- Multimodal Analogical Reasoning over Knowledge Graphs - [[ArXiv](https://arxiv.org/abs/2210.00312)] [[QA](./papers/2210.00312.md)].

### July 2022
- LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,   Vision, and Action - [[ArXiv](https://arxiv.org/abs/2207.04429)] [[QA](./papers/2207.04429.md)].

### June 2022
- Language Models are General-Purpose Interfaces - [[ArXiv](https://arxiv.org/abs/2206.06336)] [[QA](./papers/2206.06336.md)].
