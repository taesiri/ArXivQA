# [Exploring the Universal Vulnerability of Prompt-based Learning Paradigm](https://arxiv.org/abs/2204.05239)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions/hypotheses addressed in this paper are:1. Whether the prompt-based learning paradigm inherits vulnerabilities from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text.2. Whether attackers can exploit these vulnerabilities by either injecting backdoor triggers or searching for adversarial triggers using only plain text, in order to control or severely degrade the performance of prompt-based models fine-tuned on arbitrary downstream tasks. 3. Whether adversarial triggers have good transferability among language models. 4. Whether conventional fine-tuning models are also vulnerable to adversarial triggers constructed from pre-trained language models.5. Whether a potential solution (outlier word filtering) can help mitigate these attack methods.In summary, the central focus is on demonstrating and analyzing the "universal vulnerability" of the prompt-based learning paradigm to triggers that can mislead model predictions, and exploring different attack scenarios as well as defenses. The key hypothesis is that prompt-based models inherit vulnerabilities from pre-training, allowing attackers to easily construct triggers that undermine performance across tasks.


## What is the main contribution of this paper?

The main contributions of this paper are:- It demonstrates the universal vulnerabilities of the prompt-based learning paradigm in two different attack scenarios - backdoor attack and adversarial attack. - It proposes two attack methods against prompt-based models:    - Backdoor Triggers on Prompt-based Learning (BToP), which injects backdoor triggers during pre-training to control downstream prompt-based models.    - Adversarial Triggers on Prompt-based Learning (AToP), which searches for adversarial triggers on pre-trained LMs to attack prompt-based models.- It evaluates the proposed attacks on 6 datasets and shows they can achieve high attack success rates, especially BToP which gets 99.5% on average.- It analyzes the influence of different factors like prompt templates, number of shots, and transferability of triggers. - It reveals prompt-based learning inherits vulnerabilities from pre-training, which is the first work studying security issues in this emerging paradigm.- It proposes a potential defense method based on outlier word filtering and shows it can mitigate AToP but not BToP.In summary, this paper comprehensively studies the vulnerability of prompt-based learning, proposes effective attack methods, and calls for attention on security issues before prompt-based models are widely deployed. The exploration of attacks and defenses will help build more robust prompt-based models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper explores the universal vulnerability of prompt-based learning models to both backdoor and adversarial attacks using triggers injected into the input text, and shows these attacks are effective at controlling model outputs across different downstream tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of adversarial attacks on natural language processing models:- The key contribution is showing the vulnerability of prompt-based learning methods to both backdoor and adversarial attacks. Prompt-based learning is a relatively new paradigm in NLP, so demonstrating potential security issues is an important finding.- The backdoor attack is similar to prior work like BadNL that poisons the training data to implant backdoors. The difference is adapting the attack specifically for prompt-based models by targeting the <mask> token embedding. - The adversarial attack builds on prior work on universal adversarial triggers, but tailors the attack to prompt-based models rather than standard fine-tuned models. The triggers found can attack multiple downstream prompt models.- Most prior adversarial attack methods in NLP need many queries to construct examples. A strength here is finding triggers using only plain text corpora, no model queries.- Analyzing attack transferability between language models and comparing to fine-tuned models provides useful insights. For instance, fine-tuned models are more robust to these adversarial triggers.- Proposing a potential mitigation method and testing it against the attacks is a nice addition rather than just demonstrating vulnerabilities.Overall, I think the authors make good contributions in analyzing potential vulnerabilities in an emerging NLP technique. The attacks are tailored to prompt-based learning and highlight security issues to watch out for. Testing transferability and defenses also provides value. It's an interesting study on an important and timely topic.


## What future research directions do the authors suggest?

Here are some of the main future research directions suggested by the authors:- Developing methods to mitigate the universal vulnerability of prompt-based learning. The authors propose and test a simple outlier word filtering method, but suggest more advanced defenses could be developed.- Further analyzing the reasons behind the vulnerability of prompt-based learning to understand it better. For example, analyzing the effect of different prompting functions.- Exploring whether conventional fine-tuning models have similar vulnerabilities, or understanding why they seem more robust. The authors found prompt-based models are vulnerable but fine-tuned models are not.- Studying if continuous prompting methods have similar vulnerabilities. This work focuses on manual prompting.- Evaluating vulnerability of prompt-based models in more complex domains like dialog and QA. This work uses text classification tasks.- Considering multi-modal prompts that include images, as they may be more robust. This work only uses text prompts.- Exploring the effect of different pre-training objectives and datasets on the vulnerability. The pre-trained models used were trained in standard ways.In general, the authors call for more research on the security and robustness issues of prompt-based learning before it is widely deployed, considering its demonstrated vulnerability to both backdoor and adversarial attacks.
