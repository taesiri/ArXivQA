# [Don't Rank, Combine! Combining Machine Translation Hypotheses Using   Quality Estimation](https://arxiv.org/abs/2401.06688)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural machine translation (NMT) models estimate probabilities of target sentences given source sentences, but these estimates may not align with human preferences. Their outputs can exhibit complementary errors with no single best candidate.

Proposed Solution: 
- The paper introduces QE-fusion, a method that utilizes a quality estimation (QE) metric to synthesize an improved translation from a pool of candidate translations sampled from a model. 
- It identifies divergent spans across candidates and traverses them, selecting the span at each step that contributes to a higher QE score. The chosen spans are merged to form a refined translation.

Main Contributions:
1. Design of the QE-fusion algorithm that fuses complementary information from multiple translation candidates using a QE metric as guidance.
2. Demonstration that QE-fusion outperforms standard decoding algorithms, Minimum Bayes Risk, and QE-reranking across various language pairs and models, including large language models and multilingual NMT models.
3. Analysis showing QE-fusion benefits more from the diversity of candidates generated by LLMs compared to NMT models.
4. Empirical analysis establishing that QE-fusion works well even with small candidate pools and exhibits linear time complexity.
5. Proof that QE-fusion can effectively enhance LLM translation quality without extra training, since it works as a post-processing technique.

In summary, the paper introduces a novel method called QE-fusion to synthesize improved machine translations in a quality-guided manner from complementary candidates produced by neural models. Experiments demonstrate consistent gains across models and language pairs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces QE-fusion, a novel method that leverages quality estimation metrics to synthesize improved machine translations by combining fragments from multiple candidate translations sampled from a model.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Designing a novel algorithm called QE-fusion that generates improved translations from a pool of candidates using a quality estimation (QE) metric. 

2. Demonstrating that QE-fusion outperforms recently proposed QE-reranking and minimum Bayes risk (MBR) decoding methods on various language pairs, using several open-source large language models (LLMs) and multilingual neural machine translation (NMT) models.

3. Explaining the larger improvements of QE-fusion for LLMs compared to NMT by showing that LLMs generate more diverse translation candidates.

4. Showing that QE-fusion does not need a large pool of candidates and thus does not entail significant computational cost.

In summary, the main contribution is the QE-fusion algorithm that can enhance translation quality from LLMs and NMT models by combining complementary information from multiple translation candidates guided by a QE metric.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper are:

- Quality estimation (QE) metrics
- QE-fusion - the proposed method to combine machine translation hypotheses using QE metrics
- COMET and BLEURT - neural reference-based MT evaluation metrics
- CometKiwi - reference-free QE metric 
- Minimum Bayes Risk (MBR) decoding
- QE-reranking - reranking MT outputs using QE metrics
- Large language models (LLMs)
- Nucleus sampling - method to generate diverse translations from LLMs
- Epsilon sampling - method to generate diverse translations from NMT models
- Translation candidate pool - set of translation hypotheses generated from a model
- Divergent spans - differing text spans among translation candidates
- Beam search - standard decoding method for MT models

The main focus of the paper is on using QE metrics to combine complementary information from multiple MT hypotheses to produce improved translations, compared to standard decoding, sampling and reranking methods. The evaluations are done over various LLMs and NMT models on multiple language pairs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the QE-fusion method proposed in the paper:

1. How does QE-fusion leverage the complementary nature of translation candidates to synthesize improved outputs? Explain the key steps of identifying divergent spans and selectively combining them guided by a QE metric.

2. What are the key benefits of QE-fusion over standard decoding algorithms like beam search and greedy decoding? Discuss how it addresses their limitations.

3. How does QE-fusion compare to other reranking methods like Minimum Bayes Risk (MBR) decoding and Quality Estimation (QE) reranking? What are the differences in performance and computational complexity?

4. What role does the diversity of the candidate pool play in the effectiveness of QE-fusion? Analyze the impact of sampling temperature and model type (LLM vs NMT) on diversity and final translation quality.  

5. How does QE-fusion scale with increasing pool sizes in terms of both translation performance and computational complexity? Does it require a large pool to be effective?

6. In what percentage of cases does QE-fusion generate novel translations not found in larger candidate pools? What does this suggest about its ability to synthesize new candidates?

7. Why does QE-fusion achieve larger improvements when applied to LLMs compared to NMT models? Discuss the difference in diversity between LLM and NMT outputs.  

8. How does QE-fusion, which operates only on model outputs, compare against training separate post-editing models? Analyze the tradeoffs.

9. What optimizations are made to improve the runtime efficiency of QE-fusion? How do the empirical runtime results compare against reranking methods?

10. What directions can QE-fusion be extended to, beyond machine translation? Discuss its potential applicability to other text generation tasks where evaluation metrics are available.
