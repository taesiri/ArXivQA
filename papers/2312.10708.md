# [The Conditioning Bias in Binary Decision Trees and Random Forests and   Its Elimination](https://arxiv.org/abs/2312.10708)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Binary decision trees are commonly used machine learning models. Popular implementations like scikit-learn use the midpoint between feature values as the threshold (t) and conditioning like x<=t or x<t when splitting nodes. 
- With continuous features, the choice of conditioning operator (< vs <=) has a negligible effect. However, with features taking values from a small discrete set, the thresholds can align with feature values, making the predictions dependent on the arbitrary choice of conditioning operator.
- This phenomenon is referred to as the "conditioning bias". It can emerge when features take values from a lattice with high probability, especially in deeper trees. Examples of such lattice features are age, rounded decimals, monetary values, etc.

Proposed Solution: 
- Techniques are proposed to eliminate the conditioning bias by integrating out the effect of the conditioning operator. 
- For single trees, predictions are made with both operators and averaged. For random forests, half the trees use one operator and half use the other.
- The proposed solution incurs no additional computation cost for training or inference with random forests.

Main Contributions:
- Establishes the existence of the conditioning bias through extensive experiments, showing variations of 0.1-0.2 percentage points in AUC and r^2 scores.
- Proposes practical techniques to mitigate the bias with different scenarios based on accessibility of decision trees.
- Demonstrates the proposed methods lead to statistically significant improvements over the worst performing conditioning, with gains of up to 1.5 percentage points in r^2 score.
- An important finding is that with random forests, the proposed solution eliminates the bias at no additional computational cost.

In summary, the paper investigates an implicit modeling bias caused by arbitrary choices in binary decision tree implementations, proposes practical solutions to mitigate it, and demonstrates their effectiveness through rigorous experiments. A key contribution is a "free lunch" improvement for random forest users when dealing with certain types of features.
