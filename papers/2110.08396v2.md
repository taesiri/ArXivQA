# [Comparing Human and Machine Bias in Face Recognition](https://arxiv.org/abs/2110.08396v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions seem to be:

1) How and to what extent do humans exhibit bias in their accuracy on facial recognition tasks like identification and verification? (RQ1) 

2) How does this human bias compare to the bias exhibited by machine learning-based facial recognition models? (RQ2)

The key hypotheses appear to be:

- Humans will exhibit biases in facial recognition accuracy based on gender and skin tone of the subjects.

- Machine learning models will also exhibit biases, but the magnitude of these biases will be comparable to human biases. 

- Overall, machine learning models will be more accurate than humans on facial recognition tasks.

So in summary, the main research questions focus on quantifying and comparing biases in facial recognition between humans and machines. The key hypothesis is that while machines are more accurate overall, they exhibit similar levels of bias to humans when broken down by demographic factors. The paper aims to demonstrate and quantify these biases through facial recognition experiments on humans and machine learning models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Creating a new facial recognition dataset (\dataname{}) by extensively hand-curating and checking the labels and metadata of images from existing datasets (LFW and CelebA). This resulted in a cleaner dataset with more robust labels compared to the original datasets.

- Designing a survey with challenging facial verification and identification questions using the new dataset. The questions are balanced across gender, age, and skin type.

- Administering the survey to a demographically diverse sample of human participants (n=\nsurvey) as well as evaluating academic and commercial facial recognition models on the same questions. 

- Comparing the performance and biases of humans versus machines on these facial recognition tasks. Key findings include:

 - Both humans and academic models exhibit biases, performing better on male and lighter-skinned subjects. The magnitude of these biases is statistically comparable between humans and academic models.

 - Humans perform better on subjects that match their own demographics.

 - Academic models and humans both perform better on verification than identification.

 - Commercial APIs are extremely accurate overall and did not exhibit detectable biases.

 - Computer models substantially outperform humans in terms of accuracy on both tasks.

So in summary, the main contribution is rigorously evaluating and comparing the accuracy and biases of humans versus machines on facial recognition using a novel high-quality dataset and survey methodology. This provides new insights into how algorithmic biases compare to human biases.
