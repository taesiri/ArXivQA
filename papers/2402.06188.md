# [A self-supervised framework for learning whole slide representations](https://arxiv.org/abs/2402.06188)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Whole slide images (WSIs) present unique challenges for computer vision due to their massive gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent annotations. These issues make it difficult to learn high-quality whole slide representations using only supervised learning. Self-supervised learning provides an opportunity to achieve stronger whole slide representations without reliance on labels. However, existing self-supervised methods have shown limited success for whole slides.

Proposed Solution:
This paper introduces S3L, a flexible self-supervised learning framework optimized for learning whole slide representations. S3L treats gigapixel WSIs as a sequence of smaller image patches. It applies a combination of splitting, cropping, and masking transformations informed by properties of WSIs to generate high-quality augmented views of the same slide. These paired views are fed into a transformer-based whole slide encoder for self-supervised pre-training using contrastive losses.  

The key components of S3L are:
1) A pre-trained CNN patch encoder to extract features from image patches
2) Whole slide transformations like splitting, cropping and masking that leverage properties of WSIs to generate informative paired views 
3) A transformer architecture that encodes patch sequences into whole slide representations
4) Self-supervised pre-training objectives like SimCLR, BYOL and VICReg

Main Contributions:
- Introduces S3L, a new self-supervised framework optimized for whole slide representation learning 
- Designs domain-specific transformations combining vision and NLP strategies for creating informative paired views from gigapixel WSIs
- Demonstrates S3L can effectively learn whole slide representations, significantly outperforming pooling baselines
- Shows flexibility of S3L framework across patch encoders, tasks and microscopy modalities
- Provides first demonstration of self-supervised attention maps on full gigapixel slides

The main impact is a generalizable framework for self-supervised whole slide representation learning to better support computational pathology tasks.
