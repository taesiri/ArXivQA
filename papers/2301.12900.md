# [DepGraph: Towards Any Structural Pruning](https://arxiv.org/abs/2301.12900)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to enable general and automatic structural pruning for arbitrary neural network architectures, including CNNs, RNNs, GNNs, and Transformers. The key hypothesis is that explicitly modeling the dependency relationships between parameters in different layers can allow consistent and effective pruning across diverse network architectures in a generalizable manner.Specifically, the paper proposes a Dependency Graph (DepGraph) framework to automatically model the inter-layer dependencies in neural networks. The core idea is that pruning one layer will affect coupled layers due to parameter dependencies. By modeling these dependencies as a graph and finding connected components, structurally related parameters can be identified and removed simultaneously while avoiding breakage of the model structure. The hypothesis is that by using DepGraph to consistently identify and prune unimportant parameters in grouped layers, the approach can achieve effective acceleration and compression for arbitrary architectures in a general way, without requiring manual network-specific analysis.The experiments across CNNs, RNNs, GNNs and Vision Transformers on various datasets aim to validate that the proposed DepGraph framework enables general automatic structured pruning across network families, architectures, and data modalities.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a general and fully automatic method called Dependency Graph (DepGraph) for structural pruning of arbitrary neural network architectures. Specifically, the paper makes the following key contributions:- Proposes DepGraph to explicitly model inter-layer dependencies in neural networks. DepGraph allows automatic parameter grouping for consistent pruning across coupled layers. - DepGraph provides a general framework for structural pruning that can be readily applied to various architectures like CNNs, RNNs, Transformers, and GNNs.- Demonstrates the effectiveness of DepGraph for accelerating popular models on image classification (ResNets, DenseNets etc on CIFAR and ImageNet), text classification (LSTM), 3D point cloud classification (DGCNN), and graph classification (GAT).- Achieves competitive pruning results compared to state-of-the-art architecture-specific pruning methods, while being more generalizable. For example, obtains 2.57x speedup on ResNet-56 with higher accuracy than original model on CIFAR.In summary, the key contribution is proposing DepGraph as a generic and automatic scheme for structural pruning of arbitrary neural network architectures. This provides a significant step towards the goal of general and automated model compression.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method called Dependency Graph (DepGraph) to enable fully automatic structural pruning of arbitrary neural network architectures like CNNs, RNNs, GNNs, and Transformers by explicitly modeling parameter dependencies across layers to identify coupled parameters that must be pruned simultaneously.
