# Logical Reasoning over Natural Language as Knowledge Representation: A   Survey

## What is the central research question or hypothesis that this paper addresses?

The central research question of this paper is: How can natural language be used as knowledge representation for logical reasoning, overcoming challenges with traditional formal representations? The key points are:- Traditionally, logical reasoning in AI uses formal languages like first-order logic as knowledge representation. However, this has faced challenges like brittleness, knowledge acquisition bottleneck, inability to handle raw natural language input, sensitivity to errors, etc. - Recently, there is a new paradigm of using natural language as the knowledge representation for logical reasoning instead. Pretrained language models (PLMs) are used as the reasoning engine. - This paper provides a comprehensive survey of this new paradigm of "logical reasoning over natural language" (LRNL). It reviews the literature on using LRNL for the three main types of logical reasoning - deductive, inductive and abductive reasoning.- LRNL has advantages over formal representations like being less brittle, alleviating the knowledge acquisition bottleneck, handling raw text input, and being robust to errors. It also has benefits over pure neural methods like interpretability, controllability, and less catastrophic forgetting.- The paper summarizes task formulations, datasets, methods, challenges, and future directions for LRNL across deductive, inductive and abductive reasoning. It aims to provide a unified view of using natural language as knowledge representation for logical reasoning.In summary, the central focus is surveying and analyzing the emerging paradigm of performing logical reasoning by using natural language as knowledge representation instead of formal representations.


## What is the main contribution of this paper?

Based on my reading, the main contributions of the paper are:1. It proposes the concept of "logical reasoning over natural language as knowledge representation" (LRNL). This refers to using natural language as the knowledge representation and pretrained language models as the reasoners for logical reasoning tasks. 2. It provides a comprehensive review of existing work on LRNL, covering the three main types of logical reasoning - deductive, inductive, and abductive reasoning. For each reasoning type, it summarizes the task formulations, datasets, and methods.3. It discusses the advantages of LRNL over traditional symbolic approaches using formal representations, as well as over pure neural approaches. The key advantages highlighted are less brittleness, alleviating the knowledge acquisition bottleneck, handling raw natural language input, and improved robustness.4. It analyzes the current challenges and limitations of LRNL, including computational efficiency, reliability of rule/explanation generation, need for better evaluation metrics and benchmarks, etc.5. It provides suggestions for desirable future directions, such as incorporating probabilistic reasoning, reasoning with incomplete information, inductive reasoning on web corpora, abductive reasoning with long theories, interactions between reasoning types, etc.6. It relates LRNL to other relevant fields like neuro-symbolic computing, natural language inference, question answering, etc. to provide a clear picture of where LRNL stands in NLP.In summary, the paper provides a structured overview of the emerging field of LRNL, highlights its advantages and challenges, reviews the state-of-the-art, and offers insights into future directions. It brings together research from deductive, inductive and abductive reasoning under the unified lens of LRNL.
