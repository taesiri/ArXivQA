# [How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on   Deceptive Prompts](https://arxiv.org/abs/2402.13220)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent advancements in Multimodal Large Language Models (MLLMs) have made them very capable, but they remain vulnerable to hallucination when provided with deceptive information in prompts that is inconsistent with the accompanying image.  
- No existing work has comprehensively studied the robustness of MLLMs to deceptive prompts across multiple deception categories.

Proposed Solution:
- The authors introduce MAD-Bench, a carefully curated benchmark with 850 image-prompt pairs spanning 6 deception categories (object count, non-existent objects, object attributes, scene understanding, spatial relationships, visual confusion).
- They systematically test several state-of-the-art MLLMs on MAD-Bench, including open-sourced models like LLaVA-1.5 and proprietary systems like GPT-4V.

Key Findings:
- GPT-4V performs the best but still fails 25% of the time, while other models have 5-35% accuracy, showing high vulnerability to deception.
- The authors analyze common causes of errors and propose a simple method of prepending a descriptive system prompt which doubles accuracy for some models.
- However, absolute accuracy numbers remain low, indicating need for further research into enhancing robustness to deception.

Main Contributions:  
- Introduction of MAD-Bench benchmark to stimulate research on MLLM robustness to deceptive prompts
- Comprehensive analysis of multiple state-of-the-art MLLMs using MAD-Bench
- Demonstration that a simple prompt engineering method can significantly boost accuracy
