# [LLM-ARK: Knowledge Graph Reasoning Using Large Language Models via Deep   Reinforcement Learning](https://arxiv.org/abs/2312.11282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-4 show promising reasoning capabilities but still fall short on knowledge graph (KG) reasoning tasks compared to smaller models. 
- Two main challenges of applying LLM-based agents: 1) Limited perception of reasoning environments/lack of grounding limits competence. 2) Lack of mechanisms to optimize intermediate reasoning processes in multi-hop tasks.

Proposed Solution:
- Introduce LLM-ARK - a framework that uses LLMs as agents to perform reasoning on KGs via reinforcement learning.
- Employs a Full Textual Environment (FTE) prompt to provide complete state information to the LLM at each step.
- Reframes KG multi-hop inference as a sequential decision making problem and uses Proximal Policy Optimization (PPO) to optimize the model.
- Only updates parameters of addon actors, keeps LLM itself frozen. Enables learning from diverse reward signals.

Main Contributions:
- Assesses capabilities of GPT-4 on KG reasoning tasks and analyzes limitations.
- Proposes LLM-ARK which expresses KG reasoning as an RL problem and leverages FTE prompts plus PPO to optimize LLM agent's policies.
- Achieves state-of-the-art results on OpenDialKG dataset, significantly outperforming GPT-4 and prior methods. Demonstrates efficacy of grounding LLMs and using RL with policy optimization.

In summary, the paper tackles challenges in applying LLMs to KG reasoning by proposing an RL-based framework using FTE prompts and policy optimization to deliver precise and adaptable predictions that surpass previous approaches.
