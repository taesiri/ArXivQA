# [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating alignment of large language models (LLMs) with human preferences is challenging. Existing benchmarks have limitations in assessing real-world performance.
- Need for open, live benchmark to evaluate LLMs based on human preferences in diverse scenarios.

Proposed Solution - \system:  
- Crowdsourced platform where users conduct battles between LLMs by providing prompts and voting on preferred responses.
- Pairs of anonymous LLMs are shown to users. After multi-turn conversation, users vote on preferred LLM.  
- Collected over 240K votes from 90K users in 100+ languages.
- Employ statistical techniques like Bradley-Terry model to estimate model rankings from preferences.
- Designed efficient sampling algorithm to choose model pairs that accelerate ranking convergence.

Contributions:
- Built first large-scale, live benchmark for evaluating LLMs via human preference.  
- In-depth analysis of collected data - topic modeling for prompt diversity, crowdsourced vote quality vs experts.
- Developed active sampling method with statistical guarantees to improve ranking efficiency.
- Public release of dataset with 100K+ pairwise votes between LLMs based on human judgment.
- Leaderboard emerged as one of most referenced in LLM field.

In summary, the paper introduces \system, an open platform to evaluate LLMs through crowdsourced battles judged by human preference votes. Detailed analyses ensure collected data quality. Novel sampling algorithm improves ranking efficiency. The benchmark has become widely recognized and the public dataset enables further research.
