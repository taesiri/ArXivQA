# [Language Model Alignment with Elastic Reset](https://arxiv.org/abs/2312.07551)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Reinforcement learning from human feedback (RLHF) is a promising approach for aligning language models, but optimizing a learned reward signal can lead to "reward hacking", where the model achieves high reward but loses capabilities and drifts away from natural language. This is known as language drift, alignment tax, or reward model overoptimization. 
- The standard method to mitigate this issue is to add a KL divergence penalty between the finetuned and initial models. However, this slows down training, represents a tradeoff between reward and drift, and may be unnecessary. 

Proposed Solution - Elastic Reset:
- The authors propose Elastic Reset, which periodically resets the policy model to an exponentially moving average (EMA) of itself, and then resets the EMA to the initial model.  
- This allows the model to explore and achieve high reward, but then snap back to a more capable policy via the EMA reset, limiting drift. The EMA helps maintain performance after resets.

Main Contributions:
- Outperforms prior state-of-the-art on a pivot translation benchmark, achieving higher reward with equivalent drift levels.
- Proposes a Pareto Frontier graph to compare tradeoffs between reward and drift for different methods. Elastic Reset dominates other methods on this metric.
- Shows stronger performance on an IMDB mock sentiment task. Ablations demonstrate Elastic Reset is robust to most hyperparameters.
- Applies method to finetune LLaMA-7B with RLHF on a Stack Exchange dataset. Elastic Reset matches reward while reducing alignment tax.

Overall, Elastic Reset mitigates language drift in RLHF, achieving better alignment with less degradation of model capabilities. The paper demonstrates improved performance across tasks and model sizes. The computationally cheap resets make it an effective and scalable solution applicable to large language models.
