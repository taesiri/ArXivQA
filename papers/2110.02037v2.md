# [Autoregressive Diffusion Models](https://arxiv.org/abs/2110.02037v2)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be that autoregressive diffusion models (ARDMs) can match or exceed the performance of discrete diffusion models for generative modeling of complex, high-dimensional data like images, text, and audio, while being more computationally efficient due to requiring fewer steps/network calls during sampling and likelihood evaluation. Specifically, the paper proposes ARDMs as a variant of autoregressive models that incorporates ideas from recent diffusion models to allow flexible ordering and parallel generation of variables. The key claims are:- ARDMs generalize order-agnostic autoregressive models and absorbing discrete diffusion models.- Unlike standard autoregressive models, ARDMs don't require strict triangular masking of the model architecture.- ARDMs can be trained efficiently on individual steps of the generation process, like diffusion models. - ARDMs require far fewer steps than discrete diffusion to achieve the same performance.- ARDMs allow parallel generation of multiple tokens at once via dynamic programming, with minimal reduction in performance.- Empirically, ARDMs match or exceed the performance of discrete diffusion models on image, text and audio datasets, while requiring fewer sequential steps.- ARDMs are uniquely suited for neural lossless compression, achieving state-of-the-art results on compressing individual datapoints due to their flexible parallel generation.So in summary, the central hypothesis is about the modeling capacity, efficiency, and flexibility of ARDMs compared to previous autoregressive and diffusion models. The paper aims to demonstrate these advantages both theoretically and empirically.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The introduction of Autoregressive Diffusion Models (ARDMs), a new class of generative models that encompass and generalize order-agnostic autoregressive models and absorbing discrete diffusion models. 2. Showing that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance.3. Demonstrating that ARDMs can be parallelized using dynamic programming techniques, allowing them to generate multiple tokens simultaneously without substantially reducing performance. This also enables competitive lossless compression using a modest number of network calls.4. Providing evidence that ARDMs perform similarly to or better than discrete diffusion models, while being more efficient in the number of modelling steps required.5. Deriving an equivalence between ARDMs and absorbing diffusion models in the continuous time limit.In summary, the main contribution seems to be the proposal and empirical validation of ARDMs as a new class of generative models that are simple, flexible, and efficient compared to previous approaches like discrete diffusion models and standard autoregressive models. The ability to parallelize and upscale ARDMs while achieving strong performance is a key advantage demonstrated.
