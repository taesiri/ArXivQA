# AVIS: Autonomous Visual Information Seeking with Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an autonomous visual question answering agent that can dynamically utilize different external tools and APIs to gather the necessary knowledge needed to answer complex visual questions?The key hypotheses appear to be:1) Leveraging a large language model as the core reasoning engine will allow the system to strategically choose which tools/APIs to invoke at each step and analyze their outputs effectively.2) Guiding the system with real examples of human tool usage and decision making (gathered through a user study) will enhance the system's capability to make informed choices about tool selection and query formulation. 3) A dynamic decision-making approach where the system decides the next action based on previous execution results will be more effective than pre-planning the full sequence of steps initially.4) Retaining intermediate information in a working memory and allowing backtracking will improve the system's ability to explore the large combinatorial search space of tool combinations efficiently.In summary, the central research direction is developing an autonomous agent that can leverage different visual APIs/tools dynamically to gather the knowledge needed to answer complex visual questions, with hypotheses around using LLMs, human behavioral data, dynamic planning, and working memory to achieve this capability.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel visual question answering framework that uses a large language model (LLM) to dynamically determine which external tools to utilize (such as vision APIs, web search, image search) and analyze their outputs in order to gather the knowledge needed to answer complex visual questions.2. Conducting a user study to collect examples of human decision-making when using tools to answer visual information seeking questions. Using this data to develop a structured framework that guides the LLM to make informed choices about tool selection and query formation by following the user examples. 3. Achieving state-of-the-art results on knowledge-based VQA datasets like Infoseek and OK-VQA by using this proposed framework. On the Infoseek unseen entity split, the method reaches 50.7% accuracy, significantly outperforming prior work like fine-tuned PALI which gets 16.0% accuracy.4. Showing the advantage of dynamic decision making over fixed tool execution, with experiments indicating the framework outperforms baselines that use the same tools but in a predefined order without dynamic choices.In summary, the core novelties seem to be in enabling the LLM to dynamically decide which tools to leverage based on prior outputs, using human data to guide the LLM's decisions, and demonstrating improved performance on knowledge-intensive VQA as a result of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary: The paper proposes a novel visual question answering framework called AVIS that leverages a large language model to dynamically utilize external tools and analyze their outputs to gather the knowledge needed to answer complex visual questions.


## How does this paper compare to other research in the same field?

This paper presents a novel visual question answering framework that dynamically utilizes external tools and leverages large language models (LLMs) for planning and reasoning. Here are some key ways it compares to related work:- It focuses on handling complex, knowledge-intensive visual questions that require retrieving information from multiple sources. This differentiates it from much prior VQA research that focuses on simpler questions answerable from the image alone.- It takes a dynamic planning approach where the LLM decides which tool to invoke at each step based on previous results. This contrasts with many previous methods that pre-plan a fixed sequence of tools/APIs to call. The dynamic approach allows more flexible querying.- It incorporates real human decision-making data from a user study into the framework design. This guides the LLM via examples and constrains the action space. Most prior work lacks this injection of human behavior.- It achieves state-of-the-art results on knowledge-intensive VQA datasets, significantly outperforming prior methods. This demonstrates the efficacy of the proposed techniques.- It focuses on a different set of tools compared to some related work. For example, it utilizes an image search API and fine-grained computer vision models rather than just general web search.Overall, the key novelties are in dynamic planning with LLMs, leveraging human data for guidance, and the application to complex knowledge-retrieval VQA where this approach excels over alternatives. The results validate this approach and highlight the remaining challenges in this niche area of VQA research.
