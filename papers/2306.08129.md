# AVIS: Autonomous Visual Information Seeking with Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an autonomous visual question answering agent that can dynamically utilize different external tools and APIs to gather the necessary knowledge needed to answer complex visual questions?The key hypotheses appear to be:1) Leveraging a large language model as the core reasoning engine will allow the system to strategically choose which tools/APIs to invoke at each step and analyze their outputs effectively.2) Guiding the system with real examples of human tool usage and decision making (gathered through a user study) will enhance the system's capability to make informed choices about tool selection and query formulation. 3) A dynamic decision-making approach where the system decides the next action based on previous execution results will be more effective than pre-planning the full sequence of steps initially.4) Retaining intermediate information in a working memory and allowing backtracking will improve the system's ability to explore the large combinatorial search space of tool combinations efficiently.In summary, the central research direction is developing an autonomous agent that can leverage different visual APIs/tools dynamically to gather the knowledge needed to answer complex visual questions, with hypotheses around using LLMs, human behavioral data, dynamic planning, and working memory to achieve this capability.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel visual question answering framework that uses a large language model (LLM) to dynamically determine which external tools to utilize (such as vision APIs, web search, image search) and analyze their outputs in order to gather the knowledge needed to answer complex visual questions.2. Conducting a user study to collect examples of human decision-making when using tools to answer visual information seeking questions. Using this data to develop a structured framework that guides the LLM to make informed choices about tool selection and query formation by following the user examples. 3. Achieving state-of-the-art results on knowledge-based VQA datasets like Infoseek and OK-VQA by using this proposed framework. On the Infoseek unseen entity split, the method reaches 50.7% accuracy, significantly outperforming prior work like fine-tuned PALI which gets 16.0% accuracy.4. Showing the advantage of dynamic decision making over fixed tool execution, with experiments indicating the framework outperforms baselines that use the same tools but in a predefined order without dynamic choices.In summary, the core novelties seem to be in enabling the LLM to dynamically decide which tools to leverage based on prior outputs, using human data to guide the LLM's decisions, and demonstrating improved performance on knowledge-intensive VQA as a result of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary: The paper proposes a novel visual question answering framework called AVIS that leverages a large language model to dynamically utilize external tools and analyze their outputs to gather the knowledge needed to answer complex visual questions.


## How does this paper compare to other research in the same field?

This paper presents a novel visual question answering framework that dynamically utilizes external tools and leverages large language models (LLMs) for planning and reasoning. Here are some key ways it compares to related work:- It focuses on handling complex, knowledge-intensive visual questions that require retrieving information from multiple sources. This differentiates it from much prior VQA research that focuses on simpler questions answerable from the image alone.- It takes a dynamic planning approach where the LLM decides which tool to invoke at each step based on previous results. This contrasts with many previous methods that pre-plan a fixed sequence of tools/APIs to call. The dynamic approach allows more flexible querying.- It incorporates real human decision-making data from a user study into the framework design. This guides the LLM via examples and constrains the action space. Most prior work lacks this injection of human behavior.- It achieves state-of-the-art results on knowledge-intensive VQA datasets, significantly outperforming prior methods. This demonstrates the efficacy of the proposed techniques.- It focuses on a different set of tools compared to some related work. For example, it utilizes an image search API and fine-grained computer vision models rather than just general web search.Overall, the key novelties are in dynamic planning with LLMs, leveraging human data for guidance, and the application to complex knowledge-retrieval VQA where this approach excels over alternatives. The results validate this approach and highlight the remaining challenges in this niche area of VQA research.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the key future research directions suggested by the authors include:- Extending the dynamic decision-making framework to other reasoning tasks beyond visual question answering. The current system is specialized for VQA, but the general approach could potentially be applied to other multi-step reasoning problems.- Investigating the use of lighter weight language models than PALM. The system currently relies on a very large and computationally demanding LLM. Exploring more efficient LLMs that can still effectively perform the planning and reasoning would be valuable.- Incorporating additional tools and capabilities beyond the current set of visual APIs, search engines, etc. Expanding the diversity of available tools could enhance the range of questions and tasks that can be addressed.- Conducting additional user studies to further guide and improve the system's decisions. More data on human tool usage could help refine the transition graph and prompt examples used to direct the LLM.- Evaluating the framework on a wider set of VQA datasets and benchmark tasks to better understand its strengths and limitations.- Exploring alternative implementations of the planner, reasoner, and memory components that make up the overall system architecture.So in summary, the key suggestions are to generalize the approach to new tasks, scale down its computational requirements, expand the tool capabilities, gather more human data, and further evaluate and refine the system components. The paper lays out an interesting overall framework amenable to much future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a novel autonomous information seeking visual question answering framework called AVIS. AVIS leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools like computer vision models, web search, and image search to gather the knowledge needed to answer visual questions. It consists of an LLM-powered planner that determines which tool to invoke, a reasoner that analyzes tool outputs, and a working memory to retain information. A user study was conducted to collect examples of human tool usage, which informed a transition graph to guide tool selection and provide prompts to the planner and reasoner. AVIS achieves state-of-the-art results on knowledge-intensive VQA datasets like Infoseek and OK-VQA. The key innovation is the dynamic decision-making, where the next action is determined based on previous tool outputs rather than pre-planning all steps initially. This allows greater flexibility and backtracking when needed.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel visual question answering framework called AVIS that leverages a large language model (LLM) to dynamically utilize external tools and analyze their outputs to gather the knowledge needed to answer complex visual questions. AVIS is comprised of three main components: an LLM-powered planner that decides which tool to use next, an LLM-powered reasoner that extracts key information from the tool outputs, and a working memory that retains acquired information. To guide the LLM's decision-making, the authors conduct a user study to collect examples of how humans use tools to answer visual questions. From this, they create a transition graph that delineates possible actions at each state and provides relevant examples for the planner and reasoner. AVIS achieves state-of-the-art results on knowledge-intensive VQA datasets by making dynamic decisions grounded in previous executions and iteratively searching for effective tool combinations, unlike previous plan-then-execute approaches. The human decision-making data is key to structuring AVIS's framework and enhancing the LLM's capacity for informed choices regarding tool selection and query formation.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes an autonomous visual question answering framework called AVIS that leverages a large language model (LLM) to dynamically determine which external tools to utilize and how to process their outputs in order to gather the knowledge needed to answer complex visual questions. AVIS is comprised of three main components - an LLM-powered planner that decides which tool to use next and what query to send it, an LLM-powered reasoner that analyzes the tool outputs to extract key information, and a working memory that retains information throughout the process. To guide the LLM's decision making, user behavior data is collected through a study and used to construct a transition graph that restricts the action space and provide relevant examples for each state. The planner refers to this graph and examples at each step to choose the optimal action. The reasoner then processes the output and updates the state and memory accordingly. This iterative planning and reasoning continues until sufficient knowledge is obtained to answer the question.
