# [Adaptive Fusion of Single-View and Multi-View Depth for Autonomous   Driving](https://arxiv.org/abs/2403.07535)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-view depth estimation relies on accurate camera poses and can fail under noisy/incorrect poses in real-world scenarios like autonomous driving.  
- Single-view methods are more robust but less accurate due to scale ambiguity.
- Existing fusion methods don't properly handle inaccurate poses either.

Proposed Solution - Adaptive Fusion Network (AFNet):

- Uses both a single-view and a multi-view branch to predict depth and confidence maps.
- Proposes an Adaptive Fusion (AF) module to selectively combine single-view and multi-view outputs based on confidence.
- AF module uses 3 confidence maps - from the two branches and a wrapping confidence map based on texture consistency.  
- Adaptively selects more reliable pixels from either branch into the final output.

Main Contributions:

- Proposes a robustness benchmark to test multi-view performance under noisy poses.
- AFNet outperforms state-of-the-art on benchmark by using AF module to handle inaccuracies.
- Achieves state-of-the-art depth accuracy on KITTI and DDAD datasets.
- Shows improved handling of dynamic objects compared to traditional multi-view methods.
- Demonstrates better cross-dataset generalization ability indicating robustness.

In summary, the paper presents AFNet that can robustly estimate depth by adaptively fusing complementary single and multi-view information based on confidence predictions. This allows it to match state-of-the-art accuracy while being more robust to real-world challenges like pose noise.
