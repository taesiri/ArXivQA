# [CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer   Learning](https://arxiv.org/abs/2402.12736)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Achieving universally high accuracy in object detection is challenging. Mainstream focus is on detecting specific object classes, which requires deploying multiple networks and large amounts of GPU memory and storage. This presents challenges in effectively coordinating multiple detection tasks under resource constraints.  

- In new detection tasks, training from scratch is expensive and risks overfitting. Transfer learning via fine-tuning leverages pre-trained weights but full fine-tuning updates too many parameters. Existing parameter-efficient transfer learning (PETL) methods have focused more on classifiers rather than detection networks.

Proposed Solution:
- The paper proposes a lightweight fine-tuning strategy called Calibration Side Tuning (CST) that adapts successful adapter tuning and side tuning techniques from transformers for use with ResNet. 

- A separate small side network is introduced that operates synchronously with the ResNet backbone network. Features from side network are used to calibrate the backbone network features via a gating mechanism called Maximum Transition Calibration (MTC).

- Lightweight design choices are explored for the side network, using knowledge distillation and pruning techniques. Latter stages of ResNet tend to have more parameters, so intermediate residual layers in side network are eliminated.

Main Contributions:
- Analysis and implementation of multiple fine-tuning strategies applied to ResNet for object detection
- Introduction of CST architecture that integrates adapter and side tuning aspects along with MTC gating to enhance performance while ensuring smooth training
- Extensive experiments on 5 benchmark datasets demonstrating superior performance over state-of-the-art methods
- Better balance achieved between complexity and effectiveness compared to other fine-tuning techniques
- Analysis of fine-tuning performance across different types of datasets based on their correlation to the pre-training data
