# [Interventional Bag Multi-Instance Learning On Whole-Slide Pathological   Images](https://arxiv.org/abs/2303.06873)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It addresses the problem of slide-level label bias and dataset bias in multi-instance learning (MIL) for whole-slide image (WSI) classification. The authors argue that existing MIL methods for WSIs are susceptible to biases from the dataset/slides that can lead models to make predictions based on spurious correlations rather than meaningful tissue features. 

- The main hypothesis is that explicitly accounting for these biases and removing their effects via causal intervention will improve model robustness and performance on WSI classification tasks. 

- To test this hypothesis, the authors propose a novel Interventional Bag Multi-Instance Learning (IBMIL) method. The key idea is to use techniques from causal inference, specifically the backdoor adjustment, to eliminate the effect of confounding factors like slide-level dataset biases. 

- They introduce a structural causal model for the MIL pipeline and use it to guide an intervention approach. The confounders are approximated in an unsupervised fashion and their influence is removed during model training and inference via backdoor adjustment.

- Experiments on public WSI datasets demonstrate consistent improvement over state-of-the-art MIL techniques when applying IBMIL, supporting the hypothesis that causal intervention can make MIL models more robust. The proposed approach is general and can empower existing MIL methods.

In summary, this paper hypothesizes that causal intervention can reduce dataset bias effects in MIL-based WSI classification, leading to performance gains, and introduces IBMIL to test this hypothesis. The consistent boosting of multiple MIL techniques provides evidence supporting their hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a novel scheme called Interventional Bag Multi-Instance Learning (IBMIL) for whole-slide image classification. 

2. It analyzes the multi-instance learning (MIL) framework through the lens of causality and identifies the issue of "bag contextual prior" which causes spurious correlations between bags and labels.

3. It introduces a causal graph and uses the backdoor adjustment to remove the confounding effect of the bag contextual prior. This allows the model to focus on the true causal relationships between bags and labels. 

4. The proposed IBMIL method is compatible with existing MIL methods for WSIs and can empower them with causal intervention, leading to consistent performance improvements.

5. Extensive experiments on public datasets demonstrate the effectiveness of IBMIL. It boosts various state-of-the-art MIL methods significantly across different feature extractors and datasets.

In summary, the key innovation is the introduction of causal intervention to address the bias caused by bag contextual priors in MIL for WSIs. By eliminating this confounder, IBMIL enables existing MIL methods to achieve better generalization. The overall framework and experimental results are solid and demonstrate the utility of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Interventional Bag Multi-Instance Learning (IBMIL), a novel scheme for whole-slide image classification that uses causal inference techniques to reduce biases from dataset context and better link image labels to meaningful content.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research in the field of multi-instance learning on whole-slide pathological images:

- The paper proposes a new framework called Interventional Bag Multi-Instance Learning (IBMIL) that aims to reduce dataset bias and spurious correlations by using causal inference concepts like backdoor adjustment. This is a novel approach compared to most prior work that focuses on improving feature extraction or aggregation architectures. 

- Most prior MIL methods do not explicitly consider the potential confounding effects of dataset biases. The IBMIL framework is one of the first to analyze MIL through a causal inference lens and propose interventions to reduce bias.

- IBMIL is designed as a general framework that can be applied on top of existing MIL methods to improve performance. Experiments show consistent gains when IBMIL is added, demonstrating it is complementary to advances in feature learning. 

- The paper compares IBMIL against several recent state-of-the-art MIL techniques like ABMIL, DSMIL, TransMIL, and DTFD-MIL. The consistent gains of IBMIL across different base methods shows it is widely applicable.

- The IBMIL concept of using backdoor adjustment for debiasing is a new idea in MIL. Prior work either performed instance-level debiasing or relied more on data augmentation. IBMIL operates at the novel bag-level.

- The paper uses standard MIL benchmarks like Camelyon16 and TCGA-NSCLC to evaluate IBMIL. The gains on these datasets demonstrate the effectiveness over strong baselines.

In summary, the paper introduces a new debiasing perspective to MIL using causal inference, proposes the novel IBMIL technique for bag-level intervention, and shows consistent improvements over recent state-of-the-art MIL methods on standard benchmarks. This differentiates it from prior work in the field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing more efficient and elegant ways to approximate the confounder set for interventional training. The authors mention exploring incorporating expert knowledge or other approaches to get a better approximation of confounders without needing to use unsupervised clustering on the whole dataset.

- Exploring alternative implementations of the backdoor adjustment formulation, beyond the ones experimented with in this paper. The authors showed several implementations can work, but suggest more could be tried.

- Applying the proposed interventional training scheme to other MIL frameworks and tasks beyond classification, like segmentation and detection. The principle of this method is general so could likely benefit other MIL setups. 

- Incorporating the label information in some way during approximation of confounders, instead of the unsupervised clustering approach. The authors discuss this could help but they leave the exploration for future work.

- Experimenting with more reasonable assumptions for the confounder prior distribution P(c_i), instead of just a uniform distribution. Incorporating expert knowledge could help here.

- Simplifying the overall pipeline to avoid the extra stage for training aggregators before intervention. The authors propose and validate one way to do this using non-parametric pooling. More elegant solutions could be developed.

- Applying the interventional training approach to single-instance classification problems, not just MIL, to remove dataset biases and confounders.

So in summary, the main directions are improving the confounder approximation, simplifying the training process, expanding to new tasks and frameworks, and incorporating more informed priors or expert knowledge. The core idea of intervening to remove confounders is general and powerful.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel scheme called Interventional Bag Multi-Instance Learning (IBMIL) for whole-slide image classification. Unlike traditional likelihood-based strategies, IBMIL is based on causal inference and the backdoor adjustment formulation to remove the confounding effect of contextual biases in the training data. The authors introduce a structural causal model to analyze the causal relationships between the whole-slide images, their labels, and contextual biases. To eliminate the effect of these biases, IBMIL approximates the confounders using clustering and applies the backdoor adjustment during training to achieve deconfounded bag-level predictions. Experiments on two whole-slide image datasets demonstrate that IBMIL consistently improves state-of-the-art MIL methods by suppressing the bias caused by contextual information in the training data. The principle behind IBMIL is orthogonal to existing MIL methods, allowing it to boost their performance across different feature extractors and aggregation networks. Overall, IBMIL provides a new perspective on Multi-Instance Learning through the lens of causality.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a novel scheme called Interventional Bag Multi-Instance Learning (IBMIL) for whole-slide image classification. Multi-instance learning (MIL) is commonly used for this task, where each whole-slide image is treated as a labeled bag containing many unlabeled patch instances. Existing MIL methods focus on improving the feature extractor and aggregator components. However, they overlook the problem of "bag contextual prior" - spurious correlations between visual cues like color and the slide labels. IBMIL tackles this via causal inference. It introduces a causal graph relating the slide image, label, and contextual prior. The key difference is that instead of using likelihood P(Y|X) for prediction, IBMIL uses causal intervention P(Y|do(X)) based on backdoor adjustment. This allows suppressing the bias caused by the contextual prior. 

IBMIL has three main stages. First, a feature extractor is trained on patch instances. Next, an aggregator combines instance features into a bag feature for classification. Finally, the causal intervention is applied: the trained aggregator approximates confounders, which are then marginalized out using backdoor adjustment to achieve deconfounded bag predictions. Experiments on Camelyon16 and TCGA-NSCLC datasets show IBMIL boosts multiple MIL methods like ABMIL, DSMIL, TransMIL, and DTFD-MIL. Further analyses demonstrate the effectiveness and model design choices. Overall, IBMIL provides a novel perspective to handle biases in MIL via causal inference. As a general framework, it can empower existing MIL methods for whole-slide image analysis.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel scheme called Interventional Bag Multi-Instance Learning (IBMIL) for whole-slide pathological image classification. IBMIL introduces a causal graph to analyze the problem and identifies that the bag contextual prior information acts as a confounder between the bag content (key instances) and the label, causing spurious correlations. To eliminate this confounding effect, IBMIL applies the backdoor adjustment formula from causal inference, which removes the influence of confounders by incorporating them fairly. Specifically, it uses the trained MIL model to approximate confounders via clustering bag features. Then in the intervention stage, these confounders are integrated into the bag representation to achieve deconfounded predictions. By causal intervention, IBMIL is able to boost existing MIL methods by suppressing the bias caused by contextual priors in the training data. Experiments on public datasets across different MIL settings demonstrate the effectiveness and generalization ability of the proposed IBMIL framework.
