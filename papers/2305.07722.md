# [In Search of Verifiability: Explanations Rarely Enable Complementary   Performance in AI-Advised Decision Making](https://arxiv.org/abs/2305.07722)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Are explanations provided by AI systems actually helpful for human decision makers when used in an AI-assisted decision making context? The authors review prior work on explainable AI (XAI) for decision making and find mixed empirical results - some studies show explanations improve performance while others find no benefit or even negative effects. To make sense of these conflicting findings, the authors propose a theory that explanations are only useful to the extent they allow the human decision maker to verify the correctness of the AI's prediction. The key hypotheses appear to be:- Most existing XAI methods do not enable efficient verification and thus do not improve human-AI decision making performance.- The only examples where XAI explanations have been shown to achieve complementary performance are ones where the explanation supports answer verification.- Explanations should aim to provide "strategy-graded reliance" rather than "outcome-graded reliance." The former focuses on expected performance while the latter is problematic as it is post-hoc and nondeterministic.In summary, the main research question seems to be assessing whether explanations actually help or hurt in AI-assisted decision making, with a proposed theory that their utility depends on enabling verification of the AI's recommendation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a theory that explains why AI explanations often fail to improve human-AI decision making performance. The key points are:- AI explanations are only helpful for decision making if they allow the human to verify the correctness of the AI's recommendation. Explanations that just describe the AI's reasoning process typically do not support verification.- Most real-world decision making tasks do not easily allow for verification of the AI's answer. Hence explanations tend not to produce complementary performance where the human-AI team exceeds either alone.- The exceptions are tasks like QA and maze solving where the explanation provides key information that lets the human quickly check the AI's solution. This resembles how solutions to NP-complete problems can be efficiently verified. - The paper also clarifies the difference between outcome-graded reliance and strategy-graded reliance. It advocates for using strategy-graded reliance as the goal when evaluating human-AI decision making.In summary, the main insight is that explanations must enable verification to be useful for decision making, which most explanation types fail to do. This theory helps explain the mixed empirical results on whether explanations improve human-AI team performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper argues that AI explanations for decision making are only helpful to the extent they allow efficient human verification of the AI's recommendation, rather than just describing the AI's reasoning process.


## How does this paper compare to other research in the same field?

This paper provides an interesting perspective on the utility of explanations in AI-advised decision making. Here are a few key points of comparison to other work in this field:- Scope and Focus: The paper focuses specifically on the role of explanations in engendering complementary performance between humans and AI systems in decision making contexts. This is a more narrow focus compared to many XAI papers that examine explanations more broadly across applications like model development, knowledge discovery, etc.- Central Argument: The main argument is that explanations are only useful insofar as they allow humans to verify the AI's recommendation. This contrasts with common goals like making the model more interpretable or providing insight into its reasoning process. The emphasis on verification as the key criteria is quite distinctive.- Empirical Synthesis: The paper summarizes a wide array of recent empirical XAI studies and organizes them based on whether explanations enabled verification and complementary performance. This analysis helps explain the seemingly conflicting results in the literature. - Conceptual Clarity: There is an effort to bring more precision to ideas like appropriate reliance. Proposing alternate notions of outcome-graded vs. strategy-graded reliance clarifies this debate.- Practical Implications: By focusing squarely on performance in decision making, the paper questions the value of many popular XAI methods that don't enable verification, and suggests more research into interactive and evaluative explanations.Overall, this paper makes a strong conceptual argument by critically analyzing prior work and grounding its thesis firmly in the practical end goal of improving team performance. The emphasis on verification as the touchstone for useful explanations in decision making contexts distinguishes it from most research that takes interpretation or faithfulness as primary goals for XAI.


## What future research directions do the authors suggest?

The authors of this paper suggest several potential future research directions:- Further exploring the relationship between explanation verifiability and complementary performance. They suggest studying additional decision making tasks and explanation methods to strengthen or refine their theory.- Investigating alternative paradigms for human-AI collaboration that could achieve complementary performance without relying on explanations that enable verification, such as interactive or evaluative XAI systems, human-AI delegation workflows, or the use of cognitive forcing strategies. - Examining the utility of explanations for purposes beyond AI-assisted decision making on individual instances, such as model development, auditing, and knowledge discovery. The authors argue explanations may be better suited for these holistic AI processes compared to instance-level decision making support.- Clarifying definitions around appropriate reliance and studying reliance behaviors in human-AI decision making, potentially using the proposed notions of outcome-graded reliance versus strategy-graded reliance. - Considering the relationships between explanation verifiability, faithfulness, interactivity, and other explanation design choices. For example, studying whether unfaithful but verifiable explanations can support decision making.In summary, the authors advocate for more research on aligning XAI design with specific use cases and suggest their theory around verifiability provides a promising direction for improving explanations in AI-assisted decision making contexts. They encourage the exploration of alternative paradigms and a broader view of XAI utility beyond instance-level decision support.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper argues that AI explanations are only useful for improving human-AI decision making performance when they allow the human to easily verify the correctness of the AI's recommendation. The authors claim that most explanations found in the literature focus on describing the AI's reasoning process rather than enabling verification, and therefore fail to achieve complementary performance where the human-AI team outperforms either alone. They propose that the ability of an explanation to support verification lies on a spectrum, with tasks like maze solving that resemble NP-complete problems on one end, and real-world tasks like income prediction on the other. The authors also distinguish between two notions of appropriate reliance - outcome-graded and strategy-graded - advocating for the latter. Overall, they conclude that the goal of explanations should be to facilitate verification in order to achieve complementary human-AI performance in decision making.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper argues that AI explanations are only useful for improving human-AI decision making performance when they allow the human to easily verify the correctness of the AI's recommendation. Prior work has had mixed results on whether explanations improve human-AI decision making. Some studies find explanations increase blind trust without improving performance beyond the AI alone, while others show explanations can enable complementary performance where the team outperforms either individually. The authors propose that the key factor determining an explanation's utility is whether it facilitates verification of the AI's answer. Explanations describing the model's reasoning process rarely enable verification, and hence fail to improve performance. In contrast, explanations that highlight key features supporting the answer, like rationales for QA, can enable efficient verification and complementary performance. The paper further clarifies the notion of appropriate reliance, advocating for a strategy-based definition over an outcome-based one. Overall, the paper provides a simple theory unifying the conflicting empirical results around explanations for human-AI decision making.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper presents a theoretical perspective to explain the inconclusive results on whether AI explanations improve human-AI decision making performance. The authors argue that AI explanations are only useful for decision making to the extent they allow a human to verify the correctness of the AI's recommendation. They claim most prior work has focused on explaining the AI's reasoning process rather than enabling verification. The paper reviews studies across various decision making tasks, categorizing them based on whether the explanations enabled verification and whether complementary performance was achieved. The results show that only tasks where explanations supported verification resulted in complementary performance, while most other explanation types did not improve decision making. Based on this categorization, the authors propose that verification is the key factor determining when AI explanations will be useful for decision making.


## What problem or question is the paper addressing?

The paper is addressing the question of when AI explanations are useful for AI-assisted decision making. Specifically, it is trying to make sense of the conflicting results in prior work on whether explanations help humans make better decisions when working with an AI system.The paper notes that some studies have found explanations help improve human-AI decision making performance, while other studies have found explanations do not help or can even hurt performance. The authors aim to provide a unifying theory that explains these contradictory findings.Their main argument is that AI explanations are helpful for decision making to the extent they allow a human to verify the correctness of the AI's recommendation. Explanations that focus on explaining the AI's reasoning process rather than enabling verification often fail to improve decision making performance. The paper also discusses concepts like complementary performance, where the human-AI team outperforms either alone, and appropriate reliance, where the human trusts the AI when it is likely to be right. The goal is to understand when explanations can achieve these objectives in an AI-assisted decision making context.In summary, the key question is: under what circumstances can AI explanations actually improve human decision making when working with an AI assistant? The paper proposes that verifiability of the AI's recommendation is the key factor.
