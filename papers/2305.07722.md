# In Search of Verifiability: Explanations Rarely Enable Complementary   Performance in AI-Advised Decision Making

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Are explanations provided by AI systems actually helpful for human decision makers when used in an AI-assisted decision making context? The authors review prior work on explainable AI (XAI) for decision making and find mixed empirical results - some studies show explanations improve performance while others find no benefit or even negative effects. To make sense of these conflicting findings, the authors propose a theory that explanations are only useful to the extent they allow the human decision maker to verify the correctness of the AI's prediction. The key hypotheses appear to be:- Most existing XAI methods do not enable efficient verification and thus do not improve human-AI decision making performance.- The only examples where XAI explanations have been shown to achieve complementary performance are ones where the explanation supports answer verification.- Explanations should aim to provide "strategy-graded reliance" rather than "outcome-graded reliance." The former focuses on expected performance while the latter is problematic as it is post-hoc and nondeterministic.In summary, the main research question seems to be assessing whether explanations actually help or hurt in AI-assisted decision making, with a proposed theory that their utility depends on enabling verification of the AI's recommendation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a theory that explains why AI explanations often fail to improve human-AI decision making performance. The key points are:- AI explanations are only helpful for decision making if they allow the human to verify the correctness of the AI's recommendation. Explanations that just describe the AI's reasoning process typically do not support verification.- Most real-world decision making tasks do not easily allow for verification of the AI's answer. Hence explanations tend not to produce complementary performance where the human-AI team exceeds either alone.- The exceptions are tasks like QA and maze solving where the explanation provides key information that lets the human quickly check the AI's solution. This resembles how solutions to NP-complete problems can be efficiently verified. - The paper also clarifies the difference between outcome-graded reliance and strategy-graded reliance. It advocates for using strategy-graded reliance as the goal when evaluating human-AI decision making.In summary, the main insight is that explanations must enable verification to be useful for decision making, which most explanation types fail to do. This theory helps explain the mixed empirical results on whether explanations improve human-AI team performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper argues that AI explanations for decision making are only helpful to the extent they allow efficient human verification of the AI's recommendation, rather than just describing the AI's reasoning process.


## How does this paper compare to other research in the same field?

This paper provides an interesting perspective on the utility of explanations in AI-advised decision making. Here are a few key points of comparison to other work in this field:- Scope and Focus: The paper focuses specifically on the role of explanations in engendering complementary performance between humans and AI systems in decision making contexts. This is a more narrow focus compared to many XAI papers that examine explanations more broadly across applications like model development, knowledge discovery, etc.- Central Argument: The main argument is that explanations are only useful insofar as they allow humans to verify the AI's recommendation. This contrasts with common goals like making the model more interpretable or providing insight into its reasoning process. The emphasis on verification as the key criteria is quite distinctive.- Empirical Synthesis: The paper summarizes a wide array of recent empirical XAI studies and organizes them based on whether explanations enabled verification and complementary performance. This analysis helps explain the seemingly conflicting results in the literature. - Conceptual Clarity: There is an effort to bring more precision to ideas like appropriate reliance. Proposing alternate notions of outcome-graded vs. strategy-graded reliance clarifies this debate.- Practical Implications: By focusing squarely on performance in decision making, the paper questions the value of many popular XAI methods that don't enable verification, and suggests more research into interactive and evaluative explanations.Overall, this paper makes a strong conceptual argument by critically analyzing prior work and grounding its thesis firmly in the practical end goal of improving team performance. The emphasis on verification as the touchstone for useful explanations in decision making contexts distinguishes it from most research that takes interpretation or faithfulness as primary goals for XAI.
