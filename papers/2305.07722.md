# In Search of Verifiability: Explanations Rarely Enable Complementary   Performance in AI-Advised Decision Making

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Are explanations provided by AI systems actually helpful for human decision makers when used in an AI-assisted decision making context? The authors review prior work on explainable AI (XAI) for decision making and find mixed empirical results - some studies show explanations improve performance while others find no benefit or even negative effects. To make sense of these conflicting findings, the authors propose a theory that explanations are only useful to the extent they allow the human decision maker to verify the correctness of the AI's prediction. The key hypotheses appear to be:- Most existing XAI methods do not enable efficient verification and thus do not improve human-AI decision making performance.- The only examples where XAI explanations have been shown to achieve complementary performance are ones where the explanation supports answer verification.- Explanations should aim to provide "strategy-graded reliance" rather than "outcome-graded reliance." The former focuses on expected performance while the latter is problematic as it is post-hoc and nondeterministic.In summary, the main research question seems to be assessing whether explanations actually help or hurt in AI-assisted decision making, with a proposed theory that their utility depends on enabling verification of the AI's recommendation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a theory that explains why AI explanations often fail to improve human-AI decision making performance. The key points are:- AI explanations are only helpful for decision making if they allow the human to verify the correctness of the AI's recommendation. Explanations that just describe the AI's reasoning process typically do not support verification.- Most real-world decision making tasks do not easily allow for verification of the AI's answer. Hence explanations tend not to produce complementary performance where the human-AI team exceeds either alone.- The exceptions are tasks like QA and maze solving where the explanation provides key information that lets the human quickly check the AI's solution. This resembles how solutions to NP-complete problems can be efficiently verified. - The paper also clarifies the difference between outcome-graded reliance and strategy-graded reliance. It advocates for using strategy-graded reliance as the goal when evaluating human-AI decision making.In summary, the main insight is that explanations must enable verification to be useful for decision making, which most explanation types fail to do. This theory helps explain the mixed empirical results on whether explanations improve human-AI team performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper argues that AI explanations for decision making are only helpful to the extent they allow efficient human verification of the AI's recommendation, rather than just describing the AI's reasoning process.
