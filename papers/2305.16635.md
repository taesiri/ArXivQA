# [Impossible Distillation: from Low-Quality Model to High-Quality Dataset   &amp; Model for Summarization and Paraphrasing](https://arxiv.org/abs/2305.16635)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether language models can learn to summarize and paraphrase sentences without requiring massive scale, instruction data, or human feedback. The key hypothesis appears to be that small, off-the-shelf language models possess latent knowledge about these tasks, even if the models themselves cannot reliably solve the tasks. The paper proposes that this latent knowledge can be identified and amplified into a high-quality dataset and task-specific model, allowing efficient and effective training without the typical resources used for unsupervised summarization and paraphrasing.In summary, the central research question is whether task-specific models for summarization and paraphrasing can be distilled from small language models, without scale or supervision. The hypothesis is that small LMs contain latent knowledge that can be extracted and amplified to create high-quality datasets and models for these tasks.
