# [Impossible Distillation: from Low-Quality Model to High-Quality Dataset   &amp; Model for Summarization and Paraphrasing](https://arxiv.org/abs/2305.16635)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether language models can learn to summarize and paraphrase sentences without requiring massive scale, instruction data, or human feedback. The key hypothesis appears to be that small, off-the-shelf language models possess latent knowledge about these tasks, even if the models themselves cannot reliably solve the tasks. The paper proposes that this latent knowledge can be identified and amplified into a high-quality dataset and task-specific model, allowing efficient and effective training without the typical resources used for unsupervised summarization and paraphrasing.In summary, the central research question is whether task-specific models for summarization and paraphrasing can be distilled from small language models, without scale or supervision. The hypothesis is that small LMs contain latent knowledge that can be extracted and amplified to create high-quality datasets and models for these tasks.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a new framework called Impossible Distillation that can distill a high-quality task-specific dataset and model from an off-the-shelf language model, without requiring large scale or human supervision. Specifically, the key ideas are:1) Generating candidate input-output pairs for a task (e.g. summarization) directly from a pre-trained LM using constrained decoding strategies.2) Filtering high-quality pairs using task-specific filters based on entailment, length, diversity etc. 3) Using the generated dataset to train an initial task model, then further amplifying its capability via self-distillation.The end result is a compact yet powerful model that can outperform much larger LMs like GPT-3 on summarization and paraphrasing, as demonstrated through both automatic metrics and human evaluation.Additionally, the framework produces a large high-quality dataset as a byproduct, which exhibits more diversity and effectiveness than human-authored datasets based on their analysis.Overall, the key novelty seems to be distilling task knowledge directly from a pre-trained LM into a dataset and model without relying on scale or supervision, while achieving strong performance. The proposed framework and analysis around the dataset quality are the main contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel framework called Impossible Distillation that can distill high-quality datasets and models for summarization and paraphrasing tasks directly from off-the-shelf language models, without requiring massive scale or human supervision.
