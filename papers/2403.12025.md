# [A Toolbox for Surfacing Health Equity Harms and Biases in Large Language   Models](https://arxiv.org/abs/2403.12025)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT hold promise for serving complex health information needs, but also risk exacerbating health disparities if they introduce bias or fail certain groups. Evaluating and mitigating these equity harms is critical.  

- LLMs pose new evaluation challenges due to open-ended text generation and need for multidimensional assessments. Prior work studied smaller evaluations of race/gender bias.

Solution:
- The authors present resources to evaluate equity harms in LLM medical question answering:
   - Multifactorial rubrics for human rating of bias in LLM answers along 6 dimensions tied to equity. Three rubrics proposed (independent, pairwise, counterfactual).
   - "EquityMedQA" - 7 new adversarial datasets to surface equity biases, using diverse design approaches.

- Large empirical study applies these to evaluate Med-PaLM. Uses 806 raters - physicians, equity experts, consumers. Over 17,000 human ratings produced.

Main Contributions:
- Participatory iterative design of rubrics, grounded in expert input, focus groups, model failures.

- Multifactorial rubrics and EquityMedQA datasets complement one another to surface biases missed by narrower approaches.

- Study reveals importance of diverse raters - different groups flag different issues based on background.

- Resources can be extended to new models, domains etc. Not comprehensive but allows community growth.

- Overall, paper demonstrates practical framework to evaluate equity biases in medical LLMs to enable safer and more equitable AI.
