# [Improving Fairness via Federated Learning](https://arxiv.org/abs/2110.15545)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1. Is federated learning necessary for training fair models on decentralized data? In other words, can we simply train locally fair models on each decentralized dataset and aggregate them to get a fair global model?2. How does the performance (accuracy and fairness tradeoff) of current federated learning algorithms for fair learning compare to training on centralized data? Can federated learning match the performance of centralized training? 3. Can we develop an improved federated learning algorithm that achieves better accuracy-fairness tradeoff compared to current approaches and comes closer to the performance of centralized training?To summarize, the key goals of the paper appear to be:- Analyze whether federated learning provides benefits over non-federated approaches for fair decentralized learning.- Identify performance gaps between current federated fair learning algorithms and centralized training.- Propose a new federated learning algorithm called FedFB that improves on current approaches and achieves near-centralized performance.The central hypothesis seems to be that federated learning is necessary but current algorithms are limited, and the proposed FedFB algorithm can bridge the gap by more effectively mimicking centralized training. The theoretical analysis and empirical evaluations aim to validate these claims.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Develops a theoretical framework for analyzing and comparing different approaches to fair learning on decentralized data. In particular, the paper theoretically shows:1) Local Fair Training + Ensemble (LFT+Ensemble) achieves lower fairness than Local Fair Training + FedAvg (LFT+FedAvg). This demonstrates the necessity of using federated learning for fair decentralized learning.2) LFT+FedAvg achieves lower fairness than Centralized Fair Learning (CFL). This shows there is a limitation to just applying FedAvg naively for fair decentralized learning. - Proposes a new federated learning algorithm called FedFB that is designed specifically for fair decentralized learning. FedFB is inspired by the FairBatch algorithm for centralized fair learning.- Empirically evaluates LFT+Ensemble, LFT+FedAvg, FedFB, and CFL on several datasets. The results validate the theoretical findings and show that FedFB achieves significantly better fairness than LFT+FedAvg while approaching the performance of CFL.In summary, the key contribution is identifying limitations of existing approaches through theoretical analysis, and proposing a novel federated learning algorithm (FedFB) tailored for decentralized fair learning that bridges the gap between LFT+FedAvg and CFL. The paper demonstrates both theoretically and empirically that federated learning is necessary but not sufficient - designing customized algorithms like FedFB is important for good performance.


## How does this paper compare to other research in the same field?

This paper proposes a new approach for improving fairness in federated learning scenarios where data is decentralized across multiple clients. The key contributions and differences from prior work are:1. Theoretical Analysis: The paper provides a theoretical analysis comparing different decentralized fair learning approaches like Local Fair Training + Ensemble (LFT+Ensemble), LFT+FedAvg, and Centralized Fair Learning (CFL). It shows that:- LFT+Ensemble can only achieve limited fairness due to data heterogeneity across clients. - LFT+FedAvg can achieve better fairness than LFT+Ensemble by aggregating updates, but is still limited compared to CFL.This analysis formally shows the need for federated learning to improve fairness, but also its limitations compared to centralized training. Prior work had not rigorously characterized these tradeoffs.2. New Algorithm - FedFB: The paper proposes a novel federated learning algorithm called FedFB that modifies FedAvg to mimic centralized fair learning. Experiments show it matches or exceeds the fairness of CFL on several datasets.Most prior federated fair learning work had simply applied off-the-shelf algorithms like GIFAIR or q-FFL locally and aggregated with FedAvg. By co-designing the local and global updates, FedFB provides significantly better performance.3. Broader Applicability: FedFB works for various notions of group fairness (demographic parity, equal opportunity etc.) unlike some prior work tailored to specific definitions. The bilevel optimization framework makes it easy to adapt to different fairness constraints.Overall, this paper pushes forward the state of the art in decentralized fair learning through its theoretical analysis elucidating fundamental tradeoffs and a practical algorithm that demonstrates centralized-comparable fairness is achievable via federated learning. The proposed techniques and analysis help advance a nascent but important research area.
