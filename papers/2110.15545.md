# [Improving Fairness via Federated Learning](https://arxiv.org/abs/2110.15545)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions seem to be:1. Is federated learning necessary for training fair models on decentralized data? In other words, can we simply train locally fair models on each decentralized dataset and aggregate them to get a fair global model?2. How does the performance (accuracy and fairness tradeoff) of current federated learning algorithms for fair learning compare to training on centralized data? Can federated learning match the performance of centralized training? 3. Can we develop an improved federated learning algorithm that achieves better accuracy-fairness tradeoff compared to current approaches and comes closer to the performance of centralized training?To summarize, the key goals of the paper appear to be:- Analyze whether federated learning provides benefits over non-federated approaches for fair decentralized learning.- Identify performance gaps between current federated fair learning algorithms and centralized training.- Propose a new federated learning algorithm called FedFB that improves on current approaches and achieves near-centralized performance.The central hypothesis seems to be that federated learning is necessary but current algorithms are limited, and the proposed FedFB algorithm can bridge the gap by more effectively mimicking centralized training. The theoretical analysis and empirical evaluations aim to validate these claims.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:- Develops a theoretical framework for analyzing and comparing different approaches to fair learning on decentralized data. In particular, the paper theoretically shows:1) Local Fair Training + Ensemble (LFT+Ensemble) achieves lower fairness than Local Fair Training + FedAvg (LFT+FedAvg). This demonstrates the necessity of using federated learning for fair decentralized learning.2) LFT+FedAvg achieves lower fairness than Centralized Fair Learning (CFL). This shows there is a limitation to just applying FedAvg naively for fair decentralized learning. - Proposes a new federated learning algorithm called FedFB that is designed specifically for fair decentralized learning. FedFB is inspired by the FairBatch algorithm for centralized fair learning.- Empirically evaluates LFT+Ensemble, LFT+FedAvg, FedFB, and CFL on several datasets. The results validate the theoretical findings and show that FedFB achieves significantly better fairness than LFT+FedAvg while approaching the performance of CFL.In summary, the key contribution is identifying limitations of existing approaches through theoretical analysis, and proposing a novel federated learning algorithm (FedFB) tailored for decentralized fair learning that bridges the gap between LFT+FedAvg and CFL. The paper demonstrates both theoretically and empirically that federated learning is necessary but not sufficient - designing customized algorithms like FedFB is important for good performance.


## How does this paper compare to other research in the same field?

 This paper proposes a new approach for improving fairness in federated learning scenarios where data is decentralized across multiple clients. The key contributions and differences from prior work are:1. Theoretical Analysis: The paper provides a theoretical analysis comparing different decentralized fair learning approaches like Local Fair Training + Ensemble (LFT+Ensemble), LFT+FedAvg, and Centralized Fair Learning (CFL). It shows that:- LFT+Ensemble can only achieve limited fairness due to data heterogeneity across clients. - LFT+FedAvg can achieve better fairness than LFT+Ensemble by aggregating updates, but is still limited compared to CFL.This analysis formally shows the need for federated learning to improve fairness, but also its limitations compared to centralized training. Prior work had not rigorously characterized these tradeoffs.2. New Algorithm - FedFB: The paper proposes a novel federated learning algorithm called FedFB that modifies FedAvg to mimic centralized fair learning. Experiments show it matches or exceeds the fairness of CFL on several datasets.Most prior federated fair learning work had simply applied off-the-shelf algorithms like GIFAIR or q-FFL locally and aggregated with FedAvg. By co-designing the local and global updates, FedFB provides significantly better performance.3. Broader Applicability: FedFB works for various notions of group fairness (demographic parity, equal opportunity etc.) unlike some prior work tailored to specific definitions. The bilevel optimization framework makes it easy to adapt to different fairness constraints.Overall, this paper pushes forward the state of the art in decentralized fair learning through its theoretical analysis elucidating fundamental tradeoffs and a practical algorithm that demonstrates centralized-comparable fairness is achievable via federated learning. The proposed techniques and analysis help advance a nascent but important research area.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one sentence summary:The paper proposes FedFB, a novel federated learning algorithm that leverages ideas from centralized fair learning to improve model fairness on decentralized data, showing both theoretically and empirically that it outperforms existing approaches and closely matches the performance of training on centralized data.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing theoretical guarantees for the fairness-accuracy tradeoff of decentralized fair learning algorithms like FedFB. The current theory characterizes the limits of certain algorithms like LFT+Ensemble and LFT+FedAvg, but does not fully characterize the tradeoff curve for algorithms like FedFB. Extending the theory to cover the full tradeoff would be valuable.- Studying the three-way tradeoff between accuracy, fairness, and privacy more rigorously. The authors suggest analyzing the fundamental limits on what can be achieved subject to constraints on all three desiderata. - Extending the FedFB algorithm to handle other fairness notions like proportional fairness that require fairness guarantees localized to each client's data distribution. The current FedFB focuses on global fairness metrics.- Scaling up FedFB and analyzing its performance empirically with more clients. The experiments in the paper are limited to 2-4 clients. Testing on larger federated networks could reveal more about the approach's strengths and limitations.- Developing better algorithms for encrypted or differentially private federated learning to limit privacy losses. The authors suggest exploring techniques like secure aggregation and differential privacy to strengthen the privacy guarantees of approaches like FedFB.- Studying issues like fair treatment of clients with lower computational resources. Ignoring such clients in aggregation can bias the model, so addressing this issue is important.- Applying FedFB to real-world case studies and datasets to demonstrate its practical utility. The experiments are on semi-synthetic and benchmark datasets. Testing on real-world problems could surface important considerations.In summary, the authors highlight opportunities to strengthen the theory,algorithms, privacy, and experimental evaluation of decentralized fair learning approaches like FedFB. Advancing research in these directions can drive progress on training fair models from decentralized data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes a new federated learning algorithm called FedFB for training fair machine learning models on decentralized data. The key idea is to leverage the state-of-the-art centralized fair learning algorithm FairBatch in a federated setting. Specifically, FedFB modifies the FedAvg protocol so that clients not only share model parameters but also exchange fairness-related statistics (e.g. subgroup losses). The central server aggregates model parameters via FedAvg and updates the sample weights by mimicking FairBatch. This allows FedFB to optimize both model parameters and sample weights to achieve fairness, similar to centralized training. Theoretically, the paper shows that federated learning is necessary but naive approaches like Local Fair Training + FedAvg have limitations. Empirically, FedFB outperforms existing federated fair learning algorithms and achieves comparable performance to centralized training on benchmark datasets. The proposed algorithm provides an effective way to train fair models on decentralized data.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a new algorithm called FedFB for training fair machine learning models in a federated learning setting. Federated learning allows multiple parties to collaboratively train a model without sharing their raw data. The paper first analyzes existing approaches for fair federated learning, including simply training local fair models and aggregating (LFT+Ensemble), and training local models with FedAvg (LFT+FedAvg). Through theoretical analysis, the authors show that LFT+Ensemble is limited in the fairness it can achieve, while LFT+FedAvg can achieve better fairness but is still limited compared to training on centralized data. To overcome these limitations, the paper introduces FedFB, which modifies the FedAvg protocol to mimic centralized fair training. FedFB trains models locally using reweighted loss functions, aggregates subgroup-specific losses at each round, and updates the sample weights to optimize fairness. Theoretical analysis shows FedFB can converge to an optimal set of weights. Experiments on several datasets demonstrate FedFB achieves significantly better fairness than LFT+Ensemble and LFT+FedAvg, sometimes nearly matching the performance of a model trained on centralized data. The results show FedFB enables training high-performing fair models in a federated setting without sharing raw data between parties.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes FedFB, a novel federated learning algorithm for training fair machine learning models on decentralized data. FedFB is based on FairBatch (FB), a state-of-the-art centralized fair learning algorithm. The key idea is that the bi-level optimization structure of FB naturally fits with the hierarchical structure of federated learning. In particular, FB optimizes sample weights for different groups via a bi-level optimization - an inner loop that trains models by minimizing a weighted empirical risk, and an outer loop that optimizes the group weights by minimizing unfairness. FedFB runs this bi-level optimization in a federated manner - the inner loop trains models locally on each client's data, aggregates the models via FedAvg, while the outer loop aggregates subgroup losses via secure aggregation to optimize group weights centrally. This allows FB's iterative reweighting mechanism to be executed in a federated way, enabling decentralized data to collaboratively learn fair models with minimal communication overhead. Experiments show FedFB matches centralized performance, outperforming prior federated fair learning methods.


## What problem or question is the paper addressing?

 The paper is addressing the challenge of training fair machine learning models in a federated learning setting. Specifically, it focuses on the problem of how to train a model that satisfies a notion of group fairness, such as demographic parity, when the training data is decentralized across multiple clients. The key questions the paper aims to answer are:- Is federated learning necessary for training fair models on decentralized data, or can we just train locally fair models on each client's data and ensemble them?- How does the performance of existing federated learning algorithms like FedAvg compare to centralized fair learning on pooled data in terms of the fairness-accuracy tradeoff? - Can we design a novel federated learning algorithm that matches or exceeds the performance of centralized fair learning?The paper provides both theoretical analysis and empirical results to address these questions. The main contributions are:1) Demonstrating theoretically and empirically that federated learning is necessary to achieve high fairness compared to just ensembling locally fair models.2) Proving a gap exists between the fairness-accuracy tradeoff of FedAvg-based approaches and centralized fair learning.3) Proposing a new federated learning algorithm called FedFB that significantly outperforms prior methods and approximates the performance of centralized fair learning.In summary, the paper addresses the open problem of how to effectively train fair models in a federated learning setting where data is decentralized, and shows federated learning is key to achieving high fairness compared to non-federated approaches. The proposed FedFB algorithm aims to match the fairness-accuracy tradeoff of centralized training.
