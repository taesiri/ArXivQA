# [Improving Fairness via Federated Learning](https://arxiv.org/abs/2110.15545)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1. Is federated learning necessary for training fair models on decentralized data? In other words, can we simply train locally fair models on each decentralized dataset and aggregate them to get a fair global model?2. How does the performance (accuracy and fairness tradeoff) of current federated learning algorithms for fair learning compare to training on centralized data? Can federated learning match the performance of centralized training? 3. Can we develop an improved federated learning algorithm that achieves better accuracy-fairness tradeoff compared to current approaches and comes closer to the performance of centralized training?To summarize, the key goals of the paper appear to be:- Analyze whether federated learning provides benefits over non-federated approaches for fair decentralized learning.- Identify performance gaps between current federated fair learning algorithms and centralized training.- Propose a new federated learning algorithm called FedFB that improves on current approaches and achieves near-centralized performance.The central hypothesis seems to be that federated learning is necessary but current algorithms are limited, and the proposed FedFB algorithm can bridge the gap by more effectively mimicking centralized training. The theoretical analysis and empirical evaluations aim to validate these claims.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Develops a theoretical framework for analyzing and comparing different approaches to fair learning on decentralized data. In particular, the paper theoretically shows:1) Local Fair Training + Ensemble (LFT+Ensemble) achieves lower fairness than Local Fair Training + FedAvg (LFT+FedAvg). This demonstrates the necessity of using federated learning for fair decentralized learning.2) LFT+FedAvg achieves lower fairness than Centralized Fair Learning (CFL). This shows there is a limitation to just applying FedAvg naively for fair decentralized learning. - Proposes a new federated learning algorithm called FedFB that is designed specifically for fair decentralized learning. FedFB is inspired by the FairBatch algorithm for centralized fair learning.- Empirically evaluates LFT+Ensemble, LFT+FedAvg, FedFB, and CFL on several datasets. The results validate the theoretical findings and show that FedFB achieves significantly better fairness than LFT+FedAvg while approaching the performance of CFL.In summary, the key contribution is identifying limitations of existing approaches through theoretical analysis, and proposing a novel federated learning algorithm (FedFB) tailored for decentralized fair learning that bridges the gap between LFT+FedAvg and CFL. The paper demonstrates both theoretically and empirically that federated learning is necessary but not sufficient - designing customized algorithms like FedFB is important for good performance.


## How does this paper compare to other research in the same field?

This paper proposes a new approach for improving fairness in federated learning scenarios where data is decentralized across multiple clients. The key contributions and differences from prior work are:1. Theoretical Analysis: The paper provides a theoretical analysis comparing different decentralized fair learning approaches like Local Fair Training + Ensemble (LFT+Ensemble), LFT+FedAvg, and Centralized Fair Learning (CFL). It shows that:- LFT+Ensemble can only achieve limited fairness due to data heterogeneity across clients. - LFT+FedAvg can achieve better fairness than LFT+Ensemble by aggregating updates, but is still limited compared to CFL.This analysis formally shows the need for federated learning to improve fairness, but also its limitations compared to centralized training. Prior work had not rigorously characterized these tradeoffs.2. New Algorithm - FedFB: The paper proposes a novel federated learning algorithm called FedFB that modifies FedAvg to mimic centralized fair learning. Experiments show it matches or exceeds the fairness of CFL on several datasets.Most prior federated fair learning work had simply applied off-the-shelf algorithms like GIFAIR or q-FFL locally and aggregated with FedAvg. By co-designing the local and global updates, FedFB provides significantly better performance.3. Broader Applicability: FedFB works for various notions of group fairness (demographic parity, equal opportunity etc.) unlike some prior work tailored to specific definitions. The bilevel optimization framework makes it easy to adapt to different fairness constraints.Overall, this paper pushes forward the state of the art in decentralized fair learning through its theoretical analysis elucidating fundamental tradeoffs and a practical algorithm that demonstrates centralized-comparable fairness is achievable via federated learning. The proposed techniques and analysis help advance a nascent but important research area.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes FedFB, a novel federated learning algorithm that leverages ideas from centralized fair learning to improve model fairness on decentralized data, showing both theoretically and empirically that it outperforms existing approaches and closely matches the performance of training on centralized data.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing theoretical guarantees for the fairness-accuracy tradeoff of decentralized fair learning algorithms like FedFB. The current theory characterizes the limits of certain algorithms like LFT+Ensemble and LFT+FedAvg, but does not fully characterize the tradeoff curve for algorithms like FedFB. Extending the theory to cover the full tradeoff would be valuable.- Studying the three-way tradeoff between accuracy, fairness, and privacy more rigorously. The authors suggest analyzing the fundamental limits on what can be achieved subject to constraints on all three desiderata. - Extending the FedFB algorithm to handle other fairness notions like proportional fairness that require fairness guarantees localized to each client's data distribution. The current FedFB focuses on global fairness metrics.- Scaling up FedFB and analyzing its performance empirically with more clients. The experiments in the paper are limited to 2-4 clients. Testing on larger federated networks could reveal more about the approach's strengths and limitations.- Developing better algorithms for encrypted or differentially private federated learning to limit privacy losses. The authors suggest exploring techniques like secure aggregation and differential privacy to strengthen the privacy guarantees of approaches like FedFB.- Studying issues like fair treatment of clients with lower computational resources. Ignoring such clients in aggregation can bias the model, so addressing this issue is important.- Applying FedFB to real-world case studies and datasets to demonstrate its practical utility. The experiments are on semi-synthetic and benchmark datasets. Testing on real-world problems could surface important considerations.In summary, the authors highlight opportunities to strengthen the theory,algorithms, privacy, and experimental evaluation of decentralized fair learning approaches like FedFB. Advancing research in these directions can drive progress on training fair models from decentralized data.
