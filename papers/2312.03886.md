# [On The Fairness Impacts of Hardware Selection in Machine Learning](https://arxiv.org/abs/2312.03886)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Hardware selection in machine learning is often viewed as just a utility, but the choice of hardware can significantly impact model performance and fairness across different demographic groups. This issue is especially important for ML-as-a-service platforms where users lack control over hardware.

- The paper shows empirically that the same model architecture trained on different hardware can produce vastly different accuracy levels for minority vs majority groups. For example, facial recognition accuracy for minority ethnic groups varied greatly with hardware choice while accuracy for majority groups was stable. 

Key Factors Identified:
- The paper identifies two key factors that contribute to hardware-induced performance disparities across groups:
   1) Differences in gradient flows during training - hardware choices alter gradient updates, affecting model optimization differently for each group
   2) Variations in curvature of loss surface (captured by Hessian eigen values) - this impacts model separability and generalization capability differently across groups

- These factors disproportionately impact minority groups, leading to a "rich get richer, poor get poorer" effect where hardware exacerbates existing disparities.

Proposed Solution:
- The paper proposes an effective mitigation strategy that adds a regularization term to the loss function to equalize the average distance to the decision boundary across groups during training.  

- This approach does not require explicitly computing the Hessian, making it efficient.

- Experiments show this mitigation technique significantly reduces maximum difference in accuracy between groups, cutting it by 66% on one dataset, demonstrating its effectiveness.

Key Contributions:
- Identifies gradient flows and curvature of loss surface as key factors causing hardware-induced fairness issues
- Extensive experiments across diverse hardware, datasets and architectures validate findings
- Proposes computationally-efficient mitigation technique that reduces accuracy disparities across groups by explicitly regularizing model training.

In summary, this paper provides novel insights into an overlooked aspect of responsible ML - the significant influence of hardware choices on model fairness - and proposes an actionable strategy to mitigate such issues.
