# [Automated Search-Space Generation Neural Architecture Search](https://arxiv.org/abs/2305.18030)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Automated Search-Space Generation Neural Architecture Search":

Problem:
Existing neural architecture search (NAS) methods rely on manually designing a search space beforehand, which requires significant human expertise and limits their applicability. The goal is to automatically generate the search space and train a general deep neural network (DNN) to construct a high-performing, compact sub-network that achieves state-of-the-art performance with minimal human intervention.

Proposed Solution:
The paper proposes Automated Search-space Generation Neural Architecture Search (ASGNAS), an end-to-end system to automatically generate the search space of a general DNN, train it using a novel hierarchical optimizer, and construct an optimal sub-network. The main components are:

1. Automated Search Space Generation: A graph algorithm that analyzes the DNN architecture and dependency between modules to identify a set of removal structures. This forms the search space for redundancy removal.

2. Hierarchical Half-Space Projected Gradient (H2SPG): A hierarchical sparse optimizer that alternates between a search phase to identify redundant modules while maintaining validity and a training phase to optimize non-redundant parts. It produces a solution with high performance and desired sparsity.  

3. Automated Sub-Network Construction: Constructs a compact, high-performance sub-network by removing modules corresponding to redundant structures identified by H2SPG based on the hierarchy.

Main Contributions:

- First end-to-end system to automate search space generation, training, and sub-network construction for general DNNs.

- Novel graph algorithm to automatically analyze DNN architectures and generate search spaces.

- H2SPG, the first optimizer for hierarchical structured sparsity that considers network topology to ensure validity.  

- State-of-the-art results on DNNs like RegNet, StackedUnets, SuperResNet on CIFAR10, FashionMNIST, ImageNet, and other datasets with 50-80% reduction in parameters and FLOPs.

The system significantly reduces manual effort in NAS by automating the pipeline. H2SPG is a breakthrough in hierarchical sparse optimization for DNNs. Together, they enable automated, one-shot NAS for general DNN architectures.
