# [Understanding Disparities in Post Hoc Machine Learning Explanation](https://arxiv.org/abs/2401.14539)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Post-hoc explanation methods like LIME exhibit disparities in explanation fidelity (how well the explanation method approximates the black box model behavior) across subgroups based on sensitive attributes like race and gender. 
- Prior work has focused on improving explanation methods themselves to mitigate this issue, but the role of data properties and black box model characteristics is not well studied in relation to explanation disparities.

Objectives:
- Assess challenges originating from data properties that lead to explanation disparities: limited sample size, covariate shift, concept shift, omitted variable bias
- Assess challenges from model properties: inclusion of sensitive attribute, appropriate functional form

Methods:
- Use explanation quality metrics: maximum fidelity gap from average, mean fidelity gap between subgroups 
- Conduct experiments on simulated data based on causal graphs and real-world Adult dataset
- Consider logistic regression and neural network models as black box models
- Use LIME as the explanation method

Key Findings:
- Explanation disparities higher for neural networks than logistic regression models
- Sample size of disadvantaged group did not affect disparities if sensitive attribute properly excluded
- More covariate shift and concept shift increased explanation disparities, especially for neural networks
- Omitting variables with direct effects increased explanation disparities
- Findings were partially consistent on real-world data

Contributions:
- First study to systematically analyze the effect of data properties and model characteristics on explanation disparities
- Provides recommendations to improve data quality and model specification to mitigate undesirable disparities


## Summarize the paper in one sentence.

 This paper systematically investigates how properties of the data (limited sample size, covariate shift, concept shift, omitted variable bias) and models (inclusion of sensitive attributes, model complexity) contribute to disparities in the fidelity of post-hoc explanation methods across subgroups.


## What is the main contribution of this paper?

 The main contribution of this paper is a systematic investigation of how properties of the data and black box models can lead to disparities in explanations generated by the popular LIME post-hoc explanation method. 

Specifically, through controlled simulations and experiments on a real-world dataset (Adult income), the paper examines how sample size imbalance, covariate shift, concept shift, and omitted variable bias affect explanation disparities across groups defined by a sensitive attribute such as gender or race. It considers these data issues in combination with properties of the black box model, including model complexity and inclusion/exclusion of the sensitive attribute.

The key findings are that increased covariate shift, concept shift, and omitted variables lead to larger explanation disparities, especially for more complex neural network models compared to simpler linear models. The paper also observes that explanations exhibit higher disparity when sensitive attributes are included in black box model training in a manner inconsistent with the causal graph.

Based on these systematic simulation analyses, the paper provides recommendations for designing explanation methods and benchmark datasets that help mitigate undesirable disparities. The investigation underscores the need to account for properties of data and models, in addition to the explanation algorithms themselves, when assessing and addressing biases in explanations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Explainable AI (XAI)
- Post hoc explanation methods
- Local Interpretable Model-agnostic Explanations (LIME)
- Fidelity gap
- Disparities in explanations across subgroups (e.g. gender, race)
- Sample size imbalance
- Covariate shift 
- Concept shift
- Omitted variable bias
- Causal directed acyclic graphs (DAGs)
- Maximum Fidelity Gap from Average
- Mean Fidelity Gap Amongst Subgroups

The paper examines disparities in the fidelity of the popular LIME post hoc explanation method across subgroups. It systematically investigates how properties of the data (sample size, covariate shift, concept shift, omitted variables) and models (inclusion of sensitive attributes, model complexity) can contribute to these explanation disparities. The analysis uses both simulated data based on causal DAGs as well as a real-world dataset (Adult income). Overall, the key goal is to understand the factors that can lead to unfair explanations in order to provide recommendations for more robust and equitable explanation methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. This paper proposes examining four data properties - sample size, covariate shift, concept shift, and omitted variable bias - and their role in generating disparate explanations across groups. Why were these four properties chosen to study as opposed to other data properties? 

2. The paper utilizes synthetic simulations and real-world data analysis to test the hypotheses. What are the relative advantages and disadvantages of each approach? Why was it useful to leverage both methods?

3. Both linear (logistic regression) and non-linear (neural network) models were used as black box models. Why is it important to test explanation methods on both types of models? What differences might we expect to see in the results?

4. Explain in detail the data generating process used for the synthetic simulations. What motivated the choice of relationships between the variables in the causal graph? 

5. Two key metrics are used to quantify explanation disparities - Maximum Fidelity Gap and Mean Fidelity Gap. Explain what each metric captures and why both metrics were used in the analyses. 

6. For the experiments on the Adult dataset, the paper makes a case for the presence of concept shift and omitted variable bias. Walk through the evidence presented that supports the existence of these data issues for the specific prediction task.

7. The results reveal higher disparities for neural network models compared to linear models in the synthetic simulations. Why might complex models exhibit greater issues with explanation disparities?

8. The paper observes differences in the results between the simulated and real-world datasets. Discuss the key differences and why they may have emerged. What implications does this have for future benchmark datasets?

9. The paper analyzes disparities specifically for the LIME explanation method. How might the findings differ if other post-hoc explanation methods were studied instead? What are relevant future directions for methods besides LIME?  

10. Based on the findings, the paper offers recommendations such as focusing on the quality of data and choice of black box model complexity. Expand upon how researchers and practitioners can leverage these guidelines when applying explanation methods.
