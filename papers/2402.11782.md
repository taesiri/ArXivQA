# [What Evidence Do Language Models Find Convincing?](https://arxiv.org/abs/2402.11782)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Retrieval-augmented language models (LLMs) are increasingly used for open-domain question answering, where they must resolve ambiguous or controversial queries by searching through conflicting web evidence. 
- It's unclear what types of evidence these models actually find convincing when answers are ambiguous. Past work has studied human perceptions, but little on AI systems.

Proposed Solution:
- The authors create a dataset called ConflictingQA that contains controversial questions paired with real-world web paragraphs arguing different stances. 
- They use this to analyze what textual features correlate with an LLM judging a paragraph as convincing when paired against conflicting evidence.

Key Contributions:
- ConflictingQA benchmarks LLMs on resolving realistic conflicting evidence for open-ended queries.
- Analysis shows models strongly rely on relevance to the question over stylistic factors that influence humans (references, objectivity).  
- Simple relevance-based perturbations (adding a relevance statement) substantially boost paragraph convincingness.
- Results suggest the need to improve evidence quality in retrieval-augmented systems and better align training with human credibility judgements.

In summary, the paper demonstrates that current LLMs overly focus on relevance rather than argument quality when judging what evidence is convincing for ambiguous questions. The authors construct a valuable benchmark and analysis to highlight this issue.
