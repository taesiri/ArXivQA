# [COMMA: Co-Articulated Multi-Modal Learning](https://arxiv.org/abs/2401.00268)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Pretrained large-scale vision-language models like CLIP have shown impressive performance on downstream tasks. However, they are sensitive to variations in input text prompts and require careful selection of prompt templates. 
- Recently proposed methods that dynamically learn prompts during fine-tuning have two key limitations:
  1) The prompts for the vision and language branches are separated or uni-directionally correlated, which does not fully align representations.
  2) These methods tend to overfit seen classes, causing worse performance on unseen classes compared to CLIP.

Proposed Solution: 
- The authors propose Co-Articulated Multi-Modal Learning (COMMA) to address the above limitations.

- Correlated Prompt Generation: Compute prompts in each branch based on preceding prompts from both branches. This enhances correlation to better align representations.

- Alleviating Forgetting of Generic Knowledge: Minimize discrepancy between learned prompts and CLIP's hand-crafted prompts in later layers to retain essential knowledge and improve generalization.

Key Contributions:
- Propose correlated prompt generation using prompts from both modalities to better align representations.

- Alleviate forgetting generic knowledge by approximating hand-crafted CLIP prompts to improve generalization ability.

- Evaluated on base-to-novel generalization, cross-dataset transfer, and domain generalization tasks. Results show COMMA outperforms recent methods, demonstrating consistent improvements in generalization ability.

- More parameter-efficient and faster training than recent prompting methods.

In summary, COMMA enhances vision-language model adaptation via correlated prompt learning and knowledge transfer while improving efficiency. It shows state-of-the-art performance on multiple generalization tasks.
