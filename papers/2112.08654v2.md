# [Learning to Prompt for Continual Learning](https://arxiv.org/abs/2112.08654v2)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a new paradigm for continual learning that does not rely on a rehearsal buffer or known task identity at test time?The key hypothesis behind the proposed method Learning to Prompt (L2P) is that we can learn a small set of prompt parameters to dynamically instruct a frozen pre-trained model to solve new tasks sequentially, instead of adapting the model weights continually. Specifically, the paper hypothesizes that:- By learning prompt parameters in a shared pool, the model can maintain both task-invariant knowledge (in shared prompts) and task-specific knowledge (in unshared prompts). This allows knowledge transfer when beneficial while reducing interference between tasks.- Querying the prompt pool in an instance-wise fashion based on input features removes the need for known task identity at test time. - The resulting prompt-based framework can achieve strong continual learning performance without relying on a rehearsal buffer or known task boundaries.In summary, the central hypothesis is that prompting provides a new way to tackle the key challenges of catastrophic forgetting and task identity dependence in continual learning. The paper aims to demonstrate the viability of this new prompting paradigm through empirical results.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new method called Learning to Prompt (L2P) for continual learning. This is a novel framework that uses prompts (small learnable parameters) stored in a prompt pool to dynamically instruct a frozen pre-trained model to solve new tasks sequentially. - The prompt pool allows sharing knowledge between similar tasks while avoiding catastrophic forgetting on dissimilar tasks. The prompts serve as a form of memory that accumulates task-specific knowledge.- An instance-wise prompt query mechanism that allows selecting relevant prompts for each input without needing to know the task identity. This enables tackling the challenging task-agnostic continual learning setting.- Demonstrating strong performance of L2P on multiple continual learning benchmarks, including class-incremental, domain-incremental, and task-agnostic settings. It outperforms prior state-of-the-art methods consistently.- Showing that L2P achieves competitive performance even without using a rehearsal buffer, unlike most prior methods. This is useful for real-world applications where privacy concerns may prohibit buffering training data.- Introducing the idea of prompting from NLP to the field of continual learning. The prompts provide a new way to tackle catastrophic forgetting by storing knowledge compactly as "instructions" for a frozen model.In summary, the main contribution is proposing a novel prompting-based continual learning framework that mitigates forgetting, shares knowledge, and removes the need for task identity or rehearsal data. The idea of using prompts is new to continual learning and shows promising results on various benchmarks.
