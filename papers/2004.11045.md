# [Distilling Knowledge for Fast Retrieval-based Chat-bots](https://arxiv.org/abs/2004.11045)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:Can transferring knowledge from a BERT cross-encoder to a BERT bi-encoder via distillation improve the bi-encoder's performance on response retrieval tasks without affecting its inference speed?The key points related to this question seem to be:- BERT cross-encoders achieve better prediction quality on response retrieval tasks than BERT bi-encoders, but are too slow for practical use. - BERT bi-encoders are much faster but have lower prediction quality.- The authors propose using knowledge distillation to transfer knowledge from a BERT cross-encoder (teacher model) to a BERT bi-encoder (student model). - This is intended to improve the bi-encoder's prediction quality while maintaining its fast inference speed.- They introduce a new BERT cross-encoder architecture optimized for response retrieval to serve as the teacher model.- Experiments on 3 datasets aim to demonstrate the effectiveness of the proposed knowledge distillation approach at improving the BERT bi-encoder's performance without slowing inference.So in summary, the central hypothesis appears to be that knowledge distillation from the proposed cross-encoder teacher can enhance the bi-encoder student's prediction quality at no cost to its inference speed. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a knowledge distillation approach to improve the performance of BERT bi-encoders for response retrieval, without affecting their inference speed. Specifically:- They introduce an enhanced BERT cross-encoder architecture designed specifically for response retrieval, which serves as the teacher model. This model achieves better performance than regular BERT cross-encoders.- They use the BERT bi-encoder as the student model, and train it to mimic the outputs of the teacher model using knowledge distillation. - Experiments on three datasets show that the distilled BERT bi-encoder achieves significantly improved performance compared to training only on the dataset labels, while maintaining fast inference speeds.- They demonstrate a trade-off between the cross-encoder teacher which gets better results but is slow, versus the distilled bi-encoder student which gets moderately improved results but is much faster for inference.So in summary, the key contribution is using knowledge distillation to transfer knowledge from a high-quality but slow cross-encoder to a fast bi-encoder, improving the bi-encoder's performance at no cost to inference speed. The improved cross-encoder architecture and experimental validation are other notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a knowledge distillation method to transfer knowledge from an enhanced BERT cross-encoder to a BERT bi-encoder for response retrieval in chatbots, improving the bi-encoder's performance without slowing down inference time.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in response retrieval for conversational systems:- The paper focuses on improving bi-encoder models, which are faster at inference time but less accurate than cross-encoder models. This is a useful direction since inference speed is important for real applications.- Using knowledge distillation to transfer knowledge from a cross-encoder teacher to a bi-encoder student is a novel approach in this field. Most prior work has focused on modifications to the model architectures themselves.- The proposed enhanced cross-encoder architecture with cross-attention and SubMult matching is also novel for response retrieval. It achieves better accuracy than the standard BERT cross-encoder.- The paper examines the effectiveness of the methods on multiple standard datasets (UDC, DSTC7, MANtIS). This evaluation on diverse datasets strengthens the results.- However, the paper does not quite achieve state-of-the-art results on these datasets compared to some other recent papers. This seems to be largely due to computational constraints.- There is limited ablation and analysis of why the proposed methods are effective. More analysis could provide useful insights.Overall, the paper introduces two useful innovations (the enhanced cross-encoder and distillation approach) that improve upon prior work in response retrieval. The gains are clearly demonstrated empirically. More analysis and comparisons could further situate the work, but it is solidly in line with progress in this research area.
