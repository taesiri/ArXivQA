# [A Self-matching Training Method with Annotation Embedding Models for   Ontology Subsumption Prediction](https://arxiv.org/abs/2402.16278)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing ontology embedding models for concept subsumption prediction struggle with predicting similar and isolated entities. They also do not fully capture the global and local information from annotation axioms. 

- Specifically, conventional Random Forest (RF) classifiers fail to predict superclasses that are similar to the subclass or isolated from other entities seen during training.

- Word embedding models like Word2Vec primarily capture local co-occurrence information between words, but not global information about relationships between entities and words.

Proposed Solution:
- A self-matching training method for an RF classifier that improves prediction of similar and isolated entities.

- Two new ontology embedding models called InME (Inverted-index Matrix Embedding) and CoME (Co-occurrence Matrix Embedding) that capture global and local annotation information. 

- InME constructs an inverted index matrix between entities and words to represent global information about which words appear in annotations of which entities.

- CoME constructs a co-occurrence matrix to capture local information about which words appear together in annotations.

- These matrices are compressed to low-dimensional embeddings using an autoencoder. Entity embeddings are generated by averaging the word embeddings.

- The self-matching method supplements training data with self-matching samples of the form (entity, entity) to make the RF classifier more robust.

Main Contributions:

- Demonstrating that global annotation information is more useful than logical axioms for concept subsumption prediction.

- The self-matching training method to improve prediction of similar and isolated entities using entity similarity learned from supplementary self-matching samples.

- The InME and CoME models to effectively represent global and local annotation information in a way that helps concept subsumption prediction.

- Experiments showing state-of-the-art performance combining InME and self-matching on GO and FoodOn ontologies, and CoME + OWL2Vec* on HeLiS ontology.
