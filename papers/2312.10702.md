# [Can persistent homology whiten Transformer-based black-box models? A   case study on BERT compression](https://arxiv.org/abs/2312.10702)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like BERT have revolutionary performance but are computationally expensive and lack interpretability due to their complex architecture. 
- There is a need for methodologies that can provide explainability to BERT while also compressing it to make deployment more feasible.

Proposed Solution:
- The authors propose Optimus BERT Compression and Explainability (OBCE), a novel methodology to bring explainability to BERT using persistent homology. 
- It analyzes the topological features of each neuron's outputs on a dataset using 0-dimensional persistent homology. 
- This allows assessing the importance of each neuron in BERT's computations. Less important neurons are removed to compress the network.

Methodology:
- Select Wikipedia dataset for analysis 
- For each neuron, extract [CLS] tokens from outputs when evaluated on dataset
- Use persistent homology to extract topological features and distribution of rf values
- rf indicates variability of outputs - higher rf implies more informative neuron
- Remove less informative neurons based on rf distribution 
- Construct simplified BERT model with reduced parameters

Results:
- Reduced BERT Base to 58.47% original parameters (from 110M to 64M)
- Reduced BERT Large to 52.3% original parameters (from 340M to 177.2M)
- Compressed models outperform state-of-the-art on GLUE Benchmark
- Improves performance over original BERT on some GLUE tasks

Contributions:
- First methodology to use persistent homology for BERT compression & explainability 
- Interprets topological role of each neuron in BERT computations
- Constructs simplified BERT models that excel on GLUE Benchmark
- Makes BERT more explainable and efficient for deployment

In summary, the paper puts forth an innovative way to provide interpretability into the inner workings of complex LLMs like BERT while also compressing them greatly with little performance loss. The use of topological data analysis and persistent homology to identify and prune unimportant neurons enables significant model compression and represent a novel application of these mathematical concepts.
