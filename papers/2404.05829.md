# [SambaLingo: Teaching Large Language Models New Languages](https://arxiv.org/abs/2404.05829)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Large language models (LLMs) have limited availability in languages other than English/Chinese/French/Arabic. Pretraining LLMs from scratch in other languages requires massive data and compute. Multilingual LLMs also struggle to achieve good performance across all languages due to factors like data scarcity in many languages.

Proposed Solution:
- Present a comprehensive methodology to adapt pretrained LLMs like Llama 2 to new target languages via continued pretraining and alignment to human preferences.

Key Contributions:
- Best practices for adapting LLMs to new languages, evaluated across 9 typologically diverse languages at 7B and 70B scale:
  - Expanding model vocabulary for target language improves inference efficiency
  - Subword embedding initialization accelerates training convergence 
  - Mixing some data from base model's language is beneficial
- Recipe for human preference alignment with minimal need for expensive alignment data from target language
- State of the art models & evaluations across benchmarks in adapted languages, outperforming prior language-specific and multilingual models
- Publicly released model checkpoints to enable further research  

In summary, the paper proposes and comprehensively evaluates a methodology for adapting English-centric LLMs to new languages with minimal data requirements. This provides an effective path for obtaining strong language models in more languages. Code and state-of-the-art adapted models are also open-sourced.
