# [Are Vision Transformers More Data Hungry Than Newborn Visual Systems?](https://arxiv.org/abs/2312.02843)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper investigates whether vision transformers (ViTs) are more data hungry than newborn animal visual systems for learning view-invariant object recognition. The authors performed parallel controlled-rearing experiments on chicks and ViTs. Chicks were raised in impoverished environments with only one object, then tested on recognizing that object from novel views. The authors simulated the chicks' visual experience in Unity to train ViTs on the same limited data. They developed a new self-supervised ViT model called ViT-CoT that leverages temporal relationships to learn view-invariant features without labels. ViT-CoT and other ViTs matched or exceeded the chicks' recognition performance when tested on the same images and task, showing they can learn as effectively as newborns from sparse data. The results imply the attention mechanism and temporal signals in ViTs are sufficient to develop animal-like object recognition. Neither chicks nor ViTs were found to be inherently more data hungry. The study challenges assumptions about transformers requiring more data than brains and suggests ViTs offer a promising model of biological visual learning.
