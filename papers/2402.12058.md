# [Scaffolding Coordinates to Promote Vision-Language Coordination in Large   Multi-Modal Models](https://arxiv.org/abs/2402.12058)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large multi-modal models (LMMs) like GPT-4V have shown impressive capabilities in vision-language tasks. However, their performance is still limited when conducting complex reasoning that requires coordinating multiple levels of visual information. Existing methods for improving LMMs either require additional training data or rely on external tools, lacking a simple and general visual prompting scheme. 

Proposed Solution:  
The paper proposes "Scaffold", a novel visual prompting method that uses scaffolding coordinates to promote vision-language coordination in LMMs. Specifically, Scaffold overlays a dot matrix onto the input image and labels each dot with multi-dimensional Cartesian coordinates. The coordinates serve as visual anchors and are included in the textual prompt to connect visual and textual information. This scaffold steers LMMs to leverage the coordinates for solving vision-language tasks, enhancing their capability to perceive precise visual details and perform accurate textual reasoning.

Main Contributions:
- Proposes Scaffold, a simple and versatile visual prompting scheme that uses scaffolding coordinates to promote vision-language coordination in LMMs
- Demonstrates Scaffold's effectiveness across diverse vision-language tasks including spatial reasoning, compositional reasoning, fine-grained visual grounding, and hallucination
- Shows that Scaffold can be combined with other techniques like active perception and Chain-of-Thought prompting for further performance gains
- Reveals promising future directions of using Scaffold for active perception and automatic coordination of visual prompt attributes

In summary, the paper presents Scaffold, a novel visual prompting technique that overlays visual anchors and textual references to enhance LMMs' coordination between vision and language. Extensive experiments validate Scaffold's versatility and potential for unlocking LMM capabilities.
