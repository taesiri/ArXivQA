# Automatic Chain of Thought Prompting in Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How does the number of clusters used for question clustering affect the error rate of question answering when using zero-shot CoT prompting?The paper investigates clustering the questions into different numbers of clusters (2, 4, 6, and 8 clusters) and analyzing the error rate of zero-shot CoT prompting on the questions in each cluster. The hypothesis appears to be that the errors made by zero-shot CoT prompting will tend to cluster in certain groups of similar questions, so the number of clusters can illuminate this phenomenon. By showing the error rate per cluster for different numbers of clusters, the paper examines how the granularity of the clustering affects the observed clustering of errors.In summary, the central research question is about how varying the number of question clusters affects the observed grouping of errors made by zero-shot CoT prompting on a question answering task. The hypothesis is that errors will cluster in groups of similar questions.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be the proposal of an automatic chain-of-thought (Auto-CoT) prompting method to eliminate the need for manual design of demonstrations in few-shot prompting of large language models. The key ideas are:1) Using question clustering and diversity-based sampling to construct prompting demonstrations automatically, rather than relying on manual design. This helps mitigate the issue of models replicating reasoning mistakes from demonstrations.2) Leveraging the "Let's think step by step" prompt to generate reasoning chains as demonstrations from the sampled questions, taking advantage of LLMs' capabilities as decent zero-shot reasoners. 3) Applying simple heuristics to select higher quality demonstrations during sampling.The proposed Auto-CoT method is evaluated on 10 reasoning tasks and shown to match or exceed the performance of manual CoT prompting that requires hand-designed demonstrations. Overall, the main contribution is providing a way to automate the demonstration design process for few-shot prompting of LLMs, while maintaining competitive reasoning performance compared to manual demonstration design.
