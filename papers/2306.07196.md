# [Retrieval-Enhanced Contrastive Vision-Text Models](https://arxiv.org/abs/2306.07196)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems that the main research question is how to improve the performance of existing vision-text models like CLIP on fine-grained recognition tasks using an external memory. The key hypothesis is that equipping pretrained vision-text models with the ability to retrieve and incorporate cross-modal knowledge from a large external memory can enhance their representations and lead to better performance on challenging fine-grained tasks where the model may not have directly seen examples of certain rare or novel concepts during pretraining.The paper proposes a method called Retrieval-Enhanced Contrastive (RECO) learning that trains a model to refine the original CLIP embeddings using relevant information retrieved from the memory in a cross-modal manner. The main goal is to validate whether this retrieval mechanism can improve CLIP's capabilities for fine-grained zero-shot predictions without having to retrain CLIP itself.In summary, the central research question is whether retrieval and fusion of external knowledge can enhance existing vision-text models for fine-grained recognition in a zero-shot transfer setting. The key hypothesis is that cross-modal retrieval specifically will enrich the representations and alignment of vision and text encoders for improved fine-grained generalization.
