# [Learning to Pool in Graph Neural Networks for Extrapolation](https://arxiv.org/abs/2106.06210v2)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable graph neural networks (GNNs) to better generalize to out-of-distribution data through the design of pooling functions. The key hypothesis is that by using a flexible, learnable pooling function called Generalized Norm-based Pooling (GNP), GNNs can successfully extrapolate beyond the training data distribution. Specifically, the paper hypothesizes that GNP will allow a GNN model to identify the optimal pooling function for a given task, enhancing its ability to extrapolate.In summary, the main research question is how to improve the generalization and extrapolation abilities of GNNs, with the core hypothesis being that the proposed GNP pooling function can enable models to learn the appropriate pooling operator for a given task. The paper aims to demonstrate through experiments that simply using GNP consistently for aggregation and readout leads to excellent extrapolation across a variety of graph-related tasks.


## What is the main contribution of this paper?

The main contribution of this paper appears to be proposing a new learnable pooling function called Generalized Norm-based Pooling (GNP) for graph neural networks (GNNs). The key points are:- GNP is a flexible $L^p$ norm-like pooling function that can be trained end-to-end for any task. It generalizes common pooling functions like sum, mean, max, and min.- GNP allows GNNs to successfully extrapolate to out-of-distribution data. Proper choice of pooling functions is crucial for extrapolation but varies across tasks. GNP can learn to pick the right pooling functions for a given task.- Experiments on node, graph and set tasks show GNP enables extrapolation comparable or better than carefully chosen existing pooling functions. GNP also improves GNN performance on real-world graph classification and influence maximization.- GNP adds minimal overhead to GNNs as it has very few additional parameters. But some techniques are proposed to stabilize training.In summary, the main contribution is proposing GNP, a simple yet flexible learnable pooling function, and demonstrating its effectiveness for improving generalization and extrapolation of GNNs on a variety of tasks. The results suggest GNP can serve as a drop-in replacement for commonly used pooling functions.
