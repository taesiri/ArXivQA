# [TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance   Segmentation](https://arxiv.org/abs/2312.06630)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes TMT-VIS, a novel taxonomy-aware multi-dataset joint training framework for video instance segmentation (VIS). The key insight is that simply combining multiple VIS datasets leads to diluted attention across different taxonomies. To address this, TMT-VIS incorporates explicit taxonomy guidance into the model. It consists of two modules - the Taxonomy Compilation Module (TCM) which compiles likely taxonomy labels from the input video using a CLIP text encoder, and the Taxonomy Injection Module (TIM) which injects the compiled taxonomy embeddings into the decoder queries via cross-attention. This provides the queries taxonomic guidance to focus better on relevant categories. The model is trained with an additional taxonomy-aware matching loss. Extensive experiments on multiple VIS benchmarks including YouTube-VIS, OVIS and UVO show that TMT-VIS sets new state-of-the-art results, significantly outperforming prior VIS methods. Ablations validate the efficacy of the taxonomy compilation and injection modules. The consistent gains demonstrate the effectiveness of TMT-VIS in utilizing multiple datasets by incorporating semantic taxonomy guidance into the queries.


## Summarize the paper in one sentence.

 This paper proposes a taxonomy-aware multi-dataset joint training method (TMT-VIS) for video instance segmentation that effectively leverages multiple datasets by incorporating taxonomic guidance into the transformer decoder to help queries concentrate on desired categories.
