# [Bifurcated Attention for Single-Context Large-Batch Sampling](https://arxiv.org/abs/2403.08845)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Inference latency and efficiency is a major challenge for deploying large language models (LLMs) in real-world applications. A key bottleneck is the memory I/O during incremental decoding, especially with long input contexts and high batch sizes. 

- Specifically, loading the cached key-value (KV) pairs from the input context dominates memory I/O costs during incremental decoding. This significantly impacts latency when:
  (1) Generating multiple samples from a single long context (high batch size)
  (2) Handling very long input contexts, even for a single sample

Proposed Solution:
- Investigate generalized multi-query attention, which provides continuum between multi-head and multi-query attention based on the number of attention groups $g$. Lower $g$ compresses KV cache leading to faster inference but may require larger model size to match capabilities.

- Introduce "bifurcated attention", which splits attention during incremental decoding into two parts: (1) context attention (using shared KV cache) (2) decoding attention. This bifurcation reduces redundant memory I/O of context KV cache while maintaining identical results.

- Bifurcated attention works with any multi-group attention, including multi-query, and enables extreme batch sizes and context lengths not possible otherwise.

Main Contributions:
- Analysis of capabilities vs inference efficiency trade-offs with multi-group attention
- Novel "bifurcated attention" technique that reduces KV cache memory I/O during incremental decoding 
- Demonstration of multi-query + bifurcated attention supporting batch size up to 512 within same latency budget
- Quantification of accuracy improvements from increased batch size under fixed latency constraint in code generation

In summary, the paper introduces techniques to significantly lower inference latency bottlenecks, especially for long contexts and high batch sampling, enabling more practical deployment of large language models.
