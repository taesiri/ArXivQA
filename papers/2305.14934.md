# Discriminator-Guided Multi-step Reasoning with Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be:Discriminator-guided decoding can improve the multi-step reasoning abilities of language models by nudging them towards generating correct intermediate reasoning steps. The key ideas relevant to this hypothesis are:- Language models are often miscalibrated - they assign high probabilities to incorrect solutions. This leads greedy decoding to produce invalid reasoning chains.- Methods like self-consistency and verifiers operate on the level of complete solutions rather than individual steps. They do not directly address the miscalibration issue.- The proposed method GRACE uses a discriminator to score candidate next steps based on correctness. By integrating these correctness scores into the decoding process, it guides the model towards sampling more valid reasoning trajectories. - The discriminator provides finer-grained step-level control compared to previous methods. It is trained using a novel alignment algorithm and max-margin loss without requiring any human annotations.- Experiments on math reasoning datasets demonstrate GRACE improves answer accuracy and reasoning correctness over baselines like greedy decoding and self-consistency.In summary, the central hypothesis is that a step-level correctness discriminator can be used to guide decoding and enhance language models' multi-step reasoning abilities. The results support this hypothesis and highlight the benefits of integrating intermediate step information into the decoding process.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called GRACE (G uiding Multi-step R eas oning with a C orrectnE ss Di scriminator) for improving the multi-step reasoning abilities of language models. The key ideas are:- Introducing a discriminator model that is trained to score the correctness of each reasoning step. This allows guiding the decoding process towards generating valid steps.- Proposing a novel 3-step approach to train the discriminator without any step-level human annotations, by aligning and comparing model-generated solutions to reference solutions.- Integrating the discriminator in a stepwise decoding process to nudge the language model towards logical and mathematically sound reasoning chains.- Showing that GRACE significantly improves performance over greedy decoding and other baselines on multiple math reasoning datasets, in terms of both final answer accuracy and intermediate step correctness.- Demonstrating the applicability of GRACE to language models of different sizes and families like T5 and LLaMA without any fine-tuning.In summary, the key contribution is presenting a novel way to leverage a correctness discriminator to guide multi-step decoding and enhance reasoning, which does not require model re-training or human step annotations. The method is shown to boost reasoning performance over strong baselines across several benchmarks.
