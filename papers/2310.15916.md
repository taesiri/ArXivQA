# [In-Context Learning Creates Task Vectors](https://arxiv.org/abs/2310.15916)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper tries to address is: What is the underlying mechanism behind in-context learning in large language models? Specifically, the authors aim to understand if in-context learning can be framed in terms of learning parameters of functions in a hypothesis class, similar to standard statistical learning theory. The key hypothesis is that the model maps the demonstrations $S$ into a "task vector" $\thetav(S)$ that represents the task, and then applies a function $f(x;\thetav(S))$ to the query $x$ to produce the output, without direct dependence on $S$. Through experiments, the authors find evidence supporting this hypothesis and propose it as a simple mechanistic explanation of how in-context learning works in large language models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing and experimentally validating a hypothesis class view of in-context learning (ICL) in large language models (LLMs). Specifically:

- The paper argues that ICL can be seen as learning a "task vector" that represents the task demonstrated in the provided examples. This vector then modulates the LLM to perform the task on new inputs. 

- This provides a view of ICL as operating within a hypothesis class of functions parameterized by task vectors. This links ICL to the standard statistical learning framework.

- Comprehensive experiments are conducted on diverse LLMs and tasks to empirically validate this view. The key findings are:

  - The forward pass can be separated into distinct "learning" and "inference" components that generate and use the task vector respectively. This maintains strong ICL performance.
  
  - The task vectors are shown to be robust and capture meaningful information about the task. 
  
  - The task vector dominates the model's predictions even when conflicting information is present.

- Together, these experiments provide evidence for the hypothesis class view of ICL proposed in the paper.

In summary, the main contribution is providing a learning theory-grounded perspective on ICL in LLMs and extensive experiments that validate this mechanistic view. This sheds light on how LLMs perform ICL.
