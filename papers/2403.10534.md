# [VISREAS: Complex Visual Reasoning with Unanswerable Questions](https://arxiv.org/abs/2403.10534)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing visual question answering (VQA) datasets and models lack the ability to validate the authenticity and relevance of questions before providing an answer. This causes issues when models receive invalid or improperly formulated questions.
- Current VQA research focuses on answering questions about single objects and their attributes/relations, rather than reasoning about commonalities between multiple objects.
- Generative VQA models that produce reasoning programs rely heavily on external modules and APIs, limiting accessibility and end-to-end training.

Proposed Solution:
- Introduce VisReas, a new VQA dataset with over 2 million questions requiring complex reasoning about objects, attributes and relations. Questions are labeled as answerable or unanswerable based on alignment with the image.
- Propose Logic2Vision, a modular VQA model that uses a language model to produce pseudocode representing reasoning steps. The pseudocode is then executed on the image by a visual-language model to produce the answer.

Main Contributions:
- VisReas dataset that emphasizes multi-object spatial reasoning and validating question authenticity before answering.
- Logic2Vision model that performs interpretable step-by-step reasoning fully within a single visual-language model, without reliance on external modules.
- Analysis showing Logic2Vision outperforms state-of-the-art generative and classification VQA models on VisReas by a significant margin.

The key innovation is the focus on verifying question relevance and chaining logical reasoning steps on sets of objects and relations before answering, unlike prior VQA research. The introduced dataset and modular model aim to push progress on robust, semantically-grounded VQA.
