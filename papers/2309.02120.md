# [Multi-label affordance mapping from egocentric vision](https://arxiv.org/abs/2309.02120)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a multi-label, metric, and spatial-oriented perception of affordances from egocentric videos to enable detailed understanding and mapping of environments for robotic/assistive applications?The key aspects that the paper investigates are:- Developing a method to automatically extract grounded affordance labels from egocentric videos based on past interactions in a common 3D reference frame. This is used to build a large-scale affordance dataset (EPIC-Aff).- Adapting segmentation architectures to multi-label prediction to enable associating multiple affordances to the same region, providing richer understanding compared to single-label approaches. - Leveraging the metric affordance predictions and camera poses to build detailed spatial affordance maps of environments.- Demonstrating an application of the affordance maps for task-oriented navigation, where an agent can be guided to a location to perform a desired action based on learned affordances.So in summary, the central focus is on developing a complete pipeline for grounded multi-label affordance perception from videos and applying it for detailed spatial mapping and navigation tasks. The key hypothesis is that this representation can enable richer understanding compared to existing single-label or non-metric affordance methods.


## What is the main contribution of this paper?

This paper introduces a new approach for multi-label, metric affordance segmentation and mapping from egocentric videos. The main contributions are:- A method to automatically extract grounded affordance labels from videos by combining narrations, object masks, and camera poses. This is used to build EPIC-Aff, a large-scale affordance segmentation dataset based on EPIC-Kitchens.- Adapting segmentation models like UNet, FPN, and DeepLab v3 for multi-label prediction, where each pixel can have multiple affordance labels. This better captures the richness of affordances compared to single-label approaches. Experiments show DeepLab v3 with an asymmetric loss works best.- Demonstrating applications of the metric affordance maps for mapping activity-centric zones in an environment over time and for goal-directed navigation based on desired actions.In summary, the key innovation is a grounded approach to predict dense multi-label affordance maps from egocentric videos. This enables detailed understanding and reasoning about object affordances and interactions for tasks like mapping and navigation. The automatic labeling method also allows scaling up affordance datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new approach to affordance perception that enables accurate multi-label segmentation by automatically extracting grounded affordances from first person videos using a 3D map, and demonstrates applications like building spatial maps of affordances and task-oriented navigation.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of affordance perception and mapping:- This paper presents a new approach for grounded, metric, multi-label affordance perception. Most prior work has focused on single-label affordance segmentation or classification. Modeling multiple affordance labels per pixel/region is more aligned with the theory of affordances and captures the richness of possibilities better. - The paper introduces a method to automatically extract affordance labels from videos by combining narrations and object annotations. This allows creating a large-scale affordance dataset (EPIC-Aff) without costly manual labeling. Most prior datasets are either manually annotated or learn from observing human demonstrations.- The extracted affordance labels are metric and spatial, linked to camera poses and a 3D map. This allows building detailed affordance maps and enables applications like task-oriented navigation. Other works have focused more on topological affordance graphs or semantic classification. - Architecturally, the paper explores adapting standard segmentation networks to multi-label prediction using strategies like per-pixel binary classification. This is a relatively under-explored area compared to single-label segmentation.- For evaluation, the paper uses metrics common in affordance and saliency literature like SIM, KLD, AUC-J as well as segmentation metrics like mIoU. The experiments provide useful insights into multi-label affordance segmentation.Overall, I think the paper makes good contributions in terms of the multi-label affordance formulation, the automatic labeling pipeline, linking affordances to metric maps, and architectural explorations. The affordance mapping and navigation applications are also interesting. The paper pushes forward the state of the art in grounded affordance perception.
