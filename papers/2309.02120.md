# [Multi-label affordance mapping from egocentric vision](https://arxiv.org/abs/2309.02120)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a multi-label, metric, and spatial-oriented perception of affordances from egocentric videos to enable detailed understanding and mapping of environments for robotic/assistive applications?The key aspects that the paper investigates are:- Developing a method to automatically extract grounded affordance labels from egocentric videos based on past interactions in a common 3D reference frame. This is used to build a large-scale affordance dataset (EPIC-Aff).- Adapting segmentation architectures to multi-label prediction to enable associating multiple affordances to the same region, providing richer understanding compared to single-label approaches. - Leveraging the metric affordance predictions and camera poses to build detailed spatial affordance maps of environments.- Demonstrating an application of the affordance maps for task-oriented navigation, where an agent can be guided to a location to perform a desired action based on learned affordances.So in summary, the central focus is on developing a complete pipeline for grounded multi-label affordance perception from videos and applying it for detailed spatial mapping and navigation tasks. The key hypothesis is that this representation can enable richer understanding compared to existing single-label or non-metric affordance methods.


## What is the main contribution of this paper?

This paper introduces a new approach for multi-label, metric affordance segmentation and mapping from egocentric videos. The main contributions are:- A method to automatically extract grounded affordance labels from videos by combining narrations, object masks, and camera poses. This is used to build EPIC-Aff, a large-scale affordance segmentation dataset based on EPIC-Kitchens.- Adapting segmentation models like UNet, FPN, and DeepLab v3 for multi-label prediction, where each pixel can have multiple affordance labels. This better captures the richness of affordances compared to single-label approaches. Experiments show DeepLab v3 with an asymmetric loss works best.- Demonstrating applications of the metric affordance maps for mapping activity-centric zones in an environment over time and for goal-directed navigation based on desired actions.In summary, the key innovation is a grounded approach to predict dense multi-label affordance maps from egocentric videos. This enables detailed understanding and reasoning about object affordances and interactions for tasks like mapping and navigation. The automatic labeling method also allows scaling up affordance datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new approach to affordance perception that enables accurate multi-label segmentation by automatically extracting grounded affordances from first person videos using a 3D map, and demonstrates applications like building spatial maps of affordances and task-oriented navigation.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of affordance perception and mapping:- This paper presents a new approach for grounded, metric, multi-label affordance perception. Most prior work has focused on single-label affordance segmentation or classification. Modeling multiple affordance labels per pixel/region is more aligned with the theory of affordances and captures the richness of possibilities better. - The paper introduces a method to automatically extract affordance labels from videos by combining narrations and object annotations. This allows creating a large-scale affordance dataset (EPIC-Aff) without costly manual labeling. Most prior datasets are either manually annotated or learn from observing human demonstrations.- The extracted affordance labels are metric and spatial, linked to camera poses and a 3D map. This allows building detailed affordance maps and enables applications like task-oriented navigation. Other works have focused more on topological affordance graphs or semantic classification. - Architecturally, the paper explores adapting standard segmentation networks to multi-label prediction using strategies like per-pixel binary classification. This is a relatively under-explored area compared to single-label segmentation.- For evaluation, the paper uses metrics common in affordance and saliency literature like SIM, KLD, AUC-J as well as segmentation metrics like mIoU. The experiments provide useful insights into multi-label affordance segmentation.Overall, I think the paper makes good contributions in terms of the multi-label affordance formulation, the automatic labeling pipeline, linking affordances to metric maps, and architectural explorations. The affordance mapping and navigation applications are also interesting. The paper pushes forward the state of the art in grounded affordance perception.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Extending the dataset to other environments beyond just kitchen scenes. The authors note their automatic labeling pipeline could be easily adapted to other scenarios to reduce dataset bias.- Using more robust SLAM systems like ORB-SLAM rather than just COLMAP to improve the accuracy of the camera pose estimation and mapping in real-time scenarios.- Incorporating detection models for grasping points to improve the assumptions made during the automatic affordance labeling process.- Exploring different network architectures and loss functions for the multi-label segmentation task. The authors mainly evaluated modifications of existing segmentation networks.- Applying the approach to other tasks like anticipating future actions, manipulating objects, or human-robot interaction. The affordance maps could provide useful semantic information.- Evaluating on robotic systems to demonstrate real-world applications like assistive devices or robotic manipulation. The current work focuses on analysis and simulation.- Extending the affordance representation to include more complex attributes beyond just discrete actions. The authors suggest their approach could complement more intricate affordance models.- Improving the task-oriented navigation by incorporating more sophisticated planning algorithms. The current proof-of-concept uses A*.In summary, the main future directions are around improving the technical components of the pipeline, expanding the datasets, exploring new applications for the affordance maps, and demonstrating the approach on physical robotic systems. The authors lay out their method as an initial prototype that could enable lots of follow-on research.


## Summarize the paper in one paragraph.

The paper proposes a novel approach for multi-label, metric, and spatial-oriented perception of affordances. They introduce a method to automatically extract grounded affordance labels from egocentric interaction videos by mapping all past interactions to a common 3D representation. Using this, they build EPIC-Aff, a large dataset of affordance segmentation masks grounded in real human interactions. They motivate the need for multi-label affordance segmentation to capture the diverse possibilities offered by objects and environments. To enable this, they adapt several segmentation architectures to output multiple labels per pixel. Their metric spatial representation enables applications like building detailed affordance maps and task-oriented navigation. Overall, this work presents a complete pipeline for extracting, learning, and applying grounded multi-label affordance perception in a spatial context. The experiments on EPIC-Aff and applications like mapping and planning demonstrate the utility of their approach.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper presents a new approach for multi-label, metric, and spatial-oriented perception of affordances. The authors first introduce a method to automatically extract grounded affordance labels from egocentric interaction videos. They use narrations and semantic masks to determine interaction points between hands and objects. These interaction points are projected into a common 3D coordinate system using camera poses estimated with COLMAP. By accumulating these projected affordance points over multiple videos in an environment, they create a spatial distribution of grounded affordances. This process is used to build a large affordance dataset called EPIC-Aff based on the EPIC-Kitchens dataset. The authors then present methods to perform multi-label affordance segmentation on images, allowing multiple affordance labels to be predicted per pixel. This is done by modifying standard segmentation architectures to output multiple binary predictions per class rather than a single multiclass prediction. Quantitative experiments show multi-label segmentation performs much better than heuristics to extract multiple labels from a multiclass prediction. Finally, the affordance predictions are projected back into the 3D coordinate system to build detailed spatial affordance maps. As a sample application, these maps are used to perform task-oriented navigation by planning paths to locations that afford desired actions.


## Summarize the main method used in the paper in one paragraph.

The paper presents a method for extracting grounded multi-label affordance annotations from egocentric videos. The key steps are:- Use EPIC-100 narrations and VISOR object masks to identify interaction points between hands and objects in sparse video frames. - Estimate camera poses with COLMAP Structure-from-Motion and project the interaction points into a common 3D map. - Accumulate affordance labels from past interactions and reproject them to label new frames, filtering by visible objects. - Train multi-label segmentation models like DeepLab v3 with an Asymmetric loss to predict pixel-wise affordance masks.- Map affordances in 3D and use for task-oriented navigation by planning paths to affordance locations.Overall, the method enables automatic extraction of metric, multi-label affordance annotations from videos and their use for detailed semantic mapping and navigation tasks. The key novelty is the grounded multi-label affordance perception which captures richer semantics.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- The paper is addressing the problem of affordance perception and segmentation from egocentric/first-person videos. Affordances refer to the possible actions that can be taken on objects or in the environment. - Current approaches to affordance perception have limitations: they assume single label per object/pixel, lack metric/spatial understanding, and rely on full image classification losing pixel precision. - The paper proposes a new approach for grounded, interaction-based, multi-label, metric affordance perception and segmentation. - They introduce a method to automatically extract affordance labels from real-world interaction videos using narration, object segmentation, and 3D reconstruction of interaction points. This allows collecting grounded, metric, multi-label affordance data.- Using this, they build a large-scale affordance dataset called EPIC-Aff based on EPIC-Kitchens videos. This is claimed to be the most complete and largest affordance dataset.- They adapt semantic segmentation architectures for multi-label prediction to retain pixel-level affordance understanding.- The metric affordance maps are used for mapping activity-centric zones in the environment and task-oriented navigation.In summary, the key focus is on developing a grounded, metric, multi-label affordance perception approach and applying it for segmentation and mapping tasks. The automatic labeling method and EPIC-Aff dataset are enabling contributions.
