# [LoL: A Comparative Regularization Loss over Query Reformulation Losses   for Pseudo-Relevance Feedback](https://arxiv.org/abs/2204.11545)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we enhance the ability of pseudo-relevance feedback (PRF) models to suppress irrelevant information when reformulating queries, so as to alleviate query drift caused by using more feedback documents?The key hypotheses proposed in the paper are:1) Currently PRF models are optimized independently for queries revised using different amounts of feedback, ignoring the principle that more feedback should lead to better revisions. 2) Adding a comparative regularization loss that penalizes revisions using more feedback but obtaining larger reformulation losses can teach the model to suppress irrelevant information by comparing different revisions.3) This proposed Loss-over-Loss (LoL) framework is widely applicable for different PRF models and retrieval paradigms.The experiments then aim to validate whether the proposed LoL method can improve PRF effectiveness and robustness by enhancing the model's ability to ignore irrelevant information when more feedback documents are used.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel Loss-over-Loss (LoL) framework with comparative regularization for pseudo-relevance feedback in information retrieval. Specifically:- It points out a comparison principle that an ideal PRF model should guarantee: the more feedback documents, the better the reformulated query. This principle is often neglected by existing PRF methods. - It proposes a comparative regularization term to constrain the reformulation losses of multiple revisions derived from the same original query. This regularization pursues the above comparison principle by penalizing revisions that use more feedback but obtain larger losses.- It presents a simple implementation of the framework with a differentiable query reformulation method. This method operates in the vector space to directly optimize ranking performance, applicable for both sparse and dense retrieval models.- Experimental results on MS MARCO benchmarks show the effectiveness of LoL models over base retrieval models and other PRF baselines. The comparative regularization is proven to play a critical role through ablation studies and analysis.In summary, the key contribution is proposing the Loss-over-Loss framework with comparative regularization to enhance PRF models' ability to suppress irrelevant information by comparing parallel revisions of the same query. This helps alleviate the query drift problem in pseudo-relevance feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point of this paper:The paper proposes a Loss-over-Loss framework with comparative regularization for pseudo-relevance feedback models to learn to suppress irrelevant information in more feedback documents and generate better reformulated queries.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of pseudo-relevance feedback for information retrieval:- This paper proposes a new framework called Loss-over-Loss (LoL) for training pseudo-relevance feedback models. The key novelty is the use of a comparative regularization loss to ensure the model learns to produce better query reformulations when given more feedback documents. This addresses the issue of query drift in a novel way compared to prior methods.- Most prior work has focused on adding preprocessing or postprocessing steps to deal with query drift, selecting better feedback documents, or fusing results from the original and expanded queries. In contrast, LoL provides a new way to directly supervise the model during training to handle increasing amounts of feedback.- LoL is model-agnostic and can work with both sparse and dense retrievers, unlike some prior PRF methods tailored for one type of retrieval. The authors demonstrate LoL improves both a lexical sparse retriever and dense neural retriever.- The only other end-to-end trainable PRF model for dense retrieval is ANCE-PRF. But it trains separate models per feedback depth and does not have LoL's comparative regularization. LoL outperforms ANCE-PRF in the authors' experiments.- For sparse retrieval, LoL is compared to classic heuristics like RM3 and Rocchio as well as a strong lexical matching model. The gains from LoL demonstrate PRF can further improve state-of-the-art sparse retrievers.In summary, LoL provides a novel training framework for PRF models that helps address query drift in a principled way. It advances the state-of-the-art for both sparse and dense retrieval compared to prior PRF techniques. The comparative regularization approach seems promising for future exploration.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Exploring the application of other learning-to-rank losses for the comparative regularization term in the Loss-over-Loss (LoL) framework. The authors mention that the current comparative regularization aims to guarantee an unsupervised normal order of objects, and can be seen as an application of learning-to-rank. Therefore, they suggest exploring other learning-to-rank losses in place of the pairwise hinge loss currently used.2. Replacing the reformulation loss function used to map queries to differentiable values. The reformulation loss currently uses a ranking loss, but the authors suggest exploring other differentiable loss functions that could replace this mapping.3. Applying comparative regularization techniques to other tasks that have neglected normal orders that should be maintained. The authors suggest the comparative regularization idea could be applicable beyond just query reformulation for pseudo-relevance feedback.4. Exploring differentiable query reformulation methods for other retrieval frameworks beyond sparse and dense retrieval explored in this paper. The current method focuses on single-representation retrieval models.In summary, the main directions are exploring other loss functions for the comparative regularization, replacing the reformulation loss function, applying comparative regularization to other tasks, and exploring differentiable query reformulation for other retrieval models.
