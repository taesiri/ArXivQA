# [WESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for   Whisper-based Speech Interactions](https://arxiv.org/abs/2303.1639)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to develop a zero-shot, real-time whisper-to-normal speech conversion method that does not require paired whisper/normal speech data or speaker-dependent training.The key hypothesis appears to be that a self-supervised speech encoder-decoder model can learn to convert whispered speech to normal speech in a speaker-independent manner without needing paired training data, by learning to extract common speech units from both whispered and normal speech.In summary, the main research goals are:- Develop a real-time whisper-to-normal speech conversion system- Achieve speaker-independent conversion without user-specific training - Eliminate the need for paired whisper/normal speech training data- Use self-supervised pre-training to extract shared speech units from unpaired whisper and normal speechThe proposed WESPER model aims to address these challenges.


## What is the main contribution of this paper?

The main contribution of this paper is proposing WESPER, a real-time and zero-shot whisper-to-normal voice conversion method based on self-supervised learning. The key points are:- WESPER can convert whispered speech to normal speech in a speaker-independent manner without needing parallel whispered-normal speech datasets. It is based on a speech-to-unit (STU) encoder and unit-to-speech (UTS) decoder architecture.- The STU encoder is pre-trained on unlabeled whispered and normal speech to learn common speech units for both. This allows converting whispered speech to these common units. - The UTS decoder can be trained on any target speaker's speech to reconstruct speech from the common units in their voice. No text labels are needed.- This zero-shot whisper-to-normal conversion improves the quality and intelligibility of whispered speech while retaining natural prosody.- WESPER is evaluated on both normal whispered speech and speech of people with disorders. It shows improved quality and preserved prosody in both cases.- The conversion happens in real-time, enabling applications like whisper-based speech interactions.In summary, the key contribution is proposing a self-supervised method to perform zero-shot, real-time, and speaker-independent whisper-to-normal speech conversion to improve quality and intelligibility while retaining prosody.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes WESPER, a real-time whisper-to-speech conversion method using self-supervised learning, which can improve the quality and intelligibility of whispered speech without requiring parallel whisper-normal speech data.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research on whisper-to-normal speech conversion:- The proposed method WESPER requires only unpaired and unlabeled whisper and normal speech data for training, making it more flexible and easier to train compared to methods that require paired data like Parrotron, MSpeC-Net, and AGAN-W2SC. - WESPER operates in a zero-shot manner without needing per-user training or adaptation, unlike most other whisper-to-speech methods that are speaker-dependent. This makes it more practical for real-world usage.- The self-supervised speech encoder used in WESPER is pre-trained to generate a common speech representation for both whisper and normal speech. This is a novel approach not explored in other whisper-to-speech methods.- WESPER performs conversion in real-time, which is an advantage over non-realtime neural vocoder methods like WaveNet.- The evaluations show WESPER can improve speech quality and intelligibility for both normal whispered speech and disordered speech. Comparisons to other methods are lacking, but the results seem promising.- WESPER is designed for integration into practical systems and devices, with prototypes for teleconferencing and accessibility applications. Most other work focuses only on core algorithm development.In summary, the key innovations of WESPER compared to prior art seem to be the zero-shot capability, use of self-supervised pre-training to handle whisper/normal mismatch, and real-time low-latency conversion. The practical system integration is also a differentiator from most academic research. However, direct comparisons to competing methods are needed to better assess the performance of WESPER.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Combining WESPER with user-dependent fine-tuning: The authors suggest investigating if the conversion performance, especially for speakers with hearing impairments, can be improved by applying small-scale fine-tuning to each speaker's data. This would still not require text transcriptions, only paired whispered and normal speech samples from each user.- Exploring suitable audio input devices for whispered speech: The authors mention testing different microphones like headsets, directional arrays, and non-audible murmur microphones, as well as combining with noise reduction techniques. Finding an optimal configuration for detecting whispered speech is an important direction. - Investigating human-AI integration: The authors suggest that users adapt their whispering based on the machine's capabilities, demonstrating human-AI collaboration. Further exploring this synergistic learning is proposed as an interesting direction.- Evaluating language independence: Since the model uses self-supervised pretraining, the authors suggest evaluating if WESPER can perform well even for non-English languages it was not directly trained on.- Testing conversion quality and prosody preservation on more speaker types: The authors recommend additional experiments with more types of atypical voices beyond those studied.In summary, the main future directions focus on improving WESPER's performance, especially for individual speakers, finding optimal hardware configurations, studying human-AI collaboration, and expanding the scope of speakers, languages and application scenarios tested.
