# [TaskExpert: Dynamically Assembling Multi-Task Representations with   Memorial Mixture-of-Experts](https://arxiv.org/abs/2307.15324)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing a novel multi-task mixture-of-experts model called "TaskExpert" that can dynamically decode discriminative task-specific representations for multiple distinct tasks. The key ideas and components of TaskExpert include:- Using a set of expert networks to decompose the shared backbone feature into multiple representative task-generic feature spaces. This allows more fine-grained decoding of task-specific features.- Designing task-specific and context-aware gating networks to dynamically assemble the features from different experts based on the input sample, for generating the task-specific representations. This enables more discriminative decoding compared to static parameter sharing. - Introducing a multi-task feature memory that interacts with the experts across layers. It aggregates long-term task-specific representations and serves as an additional expert for dynamic decoding. - Embedding the proposed "Memorial Mixture-of-Experts" (MMoE) modules at different backbone layers to achieve multi-scale dynamic decoding of task features.Overall, the central hypothesis is that the proposed techniques of feature decomposition, dynamic feature assembling, and long-range multi-task modeling with the feature memory, will allow TaskExpert to learn more discriminative task-specific representations and improve multi-task prediction performance, compared to previous approaches with static decoder designs. The paper aims to demonstrate the effectiveness of TaskExpert through extensive experiments and comparisons.


## What is the main contribution of this paper?

This paper presents TaskExpert, a novel multi-task learning framework for dynamically assembling discriminative task-specific representations. The main contributions are:1. It proposes a Memorial Mixture-of-Experts (MMoE) module that can decompose a shared backbone feature into multiple representative task-generic features using expert networks. These features are then dynamically assembled into task-specific representations based on sample-dependent and task-specific gating scores. 2. It designs a multi-task feature memory that interacts with the MMoE modules across layers to enable long-range modeling of task-specific features. The memory serves as an additional feature expert for decoding and is updated by the decoded task features in each layer.3. Extensive experiments show TaskExpert achieves new state-of-the-art performance on two multi-task vision benchmarks, outperforming previous methods on all evaluation metrics. The results demonstrate the effectiveness of dynamic task-specific feature decoding and long-range modeling with the multi-task memory.In summary, the main contribution is proposing an effective multi-task mixture-of-experts framework to dynamically decode discriminative task-specific representations throughout the network using memorial mixture-of-experts. This leads to superior multi-task learning performance.
