# [Successor Heads: Recurring, Interpretable Attention Heads In The Wild](https://arxiv.org/abs/2312.09230)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- There is little understanding of how large language models (LLMs) produce their impressive text outputs. The field of mechanistic interpretability aims to reverse-engineer these models to understand their internal algorithms. 
- Prior work has struggled to find recurring, interpretable components in large models. The universality hypothesis states that models of different sizes/architectures learn similar internal representations, but there is little evidence for or against this in LLMs.

Key Contributions:
1) The paper introduces "successor heads" - attention heads that increment tokens with a natural ordering like numbers, weekdays, etc. They demonstrate successor heads arise in models from 31 million to 12 billion parameters, across different architectures.

2) They find successor heads rely on an abstract numeric subspace in the token embeddings that encodes the position of tokens in an ordinal sequence. This exhibits compositional structure with "mod 10 features" that represent the index modulo 10.

3) Vector arithmetic can directly manipulate the mod 10 features to alter successor head behavior and numeric representations. This provides evidence these features are casually important.

4) Analyzing model predictions shows successor heads play an important role incrementing tokens in natural language. They also serve other functions like generating acronyms, demonstrating "interpretable polysemanticity". 

5) Results provide evidence for a weak form of the universality hypothesis, as successor heads and their underlying numeric features arise robustly across very different model scales/architectures. The paper gives insights into numeric representations in LLMs.
