# [Successor Heads: Recurring, Interpretable Attention Heads In The Wild](https://arxiv.org/abs/2312.09230)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- There is little understanding of how large language models (LLMs) produce their impressive text outputs. The field of mechanistic interpretability aims to reverse-engineer these models to understand their internal algorithms. 
- Prior work has struggled to find recurring, interpretable components in large models. The universality hypothesis states that models of different sizes/architectures learn similar internal representations, but there is little evidence for or against this in LLMs.

Key Contributions:
1) The paper introduces "successor heads" - attention heads that increment tokens with a natural ordering like numbers, weekdays, etc. They demonstrate successor heads arise in models from 31 million to 12 billion parameters, across different architectures.

2) They find successor heads rely on an abstract numeric subspace in the token embeddings that encodes the position of tokens in an ordinal sequence. This exhibits compositional structure with "mod 10 features" that represent the index modulo 10.

3) Vector arithmetic can directly manipulate the mod 10 features to alter successor head behavior and numeric representations. This provides evidence these features are casually important.

4) Analyzing model predictions shows successor heads play an important role incrementing tokens in natural language. They also serve other functions like generating acronyms, demonstrating "interpretable polysemanticity". 

5) Results provide evidence for a weak form of the universality hypothesis, as successor heads and their underlying numeric features arise robustly across very different model scales/architectures. The paper gives insights into numeric representations in LLMs.


## Summarize the paper in one sentence.

 This paper introduces and analyzes "successor heads", a type of attention head in language models that increment tokens with a natural ordering, such as numbers, months, and days, using a shared set of abstract numeric representations that exhibit compositional structure.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Introducing and interpreting successor heads - attention heads in language models that perform incrementation on ordinal sequences like numbers, months, days, etc. The paper shows these heads arise across a range of model sizes and architectures.

2) Finding evidence for abstract numeric representations and interpretable "mod-10 features" underlying how successor heads increment tokens in language models. These features exhibit some compositional structure and transferability. 

3) Demonstrating the importance of successor heads for incrementation-based tasks on natural language data, such as predicting the next number in a numbered list. 

4) Highlighting the "interpretable polysemanticity" of successor heads, as they perform multiple distinct functions like incrementation and predicting acronyms. This is a novel example of attention head polysemanticity in a large language model.

In summary, the main contribution is discovering and interpreting successor heads across language models, providing evidence for abstract numeric representations, and highlighting the polysemantic nature and importance of these heads for natural language tasks involving incrementation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Successor heads - Attention heads in language models that perform incrementation on tokens with a natural ordering, like numbers, months, days.

- Mechanistic interpretability - The field aimed at reverse-engineering and explaining how neural networks like language models actually complete tasks. 

- Universality hypothesis - The idea that neural networks form common internal representations across different architectures and scales. The paper provides evidence for a weak form of universality in successor heads.

- Compositionality - The paper shows numeric representations in language models are compositional, with separate components encoding a token's position in an ordinal sequence vs which sequence it is part of. 

- Mod-10 features - Interpretable features the paper extracts that encode the modulo 10 value of a token's position in an ordinal sequence. These features exhibit compositional structure and transfer across tasks.

- Vector arithmetic - Manipulating representations by adding and subtracting mod-10 features to steer model behavior.

- Polysemanticity - Attention heads performing multiple distinct, interpretable functions. The paper shows the successor head also handles acronym-related tasks.

- Numbered listing - A widespread language task requiring incrementation that the paper shows relies strongly on successor heads.

So in summary, key terms cover successor heads, interpretability, universality, compositionality, vector arithmetic, and numbered listing.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces the concept of "successor heads" - attention heads in language models that perform incrementation on tokens with a natural ordering. How prevalent are these heads across different model architectures and sizes? Do they emerge consistently, or is there variability?

2. The paper argues that successor heads provide evidence for a weak form of universality in language models. What specifically do the authors mean by "universality" and "weak universality" in this context? How compelling is the evidence?

3. The method relies on an "effective OV circuit" to measure the direct effect of input tokens when multiplied by an OV matrix. What is this circuit conceptually and why is it more suitable than the standard OV circuit?

4. The paper finds evidence for abstract "mod-10" features underlying how successor heads increment tokens. What exactly are these features? How were they derived? And how transferable are they across tasks and models?  

5. How is the concept of "interpretably polysemantic" attention heads introduced and demonstrated with successor heads? What does this tell us about the functions and capabilities of attention heads?

6. Vector arithmetic is performed with the mod-10 features to manipulate and steer model behavior. How successful is this approach? What are some of its limitations?

7. The paper analyzes successor head behavior on natural language data, identifying four distinct categories. What are these categories of behavior? What proportions do they represent?

8. A "greater-than" bias is observed where models prefer numeric answers larger than prompt values over smaller ones. Where does this bias originate from and why isn't it captured in the mod-10 features?

9. The paper shows successor heads play an important role in incrementation-based tasks in training data. What specifically was analyzed? And what implications does this have?

10. The method relies heavily on probing techniques like sparse autoencoders. What implicit assumptions are being made about model representations and what are potential limitations of this approach?
