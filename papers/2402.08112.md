# [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Deep reinforcement learning (DRL) has shown promise in complex video games like StarCraft II. However, DRL agents have not been competitive in the IEEE microRTS game competitions, which feature a simplified real-time strategy game. Prior competition winners have predominantly been scripted agents. Training a competitive DRL agent for microRTS is challenging due to the large action space, need to make tactical and strategic decisions, real-time nature, and sparse delayed rewards. Additionally, considerable training resources are required, making adoption difficult in the academic competition setting.  

Proposed Solution:
The paper proposes the first DRL agent, called \agentName, to win the IEEE microRTS competition. \agentName consists of 7 neural network policies fine-tuned for different maps through an iterative training process involving self-play, playing against prior winners, transfer learning, and reward scheduling. This allows \agentName to defeat the 2017-2021 microRTS competition winners in over 96% of matches on 7 of the 8 competition maps.

The paper also shows promise for an alternate training methodology using behavior cloning to bootstrap policies on demonstrated expert gameplay before fine-tuning with DRL. This is more efficient and eliminates the need for handcrafted rewards. A behavior cloned agent finetuned with PPO (\bcPPOAgent) achieved comparable performance to \agentName without needing map-specific policies.

Main Contributions:
- First DRL agent to win IEEE microRTS competition
- Demonstrates iterative fine-tuning and transfer learning as effective strategies for developing competitive DRL agents
- Proposes behavior cloning to bootstrap DRL training, enabling the use of only sparse rewards  
- Provides model architectures, training methodology and benchmarks as a starting point for future microRTS DRL research
- Highlights techniques like transfer learning and imitation learning to make complex DRL problems more feasible for academic research
