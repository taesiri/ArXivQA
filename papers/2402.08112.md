# [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Deep reinforcement learning (DRL) has shown promise in complex video games like StarCraft II. However, DRL agents have not been competitive in the IEEE microRTS game competitions, which feature a simplified real-time strategy game. Prior competition winners have predominantly been scripted agents. Training a competitive DRL agent for microRTS is challenging due to the large action space, need to make tactical and strategic decisions, real-time nature, and sparse delayed rewards. Additionally, considerable training resources are required, making adoption difficult in the academic competition setting.  

Proposed Solution:
The paper proposes the first DRL agent, called \agentName, to win the IEEE microRTS competition. \agentName consists of 7 neural network policies fine-tuned for different maps through an iterative training process involving self-play, playing against prior winners, transfer learning, and reward scheduling. This allows \agentName to defeat the 2017-2021 microRTS competition winners in over 96% of matches on 7 of the 8 competition maps.

The paper also shows promise for an alternate training methodology using behavior cloning to bootstrap policies on demonstrated expert gameplay before fine-tuning with DRL. This is more efficient and eliminates the need for handcrafted rewards. A behavior cloned agent finetuned with PPO (\bcPPOAgent) achieved comparable performance to \agentName without needing map-specific policies.

Main Contributions:
- First DRL agent to win IEEE microRTS competition
- Demonstrates iterative fine-tuning and transfer learning as effective strategies for developing competitive DRL agents
- Proposes behavior cloning to bootstrap DRL training, enabling the use of only sparse rewards  
- Provides model architectures, training methodology and benchmarks as a starting point for future microRTS DRL research
- Highlights techniques like transfer learning and imitation learning to make complex DRL problems more feasible for academic research


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper describes how the first deep reinforcement learning agent to win the IEEE microRTS competition was trained through iterative fine-tuning of a base policy and transfer learning to specific maps.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

1) \agentName is the first deep reinforcement learning (DRL) agent to win the IEEE microRTS competition. Prior to this, scripted agents had won the competition in all previous years. This demonstrates that DRL is now competitive in this domain.

2) The paper provides details on how \agentName was trained, including using iterative fine-tuning of a base policy, transfer learning to specific maps, and mixing shaped and sparse rewards over training. These strategies can serve as a starting point for other researchers looking to train DRL agents efficiently for microRTS and similar domains. 

3) In follow-up work, the authors show promise for using imitation learning (behavior cloning) followed by fine-tuning with DRL as an efficient way to bootstrap a competitive agent. This can avoid the need for handcrafted shaped rewards.

In summary, the main contribution is being the first DRL agent to achieve human-level performance in microRTS, while providing training details and strategies to facilitate further research. The results also showcase the potential of imitation learning to efficiently train competitive game-playing agents.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Deep reinforcement learning (DRL)
- Real-time strategy (RTS) games
- microRTS
- IEEE microRTS competition
- Policy networks
- Neural network architectures (DoubleCone, squnet)
- Transfer learning
- Behavior cloning
- Imitation learning
- Proximal Policy Optimization (PPO)
- Self-play
- Action masking
- Reward shaping

The paper describes an agent called \agentName that is the first DRL agent to win the IEEE microRTS competition. Key elements of the approach include using multiple policy networks, transfer learning to adapt models to new maps, iteratively fine-tuning models, and strategies like behavior cloning and imitation learning to bootstrap DRL training. The paper also discusses the neural network architectures, training methodology, and incorporating techniques like invalid action masking and shaped rewards.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper mentions that transfer learning to specific maps was critical to the agent's winning performance. What strategies were used for transfer learning and how did they improve performance over the base model?

2. Iterative fine-tuning of the base policy is noted as another key strategy. What was the process for the 3 phases of fine-tuning mentioned, and how did each phase improve the agent's capabilities? 

3. What were the key differences in network architecture between the DoubleCone model and the squnet model? Why was squnet better suited for larger maps despite being a "weaker" model?

4. The paper utilizes a multi-headed value network with 3 different value functions. What are these 3 value functions and what is the rationale behind using multiple heads? 

5. Self-play is used initially but then bots are introduced partway through training. What was the reasoning behind this shift in training methodology?

6. The PPO loss function has several adjustable hyperparameters mentioned like value loss coefficient and entropy coefficient. How were these coefficients scheduled and annealed during training?

7. What specific improvements were made to the action space representation compared to prior work in MicroRTS-Py? How did these impact training and gameplay?

8. Why use a GAE estimator for the advantage function instead of a monte-carlo estimator? What benefits does GAE provide?

9. The paper mentions faster inference runtimes would improve competition performance. What specific runtime providers and optimizations are suggested? 

10. Behavior cloning is proposed to bootstrap DRL agents. How was the clone dataset generated? What advantages and disadvantages exist versus learning only through self-play?
