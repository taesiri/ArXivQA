# [Application-Agnostic Language Modeling for On-Device ASR](https://arxiv.org/abs/2305.09764)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:How can we build a single neural network language model (NNLM) that serves both a virtual assistant (VA) application and a speech-to-text (STT) application equally well, while reducing model size compared to separate application-specific models?The key points are:- The paper aims to build a single NNLM that can serve two speech applications: a virtual assistant and speech-to-text. - Typically, separate application-specific models would be built for each application. However, this increases model size. - The goal is to build a single model that maintains the accuracy of the separate models, while reducing the overall model size.- The paper explores both data-driven and model architecture approaches to build this unified application-agnostic model.So in summary, the main research contribution is developing methods to train a single on-device NNLM that serves multiple applications as accurately as individual models but with a reduced memory footprint.


## What is the main contribution of this paper?

The key contributions of this paper are:1. A method to optimally combine application-specific data sources to train an application-agnostic language model that can serve multiple applications (virtual assistant and speech-to-text). This involves creating a balanced dev set and determining sampling weights for the data sources.2. Two novel feedforward neural network language model architectures based on Fixed-size Ordinally-Forgetting Encoding (FOFE) that can effectively serve multiple applications in an on-device automatic speech recognition system:- Application-Dependent (AD) FOFE NNLM which has separate sub-networks for each application that are selectively used at train/test time.- Mixture FOFE NNLM composed of an ensemble of parallel FOFE sub-networks whose outputs are combined via learned mixture weights.3. Experiments on 3 languages (English, German, Mandarin) showing that both proposed models can match/outperform application-specific models in accuracy while reducing disk size by ~50%. The AD-FOFE also matches/outperforms them in speed.4. Comparisons to transformer NNLM showing transformer does not consistently outperform FOFE models likely due to short context length in ASR.In summary, the key idea is using data balancing and novel feedforward NNLM architectures to effectively serve multiple applications in on-device ASR under tight efficiency constraints. The AD-FOFE model in particular provides gains in accuracy, speed and disk size.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes two novel feed-forward neural network language model architectures - an Application-Dependent FOFE and an Application-Agnostic Mixture FOFE - that are able to match the accuracy of application-specific models while reducing disk size by half, for an on-device automatic speech recognition system serving both a virtual assistant and speech-to-text.
