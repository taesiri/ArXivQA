# [Contrast Is All You Need](https://arxiv.org/abs/2307.02882)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does finetuning a pretrained language model like LegalBERT with a contrastive learning objective compare to standard finetuning on a legal text classification task, especially in low-resource scenarios?The key points are:- The authors compare two finetuning approaches on the LegalBERT model - using a contrastive learning setup (SetFit) versus standard finetuning. - They evaluate on a legal provision classification task using the LEDGAR dataset.- There is a focus on low-resource scenarios where labeled legal data is small and imbalanced. The contrastive learning approach is hypothesized to help in such cases.- The models are compared on both the original imbalanced LEDGAR data and a balanced version. Evaluation metrics and extracted model features using LIME are analyzed.- The main research question is whether the contrastive learning finetuning performs better than standard finetuning on this legal classification task, especially when labeled data is scarce. The results generally show improved performance with the contrastive approach.In summary, the central hypothesis is that a contrastive learning objective can outperform standard finetuning for legal text classification when training data is imbalanced and low-resource. The experiments aim to test this hypothesis.
