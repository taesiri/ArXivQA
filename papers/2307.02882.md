# [Contrast Is All You Need](https://arxiv.org/abs/2307.02882)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does finetuning a pretrained language model like LegalBERT with a contrastive learning objective compare to standard finetuning on a legal text classification task, especially in low-resource scenarios?The key points are:- The authors compare two finetuning approaches on the LegalBERT model - using a contrastive learning setup (SetFit) versus standard finetuning. - They evaluate on a legal provision classification task using the LEDGAR dataset.- There is a focus on low-resource scenarios where labeled legal data is small and imbalanced. The contrastive learning approach is hypothesized to help in such cases.- The models are compared on both the original imbalanced LEDGAR data and a balanced version. Evaluation metrics and extracted model features using LIME are analyzed.- The main research question is whether the contrastive learning finetuning performs better than standard finetuning on this legal classification task, especially when labeled data is scarce. The results generally show improved performance with the contrastive approach.In summary, the central hypothesis is that a contrastive learning objective can outperform standard finetuning for legal text classification when training data is imbalanced and low-resource. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Comparing a contrastive learning objective (SetFit) to standard finetuning for legal text classification, using the LEDGAR dataset. 2. Evaluating the models trained with these two objectives on both the original imbalanced LEDGAR dataset, as well as a balanced version of the dataset.3. Analyzing the models using LIME to extract positive and negative features contributing to classifications. The analysis suggests the contrastive learning approach helps boost both positive and negative legally informative features.4. Showing that the contrastive learning approach (SetFit) can achieve comparable or better performance than standard finetuning, while using a fraction of the training data. The contrastive method seems to make more confident use of informative features.5. Providing evidence that contrastive learning objectives like SetFit are promising for low-resource legal text classification scenarios, where labeled data may be small or imbalanced. The contrastive approach seems to better utilize the available limited labeled data.In summary, the main contribution is demonstrating the potential of contrastive learning objectives to improve legal text classification performance in low-resource scenarios, while also producing models that seem to better leverage legally informative features. The analysis provides evidence this approach makes efficient use of scarce labeled data.
