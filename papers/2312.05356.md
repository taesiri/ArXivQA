# [Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs](https://arxiv.org/abs/2312.05356)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs":

Problem:
- Large language models (LLMs) are being adopted for code generation tasks but require regular updates to fix bugs, improve safety, and handle new versions of dependencies. 
- Retraining LLMs is costly and can lead to catastrophic forgetting. Rule-based methods lack flexibility to cover accumulated changes. 
- There is a need for effective, efficient and reliable techniques to update the knowledge in LLMs.

Proposed Solution:
- The paper proposes a neuron-level model editing approach called MENT (Model Editing via Neuron Targeting) to modify coder LLMs.
- It locates buggy neurons using attribution methods, estimates oracle parameters to overwrite those neurons, and plans neuron update order using a novel editing gain measure.  
- MENT edits a small number of neurons (1-2 neurons) to change model behavior for a specific input, enabling fast and precise updates.

Key Contributions:
- First work on neuron-level editing of generative models for improving code generation quality.
- MENT is an effective approach that outperforms state-of-the-art by a large margin on next-token prediction tasks.
- MENT is efficient, resolving errors by patching only 1-2 critical neurons on average.
- Evaluation on a new benchmark shows MENT strikes a good balance between generalization and specificity.
- Case study demonstrates MENT's compatibility with LLM chain-of-thought reasoning to automatically update dependent behaviors.

In summary, the paper proposes a novel neuron-level model editing technique called MENT that can effectively, efficiently and reliably update the knowledge in LLMs for coding tasks. It edits only a very small number of neurons to change model behaviors as needed, making it suitable for continuous learning.
