# [Towards Probing Contact Center Large Language Models](https://arxiv.org/abs/2312.15922)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Fine-tuning large language models (LLMs) on domain-specific data can enhance their capabilities, but there is limited research on using this approach for contact center applications. Contact center conversations have unique properties like disfluencies, multi-party dialogues etc that pose challenges.  

- The effectiveness of instruction fine-tuning and parameter-efficient methods like Low-Rank Adaptation (LoRA) for adapting LLMs to the contact center domain is not well studied.

Methods:
- The authors fine-tune LLMs like Flan-T5 and Llama on an internal contact center instruction dataset encompassing tasks like intent classification, summarization etc. 

- They compare contact center fine-tuned models (CC-LLMs) with out-of-the-box models (OOB-LLMs) on downstream tasks and probing tasks aimed at assessing conversational, channel and ASR properties unique to contact centers.

- Different model sizes, architectures and fine-tuning techniques (full vs LOw-Rank Adaptation based) are analyzed.

Key Findings:  
- CC-LLMs improve response quality on downstream tasks by over 48% compared to OOB-LLMs, demonstrating effectiveness of instruction fine-tuning.

- On probing tasks, CC-LLMs exhibit comparable or better capability to encode key properties without external supervision. But they rely less on surface, syntactic or semantic encodings.

- Encoder-decoder models outperform decoder-only models on probing tasks. Larger model size translates to better performance.

- LoRA-based fine-tuning, while efficient, comes at the cost of reduced performance on probing tasks compared to full fine-tuning.

Main Contributions:
- First study analyzing instruction fine-tuning for adapting LLMs to contact center domain
- Detailed analysis of properties learned by CC-LLMs using probing tasks
- Investigation of model architecture, size and fine-tuning methods on capability to encode conversational properties
