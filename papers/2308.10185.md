# ViT-Lens: Towards Omni-modal Representations

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop an effective and scalable approach to learn omni-modal representations that encompass diverse modalities beyond just images and text? The key hypothesis appears to be:By viewing a pretrained vision transformer (ViT) as a multi-modal sensor, we can enable it to perceive and comprehend novel modalities through proper input encoding, without the need for large-scale modality-specific data.In particular, the paper proposes a method called ViT-Lens that aims to:1) Unify model architectures by adopting a shared pretrained ViT to encode various modalities.2) Achieve data efficiency by effectively transferring knowledge from the pretrained ViT instead of requiring large labeled datasets. 3) Demonstrate emergent abilities where the ViT-based system can understand new modalities in a zero-shot manner without specific tuning.The paper validates these ideas through experiments on 3D shape understanding, showing ViT-Lens can outperform prior state-of-the-art approaches on tasks like zero-shot 3D classification. It also illustrates how ViT-Lens can enable multimodal language models to perceive 3D shapes without 3D-specific training.In summary, the core hypothesis is that a pretrained ViT can be adapted to act as a versatile multimodal encoder for various data types through proper input mapping, providing an efficient and unified solution for omni-modal representation learning. The paper aims to demonstrate and validate this idea.


## What is the main contribution of this paper?

Based on my reading of the LaTeX code, the main contribution of this paper seems to be proposing a novel method called "ViT-Lens" for advancing omni-modal representation learning. The key ideas presented are:- Conceptualizing a pretrained Vision Transformer (ViT) as a multi-modal sensor capable of perceiving and comprehending diverse modalities beyond just images. This allows extending the capabilities of a pretrained ViT to novel modalities without needing separate architectures or large-scale data collection for each one.- Introducing a "lens" for each new modality, consisting of modality embedding and a Perceiver, to map inputs into the input space of the pretrained ViT. The ViT can then encode outputs to align with feature spaces from an off-the-shelf foundation model.- Demonstrating the effectiveness of ViT-Lens for 3D shape understanding. The method achieves superior performance on tasks like zero-shot 3D classification and enabling a multimodal language model to understand 3D shapes without tuning.- ViT-Lens provides a unified and data-efficient approach to omni-modal representation learning. It aims to reduce the need for large-scale data collection and model retraining for new modalities, instead harnessing transfer learning from a single set of ViT parameters.In summary, the main contribution seems to be proposing ViT-Lens as a novel way to achieve omni-modal representation learning by extending a pretrained ViT to act as a multi-modal sensor, avoiding the need for separate architectures and large datasets per modality.
