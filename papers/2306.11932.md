# [Opportunities and Risks of LLMs for Scalable Deliberation with Polis](https://arxiv.org/abs/2306.11932)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are the opportunities and risks of using Large Language Models (LLMs) like Claude to address challenges with facilitating, moderating and summarizing the results of conversations using the Polis platform for collective deliberation?The key hypotheses explored in the paper appear to be:1) LLMs can augment human intelligence to help run Polis conversations more efficiently. Specifically, the authors hypothesize that capabilities like summarization and topic modeling will enable new methods to empower the public in collective meaning-making exercises. 2) However, the limitations of LLMs, such as restricted context length, will significantly impact the quality and utility of the results.3) There are risks associated with applying LLMs to deliberative systems like Polis that need to be carefully considered and mitigated. The authors seem especially concerned about the potential for bias, misinformation, and lack of transparency.4) Maintaining human feedback loops is vital for the safe and ethical application of LLMs to enhance tools like Polis.In sum, the central research question seems to revolve around assessing the opportunities and risks of using LLMs to augment human intelligence for deliberative platforms like Polis. The key hypotheses focus on the utility of LLMs for this purpose, but also the need for transparency, accountability, and human oversight to ensure the technology is applied responsibly. The paper aims to explore and validate these hypotheses through analysis and experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Demonstrating the potential for large language models (LLMs) like Anthropic's Claude to augment human intelligence and help run Polis conversations more efficiently. The authors show with pilot experiments that LLMs can assist with tasks like topic modeling, summarization, consensus identification, and vote prediction.- Discussing the opportunities and risks of using LLMs for deliberative platforms like Polis. The authors highlight capabilities like summarization that could enable new methods for public meaning-making, but also risks around bias, hallucination, and transparency that need to be addressed. - Proposing principles and techniques to mitigate risks of using LLMs with deliberative systems, such as maintaining human feedback loops, measuring model biases, and evaluating summaries.- Exploring the design space for integrating LLMs into the Polis platform to address challenges with analyzing results, facilitating conversations, routing comments, and predicting votes. The authors provide examples of prompts used in experiments with Claude.- Demonstrating that large context window LLMs can produce more detailed, nuanced summaries by considering an entire conversation, rather than comment batches.- Concluding with key strategies for applying LLMs, like recursive compilation to get around context limits, and future research directions for using LLMs to augment tools like Polis.In summary, the main contribution appears to be showing the potential of LLMs to enhance platforms like Polis for deliberation, while responsibly considering risks and mitigation strategies. The authors demonstrate capabilities with experiments, provide actionable techniques, and outline opportunities for future work in this space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper explores opportunities and risks of using large language models like Claude to augment human intelligence in deliberative processes like the Polis platform, finding that while models can assist with tasks like summarization, moderation, and facilitation, care must be taken to mitigate risks around bias, transparency, and proper application.
