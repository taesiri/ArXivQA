# [Simple steps are all you need: Frank-Wolfe and generalized   self-concordant functions](https://arxiv.org/abs/2105.13913)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. It proposes a simple variant of the Frank-Wolfe algorithm called Monotonic Frank-Wolfe (M-FW) for minimizing generalized self-concordant functions over compact convex sets. 2. It shows that M-FW achieves a O(1/t) convergence rate in terms of primal gap and Frank-Wolfe gap, using only first-order information and without the need for line searches. 3. It proves improved convergence rates for M-FW and variants in several special cases, such as when the feasible region is uniformly convex.4. It shows that the Away-Step Frank-Wolfe algorithm coupled with a backtracking line search can achieve linear convergence over polytopes.5. It provides numerical experiments highlighting the performance of M-FW compared to prior Frank-Wolfe style algorithms.In summary, the main research question addressed is how to design a simple and parameter-free Frank-Wolfe style algorithm with provable convergence guarantees for minimizing generalized self-concordant functions, which arise in many machine learning applications. The paper proposes M-FW as an answer, analyzed its convergence, and demonstrates its empirical performance.
