# [Simple steps are all you need: Frank-Wolfe and generalized   self-concordant functions](https://arxiv.org/abs/2105.13913)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. It proposes a simple variant of the Frank-Wolfe algorithm called Monotonic Frank-Wolfe (M-FW) for minimizing generalized self-concordant functions over compact convex sets. 2. It shows that M-FW achieves a O(1/t) convergence rate in terms of primal gap and Frank-Wolfe gap, using only first-order information and without the need for line searches. 3. It proves improved convergence rates for M-FW and variants in several special cases, such as when the feasible region is uniformly convex.4. It shows that the Away-Step Frank-Wolfe algorithm coupled with a backtracking line search can achieve linear convergence over polytopes.5. It provides numerical experiments highlighting the performance of M-FW compared to prior Frank-Wolfe style algorithms.In summary, the main research question addressed is how to design a simple and parameter-free Frank-Wolfe style algorithm with provable convergence guarantees for minimizing generalized self-concordant functions, which arise in many machine learning applications. The paper proposes M-FW as an answer, analyzed its convergence, and demonstrates its empirical performance.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a simple variant of the Frank-Wolfe algorithm that uses an open-loop step size rule of γ_t = 2/(t+2) to obtain a O(1/t) convergence rate for minimizing generalized self-concordant functions over compact convex sets. This avoids the need for second-order information or line searches compared to prior methods.- It shows this Frank-Wolfe variant achieves a O(1/t) convergence rate in terms of both the primal gap and Frank-Wolfe gap, and precisely characterizes the oracle complexity in terms of calls to zeroth-order, first-order, linear optimization, and domain oracles. - It proves improved convergence rates for the Frank-Wolfe algorithm with backtracking line search in various cases, including linear convergence for Away-Step Frank-Wolfe over polytopes.- It provides numerical experiments demonstrating the competitiveness of the proposed monotonic Frank-Wolfe variant compared to other Frank-Wolfe algorithms for generalized self-concordant functions.In summary, the key contribution is a simple yet powerful Frank-Wolfe algorithm for generalized self-concordant functions that matches the convergence rates of prior methods while using only first-order information and avoiding line searches or parameter tuning. The analysis provides precise oracle complexity bounds and improved rates in special cases.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a simple Frank-Wolfe algorithm variant with an open-loop step size that achieves an O(1/t) convergence rate for minimizing generalized self-concordant functions over compact convex sets, avoiding the need for second-order information or line searches.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on Frank-Wolfe algorithms for generalized self-concordant functions compares to other related work:- It establishes convergence rates for a simple, open-loop Frank-Wolfe variant using just a step size of 2/(t+2), avoiding second-order information or line searches needed in prior work. This results in cheaper iterations and precise iteration complexity bounds.- It shows the same Frank-Wolfe variant achieves O(1/t) convergence in both primal gap and FW gap, improving on prior FW results for generalized self-concordant functions.- It proves linear convergence rates for Away-Step Frank-Wolfe with backtracking line search over polytopes, complementing recent independent work analyzing Away-Step FW. - It provides convergence guarantees for Frank-Wolfe with backtracking line search in cases like uniformly convex sets and interior optima, improving on rates from prior work.- It complements theoretical results with experiments highlighting the practical performance of the proposed algorithms against prior Frank-Wolfe style approaches.Overall, this work expands understanding of Frank-Wolfe style algorithms for generalized self-concordant functions, providing simpler algorithms with strong convergence guarantees and rates. It improves theoretical convergence rates in many cases compared to prior work while also demonstrating practical performance empirically. The analysis helps advance the theory and practice of optimization for this important function class.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions in the conclusion of the paper:- Developing additional restart conditions for the Monotonic Frank-Wolfe algorithm (Algorithm 3), not just on the halving counter φt but also on the base step-size strategy. This could help improve the performance of the simple vs stateless step-size strategies.  - Extending the analysis to other types of functions beyond generalized self-concordant functions, such as Hölder smooth functions.- Considering inexact oracle models, where the linear minimization oracle may only return an approximate minimizer.- Developing distributed and parallel variants of the algorithms presented.- Applying the algorithms to other problems involving generalized self-concordant functions.- Considering stochastic/online variants of the algorithms for statistical learning problems.- Considering accelerated variants using momentum/Nesterov acceleration techniques.- Further improving the dependence on key parameters in the convergence rates, like the diameter or smoothness constants.- Developing specialized algorithms that can take advantage of problem structure beyond what is captured by generalized self-concordance.So in summary, the main directions mentioned are:1) Improved restarting strategies 2) Extensions to other function classes3) Inexact oracles4) Distributed/parallel versions5) New applications6) Stochastic/online settings7) Acceleration techniques8) Tighter analysis9) Exploiting more problem structure
