# [Continual Learning through Networks Splitting and Merging with   Dreaming-Meta-Weighted Model Fusion](https://arxiv.org/abs/2312.07082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Balancing the stability and plasticity of neural networks for continual learning is challenging. Existing methods typically focus more on stability by restricting plasticity, which harms the learning capability on new tasks. 

Proposed Method: This paper proposes a two-stage continual learning method named Split2MetaFusion to achieve better stability-plasticity trade-off:

1) Splitting stage: 
- Train a "slow" model with stability on old tasks using a proposed Task-Preferred Null Space Projector (TPNSP). TPNSP relaxes constraints compared to prior methods, enabling the model to move closer to optimal regions for new tasks.  
- Train a separate "fast" model initialized from the slow model with better plasticity on new tasks and fewer constraints.

2) Meta-Weighted Fusion stage:
- Introduce a dreaming mechanism to extract learned knowledge of slow and fast models without needing old data. 
- Design a Dreaming-Meta-Weighted fusion policy to merge models by allowing each model higher fusion weights on their preferred parameters, maintaining both old and new knowledge well.

Main Contributions:
- A two-stage continual learning strategy with model splitting and meta-weighted fusion.
- A Task-Preferred Null Space Projector (TPNSP) to narrow the gap between slow and fast models and relax constraints.
- A Dreaming-Meta-Weighted fusion approach to merge models without needing previous data.

Experiments on image classification and segmentation benchmarks demonstrate Split2MetaFusion achieves superior stability, plasticity and overall performance compared to state-of-the-art continual learning methods. The approach advances model splitting and meta-weighted fusion for effective continual learning.
