# [GOLD-NAS: Gradual, One-Level, Differentiable](https://arxiv.org/abs/2007.03331)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How to perform neural architecture search in a much larger and less constrained search space compared to prior work, while still discovering high-quality architectures efficiently? The key ideas and contributions in addressing this question are:- Proposing a greatly enlarged search space by removing many of the manual design constraints used in previous NAS methods like DARTS. This expands the space from <10^20 candidates to >10^160 candidates.- Demonstrating that many existing differentiable NAS methods fail dramatically in this new enlarged space due to challenges like discretization error when pruning architectures.- Introducing a new NAS algorithm called GOLD-NAS that uses gradual, one-level, differentiable optimization along with resource constraints to effectively search the enlarged space.- Showing GOLD-NAS can discover a Pareto front of architectures in the accuracy vs efficiency trade-off, achieving strong results on CIFAR-10 and ImageNet while using fewer parameters and FLOPs than prior NAS methods.In summary, the key hypothesis is that the constraints in existing NAS search spaces limit the architectures that can be discovered, and this can be addressed by searching a much larger space with a more effective optimization method like GOLD-NAS. The results validate this hypothesis, advancing the state-of-the-art in differentiable NAS.


## What is the main contribution of this paper?

The main contribution of this paper seems to be the proposal of a novel neural architecture search algorithm called GOLD-NAS. The key highlights of GOLD-NAS include:- It enlarges the search space by removing common constraints in previous NAS methods, resulting in a search space with over 10^160 possible architectures. This provides more flexibility to find efficient architectures.- It uses one-level optimization to avoid the burden of inaccurate gradient estimation in bi-level optimization. The authors show one-level optimization can work well with proper regularization.- It introduces a gradual pruning strategy with resource constraints to alleviate the discretization error issue in converting the continuous search space to a discrete architecture. By gradually increasing the resource regularization, weak operators are slowly pruned out.- It can find a set of Pareto-optimal architectures with different tradeoffs between accuracy and efficiency in one search. Previous methods required running multiple independent searches to achieve this.- Experiments on CIFAR-10 and ImageNet show GOLD-NAS can find very compact and accurate models. For example, it obtains 2.99% error on CIFAR-10 with only 1.58M parameters.Overall, the main contribution seems to be proposing the GOLD-NAS algorithm that can efficiently search over a very large space and find highly efficient architectures in a stable and effective manner. The enlarged search space and gradual pruning approach appear to be key innovations that allow it to outperform prior NAS techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes GOLD-NAS, a neural architecture search method that gradually prunes weak operators from a large search space using one-level differentiable optimization under resource constraints to find a set of Pareto-optimal architectures with a single search.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in neural architecture search:- The search space is enlarged compared to prior work like DARTS. By removing constraints on the number of operators per edge, number of prior inputs per node, etc. the search space grows exponentially larger. This provides a bigger pool of potential architectures.- The authors advocate for one-level optimization rather than bi-level optimization used in DARTS. They argue one-level optimization avoids issues with inaccurate gradient estimations in bi-level approaches. This simplifies the search algorithm.- A key contribution is the gradual pruning strategy to deal with discretization error. By slowly increasing the regularization coefficient, weak operators can be removed smoothly rather than abruptly pruning. This helps stabilize one-level optimization.- The search process is very efficient, finding good architectures on CIFAR-10 in 0.4 GPU days. On ImageNet, architectures are found in just 1-2 GPU days. This is faster than many competing NAS methods.- The search can produce a Pareto front of architectures balancing accuracy and efficiency in one run. This is more efficient than running separate searches for each constraint like in some prior work.- The accuracy results are competitive with state-of-the-art NAS approaches. On CIFAR-10, errors around 2.5-3% are achieved with low parameter counts. On ImageNet, top-1 errors of 24% are found under mobile settings.Overall, this paper makes contributions in enlarging the search space, simplifying the search algorithm, and stabilizing one-level optimization through gradual pruning. The efficiency and accuracy results demonstrate these ideas work well in practice compared to prior NAS research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring more complex and challenging search spaces for neural architecture search. The authors advocate for getting rid of manually designed rules and constraints in the search space, which often leads to more flexible and efficient architectures. They suggest further enlarging the search space as a way to advance NAS research.- Improving the ability to handle multiple resource constraints simultaneously during the search. The authors point out that in real applications there are often constraints like model size, latency, etc. They suggest investigating how to best schedule the regularization coefficients during gradual pruning when optimizing for multiple objectives. - Applying the search method to other vision tasks beyond image classification, such as object detection and segmentation, as well as unsupervised representation learning. The authors propose extending their GOLD-NAS approach to these other tasks to demonstrate its generalization ability.- Further reducing the optimization gap in differentiable NAS. While GOLD-NAS addresses the discretization error issue, the authors point out the need for end-to-end NAS methods that do not require separate training and search phases. Eliminating this optimization gap could enhance NAS performance.- Evaluating NAS algorithms on benchmarks with even more complex search spaces. The authors argue that the ability of NAS methods should be tested on spaces without heavy manual design, though finding the right balance of complexity remains an open question.In summary, the main future directions are focused on expanding the search space, handling multiple constraints, applying NAS to new tasks, reducing the optimization gap, and developing better NAS benchmarks. Advancing research in these areas could lead to more flexible, efficient and generalizable neural architecture search.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents GOLD-NAS (Gradual One-Level Differentiable Neural Architecture Search), a novel neural architecture search method. The key ideas are: 1) Enlarging the search space by removing heuristic constraints used in prior NAS methods like DARTS, allowing more flexibility. This results in a huge search space of over 10^160 candidates. 2) Using one-level optimization to avoid the limitations of bi-level optimization used in DARTS. 3) Introducing a gradual pruning technique based on resource constraints that avoids the discretization error problem faced by prior methods when converting the continuous architecture parameters to a discrete architecture. 4) Obtaining a Pareto front of optimal architectures in one search that trade off accuracy and efficiency. Experiments on CIFAR10 and ImageNet show GOLD-NAS can find very efficient architectures in the enlarged search space that achieve state-of-the-art accuracy-efficiency tradeoffs. The stability of the gradual pruning technique enables effective NAS in the more complex search space.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents GOLD-NAS, a novel neural architecture search method based on differentiable architecture search. GOLD-NAS relaxes the constraints commonly used in differentiable NAS methods, like having one operator per edge or fixed connections between nodes, to enlarge the search space. This enlarged space contains over 10^160 possible architectures. Many existing differentiable NAS methods struggle in this expanded space due to difficulties with discretization. To address this, GOLD-NAS introduces a gradual one-level optimization method. This adds a variable resource constraint that gradually prunes weak operators out of the network during optimization. The resource constraint acts as regularization to guide the search. GOLD-NAS is evaluated on CIFAR-10 and ImageNet classification. It can efficiently find a Pareto front of architectures balancing accuracy and computational requirements in a single search. Without manual rules, it discovers novel architectures that achieve strong performance tradeoffs. For example, it finds a 1.58M parameter model with 2.99% error on CIFAR-10, and 24.0% top-1 error under the mobile setting on ImageNet. The results demonstrate the method's ability to effectively explore the enlarged search space. Key advantages are its speed, simplicity, stability, and automatic discovery of efficient architectures.
