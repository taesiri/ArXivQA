# [ParameterNet: Parameters Are All You Need for Large-scale Visual   Pretraining of Mobile Networks](https://arxiv.org/abs/2306.14525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be: Low FLOP neural networks cannot benefit from large-scale pretraining. The authors term this the "low FLOPs pitfall."To address this, the authors propose a general design principle called "ParameterNet" which adds more parameters to a low FLOP network while keeping the FLOPs low. This is intended to enable low FLOP networks to benefit from large-scale pretraining.So in summary, the central hypothesis is that increasing parameters while maintaining low FLOPs will allow low FLOP networks to gain accuracy improvements from large-scale pretraining, overcoming the "low FLOPs pitfall." The ParameterNet design principle is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- It identifies a "low FLOPs pitfall" where low-FLOPs vision models cannot benefit from large-scale pretraining. Experiments on Swin Transformer and EfficientNetV2 show that models with <2G FLOPs do not improve from ImageNet-22K pretraining compared to ImageNet-1K pretraining. - It proposes "ParameterNet", a general design principle for constructing low-FLOP models that can benefit from large-scale pretraining. The key idea is to add more parameters while maintaining low FLOPs. This is achieved using dynamic convolutions.- ParameterNet models are shown to overcome the "low FLOPs pitfall" and achieve significant gains from ImageNet-22K pretraining (+2.6% for a 600M FLOPs model). The ParameterNet-600M model achieves higher accuracy than Swin-T while using 7x fewer FLOPs.- Experiments demonstrate the effectiveness of ParameterNet for both convolutional and transformer architectures. The scheme is general and enables various low-FLOP models to take advantage of large-scale pretraining.In summary, the main contribution is proposing and demonstrating a general strategy (ParameterNet) to allow low-FLOP models to benefit from large-scale pretraining through efficient parameterization, overcoming the "low FLOPs pitfall".
