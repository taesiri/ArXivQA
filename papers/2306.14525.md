# [ParameterNet: Parameters Are All You Need for Large-scale Visual   Pretraining of Mobile Networks](https://arxiv.org/abs/2306.14525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be: Low FLOP neural networks cannot benefit from large-scale pretraining. The authors term this the "low FLOPs pitfall."To address this, the authors propose a general design principle called "ParameterNet" which adds more parameters to a low FLOP network while keeping the FLOPs low. This is intended to enable low FLOP networks to benefit from large-scale pretraining.So in summary, the central hypothesis is that increasing parameters while maintaining low FLOPs will allow low FLOP networks to gain accuracy improvements from large-scale pretraining, overcoming the "low FLOPs pitfall." The ParameterNet design principle is proposed to test this hypothesis.
