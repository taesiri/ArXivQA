# [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that the additional components added in hierarchical vision transformers (such as shifted windows, relative position embeddings, etc.) to induce spatial biases are actually unnecessary if the model is pre-trained with a strong self-supervised task like masked autoencoding (MAE). 

The authors argue that the spatial biases induced by these extra components can simply be learned by the model through pre-training, eliminating the need for extra model complexity. Thus, the paper's goal is to show that a very simple hierarchical transformer architecture without any of these extra biasing components can achieve state-of-the-art accuracy when pre-trained with MAE, creating an efficient and high-performing model.

To test this, the authors take an existing hierarchical vision transformer (MViTv2) and progressively remove components like relative position embeddings, convolutions, attention residuals, etc. while pre-training with MAE. They find the resulting simplified architecture (Hiera) achieves better accuracy than the original MViTv2, supporting their hypothesis that these extra components are unnecessary with proper pre-training.

In summary, the central hypothesis is that strong pre-training can teach a hierarchical vision transformer the spatial reasoning abilities provided by extra architectural components, eliminating the need for that complexity. Hiera is presented as a case study supporting this claim.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of a simple hierarchical vision transformer called Hiera. The key ideas are:

- Removing the bells and whistles (e.g. shifted windows, convs, relative pos embs) from existing hierarchical vision transformers like MViTv2 or Swin transformers. 

- Using strong pre-training with Masked Autoencoders (MAE) to teach the model spatial reasoning and remove the need for specialized modules to encode spatial bias.

- As a result, Hiera is a very simple hierarchical transformer that is significantly faster and more accurate than prior work across image and video recognition tasks. 

- The simplicity enables compatibility with efficient sparse pre-training like MAE, allowing the model to be trained faster than MViTv2 or other hierarchical vision transformers.

- Extensive experiments show Hiera outperforms state-of-the-art on major image classification datasets as well as video recognition datasets like Kinetics, while being much more efficient.

In summary, the main contribution is showing that a simple hierarchical vision transformer stripped of common bells and whistles can surpass more complex models, if trained properly with pre-training objectives like MAE that teach spatial reasoning. This results in an accurate yet extremely fast vision transformer model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes Hiera, an extremely simple hierarchical vision transformer that removes unnecessary components like convolutions and shifted windows from previous models and instead relies on strong self-supervised pretraining with MAE to teach the model spatial reasoning, resulting in a model that is more accurate and significantly faster than prior work on both image and video recognition tasks.
