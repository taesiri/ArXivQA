# [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that the additional components added in hierarchical vision transformers (such as shifted windows, relative position embeddings, etc.) to induce spatial biases are actually unnecessary if the model is pre-trained with a strong self-supervised task like masked autoencoding (MAE). 

The authors argue that the spatial biases induced by these extra components can simply be learned by the model through pre-training, eliminating the need for extra model complexity. Thus, the paper's goal is to show that a very simple hierarchical transformer architecture without any of these extra biasing components can achieve state-of-the-art accuracy when pre-trained with MAE, creating an efficient and high-performing model.

To test this, the authors take an existing hierarchical vision transformer (MViTv2) and progressively remove components like relative position embeddings, convolutions, attention residuals, etc. while pre-training with MAE. They find the resulting simplified architecture (Hiera) achieves better accuracy than the original MViTv2, supporting their hypothesis that these extra components are unnecessary with proper pre-training.

In summary, the central hypothesis is that strong pre-training can teach a hierarchical vision transformer the spatial reasoning abilities provided by extra architectural components, eliminating the need for that complexity. Hiera is presented as a case study supporting this claim.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of a simple hierarchical vision transformer called Hiera. The key ideas are:

- Removing the bells and whistles (e.g. shifted windows, convs, relative pos embs) from existing hierarchical vision transformers like MViTv2 or Swin transformers. 

- Using strong pre-training with Masked Autoencoders (MAE) to teach the model spatial reasoning and remove the need for specialized modules to encode spatial bias.

- As a result, Hiera is a very simple hierarchical transformer that is significantly faster and more accurate than prior work across image and video recognition tasks. 

- The simplicity enables compatibility with efficient sparse pre-training like MAE, allowing the model to be trained faster than MViTv2 or other hierarchical vision transformers.

- Extensive experiments show Hiera outperforms state-of-the-art on major image classification datasets as well as video recognition datasets like Kinetics, while being much more efficient.

In summary, the main contribution is showing that a simple hierarchical vision transformer stripped of common bells and whistles can surpass more complex models, if trained properly with pre-training objectives like MAE that teach spatial reasoning. This results in an accurate yet extremely fast vision transformer model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes Hiera, an extremely simple hierarchical vision transformer that removes unnecessary components like convolutions and shifted windows from previous models and instead relies on strong self-supervised pretraining with MAE to teach the model spatial reasoning, resulting in a model that is more accurate and significantly faster than prior work on both image and video recognition tasks.


## How does this paper compare to other research in the same field?

 This paper presents a simple method to enable efficient masked image modeling pre-training for hierarchical vision transformers like MViTv2. The key contributions are:

- It proposes techniques like masking image regions ("mask units") rather than individual tokens to enable compatibility with sparse MAE pre-training in hierarchical models where the token size changes across stages. 

- It shows that many of the specialized components in MViTv2 like relative position embeddings, convolutional layers, overlap in pooling, etc. are not needed when pre-trained with MAE. By removing these, the method arrives at a very simple and fast hierarchical vision transformer called Hiera.

- Extensive experiments show that Hiera achieves excellent results on image classification (86.1% on ImageNet-1K), outperforming MViTv2 and other recent methods. More importantly, it far exceeds prior work on video classification, achieving SOTA 87.3% on Kinetics-400.

- Hiera is much faster than MViTv2 and competitive hierarchical vision transformers, especially for video where it is over 2x faster than ViT models. The speedups come from the model simplification and enabling sparse MAE pre-training.

This work is impactful because it shows that many bells and whistles added to transformers like MViTv2 for supervised training are not needed if pre-trained in a self-supervised manner with MAE. The resulting model Hiera is simple, fast, and achieving excellent results. 

Other related works have tried to enable sparse pre-training of hierarchical models but result in more complex solutions or lose accuracy. This paper stands out with its simple but effective approach and strong empirical results surpassing prior art, especially for video understanding. The innovations could be useful for designing efficient video transformers.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions the authors suggest are:

- Exploring pretraining \shortname{} with complementary methods like using an EMA teacher (e.g. SplitMask or Data2Vec) to further improve performance. The authors suggest this could be a promising direction since \shortname{} is readily compatible with these methods.

- Applying \shortname{} to more downstream tasks beyond the ones explored in the paper, such as video object detection, 3D detection, point cloud reconstruction, etc. The authors demonstrate the strong performance of \shortname{} on a variety of image and video classification tasks, so it would be interesting to see if it transfers well to other domains.

- Developing transformer-based solutions for downstream tasks when using \shortname{}, rather than standard convolutional approaches like Mask R-CNN. The authors note that unlike other hierarchical vision transformers, \shortname{} acts more like a vanilla ViT so transformer-based heads may work better.

- Exploring the use of techniques like Flash Attention or other attention optimizations to further increase the speed of \shortname{}. The authors did not employ these in their benchmarks but suggest they could provide additional speedups.

- Training and evaluating even larger \shortname{} variants, since the strong scaling behavior demonstrated suggests there could be further gains from bigger models.

In summary, the main future directions are exploring complementary training methods, applying to more tasks, using transformer-based downstream heads, optimizing attention, and scaling up model size. The authors convincingly demonstrate the potential of the \shortname{} architecture, so there are many interesting avenues for extending it further.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper introduces Hiera, a simple hierarchical vision transformer model without complex vision-specific operations like shifted windows or convolutions. The key idea is that strong pre-training with masked autoencoding (MAE) can teach the model spatial reasoning capabilities, removing the need for explicitly building in spatial biases through complicated architectural modifications. The authors simplify an existing hierarchical vision transformer (MViTv2) by progressively removing components like relative position biases and convolutions while training with MAE. This process results in a very simple yet accurate model called Hiera that consists only of standard transformer blocks in a hierarchical layout. Experiments across image and video tasks demonstrate that Hiera outpaces or matches prior state-of-the-art models while being significantly more efficient. By relying on pre-training to provide spatial reasoning rather than complex architectural components, Hiera shows that a simple hierarchical vision transformer can achieve excellent performance across vision domains.
