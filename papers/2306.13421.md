# [Long-range Language Modeling with Self-retrieval](https://arxiv.org/abs/2306.13421)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- Proposes a new architecture called Retrieval-Pretrained Transformer (RPT) for long-range language modeling. RPT has a native retrieval ability built into the model architecture and training.- RPT takes a chunk of text as input, retrieves semantically-relevant chunks from earlier in the document to provide context, and fuses the representations of retrieved chunks into the model to better predict the next chunk.- The retriever component is trained jointly with the language model using a semantic objective. The goal is to retrieve chunks that increase the probability of generating the subsequent chunk, as evaluated by a reference language model. - Evaluated on long-range LM tasks like books, code, and math writing. RPT improves retrieval quality and perplexity compared to strong baselines like RETRO and TRIME.So in summary, the central hypothesis is that integrating retrieval tightly into the model architecture and training process from scratch will improve long-range language modeling performance by enabling the model to leverage relevant context from anywhere in the document history. The paper aims to demonstrate this through the proposed RPT model and experiments on various long-range LM datasets.
