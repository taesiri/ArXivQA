# [Interpretable Neural Architecture Search via Bayesian Optimisation with   Weisfeiler-Lehman Kernels](https://arxiv.org/abs/2006.07556)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to perform efficient and interpretable neural architecture search (NAS) using Bayesian optimization. Specifically, the key questions are:1. How to make the graph-like and high-dimensional NAS search spaces amenable to Bayesian optimization, which typically works on continuous spaces? 2. How to improve the sample efficiency and scalability of Bayesian optimization for NAS compared to existing methods?3. How to gain useful interpretability from the NAS process, in terms of understanding which architectural motifs contribute positively or negatively to performance?4. How can the interpretability be used to further improve the NAS search process, for example via warm starting a new related task?To tackle these challenges, the core proposal is to use a Gaussian process surrogate model with the Weisfeiler-Lehman graph kernel. This allows Bayesian optimization to be directly applied on graph-structured architectures while exploiting the topological relationships between them. The graph kernel also provides interpretable features in terms of network motifs that correlate with performance. The paper shows this approach, termed NAS-BOWL, achieves state-of-the-art results on NAS benchmark datasets in terms of sample efficiency, final performance, and provides concrete examples of how interpretability enables improvement of the search process.In summary, the central hypothesis is that using Bayesian optimization with graph kernels can lead to an efficient, scalable and interpretable approach to neural architecture search. The paper provides compelling evidence to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. It proposes a Bayesian optimization (BO) approach for neural architecture search (NAS) called NAS-BOWL, which uses a Gaussian process (GP) surrogate model with the Weisfeiler-Lehman (WL) graph kernel. This allows the method to effectively optimize over the graph-based search spaces commonly used in NAS.  2. It introduces the idea of "interpretable NAS" where the method identifies interpretable network motifs and their impact on performance using the features extracted by the WL kernel. This provides insights into why certain architectures perform well.3. It shows that the proposed surrogate model is highly data-efficient compared to prior NAS methods based on graph neural networks, requiring much less observed architecture data to achieve good performance prediction.4. The method achieves state-of-the-art results on benchmark NAS datasets including NAS-Bench-101, NAS-Bench-201, and the DARTS search space, demonstrating its effectiveness.5. The extracted motifs can be used to guide architecture search, such as via transfer learning across tasks, providing a simple but effective demonstration of how the interpretability of the method can be leveraged.In summary, the key innovations are using a GP+WL kernel surrogate for efficient NAS optimization, achieving interpretability via the WL features, and showing strong empirical performance on NAS benchmarks, all of which help advance the state-of-the-art in this area. The proposed interpretable NAS idea also opens up promising new research directions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a Bayesian optimisation approach for neural architecture search that combines a Gaussian process surrogate with the Weisfeiler-Lehman graph kernel to efficiently search over graph-based neural network architectures, providing state-of-the-art performance while also enabling some degree of interpretability by identifying useful neural network motifs.


## How does this paper compare to other research in the same field?

 This paper presents an interpretable neural architecture search method using Bayesian optimization with Weisfeiler-Lehman kernels. Here are some key ways it compares to other research in neural architecture search:- Interpretability: A unique aspect of this work is the focus on interpretability. Most neural architecture search methods aim to find high-performing architectures but provide little insight into why a particular architecture works well. This paper extracts interpretable network motifs and learns their impact on performance.- Bayesian Optimization: Several recent papers have applied Bayesian optimization to neural architecture search, but they rely on encoding schemes or graph neural networks. This paper uses a Gaussian process with a Weisfeiler-Lehman kernel, which better captures the graph structure of architectures.- Efficiency: The proposed method is highly data-efficient, achieving strong performance with far fewer architecture evaluations than competing Bayesian optimization or reinforcement learning methods. It is also scalable to larger architecture spaces.- Performance: The paper shows state-of-the-art results on NAS benchmarks compared to existing NAS approaches. The method works well on both closed-domain cell-based search spaces like NAS-Bench and open-domain spaces like DARTS.- Transfer learning: The interpretability of network motifs enables a simple yet effective transfer learning approach, where motifs from previous tasks can guide search on new tasks. This demonstrates a practical use case of the interpretability.In summary, this paper pushes neural architecture search in a more interpretable direction while achieving strong efficiency and performance. The graph kernel approach and transfer learning application are innovative compared to prior NAS research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:- Extending the interpretability afforded by their method to other types of neural architecture search spaces beyond cell-based architectures. The current work focuses on demonstrating interpretability on cell-based search spaces like NAS-Bench and DARTS. The authors suggest expanding the approach to other types of search spaces as a direction for future work.- Applying the method to multi-objective neural architecture search problems. The current work focuses on single-objective optimization of validation accuracy. The authors propose extending their method to capture trade-offs between multiple objectives like accuracy, model size, and latency.- Further exploration of transfer learning. The paper gives a preliminary example of using interpretable motifs for warm-starting search on a new task. The authors suggest more extensive validation on tasks with varying degrees of similarity as future work. - Theoretical analysis of the method. While the current work is empirical, the authors suggest theoretical analysis of convergence guarantees and other properties would be beneficial.- Extending beyond cell-based architectures. The current work relies on the graph representability of cell-based spaces. Applying the ideas to spaces without an explicit graph structure is noted as future work.In summary, the main future directions mentioned are: expanding the approach to new types of search spaces, multi-objective optimization, more thorough investigation of transfer learning, theoretical analysis, and moving beyond cell-based search. Overall, the authors propose their method as a first step towards interpretable NAS, and suggest many avenues to extend the scope and utility of the approach in future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes a Bayesian optimization approach for neural architecture search (NAS) that combines a Gaussian process (GP) surrogate with the Weisfeiler-Lehman (WL) subtree graph kernel. This method, termed NAS-BOWL, is highly data-efficient and captures the topological structure of architectures, making it scalable to large graph-like search spaces. More importantly, NAS-BOWL enables interpretability by discovering useful network motifs and learning their impact on performance based on the GP surrogate's gradient information. These motifs can explain model performance and guide architecture generation. Experiments show the GP-WL surrogate achieves superior prediction versus baselines using 3-20x less data. NAS-BOWL outperforms existing NAS methods, achieving state-of-the-art on NAS benchmarks while being more efficient. Overall, NAS-BOWL not only optimizes architectures efficiently but also grants interpretability via useful motifs to explain performance and guide search.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a Bayesian optimisation approach for neural architecture search (NAS) that combines a Gaussian process (GP) surrogate with the Weisfeiler-Lehman (WL) subtree graph kernel. The method, termed NAS-BOWL, represents neural network architectures as graphs and uses the WL kernel to capture their topological structure. This allows the GP surrogate model to directly operate on the graph domain, avoiding the need for manual feature engineering or encoding schemes. The WL kernel provides a natural way to compare neural architectures based on interpretable local and global patterns. By using gradient information from the GP surrogate, the impact of different motifs on performance can be assessed. This provides a level of interpretability and allows important architecture building blocks to be identified.  Experiments demonstrate that NAS-BOWL achieves superior sample efficiency compared to existing NAS methods. The WL kernel captures meaningful architecture features that transfer well between datasets. Motifs can be used to prune inferior candidates or guide architecture generation when optimizing the acquisition function. The method is applied to benchmark datasets where it achieves state-of-the-art results. The integration of WL kernels and GP modelling provides a promising approach to architecture search that is highly data-efficient while also affording interpretability. Key advantages are the ability to operate directly on graph-based search spaces and leverage of topological patterns that impact performance.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a Bayesian optimisation (BO) approach for neural architecture search (NAS) that combines a Gaussian process (GP) surrogate model with the Weisfeiler-Lehman (WL) subtree graph kernel. The WL kernel enables the GP to directly operate on the graph-like neural architectures and capture their topological structure, avoiding the need for manual feature engineering or encoding schemes. The GP-WL surrogate is used within a BO framework to efficiently search the architecture space by modeling the performance of unseen architectures and recommending new candidates via an acquisition function. Additionally, the interpretable WL features are leveraged to identify important motifs in the architectures and learn their impact on performance based on the GP posterior derivatives. This interpretability is then used to help explain good architectures and guide the search, such as via warm-starting new tasks by transferring impactful motifs from previous searches. Overall, the proposed GP-WL surrogate achieves strong performance on NAS benchmark datasets while affording interpretability.


## What problem or question is the paper addressing?

 The key points from my reading of the paper are:- The paper is addressing the challenge of neural architecture search (NAS). NAS aims to automate the design of neural network architectures for a given task and dataset. - Existing NAS methods behave like a black box, simply returning the final optimal architecture. They offer little insight into why that architecture performs well or how to further improve it. - The authors propose a Bayesian optimization (BO) approach for NAS that provides interpretability. Their method combines a Gaussian process surrogate model with the Weisfeiler-Lehman graph kernel.- This approach allows the model to identify useful architecture motifs (building blocks) and determine their impact on performance. The interpretability helps explain why certain architectures perform well and can guide the search and generation of new promising candidates.- Their method is highly data-efficient, requiring far fewer architecture evaluations than competing NAS methods. It is also scalable to larger architecture search spaces.- They demonstrate the approach achieves state-of-the-art performance on NAS benchmark datasets while being more efficient. The identified motifs also provide useful insights into architecture design.In summary, the key contribution is a BO-based NAS method that optimizes architectures efficiently while also affording interpretability into useful architecture features and performance factors. This interpretability is a novel aspect compared to existing black-box NAS techniques.
