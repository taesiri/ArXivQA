# [Masked Images Are Counterfactual Samples for Robust Fine-tuning](https://arxiv.org/abs/2303.03052)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the robustness of fine-tuned vision models to out-of-distribution (OOD) data while maintaining good performance on in-distribution data?The key hypothesis is that using masked images as counterfactual training samples can help preserve the OOD robustness of large pre-trained vision models when fine-tuning them on downstream tasks. In particular, the paper proposes:- Masking semantics-related or unrelated image regions based on class activation maps to break spurious correlations in the training data.- Refilling the masked regions with patches from other images to further distort the original semantics.- Using the resulting masked counterfactual images for feature-based distillation with the pre-trained model during fine-tuning.The central hypothesis is that training the fine-tuned model to mimic the pre-trained model on these counterfactual samples will help retain robustness to OOD data while improving in-distribution performance. Experiments on ImageNet fine-tuning validate this hypothesis and show improved OOD accuracy over prior methods.In summary, the key research question is how to improve robustness to OOD data in fine-tuning vision models, and the central hypothesis is using counterfactual masked images can achieve this goal. The experiments aim to validate the effectiveness of the proposed approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel fine-tuning method to improve the robustness of vision models using masked images as counterfactual samples. Specifically:- The paper analyzes the issue of robustness degradation in fine-tuning from a causal perspective. It models the image generation process with a structural causal model and finds that fine-tuning tends to learn spurious correlations between semantic and non-semantic factors, which harms robustness. - To address this issue, the paper proposes to construct counterfactual images by masking and refilling patches based on class activation maps. These images help break the spurious correlations and regularize fine-tuning.- The paper investigates different strategies for masking (random, object-focused, context-focused) and refilling (no refill, single image, multiple images). Experiments show masking object-focused regions and refilling from a single image works best.- The proposed fine-tuning method is shown to achieve better robustness, measured by accuracy on out-of-distribution datasets, than previous methods like WiSE-FT and Model Soup, while maintaining competitive in-distribution accuracy.In summary, the key contribution is developing a novel fine-tuning approach that leverages masked images as counterfactual samples to improve model robustness, guided by a causal analysis of the robustness degradation problem. The effectiveness is validated by comprehensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using masked images as counterfactual samples during fine-tuning to improve robustness to distribution shifts, by breaking spurious correlations between semantic and non-semantic factors in the data and preserving generalizable knowledge from the pre-trained model.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on improving out-of-distribution robustness in fine-tuning vision models:- This paper takes a novel causal perspective on analyzing the issue of robustness degradation in fine-tuning. Previous works like LP-FT, WiSE-FT, and Model Soup have focused more on constraining model deviation from the pre-trained weights during fine-tuning. The causal analysis provides new insights into the problem.- The idea of using masked images as counterfactual samples during fine-tuning is innovative. Other recent works using masked images like MAE and MaskFeat focused on self-supervised pre-training rather than transfer learning. The proposed strategies for generating masked images tailored for improving robustness are not explored before.- The experiments demonstrate superior OOD robustness over previous state-of-the-art methods like WiSE-FT and Model Soup on benchmark datasets, without relying on model ensembles or complex training procedures. This validates the efficacy of the proposed approach.- However, the improvements on certain OOD datasets are smaller compared to WiSE-FT. As analyzed in the paper, the two methods may improve different aspects of robustness. Integrating WiSE-FT into the proposed approach leads to only minor further improvements.- The causal perspective of modeling the image generation process is interesting but remains conceptual. More rigorous causal modeling and learning algorithms conforming to the proposed perspective could be promising future directions.In summary, this paper presents a novel causal view and counterfactual learning approach for improving OOD robustness in fine-tuning, complementing other directions like weight constraint methods. The results demonstrate the efficacy of the proposed masked image training, while potential limitations are analyzed to motivate future works. More rigorous causal modeling and learning algorithms may better unlock the potential of this perspective.
