# [Masked Images Are Counterfactual Samples for Robust Fine-tuning](https://arxiv.org/abs/2303.03052)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the robustness of fine-tuned vision models to out-of-distribution (OOD) data while maintaining good performance on in-distribution data?The key hypothesis is that using masked images as counterfactual training samples can help preserve the OOD robustness of large pre-trained vision models when fine-tuning them on downstream tasks. In particular, the paper proposes:- Masking semantics-related or unrelated image regions based on class activation maps to break spurious correlations in the training data.- Refilling the masked regions with patches from other images to further distort the original semantics.- Using the resulting masked counterfactual images for feature-based distillation with the pre-trained model during fine-tuning.The central hypothesis is that training the fine-tuned model to mimic the pre-trained model on these counterfactual samples will help retain robustness to OOD data while improving in-distribution performance. Experiments on ImageNet fine-tuning validate this hypothesis and show improved OOD accuracy over prior methods.In summary, the key research question is how to improve robustness to OOD data in fine-tuning vision models, and the central hypothesis is using counterfactual masked images can achieve this goal. The experiments aim to validate the effectiveness of the proposed approach.
