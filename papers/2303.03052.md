# [Masked Images Are Counterfactual Samples for Robust Fine-tuning](https://arxiv.org/abs/2303.03052)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the robustness of fine-tuned vision models to out-of-distribution (OOD) data while maintaining good performance on in-distribution data?The key hypothesis is that using masked images as counterfactual training samples can help preserve the OOD robustness of large pre-trained vision models when fine-tuning them on downstream tasks. In particular, the paper proposes:- Masking semantics-related or unrelated image regions based on class activation maps to break spurious correlations in the training data.- Refilling the masked regions with patches from other images to further distort the original semantics.- Using the resulting masked counterfactual images for feature-based distillation with the pre-trained model during fine-tuning.The central hypothesis is that training the fine-tuned model to mimic the pre-trained model on these counterfactual samples will help retain robustness to OOD data while improving in-distribution performance. Experiments on ImageNet fine-tuning validate this hypothesis and show improved OOD accuracy over prior methods.In summary, the key research question is how to improve robustness to OOD data in fine-tuning vision models, and the central hypothesis is using counterfactual masked images can achieve this goal. The experiments aim to validate the effectiveness of the proposed approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel fine-tuning method to improve the robustness of vision models using masked images as counterfactual samples. Specifically:- The paper analyzes the issue of robustness degradation in fine-tuning from a causal perspective. It models the image generation process with a structural causal model and finds that fine-tuning tends to learn spurious correlations between semantic and non-semantic factors, which harms robustness. - To address this issue, the paper proposes to construct counterfactual images by masking and refilling patches based on class activation maps. These images help break the spurious correlations and regularize fine-tuning.- The paper investigates different strategies for masking (random, object-focused, context-focused) and refilling (no refill, single image, multiple images). Experiments show masking object-focused regions and refilling from a single image works best.- The proposed fine-tuning method is shown to achieve better robustness, measured by accuracy on out-of-distribution datasets, than previous methods like WiSE-FT and Model Soup, while maintaining competitive in-distribution accuracy.In summary, the key contribution is developing a novel fine-tuning approach that leverages masked images as counterfactual samples to improve model robustness, guided by a causal analysis of the robustness degradation problem. The effectiveness is validated by comprehensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using masked images as counterfactual samples during fine-tuning to improve robustness to distribution shifts, by breaking spurious correlations between semantic and non-semantic factors in the data and preserving generalizable knowledge from the pre-trained model.
