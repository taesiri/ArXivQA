# [Efficient Knowledge Deletion from Trained Models through Layer-wise   Partial Machine Unlearning](https://arxiv.org/abs/2403.07611)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Existing machine unlearning techniques face several practical challenges, including performance degradation after unlearning, demand for brief fine-tuning, and significant storage requirements. Specifically, approximate unlearning methods like amnesiac unlearning cause noticeable drops in model performance on retained data. Retraining-based methods like SISA require substantial computational resources. 

Proposed Solution: This paper introduces two novel classes of efficient approximate machine unlearning algorithms to address these limitations:

1. Partial Amnesiac Unlearning: Integrates layer-wise pruning into conventional amnesiac unlearning. During training, it prunes a portion of the model updates from each layer and stores them. During unlearning, it subtracts only the pruned updates made by batches containing the target data. This minimizes negative impact on retained data while erasing target data. Also reduces storage needs.

2. Layer-wise Partial Updates: Incorporates layer-wise partial updates into label-flipping and optimization-based unlearning methods. Updates only a fraction of parameters in each layer, prioritizing layers appropriately. Mitigates adverse effects on retained data representations while erasing target data.  

Main Contributions:

- Proposed partial amnesiac unlearning eliminates need for fine-tuning after unlearning, unlike conventional amnesiac unlearning, while reducing storage needs. Showed superior performance.

- Demonstrated layer-wise partial updates in label-flipping and optimization-based unlearning mitigates negative impact on retained classes unlike their naive versions.

- Extensive evaluations over diverse datasets and neural network architectures to demonstrate effectiveness of proposed methods compared to counterparts.

The key insight is strategically updating only a fraction of parameters during unlearning protects retained data representations while erasing target data knowledge. The proposed techniques offer efficient approximate machine unlearning.
