# [Accelerated Inference and Reduced Forgetting: The Dual Benefits of   Early-Exit Networks in Continual Learning](https://arxiv.org/abs/2403.07404)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Early-exit networks have been proposed to reduce inference time and improve efficiency by making predictions early in the network. However, they have only been developed for stationary data distributions.
- Continual learning deals with non-stationary data streams and the key challenge is catastrophic forgetting of old tasks.
- This paper explores continual learning of early-exit networks to benefit from their efficiency as well as mitigate forgetting.

Methods:
- The paper adapts several continual learning methods like LwF, ER, BiC, iCaRL to early-exit networks by attaching internal classifiers with feature reduction layers. 
- It analyzes forgetting and overthinking in continual early-exit networks. Forgetting is less severe in early classifiers. Overthinking is more detrimental in continual learning.
- A task-recency bias is identified in class incremental learning that reduces confidence of early predictions from old classes. 

Proposed Solution:
- A new method called Task-wise Logits Correction (TLC) is proposed to compensate for the task-recency bias. 
- TLC models a correction factor for each task's logits to equalize confidence levels across tasks. This allows earlier exit for old task data.

Experiments:
- Comprehensive analysis of adapted continual learning methods with early exits on CIFAR100, TinyImageNet and ImageNetSubset benchmarks.
- TLC further improves efficiency of early-exit networks, matching accuracy of standard networks with <70% compute.
- At full compute budget, TLC outperforms standard methods by up to 15 percentage points.

Contributions:
- First analysis of continual learning in early-exit networks showing reduced forgetting in early classifiers.
- Identification and mitigation of task-recency bias limiting early exit for old tasks.
- Extensive experiments proving efficiency and accuracy gains from using early exits in continual learning.

In summary, the paper demonstrates the synergy between early-exit networks and continual learning, making a case for their practical utility in environments with non-stationary data and computational constraints.
