# [NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning](https://arxiv.org/abs/2402.14139)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Efficient on-device convolutional neural network (CNN) training is challenging for resource-constrained mobile and edge devices. The standard backpropagation approach is memory intensive as it requires retaining intermediate activations across the CNN layers for gradient calculations. This necessitates using smaller batch sizes to fit within memory budgets, increasing overall training time. Currently, there are no training approaches that can achieve high accuracy under tight memory constraints.

Proposed Solution: 
The paper proposes \accordion{}, a novel CNN training system tailored for memory-constrained environments. The key ideas are:

1) Adaptive auxiliary networks - Assign variable number of filters to auxiliary networks based on layer type to reduce memory usage while maintaining accuracy.  

2) Adaptive batch sizes - Dynamically adjust batch size for each layer or layer groups based on their memory needs to optimize memory utilization.

3) Block-based learning - Partition CNN into blocks and only keep active block in memory during training to meet memory budget.

4) Caching and prefetching - Cache activations from trained blocks and use as inputs for next blocks to eliminate redundant forward passes and accelerate training.

Main Contributions:

1) \accordion{} system that enables on-device CNN training under tight memory budgets not possible with backpropagation.

2) Adaptive strategies for local learning - adaptive auxiliary networks and batch sizes to improve memory efficiency and training speed.  

3) Up to 6.1x faster training over backpropagation and 10.3x over vanilla local learning for the same memory budget.

4) Generates up to 29.4x smaller models without loss of accuracy. Improves inference throughput by up to 3.95x.

In summary, \accordion{} offers a disruptive memory-efficient on-device CNN training solution by using adaptive local learning techniques. It shows consistent improvements in training speed, accuracy and model compression over state-of-the-art.
