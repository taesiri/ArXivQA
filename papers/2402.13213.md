# [Softmax Probabilities (Mostly) Predict Large Language Model Correctness   on Multiple-Choice Q&amp;A](https://arxiv.org/abs/2402.13213)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can sometimes generate false yet convincing responses, which could be harmful. A key challenge is enabling LLMs to decline to answer when they encounter unfamiliar situations, rather than confidently providing misleading information.

- Prior work has shown that for image classification models, incorrect predictions tend to have lower maximum softmax probabilities (MSPs) than correct predictions. This paper investigates whether a similar phenomenon occurs for LLMs on question answering tasks.

Method:
- The authors comprehensively evaluate the relationship between MSPs/maximum logits and response correctness across 10 popular open-source LLMs on 5 multiple-choice QA datasets. 

- They use a simple zero-shot prompting approach and grade the LLMs' responses as correct or wrong. For each LLM-dataset pair, they train a classifier to predict response correctness based only on the MSP or maximum logit.

Key Findings:
- For the 6 best-performing LLMs, the classifiers significantly outperformed random guessing in 59/60 cases based on MSP and 58/60 cases based on maximum logit. The average AUROCs ranged from 60-69%, showing a clear correlation.

- However, for the 4 LLMs with weaker QA accuracy, the classifiers did not reliably beat random chance. This suggests the relationship relies on some degree of underlying competency at the QA task.

- As a proof of concept, the authors add an abstention option to the QA task and show that selective abstention based on uncertainty signals can reduce wrong answers without giving up many right answers.

Main Contributions:
- First comprehensive study showing that for competent LLMs, MSPs and maximum logits contain useful signals for identifying incorrect answers to multiple choice questions.

- Demonstration that these signals can be leveraged to improve performance by abstaining selectively.

- The findings suggest that uncertainty information from model probabilities and logits could be an important tool for averting potential LLM harms. The simple core idea of detecting unfamiliar situations warrants further investigation.


## Summarize the paper in one sentence.

 This paper comprehensively evaluates whether wrong answers from large language models on multiple-choice question answering tasks are associated with lower maximum softmax probabilities, finds strong evidence for this hypothesis, and shows how it can be leveraged to improve performance by abstaining on uncertain questions.


## What is the main contribution of this paper?

 The main contribution of this paper is showing that the maximum softmax probability (MSP) and maximum logit of a large language model's response on multiple-choice question answering tasks contain useful signals for predicting whether the response is correct or incorrect. Specifically:

- The authors comprehensively evaluate the ability of MSP and maximum logits to predict correctness on 10 popular open-source LLMs across 5 multiple-choice QA datasets. 

- They find strong evidence that for models with good QA performance, higher MSPs and maximum logits are associated with correct answers compared to incorrect ones. The AUROC is significantly better than random chance.

- They also show a proof-of-concept that selectively abstaining from answering based on low MSPs/logits can improve performance on a QA task with an option to abstain.

In summary, the key contribution is demonstrating the viability and importance of using uncertainty signals like MSPs and logits to help detect when LLMs are likely to generate incorrect responses on QA tasks. This can be leveraged to build more robust LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the key terms and keywords associated with this paper are:

large language models, question answering, uncertainty quantification, out-of-distribution detection, softmax probabilities, maximum softmax probabilities, maximum logits, area under ROC curve, abstention, misgeneralization


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper showed that the maximum softmax probability (MSP) and maximum logit contain useful signals for predicting answer correctness on multiple choice QA tasks. Do you think this would generalize to open-ended QA or other NLP tasks like summarization or translation? Why or why not?

2. For the "option to abstain" experiments, how sensitive are the results to the choice of scoring scheme penalties (currently -1 or -2 points per wrong answer)? Have you experimented with a wider range of penalties?

3. You currently select a single threshold for abstaining per model by taking the average best threshold across datasets/prompts. Have you considered instead selecting thresholds on a per-dataset or even per-question basis to account for dataset/question variability? 

4. The WinoGrande dataset was substantially harder for the MSP/logit classifiers. Do you have any hypotheses for why this was the case? Are there certain types of questions or datasets where we'd expect MSP/logits to be less predictive?

5. You show that model calibration (i.e. predicted probabilities matching empirical probabilities) is not necessary for the uncertainty signals to be useful. However, do you think improving calibration could further boost the performance of the proposed method?

6. For the "option to abstain" experiments, when the model abstains what response if any is shown to the user? Could the way this abstention is presented impact user trust or satisfaction?

7. You use greedy decoding here. Have you experimented with different decoding methods like beam search? Could decoding method interact with the usefulness of the MSP/logit signals?

8. What other types of uncertainty signals beyond MSP/logits could be incorporated? For example variance of the softmax distribution or Monte Carlo dropout. How would you expect those to compare or potentially complement the current approach?

9. What theoretical explanations might account for why MSP/logits are predictive of answer correctness? Is there something about the model, data, or task that induces this correlation?

10. How well does the relationship between uncertainty signals and correctness hold up when evaluated adversarially, for example by trying to construct questions aimed at fooling the uncertainty estimates?
