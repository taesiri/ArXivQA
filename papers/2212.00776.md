# [ResFormer: Scaling ViTs with Multi-Resolution Training](https://arxiv.org/abs/2212.00776)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we enable vision transformers (ViTs) to handle varying input resolutions effectively, so they can generalize well to resolutions not seen during training?The key points are:- ViTs currently suffer from poor resolution scalability - their performance drops significantly when evaluated on resolutions different from what they were trained on. - The paper proposes a framework called ResFormer to improve ViTs' ability to handle a wide range of resolutions.- The main ideas are:  - Train on multi-resolution images to model objects at different scales.  - Use a scale consistency loss to share information across resolutions.  - Introduce global-local positional embeddings that change smoothly across resolutions.- Experiments validate ResFormer's ability to generalize to both lower and higher resolutions than seen during training. It significantly outperforms baseline ViTs like DeiT when evaluated on novel resolutions.In summary, the central hypothesis is that training ViTs on multi-resolution images with scale consistency losses and adjustable positional embeddings will improve their generalization across a diverse range of input resolutions. The paper presents ResFormer as a method to achieve this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a vision transformer framework called ResFormer that achieves strong performance across a wide range of input image resolutions. The key ideas are:- Multi-resolution training - The model is trained on images of multiple resolutions (e.g. 128, 160, 224) in each minibatch to improve generalization to different resolutions.- Scale consistency loss - A knowledge distillation loss is applied between feature maps of different resolutions to encourage consistency and transfer knowledge from higher to lower resolutions.- Global-local positional embeddings - A combination of global (absolute) and local (relative) positional embeddings is used to enable the model to smoothly adapt to different resolutions, especially novel unseen ones.In experiments, ResFormer shows much better cross-resolution generalization ability compared to baseline DeiT models. For example, ResFormer-S achieves over 50% higher accuracy at low resolution 96x96 and over 5% higher at high resolution 640x640. The framework is also shown to be flexible, adapting well to semantic segmentation, object detection and video action recognition tasks.In summary, the main contribution is proposing techniques to make vision transformers more robust to varying input resolutions, an important capability for real-world applications. The results demonstrate ResFormer's excellent performance across a wide spectrum of resolutions compared to prior ViT models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point of the paper:The paper proposes ResFormer, a vision transformer framework that trains on multi-resolution images and uses global-local positional embeddings to achieve strong performance on a wide range of input resolutions, including both smaller and larger resolutions than seen during training.
