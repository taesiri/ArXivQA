# [Extreme Miscalibration and the Illusion of Adversarial Robustness](https://arxiv.org/abs/2402.17509)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- NLP models are vulnerable to adversarial attacks where small perturbations to the input can cause misclassification. Adversarial training (AT) methods have been proposed to increase robustness.
- However, the authors argue that some AT methods can accidentally or intentionally create highly miscalibrated models that appear more robust by disrupting attack search methods. This gives an illusion of robustness (IOR) rather than true robustness. 

Proposed Solution: 
- The authors show how to explicitly create IORs by temperature scaling during inference to make models overconfident or underconfident. 
- They also show AT methods that use gradient normalization can implicitly make models overconfident, disrupting attacks. 
- To mitigate IORs, they propose adversarial temperature optimization and calibration to rescale confidences so attacks can succeed. This reveals the lack of true robustness.
- Finally, they show genuinely increasing robustness by raising temperature during training, not inference, to force the model to learn to separate classes more.

Main Contributions:
- Identify and demonstrate existence of illusion of robustness (IOR) in NLP models
- Show IOR arises due to model miscalibration disrupting attack search
- Propose adversarial temperature scaling to pierce IOR 
- Demonstrate high temperature training improves genuine robustness to unseen attacks
- Urge community to use temperature scaling when evaluating robustness to ensure gains are real not illusions

In summary, the key insight is that extreme model calibration can mask gradients and fake robustness gains. The solutions identify this weakness and improve true adversarial robustness.
