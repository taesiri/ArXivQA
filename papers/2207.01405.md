# [I-ViT: Integer-only Quantization for Efficient Vision Transformer   Inference](https://arxiv.org/abs/2207.01405)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can vision transformers (ViTs) be quantized to low-precision integer values in a way that allows for efficient integer-only inference without loss of accuracy? 

Specifically, the paper proposes I-ViT, an integer-only quantization scheme for ViTs that enables the entire computational graph to be performed with integer arithmetic and bit-shifting, without any floating-point operations. This allows the quantized ViT models to benefit from fast integer arithmetic units in hardware for efficient inference. 

The key challenge addressed is that prior integer-only quantization techniques like dyadic arithmetic rely on assumptions like homogeneity that do not hold for the non-linear operations in ViTs like softmax, GELU, and layer norm. So the main contribution is proposing lightweight integer approximation methods like Shiftmax, ShiftGELU, and I-LayerNorm that can accurately approximate these non-linear operations using primarily bit-shifting.

In summary, the central hypothesis is that ViTs can be quantized to 8-bit integer-only representations without accuracy loss through tailored lightweight integer approximations of non-linear operations, enabling significant efficiency gains via integer-only arithmetic during inference. The paper aims to demonstrate this via the proposed I-ViT scheme.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing I-ViT, an integer-only quantization scheme for Vision Transformers (ViTs) that enables efficient integer-only inference. Specifically:

- I-ViT quantizes the entire computational graph of ViTs, including both linear and non-linear operations, to low-precision integer values. This allows the model to perform inference using only integer arithmetic and bit-shifting, without any floating-point operations. 

- For linear operations like MatMul and Dense, I-ViT applies dyadic arithmetic which uses bit-shifting to realize scaling by power-of-two factors. This follows previous work on quantizing CNNs.

- For non-linear operations like Softmax, GELU, and LayerNorm, the authors propose novel lightweight approximations named Shiftmax, ShiftGELU, and I-LayerNorm to enable accurate integer-only arithmetic. These primarily use bit-shifting operations.

- Experiments show I-ViT achieves similar or slightly higher accuracy compared to full-precision models on ImageNet classification. When deployed on a GPU using TVM, I-ViT accelerates inference by 3.72-4.11x compared to the floating-point baseline.

In summary, the key contribution is developing efficient integer approximations to quantize the entire ViT model while maintaining accuracy, enabling fast low-precision integer-only inference on hardware. This is the first work to investigate integer-only quantization for ViTs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes an integer-only quantization scheme called I-ViT for Vision Transformers that enables efficient integer-only inference by using bit-shifting approximations for non-linear operations like Softmax, GELU, and LayerNorm.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other research in integer-only quantization of vision transformers:

- This is the first work to propose an end-to-end integer-only quantization scheme specifically designed for vision transformers (ViTs). Previous methods like I-BERT focused on language models, while approaches like FQ-ViT only quantized certain operations in ViTs. 

- The proposed methods for quantizing non-linear operations like Softmax, GELU and LayerNorm are novel and tailored for ViTs, using efficient bit-shifting operations. Prior arts used more complex polynomial approximations or methods unsuitable for ViTs.

- Experiments demonstrate I-ViT maintains accuracy very close to FP models when quantized to 8-bit integer, even slightly outperforming them on some models. Many previous int-only quantization works have larger accuracy drops.

- Hardware results show significant 3.7-4.1x speedups on a GPU by leveraging integer Tensor Cores, much higher than prior works like FasterTransformer that keep float operations. This verifies the efficiency benefits of end-to-end integer quantization.

- The work focuses on image classification tasks with standard ViT models. It remains to be seen if the techniques readily apply to more complex vision tasks like detection and segmentation, and advanced ViT models.

Overall, this paper presents very promising results on enabling efficient int-only inference for ViTs, advancing the state-of-the-art in model quantization. The novel methods for quantizing operations like Softmax and GELU can inspire more work on efficient low-bit quantization for transformers.
