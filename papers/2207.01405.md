# [I-ViT: Integer-only Quantization for Efficient Vision Transformer   Inference](https://arxiv.org/abs/2207.01405)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can vision transformers (ViTs) be quantized to low-precision integer values in a way that allows for efficient integer-only inference without loss of accuracy? 

Specifically, the paper proposes I-ViT, an integer-only quantization scheme for ViTs that enables the entire computational graph to be performed with integer arithmetic and bit-shifting, without any floating-point operations. This allows the quantized ViT models to benefit from fast integer arithmetic units in hardware for efficient inference. 

The key challenge addressed is that prior integer-only quantization techniques like dyadic arithmetic rely on assumptions like homogeneity that do not hold for the non-linear operations in ViTs like softmax, GELU, and layer norm. So the main contribution is proposing lightweight integer approximation methods like Shiftmax, ShiftGELU, and I-LayerNorm that can accurately approximate these non-linear operations using primarily bit-shifting.

In summary, the central hypothesis is that ViTs can be quantized to 8-bit integer-only representations without accuracy loss through tailored lightweight integer approximations of non-linear operations, enabling significant efficiency gains via integer-only arithmetic during inference. The paper aims to demonstrate this via the proposed I-ViT scheme.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing I-ViT, an integer-only quantization scheme for Vision Transformers (ViTs) that enables efficient integer-only inference. Specifically:

- I-ViT quantizes the entire computational graph of ViTs, including both linear and non-linear operations, to low-precision integer values. This allows the model to perform inference using only integer arithmetic and bit-shifting, without any floating-point operations. 

- For linear operations like MatMul and Dense, I-ViT applies dyadic arithmetic which uses bit-shifting to realize scaling by power-of-two factors. This follows previous work on quantizing CNNs.

- For non-linear operations like Softmax, GELU, and LayerNorm, the authors propose novel lightweight approximations named Shiftmax, ShiftGELU, and I-LayerNorm to enable accurate integer-only arithmetic. These primarily use bit-shifting operations.

- Experiments show I-ViT achieves similar or slightly higher accuracy compared to full-precision models on ImageNet classification. When deployed on a GPU using TVM, I-ViT accelerates inference by 3.72-4.11x compared to the floating-point baseline.

In summary, the key contribution is developing efficient integer approximations to quantize the entire ViT model while maintaining accuracy, enabling fast low-precision integer-only inference on hardware. This is the first work to investigate integer-only quantization for ViTs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes an integer-only quantization scheme called I-ViT for Vision Transformers that enables efficient integer-only inference by using bit-shifting approximations for non-linear operations like Softmax, GELU, and LayerNorm.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other research in integer-only quantization of vision transformers:

- This is the first work to propose an end-to-end integer-only quantization scheme specifically designed for vision transformers (ViTs). Previous methods like I-BERT focused on language models, while approaches like FQ-ViT only quantized certain operations in ViTs. 

- The proposed methods for quantizing non-linear operations like Softmax, GELU and LayerNorm are novel and tailored for ViTs, using efficient bit-shifting operations. Prior arts used more complex polynomial approximations or methods unsuitable for ViTs.

- Experiments demonstrate I-ViT maintains accuracy very close to FP models when quantized to 8-bit integer, even slightly outperforming them on some models. Many previous int-only quantization works have larger accuracy drops.

- Hardware results show significant 3.7-4.1x speedups on a GPU by leveraging integer Tensor Cores, much higher than prior works like FasterTransformer that keep float operations. This verifies the efficiency benefits of end-to-end integer quantization.

- The work focuses on image classification tasks with standard ViT models. It remains to be seen if the techniques readily apply to more complex vision tasks like detection and segmentation, and advanced ViT models.

Overall, this paper presents very promising results on enabling efficient int-only inference for ViTs, advancing the state-of-the-art in model quantization. The novel methods for quantizing operations like Softmax and GELU can inspire more work on efficient low-bit quantization for transformers.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Deploying I-ViT on dedicated integer-only hardware (e.g. FPGAs) to further enhance the acceleration potential. The paper notes that the software and hardware support on GPUs like RTX 2080Ti used in their experiments is not fully optimized for integer-only inference. Dedicated hardware can better leverage the benefits of I-ViT's integer-only design.

- Extending I-ViT to more complex vision tasks beyond image classification, such as object detection and semantic segmentation. The paper only evaluates I-ViT for image classification on ImageNet dataset. Applying it to other vision applications can demonstrate the wider applicability.

- Exploring more advanced quantization techniques beyond the simple symmetric uniform quantization used in this work. The paper notes they use a basic quantization method and more sophisticated techniques like non-uniform quantization could provide further improvements.

- Comparing resource and power consumption between the integer-only and floating-point models more fairly. The paper mainly evaluates latency but does not provide detailed resource or power comparisons. This requires profiling the occupancy of integer vs floating-point units in hardware.

- Improving software and hardware support for integer-only inference on GPUs. The paper discusses limitations of current GPU software and hardware for optimal integer-only deployment, suggesting this is an area of future work.

In summary, the key future directions are deploying I-ViT on dedicated integer-only hardware, extending it to more vision tasks, exploring more advanced quantization methods, performing more detailed resource/power evaluation, and improving software and hardware support for integer-only inference on GPUs. The overall goal is to further demonstrate and leverage the benefits of the integer-only quantization approach proposed in I-ViT.


## Summarize the paper in one paragraph.

 This paper presents I-ViT, an integer-only quantization scheme for Vision Transformers (ViTs) to enable efficient integer-only inference. The key ideas are:

- Linear operations (e.g. MatMul, Dense) follow the dyadic arithmetic pipeline for integer-only calculation. 

- Novel lightweight integer approximations are proposed for non-linear operations: Shiftmax and ShiftGELU use efficient bit-shifting to approximate Softmax and GELU; I-LayerNorm calculates square root with integer iterations.

- On ImageNet, I-ViT achieves comparable or slightly higher accuracy compared to full precision models. When deployed on a GPU using TVM, I-ViT provides 3.72-4.11x speedup by utilizing integer arithmetic units.

In summary, I-ViT is the first work to enable integer-only inference for the entire computational graph of ViTs. The proposed lightweight integer approximations for non-linear operations are efficient and accurate. I-ViT demonstrates the feasibility and advantage of integer-only quantization for efficient ViT inference.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes I-ViT, a novel integer-only quantization scheme for Vision Transformers (ViTs) that enables efficient integer-only inference. ViTs have achieved superior performance on computer vision tasks but suffer from high computational complexity, making deployment on edge devices challenging. Quantization is a promising approach to reduce model complexity, but prior methods either perform partial operations with integers or rely on inefficient approximations for non-linear operations like Softmax and GELU. 

I-ViT addresses these limitations by quantizing the entire model graph. Linear operations like MatMul follow dyadic arithmetic for integer-only calculation. For non-linear operations, the paper proposes novel lightweight approximations called Shiftmax and ShiftGELU that rely primarily on bit shifting. This allows full integer-only inference without floating point operations. Experiments show I-ViT achieves similar or higher accuracy compared to floating point models on ImageNet classification. The authors use TVM to deploy I-ViT on a GPU, leveraging integer Tensor Cores to achieve 3.7-4.1x speedup over baseline. I-ViT provides an effective quantization scheme for efficient ViT inference on resource-constrained hardware.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes an integer-only quantization scheme called I-ViT to enable efficient inference of vision transformers (ViTs). The key ideas are:

1) Linear operations like MatMul and Dense follow the dyadic arithmetic pipeline to perform integer-only arithmetic. This pipeline uses integer scaling and bit shifting to avoid floating point operations. 

2) For non-linear operations like Softmax, GELU, and LayerNorm, novel lightweight integer approximations are proposed:

- Shiftmax uses integer bit shifting to approximate the exponential in Softmax. 

- ShiftGELU similarly uses bit shifting to approximate the sigmoid function needed for GELU.

- I-LayerNorm calculates the square root needed for LayerNorm with an integer iterative method.

In summary, the proposed I-ViT scheme quantizes the entire computation graph of ViTs to low-precision integers and enables efficient integer-only inference using bit shifting approximations for non-linear operations. Experiments show I-ViT achieves similar accuracy as floating point models while accelerating inference 3.7-4.1x on a GPU.
