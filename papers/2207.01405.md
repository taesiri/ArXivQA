# [I-ViT: Integer-only Quantization for Efficient Vision Transformer   Inference](https://arxiv.org/abs/2207.01405)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can vision transformers (ViTs) be quantized to low-precision integer values in a way that allows for efficient integer-only inference without loss of accuracy? Specifically, the paper proposes I-ViT, an integer-only quantization scheme for ViTs that enables the entire computational graph to be performed with integer arithmetic and bit-shifting, without any floating-point operations. This allows the quantized ViT models to benefit from fast integer arithmetic units in hardware for efficient inference. The key challenge addressed is that prior integer-only quantization techniques like dyadic arithmetic rely on assumptions like homogeneity that do not hold for the non-linear operations in ViTs like softmax, GELU, and layer norm. So the main contribution is proposing lightweight integer approximation methods like Shiftmax, ShiftGELU, and I-LayerNorm that can accurately approximate these non-linear operations using primarily bit-shifting.In summary, the central hypothesis is that ViTs can be quantized to 8-bit integer-only representations without accuracy loss through tailored lightweight integer approximations of non-linear operations, enabling significant efficiency gains via integer-only arithmetic during inference. The paper aims to demonstrate this via the proposed I-ViT scheme.
