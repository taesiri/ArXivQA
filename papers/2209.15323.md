# [SmallCap: Lightweight Image Captioning Prompted with Retrieval   Augmentation](https://arxiv.org/abs/2209.15323)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the key research question seems to be: 

How can retrieval-based prompting be used to create an efficient and lightweight image captioning model that can effectively leverage large amounts of unlabeled text data?

The authors propose a model called SmallCap which uses a prompt containing a few retrieved captions to condition the generation of a new caption for a given image. The key ideas explored in the paper are:

- Using retrieval to provide relevant captions as prompts allows the model itself to have very few trainable parameters, enabling lightweight and efficient training.

- The prompts provide useful conditioning context and knowledge, compensating for the small model size. 

- The retrieved captions come from an external datastore, which can be swapped or expanded without retraining the base model. This allows leveraging large unlabeled datasets in a training-free manner.

- The model can transfer to new domains/datasets simply by changing the contents of the datastore, without needing finetuning.

So in summary, the central hypothesis is that retrieval-based prompting is an effective technique to create lightweight yet high-performing image captioning models that can exploit large unlabeled data in a training-free way and easily transfer across domains. The SmallCap model is proposed to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposes SmallCap, a lightweight image captioning model that uses retrieval augmentation to compensate for its small size. SmallCap combines a frozen CLIP image encoder and GPT-2 text decoder, connected via a small number of trainable cross-attention layers. 

2. Shows that SmallCap can be trained efficiently using just 4 epochs on COCO, while achieving competitive performance compared to other lightweight captioning models. This is enabled by the retrieval augmentation, where relevant captions retrieved from a datastore are provided as prompt to the decoder.

3. Demonstrates that SmallCap can transfer to new domains without any finetuning, simply by changing the contents of the datastore at inference time. This allows exploiting new data, including target domain data or additional web/human labeled data, in a training-free manner.

4. Provides analysis showing the impact of different datasets in the datastore, model ablations, and comparisons to highlight the benefits of the proposed retrieval augmented approach.

In summary, the main contribution is an efficient and lightweight image captioning model that can leverage retrieval to compensate for its small size, enabling training efficiency, zero-shot domain transfer, and exploitation of external data without needing explicit finetuning.
