# VIMA: General Robot Manipulation with Multimodal Prompts

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is: Can we develop a single robot learning agent capable of solving a diverse set of manipulation tasks specified via multimodal prompts consisting of language, images, and videos?The authors propose a unified agent called VIMA that can process multimodal prompts and control a robot arm to accomplish various manipulation tasks. The key ideas and contributions are:- Formulating diverse robot tasks as multimodal prompting problems with language, images, and videos. This allows specifying tasks in a natural and flexible way.- Proposing VIMA, a transformer-based architecture to solve prompted robot tasks. It encodes prompts with a pretrained language model, and conditions a causal transformer decoder robot controller on prompts via cross-attention. - Demonstrating VIMA's ability to solve a diverse task suite spanning goal reaching, visual reasoning, imitation, etc. specified via multimodal prompts.- Systematically studying VIMA's generalization ability using multi-level evaluation protocols with increasing distribution shifts from training data.- Analyzing model scalability and data efficiency. VIMA shows strong scaling with model size and sample efficiency needing 10x less data than baselines.In summary, the central hypothesis is that a unified agent can solve a diverse set of robot manipulation tasks specified via multimodal prompts. The paper proposes VIMA as an embodiment of this idea and provides empirical evidence to validate the hypothesis.
