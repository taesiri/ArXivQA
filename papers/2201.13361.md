# [Signing the Supermask: Keep, Hide, Invert](https://arxiv.org/abs/2201.13361)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that augmenting the concept of "Supermasks" with the ability to invert the signs of weights, in addition to just pruning weights, can lead to very sparse neural network models that match or exceed the performance of standard dense models. The key ideas are:- Supermasks involve training neural networks by only selecting which initial random weights to keep, rather than adapting the weights through training. This can identify small subnetworks within larger networks.- This paper proposes "signed Supermasks" which additionally allow inverting the signs of weights (-1, 0, or 1) during Supermask training. - Using signed Supermasks and adapted weight initialization, they are able to prune up to 99% of weights in various CNN architectures on CIFAR-10/100 while matching or exceeding the performance of standard trained models.- This results in very sparse final models that can be stored more efficiently and may enable better interpretability. - The signed Supermask approach is able to uncover high-performing subnetworks with far fewer parameters compared to prior work on Supermasks.So in summary, the central hypothesis is that the proposed signed Supermask technique can lead to extremely sparse neural network models that still perform very well, enabling more efficient models with potential interpretability benefits. The experiments aim to demonstrate this across various architectures and datasets.


## What is the main contribution of this paper?

This paper introduces the concept of "signed Supermasks", which is an extension of the original Supermasks technique proposed by Zhou et al. (2019). The key contributions are:- It proposes signed Supermasks, which allow the mask to take on values of -1, 0 or 1 during training. This gives an extra degree of freedom compared to regular Supermasks where the mask is binary (0 or 1). - It adapts the weight initialization scheme (called ELUS) to account for the masking and the ELU activation function. - Experiments on MNIST and CIFAR-10 show that signed Supermask models can match or exceed the performance of baseline models and previous Supermask techniques, while using only 0.5-4% of the original parameters. For example, a Conv8 model achieves 80.91% accuracy on CIFAR-10 with only 1.17% of the original weights.- Analysis of the final mask values shows the technique is able to consistently find sparse network structures across different runs and architectures. This suggests it may be possible to identify particularly important neurons/connections.- The simplicity of the trained signed Supermask models (containing just -1, 0 or 1 per weight) means they can be stored more efficiently and may allow faster inference.- The concept is shown to work for both fully-connected and convolutional neural networks, as well as ResNets, demonstrating general applicability.In summary, signed Supermasks significantly extend Supermasks by allowing weight sign flipping, adapting the initialization, and finding extremely sparse subnetworks whose structure can be analyzed for further insights. The simplicity of the final models is also appealing.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces the concept of signed Supermasks, which extend previous work on Supermasks by allowing weights to be masked or sign-inverted during training, enabling the discovery of extremely sparse neural network subnetworks that match or exceed the performance of conventionally trained baselines while simplifying model structure.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- The concept of "signed Supermasks" builds directly on prior work on lottery tickets and Supermasks. The key novel contribution is incorporating the sign of weights into the Supermask training process. This is a natural extension of Supermasks that provides additional flexibility.- Compared to other pruning and model compression techniques like low-precision training, the goal here is more about finding sparse subnetworks rather than just compressing models. The paper emphasizes interpretability and analyzing which weights matter, rather than pure compression ratios.- The extreme sparsity levels achieved here (as low as 0.5% of original weights) are quite impressive compared to prior pruning work. The paper shows Supermasks can uncover very small but still performant subnetworks.- Using the ELU activation and proposing a matched initialization scheme is tailored to the Supermask approach. Other works on quantization/ternarization don't focus as much on initialization.- The paper examines how hyperparameters can further increase sparsity, which is less explored in other works that focus more on maintaining accuracy after compression. This provides insight into controlling the Supermask sparsity.- The visual analysis of the Supermask structures is interesting and not something I've seen much in other works. Analyzing the mask similarity across layers and runs provides some interpretability.- The comparison to related methods is fairly limited. More depth on how these Supermasks compare empirically to other pruning, quantization, and lottery ticket methods could further situate the contributions.In summary, the incorporation of weight signs, focus on sparsity and interpretability, impressive pruning levels, and analysis of the Supermask structure seem to be the main unique contributions compared to related works on compressing or pruning neural networks. More comparative evaluation could help situate the benefits and tradeoffs further.
