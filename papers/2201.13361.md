# [Signing the Supermask: Keep, Hide, Invert](https://arxiv.org/abs/2201.13361)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that augmenting the concept of "Supermasks" with the ability to invert the signs of weights, in addition to just pruning weights, can lead to very sparse neural network models that match or exceed the performance of standard dense models. The key ideas are:- Supermasks involve training neural networks by only selecting which initial random weights to keep, rather than adapting the weights through training. This can identify small subnetworks within larger networks.- This paper proposes "signed Supermasks" which additionally allow inverting the signs of weights (-1, 0, or 1) during Supermask training. - Using signed Supermasks and adapted weight initialization, they are able to prune up to 99% of weights in various CNN architectures on CIFAR-10/100 while matching or exceeding the performance of standard trained models.- This results in very sparse final models that can be stored more efficiently and may enable better interpretability. - The signed Supermask approach is able to uncover high-performing subnetworks with far fewer parameters compared to prior work on Supermasks.So in summary, the central hypothesis is that the proposed signed Supermask technique can lead to extremely sparse neural network models that still perform very well, enabling more efficient models with potential interpretability benefits. The experiments aim to demonstrate this across various architectures and datasets.
