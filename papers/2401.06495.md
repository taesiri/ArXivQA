# [An investigation of structures responsible for gender bias in BERT and   DistilBERT](https://arxiv.org/abs/2401.06495)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models like BERT have shown great performance but raise fairness concerns when used in sensitive applications. 
- It is unclear which components of BERT's neural architecture are responsible for encoding bias. Locating them could enable counter-measures.
- Model compression techniques like distillation are important for model deployment, but their impact on fairness is understudied.

Proposed Solution:
- Conduct a series of experiments on BERT and DistilBERT to identify components encoding bias.
- Fine-tune models on balanced and imbalanced datasets regarding gender to analyze differences.
- Compare attention scores and hidden states between models using divergence metrics. 
- Successively ablate each attention head at inference to quantify impact on bias.

Key Findings:
- Bias is not encoded in any specific layer for BERT or DistilBERT.
- Attention heads uniformly encode bias, except for underrepresented classes with high sensitivity attribute imbalance.
- The subset of heads encoding more bias changes when models are re-fine-tuned.  
- Bias production is more homogeneous across heads for the distilled DistilBERT model.

Main Contributions:
- First study locating components responsible for bias in BERT and DistilBERT
- Analysis of impact of data imbalance on component-level bias encoding
- Demonstration that distillation increases robustness to imbalance-related bias
- Recommendation to use DistilBERT cautiously for high class/attribute imbalance  

In summary, the paper provides valuable insights into how bias manifests in BERT and DistilBERT's neural architecture through a series of novel experiments and data imbalance analysis. Key contributions relate to locating bias encoding and showing increased robustness from model compression.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper investigates where gender bias is encoded in BERT and DistilBERT by analyzing attention heads and layers and shows bias is not isolated but DistilBERT is more robust to class and gender imbalance.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

1) Conducting a series of experiments to investigate where gender bias is encoded in BERT and DistilBERT models. Specifically:

- Comparing attention scores and hidden states between fair and unfair models to see if specific layers encode bias. The findings show that bias is not encoded in a particular layer.

- Successively ablating attention heads in the models and evaluating performance and fairness. The results show that heads produce sufficiently different representations to impact model fairness, but different heads are responsible when the model is retrained. 

2) Demonstrating that DistilBERT is more robust to class and gender imbalance than BERT. Specifically:

- The representations produced by DistilBERT's attention heads are more homogeneous compared to BERT in contexts of underrepresented classes and high gender imbalance. 

- Some of BERT's heads are fair while others are highly unfair in these contexts, while DistilBERT's heads behave more similarly.

So in summary, the main contribution is conducting experiments to locate where gender bias manifests in BERT and DistilBERT, and showing DistilBERT handles imbalance better.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Language Models - The paper studies pre-trained language models like BERT and DistilBERT.

- Fairness - A core focus of the paper is studying fairness and bias in language models.

- Imbalance - The paper looks at how class and gender imbalance in datasets impacts model fairness.

- Compression - DistilBERT is a compressed version of BERT created through knowledge distillation. The impact of compression on fairness is studied. 

- Bias - The paper aims to identify where gender bias originates from in BERT and DistilBERT.

- Attention - The multi-head attention mechanism of transformers is analyzed to determine if specific heads are biased.

- Layers - The paper analyzes if bias originates from specific layers of BERT and DistilBERT. 

- Hidden States - The evolution of hidden states during fine-tuning is studied using SVCCA distance.

- Head Ablation - Attention heads are ablated to determine their impact on model fairness.

So in summary, the key terms cover fairness, bias, model compression, as well as analyzing the internal workings of transformers like attention and layers.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes comparing attention scores and hidden states between fair and unfair models to identify where gender bias originates. What are some limitations of only analyzing attention and representations to understand model bias? Could other components like the classification head also contribute to bias?

2. The paper evaluates model fairness using equalized odds on gender. What are some other definitions of fairness that could be relevant to analyze here? How might the conclusions change if a different fairness metric was used?

3. The paper finds minimal differences in attention and representations between fair and unfair models. What are some potential explanations for this result? Could the models still be encoding bias in ways not captured by these analyses? 

4. The paper evaluates the impact of ablating different attention heads on model fairness. What are the limitations of evaluating heads independently rather than jointly removing multiple heads? Could interactions between heads play a role in producing bias?  

5. The paper finds distilled models are more robust to class and gender imbalance. What explanations could account for this? Does distillation act as a regularizer that helps mitigate bias? Or does the compression simply make fewer heads available to capture spurious correlations?

6. The analyses focus only on gender as a protected attribute. How might the presence of intersectional biases (e.g. gender and race combined) alter the conclusions? Would new experiment protocols need to be designed to study intersectionality?

7. The paper studies bias originating from pre-training and fine-tuning data. What about bias in the underlying architectures? Could Transformer self-attention itself induce gender stereotypes regardless of the data? 

8. The paper finds differences in head importance across fine-tuning runs. How consistent are the heads that produce bias? Is it the same subsets of heads each time or does this vary?

9. The paper studies two specific models. To what extent could the analysis protocol and conclusions generalize to other Transformer models besides BERT and DistilBERT? Do you expect consistent results?

10. The paper aims to understand origins of bias but doesn't propose interventions. If certain heads are found to propagation bias, how should models be altered? Should problematic heads be removed entirely? Or retrained?
