# [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that large language models (LLMs) learn coherent models of space and time, rather than just superficial statistics. Specifically, the authors test whether LLMs contain explicit linear representations of spatial and temporal features by probing the internal activations of models on various datasets of place names and event names with associated location and date metadata. They find evidence that LLMs do learn such structured representations of space and time that generalize across different types of entities and are fairly robust to changes in prompting.The key questions addressed in the paper are:- Do LLMs represent spatial and temporal information at all? If so, where internally? - Are the learned representations linear and unified across different entity types?- How sensitive are the representations to changes in prompting context?- Do the probes truly extract features used by the model versus just memorizing a mapping?- Can individual "space neurons" and "time neurons" be identified that encode these features?By providing positive answers to these questions through extensive experiments and analysis, the authors argue that modern LLMs acquire structured knowledge of fundamental dimensions like space and time. This supports the view that LLMs learn coherent world models, not just superficial statistics.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Showing that large language models (LLMs) learn linear representations of space and time across multiple scales. The authors construct spatial datasets at the world, US, and NYC level, as well as temporal datasets spanning centuries, decades, and years. Linear probes are able to predict spatial coordinates and timestamps from the internal activations of the LLM on these datasets.2. Demonstrating that these spatiotemporal representations are fairly robust to changes in prompting and unified across different types of entities like cities and landmarks. 3. Discovering individual "space neurons" and "time neurons" within the LLM that reliably encode spatial and temporal coordinates. This provides evidence that the model is actually using these learned spatiotemporal features.4. Arguing based on these findings that modern LLMs are acquiring structured knowledge about fundamental dimensions like space and time, rather than just superficial statistics. This supports the view that LLMs are learning coherent world models from the training data.In summary, the key contribution appears to be empirically demonstrating that LLMs learn linear representations of space and time that reflect real-world structure, supporting the world model hypothesis over the view that LLMs just capture shallow statistical correlations. The analysis methods and datasets developed seem generally useful for further probing the world knowledge acquired by LLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the key findings from the paper:The paper provides evidence that large language models learn linear representations of space and time that generalize across different types of entities, suggesting these models acquire structured knowledge about fundamental dimensions like geography and history.
