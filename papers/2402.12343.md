# [Emulated Disalignment: Safety Alignment for Large Language Models May   Backfire!](https://arxiv.org/abs/2402.12343)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 This paper explores the risks of open-sourcing safety-aligned large language models (LLMs). It introduces a framework called Emulated Disalignment (ED) that combines pre-trained and safety-aligned LLMs to generate harmful responses, without additional training.

The key idea is that safety alignment trains LLMs to produce helpful and harmless responses. This implicit rewarding of safe responses can be reversed to encourage harmfulness. Specifically, ED constructs a contrastive response distribution that boosts harmful tokens from the pre-trained model relative to the safety-aligned model.

Experiments using four model families (Llama, Mistral, Alpaca) on three datasets demonstrate that ED doubles the harmfulness of pre-trained models and significantly outperforms strong baselines. Further analysis reveals that safer alignment leads to greater harm from ED.

The findings suggest that open-sourcing safety-aligned models can still enable malicious usage. The ease of exploiting alignment for harm highlights the need to reconsider accessibility even for safeguarded LLMs. It also motivates more research into alignment techniques robust against such adversarial attacks.

In summary, this work exposes the latent risks persisting within safety-aligned LLMs. By revealing an inference-time attack, it argues for restricting access and improving defenses against the weaponization of language models.
