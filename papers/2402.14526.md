# [Balanced Data Sampling for Language Model Training with Clustering](https://arxiv.org/abs/2402.14526)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Data sampling strategy is important for training large language models (LLMs), but most models use simple random sampling which ignores the unbalanced distribution of data. This can lead to underfitting on rare samples and overfitting on common samples.

- Uniform sampling can help balance common and rare samples but leads to severe overfitting from repeating rare samples too much.

Proposed Solution - ClusterClip Sampling:  
- Uses data clustering to group semantically similar texts into clusters to reflect data distribution.

- Initially samples uniformly across clusters, upsampling rare clusters and downsampling common ones.

- As training progresses, applies a "clip" operation - caps the max repeats for any cluster to mitigate overfitting. Knocks out extremely oversampled clusters.  

Main Contributions:

- Proposes ClusterClip sampling that balances common/rare clusters and uses clipping to prevent overfitting. Significantly outperforms random and uniform sampling.

- Validates ClusterClip versatility - improves multiple models (Llama2, Mistral) on diverse datasets for both pretraining and fine-tuning across many language/reasoning tasks.

- Analyzes cluster-based sampling variants - all improve over random sampling, showing clustering helps guide better sampling. Clip op gives major gains by reducing overfitting. Training order (general-to-specific vs specific-to-general sampling) affects model specialization.

In summary, the paper presents ClusterClip, a novel and broadly effective data sampling technique for LLM training that leverages clustering and clipping to balance and improve learning.
