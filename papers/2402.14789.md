# [Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised   Learning](https://arxiv.org/abs/2402.14789)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Many self-supervised learning methods rely heavily on domain-specific knowledge and augmentations. This makes it difficult to extend these methods to new modalities. Methods based on masked modeling are more promising for domain agnostic self-supervised learning, but still require tailored domain-specific masking schemes. 

Proposed Solution: 
The paper proposes Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method for self-supervised learning. SMA trains an attention-based model using a masked modeling objective, while simultaneously learning to generate masks without any domain-specific assumptions. 

Key ideas:
- Leverage the model's own attention maps to identify and mask correlated tokens. Attention maps are shown to correspond to semantic regions.
- Mask highly correlated tokens identified from attention maps. Forcing the model to rely on unmasked context creates a more challenging prediction task.
- Use a query subsampling approximation technique to efficiently generate masks covering contiguous input regions.
- Demonstrate SMA on protein, chemistry, and particle physics datasets, achieving state-of-the-art without any domain knowledge.

Main Contributions:
1) Proposal of SMA, the first fully domain-agnostic self-supervised learning method based on masked modeling.
2) A method to automatically identify and mask semantic groups from attention maps without assumptions.
3) State-of-the-art self-supervised learning results on multiple scientific benchmarks using a single domain-agnostic approach.

The key insight is that meaningful masks for self-supervision can be learned from scratch based on the model's own attention representations, removing the need for manually designed domain-specific schemes. This makes the method widely applicable to any modality.
