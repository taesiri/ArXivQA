# [Visual Dependency Transformers: Dependency Tree Emerges from Reversed   Attention](https://arxiv.org/abs/2304.03282)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently induce visual dependencies and build hierarchies from images without human annotations. Specifically, the authors propose a new vision transformer model called Visual Dependency Transformers (DependencyViT) that can automatically capture dependencies between image patches and parse the image into a hierarchical structure in an unsupervised manner.The key hypotheses are:1) By reversing the attention mechanism in transformers, child nodes can be trained to attend to parent nodes, allowing information to flow bottom-up and a dependency tree to emerge. 2) The induced dependencies can enable efficient dynamic pooling, where less informative patches (leaf nodes) are merged into their parents without losing critical information. This allows building a lightweight DependencyViT-Lite model.3) The visual dependencies captured by DependencyViT, whether learned from weak supervision on ImageNet or self-supervision, can benefit downstream vision tasks like part/object segmentation, detection, and recognition.In summary, the central research question is how to induce visual dependencies and hierarchies unsupervisedly using reversed attention transformers, and the key hypotheses are that this approach enables efficient models and benefits various vision applications.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing Visual Dependency Transformers (DependencyViT) that can induce visual dependencies and build hierarchies from images without human annotations. This is achieved through a novel neural operator called reversed attention that can capture long-range dependencies between image patches.2. Formulating a dependency graph where child nodes send information to parent nodes following a probability distribution. This allows hierarchies and trees to emerge naturally from the reversed attention layers in an unsupervised manner.3. Introducing a lightweight model called DependencyViT-Lite that performs dynamic pooling based on the emerged dependency tree. Leaf nodes that rarely send messages are pruned without hindering performance. This reduces computational cost and memory footprint.4. Showing that DependencyViT works well on both self-supervised and weakly-supervised pretraining paradigms on ImageNet. It demonstrates effectiveness on 8 datasets across 5 tasks including part/saliency segmentation, recognition, and detection.5. Achieving state-of-the-art results on semantic segmentation, object detection, part segmentation and saliency detection through the visual dependency parsing capability of DependencyViT.In summary, the key contribution is proposing DependencyViT that can induce visual dependencies and hierarchies unsupervisedly via reversed attention, enabling tasks like part segmentation and achieving strong performance on various vision tasks. The dynamic pooling scheme also allows efficient lightweight models to be derived.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Visual Dependency Transformers (DependencyViT), a new vision transformer architecture that uses a novel reversed self-attention mechanism to induce visual dependencies and hierarchical structures from images in an unsupervised manner.
