# [Visual Dependency Transformers: Dependency Tree Emerges from Reversed   Attention](https://arxiv.org/abs/2304.03282)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently induce visual dependencies and build hierarchies from images without human annotations. Specifically, the authors propose a new vision transformer model called Visual Dependency Transformers (DependencyViT) that can automatically capture dependencies between image patches and parse the image into a hierarchical structure in an unsupervised manner.The key hypotheses are:1) By reversing the attention mechanism in transformers, child nodes can be trained to attend to parent nodes, allowing information to flow bottom-up and a dependency tree to emerge. 2) The induced dependencies can enable efficient dynamic pooling, where less informative patches (leaf nodes) are merged into their parents without losing critical information. This allows building a lightweight DependencyViT-Lite model.3) The visual dependencies captured by DependencyViT, whether learned from weak supervision on ImageNet or self-supervision, can benefit downstream vision tasks like part/object segmentation, detection, and recognition.In summary, the central research question is how to induce visual dependencies and hierarchies unsupervisedly using reversed attention transformers, and the key hypotheses are that this approach enables efficient models and benefits various vision applications.
