# [Visual Dependency Transformers: Dependency Tree Emerges from Reversed   Attention](https://arxiv.org/abs/2304.03282)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently induce visual dependencies and build hierarchies from images without human annotations. Specifically, the authors propose a new vision transformer model called Visual Dependency Transformers (DependencyViT) that can automatically capture dependencies between image patches and parse the image into a hierarchical structure in an unsupervised manner.The key hypotheses are:1) By reversing the attention mechanism in transformers, child nodes can be trained to attend to parent nodes, allowing information to flow bottom-up and a dependency tree to emerge. 2) The induced dependencies can enable efficient dynamic pooling, where less informative patches (leaf nodes) are merged into their parents without losing critical information. This allows building a lightweight DependencyViT-Lite model.3) The visual dependencies captured by DependencyViT, whether learned from weak supervision on ImageNet or self-supervision, can benefit downstream vision tasks like part/object segmentation, detection, and recognition.In summary, the central research question is how to induce visual dependencies and hierarchies unsupervisedly using reversed attention transformers, and the key hypotheses are that this approach enables efficient models and benefits various vision applications.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing Visual Dependency Transformers (DependencyViT) that can induce visual dependencies and build hierarchies from images without human annotations. This is achieved through a novel neural operator called reversed attention that can capture long-range dependencies between image patches.2. Formulating a dependency graph where child nodes send information to parent nodes following a probability distribution. This allows hierarchies and trees to emerge naturally from the reversed attention layers in an unsupervised manner.3. Introducing a lightweight model called DependencyViT-Lite that performs dynamic pooling based on the emerged dependency tree. Leaf nodes that rarely send messages are pruned without hindering performance. This reduces computational cost and memory footprint.4. Showing that DependencyViT works well on both self-supervised and weakly-supervised pretraining paradigms on ImageNet. It demonstrates effectiveness on 8 datasets across 5 tasks including part/saliency segmentation, recognition, and detection.5. Achieving state-of-the-art results on semantic segmentation, object detection, part segmentation and saliency detection through the visual dependency parsing capability of DependencyViT.In summary, the key contribution is proposing DependencyViT that can induce visual dependencies and hierarchies unsupervisedly via reversed attention, enabling tasks like part segmentation and achieving strong performance on various vision tasks. The dynamic pooling scheme also allows efficient lightweight models to be derived.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Visual Dependency Transformers (DependencyViT), a new vision transformer architecture that uses a novel reversed self-attention mechanism to induce visual dependencies and hierarchical structures from images in an unsupervised manner.


## How does this paper compare to other research in the same field?

This paper proposes a novel vision transformer architecture called Visual Dependency Transformers (DependencyViT) for unsupervised visual dependency parsing. Here are some key comparisons to other related work:1. Compared to classical image parsing methods like stochastic image grammars, DependencyViT takes a completely new approach based on neural attention in vision transformers. It does not rely on hand-crafted grammar rules or probabilistic graphical models.2. Compared to capsule networks, DependencyViT operates on image patches rather than pixels or geometric parts. This allows it to capture both part-level and object-level dependencies in a unified framework. It is also more scalable to complex natural images.3. Compared to visual grounding methods, DependencyViT does not require any linguistic supervision. It induces visual dependencies and hierarchies directly from images in a self-supervised manner.4. Compared to vision-language grammar induction methods, DependencyViT again does not use any text supervision. It performs purely visual parsing without grounding to language.5. Compared to hierarchical vision transformers like Swin Transformers, DependencyViT uniquely incorporates dependency parsing through its reversed attention mechanism. This allows it to capture explicit hierarchical structure.6. Compared to dynamic pruning methods like DynamicViT, DependencyViT retains hierarchical information through subtree dependencies even after pruning tokens. This allows downstream dense prediction tasks.In summary, DependencyViT proposes a novel perspective of inducing visual dependencies directly through neural attention, without grounding to language or relying on hand-designed grammar models. The comparisons show it is a unique approach for unsupervised visual parsing in the transformer era.
