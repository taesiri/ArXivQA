# [Large Language Models are Fixated by Red Herrings: Exploring Creative   Problem Solving and Einstellung Effect using the Only Connect Wall Dataset](https://arxiv.org/abs/2306.11167)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is:

How do large language models (LLMs) perform on creative problem solving tasks involving grouping heterogeneous connections and identifying open knowledge domain links between words, in the presence of misleading stimuli or "red herrings"? 

Specifically, the paper examines LLMs' capabilities on creative problem solving using a novel dataset based on the "Only Connect" quiz show, which contains built-in red herrings by design. The key hypotheses seem to be:

1) LLMs will struggle on these creative problem solving tasks compared to human performance.

2) Increasing the number of in-context examples will not significantly improve LLM performance, due to the need for open-ended, creative thinking. 

3) The presence of red herrings will negatively impact LLM performance, an effect akin to the "fixation effect" observed in human psychology experiments.

To summarize, the central research focus is evaluating how current LLMs handle creative problem solving on heterogeneous word connections with deliberately misleading information, in contrast with human cognitive patterns. The Only Connect dataset provides an interesting proxy to systematically test these hypotheses.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Introducing a new dataset called the Only Connect Wall (OCW) dataset for evaluating creative problem solving abilities of language models. The dataset is derived from the popular British quiz show Only Connect and contains tasks that require making connections between seemingly unrelated words, analogous to the Remote Associates Test (RAT) from cognitive science.

2. Evaluating a range of NLP models from static embeddings to large language models on the proposed OCW dataset. The results show that current state-of-the-art models like GPT-4 still underperform significantly compared to human baselines on these creative problem solving tasks.

3. Analyzing the effect of "red herrings" or misleading stimuli in the OCW tasks through additional experiments on modified datasets with reduced red herrings. The results align with findings from cognitive science literature that show red herrings impede human creative problem solving, and a similar negative effect is observed in the performance of language models.

4. Proposing the OCW dataset and tasks as a new benchmark for evaluating progress towards human-imitative AI and creative problem solving abilities in language models. The work makes connections between cognitive science theories like the Einstellung effect and the capabilities of modern AI systems.

In summary, the key contribution is the introduction and analysis of a new challenging dataset for testing creative problem solving in language models, drawing inspiration from cognitive science literature and showing the limitations of current state-of-the-art models on such tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new dataset and tasks based on the British quiz show Only Connect for evaluating large language models on creative problem solving, and finds that current state-of-the-art models still underperform compared to humans on these tasks.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in evaluating the creative problem-solving abilities of large language models:

- The use of the Only Connect Wall dataset as an evaluation benchmark is novel. Other benchmarks like GLUE, SuperGLUE, BIG-Bench, and HumanEval focus on different capabilities like linguistic understanding, common sense reasoning, scientific knowledge, etc. The Only Connect Wall specifically tests creative association and lateral thinking skills.

- The concept of studying "fixation" and the "Einstellung effect" in language models seems to be a new angle that has not been extensively explored before. The authors connect these cognitive science concepts to the problem of "red herrings" that can mislead language models. This provides a fresh perspective to analyzing model failures.

- The experiments with clustering embeddings and few-shot learning follow a similar methodology to some prior NLP benchmarking papers. However, the application to the unique Only Connect Wall dataset provides new insights.

- The ablation studies on reducing red herrings are a nice addition to systematically analyze the effect of misleading information. This kind of controlled experimentation is less common in benchmarking papers.

- The overall finding that even powerful models like GPT-4 still lag significantly behind human performance is consistent with results on other benchmarks. It reinforces that substantial challenges remain in developing truly human-like language intelligence.

In summary, while the general framework is similar to some past benchmarking research, the use of a new task, creative angles of analysis, and rigorous experiments make several novel contributions to evaluating language models. The paper yields insights that are complementary to existing literature.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Augmenting the OCW dataset with more connecting walls from new seasons of Only Connect, as well as incorporating fan-generated walls. The authors mention that the website PuzzGrid contains over 4200 user-generated walls that could potentially be added.

- Developing additional Only Connect inspired walls in multiple languages and incorporating clues derived from various cultures and subcultures. This could help make the dataset more cross-cultural. 

- Evaluating the sensitivity of LLM performance to the ordering of clues within each wall. The authors note they evaluated sensitivity for classical embedding models but not for LLMs due to cost limitations. 

- Applying more complicated prompting strategies like "Chain of Thought" or "Tree of Thoughts" to the dataset tasks. The authors suggest these approaches could potentially help mitigate issues like models including clues in the predicted connections.

- Evaluating retrieval augmented models on the dataset, which may be able to solve groups requiring niche/specialized knowledge by retrieving relevant information.

- Developing methods to prevent test sets of public datasets like OCW from being included in LLM training sets, which remains an open problem.

- Organizing shared tasks using the dataset to encourage community benchmarking and advancement.

- Developing additional "red herring" test sets in other domains to further analyze the effects on LLM performance.

- Exploring model architectures and training techniques to improve creative problem solving and reduce susceptibility to red herrings.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces a new dataset called the Only Connect Wall (OCW) dataset for evaluating creative problem solving abilities of large language models (LLMs). The dataset is based on the popular British quiz show Only Connect and contains over 600 "walls" - puzzles where 16 jumbled words must be grouped into 4 connected groups. The paper frames these grouping and connection-naming tasks as proxies for the Remote Associates Test in cognitive science, which is used to measure creativity. After describing the dataset collection and structure, the authors evaluate various language models on the two tasks, finding that even state-of-the-art LLMs like GPT-4 perform far below human expert baseline. They introduce two additional versions of the dataset with reduced "red herrings" to analyze the concept of fixation from cognitive science, finding improved performance when distracting words are removed. Overall, the OCW dataset provides a challenging new benchmark for evaluating creative problem solving in LLMs, which even the most advanced models currently struggle with. The code and dataset are openly available.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new dataset called the Only Connect Wall (OCW) dataset for evaluating creative problem solving abilities in large language models (LLMs). The dataset is based on the "Connecting Walls" segment from the British TV quiz show Only Connect, where contestants must group 16 jumbled words into 4 related groups and identify the connections between them. This mimics the Remote Associates Test (RAT) from cognitive science which measures creative thinking in humans. The OCW dataset contains over 600 connecting wall puzzles curated from 15 seasons of Only Connect, along with human performance statistics. The authors frame these puzzles and the presence of misleading "red herring" words as a proxy task for analyzing fixation effects and the Einstellung paradigm from cognitive psychology in LLMs. 

The authors evaluated a range of NLP models from word embeddings to large PLMs on the grouping and connection tasks. The results show that even state-of-the-art LLMs like GPT-4 perform significantly worse than human baselines, only partially solving a fraction of the walls. The addition of more in-context examples surprisingly does not help LLMs, perhaps indicating the need for external knowledge. Additional experiments with modified datasets containing fewer red herrings show improved LLM performance, aligning with theories of fixation effects. Overall, the OCW dataset presents creative, open-ended challenges not solved by current LLMs, providing a valuable benchmark for continued progress on human-like language intelligence.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces a new dataset called Only Connect Walls (OCW) based on the connecting wall puzzles from the British quiz show Only Connect. The dataset contains 618 wall puzzles along with solutions from 15 seasons of the show. Each puzzle consists of 16 jumbled clue words that must be grouped into 4 connected groups of 4 words each. The method then evaluates various NLP models on two tasks using this dataset: 1) Grouping the 16 words into the correct 4 groups, and 2) Identifying the underlying connection for each group. For the grouping task, the authors test embedding-based clustering techniques using static and contextualized word embeddings from classical embedding models and pre-trained language models. For the connection task, they prompt large language models like GPT-3.5 and GPT-4 in a few-shot setting to predict the connections. The paper analyzes model performance on these two tasks compared to human baselines from the show. Additional ablation experiments are done by generating modified test sets to study the effect of "red herrings" or misleading words on model performance.


## What problem or question is the paper addressing?

 Based on my reading, the key points about the problem and questions addressed in this paper are:

- The paper is exploring creative problem solving abilities and the "fixation effect" or "Einstellung effect" in large language models (LLMs). These concepts come from cognitive science and refer to how exposure to misleading information or "red herrings" can impede human creativity and problem solving. 

- There is currently no standardized way to evaluate creative problem solving in LLMs, as existing benchmarks like BIG-Bench focus more on linguistic capabilities. The paper introduces a new dataset and tasks based on the TV show "Only Connect" to serve as a proxy for evaluating creativity.

- The two key questions examined are: 1) How do LLMs perform on creative problem solving tasks like grouping heterogeneous clues and identifying open-ended connections? 2) Does the presence of "red herrings" impede the performance of LLMs, similar to the fixation effect seen in humans?

- The authors generate additional synthetic datasets to vary the amount of red herrings and test their "red herring hypothesis" about the negative impact of misleading stimuli on LLMs.

- Overall, the work aims to bridge concepts from cognitive science like the fixation effect with LLMs, while also providing a new challenging benchmark to assess progress towards more human-like creative problem solving abilities.
