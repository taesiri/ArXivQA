# [A Comparative Study on Annotation Quality of Crowdsourcing and LLM via   Label Aggregation](https://arxiv.org/abs/2401.09760)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Whether large language models (LLMs) can outperform crowdsourcing for data annotation tasks has recently attracted interest. Some works have compared LLM workers to crowd workers by collecting new datasets, but existing crowdsourcing datasets have not been utilized for evaluation. 

- The quality of aggregated labels from multiple noisy labels is crucial when using crowdsourcing, but evaluations have focused on individual labels rather than aggregated labels.

Contributions
1) Investigate existing crowdsourcing datasets to create a benchmark for comparing LLM and crowd worker performance, using categorical label tasks.

2) Compare quality of individual crowd vs LLM labels. Find that crowd workers can still outperform even good LLM workers like ChatGPT when quality control methods are used to select high-ability crowd workers.

3) Evaluate quality of aggregated labels from label aggregation models applied to crowd-only, LLM-only and crowd-LLM hybrid label sets. 

Key Findings
- Adding labels from good LLMs to existing crowd datasets improves aggregated label quality beyond using crowd or LLM labels alone.  

- Crowd-LLM hybrid aggregation outperforms even the good LLM labels themselves.  

- Normal LLMs like Vicuna cannot replace or improve upon crowd worker labels.

- More crowd labels can still improve aggregated quality despite also having LLM labels. Advanced aggregation models outperform basic models.

Limitations and Future Work
- Only categorical labels evaluated. Other types of labels need investigation.


## Summarize the paper in one sentence.

 This paper investigates using existing crowdsourcing datasets to compare individual and aggregated label quality between crowd workers and large language models, and proposes a hybrid crowd-LLM label aggregation method which shows adding good LLM labels can improve aggregated label quality.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Investigating existing crowdsourcing datasets and proposing a benchmark to provide reliable evaluations for comparing crowd labels and LLM labels.

2) Comparing the quality between individual crowd labels and LLM labels, as well as evaluating the aggregated labels from label aggregation models.

3) Proposing a Crowd-LLM hybrid label aggregation method that combines both crowd and LLM labels, and showing that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels.

So in summary, the main contribution is creating a benchmark for evaluating crowd vs LLM labels, comparing their quality, and showing that combining them can improve aggregated label quality over using either alone. The key findings are that good LLMs can improve label aggregates and normal LLMs are not yet good enough to replace crowd workers.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it include:

- Crowdsourcing
- Label aggregation 
- Large language models (LLMs)
- Data annotation
- Label quality
- Crowd workers
- LLM workers  
- Hybrid label aggregation
- Categorical labels
- Comparative study
- Benchmark dataset

The paper proposes a benchmark dataset for comparing the label quality between crowd workers and LLM workers on data annotation tasks with categorical labels. It focuses on utilizing existing crowdsourcing datasets, evaluating both individual labels and aggregated labels, and proposing a crowd-LLM hybrid label aggregation method. The key goal is to study whether adding LLM labels can improve the label aggregation quality for existing crowdsourcing datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a Crowd-LLM hybrid label aggregation method. Can you explain in detail how this method works and how it aggregates the labels from both crowd workers and LLMs? 

2. The paper finds that adding LLM labels from good LLMs can enhance the quality of the aggregated labels. What are the possible reasons that explain why combining crowd and LLM labels leads to better performance than using them separately?

3. The paper uses two different LLMs - ChatGPT and Vicuna. Can you analyze the differences in their performance and explain why ChatGPT gives better results in the experiments? 

4. The paper experiments with different temperature parameters for the LLMs. Can you explain what the temperature parameter controls and why using multiple values helps represent randomness in LLM outputs?

5. The paper prompts and post-processes the LLM outputs to fit the format of categorical labels. Can you suggest some ways the prompting and rules could be improved to generate better quality labels from the LLMs?  

6. The paper evaluates performance using accuracy of aggregated labels. What are some other evaluation metrics that could be used? What are the trade-offs of using accuracy versus other metrics?

7. The paper focuses on categorical labels. Can you suggest some approaches to extend the hybrid label aggregation method to other types of labels like numerical, pairwise comparisons etc?

8. The paper uses several label aggregation models like MV, DS and GLAD. Can you analyze the differences in their working and explain when one might be preferred over others? 

9. The paper creates label subsets with only a few crowd labels per instance. How could the label aggregation performance for such sparse label settings be further improved? 

10. The paper findings are on specific crowdsourcing datasets. What are some ways the experimental analyses could be extended to better generalize the conclusions?
