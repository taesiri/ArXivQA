# [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train language models to generate high-quality textual summaries by optimizing directly for human preferences, rather than using standard supervised learning techniques? The key hypothesis is that training summarization models to predict human preferences and optimizing a policy to maximize the resulting "reward" will lead to better quality summaries than models trained via maximum likelihood on reference summaries.The paper aims to show:- Training with human feedback allows models to significantly outperform very strong summarization baselines that use standard supervised training, even when the baseline models are much larger.- Models trained with human feedback generalize much better to new domains than supervised models. - Extensive empirical analysis provides evidence that the trained reward model captures nuances of summary quality better than automatic metrics like ROUGE, and that directly optimizing this reward model results in better summaries.So in summary, the central research question is how to produce better text summarization through optimizing human preferences rather than standard supervised objectives, with the key hypothesis being that this can be achieved by training a reward model on human comparisons and using it to optimize a policy via reinforcement learning.


## What is the main contribution of this paper?

The main contribution of this paper is developing methods to train summarization models using human feedback instead of just maximum likelihood training on reference summaries. The key ideas are:- Collecting a large dataset of human comparisons between model-generated summaries. This allows training a reward model to predict which summary humans prefer. - Using the reward model to optimize a policy with reinforcement learning to generate higher quality summaries as judged by humans. - Showing this approach leads to better generalization and summaries preferred by humans over those from maximum likelihood training, for large Transformer models tested on the TL;DR Reddit and CNN/DailyMail news summarization datasets.The paper also includes detailed analysis on the quality of the human feedback data, how well different automatic metrics correlate with human judgments, and how model performance scales with data and model size. Overall, it makes a strong case for how optimizing for human feedback enables summarization models to better capture notions of quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review, I would summarize the key points of this paper as:This paper proposes using human feedback to train summarization models that produce higher-quality outputs more aligned with human preferences, rather than just optimizing likelihood of human reference summaries. The authors collect a large dataset of human comparisons between summaries, train models to predict human preferences, and use this as a reward signal for RL fine-tuning. Their Reddit-trained models outperform much larger supervised models and also transfer well to summarizing news articles without further training. The authors conduct extensive analysis to understand their models and reward learning process.In one sentence, I would summarize it as: This paper trains summarization models using human feedback as a reward signal, outperforming supervised methods and transferring to new domains.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in summarization:- It focuses on improving summary quality by directly optimizing for human preferences, rather than proxy metrics like ROUGE. Most prior work has focused on supervised learning or optimizing automatic metrics. Using human feedback for summarization has been explored before, but is still relatively uncommon.- It uses very large Transformer models, up to 6.7 billion parameters. Prior work often uses smaller models, like BERT. Using such massive models allows them to set new state-of-the-art results.- The scale of the human feedback dataset collected is larger than in prior work. They collect over 60,000 comparisons on the TL;DR dataset. Previous human feedback summarization papers used at most a few thousand comparisons.- It studies the transfer of models trained on Reddit posts to summarizing news articles. Most prior work focuses on a single domain like news. Analyzing cross-domain transfer is still relatively uncommon in summarization.- It includes extensive analysis and ablations to understand their models, like studying overoptimization of the reward model and analyzing different automatic metrics. Many papers focus only on end results. The analyses help provide insights into why their approach works.- It uses the TL;DR dataset based on Reddit, whereas most work uses news datasets like CNN/DailyMail. TL;DR has seen less summarization research so far.Overall, this paper pushes the boundaries on scale and rigor in studying human-feedback summarization. The analyses help provide valuable insights into model behavior. It represents an advance in techniques for training models that produce outputs better aligned with human preferences.


## What future research directions do the authors suggest?

The authors suggest several future research directions:- Applying the methods to other tasks where humans can compare samples, such as dialogue, machine translation, question answering, speech synthesis, open-ended games, and music generation. They mention that the method may be particularly useful for generating long samples.- Training models to predict feedback across many tasks, which may improve sample efficiency.- Scaling human feedback to tasks where humans can't easily evaluate the outputs, such as by training ML systems to help humans perform the evaluation task quickly and accurately.- Exploring other types of human feedback beyond binary comparisons, such as soliciting high-quality demonstrations, having humans edit outputs, or having humans provide explanations. These could also be used as training signals.- Studying how to handle cases where individual humans' preferences conflict, which will be important for more complex tasks. The authors mention ensuring individuals impacted by the technology should help define the notion of "good" behavior that is reinforced.- Mitigating potential risks from malicious use of techniques to train models aligned with harmful goals.In summary, they suggest exploring more advanced forms of human feedback, transferring across tasks, handling disagreements between humans, and addressing potential risks. The key theme is developing techniques to train models that reliably capture nuanced human preferences.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a method for training summarization models to produce higher quality summaries as judged by humans, rather than simply optimizing likelihood of human-written reference summaries. The authors collect a large dataset of human comparisons between model-generated summaries. They then train a "reward model" to predict the human-preferred summary in each comparison, and use this as a reward signal to fine-tune a policy via reinforcement learning. When evaluated on the TL;DR Reddit dataset, the authors' human feedback models significantly outperform both the dataset's reference summaries and much larger supervised models, even transferring well to summarizing news articles without news-specific fine-tuning. The authors conduct extensive analyses on the quality of their human data, reward model behavior, and model outputs. They argue that directly optimizing human preferences enables models to better capture nuances of summary quality compared to likelihood, and this technique could be applied to improve alignment in other domains.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a method for training text summarization models using human feedback. The key idea is to collect human comparisons between model-generated summaries, then train a "reward model" to predict the human-preferred summary. This reward model is then used to fine-tune a summarization model via reinforcement learning. The authors apply this technique to summarizing Reddit posts on the TL;DR dataset. They find their human feedback models significantly outperform supervised baselines, generating better summaries than the original human-written ones according to labelers. The Reddit-trained models also transfer well to summarizing news articles in the CNN/DM dataset, despite no news-specific training. Extensive analyses are provided studying the impact of model scale, optimization, and choice of reward model. Overall, this work demonstrates that directly optimizing for human judgments of summary quality through reward learning and reinforcement can substantially improve performance over maximum likelihood training alone.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper trains models to generate high-quality text summaries by optimizing for human preferences, rather than maximizing the likelihood of reference text as in traditional supervised learning. First, they collect a large dataset of binary human comparisons between summaries. They train a reward model to predict which summary in each pair humans prefer. This reward model is used to provide rewards for training a summarization policy via reinforcement learning. The policy generates summaries token-by-token, and is updated using the PPO algorithm to maximize the predicted reward from the model. They iterate this process of collecting more comparisons using samples from the latest policy, retraining the reward model, and retraining the policy. Using this technique with large Transformer models, they are able to train summarization models that generate higher quality summaries according to humans than both the reference summaries in their dataset and supervised baselines.
