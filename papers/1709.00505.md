# [ShapeCodes: Self-Supervised Feature Learning by Lifting Views to   Viewgrids](https://arxiv.org/abs/1709.00505)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether learning to perform "mental rotation" of 3D objects and predict their appearance from novel viewpoints can help with general object recognition tasks. Specifically, the authors propose an approach called "ShapeCodes" where a convolutional neural network is trained in a self-supervised manner to take a single view of a 3D object and predict the full multi-view "viewgrid" depicting that object from different angles. The key hypothesis is that by training the network to perform this mental rotation and view prediction task, it will learn a feature representation that captures useful 3D shape properties and geometric regularities, allowing it to generalize better to recognizing new object categories not seen during training.The authors test this hypothesis by training ShapeCodes on synthetic 3D object datasets in a class-agnostic manner, and then evaluating how well the learned features transfer to object classification and retrieval on both seen and unseen categories. Their main finding is that the ShapeCodes representation significantly outperforms other unsupervised feature learning methods, and even competes with fully supervised features, confirming their hypothesis that exploiting 3D geometric reasoning is beneficial for learning visual features for recognition.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an unsupervised feature learning approach that embeds 3D shape information into a single-view image representation. Specifically:- They introduce a self-supervised training objective that requires predicting unseen views of an object from a single input view. This forces the model to lift the 2D view to a 3D understanding of shape.- They implement this idea as an encoder-decoder convolutional neural network. The encoder maps the input view to a latent space (the "ShapeCode") from which the decoder can reconstruct views of the object from other viewpoints.- The ShapeCode representation is learned without manual labels, in a class-agnostic manner, to capture basic 3D shape properties that generalize across objects. - Experiments show the model successfully performs "mental rotation", generating missing views of objects even from unseen categories.- The ShapeCode features transfer well to object recognition tasks, outperforming other unsupervised learning approaches. This validates that forcing 3D understanding is a useful pretext task for representation learning.In summary, the key contribution is a new self-supervised approach to learn visual features that lift 2D views to 3D shape representations, without manual labels. This is shown to be beneficial for general visual recognition tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes an unsupervised learning approach that trains a convolutional neural network to embed 3D shape information into a 2D image representation by predicting unseen views of objects from a single input view.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other related work:- The paper introduces a new self-supervised approach for learning visual representations by training a model to perform "mental rotation". This is a novel pretext task compared to other common self-supervised tasks like predicting context, colorization, etc. - The idea of learning representations by predicting multiple views of an object is related to work on novel view synthesis and multi-view reconstruction. However, unlike most prior work, this paper uses view prediction in a self-supervised, class-agnostic way for representation learning rather than 3D reconstruction as the end goal.- The proposed model is trained on synthetic 3D shape datasets (ModelNet, ShapeNet) which is common in 3D vision papers. However, the goal here is 2D image representation learning rather than 3D shape analysis.- For recognition tasks, the ShapeCodes features are shown to outperform other unsupervised learning methods like context prediction, ego-motion, etc. The features even compete well with fully supervised ImageNet pre-trained features. This demonstrates the power of the shape-based pretext task.- The idea of training visual representations to be predictive of 3D properties like viewpoint has connections to other works that encourage equivariance, but the use of viewgrids as supervision is novel.- Overall, the self-supervised objective of predicting full viewgrids in a class-agnostic way is a new technique for learning 2D image features. The results demonstrate advantages over existing unsupervised methods by explicitly targeting 3D geometry.In summary, the paper introduces a novel approach for self-supervised visual representation learning that outperforms prior methods by exploiting 3D shape information through viewgrids. The idea of transferring features from this shape-based pretext task to 2D recognition is quite unique.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Testing whether features trained on synthetic object models could generalize to real images. The models in this work are trained and tested on rendered images of 3D models from ShapeNet and ModelNet. The authors suggest investigating if similar features could be learned from real images.- Training on real objects by having an embodied agent physically inspect objects to acquire viewgrids. Currently the viewgrids are generated from 3D models, but the authors propose the idea of an agent manipulating real objects to build up the training data. - Extending the approach to allow sequential accumulation of observed views of real objects over time. The current method assumes a full viewgrid is available in one shot. The authors suggest exploring incremental accumulation of views of real objects.- Investigating reconstruction losses at a more abstract level rather than pixel level losses. For example, using a feature content loss rather than pixel loss.- Exploring the integration of the method with active perception and control policies for view selection. The paper generates viewgrids with fixed sampling but learned intelligent view selection could improve results.- Scaling up the approach with bigger and deeper models, and experimenting on more complex real images rather than just rendered shapes.In summary, the key directions are: generalizing to real images, active view selection, incremental multi-view accumulation, more abstract losses beyond pixels, and scaling up to bigger and more complex data. The core idea of exploiting viewgrids for self-supervised 3D understanding seems very promising.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper: The paper proposes an unsupervised feature learning approach called ShapeCodes that embeds 3D shape information into a single-view image representation. The main idea is a self-supervised training objective where, given only a single 2D image, the model must predict unseen views of the object from that image. This is implemented as an encoder-decoder convolutional neural network that maps an input image to a latent space, from which a decoder generates the object's full viewgrid showing it from different viewpoints. By training on objects from diverse categories in a class-agnostic manner, the model is encouraged to learn fundamental 3D shape primitives and regularities without manual labels. Experiments on 3D object datasets show the model successfully learns to perform "mental rotation" to generate missing views even for unseen categories. Furthermore, the learned latent features achieve strong performance on object recognition tasks, outperforming existing unsupervised methods. The results demonstrate the benefits of targeting 3D geometric reasoning for self-supervised feature learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper introduces ShapeCodes, a self-supervised feature learning approach that embeds 3D shape information into a single-view image representation. The main idea is to train a convolutional neural network to predict unseen views of an object given only a single input view. Specifically, the network is trained to map an input image to a latent space from which a decoder can reconstruct a viewgrid showing the object from all viewing angles. By training the network to perform this "mental rotation" in a class-agnostic manner over a large dataset of 3D object models, it learns to capture basic shape primitives, semantic regularities, and shading cues useful for recognizing objects in 2D images. The authors validate their approach on the ModelNet and ShapeNet datasets. First, they show the network successfully learns to reconstruct viewgrids for objects from both seen and unseen categories. This demonstrates it captures class-general shape information. Second, they extract features from the network and show they significantly outperform existing unsupervised learning methods when transferred to object classification and retrieval tasks. The features even compete well with supervised ImageNet-pretrained features, demonstrating their strong transferability. Overall, the results show explicitly targeting 3D reasoning is a promising path to learn useful image features without manual labels.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method in the paper:The paper proposes an unsupervised approach to learn image features that embed 3D shape information. The key idea is to train a convolutional neural network to perform a self-supervised "mental rotation" task: given a single 2D view of an object from an arbitrary unknown viewpoint, predict views of that object from all other viewpoints (the "viewgrid"). Specifically, the network has an encoder-decoder structure. The encoder maps the input view to a latent vector representing the 3D shape. The decoder then lifts that latent vector to reconstruct the full viewgrid showing the object from all angles. The model is trained in a class-agnostic manner on a dataset of 3D object models, so the learned representation captures general 3D shape properties rather than category-specific details. At test time, the encoder produces a "ShapeCode" feature vector embedding 3D properties, which can then be used for recognition tasks on new object categories not seen during training. By training the network for this novel self-supervised task of "mental rotation", the hope is that it will learn a representation capturing useful 3D shape cues to benefit recognition, without needing manual labels.


## What problem or question is the paper addressing?

 The paper is addressing the problem of learning useful image representations for object recognition in a self-supervised manner, without requiring manual labels. Specifically, the authors propose an approach to learn image features that embed 3D shape information by training on a "mental rotation" task of predicting views of objects from unseen viewpoints.The key questions the paper tackles are:1) Can a model learn to perform mental rotation and generate novel views of 3D objects from just a single view, in a class-agnostic, self-supervised manner?2) Do the representations learned by training on this mental rotation task transfer well to object recognition tasks, compared to other self-supervised approaches?3) Does explicitly targeting 3D understanding and viewpoint transformations in a self-supervised task lead to image features that are more useful for recognition compared to more common self-supervised objectives like context prediction or ego-motion?So in summary, the paper introduces a new self-supervised approach to learn visual representations by reconstructing 3D structure, and evaluates how well the learned features transfer to recognition tasks in comparison to other unsupervised methods. The key novelty is exploiting 3D geometric cues through viewpoint prediction as a pretext task.
