# [ShapeCodes: Self-Supervised Feature Learning by Lifting Views to   Viewgrids](https://arxiv.org/abs/1709.00505)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether learning to perform "mental rotation" of 3D objects and predict their appearance from novel viewpoints can help with general object recognition tasks. Specifically, the authors propose an approach called "ShapeCodes" where a convolutional neural network is trained in a self-supervised manner to take a single view of a 3D object and predict the full multi-view "viewgrid" depicting that object from different angles. The key hypothesis is that by training the network to perform this mental rotation and view prediction task, it will learn a feature representation that captures useful 3D shape properties and geometric regularities, allowing it to generalize better to recognizing new object categories not seen during training.The authors test this hypothesis by training ShapeCodes on synthetic 3D object datasets in a class-agnostic manner, and then evaluating how well the learned features transfer to object classification and retrieval on both seen and unseen categories. Their main finding is that the ShapeCodes representation significantly outperforms other unsupervised feature learning methods, and even competes with fully supervised features, confirming their hypothesis that exploiting 3D geometric reasoning is beneficial for learning visual features for recognition.
