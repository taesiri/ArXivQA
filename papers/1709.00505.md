# [ShapeCodes: Self-Supervised Feature Learning by Lifting Views to   Viewgrids](https://arxiv.org/abs/1709.00505)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether learning to perform "mental rotation" of 3D objects and predict their appearance from novel viewpoints can help with general object recognition tasks. Specifically, the authors propose an approach called "ShapeCodes" where a convolutional neural network is trained in a self-supervised manner to take a single view of a 3D object and predict the full multi-view "viewgrid" depicting that object from different angles. The key hypothesis is that by training the network to perform this mental rotation and view prediction task, it will learn a feature representation that captures useful 3D shape properties and geometric regularities, allowing it to generalize better to recognizing new object categories not seen during training.The authors test this hypothesis by training ShapeCodes on synthetic 3D object datasets in a class-agnostic manner, and then evaluating how well the learned features transfer to object classification and retrieval on both seen and unseen categories. Their main finding is that the ShapeCodes representation significantly outperforms other unsupervised feature learning methods, and even competes with fully supervised features, confirming their hypothesis that exploiting 3D geometric reasoning is beneficial for learning visual features for recognition.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an unsupervised feature learning approach that embeds 3D shape information into a single-view image representation. Specifically:- They introduce a self-supervised training objective that requires predicting unseen views of an object from a single input view. This forces the model to lift the 2D view to a 3D understanding of shape.- They implement this idea as an encoder-decoder convolutional neural network. The encoder maps the input view to a latent space (the "ShapeCode") from which the decoder can reconstruct views of the object from other viewpoints.- The ShapeCode representation is learned without manual labels, in a class-agnostic manner, to capture basic 3D shape properties that generalize across objects. - Experiments show the model successfully performs "mental rotation", generating missing views of objects even from unseen categories.- The ShapeCode features transfer well to object recognition tasks, outperforming other unsupervised learning approaches. This validates that forcing 3D understanding is a useful pretext task for representation learning.In summary, the key contribution is a new self-supervised approach to learn visual features that lift 2D views to 3D shape representations, without manual labels. This is shown to be beneficial for general visual recognition tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an unsupervised learning approach that trains a convolutional neural network to embed 3D shape information into a 2D image representation by predicting unseen views of objects from a single input view.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other related work:- The paper introduces a new self-supervised approach for learning visual representations by training a model to perform "mental rotation". This is a novel pretext task compared to other common self-supervised tasks like predicting context, colorization, etc. - The idea of learning representations by predicting multiple views of an object is related to work on novel view synthesis and multi-view reconstruction. However, unlike most prior work, this paper uses view prediction in a self-supervised, class-agnostic way for representation learning rather than 3D reconstruction as the end goal.- The proposed model is trained on synthetic 3D shape datasets (ModelNet, ShapeNet) which is common in 3D vision papers. However, the goal here is 2D image representation learning rather than 3D shape analysis.- For recognition tasks, the ShapeCodes features are shown to outperform other unsupervised learning methods like context prediction, ego-motion, etc. The features even compete well with fully supervised ImageNet pre-trained features. This demonstrates the power of the shape-based pretext task.- The idea of training visual representations to be predictive of 3D properties like viewpoint has connections to other works that encourage equivariance, but the use of viewgrids as supervision is novel.- Overall, the self-supervised objective of predicting full viewgrids in a class-agnostic way is a new technique for learning 2D image features. The results demonstrate advantages over existing unsupervised methods by explicitly targeting 3D geometry.In summary, the paper introduces a novel approach for self-supervised visual representation learning that outperforms prior methods by exploiting 3D shape information through viewgrids. The idea of transferring features from this shape-based pretext task to 2D recognition is quite unique.
