# [ShapeCodes: Self-Supervised Feature Learning by Lifting Views to   Viewgrids](https://arxiv.org/abs/1709.00505)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether learning to perform "mental rotation" of 3D objects and predict their appearance from novel viewpoints can help with general object recognition tasks. Specifically, the authors propose an approach called "ShapeCodes" where a convolutional neural network is trained in a self-supervised manner to take a single view of a 3D object and predict the full multi-view "viewgrid" depicting that object from different angles. The key hypothesis is that by training the network to perform this mental rotation and view prediction task, it will learn a feature representation that captures useful 3D shape properties and geometric regularities, allowing it to generalize better to recognizing new object categories not seen during training.The authors test this hypothesis by training ShapeCodes on synthetic 3D object datasets in a class-agnostic manner, and then evaluating how well the learned features transfer to object classification and retrieval on both seen and unseen categories. Their main finding is that the ShapeCodes representation significantly outperforms other unsupervised feature learning methods, and even competes with fully supervised features, confirming their hypothesis that exploiting 3D geometric reasoning is beneficial for learning visual features for recognition.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an unsupervised feature learning approach that embeds 3D shape information into a single-view image representation. Specifically:- They introduce a self-supervised training objective that requires predicting unseen views of an object from a single input view. This forces the model to lift the 2D view to a 3D understanding of shape.- They implement this idea as an encoder-decoder convolutional neural network. The encoder maps the input view to a latent space (the "ShapeCode") from which the decoder can reconstruct views of the object from other viewpoints.- The ShapeCode representation is learned without manual labels, in a class-agnostic manner, to capture basic 3D shape properties that generalize across objects. - Experiments show the model successfully performs "mental rotation", generating missing views of objects even from unseen categories.- The ShapeCode features transfer well to object recognition tasks, outperforming other unsupervised learning approaches. This validates that forcing 3D understanding is a useful pretext task for representation learning.In summary, the key contribution is a new self-supervised approach to learn visual features that lift 2D views to 3D shape representations, without manual labels. This is shown to be beneficial for general visual recognition tasks.
