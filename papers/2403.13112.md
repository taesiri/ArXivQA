# [Encode Once and Decode in Parallel: Efficient Transformer Decoding](https://arxiv.org/abs/2403.13112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer models are powerful but computationally expensive, limiting their deployment. 
- Tasks with a shared input document that require multiple output queries (e.g. question answering, summarization) suffer from redundant input encoding costs.

Proposed Solution:
- Introduce a new configuration, prompt-in-decoder (PiD), that encodes the input only once and decodes outputs in parallel.
- Avoid duplicate input encodings and reduce decoder memory footprint to improve efficiency.
- Apply to tasks like dialogue state tracking, summarization, QA where a single input has multiple associated outputs.  

Key Contributions:
- Propose the prompt-in-decoder configuration for transformers that shares input embeddings across prompts.
- Achieve comparable or better performance to state of the art on dialogue state tracking, summarization and QA datasets.
- Demonstrate efficiency gains in FLOPs and latency that scale with the number of subtasks, up to 4.6x faster decoding than prompt-in-encoder.  
- Open source training and inference code to facilitate further research.

In summary, the paper introduces an efficient transformer decoding strategy that encodes inputs once and decodes multiple outputs in parallel, validated on structured output tasks. It reduces redundant computation and accelerates decoding while maintaining accuracy.
