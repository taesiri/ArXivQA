# [Language in a Bottle: Language Model Guided Concept Bottlenecks for   Interpretable Image Classification](https://arxiv.org/abs/2211.11158)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can we automatically construct high-performance Concept Bottleneck Models without requiring manual specification of human-interpretable concepts?The key hypotheses appear to be:1) Large language models like GPT-3 contain significant world knowledge that can be elicited to generate candidate textual concepts for arbitrary image classification tasks.2) By prompting GPT-3 to generate a large set of candidate concepts per class and selecting an optimal subset using a submodular function, we can create discriminative and diverse bottlenecks that are as effective or better than general black box models, especially in low-data regimes. 3) The automatically generated textual concepts can be effectively aligned to images using pretrained multimodal models like CLIP, removing the need for training bespoke concept classifiers while still allowing optimization of the concept-class associations.4) Concept bottlenecks created in this way will be inherently more interpretable and allow greater human intervention compared to post-hoc explanation techniques or unmodified end-to-end models.In summary, the key research question is how to automate the concept bottleneck generation process using modern language and vision-language models to create classifiers that are high-performing yet interpretable by design. The paper aims to demonstrate this is viable across diverse datasets and low-data settings.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel interpretable-by-design image classification model called LaBo (Language-Model-Guided Concept Bottleneck Model) that leverages large language models like GPT-3 and vision-language models like CLIP to automatically construct high-performance concept bottleneck models without requiring manual annotation of concepts. 2. Using GPT-3 to generate a large set of candidate textual concepts for each class through prompting. A submodular optimization method is then used to select an informative and diverse subset of concepts to form the bottleneck layer.3. Initializing the concept-class association weights in the final linear layer using priors from GPT-3, which helps improve performance especially in low-data regimes.4. Demonstrating through comprehensive experiments on 11 diverse image classification datasets that LaBo outperforms blackbox models like linear probes in few-shot settings while achieving comparable accuracy with more data.5. Introducing two human evaluation metrics, factuality and groundability, to quantify the interpretability of the automatically constructed bottlenecks. Human judgments show LaBo bottlenecks are more factual and groundable than those constructed from Wikipedia or WordNet.In summary, the main contribution is proposing an interpretable framework LaBo that can automatically construct high-quality concept bottlenecks for arbitrary image classification tasks without needing manual annotation. LaBo is shown to outperform prior concept bottleneck methods and blackbox models in low-data regimes while maintaining interpretability.
