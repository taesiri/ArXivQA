# [On the impossibility of discovering a formula for primes using AI](https://arxiv.org/abs/2308.10817)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper explores the theoretical limits of machine learning within the framework of Kolmogorov complexity theory. Specifically, it aims to address whether machine learning can be used to discover mathematical formulas and proofs, using the problem of finding a formula for prime numbers as a case study. 

Main Contributions:

1. Provides information-theoretic proofs of key results in probabilistic number theory such as the Erdős–Kac theorem on the normality of the number of prime divisors of integers, Chebyshev's theorem on primes, and the Prime Number Theorem on the distribution of primes.

2. Demonstrates an information-theoretic derivation of the Hardy–Ramanujan theorem on the normal order of the number of prime divisors of an integer.

3. Proves the "Prime Coding Theorem" which states that the expected Kolmogorov complexity of the sequence of prime locations below an integer N is on the order of N. This indicates the locations of primes behave like independent random bits. 

4. Uses the Prime Coding Theorem to prove that no formula for generating primes can be discovered through machine learning, regardless of the amount of compute power or training data available. This establishes theoretical limitations of ML for certain mathematical discoveries.

5. Provides a philosophical argument that profound mathematical theorems like the Erdős–Kac theorem which are experimentally unverifiable require creative leaps that may be beyond purely inductive ML methods.

In conclusion, the paper leverages algorithmic information theory to elucidate fundamental limits on the ability of ML to make mathematical discoveries like a prime formula, highlighting the need for incorporating creative human reasoning to advance mathematics. The information-theoretic proofs of major results in number theory are also noteworthy technical contributions.


## Summarize the paper in one sentence.

 This paper explores the theoretical limits of machine learning, establishing the impossibility of discovering a formula for primes using AI through an information-theoretic analysis involving Kolmogorov complexity and algorithmic probability.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1) Using Kolmogorov's theory of algorithmic probability to establish fundamental limits on machine learning, specifically showing that no formula/model for generating prime numbers can be learned. This is formally shown via the Prime Coding Theorem and its corollary.

2) Providing information-theoretic proofs of some key results in probabilistic number theory, including the Erdős–Kac theorem on the normality of the number of distinct prime factors of an integer, and the Hardy–Ramanujan theorem on the normal order of the number of distinct prime divisors of an integer.

3) More broadly, using algorithmic information theory to elucidate concepts like Occam's razor, maximum entropy, the relation between Shannon entropy and expected Kolmogorov complexity, etc. The paper aims to demonstrate how Kolmogorov complexity provides a formal foundation for scientific induction and machine learning.

In summary, the main focus seems to be on exploring the theoretical limitations of machine learning using algorithmic probability theory, especially in the context of number theory and distributional properties of prime numbers.


## What are the keywords or key terms associated with this paper?

 Based on a review of the paper, some of the key terms and concepts include:

- Kolmogorov complexity
- Algorithmic probability
- Occam's razor
- Levin's universal distribution
- Prefix-free codes
- Maximum entropy methods
- Probabilistic number theory
- Prime number theorem
- Erdös-Kac theorem 
- Hardy-Ramanujan theorem
- Gödel's incompleteness theorems

The paper discusses Kolmogorov's theory of algorithmic probability and how it can be used to formalize concepts like Occam's razor. It applies these ideas to problems in probabilistic number theory, deriving results like the prime number theorem, Erdös-Kac theorem, and Hardy-Ramanujan theorem. The paper also discusses maximum entropy methods and prefix-free codes. It concludes by proving Gödel's incompleteness theorem using the idea of algorithmic randomness. So these are some of the main topics and key terms covered in the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1) The paper argues that Kolmogorov Complexity formalizes Occam's razor in the context of scientific induction. However, Kolmogorov Complexity is not computable. So how can we reliably apply Occam's razor in practice using the proposed framework?

2) When defining the algorithmic probability of a string, the paper notes that we need to consider only prefix-free languages to ensure convergence. Can you further explain the connection between prefix-freeness and convergence of the sum?

3) Shannon Entropy is linked to expected Kolmogorov Complexity. However, Shannon Entropy can be negative while Kolmogorov Complexity is non-negative. How do we resolve this potential contradiction? 

4) The proof of the Invariance Theorem relies on the theory of compilers to show the constant $c(U,U')$ is small. But what restrictions need to be placed on the universality of $U$ and $U'$ to ensure the constant stays small? 

5) How exactly does Levin's Coding Theorem allow us to formalize the "entropy of an event"? What interpretation does this provide beyond the standard information-theoretic view?

6) The maximum entropy derivation of the Prime Number Theorem relies on modeling prime locations as independent binary variables. What implications does this independence assumption have on potential ways to predict primes?

7) What is the insight provided by linking the Hardy-Ramanujan Theorem to typical sets via the Asymptotic Equipartition Property? Does this connection generalize?  

8) The paper shows the Erdös-Kac Theorem emerges from modeling integer factorization using independent random variables. What other number theoretic results could be derived by extending this probabilistic modeling approach?

9) How does the use of Algorithmic Probability in the proof relate to or differ fundamentally from a standard probabilistic analysis of integer factorization?

10) What potential objections could one raise about the paper's argument that no machine learning model can reliably predict primes based on past observations? Are there assumptions that could be challenged?
