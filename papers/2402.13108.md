# [On the Stability of Gradient Descent for Large Learning Rate](https://arxiv.org/abs/2402.13108)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper investigates the dynamics and convergence properties of gradient descent when training linear neural networks with a quadratic loss function. Specifically, it aims to understand the behavior when using a large, fixed learning rate (step size).

- Recent work has shown that neural networks can be trained using large learning rates that violate common assumptions, entering an "edge of stability" (EOS) regime characterized by non-monotonic decrease of the loss. However, explanations for this phenomenon in neural nets are lacking.

Proposed Solution:
- The paper proves that for linear nets, the gradient descent map satisfies a key property - it is non-singular, meaning it preserves sets of measure zero under preimages. This allows extending local results to global ones.

- It shows the set of global minima (optima) forms a smooth manifold under mild data conditions. This characterizes the local geometry of the loss landscape near optima.  

- It proves the eigenvalues of the Hessian at optima, representing local sharpness, are proper maps that tend to infinity. Thus stable optima exist only within a bounded region, determined by the step size.

Main Contributions:
- Shows gradient descent for linear nets satisfies a conjecture on avoiding suboptimal extrema via non-singularity, explaining an aspect of EOS.

- Characterizes conditions for stability/instability of optima, proving they disappear outside a bounded set when step size is too large.

- Provides concrete bounds on the step size above which the set of initializations that converge has measure zero, explaining the EOS phenomenon.

- Overall gives a theoretical justification for the edge of stability behavior in linear networks. The analysis techniques could extend to other networks.
