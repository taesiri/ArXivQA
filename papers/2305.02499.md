# [AutoML-GPT: Automatic Machine Learning with GPT](https://arxiv.org/abs/2305.02499)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we build an effective Automatic Machine Learning (AutoML) system using large language models (LLMs) like GPT that can automate machine learning pipelines for diverse AI tasks?The key hypothesis is that by leveraging the natural language processing capabilities and knowledge contained in LLMs like GPT, it is possible to create an AutoML system that can understand task instructions and dataset/model metadata provided by users, and automatically carry out full machine learning workflows including data processing, model architecture design, hyperparameter tuning, training, and evaluation. The authors propose and develop an AutoML system called AutoML-GPT that aims to demonstrate this hypothesis. AutoML-GPT takes as input data cards, model cards, evaluation metrics and other user requests describing the task and data, and leverages GPT to automatically generate workflows for data processing, model selection, hyperparameter tuning, training, and evaluation. The goal is to show that the language modeling capabilities of GPT can effectively automate complex end-to-end machine learning pipelines for diverse tasks spanning computer vision, NLP, etc. The experiments on various datasets aim to validate whether AutoML-GPT can deliver strong performance across different domains in a generalizable and effective way.In summary, the central research question is whether LLMs like GPT can be utilized to create an AutoML system that automates machine learning workflows, and the key hypothesis is that the natural language processing power of GPT can enable such an effective general-purpose AutoML system. The AutoML-GPT system and experiments are designed to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method for improving question answering systems by distilling knowledge from the reader module to the retriever module. Specifically, the paper proposes a distillation framework that allows the retriever to mimic the reader's behavior. This is done by training the retriever to predict the reader's logits over candidate answers using the question and retrieved passages. The key ideas are:- Using a student-teacher distillation setup where the reader acts as the teacher and the retriever acts as the student. The retriever is trained to regress to the reader's output logits.- Adding distillation losses to the retriever training objective in addition to the standard cross-entropy loss. The distillation losses encourage the retriever to focus on passages that lead to correct answers according to the reader.- Using hard negatives mining during distillation training to select challenging incorrect passages that the retriever should avoid retrieving.The proposed distillation approach improves the quality of retrieved passages since the retriever learns to retrieve passages that are useful for answering the question according to the reader's modeling.Experiments on multiple QA datasets demonstrate significant gains in QA accuracy from distilling the reader's knowledge into the retriever. The distillation framework provides an effective way to transfer knowledge from the sophisticated reader model to improve the retriever module in QA systems.In summary, the core contribution is a novel distillation technique to improve retrievers in QA by teaching them to mimic reader behavior for better passage retrieval. The gains show the promise of transferring knowledge from readers to retrievers in QA.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes AutoML-GPT, an automated machine learning system that uses GPT as a bridge to connect diverse AI models and datasets through natural language instructions, enabling automated training pipelines for tackling a wide range of AI tasks.
