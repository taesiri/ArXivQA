# [Cross-Modal Adaptive Dual Association for Text-to-Image Person Retrieval](https://arxiv.org/abs/2312.01745)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel Cross-modal Adaptive Dual Association (CADA) framework for text-to-image person retrieval. The key idea is that cross-modal associations depend on the anchor (text vs image) and thus should be modeled bidirectionally. CADA features an encoder-decoder architecture, where the encoder aligns global features using a new Normalized Distribution Fitting loss, and the decoder enables local-level dual associations. Specifically, the Association of Text Tokens to Image Patches (ATP) module passes information from text tokens to image patches to build token-level associations. And the Association of Image Regions to Text Attributes (ARA) module generates masked text phrases by locating related image regions, requiring the model to learn associations from images to text. Experiments on three datasets demonstrate state-of-the-art performance. Ablations validate the efficacy of the proposed dual association formulation over a single direction, as well as the utility of the new losses and modules introduced. Key strengths include more accurate cross-modal understanding and generalizability.
