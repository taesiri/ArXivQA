# [Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding](https://arxiv.org/abs/2402.05109)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Autoregressive language model (LLM) decoding is inefficient as it generates tokens one-by-one in sequence, making it memory bandwidth bound. This underutilizes GPU compute capacity. 
- Speculative decoding frameworks like Medusa use lightweight "draft heads" to propose candidate multi-token continuations in parallel, which are then verified by the base LLM. However, existing draft heads make independent predictions, limiting accuracy.

Proposed Solution: 
- Propose Hydra heads, which are sequentially dependent draft heads. Each Hydra head takes as input embeddings of earlier speculated tokens when predicting later tokens.
- Explore objectives like teacher loss and architectures like PrefixMLP to further improve Hydra heads.
- Propose Hydra++ heads combining best techniques, including teacher loss and PrefixMLP architecture.

Contributions:
- Propose Hydra heads as simple but more accurate drop-in replacement over prior sequential independent heads.
- Show Hydra decoding improves throughput by up to 1.11x over Medusa.
- Explore objectives and architectures for draft heads to boost throughput further. 
- Hydra++ improves throughput by up to 1.31x and 2.7x over Medusa and autoregressive decoding.

In summary, the paper identifies independence of draft heads as a key limitation of prior speculative decoding schemes. The proposed Hydra heads introduce sequential dependence through token embeddings, significantly boosting throughput. Further optimizations to objectives and architectures lead to additional large improvements by Hydra++ heads.
