# [Split &amp; Merge: Unlocking the Potential of Visual Adapters via Sparse   Training](https://arxiv.org/abs/2312.02923)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes Mixture of Sparse Adapters (MoSA), a novel Adapter Tuning method that enhances the performance of standard adapters for visual tasks without introducing additional parameters or computation. MoSA first splits the standard adapter into multiple non-overlapping sparse adapter modules. During training, it stochastically activates different sparse adapter modules to increase model capacity. Consistency regularization is applied to align the outputs of different modules. After training, the sparse modules are merged into a complete dense adapter for efficient inference. Through this split-merge approach combined with sparse mixed training, MoSA fully unleashes the potential of standard adapters, eliminating redundant parameters. Comprehensive experiments on 27 visual tasks demonstrate that MoSA consistently outperforms all baselines, including full fine-tuning, achieving state-of-the-art results for Adapter Tuning methods. Notably, MoSA excels in challenging low-resource and multi-task scenarios. The integration of sparsity and mixture-of-experts in MoSA maximizes parameter efficiency while ensuring performance.
