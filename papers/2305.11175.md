# VisionLLM: Large Language Model is also an Open-Ended Decoder for
  Vision-Centric Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it aims to address is:How can we develop a unified generalist framework that leverages the capabilities of large language models (LLMs) to handle vision-centric tasks in an open-ended and customizable manner through language instructions?The key ideas and goals of the paper can be summarized as:- LLMs have shown impressive performance on various NLP tasks through user-tailored prompts/instructions. However, their applicability to vision tasks has been limited due to differences in modality and task format. - The paper proposes VisionLLM, a novel framework to align vision tasks with LLM methodologies using language instructions. This allows flexibly defining and customizing vision tasks like LLMs.- VisionLLM consists of three components: (1) unified language instructions for vision tasks, (2) language-guided image tokenizer, (3) LLM-based open-ended task decoder.- It supports customizable vision tasks at different granularities, from object-level to task-level, through language prompts.- Experiments show it achieves strong performance on diverse vision tasks like detection, segmentation, captioning. Remarkably, it gets 60% mAP on COCO using a generalist LLM-based model.In summary, the key research question is how to develop a generalist vision framework that can harness LLMs' reasoning and language capabilities to handle customizable vision tasks through language instructions in an open-ended manner. The VisionLLM model is proposed as a solution.
