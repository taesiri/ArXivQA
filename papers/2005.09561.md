# [Normalized Attention Without Probability Cage](https://arxiv.org/abs/2005.09561)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How do different pooling operations in self-attention affect the ability of Transformer models to perform sequence-level tasks requiring positional reasoning and generalization to longer sequences?The key points are:- The paper compares different pooling operations like max, sum, mean, etc. when using self-attention models like Transformers for sequence-level tasks. - It focuses on tasks that require understanding position and order information in the sequence, like finding the mode in a sequence.- It evaluates how the models generalize when tested on longer sequences than seen during training.- The main hypothesis is that different pooling operations will lead to vastly different abilities to perform positional reasoning and generalize to longer sequences. The paper aims to analyze the trade-offs between common pooling strategies.So in summary, the central question is how pooling in self-attention affects positional reasoning and generalization, with the hypothesis that the choice of pooling has a big impact on these sequence-level capabilities. The paper does an empirical evaluation to analyze the trade-offs.
