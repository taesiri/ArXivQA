# [Towards Open-vocabulary Scene Graph Generation with Prompt-based   Finetuning](https://arxiv.org/abs/2208.08165)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to predict visual relationships for unseen objects, which the authors formulate as the problem of open-vocabulary scene graph generation (Ov-SGG). Specifically, the paper proposes a method to train a model on a set of base/seen object categories and then predict relations on novel/unseen object categories at test time. The key ideas are:1) Pre-train a visual-relation model on region-caption pairs to align visual concepts with open-vocabulary textual descriptions of relations. 2) Use prompt-based finetuning strategies (hard prompt and soft visual-relation prompt) to leverage the knowledge in the pre-trained model to make predictions on unseen objects, without modifying the model parameters.So in summary, the main hypothesis is that by pre-training on open-vocabulary region captions and using prompt-based finetuning, the model can predict relations on novel object categories not seen during training. This addresses the limitation of prior SGG methods that require all object classes to be specified upfront.


## What is the main contribution of this paper?

The main contribution of this paper is proposing the new task of open-vocabulary scene graph generation (Ov-SGG) and developing a method to address it. Specifically:- The paper defines the novel Ov-SGG task, where a model is trained on base object classes but tested on predicting relations for both base and unseen target object classes. This is more realistic and challenging than standard closed-vocabulary SGG.- A two-step method is proposed involving visual-relation pretraining and prompt-based finetuning. First, a visual-relation model (VRM) is pretrained on region-caption pairs to align visual concepts with textual descriptions. Then the VRM is finetuned via prompt engineering without modifying its parameters, to preserve its generalization ability. - Two prompt strategies are designed - hard prompt and soft visual-relation prompt. The latter uses a visual-to-textual decoder to generate soft prompts incorporating both language and visual information.- Experiments on major SGG benchmarks (Visual Genome, GQA, OpenImages) show the method significantly outperforms recent SGG techniques on Ov-SGG. It also surpasses baselines on closed vocabulary SGG and is the only method that can handle zero-shot object SGG.In summary, the key contribution is proposing the more realistic Ov-SGG task and developing a novel visual-relation pretraining + prompt finetuning approach that achieves strong performance on this challenging open-vocabulary setting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes open-vocabulary scene graph generation where models trained on base object classes are evaluated on predicting relationships involving unseen target classes, using a two-step method of pretraining a visual-relation model on region captions then finetuning it with prompt strategies without changing the pretrained parameters, outperforming prior methods on open-vocab and closed-vocab scene graph tasks.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper on open-vocabulary scene graph generation compares to other related work:- It addresses a new problem setting - open-vocabulary scene graph generation (Ov-SGG) - where the model must predict relationships between objects, including novel object categories not seen during training. This is more challenging and realistic than most prior work which assumes a closed vocabulary of objects. - The proposed method uses a two-step approach with visual-language pretraining on dense region captions, followed by prompt-based finetuning. This differs from prior work which typically uses end-to-end training and standard finetuning techniques. The visual-language pretraining and prompt tuning aims to improve generalization to novel objects.- Extensive experiments are conducted on three datasets - Visual Genome, GQA, and OpenImages - to evaluate both the new Ov-SGG setting as well as the conventional closed SGG setting. The method outperforms recent state-of-the-art models on both tasks.- This is the first work to explore prompt-based learning for scene graph generation. Most prior work on prompts has been in NLP. By adapting prompts for SGG, the method achieves better generalization than standard finetuning.- The only other work on open-vocabulary SGG is concurrent work by Zhong et al. However, their setting assumes the target classes are a subset of the training classes, whereas this work tackles unseen classes not in the training data.In summary, this paper introduces a new, challenging problem formulation, an approach tailored to improving generalization via pretraining and prompts, extensive experiments on multiple datasets, and comparisons showing state-of-the-art performance on both the new Ov-SGG task and standard SGG. The techniques could prove useful for other vision-language tasks as well.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Integrating open-vocabulary object detection into the model for Ov-SGG, so that it can handle the scene graph detection (SGDet) task under the open-vocabulary setting. The current model cannot detect novel objects, only classify them.- Jointly training the object detection network and visual-relation alignment model in a unified framework. Currently the object detector is fixed during pre-training. End-to-end training could improve performance. - Exploring different decoder networks and prompts for the soft visual-relation prompt strategy. The design of prompts and decoders is a key aspect that can likely be further optimized.- Evaluating the model on more diverse datasets beyond VG, GQA and OpenImages used in the paper. Testing generalization to other datasets would be useful.- Extending the model to handle other open-vocabulary vision-language tasks like image captioning and VQA, not just focused on SGG. The pretrained visual-relation model may transfer well.- Improving computational efficiency. The model is computationally intensive, especially the soft prompt strategy. Reducing cost would be important for real applications.In summary, the key future directions are improving the model's capabilities for full end-to-end Ov-SGG, exploring different prompt strategies, testing generalization, and improving efficiency - while also extending the overall approach to other vision-language tasks. The concept of open-vocabulary modeling is highly promising.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes the new task of open-vocabulary scene graph generation (Ov-SGG), where a model is trained on a set of base object classes but tested on predicting relations for both base and novel target object classes. The authors develop a two-step method involving visual-relation pretraining on region-caption pairs and prompt-based finetuning. First, they pretrain a cross-modal model with transformers to align visual concepts from regions with corresponding textual descriptions from dense captions in Visual Genome. This provides open-vocabulary textual grounding. Then they finetune using prompt engineering strategies like hard prompt and soft visual-relation prompt, which adapt the pretrained model to Ov-SGG without updating parameters. Experiments on Visual Genome, GQA, and OpenImages show their method significantly outperforms recent SGG techniques on Ov-SGG and also on standard closed SGG. Their model is the only one capable of handling completely unseen target objects. The work represents an important advance towards real-world application of SGG.
