# [GETMusic: Generating Any Music Tracks with a Unified Representation and   Diffusion Framework](https://arxiv.org/abs/2305.10841)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is: How can we develop a unified model that is capable of flexibly generating any desired target instrumental tracks in symbolic music, either from scratch or conditioned on arbitrary user-provided source tracks?The key issues this paper tries to address are:- Prior sequence-based and image-based approaches for symbolic music generation have limitations in generating diverse target tracks due to constraints in the music representations and model architectures used.- There is a need for a unified model that can handle the diverse and flexible combination of source and target tracks in music composition.To address these issues, the paper proposes GETMusic, which consists of two main components:- A new multi-track music representation called GETScore that efficiently represents notes as tokens in a 2D structure with tracks stacked vertically.- A conditional discrete diffusion model called GETDiff that can flexibly generate any user-specified target tracks by masking and then denoising the target tracks based on provided source tracks during training.The central hypothesis is that the co-designed representation and diffusion model will enable flexible control over generating any desired target tracks from scratch or conditioned on arbitrary source tracks to meet diverse music composition needs with a single unified model.In summary, the key research question is how to develop a unified symbolic music generation model capable of producing any target instrumental tracks flexibly based on arbitrary source tracks, which GETMusic aims to address through its novel representation and architecture.
