# [Transferable Adversarial Attacks on Vision Transformers with Token   Gradient Regularization](https://arxiv.org/abs/2303.15754)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to improve the transferability of adversarial attacks on vision transformers (ViTs). Specifically, the authors aim to develop an effective transfer-based attack method that can generate adversarial examples using a source ViT model that can fool other target ViT and CNN models. 

The key hypothesis is that by regularizing the backpropagated gradients in the intermediate blocks of ViTs during the attack generation process, they can reduce the variance of the gradients and make the adversarial examples less reliant on model-specific features. This should improve the transferability of the attacks across different model architectures.

To test this hypothesis, they propose a Token Gradient Regularization (TGR) method that eliminates extreme gradients for individual patch tokens in the Attention, MLP, and QKV components of ViT blocks. By smoothing the gradients in this way, the adversarial examples are less likely to get stuck in poor local optima during optimization and should transfer better.

The main research contributions are:

- Proposing TGR to regularize gradients inside ViT blocks to improve transferability.

- Extensive experiments showing TGR outperforms prior attacks by significant margins against ViT and CNN models.

- Demonstrating TGR can be combined with other attacks like PatchOut for further gains.

- Analyses revealing TGR reduces gradient variance throughout ViT models compared to other attacks.

So in summary, the key research question is how to craft more transferable adversarial attacks on ViTs, with the hypothesis that regularizing intermediate gradients will help. The TGR method is proposed and evaluated to address this question.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new attack method called Token Gradient Regularization (TGR) for generating transferable adversarial examples against vision transformers (ViTs). 

2. TGR regularizes the backpropagated gradients in each internal block of ViTs in a token-wise manner to reduce the gradient variance and make the update direction more stable. This helps the adversarial examples escape poor local optima and be more transferable. 

3. Extensive experiments show that TGR outperforms state-of-the-art attacks like MIM, VMI, SGM, and PNA by a significant margin in attacking both ViTs and CNNs. It improves transferability by 8.8% on average against ViTs and 6.2% against CNNs.

4. Analysis shows that TGR effectively reduces the gradient variance in intermediate layers during backpropagation. Experiments also demonstrate its effectiveness when adapted to attack CNNs.

5. Ablation studies validate the contribution of different components in ViTs to transferability and the impact of the number of regularized tokens.

In summary, the key contribution is proposing TGR to improve transferable adversarial attacks on ViTs by regularizing backpropagated gradients in a token-wise manner according to the architecture of ViTs. Both analysis and comprehensive experiments demonstrate its effectiveness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a Token Gradient Regularization (TGR) method to generate more transferable adversarial samples against vision transformers by reducing the variance of backpropagated gradients in the intermediate layers of the model during the attack process.
