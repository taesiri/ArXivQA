# [Transferable Adversarial Attacks on Vision Transformers with Token   Gradient Regularization](https://arxiv.org/abs/2303.15754)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to improve the transferability of adversarial attacks on vision transformers (ViTs). Specifically, the authors aim to develop an effective transfer-based attack method that can generate adversarial examples using a source ViT model that can fool other target ViT and CNN models. 

The key hypothesis is that by regularizing the backpropagated gradients in the intermediate blocks of ViTs during the attack generation process, they can reduce the variance of the gradients and make the adversarial examples less reliant on model-specific features. This should improve the transferability of the attacks across different model architectures.

To test this hypothesis, they propose a Token Gradient Regularization (TGR) method that eliminates extreme gradients for individual patch tokens in the Attention, MLP, and QKV components of ViT blocks. By smoothing the gradients in this way, the adversarial examples are less likely to get stuck in poor local optima during optimization and should transfer better.

The main research contributions are:

- Proposing TGR to regularize gradients inside ViT blocks to improve transferability.

- Extensive experiments showing TGR outperforms prior attacks by significant margins against ViT and CNN models.

- Demonstrating TGR can be combined with other attacks like PatchOut for further gains.

- Analyses revealing TGR reduces gradient variance throughout ViT models compared to other attacks.

So in summary, the key research question is how to craft more transferable adversarial attacks on ViTs, with the hypothesis that regularizing intermediate gradients will help. The TGR method is proposed and evaluated to address this question.
