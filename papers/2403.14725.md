# [Jailbreaking is Best Solved by Definition](https://arxiv.org/abs/2403.14725)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Recently, attackers have been able to elicit unsafe or undesirable responses from language models through "jailbreaking" attacks. This has led to a flurry of defenses aimed at preventing such outputs. 

- The paper critically examines two stages of defenses: (1) defining what constitutes unsafe outputs, and (2) enforcing that definition via input processing or model fine-tuning. 

- It introduces the "Purple Problem" as a simple, well-defined task of preventing a model from outputting the word "purple". Surprisingly, even for this simple problem, current defenses fail.

Key Insights
- The paper shows that current enforcement mechanisms (input filtering, fine-tuning) fail to defend even against the Purple Problem, whereas output filtering is perfectly robust. 

- This suggests that the real challenge lies in obtaining a good definition of unsafe responses. With a good definition, output filtering works well; without it, no enforcement method can succeed.

- The paper argues that most innovations around enforcement provide little value without clear progress on better specifying unsafe behaviors. It advocates for more research effort into developing methods for capturing nuanced definitions of unsafe outputs.

Proposed Solutions and Contributions
- The paper recommends that defenses be rigorously benchmarked against adaptive attacks, even for simple definitions like the Purple Problem. This prevents a false sense of security.

- It highlights output filtering as an important baseline for language model security, owing to its reliability in enforcing definitions.

- The key insight is that progress on language model security fundamentally relies on better definitions. The paper contends innovation should focus more on this rather than enforcement.

In summary, the paper makes a strong case that language model security research should prioritize developing methods for precisely defining unsafe behaviors over innovations in enforcement algorithms. It supports this using experiments revealing deficiencies in current defenses even for very simple definitions of unsafe outputs.
