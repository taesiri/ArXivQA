# [Leaping Into Memories: Space-Time Deep Feature Synthesis](https://arxiv.org/abs/2303.09941)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, it seems the main research question is: How can we visualize and interpret the learned internal representations of video action recognition models? Specifically, the authors propose a method called LEAPS (LEArned Preconscious Synthesis) to synthesize videos that represent the learned spatiotemporal features of video models. The key ideas are:- Using "model priming" with a stimulus video to activate relevant learned concepts in the model related to a target class. This is inspired by visual priming in cognitive science.- Iteratively optimizing a video initialized with noise to match the internal representations of the primed model for the target class.- Adding regularizers to encourage temporal coherence across frames and diversity of features. - Evaluating the approach on a range of video models like 3D CNNs and temporal transformers without modifications.So in summary, the main research question is how to visualize and interpret what spatiotemporal video models have learned about different action classes, by synthesizing representative video features through model inversion. The LEAPS method is proposed as a generalizable approach to address this question across architectures.
