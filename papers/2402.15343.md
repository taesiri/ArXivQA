# [NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data](https://arxiv.org/abs/2402.15343)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Named Entity Recognition (NER) is an important NLP task for extracting and classifying entities from text. Custom NER models typically rely on transformer encoders like BERT that are pre-trained on masked language modeling and then fine-tuned on human-annotated data. However, this requires substantial human effort to annotate data for every new NER problem. 

Recently, large language models (LLMs) like GPT-3 have shown impressive abilities to annotate data automatically. But using them directly for inference has issues like high computational cost.

Proposed Solution:
This paper proposes to use GPT-3.5 to auto-annotate a diverse multi-domain corpus and then train a compact "task-specific foundation model" called NuNER specialized for NER. NuNER leverages the auto-annotation abilities of LLMs without their inference costs.  

Specifically, they prompt GPT-3.5 to freely annotate entities and concepts from a subset of the C4 corpus, obtaining 4.38M annotations spanning 200K concepts. They then train a RoBERTa model on this data using a contrastive learning objective that aligns token embeddings with relevant concept embeddings.

The resulting 155M-parameter NuNER model can be fine-tuned on downstream NER tasks in a data-efficient few-shot way, outperforming regular RoBERTa and RoBERTa trained on other NER datasets. In experiments, NuNER also competes with finetuned GPT-3.5 and GPT-4 for over 8 training examples per type.

Main Contributions:

1) Demonstrate a method to create compact task-specific foundation models for NER using LLM-annotations.

2) Identify design choices like concept diversity and data size that improve model performance.

3) Release NuNER, an encoder model specialized for NER that achieves SOTA few-shot performance and can function as a drop-in replacement for RoBERTa.

4) Release the LLM-annotated NER dataset used to train NuNER, containing rich concept diversity.

In summary, this paper shows the promise of using LLMs to create customizable high-quality models for classic NLP problems. The proposed NuNER model and training methodology significantly advance the state of the art in data-efficient NER.
