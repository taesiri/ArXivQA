# [REPrune: Channel Pruning via Kernel Representative Selection](https://arxiv.org/abs/2402.17862)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Channel pruning techniques aim to accelerate convolutional neural networks (CNNs) by removing redundant filters/channels, leading to a narrower yet dense model that can be readily deployed. However, pruning at the granularity of entire channels often results in significant accuracy drops due to the inability to precisely determine what and where to prune within the CNN. Prior works have shown that finer-grained kernel pruning can better preserve accuracy but produces sparse CNN models that require extra software/hardware optimization for efficient execution.

Proposed Solution:
This paper proposes REPrune, a novel channel pruning technique that emulates the finer granularity of kernel pruning to avoid accuracy degradation while retaining the dense pruned model benefits of channel pruning. 

The key idea is to identify and group similar kernels within each channel using agglomerative clustering with Ward's linkage. The monotonic distance growth in Ward's method allows determining consistent per-channel clusters based on the target pruning ratio of each layer. REPrune then selects channels that maximize the coverage of representative kernels from each cluster through a proposed optimization of the maximum coverage problem (MCP). This way, redundancy is minimized while preserving as much information as possible.

REPrune is embedded within a progressive training-pruning loop, avoiding separate fine-tuning. It starts by pruning less important channels during training based on batch norm scaling factors. The pruned channels are gradually recovered to enable continuous model optimization. At intervals, REPrune further prunes channels through the proposed techniques.

Main Contributions:
- Proposes REPrune, a channel pruning technique that identifies critical kernels through clustering and selects filters based on their coverage to emulate kernel pruning effects while retaining dense models.

- Embeds REPrune within concurrent training-pruning by utilizing batch norm scaling factors and channel recovery to enable efficient automated model pruning.

- Demonstrates state-of-the-art image classification accuracy in CIFAR and ImageNet datasets compared to prior arts, even at high pruning rates. Also shows minor accuracy drops for object detection in COCO.
