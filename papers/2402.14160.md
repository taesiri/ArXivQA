# [Recursive Speculative Decoding: Accelerating LLM Inference via Sampling   Without Replacement](https://arxiv.org/abs/2402.14160)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) suffer from slow inference speed due to their auto-regressive nature and memory bandwidth bottlenecks. 
- Speculative decoding methods have been proposed to accelerate LLM inference by using a smaller draft model to generate draft tokens in parallel, followed by the target LLM verifying them.
- Recent tree-based methods build a draft token tree for better diversity, but sample tokens independently without considering dependencies, limiting the tree's diversifiability.

Proposed Solution:
- The paper proposes Recursive Speculative Decoding (RSD), a novel tree-based method that constructs the draft token tree by sampling without replacement to maximize diversity.
- Two versions are presented - RSD with Constant branching factors (RSD-C) uses Gumbel-Top-k to sample tokens without replacement, while RSD with Stochastic Beam Search (RSD-S) samples sequences without replacement, allowing early truncation of unlikely sequences to reduce computational cost.
- A new theoretical concept called recursive rejection sampling is introduced to verify the sampling-without-replacement draft tree while still recovering the exact target LLM distribution.

Main Contributions:
- Introduction of recursive rejection sampling for speculative decoding with sampling without replacement draft distributions.
- Proposal of RSD algorithms that construct highly diverse draft token trees via sampling without replacement using Gumbel-Top-k (RSD-C) or Stochastic Beam Search (RSD-S). 
- Empirical evaluation from two perspectives - fixed draft sequence length and fixed computational budget, showing RSD outperforms baselines like speculative decoding and SpecTr overall.
- Establishes the benefit of sampling without replacement for improving diversity of draft token trees in speculative decoding.

In summary, the key innovation of the paper is RSD, which builds a diverse draft token tree by sampling without replacement, enabling faster and more accurate LLM inference. The theoretical concept of recursive rejection sampling supports this approach.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes Recursive Speculative Decoding, a novel tree-based speculative decoding method for accelerating large language model inference that constructs draft token trees by sampling without replacement and verifies the trees via a generalized multi-round rejection sampling approach to recover the target model's distribution.


## What is the main contribution of this paper?

 This paper proposes Recursive Speculative Decoding (RSD), a novel tree-based speculative decoding method for accelerating inference of large language models. The main contributions are:

1) It proposes recursive rejection sampling, a generalization of multi-round speculative decoding, that can verify draft tokens sampled without replacement and recover the target model's distribution.

2) It presents two RSD algorithms - RSD with Constant branching factors (RSD-C) and RSD with Stochastic Beam Search (RSD-S) - that build draft token trees by sampling without replacement to maximize diversity. 

3) It conducts experiments from two perspectives - fixed draft sequence length and fixed target computational budget. It shows RSD consistently outperforms baseline speculative decoding methods in the first perspective and achieves superior performance over baselines in most experiments in the second perspective.

In summary, the main contribution is proposing RSD, a novel tree-based speculative decoding method that uses sampling without replacement to maximize draft token diversity and leverages recursive rejection sampling to verify the draft tokens, outperforming previous works.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Speculative decoding - An inference acceleration method for large language models that uses a smaller "draft" model to generate draft token sequences in parallel, which are then verified by the target model.  

- Draft model - The smaller language model used in speculative decoding to generate draft token sequences.

- Target model - The large language model that speculatively decodes the draft token sequences from the draft model in parallel.

- Sampling without replacement - A key aspect of the proposed Recursive Speculative Decoding (RSD) method, where draft tokens are sampled without replacement to maximize diversity of the draft token tree.

- Draft token tree - The tree data structure composed of draft tokens generated by the draft model, which allows parallel decoding by the target model.

- Recursive rejection sampling - The proposed method to verify the sampling-without-replacement draft token tree while still recovering the target model distribution.

- Gumbel-Top-k trick - A technique used to efficiently draw samples without replacement. Used in one version of RSD.  

- Stochastic Beam Search - A method to sample sequences without replacement used in the other proposed version of RSD. Allows early truncation of unlikely sequences.

- Block efficiency - One of the metrics used to evaluate performance, measuring tokens generated per target model call.

- Memory-Bound Speed-Up (MBSU) - Another metric used, measuring speedup by assuming runtime is proportional to model size.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the recursive speculative decoding method proposed in the paper:

1) How does recursive rejection sampling generalize multi-round rejection sampling to allow for sampling without replacement distributions? What is the key theoretical result that enables this?

2) What are the key differences between RSD with constant branching factors (RSD-C) versus RSD with stochastic beam search (RSD-S) for building the draft token tree? What are the tradeoffs between these two approaches? 

3) How does recursive rejection sampling exploit the sampling without replacement property during the draft token verification phase? Why is this important for improving diversity and acceptance rates?

4) What modifications need to be made at the software/hardware level to allow the recursive speculative decoding method to work efficiently in practice? 

5) The paper claims improved empirical results but primarily uses BLEU/ROUGE as evaluation metrics. How could the method be evaluated more directly in terms of accuracy and quality?

6) Could the method be extended to use multiple draft models? What changes would need to be made to recursive rejection sampling and the tree construction phase?

7) How does the performance compare when using a very small draft model versus a draft model that is an order of magnitude smaller than the target model? What are the tradeoffs?

8) How does recursive speculative decoding compare to other parallel decoding methods like simultaneous greed or beam search? What are the pros and cons?

9) What language-specific characteristics could make recursive speculative decoding not work well? How could the method be adapted for morphologically rich languages?

10) From an industrial deployment standpoint, what engineering changes would be required in practice to adopt recursive speculative decoding without sacrificing too much accuracy or latency?
