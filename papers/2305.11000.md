# [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal   Conversational Abilities](https://arxiv.org/abs/2305.11000)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an inherent cross-modal large language model with the capability of perceiving multi-modal inputs and generating multi-modal outputs?More specifically, the key research questions appear to be:- How can we integrate speech into large language models through discrete representations to enable both understanding and generation capabilities?- How can we construct a large-scale cross-modal speech-text instruction dataset to train the model for diverse human instructions and spoken dialogues? - What training strategies can enable efficient cross-modal transfer in the large language model?The overarching goal is to develop a single large language model that can handle multiple modalities using discrete representations, with a focus on speech in this work. The hypothesis seems to be that incorporating speech units into the vocabulary and further pretraining and fine-tuning will allow the model to perceive speech input and generate speech output in response to instructions and during dialogues. The experiments aim to validate whether the proposed SpeechGPT model achieves strong cross-modal and spoken dialogue capabilities.In summary, the central research question is how to create a cross-modal language model that can understand and generate speech, and the key hypothesis is that this can be achieved by integrating discrete speech representations into the model vocabulary and training framework. The model capabilities are evaluated through human assessments.
