# [Evolution Transformer: In-Context Evolutionary Optimization](https://arxiv.org/abs/2403.02985)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Evolutionary optimization algorithms like evolution strategies (ES) are inspired by biological evolution and struggle to leverage information obtained during optimization. They lack adaptability to the problem at hand. Recent work has tried to address this by meta-optimizing parts of ES algorithms, but this still has limitations.

Proposed Solution:
The paper introduces EvoTransformer (EvoTF), a causal Transformer architecture that can flexibly represent an ES algorithm. EvoTF takes as input a trajectory of evaluations, search distribution statistics, and outputs an update to improve the search distribution. 

Key features of EvoTF:
- Uses self- and cross-attention to induce desirable properties: invariant to order of population members, equivariant to order of search dimensions.
- Can be trained via Evolutionary Algorithm Distillation (EAD) - uses trajectories from teacher ES algorithms to supervise EvoTF with KL divergence loss between teacher and student distribution updates.
- After EAD, EvoTF shows in-context optimization ability on unseen tasks and can generalize to new search dimensions/population sizes.

Other contributions:
- Analysis shows EvoTF captures desirable ES properties like scale-invariance and perturbation adaptation
- Compare EAD to meta-evolving EvoTF weights - meta-evolution overfits, EAD generalizes better
- Propose Self-Referential EAD to remove need for teacher algorithm - uses perturbations of EvoTF to generate trajectories, filters them by performance, distills the best ones. Can bootstrap improvements.

In summary, the paper introduces EvoTF, a Transformer ES architecture that can leverage trajectories from teacher algorithms or its own perturbations to learn in-context evolutionary optimization. This is a promising approach for data-driven discovery of optimization algorithms.
