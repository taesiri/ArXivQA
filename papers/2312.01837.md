# [Prompting Disentangled Embeddings for Knowledge Graph Completion with   Pre-trained Language Model](https://arxiv.org/abs/2312.01837)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes PDKGC, a novel prompt-tuning based method for knowledge graph completion. PDKGC is built on top of a frozen pre-trained language model and incorporates both textual and structural information through two specialized prompts. It includes a disentangled graph learner to capture multiple semantic aspects of entities via relation-aware neighborhood attention. These disentangled entity embeddings are projected into a prompt sequence which is fed into the language model along with the textual description of the triple, reformulated as a masking task via a hard prompt. This allows the language model to predict the missing entity by fusing relevant structure and text knowledge. Additionally, a separate structural predictor feeds the disentangled embeddings to a knowledge graph embedding model for complementing predictions. Evaluations on two benchmarks show PDKGC outperforming state-of-the-art fine-tuned language model methods and joint structure-text models. The disentangled prompts are shown to effectively incorporate multi-aspect neighborhood semantics while avoiding full fine-tuning. PDKGC provides an accurate, parameter-efficient approach to knowledge graph completion through targeted prompt-tuning.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Knowledge graphs (KGs) suffer from incompleteness despite their increasing use in various applications. Knowledge graph completion (KGC) aims to predict missing facts based on existing ones. Recent methods utilize pre-trained language models (PLMs) to encode textual information for KGC, but mostly by fine-tuning PLMs which is costly and prone to overfitting. Meanwhile, how to effectively incorporate structural knowledge remains a challenge.

Proposed Solution: 
The paper proposes a novel prompt-tuning based method called PDKGC for KGC using frozen PLMs. It includes two key components:

1) A disentangled graph learner that learns multiple representation components for each entity to encode different semantic aspects based on its neighbors.

2) A hard task prompt reformulating KGC as masked token prediction and a disentangled structure prompt generated from the learned components to incorporate relevant structural knowledge into the PLM. 

After encoding by the frozen PLM, PDKGC has two predictors - a textual one directly predicting entities based on the output vector of the masked token, and a structural one forwarding the output prompts to a KGE model for prediction. Their combination leads to more comprehensive results.

Main Contributions:
- Proposes prompt-tuning for KGC to reduce training costs and overfitting risks of fine-tuning PLMs.
- Designs a hard task prompt to adapt KGC to PLMs' pre-training objectives. 
- Learns disentangled entity representations and generates structure prompts to enable fine-grained fusion of textual and structural knowledge using PLMs' self-attention.
- Builds two complementary predictors based on structure-enhanced text encoding and text-enhanced structure encoding for entity prediction.

Experiments show PDKGC often outperforms state-of-the-art methods on two benchmarks. The effectiveness of its components is also verified.
