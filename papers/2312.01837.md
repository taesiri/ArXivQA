# [Prompting Disentangled Embeddings for Knowledge Graph Completion with   Pre-trained Language Model](https://arxiv.org/abs/2312.01837)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes PDKGC, a novel prompt-tuning based method for knowledge graph completion. PDKGC is built on top of a frozen pre-trained language model and incorporates both textual and structural information through two specialized prompts. It includes a disentangled graph learner to capture multiple semantic aspects of entities via relation-aware neighborhood attention. These disentangled entity embeddings are projected into a prompt sequence which is fed into the language model along with the textual description of the triple, reformulated as a masking task via a hard prompt. This allows the language model to predict the missing entity by fusing relevant structure and text knowledge. Additionally, a separate structural predictor feeds the disentangled embeddings to a knowledge graph embedding model for complementing predictions. Evaluations on two benchmarks show PDKGC outperforming state-of-the-art fine-tuned language model methods and joint structure-text models. The disentangled prompts are shown to effectively incorporate multi-aspect neighborhood semantics while avoiding full fine-tuning. PDKGC provides an accurate, parameter-efficient approach to knowledge graph completion through targeted prompt-tuning.
