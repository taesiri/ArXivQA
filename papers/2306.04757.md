# [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large   Language Models](https://arxiv.org/abs/2306.04757)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question or hypothesis is:How can we conduct a more comprehensive, holistic evaluation of the capabilities of instruction-tuned large language models, in order to gain a deeper understanding of their potential? The authors note that despite the impressive capabilities shown by models like GPT-4, there is still a lack of systematic, rigorous assessment to truly understand their full abilities. This is due to factors like the black-box nature of many models, and the lack of multi-faceted evaluation studies. To address this gap, the paper introduces InstructEval, a new benchmark suite designed specifically to evaluate instruction-tuned LLMs in a more well-rounded way. InstructEval aims to assess models across three key dimensions:1) Problem-solving abilities (through exams, reasoning tasks, etc) 2) Writing capabilities 3) Alignment with human valuesBy testing models in this comprehensive manner, spanning both objective metrics and more qualitative human judgments, the goal is to shed light on the factors that determine performance, and foster advancements in the capabilities of these AI systems.In summary, the central hypothesis is that current evaluation practices are limited, and a more holistic methodology (as proposed through InstructEval) is needed to truly understand and advance instruction-tuned LLMs. The research questions surround how to design such a methodology, and what insights it will reveal about these models.
