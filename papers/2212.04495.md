# [MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis](https://arxiv.org/abs/2212.04495)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question/hypothesis of this paper is:

Can denoising diffusion probabilistic models (DDPMs) be effectively applied to conditional 3D human motion synthesis to generate diverse, temporally plausible, and semantically accurate motion sequences? 

The key ideas and contributions seem to be:

- Proposing the first DDPM-based framework (called MoFusion) for conditional 3D human motion synthesis.

- Demonstrating how to incorporate kinematic losses into the DDPM training framework through a time-varying weight schedule. This helps enforce motion plausibility constraints.

- Showing how the framework can be conditioned on different modalities like music (for dance generation) and text (for text-to-motion synthesis).

- Achieving state-of-the-art performance on diversity/multi-modality metrics while generating motions that are temporally smooth and semantically consistent with the conditioning. 

- Formulating interactive applications like motion forecasting, in-betweening, and editing by leveraging the learned latent space.

So in summary, the central hypothesis is on the viability of DDPMs for conditional motion synthesis to achieve diverse but high-quality results. The method and experiments provide evidence to support this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes MoFusion, the first method for conditional 3D human motion synthesis using denoising diffusion models (DDPM). 

2. It shows how domain-inspired kinematic losses can be incorporated into the diffusion framework during training, thanks to a proposed time-varying weight schedule. This results in synthesized motions that are diverse, temporally plausible, and semantically accurate.

3. The method is conditioned on various signals - music and text. For music-to-choreography generation, the results generalize well to new music and do not suffer from repetitiveness.

4. The proposed framework enables interactive editing applications of synthesized motions, like motion forecasting, in-betweening, and seed-conditioned synthesis. 

5. Comprehensive quantitative evaluations and a perceptual user study demonstrate the effectiveness of the proposed MoFusion method compared to prior state-of-the-art on established benchmarks.

In summary, the key novelty is the proposal of a new diffusion-based framework for conditional human motion synthesis. By incorporating domain losses through the proposed weighting schedule, it generates high quality and diverse motions conditioned on modalities like music and text. The experiments validate the claims both quantitatively and via a user study.
