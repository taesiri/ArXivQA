# [AI for Biomedicine in the Era of Large Language Models](https://arxiv.org/abs/2403.15673)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
With recent advances in large language models (LLMs) like ChatGPT for natural language tasks, an intriguing question arises - can we leverage LLMs to drive discoveries in biomedicine using the three key types of biomedical data: textual, biological sequences, and brain signals? These data types resemble language in their sequential structure. 

Proposed Solution:
This paper explores the application of LLMs to the three biomedical data types to harness their potential for biomedical insights. 

Key Contributions:

1) LLMs on Biomedical Textual Data:
- Reviews transformer-based models like SciBERT, ClinicalBERT, BioBERT, and recent models like BioMegatron, SciFive, PubMedBERT, BioLinkBERT, Galactica, BioGPT, etc.
- Discusses clinical applications like treatment planning and research applications like information extraction.

2) LLMs on Biological Sequences: 
- Discusses application to DNA (models like Enformer, Nucleotide Transformer), RNA (RNABERT, RNA-FM), protein (ProteinBERT, ESM-1b) and multi-omics (scBERT, Geneformer) sequences.
- Models predict structures, interactions, functions and associations with diseases.

3) LLMs on Brain Signals:
- Introduces models like BrainBERT, MMM, LaBraM for pre-training representations from EEG signals. 
- Discusses applications in decoding signals to text, images and other modalities.

Overall, the paper provides a comprehensive overview of emerging research at the intersection of large language models and biomedicine spanning diverse data types. It highlights state-of-the-art models, promising applications and exciting future directions in this rapidly evolving field.
