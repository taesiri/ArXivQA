# [Position Prediction as an Effective Pretraining Strategy](https://arxiv.org/abs/2207.07611)

## What is the central research question or hypothesis that this paper addresses?

 Based on the provided abstract, this paper proposes a novel pretraining strategy called "Position Prediction as an Effective Pretraining Strategy" for transformers in computer vision and speech recognition tasks. The key ideas are:

- Instead of reconstructing masked image patches or speech frames like in MAE or BERT, they propose to predict the positions of masked patches/frames based only on their content. This forces the model to learn about relationships between different parts of the input.

- They formulate it as a classification task among all possible positions for each masked patch/frame. 

- This approach does not require reconstructing dense outputs like images or speech.

- They show this pretraining strategy brings improvements over strong baselines in image classification on CIFAR-100, Tiny ImageNet and ImageNet-1K, as well as a speech command classification task.

- Their method enables transformers without position embeddings to outperform ones trained with position information, suggesting much of the transformer's power lies in modeling co-occurrence of input tokens.

In summary, the central hypothesis is that position prediction based on content is an effective pretraining strategy for transformers in computer vision and speech tasks, even without using position embeddings. The key advantage is bypassing reconstruction while still learning relationships between different parts of the input.


## What is the main contribution of this paper?

 Based on the abstract, the main contribution of this paper seems to be proposing a novel unsupervised pretraining technique for Transformers called Masked Patch Position Prediction (MP3). The key ideas are:

- Instead of reconstructing masked input tokens like in BERT and MAE, MP3 removes the positional information of input tokens and trains the model to predict the position of each token. 

- This forces the model to learn about relationships between different parts of the input from the content alone, without relying on their positions. 

- MP3 only uses a simple cross-entropy loss on predicted token positions, avoiding more complex reconstruction losses.

- Experiments on vision and speech benchmarks show MP3 improves over supervised baselines and matches performance of other unsupervised methods like MAE, despite its simplicity. 

- Remarkably, MP3 enables strong performance even without using positional embeddings, sometimes outperforming models trained with full position information.

In summary, the main contribution is proposing MP3 as an effective and simple alternative to reconstruction-based pretraining like BERT and MAE, where the model learns by predicting token positions instead of reconstructing token values.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a simple yet effective self-supervised pretraining method for Transformers where the model predicts patch positions from content alone, without relying on reconstructing dense patch values like in autoencoder-based methods.
