# [Lessons from Usable ML Deployments and Application to Wind Turbine   Monitoring](https://arxiv.org/abs/2312.02859)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper discusses lessons learned from deploying usable machine learning systems in real-world settings, focusing specifically on an application for predicting wind turbine brakepad failures. The authors highlight three main lessons. First, they introduce the concept of "bridges" - people within companies tasked with connecting domain experts to ML developers. Bridges play a key role in iterating on usable ML interfaces. Second, the authors present their system, Sibyl, for easily configuring usable ML interfaces. Sibyl has a Python library for generating explanations, an API, and an editable UI. Third, the authors argue for continuous evaluation by tracking domain-specific key performance indicators (KPIs) after deployment, rather than one-off user studies. They are applying these lessons to develop a usable ML system that helps turbine operators efficiently decide when to investigate potential brakepad failures onsite. Configurable interfaces show model explanations and supplementary information to aid this decision-making. Through careful application of these lessons around roles, systems, and evaluation, the authors demonstrate the potential for usable ML to positively impact the renewable energy industry.


## Summarize the paper in one sentence.

 This paper presents lessons learned from deploying usable machine learning systems to real-world domains, including the emergence of "bridge" roles connecting ML developers and domain experts, the need for an easy-to-configure system to develop interfaces, and the importance of continuous evaluation using key performance indicators, applied to a case study of monitoring wind turbine brake pad failure predictions.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is presenting three key lessons learned from the authors' past experiences in deploying usable machine learning systems to real-world domains. Specifically:

1) Identifying the emergence of a new role called "bridges" - individuals within companies tasked with connecting domain experts with machine learning developers. The paper argues these bridges are well-positioned to help design and evaluate usable ML interfaces.

2) Emphasizing the need for an easy-to-configure system for developing usable ML interfaces to enable rapid iteration based on domain expert feedback. The authors present their "Sibyl" system aimed at meeting this need.

3) Arguing for the importance of continuous evaluation based on tracking key performance indicators, as opposed to one-off user studies, in order to measure the real-world impact of introducing usable ML systems.

The authors apply these lessons to an ongoing case study on monitoring wind turbine brake pad failure predictions. Overall, the main contribution is synthesizing these three insights from past experience deploying usable ML systems in order to provide guidance to practitioners.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Usable ML: Going beyond just explainable ML to include explanations and other augmenting information to aid in decision-making. 

- Bridges: People who bridge the gap between ML developers and domain experts. They help configure and evaluate usable ML interfaces.

- Sibyl: The system developed by the authors to enable easy configuration of usable ML interfaces. Includes a Python library, API, and configurable frontend.

- Wind turbine monitoring: The application domain focused on in the paper. Specifically looking at predicting brake pad failures.

- Key performance indicators (KPIs): Metrics tracked to evaluate the real-world impact of introducing the usable ML system. Examples include turbine downtime, number of brake pad failures, etc.

- Explanations: Different types of explanations generated to help users understand the ML model outputs, including local feature contributions, global feature importance, nearest neighbors, etc.

- Evaluation: Discusses challenge of evaluating real-world impact of usable ML, and proposes tracking existing KPIs through live deployment rather than just doing user studies.

So in summary, some core concepts are usable ML, the bridge role, the Sibyl system, evaluation via KPIs, and explanations for turbine monitoring.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces the concept of "bridges" - people who connect domain experts with ML developers. What are some examples of potential "bridge" roles in other domains beyond child welfare and wind turbine monitoring? What skills and background would make someone well-suited to be a "bridge"?

2. The paper proposes using an iterative development process with continual feedback from "bridges" to create usable ML interfaces. What are some potential challenges with relying too heavily on feedback from "bridges" instead of end-users? How can you ensure the interfaces will meet end-users' needs?  

3. The Sibyl system consists of three main components (Pyreal, Sibyl-API, and Sibylapp). What is the purpose of each component and how do they work together to enable rapid development of usable ML interfaces? What are the advantages of having this modular system?

4. The paper discusses five different usable ML interfaces developed for the wind turbine monitoring system (feature contributions, nearest neighbors, change over time, global explanations, explore a feature). Which of these interfaces do you think would be most useful for end-users and why? How could the interfaces be improved?

5. Continuous evaluation through tracking key performance indicators (KPIs) is proposed to measure the real-world impact of the usable ML system. What types of KPIs would be optimal to track the benefits of the system? What are some challenges with identifying the "right" KPIs?  

6. Beyond tracking KPIs, what other evaluation methods could be used to assess the effectiveness of the usable ML interfaces with end-users over time? What benefits or limitations do you see with different evaluation approaches?

7. The paper focuses on developing interfaces for the Monitoring and Analysis team who act as "bridges." What types of tailored interfaces could be developed for the end-users (the Operations and Maintenance team) to directly support their decision-making?

8. How could the lessons from this wind turbine monitoring system be applied to developing usable ML interfaces for other complex real-world problems such as medical decision making or financial analysis? What components are generalizable?

9. The paper argues usable ML goes beyond explainable ML to provide users with actionable information. What are some examples of additional information beyond explanations that could make model outputs more usable and actionable for end-users?

10. What long-term data should be collected about the wind turbine monitoring system over time to assess on-going performance, user satisfaction, and identify areas for improvement in the usable ML interfaces? What analyses could be conducted?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Deploying machine learning model outputs (MLOs) for real-world decision making is challenging. MLOs require more than just explanations to be usable - they need configurable interfaces tailored to the specific domain.  
- Lessons from past usable ML deployments can inform best practices, but have not yet been synthesized.

Case study:
- The paper focuses on a use case of monitoring wind turbine brake pad failures. Engineers must decide whether to do costly in-person brake pad investigations. Usable ML interfaces could aid this decision process.

Key lessons:
1) A new role is emerging of "bridges" who connect ML developers with domain experts. Bridges help iterate on interfaces. 
2) An easy-to-configure system like Sibyl enables quicker iterations on interfaces through collaboration with bridges.
3) Continuously tracking domain-specific KPIs is essential to evaluate real-world impact.

Proposed solution:
- Work with "bridges" (a monitoring and analysis team) to develop usable ML interfaces with the Sibyl system for turbine monitoring. 
- Plan a continuous evaluation by tracking existing KPIs on turbine efficiency after deployment.

Main contributions:
- Identified the emerging role of "bridges" in usable ML deployment and how they enable effective collaboration.
- Emphasized the need for an easily configurable system like Sibyl to allow quicker iteration on interfaces.
- Stressed the importance of continuous evaluation tracking existing KPIs, not just formal user studies.
- Provided lessons from past experience that can inform future usable ML deployments.
