# [Auto-Meta: Automated Gradient Based Meta Learner Search](https://arxiv.org/abs/1806.06927)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new approach for automatically designing neural network architectures for gradient-based meta-learners. The main research questions/goals are:- Can automated neural architecture search methods like PNAS be applied successfully to find good architectures for gradient-based meta-learners? - Will the automatically found architectures for meta-learners outperform manually/intuitively designed architectures used in prior work?The key hypothesis is that automated architecture search through methods like PNAS will be beneficial for discovering architectures that lead to improved performance of gradient-based meta-learning algorithms like MAML/Reptile on few-shot image classification tasks. The paper tests this hypothesis by applying PNAS architecture search to find good convolutional cells and constructing full CNNs from them to use with the Reptile meta-learning algorithm. The results on few-shot classification benchmarks like Omniglot and Mini-ImageNet show that the automatically found architectures achieve state-of-the-art results, outperforming human-designed architectures used with meta-learners in prior work. This supports the hypothesis that automated neural architecture search can successfully find architectures better optimized for meta-learning than manual designs.In summary, this paper explores a novel combination of neural architecture search and gradient-based meta-learning, validating that this synergy of AutoML techniques can discover architectures that substantially improve meta-learner performance on few-shot image classification.


## What is the main contribution of this paper?

This appears to be a LaTeX template for a conference paper submission, rather than a full published paper. Some key things I noticed:- It seems to be a template for submitting to NeurIPS (Neural Information Processing Systems). The neurips_2018 LaTeX package is included.- The abstract describes using progressive neural architecture search to automatically find optimal architectures for gradient-based meta-learners. The goal is to automate the design of neural network architectures for meta-learning algorithms like MAML. - The results show they were able to achieve state-of-the-art results on a 5-shot 5-way image classification benchmark using their automatically designed architecture, improving substantially over MAML's performance.- The introduction and related works provide background on meta-learning, neural architecture search, and relevant prior work in both areas.So in summary, the main contribution appears to be using progressive neural architecture search to automate the design of architectures for gradient-based meta-learning algorithms, and showing this can improve performance over human-designed architectures on few-shot image classification tasks. The paper aims to demonstrate the first successful combination of neural architecture search with meta-learning.


## How does this paper compare to other research in the same field?

This paper presents an automated neural architecture search approach for gradient-based meta-learning. Here is a comparison to related work in meta-learning and neural architecture search:Meta-learning:- The paper builds on model-agnostic meta-learning (MAML) and its first-order approximation Reptile. These methods learn an initialization that can be quickly adapted with gradient steps for new tasks. - Other meta-learning methods use recurrent models or learned metrics. This work focuses on gradient-based methods like MAML/Reptile which are model-agnostic.- The key novelty is using architecture search to optimize the model for meta-learning with MAML/Reptile. Prior meta-learning papers use hand-designed or standard architectures.Neural Architecture Search:- Most NAS focuses on optimizing architectures for standard supervised learning. This paper optimizes architectures for meta-learning problems.- It uses a progressive search like PNAS, expanding the search space incrementally. Other papers use RL, evolutionary, or one-shot methods.- The architectures found are much deeper than normal for NAS. This aligns with theory on depth for gradient-based meta-learners.- No other NAS papers have looked at optimizing for meta-learning performance. The combination of NAS and meta-learning is novel.In summary, this paper uniquely combines neural architecture search with gradient-based meta-learning. It is the first demonstration of using NAS to optimize architectures for meta-learning performance. The results significantly improve over standard architectures for meta-learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring the effect of different operators in the progressive neural architecture search method, such as trying different types of blocks or connectivity patterns. - Further empirical studies on the characteristics of good cells for few-shot learning tasks, especially in terms of depth and width trade-offs. The authors suggest analyzing the cell depth distributions in more detail.- Trying more types of neural architecture search methods in the context of meta-learning, such as differentiable architecture search where the model parameters and architecture can be jointly optimized.- Testing the applicability of the automated architecture search approach to other domains beyond few-shot image classification, such as few-shot natural language processing tasks.- Analysis of the theoretical properties of the architectures found through this automated search process. The authors suggest linking findings to the theory around the universality and optimization landscape of meta-learners.- Developing more efficient search methods tailored to finding good meta-learner architectures, building on this proof-of-concept.In summary, the main directions are around developing a deeper theoretical and empirical understanding of the relationship between architectures and meta-learning performance, testing the generalizability of the approach, and improving the efficiency of the search process. The authors have demonstrated promising initial results but highlight many opportunities for further research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes an approach to automatically design neural network architectures for gradient-based meta-learners. The goal is to find optimal architectures that enable quick adaptation to new tasks with a small amount of training data. The method combines progressive neural architecture search to incrementally explore more complex network architectures with meta-learning using Reptile, a first-order approximation of MAML, to evaluate candidate architectures. Experiments on few-shot image classification tasks show the method can discover architectures that significantly outperform hand-designed networks trained with Reptile. The best architecture found achieves state-of-the-art results on 5-shot 5-way Mini-ImageNet classification, improving 11.54% over the MAML result. This demonstrates the benefit of automating architecture search for meta-learning and provides the first successful application of neural architecture search in the meta-learning setting.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new method for automatically designing neural network architectures for gradient-based meta-learners. The goal is to find the optimal network architecture that minimizes the loss function for given tasks represented by training and test data. The approach takes a simple and natural combination of two techniques: neural architecture search using progressive neural architecture search (PNAS), and gradient-based meta-learning using the Reptile algorithm. PNAS incrementally expands the search space from simple to complex architectures, evaluating performance using a surrogate predictor without expensive training. Reptile is a first-order approximation of model-agnostic meta-learning (MAML) that finds good initialization parameters for fast adaptation on new tasks. The method was applied to few-shot image classification using the Omniglot and Mini-ImageNet datasets. The automatically found architecture outperformed human-designed networks trained with Reptile on Omniglot. On Mini-ImageNet, it achieved state-of-the-art results, improving accuracy by 11.54% over MAML on the 5-shot 5-way task. This demonstrates the benefit of combining automated architecture search with meta-learning for finding high-performing models adapted for fast learning on new tasks with limited data. Further work could explore different search methods and study what cell characteristics lead to good performance.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new approach to automatically design neural network architectures for gradient-based meta-learners. Specifically, the authors use the progressive neural architecture search (PNAS) algorithm to find an optimal convolutional cell for few-shot image classification tasks. They use Reptile, a first-order approximation of MAML, as the gradient-based meta-learning algorithm. In PNAS, neural network architectures are represented by an abstraction with three layers - block, cell, and network. Blocks representing combinations of operators are included in cells, which are stacked to form the full CNN. During search, cells progressively become more complex by adding blocks. A surrogate predictor (e.g. LSTM) evaluates new candidate cells without full training. The search continues until the maximum number of blocks per cell is reached. By combining architecture search with gradient-based meta-learning, the authors are able to find cells tailored for few-shot learning that improve over human-designed and generic architectures. Their method achieves state-of-the-art results on few-shot image classification benchmarks.


## What problem or question is the paper addressing?

The paper is addressing the problem of how to automatically find optimal neural network architectures for gradient-based meta-learners. Meta-learning algorithms like MAML and Reptile aim to learn a model initialization that can quickly adapt to new tasks with a few gradient steps. However, they use simple hand-designed network architectures. The paper proposes to use neural architecture search to automatically find better architectures tailored for gradient-based meta-learning. Specifically, the paper combines progressive neural architecture search with Reptile meta-learning for few-shot image classification. The search algorithm progressively expands the space of candidate architectures to find cells that work well when stacked into full convolutional networks and trained with Reptile. This allows efficiently searching for good architectures without training all candidates from scratch.The key ideas are:- Formulate architecture search for meta-learners as optimizing the loss after adapting to new tasks with gradient steps - Use progressive NAS to expand the search space and an LSTM predictor to select promising candidates- Show this can find architectures that improve on hand-designed networks for Reptile on few-shot classificationSo in summary, the paper addresses how to automate neural architecture search to improve performance of gradient-based meta-learning algorithms like MAML and Reptile. The main innovation is combining progressive NAS with meta-learning loss objectives.
