# [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can recent advances in image generation using autoregressive transformers and diffusion models be applied to improve the quality and expressiveness of text-to-speech systems?The key hypotheses appear to be:1) Autoregressive transformers can be used to generate high-quality discrete speech representations (tokens) from text inputs. 2) Diffusion models can then convert those discrete speech tokens into realistic and expressive speech waveforms.3) Conditioning the models on speaker embeddings can allow for multi-speaker text-to-speech with cloned voices.4) Scaling up datasets, compute, and model capacity leads to better results compared to prior TTS systems.So in summary, the central research direction is leveraging advances in image generation to build a new text-to-speech pipeline that combines autoregressive and diffusion models, conditioned on speaker identity, and scaled up in data and compute. The hypothesis is that this approach can surpass the state-of-the-art in TTS realism and expressiveness.
