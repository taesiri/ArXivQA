# [Shapley-PC: Constraint-based Causal Structure Learning with Shapley   Values](https://arxiv.org/abs/2312.11582)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Causal structure learning (CSL) aims to extract causal relationships from observational data and represent them as graphs. This is an important step towards robust and transparent models that can estimate causal effects without requiring interventions. Constraint-based CSL methods use conditional independence tests to perform causal discovery but can make errors on finite samples. 

Proposed Solution: 
The paper proposes a novel constraint-based CSL method called Shapley-PC that improves the discovery of causal orientations (v-structures) by analyzing conditional independence tests using Shapley values. Shapley values quantify the marginal contribution of a variable to the dependence between two other variables. This provides a principled way to determine which variable is more likely to be the collider in a v-structure based on its effect on the test statistics.

The proposed Shapley Independence Value (SIV) calculates a variable's marginal contribution to the p-value of the conditional independence test, averaged over different conditioning sets. Variables with the most negative SIV are most likely colliders. This decision rule is implemented within the PC-stable algorithm to orient v-structures.

Main Contributions:

- Proposal of a new decision rule for v-structure orientation in constraint-based CSL using Shapley values (SIV)
- Proof that the soundness and asymptotic consistency guarantees of PC-algorithm are preserved
- Extensive evaluation showing improved performance over state-of-the-art constraint-based, score-based and functional causal model methods
- Analysis of performance wrt graph density and data-generating process, with consistent improvement across diverse scenarios

In summary, the paper presents a principled way to leverage more information from conditional independence tests to improve v-structure discovery. This enhances the overall causal graph extraction and demonstrates the value of applying concepts from cooperative game theory within causal discovery.


## Summarize the paper in one sentence.

 The paper proposes Shapley-PC, a novel constraint-based causal structure learning algorithm that uses Shapley values to improve the orientation of v-structures, and demonstrates its improved performance over state-of-the-art methods on both synthetic and real-world datasets.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a novel decision rule that can be applied to constraint-based causal structure learning algorithms to guide the search for v-structures. This decision rule is based on using Shapley values to quantify the contribution of each variable to the conditional (in)dependences observed when evaluating an unshielded triple.

2. It implements this decision rule within the PC-stable algorithm, obtaining a new algorithm called Shapley-PC. The paper proves that this modification preserves the soundness and asymptotic consistency of the original PC algorithm.

3. It provides an extensive evaluation of Shapley-PC compared to several constraint-based, score-based, and functional causal model-based baselines on both synthetic and real-world datasets. The results show that Shapley-PC can outperform state-of-the-art methods across a range of scenarios and graph structures. 

In summary, the main contribution is a novel constraint-based causal discovery algorithm that leverages Shapley values to improve the orientation of v-structures. Both theoretical properties and extensive empirical results are provided to demonstrate the usefulness of this approach.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords associated with it:

- Causal Structure Learning (CSL)
- Constraint-based CSL 
- Conditional independence tests
- d-separation
- Peter Clark (PC) algorithm
- Faithfulness assumption
- Markov Equivalence Class (MEC)
- Completed partially directed acyclic graph (CPDAG)
- Conditional Independence Test (CIT)
- Shapley values
- Soundness
- Asymptotic consistency 
- Structural Hamming Distance (SHD)
- Structural Intervention Distance (SID)
- Precision and recall metrics
- Graph saturation
- Directed acyclic graph (DAG)
- Structural Equation Model (SEM)
- Gaussian Processes (GPs)
- Multi-layer Perceptron (MLP)

The paper proposes a novel constraint-based CSL algorithm called Shapley-PC that uses Shapley values to improve the discovery of v-structures and overall graph accuracy compared to prior PC-based algorithms. It provides proofs of soundness and asymptotic consistency as well as extensive experiments on synthetic and real-world datasets demonstrating improved performance over several state-of-the-art baselines.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How exactly does the Shapley value quantify the contribution of a variable to the independence between two other variables? Explain the intuition behind using game theory concepts for causal discovery. 

2. Theorem 1 states that with perfect conditional independence information, the sign of the Shapley Independence Value identifies colliders. Walk through the proof and explain why this result holds. 

3. The paper claims the proposed method is sound and asymptotically consistent. Explain what soundness and consistency mean in the context of causal discovery algorithms. Why is asymptotic consistency important?

4. Explain the high level intuition behind why Shapley-PC performs well on sparse graphs but gets worse on very dense graphs, especially for non-linear models. What could be done to improve performance on dense graphs?

5. The conditioning sets used to compute Shapley Values grow with the degree of the graph. How does this impact computational complexity compared to other PC-based algorithms? Could approximations be used?

6. The decision rule for declaring colliders is currently conservative, choosing only the variable with the lowest Shapley value. Propose some alternative heuristic decision rules and explain the tradeoffs.  

7. The paper standardizes the synthetic data to avoid issues from scale differences between variables. Why is this an important preprocessing step for causal discovery? How could it impact algorithm performance?

8. What is the key difference between constraint-based and score-based causal discovery methods? Explain relative pros and cons and how the proposed approach relates.

9. The method assumes causal sufficiency. What would be required to extend it to latent variable models? Would the theoretical guarantees still hold?

10. The empirical evaluation relies heavily on synthetic data. Discuss the limitations of evaluations using synthetic data and how additional experiments could strengthen the validation of the method.
