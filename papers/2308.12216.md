# [SG-Former: Self-guided Transformer with Evolving Token Reallocation](https://arxiv.org/abs/2308.12216)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to design an efficient vision transformer backbone that can handle large high-resolution feature maps while still capturing both global and local information. The key ideas proposed in the paper to address this question are:1) Using a "significance map" estimated by the model itself to guide adaptive token allocation, assigning more tokens to salient regions for fine-grained attention and fewer tokens to less important regions for efficiency. 2) Introducing a hybrid-scale self-attention mechanism within each transformer block to jointly model local and global dependencies without increasing computational cost.3) Combining the significance map-guided "self-evolving attention" with the hybrid-scale attention modules into an overall transformer backbone called SG-Former.So in summary, the main hypothesis is that by using a self-guided, evolving token allocation strategy driven by an internal significance estimation, along with hybrid multi-scale attention, they can design an efficient vision transformer that captures both local details and global context for high-resolution visual understanding tasks. The experiments on image classification, object detection, and segmentation aim to validate this central hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel vision transformer model called SG-Former that aims to perform efficient global self-attention while preserving fine-grained details in salient image regions. The key ideas include:1) Using a "significance map" estimated from the model itself to guide token reallocation, assigning more tokens to salient regions and fewer to minor regions. This allows concentrating computational resources on important areas. 2) Introducing a "hybrid-scale self-attention" module that extracts both fine-grained local and coarse global information within a single layer, at a similar cost to previous approaches like Swin Transformer. This provides guidance for determining region significance.3) Evaluating the method on image classification, object detection, and semantic segmentation, showing superior performance to prior works like Swin Transformer and CSWin Transformer under similar model sizes.In summary, the main contribution is a more efficient transformer backbone that adapts its computation across spatial locations based on a self-estimated significance map, outperforming prior approaches. The hybrid-scale self-attention and self-guided reallocation mechanisms are key to achieving this.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new vision transformer architecture called Self-guided Transformer (SG-Former) that adaptively aggregates tokens based on region significance using hybrid-scale self-attention guidance to achieve efficient global self-attention with fine granularity.


## How does this paper compare to other research in the same field?

Here are a few key points of comparison between this paper and other related work in vision transformers:- The main contribution is proposing a self-guided transformer (SG-Former) that can adaptively aggregate tokens based on region significance. This allows it to preserve fine-grained details in salient regions while reducing computation cost in less important regions. - It introduces a hybrid-scale self-attention mechanism to extract multi-scale information and provide guidance for the token aggregation. This is a novel way to incorporate both local and global context.- SG-Former achieves state-of-the-art results on ImageNet classification, COCO detection, and ADE20K segmentation, outperforming previous vision transformers like Swin and CSWin. This demonstrates its effectiveness as a general vision backbone.- Compared to locality-constrained transformers like Swin and CSWin, SG-Former preserves global attention while reducing cost. Compared to cost-reduced global transformers like PVT, it can adaptively preserve details.- The self-guided aggregation is more flexible than pre-defined strategies like windows or static downsampling. The guidance evolves during training rather than being fixed.- The overall architecture follows a hierarchical pyramid design similar to Swin, CSWin etc. But it inserts the new self-guided blocks in early stages for token aggregation.In summary, this paper presents a novel way to get the benefits of both global context and local details for vision transformers. The self-guided aggregation guided by adaptive significance maps is the key novelty compared to prior work.
