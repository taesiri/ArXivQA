# [SG-Former: Self-guided Transformer with Evolving Token Reallocation](https://arxiv.org/abs/2308.12216)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to design an efficient vision transformer backbone that can handle large high-resolution feature maps while still capturing both global and local information. The key ideas proposed in the paper to address this question are:1) Using a "significance map" estimated by the model itself to guide adaptive token allocation, assigning more tokens to salient regions for fine-grained attention and fewer tokens to less important regions for efficiency. 2) Introducing a hybrid-scale self-attention mechanism within each transformer block to jointly model local and global dependencies without increasing computational cost.3) Combining the significance map-guided "self-evolving attention" with the hybrid-scale attention modules into an overall transformer backbone called SG-Former.So in summary, the main hypothesis is that by using a self-guided, evolving token allocation strategy driven by an internal significance estimation, along with hybrid multi-scale attention, they can design an efficient vision transformer that captures both local details and global context for high-resolution visual understanding tasks. The experiments on image classification, object detection, and segmentation aim to validate this central hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel vision transformer model called SG-Former that aims to perform efficient global self-attention while preserving fine-grained details in salient image regions. The key ideas include:1) Using a "significance map" estimated from the model itself to guide token reallocation, assigning more tokens to salient regions and fewer to minor regions. This allows concentrating computational resources on important areas. 2) Introducing a "hybrid-scale self-attention" module that extracts both fine-grained local and coarse global information within a single layer, at a similar cost to previous approaches like Swin Transformer. This provides guidance for determining region significance.3) Evaluating the method on image classification, object detection, and semantic segmentation, showing superior performance to prior works like Swin Transformer and CSWin Transformer under similar model sizes.In summary, the main contribution is a more efficient transformer backbone that adapts its computation across spatial locations based on a self-estimated significance map, outperforming prior approaches. The hybrid-scale self-attention and self-guided reallocation mechanisms are key to achieving this.
