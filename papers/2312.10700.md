# [Cross-Domain Robustness of Transformer-based Keyphrase Generation](https://arxiv.org/abs/2312.10700)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Keyphrase generation aims to predict a set of keyphrases that summarize the content of a source text. This is useful for tasks like searching documents in databases. 
- Most prior work focuses on a few common domains like computer science. Performance often drops significantly when models are applied to new domains without in-domain training data (zero-shot setting).

Methods:
- The paper experiments with BART, a transformer model for text generation. BART is fine-tuned to generate keyphrase sequences from text.
- BART is evaluated on 6 keyphrase datasets spanning computer science, biomedicine and news articles. Both in-domain and cross-domain performance is measured.
- Strategies are explored to improve cross-domain robustness: fine-tuning on mixed domains, transfer learning between domains, low-resource training.

Results:
- In-domain BART matches or exceeds baseline methods on most datasets. But zero-shot cross-domain performance drops 30-80%.  
- Mixing training data from related domains improves robustness. Preliminary out-of-domain fine-tuning further boosts performance with limited target data.
- With only 50 target examples, two-stage fine-tuning outperforms BART trained on full target dataset. Competitive performance is achieved with 40-60% less annotated data.

Conclusions:
- BART is sensitive to domain shift, but preliminary out-of-domain training significantly improves robustness. Two-stage fine-tuning enables low-resource transfer to new domains.
- Future work will focus on transferring from high-resource languages (e.g. English) to lower-resource languages.

The summary covers the key points on the problem being addressed, the BART model and training strategies used, the major results on in-domain and cross-domain performance, and the primary conclusions reached. It highlights the core contributions around improving cross-domain robustness and low-resource domain adaptation using two-stage fine-tuning.
