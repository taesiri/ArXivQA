# [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) tend to hallucinate and generate factually incorrect responses when queried on concepts not well-represented in their training data (termed "unfamiliar inputs"). This is problematic, especially for long-form generation tasks like biography or plot generation. 

Main Conceptual Insight:  
- The paper shows that rather than producing arbitrary outputs, finetuned LLM predictions tend to default to a "hedged prediction" that minimizes loss over unfamiliar examples from finetuning. This hedged prediction is determined by the supervision targeted to unfamiliar finetuning examples.

- This insight suggests controlling LLM hallucinations by modifying supervision for unfamiliar finetuning examples to steer the model's default hedged prediction towards more desirable responses like admitting uncertainty.

Proposed Solution and Key Technical Contributions:
- The paper leverages the conceptual insight to tackle reward model hallucinations in RL-based factuality finetuning. Specifically, they propose learning "conservative" reward models trained to underestimate rewards on unfamiliar inputs to mitigate negative impacts of reward hallucinations.

- Experiments on biography and plot generation tasks demonstrate that RL-finetuning with conservative reward models outperforms standard supervised finetuning and RL with normal reward models in minimizing hallucinated false facts while retaining true facts, especially as inputs become more unfamiliar.

So in summary, key contributions are (1) conceptual understanding of how supervision targeted to unfamiliar examples governs LLM hallucinations, and (2) approach to control reward model hallucinations to enable more reliable RL-based factuality finetuning on long-form generation.


## Summarize the paper in one sentence.

 The paper proposes a conceptual model explaining how finetuned language models tend to default to a "hedged prediction" on unfamiliar inputs, determined by the supervision of unfamiliar examples during finetuning, and leverages this insight to control model hallucinations and improve factuality of long-form generations via reinforcement learning with conservative reward models.


## What is the main contribution of this paper?

 The paper makes two main contributions:

1. It presents a conceptual model for understanding how language models make predictions when faced with unfamiliar inputs. The key insight is that model predictions tend to revert to a "hedged prediction" that minimizes the aggregate loss over unfamiliar examples seen during finetuning. 

2. It proposes an approach to control the form of this hedged prediction in order to improve factuality for long-form generation tasks. Specifically, it trains "conservative" reward models that avoid overestimating rewards for unfamiliar inputs. Reinforcement learning with these conservative reward models is shown to generate more factual text compared to supervised finetuning or RL with standard reward models.

So in summary, the main contributions are (1) a better understanding of how models hallucinate, and (2) a method to leverage this understanding to improve factuality using conservative reward models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Language models - The paper focuses on understanding and controlling the hallucination behavior of large language models (LLMs).

- Hallucinations - A main focus of the paper is investigating why LLMs tend to hallucinate incorrect responses, especially for unfamiliar inputs. 

- Unfamiliar inputs - Inputs that contain concepts rarely mentioned in the model's training data. The paper studies model predictions as inputs become more unfamiliar.

- Finetuning - The paper analyzes model behavior after finetuning with supervised learning and reinforcement learning. 

- Hedged predictions - The paper hypothesizes finetuned models learn a default "hedged prediction" for unfamiliar inputs that minimizes aggregate finetuning loss.  

- Controlling hallucinations - A key goal is developing methods to control how models hallucinate, such as by changing supervision for unfamiliar finetuning examples.

- Factuality - Improving factuality and reducing false information in LLM generations is an end goal.

- Reward models - The paper proposes using conservative reward models that avoid overestimating rewards to mitigate issues with reward model hallucinations.

- Biography generation, plot generation - Long-form text generation tasks used to evaluate proposed reinforcement learning finetuning approach.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes learning "conservative" reward models for RL-based factuality finetuning. What is the intuition behind why conservative reward models are better for this task compared to standard reward models? How do they operationalize the notion of a conservative reward model?

2. The key insight in learning conservative reward models is to use the same base model for generating the finetuning data as the one used for finetuning the reward model. Why does this data collection process result in more conservative reward predictions? 

3. The paper argues that standard supervised finetuning approaches for mitigating hallucinations do not scale well to long-form generation tasks. What are some of the challenges they highlight? How does their proposed RL-based approach help address these challenges?

4. When evaluating the efficacy of using conservative reward models, the paper compares against RL factuality finetuning with a standard reward model. What data does the standard reward model use during training and why does this lead to poorer performance?

5. The conceptual model proposes that LLMs learn a default "hedged prediction" on unfamiliar examples during finetuning. How is this hedged prediction determined and why does it explain model hallucinations? Provide an example to illustrate.

6. The paper discusses two existing approaches for mitigating hallucinations and explains them through the lens of their conceptual model. Summarize these approaches and how steering the hedged prediction leads to their efficacy.  

7. When evaluating conservative reward models, the paper plots reward predictions on samples from a supervised finetuned (SFT) model. Why choose samples from the SFT model rather than other possible data sources?

8. For the biography generation experiments, how is the notion of unfamiliarity operationalized when analyzing model performance across different levels of unfamiliarity?

9. The paper assumes access to ground truth rewards during conservative reward model training. How might the approach change if only rough proxy rewards were available (e.g. based on likely factuality)?  

10. The approach trains conservative reward models using samples from the same base model that is later finetuned. How might the method perform if applied to an unseen model (e.g. a larger model released in the future)?
