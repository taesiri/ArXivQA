# [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) tend to hallucinate and generate factually incorrect responses when queried on concepts not well-represented in their training data (termed "unfamiliar inputs"). This is problematic, especially for long-form generation tasks like biography or plot generation. 

Main Conceptual Insight:  
- The paper shows that rather than producing arbitrary outputs, finetuned LLM predictions tend to default to a "hedged prediction" that minimizes loss over unfamiliar examples from finetuning. This hedged prediction is determined by the supervision targeted to unfamiliar finetuning examples.

- This insight suggests controlling LLM hallucinations by modifying supervision for unfamiliar finetuning examples to steer the model's default hedged prediction towards more desirable responses like admitting uncertainty.

Proposed Solution and Key Technical Contributions:
- The paper leverages the conceptual insight to tackle reward model hallucinations in RL-based factuality finetuning. Specifically, they propose learning "conservative" reward models trained to underestimate rewards on unfamiliar inputs to mitigate negative impacts of reward hallucinations.

- Experiments on biography and plot generation tasks demonstrate that RL-finetuning with conservative reward models outperforms standard supervised finetuning and RL with normal reward models in minimizing hallucinated false facts while retaining true facts, especially as inputs become more unfamiliar.

So in summary, key contributions are (1) conceptual understanding of how supervision targeted to unfamiliar examples governs LLM hallucinations, and (2) approach to control reward model hallucinations to enable more reliable RL-based factuality finetuning on long-form generation.
