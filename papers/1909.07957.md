# An Internal Learning Approach to Video Inpainting

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new video inpainting method based on internal learning. The central hypothesis is that it is possible to train a single frame-wise generative model to produce coherent video inpainting results by exploiting the internal statistics within a video, without relying on any external training data. Specifically, the key research questions explored in this paper are:- Can a frame-wise generative model produce coherent video inpainting results when trained only on a single input video? - What are effective training strategies and loss functions to enable a generative model to capture spatial-temporal constraints for the video inpainting task through internal learning?- How does jointly generating appearance and flow help enforce consistency during internal learning for video inpainting?- How does this internal learning approach for video inpainting compare with existing methods that utilize external training data? What are its limitations and strengths?In summary, this paper focuses on exploring the potential and limitations of internal learning for the challenging problem of video inpainting. It aims to demonstrate that, with proper training strategies, a single frame-wise generative model can produce surprisingly good results by exploiting internal statistics within a video, without relying on any external training data.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel video inpainting algorithm based on internal learning. The method extends the Deep Image Prior approach to video by training a generative convolutional neural network on an input video to fill in missing regions, without relying on any external training data.2. Developing consistency-aware training strategies to address the key challenge of temporal consistency in video inpainting. The main idea is to jointly generate appearance frames and optical flow, and exploit the consistency between them during training. This helps propagate information across frames to achieve better coherence.3. Demonstrating that a regular 2D CNN image generation model can produce high quality and temporally coherent video inpainting results when trained properly with the right objectives on the input video. The method performs comparably or better than state-of-the-art optimization or learning based video inpainting techniques.4. Providing insights into internal learning for video generation tasks. The work explores appropriate network architectures, training methodologies, and shows the potential of video-specific internal learning over image-based models.In summary, the key contribution is an in-depth study and novel framework exploring the idea of internal learning without external data for the challenging task of video inpainting. The method achieves strong results while only training on the individual input video.
