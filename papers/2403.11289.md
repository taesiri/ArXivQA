# [ManipVQA: Injecting Robotic Affordance and Physically Grounded   Information into Multi-Modal Large Language Models](https://arxiv.org/abs/2403.11289)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing multimodal large language models (MLLMs) lack critical robotic knowledge such as affordances and physical principles, hampering their effectiveness for robotic manipulation tasks. This limits the variety and precision of tasks that robots can execute. Thus, there is a need to bridge the gap between MLLM capabilities and demands of robotic systems.

Method:
The paper proposes ManipVQA, a novel framework to equip MLLMs with manipulation-centric knowledge using a visual question-answering (VQA) approach. Key aspects:

1) Collected diverse images of interactive objects covering challenges in object detection, affordance recognition and physical concepts. 

2) Adopted a unified VQA format and devised a fine-tuning strategy that retains original vision-reasoning strengths of MLLMs while incorporating robotic insights for enhanced tool detection, affordance recognition and physical understanding.

3) Augmented existing affordance datasets with complex tasks using GPT-4 to ensure model interprets implicit tasks. 

4) Evaluated on robotic affordance dataset HANDAL, outperforming baselines in unified detection of full objects and affordance regions.

5) Demonstrated strong physical concept grounding, surpassing specialized models by large margins on PhysObjects dataset.

6) Exhibited generalization ability with competitive affordance grounding on unseen AGD20K dataset.

7) Achieved promising simulated manipulation without task-specific fine-tuning by combining with basic policies.


Main Contributions:

1) Novel approach to bridge gap between MLLMs and robotic systems via VQA paradigm.

2) Comprehensive dataset merging robotic, visual and physical annotations.  

3) Strong empirical performance on affordance, physical and manipulation tasks.

4) Resources shared to facilitate future research.

In summary, ManipVQA injects essential robotic knowledge into MLLMs, enhancing their applicability for manipulation tasks while retaining visual reasoning capacities. Evaluations validate the robustness across diverse affordance and physical understanding benchmarks.
