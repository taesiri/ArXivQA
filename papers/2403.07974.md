# [LiveCodeBench: Holistic and Contamination Free Evaluation of Large   Language Models for Code](https://arxiv.org/abs/2403.07974)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Evaluating and comparing large language models (LLMs) for code has become an important challenge, but existing benchmarks like HumanEval, MBPP, and APPS suffer from issues like contamination, lack of diversity, and overfitting. Specifically, these benchmarks focus solely on natural language to code generation and may already be present in LLM training datasets, leading to inflated scores. Additionally, code generation is just one facet of programming ability.

Proposed Solution:
This paper introduces LiveCodeBench, a comprehensive, contamination-free benchmark for evaluating LLMs on coding tasks. Key aspects are:

1) Uses live updates of problems from platforms like LeetCode, AtCoder to prevent contamination. Assigns contest release dates and evaluates models on unseen problems. 

2) Evaluates on broader tasks beyond code generation including - self-repair from errors, code execution, test case output prediction to capture different programming capabilities.

3) Sources high-quality problems from reputable competition platforms validated by users. Provides many test cases (>59 on average) for robust evaluation.

4) Ensures balanced distribution over easy, medium, hard problems unlike existing benchmarks focusing solely on easier tasks. Enables more granular comparisons.


Currently, LiveCodeBench contains 400 coding problems published between May 2023 and Feb 2024. Models spanning different scales and 9 base + 20 instruction-tuned models are evaluated.


Key Findings:

1) Detected contamination in DeepSeeks models based on performance drop over time.

2) Relative model order is consistent but performance differences vary across scenarios highlighting need for holistic evaluation.

3) Many models might overfit on HumanEval while generalizing poorly to LiveCodeBench.

4) DeepSeeks models outperform CodeLlamas and StarCoders across base model comparisons. Instruction tuning is crucial for better performance.  

5) Closed models still outperform open models in most cases. Only some large 30B+ instruction-tuned models can match them in performance.


In summary, LiveCodeBench is uniquely designed to enable thorough and fair evaluations of coding LLMs using continuous problem updates and expanded programming scenarios. The benchmark reveals novel findings on benchmark contamination, generalization, model capacities missed by existing setups.
