# [Is Independent Learning All You Need in the StarCraft Multi-Agent   Challenge?](https://arxiv.org/abs/2011.09533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can independent learning methods, where agents learn decentralized policies without sharing information, perform competitively on cooperative multi-agent tasks compared to joint/centralized learning methods?Specifically, the authors evaluate Independent PPO (IPPO), a multi-agent variant of PPO based on independent learning, on the StarCraft Multi-Agent Challenge (SMAC) benchmark. Their central hypothesis is that despite theoretical limitations of independent learning, IPPO can match or exceed the performance of state-of-the-art joint/centralized MARL algorithms like QMIX on SMAC. The key findings that address this question are:- IPPO matches or exceeds the performance of algorithms like QMIX and MAVEN that use centralized training on several hard SMAC maps. This is surprising given recent focus on centralized training.- Ablation studies show PPO's policy clipping is crucial to IPPO's strong performance, likely by reducing environment non-stationarity.- IPPO outperforms variants using centralized value functions on some maps, suggesting centralized state may not be as useful in SMAC as believed.- IPPO is prone to relative overgeneralization in theory but this does not seem to limit performance on SMAC maps, implying it may not matter much in practice.In summary, the central hypothesis is that independent learning can work well on cooperative MARL tasks like SMAC despite its limitations, which the IPPO results generally validate. The findings suggest revisiting assumptions about both independent learning and the utility of centralized training/state in this setting.
