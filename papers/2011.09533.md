# [Is Independent Learning All You Need in the StarCraft Multi-Agent   Challenge?](https://arxiv.org/abs/2011.09533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can independent learning methods, where agents learn decentralized policies without sharing information, perform competitively on cooperative multi-agent tasks compared to joint/centralized learning methods?Specifically, the authors evaluate Independent PPO (IPPO), a multi-agent variant of PPO based on independent learning, on the StarCraft Multi-Agent Challenge (SMAC) benchmark. Their central hypothesis is that despite theoretical limitations of independent learning, IPPO can match or exceed the performance of state-of-the-art joint/centralized MARL algorithms like QMIX on SMAC. The key findings that address this question are:- IPPO matches or exceeds the performance of algorithms like QMIX and MAVEN that use centralized training on several hard SMAC maps. This is surprising given recent focus on centralized training.- Ablation studies show PPO's policy clipping is crucial to IPPO's strong performance, likely by reducing environment non-stationarity.- IPPO outperforms variants using centralized value functions on some maps, suggesting centralized state may not be as useful in SMAC as believed.- IPPO is prone to relative overgeneralization in theory but this does not seem to limit performance on SMAC maps, implying it may not matter much in practice.In summary, the central hypothesis is that independent learning can work well on cooperative MARL tasks like SMAC despite its limitations, which the IPPO results generally validate. The findings suggest revisiting assumptions about both independent learning and the utility of centralized training/state in this setting.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It empirically demonstrates that Independent PPO (IPPO), a simple multi-agent variant of PPO that uses independent learning without joint/centralized critics or extra state information, performs competitively or even better than more complex state-of-the-art MARL algorithms like QMIX on a range of StarCraft II micromanagement benchmark tasks. This is surprising given previous assumptions about the limitations of independent learning.- Through ablation studies, it shows that PPO's policy clipping objective plays a crucial role in the strong performance of IPPO, and that this effect cannot be simply explained by reducing the effective learning rate. The authors hypothesize that policy clipping helps stabilize learning in the non-stationary multi-agent setting.- The results suggest that some previously assumed limitations of independent learning, like the inability to learn certain coordination tasks, may not occur in practice in complex environments like StarCraft II. The authors suggest revisiting assumptions about issues like relative overgeneralization in MARL.- The performance of IPPO raises questions about the utility of centralized state information in benchmark tasks like SMAC, since IPPO performs competitively without using it while other algorithms leverage it.In summary, the key contribution is demonstrating that a simple independent learning method can outperform more complex joint/centralized MARL algorithms on benchmark tasks, challenging common assumptions and highlighting open questions about things like the value of centralized learning.
