# [Beyond Probabilities: Unveiling the Misalignment in Evaluating Large   Language Models](https://arxiv.org/abs/2402.13887)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current evaluation methods for large language models (LLMs) rely on output probabilities to make predictions, diverging from real-world usage where LLMs generate free-form responses. 
- Paper questions if probability-based assessments accurately evaluate LLM capabilities.

Investigation & Findings:  
- Compared label-based, sequence-based and generation-based predictions across multiple LLM model sizes on MMLU, TruthfulQA and Belebele benchmarks.
- Empirical analysis shows significant disconnect between probability-based and generative predictions. Even when predictions were correct, consistency remained low.
- Many benchmark rankings did not correlate well with human preference judgments for free-text generation output.
- Raises doubts on reliability of popular benchmarks relying on probability-based evaluations.

Proposed Solution & Contributions:
- Highlights urgent need to re-evaluate LLM assessment practices to ensure accuracy & reliability.
- Recommends aligning evaluations closer to real-world generative usage scenarios.  
- Emphasizes adopting comprehensive evaluation protocols beyond just benchmarks.
- Provides empirical evidence revealing deficiencies of existing proxy MCQ evaluations.
- Warns risk of misjudging LLM capabilities without proper assessments aligned with practical utility.
- Results offer insights to guide future research towards better evaluation methodologies for LLMs.

In summary, the paper conducts an extensive investigation that uncovers serious limitations of prevalent probability-based proxy evaluations for determining LLM competency. It cautions against over-reliance on such methods and advocates aligning assessments to real-world generative applications, proposing recommendations to guide progress in this crucial area of LLM research.
