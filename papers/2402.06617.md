# [FaBERT: Pre-training BERT on Persian Blogs](https://arxiv.org/abs/2402.06617)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing Persian language models like ParsBERT show good performance but still have room for improvement, especially in handling informal Persian text which has unique features like flexible sentence structures, cultural references, informal lexicon, and slang.

- Large language models (LLMs) come with drawbacks like slower response times, need for advanced hardware, and privacy concerns.

Solution:
- The authors pre-train FaBERT, a compact BERT-base model, exclusively on the cleaned HmBlogs corpus containing both formal and informal Persian text. 

- FaBERT has only 124 million parameters compared to over 300 million parameters in LLMs, making it suitable for use in standard computers.

Methodology:
- HmBlogs corpus containing over 20 million Persian blog posts is cleaned by filtering noisy content and standardizing characters.

- FaBERT replicates BERT-base architecture with 12 transformer layers and is pre-trained from scratch on HmBlogs corpus using masked language modeling objective.

- A vocabulary size of 50,000 is used to balance capturing linguistic details and computational demands.

Contributions:
- Release FaBERT model publicly for use in Persian NLP tasks. 

- Evaluate performance on 12 datasets covering tasks like sentiment analysis, named entity recognition, question answering, textual entailment etc.

Results:
- FaBERT outperforms previous Persian BERT models across 9 out of 12 tasks, demonstrating state-of-the-art results.

- It excels in informal texts while maintaining robust performance on formal texts.

- The results highlight the importance of using cleaned and diverse corpora like HmBlogs for pre-training language models.

Conclusion:
- Persian blogs with diverse writing styles can significantly enhance language model performance for Persian NLP tasks.

- Compact models like FaBERT strike a balance between capability and efficiency compared to large models.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The authors pre-train a compact Persian BERT model called FaBERT on cleaned blog texts from the HmBlogs corpus and demonstrate its state-of-the-art performance on diverse Persian natural language understanding tasks involving sentiment analysis, named entity recognition, question answering, and more.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this paper are:

1. Pre-training a BERT-base model from scratch exclusively on the cleaned HmBlogs corpus, which consists of raw texts from Persian blogs. This model is called FaBERT.

2. Evaluating FaBERT's performance on 12 datasets across various downstream tasks, including sentiment analysis, irony detection, natural language inference, question paraphrasing, named entity recognition, and question answering. The results show that FaBERT achieves state-of-the-art performance on many of these tasks.

So in summary, the main contributions are pre-training a new BERT model for Persian called FaBERT using a corpus of Persian blog texts, and comprehensively evaluating this model on a diverse set of Persian natural language understanding tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper are:

- FaBERT - The name of the Persian BERT-base model pre-trained on the HmBlogs corpus that is introduced in this paper.

- BERT - FaBERT is a BERT-base model, so BERT is a core concept.

- Pre-training - The paper focuses on pre-training FaBERT on the HmBlogs corpus.

- Persian - FaBERT is designed for the Persian language, so Persian language processing is a key aspect. 

- HmBlogs - The corpus that FaBERT is pre-trained on, containing informal and formal Persian text.

- Downstream tasks - Various tasks like sentiment analysis, NER, NLI etc. that FaBERT is evaluated on.

- Natural Language Understanding (NLU) - FaBERT is designed to excel in traditional NLU tasks.

- Informal text - A unique aspect of Persian text that FaBERT aims to handle effectively.

- Sentence structures - Understanding complex Persian sentence structures is a goal. 

- Performance - Evaluating and comparing FaBERT's performance to other models.

So in summary - FaBERT, BERT, pre-training, Persian, HmBlogs, downstream tasks, NLU, informal text, sentence structures, and performance evaluation seem to be key terms associated with this paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using the HmBlogs corpus for pre-training FaBERT. What steps were taken to clean and process this corpus before using it for pre-training? What impact did using this corpus have on FaBERT's performance?

2. FaBERT uses a smaller vocabulary size of 50,000 tokens compared to other BERT models like ParsBERT. What motivated this design choice? How does the smaller vocabulary impact FaBERT's ability to handle longer input sequences?

3. The paper omits the Next Sentence Prediction (NSP) pre-training task used in original BERT. What was the rationale behind this decision? Does removing NSP impact the model's capability for tasks involving sentence relationships?

4. What hyperparameters were used during FaBERT's pre-training? How were values like learning rate, batch size, and number of training steps decided upon? Did you try other configurations?

5. The paper evaluates FaBERT on 12 different datasets spanning tasks like sentiment analysis, NER, NLI etc. What findings from these comprehensive evaluations stand out the most regarding FaBERT's capabilities?

6. How does FaBERT compare to other BERT models like mBERT, XLM-R, ParsBERT etc. across the various downstream tasks? In which tasks does it outperform others by a significant margin?

7. FaBERT demonstrates state-of-the-art results on certain datasets but lags on a few others. What factors might contribute to why a particular model is better suited for certain tasks or datasets?

8. For question answering tasks, FaBERT's performance exceeds other models on ParsiNLU RC and PCoQA but slightly trails on PQuAD. What underlying differences between these QA datasets could explain this discrepancy?

9. The cleaned corpus from Persian blogs is hypothesized to enhance FaBERT's informal text understanding. Do the results align with this hypothesis? In which of the downstream tasks is this benefit most prominently observed?

10. The paper demonstrates promising results using a BERT-base model design. Do you think scaling up FaBERT to a larger architecture with more parameters could lead to further improvements? What additional experiments could be helpful?
