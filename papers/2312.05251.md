# [Reconstructing Hands in 3D with Transformers](https://arxiv.org/abs/2312.05251)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points in the paper:

This paper presents HaMeR, a robust and accurate approach for 3D hand mesh reconstruction from monocular RGB images or video frames. The key insight is scaling up both the training data and model capacity. For data, the authors consolidate multiple datasets with 2D or 3D hand annotations, resulting in 2.7 million training examples. For the model, they adopt a simple but high capacity transformer architecture based on Vision Transformer (ViT). Experiments demonstrate state-of-the-art performance on standard 3D pose benchmarks like FreiHAND and HO3D. More importantly, HaMeR shows significant gains in robustness on their newly collected challenging in-the-wild dataset HInt, with 2-3x higher PCK scores. This highlights the importance of scaling up data and model size. Additionally, the paper contributes HInt, sourced from egocentric and 3rd person videos, providing diverse real-world hands with occlusion labels. In summary, through simple scaling, HaMeR sets a new state-of-the-art for 3D hand pose estimation, with particularly strong gains in robustness, as evidenced by large margins of improvement on real world data. The code, data, and models are made publicly available.
