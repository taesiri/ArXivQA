# [DeepEraser: Deep Iterative Context Mining for Generic Text Eraser](https://arxiv.org/abs/2402.19108)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Text removal in images is an important task with applications like concealing sensitive information and text editing. Although there has been good progress, existing methods still suffer from issues like artifacts, incomplete erasure, and insufficient semantic extraction. 

Proposed Solution:
This paper proposes DeepEraser, an end-to-end deep network for generic text removal. The key idea is to erase text iteratively by mining context and updating the image. Specifically,

1) It uses a recurrent architecture with a shared erasing module that operates for K iterations. 

2) At each iteration, the module aggregates previous progress and mines additional semantic context to erase the target text. It outputs a residual image to update the result.

3) Through iterative refinement, the text regions are progressively replaced with more appropriate content that matches the surrounding context.

Main Contributions:

1) An iterative text erasing strategy based on mining context to replace text with suitable content.

2) A custom mask generation strategy to facilitate adaptive text removal instead of simply removing all text.

3) Compact network design with only 1.4M parameters and a simple L1 loss for training.

Experiments on standard benchmarks like SCUT-EnsText demonstrate state-of-the-art performance of DeepEraser for generic text removal. Both qualitative and quantitative results also show its strong generalization ability for custom mask text removal.
