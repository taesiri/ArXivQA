# [Rethinking Data Distillation: Do Not Overlook Calibration](https://arxiv.org/abs/2307.12463)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is: Why are neural networks trained on distilled datasets (DDNNs) not calibratable using existing calibration methods, and how can we improve their calibration? Specifically, the paper identifies two key problems:1) DDNNs suffer from overconfidence, producing probabilities higher than their actual accuracy. 2) Existing calibration methods like temperature scaling and mixup, which work well for networks trained on full datasets, tend to over-calibrate DDNNs and make them underconfident. The central hypothesis is that distilled datasets lead to networks that are not calibratable due to:(i) A more concentrated distribution of maximum logits, leaving less room for adjusting confidence post-training.(ii) Loss of semantically meaningful information unrelated to the classification task, making it harder to calibrate during training.To address this, the paper proposes two masking techniques - Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) - to improve the calibration of DDNNs by perturbing validation data and the distillation process to retain more information.In summary, the main research question is why DDNNs are poorly calibrated by existing methods, and the central hypothesis for the reasons behind this, which the proposed techniques aim to mitigate. The effectiveness of the new techniques is then experimentally validated.


## What is the main contribution of this paper?

This paper makes several contributions related to studying and improving the calibration of neural networks trained on distilled datasets (DDNNs):- It identifies for the first time that DDNNs suffer from overconfidence and are not calibratable using existing calibration methods designed for networks trained on full datasets. - It provides analysis on why DDNNs are difficult to calibrate, attributing it to two key factors: (1) DDNNs produce a more concentrated distribution of maximum logits compared to networks trained on full data, and (2) distilled datasets lose semantically meaningful information unrelated to the classification task during the distillation process.- The paper proposes two techniques to address these limitations and improve the calibration of DDNNs:  - Masked Temperature Scaling (MTS) introduces perturbations to the validation data to make the DDNN produce more diverse and smaller logits, enabling better calibration.  - Masked Distillation Training (MDT) perturbs the synthetic data during distillation to force the extraction of richer information and improve encoding ability.- Extensive experiments demonstrate that the proposed techniques can reduce the Expected Calibration Error (ECE) of DDNNs significantly (by up to 91.05%) across various datasets, distillation methods, and model architectures, while maintaining accuracy.In summary, the key contribution is identifying the calibration problem with DDNNs and providing both analysis and solutions to enable more powerful and calibratable DDNNs, given the increasing use of dataset distillation for efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper shows that neural networks trained on distilled datasets suffer from overconfidence and are not calibratable with existing calibration methods due to the distilled data's concentrated information and loss of semantically meaningful details unrelated to classification; the authors propose masked distillation training and masked temperature scaling to improve calibration of networks trained on distilled data.
