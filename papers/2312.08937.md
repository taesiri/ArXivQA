# [BiPFT: Binary Pre-trained Foundation Transformer with Low-rank   Estimation of Binarization Residual Polynomials](https://arxiv.org/abs/2312.08937)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes the first Binary Pretrained Foundation Transformer (BiPFT) for natural language understanding tasks. BiPFT remarkably saves 56× operations and 28× memory compared to full-precision models by using 1-bit weights and activations. In contrast to previous task-specific binary transformers that rely heavily on distillation and hyperparameter tuning, BiPFT exhibits much enhanced learning capabilities and stability, enabling straightforward finetuning on downstream tasks. Specifically, the authors first build a strong binary transformer baseline and analyze its weaknesses including unreliability to hyperparameters and over-reliance on task-specific distillation. They then propose to pretrain the binary foundation model using masked language modeling and next sentence prediction objectives. Experiments show BiPFT significantly outperforms the binary baseline and rivals full-precision BERT. Furthermore, a data-driven binarization method is introduced to model the residual errors during binarization using low-rank estimators. This further improves BiPFT's performance. Extensive analyses demonstrate BiPFT's effectiveness, including improved optimization efficiency, reduced dependence on distillation, and better generalization across tasks. The released binary foundation model and code simplify future deployment of binary transformers.
