# [Asking Multimodal Clarifying Questions in Mixed-Initiative   Conversational Search](https://arxiv.org/abs/2402.07742)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- In conversational search systems, ambiguous or incomplete queries can lead to irrelevant results. Query clarification is important to resolve ambiguities and capture user intent. 
- Prior work has focused on text-only clarification questions. This paper investigates the novel task of multimodal query clarification (MQC) - using images along with clarifying questions. 

Proposed Solution:
- A new dataset called Melon with over 4k multimodal clarifying questions enriched with 14k images is collected to facilitate MQC research.
- A generative multimodal model named Marto is proposed for MQC. It adopts a prompt-based fine-tuning strategy to perform different subtasks:
   1) Classify if a question needs images or not  
   2) Select the most relevant image for the question
   3) Generate document identifiers for retrieval

Main Contributions:  
- Formulation of the MQC task and creation of the Melon dataset
- Proposal of the Marto model for MQC based on generative modeling and prompt tuning
- Extensive analyses showing images lead to significant improvements in retrieval (up to 90% gain). Multimodal questions also result in more contextualized and informative user responses
- Demonstration that Marto outperforms competitive baselines in efficiency and effectiveness for document retrieval

In summary, this paper formalizes the novel MQC task, provides the Melon dataset to facilitate research, and develops the Marto model to effectively perform MQC for improved document retrieval in conversational search systems.
