# [ViTamin: Designing Scalable Vision Models in the Vision-Language Era](https://arxiv.org/abs/2404.02132)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current vision-language models (VLMs) mostly use vanilla Vision Transformers (ViTs) as the image encoder without much exploration of other architectures. This is partly due to high computing demands of training VLMs and the assumption that ViT scales better with more data.
- Existing VLM benchmarks focus on classification/retrieval and lack downstream tasks like open-vocabulary detection/segmentation and evaluation of VLMs as backbones in large multimodal models.

Proposed Solution:
- Establish evaluation protocols to benchmark vision models under CLIP framework using public DataComp-1B dataset. Examine model architectures like ViT, ConvNeXt, CoAtNet across model scales, data scales and output feature resolution.
- Propose ViTamin, a 3-stage hybrid architecture with convolutional blocks in first two stages and transformer blocks in last stage to get high resolution features. Also propose enhancements like MBConv-LN and TFB-GeGLU blocks.
- Introduce Locked-Text Tuning to exploit a frozen pretrained text encoder to guide image encoder training.
- Benchmark on suite of downstream tasks - open-vocabulary detection/segmentation using F-ViT and FC-CLIP frameworks and large multimodal modeling using LLaVA framework.

Main Contributions:
- Comprehensive benchmarking reveals ViT's better scalability but importance of feature resolution and effectiveness of hybrid architectures like CoAtNet.
- Proposed ViTamin sets new SOTA on ImageNet zero-shot accuracy among models trained on public DataComp-1B dataset. ViTamin-XL achieves 82.9% accuracy with 436M parameters.
- Locked-Text Tuning brings significant gains over baseline without extra cost.
- New downstream tasks benchmark show consistent gains over Vision Transformer baselines.
- Work motivates more research into neural architecture search for image encoders in VLMs instead of just using vanilla ViTs.

The summary covers the key aspects of the problem being addressed, the proposed ViTamin model and training improvements, extensive benchmarking analysis, and the main results showing state-of-the-art capabilities and potential future research directions.
