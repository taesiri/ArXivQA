# [Shifted Diffusion for Text-to-image Generation](https://arxiv.org/abs/2211.15388)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we improve text-to-image generation by designing a better diffusion process that incorporates prior knowledge from pre-trained models like CLIP?

The key hypotheses appear to be:

1) The vanilla diffusion process used in prior work like DALL-E 2 may not be ideal for generating high-quality CLIP image embeddings, since the noise distribution is far from the true distribution of embeddings. 

2) By designing the diffusion to start from an distribution estimated from real embeddings and shift towards that distribution during sampling, it can better approximate the target embedding.

3) This "shifted diffusion" can lead to improved text-to-image generation, especially when combined with using multiple adjustable Gaussians to capture multimodal aspects of the true embedding distribution.

4) The proposed framework also enables beneficial applications like semi-supervised and language-free training of text-to-image models.

So in summary, the central focus seems to be improving text-to-image generation through a novel shifted diffusion technique that incorporates insights about the CLIP latent space, with flexible training as an additional benefit. The paper aims to validate these hypotheses through quantitative metrics and human evaluation.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes Corgi, a novel diffusion model for text-to-image generation. Corgi incorporates prior knowledge from pre-trained models like CLIP into the diffusion process through a new initialization distribution and transition step. 

2. It enables semi-supervised and language-free training for text-to-image generation. With only 1.7% captioned images, Corgi achieves results comparable to SOTA models on MS-COCO. It also achieves new SOTA on language-free text-to-image generation.

3. Extensive experiments are conducted on large datasets. Both quantitative metrics and human evaluation show Corgi's superior text-to-image generation ability compared to strong baselines like DALL-E 2. 

4. Ablation studies demonstrate the benefits of the proposed shifted diffusion over standard diffusion for text-to-image generation. Experiments also analyze different design choices for the initialization distribution.

In summary, the key contribution is a new diffusion model that better utilizes pre-trained models like CLIP, enables semi-supervised and language-free training, and achieves improved text-to-image generation ability. The effectiveness is shown through comprehensive experiments and comparisons.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes Corgi, a novel diffusion model for text-to-image generation that incorporates knowledge from pre-trained CLIP models into the diffusion process, enabling flexible applications like semi-supervised and language-free training while achieving improved performance over strong baselines.
