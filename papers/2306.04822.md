# [Optimizing ViViT Training: Time and Memory Reduction for Action   Recognition](https://arxiv.org/abs/2306.04822)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we reduce the substantial training time and memory consumption associated with video transformers, specifically the factorised encoder variant of the ViViT model, in order to make them more accessible for researchers and practitioners with limited resources?The key hypothesis seems to be:By freezing the spatial transformer and adding a lightweight adapter module during fine-tuning, along with properly initializing the temporal transformer, it is possible to significantly cut down on training costs and memory usage of the ViViT model without sacrificing accuracy or model sophistication.In summary, the paper aims to investigate strategies to optimize ViViT training to address the challenges posed by the model's demanding computational requirements, with the goal of enabling broader access and application of advanced video transformers. The proposed methods focus on harnessing the sophistication of ViViT while minimizing its resource demands.


## What is the main contribution of this paper?

This paper presents a method to optimize the training of the ViViT model, a video transformer model, for action recognition. The key contributions are:- It proposes a two-stage training strategy to reduce training time and memory requirements of ViViT while maintaining accuracy. - In stage 1, it pretrains a low-frame version of ViViT (e.g. 8 frames) to initialize the model. This serves to initialize both the spatial and temporal transformers.- In stage 2, it fine-tunes the model on more frames (e.g. 32-128 frames) while freezing the spatial transformer and adding a lightweight adapter module. This allows end-to-end training to focus on the temporal transformer. - It shows this approach can reduce training time by ~50% and memory usage while achieving comparable or slightly better accuracy than baseline ViViT training.- It demonstrates the capability to handle more input frames (up to 80) on typical GPUs with this method.- It also shows the ability to incorporate larger backbone vision transformers (e.g. ViT-G) in the spatial transformer, enabled by the memory savings.In summary, the key contribution is a training recipe that makes training video transformers like ViViT significantly more efficient in terms of speed and memory, opening up access to larger models and input videos. This could help democratize training of advanced video models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient training strategy for video transformers that uses an adapter module and initialization with a low-frame model to reduce training time and memory usage while maintaining accuracy.
