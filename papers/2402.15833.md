# [Prompt Perturbation Consistency Learning for Robust Language Models](https://arxiv.org/abs/2402.15833)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 show promising performance on many NLP tasks but lag behind on structured prediction tasks like intent classification and slot filling (IC-SF) which are critical for voice assistants. 
- LLMs are also sensitive to minor perturbations in input prompts, leading to inconsistent and inaccurate predictions. This lack of robustness limits their reliability for real-world applications.

Methodology:
- Use a structured prompt format with sentinel markers for instruction fine-tuning of LLMs on IC-SF tasks. This achieves accuracy comparable to state-of-the-art discriminative models.
- Evaluate model robustness by introducing perturbations - oronyms, synonyms, paraphrasing - into test prompts. Find significant performance drops especially for slot filling. 
- Propose prompt perturbation consistency learning (PPCL) to improve robustness. Adds perturbation consistency loss between predictions on original and perturbed prompts during fine-tuning.

Contributions:
- Show instruction fine-tuning of LLMs can achieve highly competitive performance on IC-SF, closing the gap with discriminative models.
- Demonstrate lack of robustness of LLMs on IC-SF tasks under different types of prompt perturbations.
- Introduce PPCL framework to explicitly enforce prediction consistency between perturbed prompts during fine-tuning. Recovers average 59% drop in intent classification and 69% in slot filling.
- PPCL outperforms data augmentation baseline while using 10x fewer augmented examples. Enhances efficiency and scalability.

In summary, the paper demonstrates LLMs can reach SOTA accuracy on structured tasks with appropriate fine-tuning, analyzes their robustness issues, and proposes an effective prompt consistency regularization method to enhance robustness at low data cost.
