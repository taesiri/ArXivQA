# [The pitfalls of next-token prediction](https://arxiv.org/abs/2403.06963)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper challenges the effectiveness of next-token prediction frameworks like teacher-forcing for training language models that can plan ahead and solve complex reasoning tasks. 
- It argues that existing criticisms of next-token prediction focus on failures during autoregressive inference due to error compounding. However, these assume teacher-forcing has already learned an accurate next-token predictor, which may not be the case.

Proposed Failure Modes of Teacher-Forcing:
- The paper identifies two key issues that can arise during teacher-forcing training in "lookahead tasks" that require anticipating future tokens:
  1) Clever Hans Cheating: Model exploits ground truth tokens revealed by teacher to simplistically fit later tokens, without learning true task structure. This diminishes useful training signal.
  2) Indecipherable Token: Due to 1), model loses supervision signal to properly learn initial token(s), as information about later tokens is absorbed by the cheat. So the initial token(s) become impossible to fit.

- Together, these can cause in-distribution failure even if trained on infinite data.

Verifying Failure Modes:
- Paper designs a simple path-finding task on graphs that requires lookahead, and is straightforward to solve.
- Empirically demonstratescomplete in-distribution failure of Transformers and Mamba architecture due to above mechanisms, despite task simplicity.
- Additional experiments confirm and quantify extent of Clever Hans cheating and indecipherability of first token under standard training.

Alternatives to Circumvent Failure:
- Finds that "teacherless" training (no ground truth tokens revealed during training) prevents cheating and enables learning, verifying hypotheses.
- Reversing targets also removes indecipherable token by eliminating need for lookahead, enabling learning.

Contributions:
- Conceptually articulates and empirically demonstrates new failure modes stemming from teacher-forcing, beyond existing criticisms.
- Shows remarkably, teacher-forcing fails on simple tasks requiring lookahead.
- Provides initial evidence that teacherless training is a promising approach to mitigate issues.
- Lays precise groundwork to inspire research on limitations of and alternatives to next-token prediction.
