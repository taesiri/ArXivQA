# [DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis   with 3D Gaussian Splatting](https://arxiv.org/abs/2312.00112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Accurately and efficiently modeling dynamic 3D scenes and their motions is very challenging due to the complexity of temporal dynamics and motion. Existing methods have limitations in quickly training models, achieving real-time rendering speeds, and synthesizing high-quality novel views of complex dynamic scenes. 

Proposed Solution:
The paper proposes DynMF, a compact and efficient neural representation that decomposes a dynamic scene into a small set of shared neural trajectories. The key ideas are:

- Represent the motion of each 3D point as a linear combination of a globally shared set of B learned trajectory basis functions modeled by a time-based MLP. This motion factorization constrains the ill-posed dynamic scene modeling problem.

- Bind this motion representation with 3D Gaussian splatting scene representation for efficient and expressive novel view synthesis. The mean and rotation of each Gaussian follows the trajectory basis functions over time.

- Apply sparsity loss on motion coefficients to disentangle motions and enable controllable synthesis of new scenes.

Main Contributions:

- Expressively models complex non-rigid deformations and scene dynamics through simple and interpretable neural motion factorization into just a few shared basis trajectories.

- Extremely efficient, enabling real-time 120FPS rendering of 1K resolution images after quick 30 minute training. Outperforms state-of-the-art in speed and quality.

- Enables disentangling of motions to independently control scene dynamics and generate novel motions. Demonstrates video editing capabilities.

- Effectively handles ill-posed monocular dynamic scene modeling without prior scene knowledge or dense correspondences. Achieves high quality results on challenging real and synthetic datasets.

In summary, the paper introduces a compact neural representation DynMF that achieves state-of-the-art real-time rendering of dynamic scenes by factorizing motions into a small set of shared neural trajectories. The motion disentanglement also enables controllable generation of new scenes.
