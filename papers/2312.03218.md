# [Accelerated Gradient Algorithms with Adaptive Subspace Search for   Instance-Faster Optimization](https://arxiv.org/abs/2312.03218)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new perspective for understanding the complexity of optimization problems that is closer to reality. It introduces the concepts of $(\alpha,\tau_{\alpha})$-degeneracy to characterize the sharp drop of Hessian eigenvalues observed in many practical problems like machine learning. Based on this, the authors design adaptive algorithms like AGMAS that automatically fit the structure of problems without needing to know $\alpha$ or $\tau_{\alpha}$ in advance. These algorithms provably achieve faster convergence rates beyond known lower bounds for simpler problems, like attaining the information-theoretically optimal $\tilde{\mathcal{O}}(\mu^{-1/3})$ rate for linear regression when the nuclear norm is constant. The algorithms are also extended to generic convex and nonconvex settings. Moreover, the lower bound framework is strengthened to show the optimality of AGMAS across the entire range where acceleration is possible. Overall, this work opens up new possibilities for optimization algorithms that are faster and more aligned with the true complexity of modern problems.
