# [Investigating the Quality of DermaMNIST and Fitzpatrick17k   Dermatological Image Datasets](https://arxiv.org/abs/2401.14497)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
The paper investigates data quality issues in two popular skin image datasets - DermaMNIST and Fitzpatrick17k. These datasets are widely used to develop and evaluate deep learning models for automated skin disease diagnosis. However, the authors uncover concerning problems like data leakage, duplication, mislabeling, and lack of proper train-test splits, which can result in unreliable and overly optimistic model performance. 

DermaMNIST Analysis
- The authors find extensive data leakage in DermaMNIST, with over 13% of unique skin lesions present in multiple partitions. This means models can exploit this leakage to perform better on the test set.
- The image resizing operations used to create DermaMNIST result in loss of details. The authors propose improved resizing to retain more information.
- Two new versions of the dataset are proposed - DermaMNIST-C without any data leakage, and DermaMNIST-E with added complexity using external test sets.

Fitzpatrick17k Analysis
- Large scale analysis uncovers thousands of duplicate and near duplicate images pairs, with conflicting labels.
- Numerous mislabeled and erroneous non-skin images are identified.
- Lack of a proper held-out test set means reported performance metrics are unreliable. 
- A cleaned version called Fitzpatrick17k-C is proposed with duplicates removed and standardized data splits.

Contributions 
- Thorough analysis of data quality issues in two highly popular skin image datasets using automated and manual techniques. 
- Quantitative analysis of the extent of problems uncovered - data leakage, duplicates, mislabels.
- Development of corrected/improved versions of both datasets to enable more rigorous evaluation.
- Updated benchmarks using corrected datasets to accurately reflect model capabilities.
- Detailed guidelines and publicly available code to facilitate analysis of other datasets.

In summary, this paper makes important contributions towards uncovering and addressing critical data quality issues in medical imaging datasets to enable more reliable and robust model development and evaluation.


## Summarize the paper in one sentence.

 This paper investigates data quality issues such as data leakage, duplication, mislabeling, and lack of standardized partitions in two popular dermatology image datasets, DermaMNIST and Fitzpatrick17k, proposes corrected versions of the datasets, and provides updated benchmarks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) Thorough analysis of data quality issues in two popular skin image datasets - DermaMNIST and Fitzpatrick17k. The analyses uncover problems like data leakage, duplicates, mislabeled images, absence of proper train/validation/test splits, etc.

2) Proposal of corrected versions of the datasets - DermaMNIST-C, DermaMNIST-E, and Fitzpatrick17k-C - that address the data quality issues uncovered in the analyses. These include removing duplicates, fixing data leakage, standardizing dataset splits, etc.

3) Quantitative benchmarking of models on the original and corrected datasets to demonstrate the effects of data quality issues and the reliability of the corrected datasets.

4) Public release of the analysis code, cleaned datasets, benchmark results, and visualizations to enable further analyses by other researchers and facilitate identification and resolution of data quality issues in other datasets. 

In summary, through meticulous analysis, the paper demonstrates pervasive data quality issues in popular skin image datasets, proposes solutions via corrected datasets, benchmarks them appropriately, and publicly releases artifacts to facilitate similar endeavors by the community.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with it are:

- Data quality
- Data leakage
- Duplicate detection
- Data partitioning
- Benchmarking
- Skin image analysis
- DermaMNIST
- Fitzpatrick17k
- Model training
- Model evaluation

The paper conducts thorough analyses of two popular skin image datasets - DermaMNIST and Fitzpatrick17k - uncovering issues like data duplication, leakage across partitions, mislabeled images, etc. It proposes corrected versions of these datasets to address these data quality issues. The key focus areas are ensuring proper data partitioning and benchmarking practices, identifying duplicate images, and detecting erroneous/outlier images. The analysis aims to raise awareness about potential data quality issues in machine learning datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper proposes two corrected/extended versions of the DermaMNIST dataset: DermaMNIST-C and DermaMNIST-E. What are the key differences between these datasets and the original DermaMNIST? How were they created and what issues do they aim to address?

2. When creating the high resolution (224x224) images for DermaMNIST-C and DermaMNIST-E, the authors use a different approach than the original DermaMNIST paper. What was the issue with DermaMNIST's approach and how does the proposed approach in this paper improve on it? 

3. What method does the paper use to detect duplicate images in the Fitzpatrick17k dataset? Explain the high-level approach and why this method was chosen over using something like checksums on the image files directly.  

4. The paper manually reviews a subset of the duplicate images found to verify the accuracy of the automated duplicate detection method. What evaluation metric is reported and what was the agreement between the reviewers? Why was manual verification still necessary despite using an automated method?

5. For dealing with duplicate images in Fitzpatrick17k, the paper decides to remove images from duplicate clusters only if there are no conflicting diagnosis/FST labels within that cluster. Explain the rationale behind this design decision and the tradeoffs involved.

6. The authors use a nearest neighbor-based outlier detection method to find potentially erroneous non-skin images in Fitzpatrick17k. Walk through how this outlier detection process works step-by-step. What are some limitations of relying on this method?

7. What are some key differences highlighted between the original Fitzpatrick17k benchmark results and the updated benchmark results on Fitzpatrick17k-C? What reasons may account for these differences?

8. For the Fitzpatrick17k experiments, the paper finds that Groh et al. did not use a held-out test set and used the validation set for both model selection and final testing. Why is this an issue?

9. The data leakage taxonomy by Kapoor et al. is referenced in the paper. What specific types of data leakage were found in DermaMNIST and Fitzpatrick17k based on this taxonomy? 

10. The paper mentions the risks of data quality issues like those found in DermaMNIST and Fitzpatrick17k in terms of model robustness and generalizability. Elaborate further on how such data issues can negatively impact model performance and reliability.
