# [Adaptive Computation Modules: Granular Conditional Computation For   Efficient Inference](https://arxiv.org/abs/2312.10193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Adaptive Computation Modules: Granular Conditional Computation For Efficient Inference":

Problem:
- State-of-the-art transformer models like BERT and ViT are very large and computationally expensive, limiting their use in latency-sensitive and energy-constrained applications. 
- Existing methods like quantization, knowledge distillation, and sparsification can reduce costs but often incur an accuracy drop. 
- Globally reducing computation for all inputs is suboptimal as different inputs and even different tokens within an input may need differing amounts of computation.

Proposed Solution:
- Introduce Adaptive Computation Modules (ACMs) - a modular unit that can adaptively determine the computational load per input token through a learned gating mechanism.
- An ACM consists of N parallel learners that progressively refine the representations, and a gating network that selects how many of these learners to execute per token.
- Convert any pre-trained model to use ACMs via a 3-stage process:
   1) Distill knowledge from original layers into ACM learners
   2) Train gating network with artificial supervision
   3) End-to-end fine-tune with auxiliary losses
- Achieves dynamism in computation across samples and spatial locations in the input.

Main Contributions:
- Propose concept of ACMs for spatially adaptive conditional computation in transformers
- Show that large static modules like MLPs are inefficient, and full capacity is needed only for a subset of tokens 
- Provide an efficient way to convert pre-trained models to use ACMs via distillation
- Demonstrate state-of-the-art accuracy vs efficiency tradeoffs on image classification and speech recognition tasks

In summary, the paper introduces a novel module called ACMs that can greatly reduce the inference cost of transformers without compromising accuracy. The key ideas are making computation adaptive across input tokens, and providing an easy way to convert pre-trained models via distillation.
