# [COLA: A Benchmark for Compositional Text-to-image Retrieval](https://arxiv.org/abs/2305.03689)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions addressed in this paper are:1. How can we design a benchmark to test the compositional reasoning capability of vision-language models, specifically their ability to compose objects with attributes?2. What adaptation strategies can improve the compositional attribute-object binding abilities of existing vision-language models like CLIP and FLAVA?The paper introduces COLA, a new benchmark for Composing Objects Localized with Attributes. COLA contains single object and multi-object textual queries that require associating objects with the correct attributes and avoiding distractor compositions.Using COLA as a testbed, the paper explores different strategies for adapting pretrained vision-language models. The main hypothesis is that using a lightweight multimodal adapter module with cross-attention over image patches and text tokens will yield the best compositional binding capability.The key findings are:- The multimodal adapter significantly outperforms other adaptation methods like prompt tuning, linear probing, and tuning later vision/text encoder layers.- The multimodal adapter improves both CLIP and FLAVA models to comparable performance, suggesting the importance of training the multimodal layers on attribute-object data rather than just using pretrained multimodal layers.- COLA is a harder benchmark than the existing CREPE, requiring the multimodal adapter while simpler tuning works for CREPE.In summary, the paper introduces a new compositional benchmark and shows that a simple multimodal adaptation strategy can best improve attribute-object compositional reasoning in vision-language models.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors propose a new benchmark task called $\mathcal{C}ola$ for evaluating compositional reasoning in vision-language models. The task requires models to retrieve images that contain objects with the correct configurations of attributes, rather than retrieving distractor images with the same objects/attributes in incorrect configurations.2. They introduce two settings for the $\mathcal{C}ola$ benchmark - single object queries with multiple attributes (e.g. "round white table") and multi-object queries where each object has multiple attributes (e.g. "square white plate on round brown wooden table"). 3. Using $\mathcal{C}ola$ as a testbed, they systematically explore different strategies for adapting pretrained vision-language models like CLIP and FLAVA to perform better on compositional reasoning tasks. They find that adding a lightweight multimodal adapter module with cross-attention performs the best.4. Their proposed adapter approach brings the performance of CLIP to be comparable to fine-tuning a larger model like FLAVA, suggesting that training the multimodal adapter on attribute-object compositions is crucial.5. Experiments on $\mathcal{C}ola$ and another compositional benchmark CREPE show their adapter strategy significantly outperforms other tuning methods like prompt tuning or fine-tuning later layers. 6. Analysis of results suggests $\mathcal{C}ola$ is a challenging benchmark, with significant gap between model and human performance, indicating room for progress on compositional reasoning.In summary, the key contributions are proposing the new $\mathcal{C}ola$ benchmark, using it to explore model tuning strategies, and showing a simple but effective adapter approach to improve compositional reasoning in vision-language models. The benchmark and analysis provide a strong foundation for future work on this important capability.
