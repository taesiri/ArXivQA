# [COLA: A Benchmark for Compositional Text-to-image Retrieval](https://arxiv.org/abs/2305.03689)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions addressed in this paper are:1. How can we design a benchmark to test the compositional reasoning capability of vision-language models, specifically their ability to compose objects with attributes?2. What adaptation strategies can improve the compositional attribute-object binding abilities of existing vision-language models like CLIP and FLAVA?The paper introduces COLA, a new benchmark for Composing Objects Localized with Attributes. COLA contains single object and multi-object textual queries that require associating objects with the correct attributes and avoiding distractor compositions.Using COLA as a testbed, the paper explores different strategies for adapting pretrained vision-language models. The main hypothesis is that using a lightweight multimodal adapter module with cross-attention over image patches and text tokens will yield the best compositional binding capability.The key findings are:- The multimodal adapter significantly outperforms other adaptation methods like prompt tuning, linear probing, and tuning later vision/text encoder layers.- The multimodal adapter improves both CLIP and FLAVA models to comparable performance, suggesting the importance of training the multimodal layers on attribute-object data rather than just using pretrained multimodal layers.- COLA is a harder benchmark than the existing CREPE, requiring the multimodal adapter while simpler tuning works for CREPE.In summary, the paper introduces a new compositional benchmark and shows that a simple multimodal adaptation strategy can best improve attribute-object compositional reasoning in vision-language models.
