# [OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model   Pre-trained from Scratch](https://arxiv.org/abs/2309.10706)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we develop an open-sourced 15B bilingual asymmetric seq2seq model pre-trained from scratch that achieves strong performance across a variety of natural language understanding and generation tasks? The key hypothesis appears to be that by:1) Carefully collecting and processing open-source pre-training data in both Chinese and English 2) Constructing a high-quality bilingual Flan dataset for instruction tuning3) Using an asymmetric shallow-encoder deep-decoder architecture 4) Employing a stage-wise training strategy with different objectives5) Incorporating various model architecture improvements and training techniquesThey can pre-train a 15B parameter bilingual model from scratch that attains excellent results on benchmarks like SuperGLUE, MMLU, CMMLU, C-Eval etc., even outperforming some models trained on much more data.So in summary, the central research direction is on developing a performant open-sourced bilingual seq2seq model through innovations in data, architecture, training approach and techniques. The key hypothesis is that their proposed techniques and methodology can lead to a highly capable model despite using only open-source data and limited pre-training resources.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. It presents OpenBA, an open-sourced 15B parameter bilingual asymmetric seq2seq model pre-trained from scratch. The model architecture uses a shallow encoder and deep decoder design.2. It provides details on how to construct the pre-training data and bilingual Flan data from open resources. The pre-training data contains balanced English and Chinese tokens. The bilingual Flan data combines English Flan and manually constructed Chinese Flan data. 3. It describes the training methodology, including a 3-stage training strategy (UL2 pre-training, length adaptation, and bilingual Flan training), training objectives, and techniques to enhance training efficiency and stability.4. It evaluates the model on a range of benchmarks for understanding, reasoning, and generation tasks. The results show OpenBA achieves strong performance compared to other models while being highly efficient in terms of training cost.5. It open-sources the implementation details, data, model checkpoints, and evaluation code to enable reproducibility and facilitate future research.In summary, the main contribution is presenting an open-sourced bilingual seq2seq model pre-trained from scratch with competitive performance, along with all the necessary details to reconstruct the training pipeline. This contributes an alternative to the predominant decoder-only LLMs and provides a strong generative model to the open-source community.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:This paper presents OpenBA, a 15 billion parameter open-sourced bilingual asymmetric seq2seq model pre-trained from scratch using 380 billion tokens, which achieves strong performance on natural language understanding, generation, and reasoning benchmarks despite its smaller model size and training data compared to other large language models.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in the field:- This paper presents OpenBA, an open-sourced 15 billion parameter bilingual asymmetric seq2seq model. Many recent large language models have been proprietary or closed-source (e.g. GPT-3, PaLM, LaMDA), so OpenBA helps advance open-source LLMs.- OpenBA adopts an asymmetric encoder-decoder architecture, with a shallow encoder and deep decoder. This is different from other recent LLMs like GPT-3, PaLM, and LaMDA which use a decoder-only architecture. The motivation is to enhance generative capabilities compared to a symmetric architecture.- The training data uses a balanced mix of English and Chinese text, allowing OpenBA to develop strong bilingual capabilities. Many other large multilingual models skew heavily toward English data.- OpenBA is trained in multiple stages for pretraining, length adaptation, and task instruction tuning. Other models like GPT-3 are trained in a simpler single-stage process. The staged approach aims to optimize different objectives.- Efficiency is emphasized, with OpenBA achieving strong results after only 380B training tokens. This is far less than models like LLaMA (1.0T tokens) and GPT-3 (300T tokens). Advanced techniques like 3D parallelism help accelerate training.- Evaluation shows OpenBA achieves excellent performance on language understanding, generation, and reasoning tasks, competitive with or exceeding proprietary models with far more parameters. Releasing the model, code, and details will support research into these architectures and training methods.So in summary, OpenBA makes contributions in releasing an open-source bilingual LLM with an asymmetric encoder-decoder design, efficient staged training, and strong performance, helping advance research in this domain. The architectural and training innovations differentiate it from prior work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Conducting further evaluation to comprehensively calibrate the generation capability of OpenBA, especially for various tasks of controlled text generation and open-ended long text generation.- Improving the alignment of OpenBA through techniques like chain-of-thought prompting to reduce biases and toxicity. Testing effective detoxification strategies like detox-chain on the model.- Optimizing the conversational capabilities of OpenBA for dialogue use cases, such as improving generation correctness through techniques like grammatical error correction. - Enhancing the ability to invoke tools and enable multi-modal generation.- Extending the input and output lengths of OpenBA to adapt it to a wider range of tasks like dialogue generation.- Continuing to scale up OpenBA to even larger sizes for further performance improvements.- Testing variations of the asymmetric encoder-decoder architecture.- Applying OpenBA to real-world applications and documenting any challenges encountered.- Releasing more training details like hyperparameters to aid reproducibility.In summary, the main future directions focus on improving OpenBA's capabilities (e.g. generation, alignment), scaling it up further, optimizing it for conversational tasks, releasing more implementation details, and applying it to real-world problems. The authors aim to advance OpenBA as a strong open-sourced model for both research and practical usage.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper presents OpenBA, an open-sourced 15 billion parameter bilingual asymmetric sequence-to-sequence model pre-trained from scratch. The authors provide details on how they constructed the pre-training data from publicly available sources like Common Crawl and the Pile corpus. They also describe how they collected instructional data in Chinese and English to create a bilingual Flan dataset for fine-tuning. The model uses an asymmetric encoder-decoder architecture with a shallow encoder and deep decoder to enhance generative capabilities. It is trained in three stages: unsupervised pre-training with a mixture of denoising strategies, length adaptation, and instructional fine-tuning on the bilingual Flan data. Despite using only 380 billion training tokens, OpenBA achieves strong performance on benchmarks like SuperGLUE, MMLU, CMMLU, and BELEBELE, even surpassing some models trained on much more data. The authors share implementation details to allow replication and aim to contribute an open-sourced bilingual seq2seq model to supplement the LLMs currently available.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper presents OpenBA, an open-sourced 15 billion parameter bilingual asymmetric seq2seq model pre-trained from scratch. The authors provide details on how they constructed the pre-training data, which consists of a balanced mix of Chinese, English, and code tokens filtered from public sources like Common Crawl and the Pile corpus. They also introduce their Bilingual Flan dataset, compiled from existing Chinese NLP datasets and combined with the English Flan data. The paper describes the asymmetric encoder-decoder architecture of OpenBA, which uses a shallow encoder and deep decoder to enhance generative capabilities. The training process involves UL2 pre-training, length adaptation, and bilingual Flan tuning. Despite using only 380B tokens, OpenBA achieves strong performance on benchmarks like MMLU, CMMLU, C-Eval, and BELEBELE, even surpassing some models trained on much more data. The authors have open-sourced checkpoints, data processing details, and code to allow reproducibility. Overall, the paper makes valuable contributions towards open-source large language models, especially for the Chinese language.


## Summarize the main method used in the paper in one paragraph.

 The paper presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model pre-trained from scratch. The key aspects of their method include:1. Data Collection and Processing: They collect pre-training data from public sources like Common Crawl, C-Book, and Pile, with balanced Chinese and English text. For bilingual Flan data, they collect Chinese instruction data from competitions/papers and combine it with English Flan in a 1:2 ratio. The data is filtered for quality and privacy.2. Model Architecture: They use an asymmetric shallow encoder (12 layers) - deep decoder (36 layers) transformer architecture to enhance the generative capability. Techniques like sandwich layer normalization, rotary embedding, and mT5 tokenizer are incorporated. 3. Training Process: A 3-stage training strategy is adopted - UL2 pretraining, length adaptation, and bilingual Flan training. Different corruption strategies, context lengths, and batch sizes are used in each stage.4. Model Implementation: Optimization techniques like 3D parallelism, checkpoint activation, and distributed optimizer are leveraged to enable efficient large-scale training on multiple GPUs. In summary, the core innovations are the asymmetric model architecture, multi-stage training strategy combining generative pretraining with bilingual instruction tuning, and optimizations to enable efficient training and implementation. With only 380B tokens, OpenBA achieves strong performance across language understanding, reasoning and generation benchmarks.
