# [Aligning Step-by-Step Instructional Diagrams to Video Demonstrations](https://arxiv.org/abs/2303.13800)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we align in-the-wild web videos of furniture assembly with the respective diagrams in the instruction manuals?

Specifically, the paper introduces a new multimodal alignment task between:

(i) Instruction steps depicted as assembly diagrams in instruction manuals 

(ii) Video segments from real-world videos demonstrating the assembly process

The key challenges highlighted are:

- Instructional diagrams can be highly abstract compared to text/audio 

- Subtle differences between instruction steps 

- Assembly actions depicted may be unclear to machines

- No standard visual language followed in manuals

To address these challenges, the paper proposes a novel contrastive learning framework to align videos and diagrams using specialized losses. The effectiveness of the approach is evaluated on two tasks:

1) Nearest neighbor retrieval between video clips and diagrams

2) Aligning the instruction steps to video segments

The core hypothesis seems to be that the proposed losses will enable more effective video-diagram alignment on these tasks compared to alternatives. The paper introduces a new dataset called IAW to study this problem in a realistic setting.


## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of a novel method for aligning step-by-step instructional diagrams with in-the-wild web videos of furniture assembly. Specifically:

- They propose a supervised contrastive learning approach with three novel losses designed for the task of aligning abstract instructional diagrams with real-world videos. 

- They introduce a new dataset called Ikea Assembly in the Wild (IAW) containing 183 hours of assembly videos crawled from YouTube and nearly 8,300 instructional diagrams scraped from Ikea manuals. The videos and diagrams are annotated with ground truth alignments.

- They define two tasks on this dataset - nearest neighbor retrieval between videos and diagrams, and full alignment of diagrams to video clips.

- They demonstrate through experiments that their method outperforms compelling alternatives like CLIP on both retrieval and alignment on this challenging new dataset.

So in summary, the main contribution is a novel supervised contrastive learning method tailored for aligning abstract instructional diagrams with real videos, evaluated on a new challenging dataset collected specifically for this purpose. The introduced dataset itself to study this problem is also a contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new method and dataset for aligning real-world instructional videos of furniture assembly with diagrams from instruction manuals using contrastive learning and novel loss functions.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this CVPR 2022 paper compares to other related research:

- The problem of aligning instructional diagrams to real-world demonstration videos is quite novel. Most prior work on multimodal alignment focuses on text, audio, and visual data. Aligning abstract diagrams to complex video is an interesting and challenging new direction.

- The proposed dataset IAW is unique and fills an important gap. While there are some related assembly instruction datasets (e.g. IKEA ASM, IKEA-FA, LEGO), none focus on aligning in-the-wild videos to manuals. IAW pushes multimodal understanding into a realistic setting.

- The method builds on ideas like contrastive learning from CLIP, but introduces specialized losses tailored to this problem. The video-manual and intra-manual contrastive losses are creative extensions to handle the nuances of aligning steps within a manual to unlabeled video clips.

- The sinusoidal progress rate feature is a simple but clever idea to inject weak temporal information into the model without strict temporal assumptions. This seems to work better than alternatives like position encodings or dynamic time warping.

- The results demonstrate superior performance over intuitive baselines like cosine similarity and off-the-shelf CLIP. The gains from 9-12% appear meaningful for this challenging task.

- The paper explores an interesting direction connecting computer vision and instructional videos. The ideas could be extended to other instructional domains like recipes, LEGO building, etc. 

Overall, this paper introduces a novel problem setup and dataset, proposes tailored techniques to address the challenges, and shows promising results. It opens up an interesting research area at the intersection of vision, language, and robotics.
