# [Spatial-then-Temporal Self-Supervised Learning for Video Correspondence](https://arxiv.org/abs/2209.07778)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve synergy between spatial and temporal cues for learning effective video correspondence representations in a self-supervised manner. 

Specifically, the key research questions/hypotheses are:

- How can we leverage both spatially discriminative features and temporally repetitive features to learn robust video correspondence representations without manual annotations?

- Can we design a self-supervised pretext task that firstly learns spatial features and then enhances them by exploiting temporal cues? 

- How can we retain the learned spatial discriminative ability when adding a temporal objective in the second stage?

- How can we alleviate the problem of temporal discontinuity that harms the learning of temporal features?

To summarize, this paper proposes a novel spatial-then-temporal self-supervised learning approach to learn spatiotemporal features for video correspondence by combining the advantages of spatial and temporal feature learning. The key novelty is the two-step design with proposed distillation losses to achieve synergy between spatial and temporal cues.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a spatial-then-temporal self-supervised learning method for video correspondence. This involves first learning spatial features from unlabeled images via contrastive learning, and then enhancing the features by exploiting temporal cues from unlabeled videos via reconstructive learning.

2. Designing a global correlation distillation loss to retain the spatially discriminative features learned in the first step when exploiting temporal cues in the second step. 

3. Proposing a local correlation distillation loss to facilitate learning of temporal features at coarser pyramid levels by distilling knowledge from finer pyramid levels. This helps combat the temporal discontinuity that harms reconstruction.

4. Achieving state-of-the-art performance on multiple correspondence-based video analysis tasks like video object segmentation, human part propagation, and pose keypoint tracking. The method outperforms previous self-supervised methods and is comparable to some fully supervised task-specific algorithms.

5. Performing ablation studies to demonstrate the benefits of the proposed two-step design and the distillation losses.

In summary, the key novelty is in proposing a spatial-then-temporal self-supervised learning framework along with distillation losses to achieve synergistic spatial-temporal feature learning from unlabeled images and videos for video correspondence.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a spatial-then-temporal self-supervised learning method that first extracts spatial features from images via contrastive learning, then enhances those features by exploiting temporal video cues via reconstructive learning, using novel global and local correlation distillation losses to retain spatial discriminability while combating temporal discontinuities.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on self-supervised learning for video correspondence:

- It proposes a two-step training process, first learning spatial features from images then temporal features from videos. Most prior work has focused on learning either spatial or temporal features, not both. The two-step approach allows the model to learn robust spatial features before fine-tuning on temporal patterns.

- It uses both a global and local distillation loss when fine-tuning the model on videos in the second step. The global loss retains discriminative spatial features, while the local loss combats temporal discontinuities. This distillation strategy is novel. 

- Experiments show the method achieves state-of-the-art results on several correspondence tasks compared to other self-supervised approaches. It also approaches the performance of some fully supervised methods.

- The model is trained on a smaller dataset (ImageNet + YouTube-VOS) than some prior work like CRW and CLSC that use large video datasets like Kinetics. Yet it still achieves better performance, demonstrating the effectiveness of the training procedure.

- Unlike some two-stream approaches like SFC that use separate models for spatial and temporal features, this method learns both within a single model. This is more efficient.

Overall, the key novelties are the two-step training strategy, use of global and local distillation losses, strong performance compared to prior self-supervised methods, and learning spatiotemporal features in a single model. The experiments demonstrate the value of achieving synergy between spatial and temporal cues for video correspondence.
