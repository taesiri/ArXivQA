# [GTC: GNN-Transformer Co-contrastive Learning for Self-supervised   Heterogeneous Graph Representation](https://arxiv.org/abs/2403.15520)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) suffer from over-smoothing problem which hinders them from going deeper to capture multi-hop neighbors and global graph structure. 
- Existing methods try to alleviate over-smoothing by modifying GNN architectures or adding constraints, but they still rely on GNNs and cannot fundamentally avoid the problem.
- Transformers can effectively model global structure and show immunity to over-smoothing, but cannot aggregate local neighbor information as good as GNNs.

Proposed Solution:
- Propose a GNN-Transformer collaborative learning scheme where GNN and Transformer encode local and global views respectively.
- Design a GTC architecture with two branches:
  - GNN branch encodes graph schema view 
  - Transformer branch encodes hops view via proposed Metapath-aware Hop2Token and CG-Hetphormer
- Establish cross-view contrastive learning between two views to enable self-supervised representation learning.

Main Contributions:
- First work to leverage both GNN and Transformer for collaborative contrastive learning in graph representation learning.
- Propose efficient Metapath-aware Hop2Token to transform heterogeneous graph structure into tokens.
- Design CG-Hetphormer to fuse token-level and semantic-level information in Transformer branch.  
- Construct cross-view contrastive learning between GNN and Transformer branches for self-supervised heterogeneous graph representation.
- Experiments show superiority over state-of-the-arts and ability to capture multi-hop neighbors without over-smoothing.

In summary, the paper innovatively establishes collaborative learning between GNN and Transformer to effectively integrate their complementary strengths in local and global modeling while overcoming their limitations. The proposed cross-view contrastive learning scheme enables superior performance.
