# [Learning to Generate Image Embeddings with User-level Differential   Privacy](https://arxiv.org/abs/2211.10844)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we train large image embedding models with strong user-level differential privacy guarantees?

More specifically, the authors aim to develop a user-level differentially private algorithm that can effectively train large deep neural networks to generate image embeddings, while achieving strong privacy guarantees and maintaining high utility. 

The key challenges they identify are:

- Standard DP-SGD methods fail when naively applied to train large embedding models, as the softmax classification layer grows linearly in size with the number of training user identities/classes. Adding noise to the full model results in very poor signal-to-noise ratio.

- Prior DP methods have only been applied to small models with a few million parameters. Scaling embedding models up presents challenges in terms of the privacy-utility tradeoff. 

To address these issues, the authors propose DP-FedEmb, a federated learning-based approach that:

- Uses virtual clients and local fine-tuning to get useful updates for the shared backbone network while avoiding noisy updates to the large softmax head.

- Aggregates updates across virtual clients and adds noise only to the backbone updates to ensure privacy.

- Leverages public pretraining of the backbone on unlabeled data before fine-tuning on private data.

The central hypothesis is that this approach can effectively train large embedding models (e.g. ResNet-50) with strong user-level DP guarantees by privatizing only the compact backbone network. The experiments aim to validate the superior privacy-utility tradeoff of DP-FedEmb compared to baselines.
