# [Masked Autoencoders Enable Efficient Knowledge Distillers](https://arxiv.org/abs/2208.12256)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively transfer the knowledge from large pre-trained Vision Transformer (ViT) models to smaller ViT models via knowledge distillation. Specifically, the authors propose a novel distillation method called DMAE that distills knowledge from Masked Autoencoder (MAE) pre-trained models rather than from models fine-tuned on downstream tasks. The key hypothesis is that distilling from MAE pre-trained models can lead to better student models compared to traditional distillation from fine-tuned models.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new knowledge distillation method called DMAE (Distilling Masked Autoencoders) to efficiently distill knowledge from large pre-trained masked autoencoder models into smaller student models. Specifically:- They propose to perform knowledge distillation during the self-supervised pre-training stage of MAE, rather than distilling from a supervised fine-tuned model. This allows them to leverage the rich feature representations learned during pre-training.- They align the intermediate feature representations between the teacher and student model, rather than aligning the final logits. This enables them to reduce the computation cost of the teacher model during training. - They show the method works well even with very high masking ratios during pre-training (e.g. 95%), indicating the student can learn effectively from very limited visible patches. - Experiments show their method outperforms baselines of standard MAE pre-training, distillation from supervised models, and other distillation techniques like logit matching. It achieves improved accuracy with similar or lower training cost.In summary, the key novelty is performing knowledge distillation during self-supervised pre-training in a computationally efficient way to transfer knowledge from large masked autoencoders to smaller models. This provides an effective way to balance accuracy and efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a knowledge distillation method called DMAE that aligns intermediate feature representations between a large pre-trained teacher model and a smaller student model during masked autoencoder pre-training, achieving improved performance over baseline distillation techniques.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research on knowledge distillation:- This paper focuses specifically on distilling knowledge from masked autoencoder (MAE) models, which have recently shown strong performance for self-supervised pre-training. Most prior work on knowledge distillation uses supervised trained models. Distilling self-supervised models is a relatively new direction.- The distillation approach is to align intermediate layer features between the teacher MAE and student model during pre-training. This differs from common practices of aligning softmax outputs or attention maps from supervised models. - They show distilling from a self-supervised pre-trained model outperforms distilling from a supervised fine-tuned model. This suggests pre-trained models contain useful generalizable knowledge beyond just task-specific fine-tuning.- The method allows using very high masking ratios during distillation (e.g. 95%), much higher than normal MAE pre-training. This indicates distillation provides extra guidance to learn from limited visible patches.- They demonstrate consistent improvements by distilling larger MAE models into smaller ones across varying architectures (ViT-B, ViT-S etc). The approach seems widely applicable.- The distillation framework has low computational overhead compared to training MAE models from scratch. It is a lightweight way to transfer knowledge.Overall, this paper provides good evidence that distilling self-supervised models with feature alignment is an efficient and effective approach. The high masking ratios are an interesting finding. The method seems flexible and wideely applicable across model sizes and architectures.
