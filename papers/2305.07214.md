# [MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition](https://arxiv.org/abs/2305.07214)

## What is the central research question or hypothesis that this paper addresses?

The key research focus of this paper is studying the generalization ability of multimodal action recognition models in two novel scenarios: 1) Missing modality generalization: Models are trained on multiple modalities (video, audio, IMU) but tested on only a subset of those modalities. This tests how well models can generalize when some modalities are missing during inference.2) Cross-modal zero-shot generalization: The training and testing modalities are completely disjoint. For example, models may be trained only on audio and IMU but tested on video. This is an even harder setting that tests whether knowledge can transfer across different modalities.To facilitate research on these new problems, the paper introduces a new benchmark called MMG-Ego4D based on the Ego4D dataset. It contains thoroughly annotated multimodal egocentric video clips to enable studying missing modality and cross-modal generalization. The central hypothesis is that current multimodal models have limited generalization ability in these practical scenarios with mismatch between train and test modalities. The paper proposes and evaluates methods to enhance multimodal generalization, including a transformer-based fusion module, cross-modal contrastive training, and a new cross-modal prototypical loss. The goal is to build systems that can robustly handle missing or unseen modalities like humans.In summary, the key research focus is benchmarking and improving the generalization ability of multimodal action recognition models, especially when train and test modalities differ. This is an important and practical problem for real-world deployment.
