# [Tri-Modal Motion Retrieval by Learning a Joint Embedding Space](https://arxiv.org/abs/2403.00691)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Motion retrieval is valuable for obtaining high-quality and diverse human motions, but relies heavily on large datasets. However, existing motion datasets are limited.  
- Prior works in motion retrieval mainly focus on dual-modality learning (e.g. text-to-motion), but aligning text and motion modalities is challenging due to their vast differences.

Proposed Solution:
- Introduce a novel tri-modal learning framework called LAVIMO that aligns language, video, and motion in a joint embedding space.  
- Leverage video as an intermediary modality between text and motion to enhance their alignment. Videos are more compact representations of motions compared to text.
- Implement a custom attention mechanism for effective fusion of information from the three modalities when reconstructing the motion. This helps address gaps in existing methods that overlook valuable contextual details from the other modalities.

Main Contributions:
- First framework to utilize video as an additional modality for improving text-motion alignment and enabling both text-to-motion and video-to-motion retrieval
- Specially designed attention mechanism to merge information from text, video and motion modalities for better reconstruction and alignment
- State-of-the-art performance on HumanML3D and KIT-ML datasets for text-motion retrieval
- Introduction of video-motion retrieval task and demonstration of model's ability to generalize to real-world videos

In summary, the paper introduces a novel tri-modal learning approach for motion retrieval that leverages videos to bridge the gap between text and motions. A custom attention mechanism further aligns the modalities in a joint space for achieving enhanced cross-modal retrieval performance.
