# [DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions](https://arxiv.org/abs/2403.12202)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Depth completion aims to generate a dense and accurate depth map from a sparse depth map and aligned RGB image. While there has been progress with convolutional networks, most methods focus on 2D feature learning and processing. They fail to effectively exploit 3D geometric information. Recent works attempt to utilize 3D representations but are limited by the extreme sparsity or insufficient accuracy of initial depth maps.

Method:
The paper proposes DeCoTR, a novel transformer-based approach, to perform feature learning in full 3D for depth completion. It consists of two main components:

1) S2D-TR: Enhances the commonly used S2D network by applying efficient attention to its 2D features. This significantly boosts initial depth accuracy to be on par with state-of-the-art.

2) 3D-TR: Leverages the more accurate depth map from S2D-TR to uplift 2D features to a dense 3D point cloud. Applies transformer layers to enable powerful 3D feature learning through neighborhood-based cross attention. Further incorporates efficient global attention on lower-scale point cloud to capture long-range context.

Contributions:
- Presents the first work to perform comprehensive feature learning in 3D for depth completion using transformers, achieving new state-of-the-art accuracy.
- Proposes to enhance initial 2D network with efficient attention to provide better depth maps for subsequent operations.  
- Devises normalization and global attention techniques tailored for 3D point cloud to enable effective 3D feature learning.
- Demonstrates superior performance on NYU Depth v2 and KITTI datasets. Also shows better generalization capability on ScanNet and DDAD through zero-shot evaluation.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel depth completion method, DeCoTR, that utilizes efficient 2D attention to enhance a convolutional baseline network and powerful 3D cross-attention on an uplifted point cloud to enable geometry-aware feature learning, achieving state-of-the-art performance without needing iterative spatial propagations.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. It presents DeCoTR, a novel transformer-based approach to perform full 3D feature learning for depth completion. This enables high-quality depth estimation without requiring iterative processing steps.

2. It upgrades the commonly used S2D network by enhancing its bottleneck and skip connection features using transformers. The resulting S2D-TR model performs on par with state-of-the-art while providing more accurate initial depths. 

3. It devises useful techniques to normalize the uplifted 3D feature point cloud from 2D guidance features, which improves the model learning. It also applies efficient global attention to the 3D points to capture long-range context.

4. Through extensive experiments, it demonstrates state-of-the-art performance of DeCoTR on standard benchmarks like NYU Depth v2 and KITTI-DC. Furthermore, zero-shot evaluation shows DeCoTR has better generalizability compared to existing methods.

In summary, the main contribution is the proposal of DeCoTR, a novel transformer-based model that performs full 3D feature learning to achieve state-of-the-art depth completion accuracy and generalization ability.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, here are some of the key terms and keywords associated with it:

- Depth completion - The main task that the paper focuses on, aimed at generating dense and accurate depth maps from sparse measurements.

- Image guidance - Using paired RGB images to provide semantic guidance for depth completion. 

- Transformers - The paper leverages transformer architecture and attention mechanisms for feature learning.

- 2D attention - Efficient attention mechanisms applied to 2D image features to enhance the baseline network. 

- 3D attention - Cross-attention layers operating on a 3D point cloud representation to enable geometry-aware feature processing.

- Point cloud - A 3D representation formed by uplifting 2D features using the initial predicted depths. Used to apply 3D attention.

- Normalization - Techniques proposed to normalize the 3D point cloud representation before transformer layers. 

- Global attention - Additional long-range attention mechanism introduced on downsampled point cloud features.

- Generalizability - Zero-shot cross-dataset evaluation performed to analyze model generalization capability.

- State-of-the-art - The method achieves new state-of-the-art accuracy on depth completion benchmarks like NYUD-v2 and KITTI-DC.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes to first enhance the commonly used S2D network by integrating transformer layers on bottleneck and skip connection features. What is the motivation behind upgrading S2D in this manner before applying 3D feature learning? How much performance gain does S2D-TR provide over the original S2D?

2. The paper constructs a 3D point cloud by unprojecting 2D guidance features from S2D-TR using the predicted depth values. What are the advantages of using features from S2D-TR rather than directly using the sparse input depth? How does this lead to more effective 3D feature learning?

3. Neighborhood-based cross attention is applied on the 3D points. What modifications were made compared to the original point transformer architecture? Why is cross attention useful here and how does it help capture geometric relationships?  

4. The paper proposes a point cloud normalization scheme before feeding points into the 3D transformer layers. Why is this useful? How does normalizing the points improve depth completion performance?

5. For computational efficiency, global attention is only applied to downsampled point cloud features. What is the motivation behind this? Approximately how much computation is saved by using lower-resolution global attention?

6. How many 3D transformer encoder layers are used in the model? Is there value in going deeper with more layers or does performance saturate? 

7. The predicted depth from S2D-TR is good enough to not require iterative spatial propagations as done in many other papers. Why do you think this is the case? 

8. How does the performance of DeCoTR compare with other methods that also leverage 3D representations and/or transformers, such as GraphCSPN, PointDC, and CompletionFormer? What enables DeCoTR to outperform them?

9. Why do you think DeCoTR generalizes better to unseen datasets compared to existing state-of-the-art methods, as evidenced by the zero-shot evaluation results?

10. If computation/memory is extremely limited, what modifications can be made to DeCoTR while still preserving most of its accuracy gains? Which components are most expensive that can be first removed or substituted?
