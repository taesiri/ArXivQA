# [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to rigorously define and evaluate interpretability in machine learning systems. The key points are:- Interpretability refers to the ability of a machine learning system to explain its reasoning or predictions in understandable terms to a human. It is often used to verify other desiderata like fairness, privacy, causality, etc. when those are not fully formalized.- Interpretability is needed when there is an "incompleteness" in the problem formulation that prevents full specification and optimization. This creates a need for explanation to reveal potential issues. - The paper proposes a taxonomy for evaluating interpretability: 1) Application-grounded evaluation: Test with real users on real tasks 2) Human-grounded evaluation: Simplified experiments with humans to test general notions of interpretability3) Functionally-grounded: No humans, use proxies like sparsity as criteria- There are open problems in linking these types of evaluation and discovering the key factors that make tasks and methods more or less interpretable. The paper suggests some hypotheses about these factors.- Researchers should match claims to evaluation type, categorize applications/methods on shared taxonomy, and collectively build repositories of tasks needing interpretability.In summary, the key research question is how to rigorously define and evaluate interpretability in ML using a taxonomy of evaluation approaches.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for rigorously defining and evaluating interpretability in machine learning. The key points are:- Interpretability is defined as the ability to explain or present something in understandable terms to a human. It is needed when there is some incompleteness in the problem formulation that creates a barrier to optimization and evaluation.- A taxonomy is presented for evaluating interpretability:    - Application-grounded: Evaluating in the context of a real-world application with domain experts    - Human-grounded: Simplified experiments with lay humans to test general notions of explanation quality     - Functionally-grounded: Using proxies for explanation quality without human experiments- Several open problems are discussed around determining what evaluation proxies link to real applications, designing simplified human experiments that maintain the essence of real tasks, and characterizing good proxies for explanation quality. - Recommendations are made for researchers such as matching claims to evaluation types, using a common taxonomy to characterize applications and methods, and being careful about the costs of human evaluation.Overall, the paper lays out a research agenda for making the notions of interpretability and explanation more concrete and rigorous through thoughtful evaluation grounded in applications, human studies, and formal proxies.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a taxonomy and framework for rigorously evaluating interpretability in machine learning, arguing it is needed to address incomplete problem formalizations and should be grounded in applications, human studies, or functional proxies.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on interpretability in machine learning:- It provides a conceptual analysis of what interpretability is and why it matters. The paper argues interpretability is important when there is an "incompleteness" in the problem formulation that prevents full specification and optimization. This philosophical framing is unique compared to other more application-focused papers.- It proposes a taxonomy for evaluating interpretability, categorizing approaches as application-grounded, human-grounded, or functionally-grounded. This structured framework for thinking about evaluation seems novel compared to prior work. - The paper advocates creating shared tasks and repositories for interpretability evaluation, similar to existing resources for classification and clustering problems. This concrete suggestion could lead to more standardized benchmarks.- It offers hypotheses about latent factors that may determine interpretability needs, like time constraints and user expertise. Testing these hypotheses could lead to new insights about what makes a "good" explanation.- Overall, the paper takes a more rigorous, formal approach compared to prior work that often relies on intuition about interpretability. The focus on evaluation frameworks and quantifiable proxies attempts to make interpretability more scientific.In summary, this paper pushes the field forward by providing conceptual clarity, proposing structured evaluation frameworks, and advocating data-driven discovery of what makes explanations useful. This rigorous perspective stands out from prior interpretability research.


## What future research directions do the authors suggest?

The authors suggest several future research directions related to developing a rigorous science of interpretability in machine learning:1. Creating repositories of real-world tasks and cases that require human input/assessment, to allow for application-grounded evaluation of interpretability methods. These repositories could be used to discover latent factors that characterize tasks and methods along interpretability dimensions.2. Conducting human-grounded experiments to test hypotheses about task-related factors like time constraints, nature of user expertise, etc. that may impact interpretability needs. 3. Conducting human-grounded experiments to test hypotheses about method-related factors like form/number of cognitive chunks, compositionality, etc. that may impact interpretability.4. Using taxonomies and common terminology to characterize applications and methods along interpretability dimensions like the incompleteness motivating the need for interpretability, the evaluation type, relevant task factors, and relevant method factors. This can enable better comparison of related work.5. Matching the claims of research contributions to the appropriate type of evaluation (application-grounded, human-grounded, or functionally-grounded).In summary, the authors advocate for more rigorous, evidence-based, and standardized approaches to defining, evaluating and comparing interpretability methods in ML. This includes creating data repositories, conducting controlled experiments, developing taxonomies, and aligning claims to appropriate evaluation techniques.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper argues that there is a lack of consensus on how to define and evaluate interpretability in machine learning systems. It proposes that interpretability refers to the ability to explain or present machine learning models in understandable terms to a human. Interpretability is important when there is some incompleteness in the problem formulation that makes it hard to fully specify the desired behavior. The authors present a taxonomy for evaluating interpretability: application-grounded evaluation involves real humans performing real tasks, human-grounded evaluation uses simplified tasks with regular people, and functionally-grounded evaluation relies on formal proxies for explanation quality without human involvement. The paper discusses open problems in determining good proxies and dimensions for characterizing interpretability. It advocates creating shared repositories of real-world tasks requiring human input to help discover these factors. The authors encourage matching claims to appropriate evaluation, using a common taxonomy to describe applications and methods, and refining factors of interpretability.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper argues that machine learning systems increasingly require interpretability, which is defined as the ability to explain or present something in understandable terms to a human. Interpretability is needed when there is some incompleteness in the problem formulation that creates barriers to optimization and evaluation. For example, interpretability may be needed for issues like safety, ethics, scientific understanding, and other objectives that can't be fully formalized. The paper proposes a taxonomy for evaluating interpretability with three types: application-grounded evaluation in real tasks with real humans, human-grounded evaluation with simplified tasks and lay humans, and functionally-grounded evaluation using proxies with no humans. It also discusses open problems in linking these evaluations, like determining what proxies work for real applications, designing simplified tasks that maintain real-world essence, and choosing good proxies for explanation quality. The paper concludes with suggestions like matching claims to evaluation type, categorizing applications and methods by common factors, and building shared repositories of real-world tasks needing human input.
