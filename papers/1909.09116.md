# [Self-Training for End-to-End Speech Recognition](https://arxiv.org/abs/1909.09116)

## What is the central research question or hypothesis that this paper addresses?

The central research question of this paper is whether self-training can be an effective semi-supervised learning approach for end-to-end speech recognition models. Specifically, the authors investigate whether training sequence-to-sequence models on pseudo-labels generated from a baseline model can improve accuracy compared to the baseline, and how factors like filtering mechanisms and model ensembling impact the effectiveness of self-training. The main hypothesis is that self-training with proper pseudo-label filtering and model ensembling can substantially improve the accuracy of end-to-end speech recognition models by taking advantage of unlabelled data.


## What is the main contribution of this paper?

The main contribution of this paper is developing an effective semi-supervised learning method for end-to-end speech recognition using self-training with pseudo-labels. The key ideas include:- Using a strong baseline acoustic and language model to generate high-quality pseudo-labels from unlabelled data. The baseline models are trained on a small set of labelled data.- Designing heuristic filtering techniques tailored to common mistakes of sequence-to-sequence models like looping and early stopping. This removes noisy pseudo-labels. - Proposing an ensemble method during training to improve pseudo-label diversity and model robustness.- Demonstrating significant improvements in word error rate from self-training, achieving high word error rate recovery compared to previous semi-supervised approaches on the LibriSpeech corpus.The main novelty is showing that with proper filtering and ensembling, a simple self-training approach can effectively leverage unlabelled data to improve end-to-end speech recognition. The techniques help balance the amount and quality of pseudo-labels. The results set a strong benchmark for semi-supervised learning methods on the LibriSpeech dataset.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes using self-training with pseudo-labels generated from a strong baseline model and large unlabeled datasets to improve end-to-end speech recognition. The key findings are that proper filtering of noisy pseudo-labels and using model ensembles can yield substantial WER reductions compared to the baseline model.
