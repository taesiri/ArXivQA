# [Paint by Example: Exemplar-based Image Editing with Diffusion Models](https://arxiv.org/abs/2211.13227)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question/hypothesis of this paper is:

How can we perform precise and controllable image editing by using an exemplar image as guidance, rather than text prompts? 

The key ideas and contributions are:

- Proposes a new exemplar-based image editing approach, where an exemplar image provided by the user is transformed and blended into the source image for semantic manipulation. This allows more precise control compared to text-guided editing.

- Trains an image-conditioned diffusion model in a self-supervised manner, where the exemplar is a cropped region from the source image. Identifies and handles the issue of the model learning to naively copy-paste the exemplar. 

- Introduces several techniques - leveraging image prior, strong augmentation, information bottleneck, and classifier-free guidance - to avoid the copy-paste issue and improve image quality and controllability.

- Achieves high-quality and controllable semantic image editing results on complex images, without needing image-specific fine-tuning. Outperforms prior arts quantitatively and qualitatively.

In summary, the main hypothesis is that exemplar-based conditioning can enable more precise image editing than text guidance. The paper proposes techniques to train such an exemplar-based diffusion model in a self-supervised setting, while preventing trivial copy-paste solutions.
