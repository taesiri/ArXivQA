# [A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition](https://arxiv.org/abs/2403.05583)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Silent speech interfaces (SSIs) offer a non-invasive alternative to brain-computer interfaces for soundless verbal communication. However, they face challenges due to the absence of phonetic information and limited training data, making it difficult to achieve error rates suitable for practical use. 

- The performance threshold for SSIs to become a viable alternative to existing automatic speech recognition (ASR) systems is approximately 15% word error rate (WER). Despite advances, improving the accuracy and applicability of SSIs to reach this level remains a key challenge.

Proposed Solution:
- The paper introduces Multimodal Orofacial Neural Audio (MONA), a novel cross-modal training approach using facial electromyography (EMG) and audio data. 

- Two new loss functions are proposed - cross-contrastive (crossCon) and supervised temporal contrastive (supTcon) learning. These align latent spaces and facilitate training on additional audio-only datasets like LibriSpeech.

- Dynamic time warping enables alignment between silent and vocalized utterances. 

- A technique called LLM Integrated Scoring Adjustment (LISA) is introduced that uses large language models like GPT-3.5 to correct beam search output and significantly improve accuracy.

Main Contributions:
- The crossCon and supTcon losses combined with LibriSpeech training data lead to improved silent speech recognition over baseline models.

- LISA reduces the state-of-the-art word error rate on a benchmark dataset from 28.8% to 12.2% on silent EMG, achieving under 15% WER threshold required for viability.

- On vocal EMG, the error rate is reduced from 23.3% to 3.7%, substantially closing the gap with traditional ASR.

- Winning entry in Brain-to-Text competition, reducing top error rate from 9.8% to 8.9%, demonstrating applicability to brain data.

In summary, the paper makes significant advances in SSI accuracy through innovative cross-modal training and integration of language models, demonstrating potential as a practical speech alternative.
