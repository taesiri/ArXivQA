# [DETRs with Collaborative Hybrid Assignments Training](https://arxiv.org/abs/2211.12860)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can we improve the training efficiency and performance of DETR-based object detectors, while maintaining their end-to-end detection capability?

The key ideas proposed to address this are:

1. Using a collaborative hybrid assignment training scheme involving auxiliary prediction heads with one-to-many label assignments (e.g. ATSS, Faster R-CNN) to enhance encoder feature learning. 

2. Generating customized positive queries in the decoder using coordinates of positive samples from the auxiliary heads, to improve cross-attention learning.

The overall goal is to enhance both the encoder and decoder components of DETR frameworks through versatile label assignment strategies, without losing the end-to-end merits or adding overhead during inference. Experiments show clear improvements in convergence speed and accuracy over baselines.

In summary, the core research question is how to improve DETR-based detectors using collaborative training schemes, while retaining their end-to-end detection capability. The proposed method involving hybrid label assignments provides a way to achieve this.


## What is the main contribution of this paper?

 This paper proposes a novel collaborative hybrid assignments training scheme (Co-DETR) to improve the training efficiency and effectiveness of DETR-based object detectors. The key contributions are:

1. They incorporate auxiliary heads with one-to-many label assignment paradigms like ATSS and Faster R-CNN after the encoder. The dense supervisions from these heads force the encoder to learn more discriminative features. 

2. They generate extra customized positive queries by extracting the foreground coordinates from the outputs of these auxiliary heads. These queries are fed to the decoder to provide more positive samples and improve its attention learning.

3. Extensive experiments show Co-DETR improves various DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. It achieves new state-of-the-art detection performance on COCO and LVIS datasets when combined with a ViT-L backbone.

4. Co-DETR reveals the complementarity between conventional one-to-many assignment detectors and end-to-end one-to-one set matching detectors. It is a simple yet effective approach to enhance the learning of DETR-based detectors.

In summary, the core idea is to leverage one-to-many label assignments to provide dense supervisions for the encoder and generate more positive queries for the decoder, thus improving both components in DETR's transformer architecture. The experiments demonstrate the efficiency and effectiveness of this collaborative training scheme.
