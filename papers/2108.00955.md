# [Evaluating Deep Graph Neural Networks](https://arxiv.org/abs/2108.00955)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions seem to be:

1) What actually limits the stacking of deep convolution operations in GNN designs, and why does disentangling embedding propagation (EP) and embedding transformation (ET) allow more EP operations?

2) What are the insights and guidelines for researchers to design deep GNNs? In particular, when and how to enlarge the number of EP and ET operations? 

3) With the proposed insights and guidelines, can the authors stack more EP and ET operations and outperform state-of-the-art GNNs?

The key hypothesis appears to be that disentangling and independently controlling EP and ET operations will allow construction of deeper and better-performing GNN models compared to prior "entangled" designs. 

The authors conduct experiments to evaluate the effects of EP and ET depth separately, and find that increasing EP depth leads to over-smoothing while increasing ET depth leads to model degradation. They propose design guidelines based on these findings - use larger EP depth for sparse graphs and larger ET depth for large graphs. Their model DGMLP implements these insights allowing flexible ET and EP depths. Experiments show DGMLP achieves state-of-the-art performance, supporting the hypothesis.

In summary, the central questions revolve around understanding limitations of GNN depth, providing insights to address this, and demonstrating improved performance from a model implementing these insights.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It conducts an experimental evaluation to identify the root causes of the shallow architecture limitation of most existing graph neural networks (GNNs). Through comprehensive experiments, the paper finds that both over-smoothing introduced by large propagation depth and model degradation introduced by large transformation depth limit the performance of deep GNNs. 

2. It provides insights and guidelines on when and how to construct deep GNN architectures based on the analysis. Specifically, it suggests using larger propagation depth for sparse graphs, and larger transformation depth for large graphs. It also proposes techniques like node-adaptive combination and residual connections to allow deeper architectures.

3. It proposes a novel GNN model called Deep Graph Multi-Layer Perceptron (DGMLP) that can support both large propagation and transformation depths based on the insights. Experiments show DGMLP achieves state-of-the-art performance while maintaining high efficiency and scalability on several benchmark datasets.

In summary, the key contribution is providing an experimental analysis of the limitations of deep GNNs, insights on constructing deep GNNs, and proposing a novel model DGMLP that outperforms previous state-of-the-art with deeper architectures. The analysis and insights could open up new perspectives for designing better-performed deep GNN models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper provides an experimental evaluation of deep graph neural networks, identifies the causes of performance degradation in deep models, and proposes design guidelines and a new model called Deep Graph Multi-Layer Perceptron (DGMLP) to enable deeper and more flexible architectures.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research on evaluating deep graph neural networks:

- It clearly separates the concepts of propagation depth and transformation depth, and analyzes their effects separately. Many other papers conflate the two under just "depth". 

- It systematically evaluates potential causes of performance degradation in deep GNNs - oversmoothing, overfitting, entanglement, etc. - through controlled experiments. Other works tend to focus on one particular hypothesis.

- The paper proposes guidelines for when and how to build deep GNN architectures based on graph properties like sparsity and size. Most other work lacks this level of practical insight.

- The proposed model DGMLP incorporates techniques like node-adaptive weighting and residual connections to enable deeper propagation and transformation. This showcases the value of the guidelines.

- Experiments are quite comprehensive, evaluating DGMLP on a range of datasets. Many papers focus on just one or two standard benchmarks.

- The paper frames convolution on graphs as a mining operation, studying how to stack such operations. This viewpoint relates GNNs to data mining. 

Overall, the systematic and thorough evaluation of deep GNN architectures makes this paper stand out. The proposed guidelines and model aim to open up new perspectives on designing well-performed GNNs.


## What future research directions do the authors suggest?

 The paper "Evaluating Deep Graph Neural Networks" suggests several promising future research directions:

1. Explore more disentangled deep graph models with different propagation and transformation patterns: The authors propose separating the propagation and transformation operations in GNNs, allowing more flexible and deeper architectures. They suggest exploring other disentangled architectures and patterns beyond their proposed DGMLP model. 

2. Improve efficiency and scalability of deep graph models: The paper shows DGMLP has higher efficiency and scalability than previous models. However, training extremely deep and large graph models remains challenging. Further improving efficiency, parallelization, and distributed training is an important direction.

3. Apply deep graph models to more applications: The advantages of deep graph models have been shown on node classification tasks. Testing these models on graph-level prediction tasks like graph classification and graph generation is an interesting direction. Applying them to emerging tasks like explainable recommendation is also promising.

4. Theoretical analysis of model depth: While the paper provides useful empirical insights, theoretical analysis of how model depth affects over-smoothing and model degradation is still lacking. Formalizing the trade-offs can help guide architecture designs.

5. Pre-training techniques: Pre-trained models like GPT have been very successful in NLP. Developing pre-training techniques to learn rich graph representations is a promising direction to boost deep graph model performance.

In summary, the paper provides a solid experimental analysis of deep graph models and points out several interesting future directions to build on this work, including exploring model architectures, improving efficiency, applying models to downstream tasks, theoretical analysis, and pre-training.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents an experimental evaluation of deep graph neural networks (GNNs) to understand the limitations of GNN depth and provide insights on designing effective deep GNN architectures. The key findings are: 1) GNNs have two depths - embedding propagation (EP) depth and embedding transformation (ET) depth. Large EP depth causes over-smoothing while large ET depth leads to model degradation. Model degradation is more significant than over-smoothing. 2) Sparse graphs need large EP depth while large graphs need large ET depth. 3) Entangled GNNs like GCN fail in depth due to model degradation. Disentangled GNNs avoid this by increasing only EP depth. 4) The paper proposes DGMLP, a flexible deep GNN with large EP and ET depths. It uses node-adaptive propagation and residual connections in ET to address over-smoothing and model degradation. Experiments show DGMLP achieves state-of-the-art performance while maintaining efficiency and scalability on seven datasets. The key contribution is identifying model degradation as the primary issue in deep GNNs and providing insights on designing deep GNN architectures.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents an experimental evaluation to analyze the limitations of deep graph neural networks (GNNs). Most GNNs have shallow architectures with only 2-3 layers, which limits their ability to fully exploit structural information in graphs. The paper investigates three commonly cited reasons for why GNNs cannot go deeper: over-smoothing, over-fitting, and entanglement between propagation and transformation steps. Through extensive experiments, the authors find that over-smoothing and model degradation are the main factors limiting GNN depth, with model degradation being more significant in most cases. 

Based on these findings, the paper proposes guidelines for constructing deep GNNs: use more propagation steps on sparse graphs to expand node receptive fields, and more transformation steps on large graphs to extract complex features. It also introduces a novel GNN called Deep Graph Multi-Layer Perceptron (DGMLP) which adapts the number of propagation and transformation steps for each node using a node-adaptive combination mechanism and residual connections. Experiments show DGMLP achieves state-of-the-art performance while maintaining efficiency and scalability on multiple benchmark datasets. The insights from this experimental study provide a basis for developing better-performing deep GNN architectures.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel deep graph neural network architecture called Deep Graph Multi-Layer Perceptron (DGMLP). DGMLP aims to address the limitations of prior graph neural networks in being able to stack multiple layers without experiencing performance degradation. The key innovations in DGMLP are: 1) It disentangles the embedding propagation (EP) and embedding transformation (ET) operations typically coupled in graph convolution layers. This allows flexibility in setting different numbers of EP and ET layers. 2) It introduces a node-adaptive combination mechanism for EP that assigns personalized weights for each node when combining features from different propagation steps. This helps control over-smoothing. 3) It adds residual connections between ET layers to alleviate model degradation issues that arise with stacking multiple ET operations. Through these innovations, DGMLP is able to achieve state-of-the-art performance by supporting deeper architectures on both EP and ET compared to prior methods. Experiments on several citation networks and large graphs demonstrate DGMLP's accuracy, flexibility, scalability and efficiency.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- Graph neural networks (GNNs) have limitations in terms of model depth, where stacking multiple layers often leads to degraded performance. The paper aims to understand the root causes of this issue.

- The paper conducts a systematic experimental evaluation to analyze the performance of various GNN architectures as model depth increases. 

- It identifies two main factors that limit GNN depth:
    - Over-smoothing: Stacking many embedding propagation (EP) operations leads to over-smoothed node representations.
    - Model degradation: Stacking many embedding transformation (ET) operations leads to model degradation.

- The paper finds that model degradation has a bigger impact than over-smoothing in limiting GNN depth in most cases. 

- It provides insights on when deeper architectures are needed (e.g. for sparse graphs) and how to construct deeper GNNs (e.g. using disentangled EP and ET).

- Based on these analyses, the paper proposes a Deep Graph Multi-Layer Perceptron (DGMLP) model that can flexibly adjust EP and ET depth and achieves state-of-the-art performance.

In summary, the key contribution is a comprehensive experimental analysis to understand and address the limitations of GNN model depth through proposed methods like DGMLP. The insights from this analysis could help guide designs of deeper GNN architectures.
