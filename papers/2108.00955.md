# [Evaluating Deep Graph Neural Networks](https://arxiv.org/abs/2108.00955)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions seem to be:1) What actually limits the stacking of deep convolution operations in GNN designs, and why does disentangling embedding propagation (EP) and embedding transformation (ET) allow more EP operations?2) What are the insights and guidelines for researchers to design deep GNNs? In particular, when and how to enlarge the number of EP and ET operations? 3) With the proposed insights and guidelines, can the authors stack more EP and ET operations and outperform state-of-the-art GNNs?The key hypothesis appears to be that disentangling and independently controlling EP and ET operations will allow construction of deeper and better-performing GNN models compared to prior "entangled" designs. The authors conduct experiments to evaluate the effects of EP and ET depth separately, and find that increasing EP depth leads to over-smoothing while increasing ET depth leads to model degradation. They propose design guidelines based on these findings - use larger EP depth for sparse graphs and larger ET depth for large graphs. Their model DGMLP implements these insights allowing flexible ET and EP depths. Experiments show DGMLP achieves state-of-the-art performance, supporting the hypothesis.In summary, the central questions revolve around understanding limitations of GNN depth, providing insights to address this, and demonstrating improved performance from a model implementing these insights.
