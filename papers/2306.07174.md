# [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable large language models (LLMs) to memorize and utilize long-term context beyond the fixed-sized inputs they are typically limited to. Specifically, the paper proposes a framework called Language Models Augmented with Long-Term Memory (LongMem) which allows LLMs to cache long previous context or knowledge into a non-differentiable memory bank and take advantage of that memory to improve language modeling and understanding. The key ideas are:1) A novel decoupled memory design, where a frozen pretrained LLM encodes context into the memory bank and an adaptive residual side network retrieves and reads from that memory. This avoids the issue of "memory staleness" that arises when using a single model for encoding and reading from memory.2) Efficient adaptation of the frozen pretrained LLM via continual training of the lightweight side network, avoiding catastrophic forgetting. 3) The ability to load various types of long-form text/knowledge into the memory bank depending on the downstream task. The paper evaluates long-context language modeling and memory-augmented in-context learning as key test cases.So in summary, the central hypothesis is that augmenting LLMs with an external long-term memory and an efficient adaptation mechanism will allow them to overcome fixed-length input constraints and better leverage long-range dependencies, improving language modeling and understanding. The proposed LongMem framework is designed to test this hypothesis.
