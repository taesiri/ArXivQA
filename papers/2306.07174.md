# [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable large language models (LLMs) to memorize and utilize long-term context beyond the fixed-sized inputs they are typically limited to. Specifically, the paper proposes a framework called Language Models Augmented with Long-Term Memory (LongMem) which allows LLMs to cache long previous context or knowledge into a non-differentiable memory bank and take advantage of that memory to improve language modeling and understanding. The key ideas are:1) A novel decoupled memory design, where a frozen pretrained LLM encodes context into the memory bank and an adaptive residual side network retrieves and reads from that memory. This avoids the issue of "memory staleness" that arises when using a single model for encoding and reading from memory.2) Efficient adaptation of the frozen pretrained LLM via continual training of the lightweight side network, avoiding catastrophic forgetting. 3) The ability to load various types of long-form text/knowledge into the memory bank depending on the downstream task. The paper evaluates long-context language modeling and memory-augmented in-context learning as key test cases.So in summary, the central hypothesis is that augmenting LLMs with an external long-term memory and an efficient adaptation mechanism will allow them to overcome fixed-length input constraints and better leverage long-range dependencies, improving language modeling and understanding. The proposed LongMem framework is designed to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a framework called Language Models Augmented with Long-Term Memory (LongMem) that enables large language models (LLMs) to memorize long historical context and utilize this long-term memory to improve language modeling. Specifically, the key contributions are:- Proposing a novel decoupled neural architecture with a frozen backbone LLM as a memory encoder and an adaptive residual side network as a memory retriever and reader. This decoupled design helps address the memory staleness issue in prior work.- Designing a memory caching and retrieval mechanism that can store unlimited-length past context in a memory bank and retrieve relevant memory to augment future inputs. - Demonstrating the effectiveness of LongMem on long-context language modeling benchmarks like modeling entire books and the ChapterBreak dataset. LongMem substantially outperforms prior methods.- Showing LongMem's ability to cache thousands of demonstration examples in memory for memory-augmented in-context learning, leading to significant improvements on NLU tasks over regular LLM fine-tuning.In summary, the key contribution is developing an effective framework to augment LLMs with long-term memory of past context, leading to gains in long-context language modeling and in-context learning. The proposed decoupled architecture and memory mechanisms are critical to the framework's success.
