# [Boosting Low-Data Instance Segmentation by Unsupervised Pre-training   with Saliency Prompt](https://arxiv.org/abs/2302.01171)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to boost the performance of query-based end-to-end instance segmentation (QEIS) models like K-Net, Mask2Former and QueryInst in low-data regimes where they tend to underperform compared to CNN-based models like Mask R-CNN. 

The key hypothesis is that the dynamic kernels/queries in QEIS models need more training data and time to learn good localization and shape priors. To address this, the authors propose an unsupervised pre-training method called "Saliency Prompt" that helps the kernels learn these priors more quickly without requiring extra labeled data or model architecture changes.

Specifically, the saliency prompt scheme involves:

1) Generating saliency mask proposals to indicate potential objects and their shapes.

2) Matching the mask proposals to kernels and converting them to "prompts" to inject localization/shape knowledge. 

3) Adding supervision at the kernel level during pre-training for more robust learning.

The central hypothesis is that this saliency prompt-based pre-training will allow QEIS models to achieve comparable performance to CNN models in low-data regimes by teaching the kernels localization and shape information in an unsupervised manner.
