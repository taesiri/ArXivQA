# [Boosting Low-Data Instance Segmentation by Unsupervised Pre-training   with Saliency Prompt](https://arxiv.org/abs/2302.01171)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to boost the performance of query-based end-to-end instance segmentation (QEIS) models like K-Net, Mask2Former and QueryInst in low-data regimes where they tend to underperform compared to CNN-based models like Mask R-CNN. 

The key hypothesis is that the dynamic kernels/queries in QEIS models need more training data and time to learn good localization and shape priors. To address this, the authors propose an unsupervised pre-training method called "Saliency Prompt" that helps the kernels learn these priors more quickly without requiring extra labeled data or model architecture changes.

Specifically, the saliency prompt scheme involves:

1) Generating saliency mask proposals to indicate potential objects and their shapes.

2) Matching the mask proposals to kernels and converting them to "prompts" to inject localization/shape knowledge. 

3) Adding supervision at the kernel level during pre-training for more robust learning.

The central hypothesis is that this saliency prompt-based pre-training will allow QEIS models to achieve comparable performance to CNN models in low-data regimes by teaching the kernels localization and shape information in an unsupervised manner.


## What is the main contribution of this paper?

 This paper proposes a novel unsupervised pre-training method to boost Query-based End-to-End Instance Segmentation (QEIS) models in low-data regimes. The key contributions are:

1. They point out that QEIS models like K-Net perform poorly in low-data regimes compared to CNN-based models like Mask RCNN because the dynamic kernels/queries need to learn localization and shape priors from scratch. 

2. They propose a new pre-training method called Saliency Prompt (SP) to inject localization and shape priors into kernels using visual prompts generated from saliency masks. This is the first work to explore prompting for instance segmentation.

3. SP contains three main components:
   - Saliency Mask Proposal to generate pseudo masks from unlabeled images.
   - Prompt-Kernel Matching to assign prompts to best-matched kernels via cosine similarity.
   - Kernel Supervision to directly supervise kernel learning.

4. Experiments show SP significantly boosts K-Net and other QEIS models like QueryInst and Mask2Former on low-data regimes of COCO, Cityscapes and CTW1500. It helps them achieve comparable performance with CNN models with faster convergence.

5. Ablations demonstrate the effectiveness of each component of SP. The method is robust to the quality of pseudo masks.

In summary, the core novelty is proposing Saliency Prompt to help QEIS models learn better localization and shape priors for low-data instance segmentation via prompt injection. This simple yet effective pre-training approach advances QEIS methods and prompts new research directions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the main points in the paper:

This paper proposes a novel unsupervised pre-training method called Saliency Prompt that uses saliency masks as prompts to inject localization and shape priors into the kernels of query-based end-to-end instance segmentation models, enabling them to achieve faster convergence and improved performance in low-data regimes.
