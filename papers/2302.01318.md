# Accelerating Large Language Model Decoding with Speculative Sampling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we accelerate the decoding of large language models to reduce latency, without compromising sample quality or modifying the model itself?The key hypotheses appear to be:1) Generating a short draft sequence in parallel with a faster model has comparable latency to sampling a single token from the larger target model.2) A novel modified rejection sampling scheme can be used to accept/reject draft tokens to recover the target model's distribution, while preserving quality. 3) The expected draft token acceptance rate is high enough to offset drafting overheads and achieve a 2-2.5x decoding speedup for large language models like Chinchilla.So in summary, the central research aim is to develop a practical method to speed up decoding of large language models through speculative parallel draft generation, while preserving sample quality. The method relies on two key hypotheses around the latency comparison of draft generation versus target sampling, and the ability of a modified rejection scheme to recover the target distribution.


## What is the main contribution of this paper?

The main contribution of this paper is an algorithm called speculative sampling (SpS) to accelerate the decoding of large language models like transformers. The key ideas are:- Generate a short "draft" sequence of tokens using a faster draft model. - Score the draft tokens using the target model to get predicted probabilities.- Use a modified rejection sampling scheme to accept/reject draft tokens to recover the distribution of the target model.By generating multiple draft tokens and scoring/accepting in parallel, this allows generating multiple tokens from each call of the expensive target model. So it speeds up decoding while maintaining the distribution of the target model.The paper shows this gives 2-2.5x speedup on a 70B parameter model with no loss of performance on summarization and dialog tasks. The speedup exceeds the theoretical limit from memory bandwidth of autoregressive decoding. The method works distributed across accelerators and can combine with other optimizations like quantization.So in summary, speculative sampling provides a practical way to reduce the latency of decoding huge language models, which is important for real-time conversational applications. The main innovation is using a conditional parallel draft to offset the cost of scoring multiple tokens from the target model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point of the paper:The paper presents a method called speculative sampling that can accelerate transformer decoding of large language models by generating multiple tokens per transformer call, yielding 2-2.5x faster decoding without compromising sample quality or biasing the distribution.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of accelerating large language model decoding:- The main contribution of this paper - speculative sampling - is a novel approach to speeding up decoding of large language models. Most prior work has focused on model compression or modifications to the transformer architecture. Speculative sampling is an orthogonal approach that does not require changing the model itself. - The idea of generating multiple tokens per inference step has been explored before in approaches like blockwise parallel decoding and aggressive decoding. However, those methods are limited to greedy decoding or bias the sample distribution. Speculative sampling generalizes this idea while preserving the exact model distribution.- This paper scales speculative sampling to very large models (70B parameter Chinchilla) in a distributed setting. Other related work on parallel decoding focused on smaller models and did not demonstrate distributed scaling. Being able to accelerate huge models like Chinchilla is critical for real-world applications.- The use of a smaller "draft" model to generate candidate sequences is simple and pragmatic compared to prior approaches like sequence-level distillation. The draft model can reuse existing training infrastructure and does not require joint training.- The theoretical analysis clearly explains why speculative sampling preserves the target distribution. The empirical evaluations on XSum and HumanEval verify that performance is uncompromised. The ablations also provide useful insights on how to optimize the method's hyperparameters. - Compared to model compression techniques like distillation and quantization, speculative sampling is complementary and orthogonal. The paper emphasizes that it can be combined with those other approaches for further speedups.In summary, this paper introduces a novel decoding algorithm that outpaces greedy approaches on huge language models in a distributed setting. The approach is theoretically principled and empirically validated. It also represents a practical way to accelerate state-of-the-art models without extensive modification.


## What future research directions do the authors suggest?

The authors suggest a few future research directions:- Experimenting with different choices of draft models, including incorporating draft generation directly into the target model architecture. They mention training a draft model that takes a portion of the target model's activations as input.- Exploring ways to train the draft model jointly with the target model, rather than just using a smaller pretrained version of the target model. This could potentially allow the draft model to specialize on providing useful samples for the target model to score.- Adapting speculative sampling to other modalities like images, as well as exploring the potential benefits in other domains like reinforcement learning.- Further optimization of the method in distributed setups, reducing overheads like communication between devices. They suggest speculative sampling could complement other optimizations like quantization and multi-query attention.- Analyzing the theoretical tradeoffs between draft length, scoring frequency, and overall speedup.- Evaluating speculative sampling in more real-world use cases beyond the summarization and code generation tasks they tested.So in summary, they propose further exploration of draft model choices and training, applying it to new domains, distributed optimizations, theoretical analysis, and more thorough real-world testing. The core speculative sampling idea seems promising, but there are many avenues to improve it.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper presents speculative sampling, an algorithm to accelerate the decoding of large transformer language models. The core idea is to first generate a short draft sequence using a faster draft model, then score this sequence in parallel using the target model to decide which tokens to accept or reject. This allows generating multiple tokens per expensive model call. A novel modified rejection sampling scheme is introduced that provably recovers the distribution of the target model. When applied to a 70 billion parameter model, Chinchilla, speculative sampling achieves a 2-2.5x speedup on benchmark tasks without compromising sample quality or requiring model modifications. The mean tokens per second with speculative sampling exceeds the theoretical limit from memory bandwidth on autoregressive decoding. Overall, speculative sampling provides an effective way to reduce transformer decoding latency through parallel drafting and scoring, with results demonstrating improved speeds at scale.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper presents speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. The algorithm first generates a short draft sequence using a faster but less powerful draft model. The draft sequence is then scored in parallel using the larger target model to compute logits. A novel modified rejection sampling scheme is applied to accept a subset of draft tokens from left to right, preserving the distribution of the target model within hardware numerics. The method is benchmarked with Chinchilla, a 70 billion parameter language model, in a distributed setup. It achieves a 2-2.5x decoding speedup without compromising sample quality or requiring model modifications. The mean tokens per second with speculative sampling often exceeds the idealized ceiling on auto-regressive sampling speed imposed by memory bandwidth. The acceptance rate and speedup varies by domain, with a task involving generating code demonstrating the highest speedup. There is a tradeoff between longer drafts requiring fewer scoring calls versus decreased efficiency and increased variance. Overall, speculative sampling provides an effective and practical method to reduce transformer sampling latency.


## Summarize the main method used in the paper in one paragraph.

The paper presents speculative sampling, an algorithm for accelerating transformer decoding by generating multiple tokens from each call to the transformer model. The key ideas are:1) Generate a short draft sequence of length K using a faster draft model. 2) Score the draft sequence in parallel using the target transformer model to get logits for continuations of length 1 to K tokens.3) Accept or reject each token in the draft using a modified rejection sampling scheme that recovers the distribution of the target model. Accepted tokens are appended to the output sequence.4) If all K tokens are accepted, sample an additional K+1'th token to maximize tokens per transformer call. By scoring multiple token continuations in parallel, speculative sampling achieves a 2-2.5x speedup in decoding from large transformer models like Chinchilla, without modifying the target model or compromising the sample quality.
