# [Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large   Neighborhood Search](https://arxiv.org/abs/2312.16767)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search":

Problem:
- Multi-agent path finding (MAPF) aims to find collision-free paths for multiple agents, and is important for applications like warehouse automation, disaster response, etc. 
- Anytime MAPF based on large neighborhood search (LNS) is a promising approach but suffers from two key limitations: (1) fixed neighborhood sizes limit flexibility and solution quality, requiring extensive tuning, (2) greedy optimization and lack of exploration due to the commonly used roulette wheel selection for destroy heuristics.

Proposed Solution: 
- The paper proposes BALANCE, a bandit-based adaptive LNS framework for anytime MAPF. 
- It uses a bi-level multi-armed bandit scheme to adapt the selection of destroy heuristics and neighborhood sizes during search. The first level bandit selects destroy heuristics, the second level bandits condition on this to select neighborhood sizes.
- Reward is defined as the cost improvement over previous solutions. Three MAB algorithm instantiations are proposed and analyzed: roulette wheel, UCB1, and Thompson sampling.

Main Contributions:
- Formulation of BALANCE - an adaptive anytime MAPF method using a bi-level MAB scheme to address limitations of state-of-the-art methods.
- Analysis and comparison of different MAB algorithms (roulette wheel, UCB1 and Thompson sampling) for BALANCE.
- Extensive experiments on standard MAPF benchmarks demonstrating at least 50% cost improvement over state-of-the-art methods in large scenarios. 
- Key findings that sufficient neighborhood size options and use of Thompson sampling significantly impact performance.

In summary, the paper presents a novel bandit-based adaptive LNS approach for anytime MAPF that is simple, flexible and outperforms existing methods without needing extensive tuning or machine learning.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a multi-armed bandit framework called BALANCE to adaptively select destroy heuristics and neighborhood sizes in anytime multi-agent path finding based on Large Neighborhood Search.


## What is the main contribution of this paper?

 According to the paper, the main contribution is proposing BALANCE, which is a Bandit-based Adaptive LArge Neighborhood search Combined with Exploration (BALANCE) for anytime multi-agent path finding. Specifically:

- They formulate BALANCE as a simple but effective MAPF-LNS framework with an adaptive selection of destroy heuristics and neighborhood sizes during search using a bi-level multi-armed bandit scheme. 

- They propose and discuss three concrete instantiations of BALANCE based on roulette wheel selection, UCB1, and Thompson Sampling.

- They empirically demonstrate cost improvements of at least 50% compared to state-of-the-art anytime MAPF approaches like MAPF-LNS and MAPF-ML-LNS in large-scale scenarios. 

- They find that Thompson Sampling performs particularly well compared to alternative multi-armed bandit algorithms for balancing exploration and exploitation.

So in summary, the main contribution is proposing the BALANCE framework for adaptive optimization of destroy heuristics and neighborhood sizes in anytime multi-agent path finding.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Multi-agent path finding (MAPF)
- Anytime planning
- Large neighborhood search (LNS)
- Multi-armed bandits (MABs)
- Thompson sampling
- UCB1
- Roulette wheel selection
- Destroy heuristics
- Neighborhood size
- Exploration-exploitation tradeoff
- Adaptive optimization
- Online learning

The paper presents a framework called BALANCE for adaptive anytime multi-agent path finding using a bi-level multi-armed bandit scheme. It aims to adapt the selection of destroy heuristics and neighborhood sizes in MAPF large neighborhood search to improve solution quality and flexibility. The key ideas involve using bandit algorithms like Thompson sampling and UCB1 to balance exploration and exploitation for these selections, comparing different variants, and showing significant improvements over state-of-the-art approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a bi-level multi-armed bandit scheme for adaptive destroy heuristic and neighborhood size selection. What are the key motivations and limitations of previous MAPF-LNS methods that this scheme aims to address?

2. Explain the bi-level multi-armed bandit scheme in detail. What are the inputs, outputs, and update procedures of the top-level $\mathcal{H}$-Bandit and bottom-level $\mathcal{N}$-Bandits? 

3. The paper discusses roulette wheel selection, UCB1, and Thompson Sampling as instantiations of the bi-level scheme. Compare and contrast their exploration mechanisms and explain why Thompson Sampling seems most promising for this setting.  

4. The proposed BALANCE framework uses the cost improvement between solutions as the reward signal for the bandits. Discuss the advantages and potential limitations of this straightforward choice compared to more complex weighted reward formulations.

5. The bi-level scheme mitigates scalability issues compared to optimizing the joint destroy heuristic and neighborhood size space directly. Explain this argument and discuss its limitations. When could a joint optimization approach potentially be beneficial?

6. Analyze the convergence behavior of different BALANCE variants over increasing time budgets based on the results. What can be concluded about the effects of exploration on performance?

7. The paper indicates that performance is more sensitive to the destroy heuristic choice than the neighborhood size. Verify and discuss this claim using the empirical results. Are there exceptions?

8. How does the number of available neighborhood size options affect the performance of different BALANCE variants? Provide an explanation based on the exploration capabilities of the bandit algorithms.

9. The results show significant performance improvements over state-of-the-art methods. Analyze and discuss the factors that contribute to these improvements in detail.

10. The paper focuses on MAPF scenarios. Discuss the potential benefits and limitations of applying the proposed BALANCE framework to other large neighborhood search optimization problems. What adaptations would be required?
