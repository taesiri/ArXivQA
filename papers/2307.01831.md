# [DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation](https://arxiv.org/abs/2307.01831)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:Can a transformer-based architecture effectively generate high-fidelity and diverse 3D point cloud shapes when trained as a denoising diffusion probabilistic model?The key points are:- Recent work has shown transformers to be very effective for image generation when trained as diffusion models (e.g. DiT). - However, it has not been well explored if transformers can work equally well for 3D shape generation, as most prior 3D diffusion methods use convolutional architectures like U-Nets.- This paper proposes a novel "Diffusion Transformer for 3D" (DiT-3D) that operates directly on voxelized point clouds and is tailored for 3D with positional embeddings, patch embeddings, window attention, etc.- Through experiments on ShapeNet, they demonstrate DiT-3D can generate higher quality and more diverse 3D point clouds compared to prior 3D diffusion methods.So in summary, the central hypothesis is that a properly designed transformer architecture can achieve state-of-the-art 3D shape generation performance when trained as a diffusion model, which they confirm through both quantitative and qualitative evaluations.
