# [How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How attentive are graph attention networks (GATs)? In particular, does the popular GAT architecture actually compute the kind of expressive, dynamic attention it is intended and assumed to compute?The key hypothesis put forth in the paper is that GATs do not actually compute dynamic attention due to the way their attention mechanism is designed. Instead, GATs compute a more limited form of static attention. The paper introduces formal definitions of static and dynamic attention to investigate this hypothesis, both theoretically and empirically.In summary, the main research question revolves around evaluating how attentive and expressive the attention mechanisms in GATs really are, compared to their intended purpose and assumed capabilities. The central hypothesis is that their attention is more static than dynamic due to specific limitations in their design.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Identifying that the popular Graph Attention Network (GAT) architecture actually computes a limited form of "static" attention, rather than the more expressive "dynamic" attention that it seems to compute. The authors formally define static and dynamic attention and prove theoretically that GAT can only compute static attention.2. Introducing a simple fix to GAT, by modifying the order of operations, to create GATv2. They prove GATv2 can compute dynamic attention.3. Demonstrating empirically on a synthetic problem that GAT cannot express dynamic attention and fails to fit the training data, while GATv2 succeeds.4. Showing GATv2 is more robust to noise/edges in graph data compared to GAT, thanks to its dynamic attention. 5. Comparing GAT and GATv2 extensively on 12 graph benchmarks and finding GATv2 outperforms GAT in all cases, sometimes significantly.In summary, the key contributions are formally analyzing the limitations of GAT's attention mechanism, proposing a simple fix to make it more expressive, and demonstrating improved performance empirically in a variety of graph tasks. The identification of GAT's limitations and the enhanced GATv2 architecture seem to be the main innovations presented.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of graph neural networks:- This paper focuses specifically on analyzing the graph attention mechanism in Graph Attention Networks (GATs). It provides formal definitions and theorems characterizing the difference between "static" and "dynamic" attention, which is a novel theoretical contribution not seen in most prior GNN papers. - The paper introduces a modified version of GAT called GATv2 that computes dynamic attention. Other recent papers have also proposed new variants of GAT, but they have different motivations and mechanisms. For example, works like GaAN and Masked GAT aimed to improve scalability, not expressiveness.- The extensive empirical evaluation across 12 graph benchmarks is quite thorough compared to most prior GNN papers. Many papers include results on just 1-2 small datasets. The comparisons to GAT and multiple baselines on varied tasks helps demonstrate the benefits of dynamic attention.- Most prior empirical evaluations of new GNN models are on node classification. This paper includes node, link and graph prediction tasks. Evaluating on multiple downstream applications strengthens the evidence that dynamic attention helps.- The analysis of model robustness to edge noise is an interesting experiment that provides insight into the benefits of dynamic vs static attention. Most papers don't evaluate model robustness empirically.- The comparisons are done mainly between GAT and GATv2. Many recent papers introduce entirely new models rather than improving existing models like GAT. The incremental improvement approach here is somewhat uncommon lately.So in summary, the formal theoretical analysis, introduction of dynamic attention, extensive empirical methodology, and focus on improving GAT help differentiate this paper from related work in the field. The comparisons between attention types and robustness experiments are novel contributions.
