# [Enhanced Sparsification via Stimulative Training](https://arxiv.org/abs/2403.06417)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
Existing pruning methods typically suppress the pruned weights (drive them to 0) to make them unimportant. However, this causes damage to the capacity and expressibility of the unpruned network before pruning, limiting the final performance. 

Solution - Enhanced Sparsification Paradigm via Stimulative Training:
- The authors reveal a "relative sparsity effect" in Stimulative Training (ST), which enhances the magnitude of chosen (kept) weights via self-distillation while maintaining the magnitude of dropped weights. This avoids damaging the capacity.

- Based on this, they propose an enhanced sparsification paradigm for structured pruning that achieves relative sparsity to facilitate lossless pruning. 

- They develop a pruning framework called Stimulative Training guided Pruning (STP) which gradually explores optimal pruned architectures guided by a knowledge distillation loss and leverages ST for enhanced sparsification.

Key Contributions:
- Reveal and analyze the relative sparsity effect of ST which serves as an enhanced sparsification paradigm without suppressing dropped weights.

- Propose the STP framework for structured pruning that explores architectures via a knowledge distillation-guided strategy and utilizes ST for sparsification.

- Introduce multi-dimension sampling for subnets, subnet mutating expansion, and several analyses and experiments to demonstrate effectiveness.

- Without fine-tuning, STP consistently achieves state-of-the-art performance across benchmarks, especially under high pruning ratios. For example, it preserves 95.11% accuracy on ImageNet while reducing 85% FLOPs for ResNet-50.
