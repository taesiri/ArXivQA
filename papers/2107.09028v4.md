# [Structured Stochastic Gradient MCMC](https://arxiv.org/abs/2107.09028v4)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the key research question this paper addresses is:

How can we hybridize Markov chain Monte Carlo (MCMC) and variational inference (VI) methods to get the best of both worlds - the asymptotic exactness of MCMC and the scalability of VI? 

Specifically, the paper proposes a new class of structured stochastic gradient MCMC (S-SGMCMC) methods that impose a factorization structure on the posterior distribution, similar to structured variational methods. This allows the MCMC sampler to more quickly explore the posterior landscape by breaking dependencies between subsets of parameters. 

The core hypotheses seem to be:

1) Imposing a structured factorization on the posterior approximation, as is commonly done in VI, can speed up mixing times for SGMCMC samplers. 

2) It is possible to sample from the optimal structured posterior approximation in terms of KL-divergence by modifying the energy function used in SGMCMC.

3) The proposed S-SGMCMC methods will retain the asymptotic exactness of MCMC while benefiting from faster mixing times akin to structured VI.

In summary, the central goal is developing scalable MCMC algorithms that can leverage ideas from variational inference to enable more efficient posterior exploration and uncertainty quantification in complex models like Bayesian neural networks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new class of stochastic gradient MCMC methods called structured SGMCMC (S-SGMCMC) that bridges variational inference and MCMC. 

Specifically, the key ideas are:

- Inspired by structured variational inference, the authors propose to impose a factorization structure on the posterior approximation produced by SGMCMC. This is done by running SGMCMC to minimize the KL divergence between the imposed structured posterior and the true posterior.

- They show the resulting procedure approximately samples from the optimal non-parametric posterior given the imposed factorization constraints. This allows trading off approximation quality for faster mixing times.

- They further propose an extension called structured dropout SGMCMC (S$_d$-SGMCMC) that is inspired by dropout regularization. This allows interpolation between regular SGMCMC and the structured SGMCMC method.

- Experiments on neural networks demonstrate that the proposed methods achieve faster mixing times and comparable or better accuracy than baseline SGMCMC and variational inference methods.

In summary, the main contribution is developing structured SGMCMC algorithms that leverage ideas from variational inference to improve the mixing time and scalability of SGMCMC, while retaining its non-parametric flexibility. The methods provide useful ways to trade off accuracy and computational efficiency in Bayesian deep learning.

Does this help summarize the core contribution? Let me know if you have any other questions!
