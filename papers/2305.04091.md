# Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning   by Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we improve zero-shot chain-of-thought reasoning by large language models through better prompting strategies?Specifically, the authors aim to address some limitations of prior zero-shot chain-of-thought (CoT) prompting strategies:- Calculation errors in reasoning steps leading to incorrect answers- Missing reasoning steps, especially for problems with many steps  - Other semantic misunderstanding errorsTo address these issues, the central hypothesis appears to be:By introducing a simple but effective prompting strategy called "plan-and-solve" prompting, we can guide large language models to generate higher quality reasoning steps and reduce errors in zero-shot CoT reasoning.The Plan-and-Solve (PS) prompting strategy has the model first devise a plan to break down the reasoning task into subtasks, before carrying out the plan to solve the problem step-by-step. This is hypothesized to reduce missing step errors.Additionally, adding more detailed instructions to the PS prompt, such as prompting the model to extract variables and calculate intermediate results carefully, results in the PS+ variant which is hypothesized to further reduce calculation errors and improve reasoning quality.So in summary, the key research question is whether "plan-and-solve" prompting can enhance zero-shot chain-of-thought reasoning by large language models, reducing common errors and improving multi-step reasoning quality without the need for demonstration examples.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Identifying three key issues with the Zero-shot-CoT method: calculation errors, missing reasoning step errors, and semantic misunderstanding errors. The authors provide an analysis showing the distribution of these error types.2. Proposing two new zero-shot prompting methods called Plan-and-Solve (PS) and Plan-and-Solve+ (PS+) to address the issues in Zero-shot-CoT. The key ideas are to prompt the model to first devise a plan to decompose the reasoning task before executing the plan, and adding more detailed instructions to guide the reasoning process. 3. Evaluating PS and PS+ prompting strategies on 10 datasets across arithmetic, commonsense, and symbolic reasoning tasks. The results show PS+ consistently and significantly outperforms Zero-shot-CoT, and is comparable or better than few-shot CoT prompting on several arithmetic datasets.4. Providing analysis to gain insights, such as showing the impact of prompts, exploring plan presence, and examining the correlation between generated reasoning and error types.In summary, the main contribution is developing and evaluating the proposed PS and PS+ zero-shot prompting strategies to improve reasoning and reduce specific error types compared to prior Zero-shot-CoT prompting. The results indicate these prompting strategies can elicit better reasoning from large language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review of the paper, here is a one sentence summary:The paper proposes a new zero-shot prompting method called Plan-and-Solve (PS) that improves reasoning and reduces errors in large language models by first prompting them to devise a plan to decompose complex reasoning tasks into subtasks before generating step-by-step reasoning.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper focuses specifically on improving chain-of-thought reasoning and multi-step problem solving abilities in large language models through prompting strategies. Other related works have explored improving reasoning abilities in LLMs through other methods like scaling model size, adding demonstrations, or using reinforcement learning. So this work provides a novel prompting-based approach.- Compared to other prompting methods like few-shot CoT or zero-shot CoT, this paper introduces the idea of having the LLM first plan out its reasoning steps before executing them. Asking the model to plan seems to substantially improve its reasoning and reduce errors. This "plan-then-solve" prompting strategy is a simple but effective novelty. - The idea of providing more detailed prompting instructions to reduce calculation errors and missing steps is also an incremental improvement over prior CoT prompting work. The analysis of how different prompts impact errors is an insightful contribution.- In terms of performance, the proposed PS+ prompting reaches state-of-the-art results for zero-shot prompting on several math reasoning datasets, outperforming prior methods like zero-shot CoT and zero-shot PoT. The fact that it can match performance of 8-shot CoT is impressive.- The experiments are thorough, testing the approach on multiple datasets across arithmetic, commonsense, and symbolic reasoning. The analyses provide insight into the method's strengths and limitations.Overall, I would say this paper makes excellent progress on an important problem, introduces some novel and effective prompting strategies, and provides useful analysis and insights that advance the state of knowledge in training LLMs for complex reasoning. The results and techniques should be of interest to other researchers working on improving reasoning abilities in large language models.
