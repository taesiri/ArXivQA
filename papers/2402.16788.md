# [Why Transformers Need Adam: A Hessian Perspective](https://arxiv.org/abs/2402.16788)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Transformers have become very popular and achieve state-of-the-art results on many tasks, but SGD performs much worse than Adam on training Transformers, unlike with CNNs where they are comparable. It is unclear why SGD fails on Transformers.

- The paper investigates the reason behind this through analyzing the Hessian matrix, which captures local curvature information about the loss landscape. 

Approach & Findings
- The full Hessian spectrum is similar between Transformers and CNNs, so does not explain the difference in optimizer performance. 

- However, the blockwise Hessian spectrum, corresponding to parameters in different modules of the networks, does reveal a key difference. Transformers exhibit "block heterogeneity", meaning very different spectrum distributions across modules, while CNNs show "block homogeneity".

- Constructing simple MLPs and quadratic problems, it is shown heterogeneity hampers SGD but not problems without heterogeneity. Heterogeneity exists in Transformers but not CNNs, correlating with optimizer performance.

- Theory suggests SGD fails as it uses one global learning rate, unable to handle heterogeneity, while Adam adapts the learning rate in different blocks.

Main Contributions
- Identified "block heterogeneity" phenomenon in Hessians of Transformers but not CNNs, correlating with bad SGD performance

- Showed heterogeneity causes SGD to fail across Transformers, MLPs and quadratic problems

- Provided theory that heterogeneity means SGD's single learning rate cannot adapt, while Adam can, explaining difference in optimizer performance

- Overall gave a Hessian-based explanation for why SGD fails on Transformers, while revealing a key property differentiating CNN and Transformer loss landscapes
