# [LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data](https://arxiv.org/abs/2208.14889)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform unsupervised image-to-image translation by using only dataset-level textual descriptions, instead of requiring per-sample domain labels or complete unsupervision. 

The key hypothesis is that by leveraging a pretrained vision-language model like CLIP to match images to textual domain descriptions, they can obtain pseudo multi-hot domain labels for each image. This allows the model to learn a mapping from source images to target domains specified by combinations of textual descriptions.

The main contributions towards this are:

- A method to obtain pseudo multi-hot domain labels for images based on similarity of CLIP image and text features.

- Techniques like prompt learning and a "slack" domain to deal with inaccurate initial textual prompts.

- A domain regularization loss to encourage translated images to match the target domain labels.

So in summary, the central hypothesis is that using dataset-level textual supervision along with techniques like prompt learning and domain regularization, they can learn a high-quality image translation model without per-sample domain labels or complete unsupervision. The experiments aim to validate that their method can perform competitively or better than previous supervised and unsupervised approaches.
