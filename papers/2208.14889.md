# [LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data](https://arxiv.org/abs/2208.14889)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform unsupervised image-to-image translation by using only dataset-level textual descriptions, instead of requiring per-sample domain labels or complete unsupervision. 

The key hypothesis is that by leveraging a pretrained vision-language model like CLIP to match images to textual domain descriptions, they can obtain pseudo multi-hot domain labels for each image. This allows the model to learn a mapping from source images to target domains specified by combinations of textual descriptions.

The main contributions towards this are:

- A method to obtain pseudo multi-hot domain labels for images based on similarity of CLIP image and text features.

- Techniques like prompt learning and a "slack" domain to deal with inaccurate initial textual prompts.

- A domain regularization loss to encourage translated images to match the target domain labels.

So in summary, the central hypothesis is that using dataset-level textual supervision along with techniques like prompt learning and domain regularization, they can learn a high-quality image translation model without per-sample domain labels or complete unsupervision. The experiments aim to validate that their method can perform competitively or better than previous supervised and unsupervised approaches.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel framework for image-to-image translation using only dataset-level supervision. The key ideas are:

- Leveraging candidate textual domain descriptions (dataset-level annotations) and their similarity to images in a pretrained vision-language model's embedding space to estimate pseudo multi-hot domain labels for each image. This allows using multi-hot labels compared to one-hot labels in previous works.

- Introducing a "null" domain and adaptive thresholding technique to filter out ambiguous/noisy images that don't match the text descriptions well.

- Making the textual domain descriptions learnable and jointly optimizing them with the image translation model using a proposed domain regularization loss. This helps improve the pseudo labels and translation. 

- Experiments show the proposed LANIT model achieves comparable or better performance compared to previous image translation models that use stronger supervision like per-sample domain labels.

So in summary, the key contribution is exploiting easy-to-obtain dataset-level textual supervision to train a high-quality image-to-image translation model, eliminating the need for intensive per-sample annotation. The proposed techniques like learnable prompts and losses allow improving the unlabeled pseudo-labeling process.
