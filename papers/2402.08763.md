# [Enhancing Robustness of Indoor Robotic Navigation with Free-Space   Segmentation Models Against Adversarial Attacks](https://arxiv.org/abs/2402.08763)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper addresses the problem of improving the robustness of indoor robotic navigation systems against adversarial attacks. Specifically, it focuses on the task of free-space segmentation, which involves identifying traversable regions in RGB images to enable path planning for mobile robots. However, deep learning models used for this task are vulnerable to imperceptible adversarial perturbations that can lead to incorrect segmentation predictions and jeopardize the safety of navigation.   

Proposed Solution
The paper proposes a novel defense method against adversarial attacks for free-space segmentation models. The key ideas are:

1) Use adversarial training to make the model more robust by incorporating adversarial examples in addition to clean examples during training. This enhances generalization and reduces sensitivity to minor input perturbations.

2) Introduce an "adversarial hidden loss" function that minimizes the divergence between clean and adversarial examples in the embedding space of the model's encoder layers. This regularizes the internal representations to be more invariant. 

3) Combine adversarial training with the proposed loss to minimize sensitivity across both the input and hidden layers of the network.

The complete pipeline involves:
(i) Training a SegFormer model on clean data 
(ii) Generating adversarial examples using PGD attack
(iii) Fine-tuning the model on a mix of clean and adversarial examples, while optimizing the combined loss function.

Contributions
The key contributions are:

- A novel adversarial hidden loss function that enhances traditional adversarial training by aligning intermediate layer representations between clean and perturbed inputs.

- Extensive experiments on a custom indoor dataset demonstrating improved robustness against perturbations using the proposed technique.

- Analysis of the effect of different regularization strengths and comparisons with standard semantic segmentation models.

- Qualitative results illustrating how the method can effectively correct false transformations of free space into obstacles by the attack.

In summary, the paper presents a practical defense strategy to improve adversarial robustness of free-space segmentation models by applying regularization both at the input and feature level.
