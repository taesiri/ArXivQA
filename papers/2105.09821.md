# [DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient   Hyperparameter Optimization](https://arxiv.org/abs/2105.09821)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new hyperparameter optimization algorithm called DEHB (Differential Evolution Hyperband) that combines ideas from differential evolution and the Hyperband algorithm. 

The key hypothesis is that DEHB will be an effective general-purpose HPO solver that is:

- Scalable to high dimensions
- Flexible to handle mixed data types
- Robust across different problems  
- Computationally efficient
- Achieves strong anytime and final performance

The authors argue that previous HPO methods like Bayesian optimization with Hyperband (BOHB) still struggle with discrete/categorical dimensions and do not scale as well. Evolutionary algorithms like differential evolution (DE) can help overcome these issues but need to be combined with multi-fidelity methods like Hyperband to get good anytime performance. 

By integrating DE and Hyperband, DEHB aims to get the benefits of both - DE's global search ability and Hyperband's fast early performance through aggression resource allocation. The goal is for DEHB to be an effective "default" HPO method that works well across diverse problems.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is a new hyperparameter optimization method called DEHB. The key highlights of DEHB are:

- It combines differential evolution (DE) with the bandit-based hyperparameter optimization method Hyperband (HB) to get the benefits of both approaches. 

- It inherits good anytime performance, scalability, and flexibility from HB. From DE it inherits robustness, simplicity, and computational efficiency.

- It performs asynchronous updates when running DE, unlike classical DE which accumulates cost before updating. This makes it more efficient.

- It uses modified DE mutation and parent selection to enable information flow between different fidelity levels in a principled way.

- It implements parallelism on top of the synchronous Hyperband brackets by asynchronous scheduling.

- It uses a continuous population representation even for discrete/categorical dimensions to maintain diversity, only discretizing before function evaluations.

- Comprehensive experiments demonstrate state-of-the-art performance across a wide range of HPO problems, including high-dimensional and NAS benchmarks. The results show DEHB is more robust and efficient than prior methods like random search, TPE, SMAC, regularized evolution, BOHB etc.

In summary, the key contribution is a new HPO method DEHB that combines the strengths of evolutionary optimization via DE and multi-fidelity optimization via Hyperband. The design choices make it simple, efficient, robust and flexible across problem types, positioning it as a strong default HPO algorithm.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one sentence summary: 

The paper proposes a new hyperparameter optimization method called DEHB that combines differential evolution with Hyperband to achieve efficient and robust optimization across a range of problems, outperforming previous methods like BOHB.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other related work:

- The paper proposes a new algorithm called DEHB that combines differential evolution (DE) with Hyperband (HB) for hyperparameter optimization. This builds on prior work on DE and HB, but combines them in a novel way.

- Compared to other multi-fidelity hyperparameter optimization methods like BOHB, Fabolas, and Dragonfly, DEHB offers a simpler algorithm without needing to fit surrogate models like Gaussian processes. The paper argues this makes DEHB more robust and efficient.

- For high-dimensional discrete search spaces, like those in neural architecture search, DEHB appears more effective than model-based methods like BOHB. This is likely because DE is a global, population-based search method well-suited to such spaces.

- The experiments compare DEHB most directly against BOHB since that was state-of-the-art. DEHB matches or exceeds BOHB across a range of benchmarks. The paper also briefly compares against other baselines like random search and TPE, showing DEHB is highly competitive.

- For neural architecture search benchmarks, DEHB is compared against regularized evolution, showing performance on par with this state-of-the-art NAS algorithm specialized for discrete spaces.

- The authors position DEHB as a new general-purpose HPO method, robust across problem types, that could become a new default choice. The goal is an off-the-shelf optimizer, rather than claiming best performance in all scenarios.

In summary, the paper shows DEHB is a simple, efficient, and robust HPO method competitive or superior to a variety of strong prior algorithms across a wide range of problem types. It aims to make DEHB a strong default choice rather than claiming superiority in all cases.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more robust and sample-efficient meta-learning algorithms. The authors note that current meta-learning methods often require many task samples to learn effectively. They suggest developing more sample-efficient meta-learning algorithms that can learn from fewer tasks.

- Scaling meta-learning to more complex tasks. Most meta-learning research has focused on relatively simple few-shot image classification tasks. The authors suggest expanding meta-learning approaches to more complex tasks like reinforcement learning.

- Combining meta-learning with unsupervised and semi-supervised learning. The authors propose combining meta-learning objectives with unsupervised pre-training or semi-supervised learning to improve generalization. 

- Developing better ways to evaluate meta-learning algorithms. The authors note issues with how meta-learning methods are currently benchmarked, and suggest developing more comprehensive and realistic benchmarks.

- Combining meta-learning with domain generalization and domain adaptation. The authors propose using meta-learning to help improve performance on out-of-distribution generalization through domain adaptation.

- Developing theoretical understandings of meta-learning. Much of the current work on meta-learning is empirical. The authors suggest developing a better theoretical grounding to understand why and when meta-learning works.

- Applying meta-learning to real-world few-shot learning problems. Most meta-learning research is on simulated few-shot tasks. The authors propose testing meta-learning approaches on real-world few-shot problems.

In summary, the main directions are improving the robustness, scalability and theoretical understanding of meta-learning algorithms, as well as expanding their applications to more complex and real-world problems. Evaluating meta-learning methods more comprehensively is also highlighted as an area needing work.


## Summarize the paper in one paragraph.

 The paper proposes a new hyperparameter optimization (HPO) method called DEHB that combines differential evolution (DE) with Hyperband. The key contributions are:

- DEHB satisfies desirable criteria for a good general HPO method, including strong anytime and final performance, scalability, flexibility, and low overhead. 

- It combines strengths of Hyperband (strong anytime performance, scalability) with DE (simplicity, efficiency, effectiveness for high-dimensional and discrete problems).

- Comprehensive experiments on toy functions, regression, feedforward nets, Bayesian neural nets, RL, and 13 NAS benchmarks show DEHB is more robust and efficient than previous HPO methods. It achieves up to 1000x speedup over random search and outperforms BOHB.

- Ablation studies justify DEHB's default hyperparameters. DEHB requires minimal tuning and is simple to implement, making it a strong candidate for a new default HPO method.

In summary, DEHB proposes a robust and efficient HPO method by combining DE and Hyperband. It demonstrates state-of-the-art performance on a diverse set of benchmarks compared to previous HPO methods. The simplicity and effectiveness of DEHB make it a promising default choice for performing HPO.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new hyperparameter optimization method called DEHB that combines differential evolution (DE) with Hyperband (HB) to achieve efficient and robust performance across a variety of problems. DEHB handles the exploration-exploitation tradeoff by using DE to guide the search at each budget level in HB instead of just random sampling. It shares information between budget levels via a parent pool that represents promising regions of the search space. Experiments on a diverse set of HPO benchmarks, including artificial functions, neural network tuning, and NAS, demonstrate that DEHB is more robust and achieves better anytime and final performance than prior methods like BOHB and random search. On some tasks, DEHB achieved speedups of over 1000x compared to random search. The benefits are particularly pronounced on high-dimensional and discrete search spaces where model-based methods struggle. DEHB is also simple, flexible, and scales well due to the efficiency of DE.

In more detail, DEHB maintains a subpopulation for each budget level in HB, with the population size set to the number of configurations HB allocates for that budget. After initialization, each HB iteration in DEHB runs DE separately on each subpopulation, using a modified mutation strategy to incorporate information from lower budgets' parent pools. This allows low-fidelity information to guide search at higher budgets without necessarily committing to those configurations. DEHB is shown to handle discrete and mixed search spaces well by representing populations along a continuous [0,1] scale. Experiments demonstrate DEHB's excellent anytime performance and convergence to top final solutions on nearly all tested benchmarks, especially high-dimensional and discrete problems where model-based techniques often falter. The simplicity and scalability of DE also makes DEHB efficient in runtime overhead compared to Bayesian optimization.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new method called DEHB for hyperparameter optimization (HPO). The key idea is to combine the bandit-based HPO method Hyperband (HB) with the evolutionary optimization method Differential Evolution (DE) to get the benefits of both approaches. 

Specifically, DEHB runs multiple iterations called DEHB brackets. Each DEHB bracket runs multiple Successive Halving (SH) brackets, similar to vanilla Hyperband. The key differences are:

1) In the first DEHB bracket, random sampling is used like Hyperband. But in subsequent brackets, configurations are evolved using DE rather than random sampling. 

2) Between SH brackets, DE evolution is run on subpopulations of configurations associated with each budget. The top configurations from lower budgets are used as parents to guide the search on higher budgets via a modified DE mutation operator.

3) Whenever improved configurations are found via DE, they are immediately injected into the population rather than waiting for a generation to complete.

So in summary, DEHB inherits Hyperband's ability to effectively use different budgets and Successive Halving, while DE provides a more guided search especially for higher budgets by building on top of good solutions from lower budgets. The immediate injection of improved solutions also helps.


## What problem or question is the paper addressing?

 The paper is addressing the problem of hyperparameter optimization (HPO) for machine learning models. The key question it is trying to answer is: how can we develop a scalable, robust, and efficient general purpose HPO method?

The paper notes that modern machine learning algorithms rely heavily on good hyperparameter settings to achieve strong performance. This makes efficient HPO methods increasingly important. The authors argue that a good general HPO method should have several desirable properties:

1. Strong anytime performance: performs well even with a small compute budget
2. Strong final performance with a large budget
3. Effective use of parallel resources  
4. Scalability with respect to dimensionality of hyperparameter space
5. Robustness and flexibility across problem domains

They note that the BOHB method combines strengths of Bayesian optimization and the Hyperband algorithm to satisfy several of these criteria. However, BOHB still struggles with discrete/categorical dimensions and high dimensional problems. 

The key contribution of this paper is a new HPO method called DEHB that combines differential evolution (DE) with Hyperband. DEHB is designed to be more robust, efficient, and scalable, especially for high dimensional discrete search spaces. The paper aims to demonstrate through experiments across a diverse set of HPO problems that DEHB meets the criteria for a good general purpose HPO method better than previous approaches like BOHB.
