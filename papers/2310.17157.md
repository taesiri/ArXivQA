# [Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://arxiv.org/abs/2310.17157)

## Summarize the paper in one sentence.

 This paper develops a new method called Deja Vu to reduce the inference latency of large language models by exploiting contextual sparsity. The key idea is to predict and utilize only a small subset of model parameters that are most relevant for a given input, allowing over 2x speedup without compromising accuracy or model capabilities.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a method called Deja Vu for reducing the inference latency of large language models (LLMs) by exploiting contextual sparsity. The key idea is that for a given input, only a small subset of attention heads and MLP parameters are needed to produce approximately the same output as the full dense model. The paper shows that such contextual sparsity exists in pre-trained LLMs and can be predicted using simple learning-based algorithms. To avoid the overhead of sequential prediction, the method uses asynchronous "lookahead" predictors that exploit the similarity of activations across consecutive layers. The end-to-end system realization called Deja Vu combines these ideas with optimized sparse matrix multiplication to achieve over 2x speedup compared to state-of-the-art baselines on 175B parameter models, without accuracy loss. A key benefit is maintaining the powerful in-context learning abilities of large LLMs while reducing latency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a system called Deja Vu that exploits the existence of contextual sparsity in large language models to predict input-dependent subsets of model parameters for efficient inference without compromising accuracy or in-context learning ability.


## What is the central research question or hypothesis that this paper addresses?

 This paper does not appear to state an explicit research question or hypothesis. However, based on my reading, the main focus seems to be on developing a system called Deja Vu to reduce the inference latency of large language models through exploiting contextual sparsity. The key ideas explored in the paper are:

- Contextual sparsity exists in large pre-trained language models, meaning for a given input there are small sets of attention heads and MLP parameters that yield approximately the same output as the full dense model.

- This contextual sparsity can be accurately predicted using lightweight learning-based algorithms without needing to modify the pre-trained models. 

- The predictions can be made efficiently on-the-fly using an asynchronous design with lookahead predictors.

- The predicted sparsity patterns can be exploited along with optimized sparse matrix multiplication implementations to achieve significant speedups in wall-clock inference time compared to dense models.

So in summary, the main research contributions appear to be:

1) Empirically verifying the existence of contextual sparsity in large pre-trained language models. 

2) Developing methods to predict this sparsity accurately and efficiently at inference time.

3) Demonstrating these predictions can be used to reduce inference latency in real-world systems while maintaining model accuracy.

The key hypothesis seems to be that contextual sparsity exists and can be predicted in a way that enables faster inference without compromising model quality. The paper presents empirical support for this hypothesis through experiments on models like OPT-175B.


## What is the main contribution of this paper?

 The main contribution of this paper seems to be introducing a method called "Deja Vu" to reduce the inference latency of large language models (LLMs) by exploiting contextual sparsity. 

Specifically, the key ideas are:

- Observing that LLMs exhibit contextual sparsity - for a given input, only a small subset of parameters (attention heads and MLP neurons) are needed to get good predictions. This enables structured pruning of LLMs on the fly during inference.

- Proposing lightweight learning-based algorithms to predict this contextual sparsity pattern for each layer's MLP and attention, based on that layer's input.

- Using an asynchronous predictor design to hide the overheads of sparsity prediction behind compute.

- Optimized sparse matrix multiply implementation to realize speedups.

Together, these allow "Deja Vu" to reduce inference latency of large LLMs like OPT-175B by over 2x without accuracy loss, compared to optimized baselines. The key advantages are not needing costly retraining or losing in-context learning abilities like other pruning methods.

In summary, the main contribution is an end-to-end system to exploit the contextual sparsity of LLMs to reduce inference latency, while maintaining model accuracy and in-context learning ability. The techniques include sparsity characterization, prediction, asynchronous execution and optimized sparse kernels.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in natural language processing and large language model efficiency:

- The paper focuses specifically on improving inference efficiency for large language models, rather than training efficiency. Many recent papers have focused more on efficient training of large models.

- The proposed method uses contextual sparsity, which predicts input-dependent sparsity patterns on the fly during inference. This differs from prior work on static pruning or structured sparsity that is fixed after training. 

- The paper shows speedups on large 175B parameter models like OPT-175B. Many recent pruning/sparsity papers focus on smaller models. Showing efficiency gains at this unprecedented scale is impressive.

- The method does not require retraining the model and works directly on pretrained models. Retraining or finetuning is common in prior pruning literature. Avoiding this provides better efficiency.

- The paper demonstrates actual wall-clock speedups on GPU hardware by using customized kernels and asynchronous execution. Many sparsity papers only report theoretical speedups. The systems techniques here are important to realize gains.

- The approach preserves the model quality and in-context learning abilities of the original dense models. Some prior structured pruning work sees degradation on certain tasks. Preserving model capabilities is crucial.

- The paper ablates different design choices like using contextual versus static sparsity. This provides intuition about why the approach works.

Overall, the paper pushes forward the state-of-the-art in efficient large language model inference by proposing a practical and scalable approach that uses contextual sparsity. The design and thorough evaluation on very large models like OPT-175B helps advance the field meaningfully.
