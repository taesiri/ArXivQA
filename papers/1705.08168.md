# [Look, Listen and Learn](https://arxiv.org/abs/1705.08168)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What can be learned by training visual and audio networks simultaneously to predict whether visual information (a video frame) corresponds or not to audio information (a sound snippet)?The authors introduce a novel "Audio-Visual Correspondence (AVC)" learning task to train visual and audio networks from scratch using unlabeled videos. Their goal is to design a system that can learn both visual and audio semantic information in a completely unsupervised manner simply by looking at and listening to a large number of videos. The central hypothesis seems to be that the correspondence between visual and audio streams in videos can provide a valuable supervisory signal for learning representations in both modalities, without needing manual labels.So in summary, the key research question is whether their proposed self-supervised AVC task can produce good visual and audio representations by exploiting the natural co-occurrence of sights and sounds in videos. The paper aims to demonstrate that semantic concepts can emerge in both modalities simply through this audio-visual correspondence training.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a novel self-supervised learning approach for training visual and audio networks simultaneously using unlabeled video. The key ideas are:- Proposing a new "Audio-Visual Correspondence (AVC)" task for networks to predict whether a video frame and audio clip correspond, without any labels. This encourages the networks to learn semantic visual and audio concepts from unlabeled video. - Training visual and audio networks jointly from scratch on this task, rather than fixing pre-trained visual networks as teachers as in prior work.- Showing that the learned representations achieve state-of-the-art on sound classification benchmarks, beating prior methods that use supervision from pre-trained vision networks.- Demonstrating that the visual features perform on par with state-of-the-art self-supervised visual methods on ImageNet classification.- Analyzing what semantics the networks learn, finding they discover fine-grained visual and audio concepts like particular instruments. - Visualizing that the networks localize objects and audio sources despite no explicit localization training.In summary, the key contribution is presenting a novel self-supervised learning approach using the natural correspondence between visual and audio streams in video, and showing it is effective at learning semantically meaningful representations in both modalities. The joint training of both networks together is shown to be beneficial.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel audio-visual correspondence task to learn visual and audio representations from unlabeled video by training networks to predict whether an audio clip and video frame correspond, achieving strong performance on sound classification and competitive results on ImageNet classification compared to other self-supervised approaches.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of self-supervised audio-visual learning:- The key novelty of this paper is in jointly training the visual and audio networks from scratch on unlabeled video, rather than pre-training one or both modalities on labeled data. Most prior work has focused on pre-training the visual network on ImageNet or other labeled datasets before using it to supervise audio network training. By training both networks simultaneously, the authors show improved performance on downstream tasks compared to methods that use fixed pre-trained visual networks.- The proposed audio-visual correspondence (AVC) task is related to prior work on learning joint embeddings for images and audio, such as Owens et al. and Aytar et al. However, those methods use the correspondence as supervision for training only the audio network, keeping the visual network fixed. This work is the first to train both modalities with the AVC task.- For self-supervised visual representation learning, this paper builds on prior work like Doersch et al., Pathak et al., and Noroozi et al. that train on surrogate pretext tasks defined on unlabeled images. The key difference is that this work uses cross-modal correspondence as the pretext task rather than an intra-modal one. The visual features learned here perform comparably to state-of-the-art self-supervised methods on ImageNet classification.- For audio representation learning, this paper significantly outperforms prior self-supervised methods like Aytar et al. as well as fully supervised baselines. This demonstrates the benefit of joint audio-visual training over methods that use fixed visual features as supervision.In summary, the key contribution of this work is in demonstrating that jointly training visual and audio networks from scratch for correspondence yields better representations than pre-training one modality or using fixed pretrained features to supervise the other. The performance gains suggest that allowing both networks to evolve together results in richer learned representations.
