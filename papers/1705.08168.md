# [Look, Listen and Learn](https://arxiv.org/abs/1705.08168)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What can be learned by training visual and audio networks simultaneously to predict whether visual information (a video frame) corresponds or not to audio information (a sound snippet)?The authors introduce a novel "Audio-Visual Correspondence (AVC)" learning task to train visual and audio networks from scratch using unlabeled videos. Their goal is to design a system that can learn both visual and audio semantic information in a completely unsupervised manner simply by looking at and listening to a large number of videos. The central hypothesis seems to be that the correspondence between visual and audio streams in videos can provide a valuable supervisory signal for learning representations in both modalities, without needing manual labels.So in summary, the key research question is whether their proposed self-supervised AVC task can produce good visual and audio representations by exploiting the natural co-occurrence of sights and sounds in videos. The paper aims to demonstrate that semantic concepts can emerge in both modalities simply through this audio-visual correspondence training.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a novel self-supervised learning approach for training visual and audio networks simultaneously using unlabeled video. The key ideas are:- Proposing a new "Audio-Visual Correspondence (AVC)" task for networks to predict whether a video frame and audio clip correspond, without any labels. This encourages the networks to learn semantic visual and audio concepts from unlabeled video. - Training visual and audio networks jointly from scratch on this task, rather than fixing pre-trained visual networks as teachers as in prior work.- Showing that the learned representations achieve state-of-the-art on sound classification benchmarks, beating prior methods that use supervision from pre-trained vision networks.- Demonstrating that the visual features perform on par with state-of-the-art self-supervised visual methods on ImageNet classification.- Analyzing what semantics the networks learn, finding they discover fine-grained visual and audio concepts like particular instruments. - Visualizing that the networks localize objects and audio sources despite no explicit localization training.In summary, the key contribution is presenting a novel self-supervised learning approach using the natural correspondence between visual and audio streams in video, and showing it is effective at learning semantically meaningful representations in both modalities. The joint training of both networks together is shown to be beneficial.
