# [Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL   Using Reinforcement Learning](https://arxiv.org/abs/2403.11205)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Parallel wire-driven legged robots can jump high and continuously due to long acceleration distance, but have no joint angle sensors in minimal configurations. 
- Estimating joint angles from wire lengths causes oscillations due to wire elongation/loosening under high tensions required for jumping.  
- This makes control unstable - in previous work, only 2+ consecutive jumps were achieved in 10/16 tries.

Proposed Solution: 
- Use reinforcement learning (RL) to develop stable continuous jumping in simulation and transfer to real robot.  
- For state inputs, use joint angle sequences instead of joint velocities that are more susceptible to noise from wire oscillations.
- Design reward function to encourage continuous jumping from standing while keeping upright, limiting drift, and avoiding joint limits.
- Add noise to simulation states/actions to match real robot characteristics and enable transfer.

Contributions:
- RL with proposed state/action/reward design enables significantly more stable jumping than a baseline controller, in both noisy and noiseless simulation environments.  
- Using joint angle sequences instead of velocities maintains performance in noisy simulation, unlike baseline and simple velocity-based RL variants.
- The approach transfers to the real robot - noisy velocity estimates still enable 4 consecutive jumps, unlike complete failure with simple velocity-based RL.
- Demonstrates feasibility of using RL for dynamic motions in wire-driven legged robots with stretching wires, an important step towards future speedy and powerful legged robot designs.

In summary, the key ideas are using joint angle sequences instead of velocity estimates in RL state inputs, designing suitable rewards, and adding sim-to-real noise to enable transferring learned policies for stable continuous jumping of a minimal wire-driven monoped legged robot.


## Summarize the paper in one sentence.

 This paper proposes a method to realize continuous jumping motion of a parallel wire-driven monopedal robot by reinforcement learning in simulation and shows its applicability to the actual robot.


## What is the main contribution of this paper?

 This paper presents a method to realize continuous jumping motion by reinforcement learning in a parallel wire-driven monopedal robot called RAMIEL. The main contributions are:

1) They propose a reward function, state representation, and noise modeling tailored for learning jumping behaviors in RAMIEL. This allows stable jumping in simulation compared to a baseline controller.

2) They address the issue of noisy joint velocity estimates due to wire elongation in RAMIEL by using joint angle histories instead of joint velocities in the state representation. This makes the learned policy more robust. 

3) They demonstrate the applicability of the learned policy on the real RAMIEL robot. The robot succeeds in making 4 consecutive jumps based on the policy learned in simulation.

In summary, the main contribution is using reinforcement learning to enable continuous jumping on a minimalistic parallel wire-driven robot in both simulation and the real world despite challenges like noisy velocity estimates. The paper shares insights into designing rewards, states, and noise for sim-to-real transfer of dynamic behaviors.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Parallel wire-driven robot
- Monopedal robot
- RAMIEL 
- Reinforcement learning
- Continuous jumping
- Wire elongation
- Wire vibration
- Joint angle estimation
- Reward function design
- Noise modeling
- Simulation experiments
- Real robot experiments

The paper focuses on enabling continuous jumping for a parallel wire-driven monopedal robot called RAMIEL using reinforcement learning. Key challenges include dealing with noise and oscillations in measurements like wire lengths and joint angle estimates due to wire elongation and vibration. The methods design appropriate reward functions, state representations, and noise models to learn jumping policies in simulation, and demonstrate the approach on the real RAMIEL hardware.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper uses joint torques as the action in reinforcement learning instead of directly using wire tensions. What is the motivation behind this design choice? How does using joint torques help simplify the learning process?

2. The state converter uses an Extended Kalman Filter to estimate joint angles and velocities from wire lengths/velocities. Why was an EKF chosen over other filtering methods? What are some pros and cons of using an EKF in this application?

3. The paper proposes two different state representations - Ours-1 using joint velocities directly, and Ours-2 using previous joint angle values instead. Why does Ours-2 perform better in noisy environments? What is the intuition behind using previous joint angles as part of the state?

4. Noise is added to the states and actions during training. What is the motivation behind adding different types of noise? How were the noise models and hyperparameters chosen? 

5. The reward function has several terms, each serving a different purpose. Why is the reward function designed this way instead of using a single reward related to jumping height?

6. How sensitive is the performance to changes in the reward function weights and hyperparameters? Was any tuning done to arrive at the final reward design?

7. The method is validated in simulation before deployment on the real robot. What simplifying assumptions are made in the MuJoCo simulation? How could the simulation be improved to better match the real-world dynamics?

8. The real robot uses visual odometry to estimate body velocity. What are some limitations of this approach? How can the velocity estimates be made more robust?

9. What other state estimation or control methods were attempted for this robot? Why did the authors choose reinforcement learning over other approaches?

10. The paper mentions some areas of improvement to achieve stable continuous jumping. What do you think are the one or two most important problems that need to be addressed?
