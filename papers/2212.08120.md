# Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue   Systems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can domain-specific knowledge be effectively injected into pre-trained language models to improve their performance on task-oriented dialogue systems, without needing to query external knowledge bases at runtime?The key hypothesis is that light-weight adapter networks can be used as repositories to store domain-specific facts learned from knowledge bases. The adapters can then provide knowledge to guide the predictions of the pre-trained language model when fine-tuned on downstream dialogue tasks. This approach aims to integrate external knowledge into the model parameters, eliminating the need for separate knowledge base queries during inference.In summary, the paper explores adapter-based knowledge injection methods and evaluates whether this enables pre-trained models to produce more knowledgeable and accurate responses in task-oriented dialog systems, compared to not having access to domain facts or having to query them from a separate knowledge base. The proposed Knowledge Probing using Response Selection (KPRS) benchmark is designed to specifically test models' ability to retrieve and reason with injected domain knowledge.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method for injecting domain-specific knowledge into pretrained language models (PLMs) for task-oriented dialogue systems. The key points are:- They use lightweight adapter networks as repositories of domain knowledge that can be integrated with PLMs. The adapters are first trained to memorize knowledge base facts, then the PLMs are fine-tuned to utilize the knowledge in the adapters. - They design a new probing task called Knowledge-Probing using Response Selection (KPRS) to evaluate how well models can retrieve injected knowledge. KPRS uses contrastive response pairs to test if models prefer responses consistent with the knowledge base.- They show improvements on KPRS and response generation from injecting knowledge with adapters compared to strong baselines lacking adapters. This demonstrates the utility of adapter-based knowledge injection.- The approach allows tightly integrating knowledge with PLMs without querying external knowledge bases at runtime. This can simplify deployment of task-oriented dialogue systems.In summary, the key contribution is proposing and demonstrating a method for effectively injecting specialized domain knowledge into PLMs using adapter networks and evaluating this with a new probing task. This improves factual consistency without external knowledge base queries.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using light-weight adapter networks to inject domain-specific knowledge from knowledge bases into pretrained language models to improve their performance on downstream task-oriented dialogue tasks like response generation, without needing to query external knowledge bases at runtime.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on injecting knowledge into language models for task-oriented dialogue:- The use of lightweight adapters as repositories of domain knowledge is a novel contribution. Other approaches like Madotto et al. (2020) fine-tune the entire pretrained language model (PLM), which makes updating knowledge inefficient. Using adapters allows efficient updating without retraining the whole model.- The proposed Knowledge Probing using Response Selection (KPRS) benchmark is also a novel contribution for evaluating factual knowledge in dialogue models. Other knowledge probing tasks like LAMA (Petroni et al. 2019) focus on broader knowledge, not specialized domain knowledge for dialogue. - Representing knowledge base facts as natural language statements rather than relational triples is not entirely new, but the template-based approach here seems effective for the small structured KBs in task-oriented dialogue. Other works like Lu et al. (2021) train generative models on triplets.- Compared to dialogue systems that query knowledge bases at runtime, injecting knowledge into the model parameters eliminates query engineering overhead. However, it is less flexible for rapidly changing KBs.- For multi-domain knowledge, using one mixed adapter performs better than selecting among multiple domain-specific adapters. Fan et al. (2021) combines retrieval and injection which could be a promising direction.Overall, the adapter-based knowledge injection approach seems effective and suited for small, specialized domain KBs in task-oriented dialogue. The KPRS probing task provides a nice way to directly evaluate factual knowledge. The tradeoffs around flexibility vs efficiency compared to querying KBs are clearly analyzed.


## What future research directions do the authors suggest?

The authors suggest several future research directions:- Improve the ability of PLMs to access and utilize the knowledge embedded in adapters for complex reasoning tasks. They note that while injecting knowledge helps make it available, the current approach may not enable sufficiently nuanced reasoning.- Develop domain-agnostic strategies for knowledge injection that do not require manual template design for each new domain. The current reliance on templates introduces some overhead.- Combine knowledge memorization with differential information retrieval, where adapters store some facts while others are fetched from external KBs at runtime as needed. This could combine the benefits of tight integration and real-time updating.- Investigate the impact of KB size on model performance more systematically. The authors found some evidence that larger KBs may be more challenging for PLMs to fully memorize.- Improve the ability of models equipped with multiple single-domain adapters to select the most relevant one(s) for a given context through more expressive fusion functions or training procedures.- Validate the proposed knowledge injection approach with different PLMs beyond BART.- Apply the ideas to a broader range of domains beyond those in MultiWOZ to further assess their general utility.In summary, the main directions are improving reasoning capabilities, reducing the need for manual engineering, combining internal and external knowledge, better handling large KBs, enhancing multi-domain integration, validating across models/domains, and evaluating on more complex reasoning tasks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes methods for injecting domain-specific knowledge into pretrained language models (PLMs) for task-oriented dialogue systems. Rather than querying external knowledge bases at runtime, the authors store specialized knowledge within lightweight adapter modules that are fused with the PLM's internal representations. They convert structured KB entries into natural language statements and train adapters to reconstruct corrupted statements, thereby memorizing the facts. The pretrained PLM is then fine-tuned to leverage the adapter's knowledge when making predictions. To evaluate fact memorization and utilization, the authors design a knowledge probing task called KPRS that contrasts responses consistent or inconsistent with the KB. Experiments demonstrate that PLMs equipped with KB-adapters outperform baselines without adapters on KPRS and response generation, showing they successfully inject and exploit domain knowledge. The modular adapter approach allows efficiently extending models to new domains. Overall, the work demonstrates an effective strategy for integrating specialized factual knowledge into PLMs to benefit task-oriented dialogue.
