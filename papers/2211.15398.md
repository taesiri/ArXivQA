# [Leveraging per Image-Token Consistency for Vision-Language Pre-training](https://arxiv.org/abs/2211.15398)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is:

How can we improve vision-language pre-training methods to learn better associations between visual and textual concepts, overcoming issues like modality bias and underutilization of unmasked tokens? 

The key hypothesis is that explicitly identifying salient visual tokens, generating inconsistent alternatives, and training the model to recognize token-image (in)consistency across all tokens can improve vision-language association learning compared to existing pre-training objectives like masked language modeling.

The authors propose a new pre-training approach called EPIC to test this hypothesis. The main components are:

1) Saliency-based masking to identify visually salient tokens to mask 

2) Inconsistent token generation by replacing masked tokens with alternatives from a language model

3) Image-token consistency task to predict whether each token is consistent with the image

Through experiments on various vision-language models and datasets, the authors demonstrate that EPIC improves performance on downstream tasks compared to strong baselines, supporting their hypothesis. The proposed method allows learning associations between more tokens and images, and reduces reliance on just the textual modality.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing EPIC, a new pre-training approach for vision-language models that aims to learn better vision-language associations. The key ideas are:

1. It introduces an Image-Token Consistency (ITC) task, where the model has to determine for each token in a sentence whether it is consistent with the given image. This allows utilizing both masked and unmasked tokens to learn associations.

2. It generates inconsistent tokens by masking salient words and replacing them with samples from a language model. This forces the model to rely on visual information to identify inconsistent tokens, reducing language bias. 

3. It determines salient words to mask based on the text-image attention scores from a teacher vision-language model. This focuses on masking words strongly related to image content.

4. Experiments show combining EPIC with several state-of-the-art vision-language pre-training methods (ViLT, ALBEF, METER, X-VLM) leads to significant gains on various downstream tasks like image-text retrieval and visual reasoning.

In summary, the main contribution is proposing the EPIC pre-training approach to address limitations like language bias and underutilization of unmasked tokens in existing methods like CMLM, in order to learn better vision-language associations.
