# [Multi-modal Auto-regressive Modeling via Visual Words](https://arxiv.org/abs/2403.07720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing large language models (LLMs) have shown powerful capabilities for language tasks, but extending them to multi-modal inputs is challenging as the continuous image features lack discrete supervised labels needed for auto-regressive pre-training. 

Proposed Solution:
- Introduce the concept of "visual words" which maps image features to probability distributions over the LLM's vocabulary, providing supervision for visual modeling.
- Propose the Visual Word guided Large Multi-Modal Model (VW-LMM) which performs auto-regressive modeling over both text and visual words for a unified objective.
- Explore representing images using pseudo features constructed from visual words and LLM embeddings.

Key Contributions:
- First model to perform unified multi-modal auto-regressive pre-training with discrete visual supervision from "visual words".
- Achieves state-of-the-art results on 5 VQA datasets and 4 benchmark toolkits, outperforming similarly sized models.
- Visualization shows visual words can capture both low-level visual features and high-level semantics.
- Ablations validate the effectiveness of visual words supervision and feasibility of representing images in the LLM's semantic space.

In summary, the paper introduces visual words to provide discrete visual supervision for large multi-modal models, enabling auto-regressive pre-training over both text and images with a unified classification objective. Both quantitative and qualitative results demonstrate the benefits of this approach.
