# [Exploring Transferability for Randomized Smoothing](https://arxiv.org/abs/2312.09020)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper explores transferring certified robustness from foundation models to downstream tasks through a pretraining plus finetuning approach. A key challenge they identify is balancing semantic learning and robustness. They propose expanding the data distribution during pretraining by training on a mixture of clean and noisy images with different noise levels. This allows the model to simultaneously learn rich semantics and robustness. During finetuning on clean downstream images, the model surprisingly maintains certified robustness inherited from pretraining, achieving strong clean accuracy and certified accuracy without compromise. They highlight the role of batch normalization statistics in hindering robustness transfer, and recommend batch-independent normalizations. Experiments across diverse downstream tasks demonstrate their method transfers semantics and robustness effectively using just a single model, outperforming prior work and simplifying model deployment. The study provides useful analysis and techniques to instill certified robustness in foundation models for reliable finetuning across applications.
