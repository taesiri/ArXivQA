# [Towards Compositional Adversarial Robustness: Generalizing Adversarial   Training to Composite Semantic Perturbations](https://arxiv.org/abs/2202.04235)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

1) How to generalize adversarial training from a single threat model (e.g. $\ell_p$ norm bounded perturbations) to defend against multiple/composite semantic perturbations? 

2) How to optimize the perturbation order when combining multiple semantic and $\ell_p$ norm perturbations into a composite adversarial attack?

3) Can the proposed generalized adversarial training (GAT) approach outperform other adversarial training baselines in defending against composite perturbations?

In particular, the authors propose a novel composite adversarial attack method that can find optimal attack compositions by attack order scheduling and component-wise projected gradient descent. They then propose GAT to train models robust to not only single threats like $\ell_\infty$ perturbations, but also combinations of multiple semantic perturbations like changes in hue, saturation, brightness, etc. 

The central hypothesis seems to be that their proposed GAT approach can achieve state-of-the-art robustness against a wide range of single and composite adversarial threats, compared to prior adversarial training methods. The results on ImageNet and CIFAR-10 appear to support this hypothesis.

In summary, the key research questions addressed are: 1) how to achieve compositional robustness via adversarial training, 2) how to optimize composite attacks, and 3) evaluating if the proposed GAT outperforms other adversarial training baselines. The central hypothesis is that GAT can provide robustness against diverse composite adversarial examples.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a novel composite adversarial attack (CAA) method that can generate adversarial examples across multiple perturbation types, including semantic perturbations (e.g. hue, saturation, rotation, etc.) and l_p norm attacks. A key part of CAA is an attack order scheduling algorithm that can find the optimal attack order to maximize the adversarial effect.

2. It proposes a generalized adversarial training (GAT) framework to train models that are robust to composite adversarial attacks. GAT incorporates the proposed CAA method to generate composite adversarial examples during training. 

3. Extensive experiments on CIFAR-10 and ImageNet show that models trained with GAT achieve much higher robust accuracy against composite attacks compared to prior adversarial training methods focused on single perturbation types. For example, GAT improves robust accuracy by 30-60% on semantic attacks and 15-22% on full attacks combining semantic and l_inf attacks.

4. Analysis shows the proposed CAA achieves higher attack success rates compared to random attack ordering, demonstrating the effectiveness of the attack order scheduling algorithm. 

In summary, the main contribution is developing novel attacks and defenses to improve robustness against composite/semantic adversarial perturbations, going beyond the typical single perturbation adversarial training. The proposed GAT framework significantly outperforms prior art in defending against a combinational of multiple perturbation types.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a novel method called generalized adversarial training (GAT) to improve the robustness of deep learning models against composite adversarial attacks involving multiple perturbation types, such as combinations of semantic and Lp-norm attacks.


## How does this paper compare to other research in the same field?

 Here are a few key comparisons between this paper and other related research:

- This paper focuses on improving adversarial robustness against multiple perturbation types, including semantic perturbations like changes in hue, saturation, etc. as well as Lp norm perturbations. Many prior works have focused only on robustness against Lp norms like Linfty. Considering multiple perturbation types together is still an open research problem.

- The proposed generalized adversarial training (GAT) method aims to train models that are robust to combinations of different perturbation types. Most prior adversarial training methods target robustness to a single perturbation model like Linfty adversarial examples. Training for robustness against multiple perturbation types is more challenging.

- A key contribution is the composite adversarial attack (CAA) method that can generate adversarial examples across multiple perturbation types and automatically learn good attack combinations. This is different from prior works that usually consider attacks limited to a single perturbation type.

- The paper shows GAT can improve robustness against a wide range of semantic and norm-based attacks both individually and in combinations. Many previous adversarial training methods have struggled to generalize robustness beyond the particular perturbation type they are trained on.

- The attack order scheduling algorithm used in CAA to find optimal sequences of perturbation types is novel. Most prior composite attacks do not explicitly optimize the attack composition.

Overall, this paper pushes research forward in considering more diverse and realistic adversarial threats during training. The results demonstrate improved generalized robustness compared to prior adversarial training methods focused on individual perturbation models like Linfty. The composite attack methodology and generalized adversarial training approach offer promising directions for future research on more comprehensive adversarial robustness.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring different attack compositions and new threat models beyond the ones studied in this paper. The authors mainly focused on a few semantic perturbations (hue, saturation, brightness, contrast, rotation) plus l-infinity, but there are many other potential semantic or physical threat models that could be incorporated into the composite attack framework.

- Developing more advanced attack scheduling algorithms. The authors propose a basic scheduling approach using relaxed doubly stochastic matrices, but more sophisticated ML-based methods could be developed to learn optimal attack schedules. 

- Generalizing the defense approach to other model architectures and threat models. The authors demonstrate their generalized adversarial training (GAT) on CNNs against the perturbations they considered, but it would be useful to evaluate the effectiveness for other models (e.g. transformers) and attacks.

- Scaling up evaluation to larger and more complex datasets. The authors used ImageNet and CIFAR-10, but testing on higher resolution, more diverse datasets could reveal limitations.

- Considering computational efficiency andMemory overhead during training and attack generation. The proposed methods add complexity, so improving efficiency is important for real-world usage.

- Studying certified or provable robustness to composite perturbations. The empirical defense approach may still have vulnerabilities, so formal verification methods could complement this.

- Analyzing theoretical connections between robustness to single vs. composite perturbations. Further analysis may reveal more fundamental insights about the principles of robustness.

- Developing adaptive, ensemble attack methods. Having attacks that can automatically select and combine perturbations could reveal additional model weaknesses.

Overall, the authors' approach opens up an interesting new direction, and they discuss many worthwhile avenues for extending this line of research on compositional robustness further. Testing new attacks, developing more advanced defenses, scaling up the problem setting, and deeper theoretical analysis seem like particularly promising follow-ons.
