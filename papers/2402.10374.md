# [Revisiting Experience Replayable Conditions](https://arxiv.org/abs/2402.10374)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Experience replay (ER) is considered only applicable to off-policy RL algorithms. However, there is evidence it has been successfully applied to some on-policy algorithms too. 
- This suggests off-policyness may be a sufficient but not necessary condition for applying ER.
- The paper aims to reconsider more strict "experience replayable conditions" (ERC) for applying ER.

Key Idea:  
- Hypothesize that every RL algorithm has a set of "acceptable" experiences that can be replayed without destabilizing learning. 
- Satisfying ERC requires selectively replaying only experiences from this acceptable set.
- The paper reveals policy gradient algorithms correspond to triplet loss optimization in metric learning. This inherits two instability factors:
  (i) Hard negatives: excessive repulsion from negative samples
  (ii) Distribution shift: bias from limited experience triplets
- Proposes two tricks to alleviate instability factors and achieve ERC:
  1) Counteraction: Expand acceptable set by regularizing policy towards behavior policy 
  2) Mining: Selectively replay experiences indistinguishable from current policy

Contributions:
- Reveals instability factors limiting ER to off-policy algorithms
- Develops two stabilization tricks to mitigate these factors 
- Shows an on-policy algorithm (A2C) can achieve ERC using the tricks
- Achieves comparable performance to state-of-the-art off-policy SAC algorithm
- Provides new perspective on experience replayable conditions based on set of acceptable experiences and on-policyness

In summary, the key idea is stabilizing policy optimization via regularization and selective experience replay to satisfy theoretical replayable conditions for on-policy algorithms. This enables experience replay for A2C and achieves strong performance.
