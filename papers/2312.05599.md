# [Not All Data Matters: An End-to-End Adaptive Dataset Pruning Framework   for Enhancing Model Performance and Efficiency](https://arxiv.org/abs/2312.05599)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep neural networks typically require massive datasets for training, but real-world data often contains redundancies and biases which do not contribute to model performance. Using large datasets increases computational/memory costs.  
- Prior dataset pruning techniques rely on hand-crafted, task-specific metrics to score and remove redundant samples, limiting scalability. The paper shows the dataset pruning problem is NP-hard.

Proposed Solution:
- The paper proposes AdaPruner, an end-to-end adaptive dataset pruning framework requiring no manually defined metrics. 
- AdaPruner contains:
   - Adaptive Dataset Pruning (ADP) module: Iteratively prunes redundant samples to reach an expected pruning ratio.  
   - Pruning Performance Controller (PPC): Maintains model performance for accurate pruning.
- AdaPruner loss function combines cross-entropy classification loss for selected samples, a data selection loss encouraging pruning of simple samples, and a compression loss.
- Jointly optimizes sample selection and model fine-tuning. Warm-up mechanism aids precise pruning.

Main Contributions:
- Establishes complexity of general dataset pruning problem (NP-hard).
- AdaPruner enables automatic, adaptive pruning to any ratio without dependence on predefined metrics.
- High scalability to diverse datasets and networks due to no assumptions imposed.
- Experiments show consistency outperforms state-of-the-art methods, enhancing model efficiency (10-30% data reduction) and performance. 
- Analyses validate AdaPruner improves training distribution and model representation learning.
