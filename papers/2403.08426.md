# [Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2403.08426)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation":

Problem:
- Semantic segmentation models rely on large amounts of pixel-level annotations, limiting their generalization to new categories. 
- Zero-shot semantic segmentation (ZS3) aims to segment both seen and unseen classes without annotation of the latter.
- Existing ZS3 methods have issues with overfitting on seen classes and small mask fragmentation.

Proposed Solution:
- Present a Language-Driven Visual Consensus (LDVC) approach to address the issues in ZS3.
- Leverage language as anchors to guide alignment of visual features instead of pushing language toward noisy visual cues. This enhances adaptability while preserving robust generalization ability from CLIP.
- Introduce a Local Consensus Transformer Decoder (LCTD) with route attention in self-attention layers. This reduces mask fragmentation by enhancing local semantic consensus. 
- Apply deep learnable vision-language prompts to further adapt CLIP encoders to segmentation task.

Main Contributions:
- A new LCTD decoder is proposed to alleviate overfitting on unseen classes and reduce small mask fragmentation.
- A vision-language prompt tuning strategy is proposed to generalize the pretrained CLIP model to zero-shot segmentation, further improving unseen class segmentation.
- Experiments show state-of-the-art performance, with mIoU gains of 4.5% on PASCAL VOC and 3.6% on COCO-Stuff for unseen classes.

In summary, the paper presents an effective ZS3 approach called LDVC that leverages language guidance and visual consensus to enhance generalization and reduce fragmentation issues. Vision-language prompt tuning further adapts the pretrained CLIP encoder. Superior experimental results demonstrate the effectiveness of the proposed techniques.
