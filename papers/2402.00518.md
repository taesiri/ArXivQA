# [EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit   Large Language Models](https://arxiv.org/abs/2402.00518)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Training large language models (LLMs) with billions of parameters as early-exit models from scratch requires massive computational resources, training data, and time. However, many practitioners have access to pre-trained standard LLMs and wish to convert them to early-exit LLMs in a more economical manner. 

Proposed Solution: This paper proposes "EE-Tuning", a lightweight two-stage procedure to transform a pre-trained standard LLM into an early-exit LLM by augmenting it with additional early-exit layers and tuning those layers while keeping the original LLM frozen.

Key Points:
- In the first stage, early-exit layers with various architectures are added to the standard LLM at certain depths and initialized by either random initialization or copying parameters from the standard LLM. 
- In the second stage, only the added early-exit layers are tuned on a small dataset with language modeling loss while the standard LLM remains fixed. This tuning process requires negligible resources compared to full pre-training.
- The method supports flexible configurations and is compatible with massive 3D parallelism for scalability.
- Experiments on LLMs up to 70B parameters validate that EE-Tuning requires less than 1/1000 of the original pre-training resources, achieves 1.2-1.6x inference speedup, and maintains comparable or sometimes better scores on downstream tasks.

Main Contributions:
1) Proposal of EE-Tuning, an economical and accessible solution to obtain well-performing early-exit LLMs
2) Efficient implementation supporting various configurations and massive parallelism 
3) Systematic experiments confirming efficacy of EE-Tuning and providing guidelines

In summary, this paper provides an economical and scalable solution to make early-exit LLMs accessible by converting pre-trained standard LLMs, together with thorough empirical validation.
