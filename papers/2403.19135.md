# [Compressing Large Language Models by Streamlining the Unimportant Layer](https://arxiv.org/abs/2403.19135)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) like GPT-3 have shown impressive performance across various natural language tasks. However, their enormous size makes deployment difficult. There is a need for techniques to compress these massive models while retaining performance. Existing approaches like quantization and distillation have limitations. Model pruning techniques have emerged as promising solutions but prior work on structured pruning focuses more on attention heads, filters etc rather than layer pruning.

Method:
This paper proposes LLM-Streamline, a simple and effective layer pruning technique. They first analyze layers in LLMs by measuring the cosine similarity between layer input and output states. Consecutive layers with high similarity induce only small perturbations, suggesting redundancy. Thus, less important layers can potentially be removed. 

They introduce a two-step approach - layer pruning followed by layer replacement. First, they remove a set of consecutive layers with lowest importance based on the cosine similarity metric and target sparsity. Then, they train a lightweight model like a MLP to replace the removed layers and restore performance.

Main Contributions:
- Analyze redundancy across LLM layers using cosine similarity between inputs and outputs 
- Propose simple but effective strategy to prune consecutive less important layers
- Demonstrate that a lightweight MLP with few training samples can effectively replace pruned layers  
- Achieve 92% performance of 7B parameter model on classification tasks and 68% on generation with 25% parameters pruned
- Outperform previous SOTA pruning techniques like ShortGPT across multiple model sizes (1B to 7B parameters)

In summary, the paper provides intuitive analysis to identify less useful layers in LLMs and introduces an remarkably simple yet powerful approach via layer pruning and replacement to compress these massive models.
