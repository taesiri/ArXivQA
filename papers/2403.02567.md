# [Eliciting Better Multilingual Structured Reasoning from LLMs through   Code](https://arxiv.org/abs/2403.02567)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current large language models (LLMs) have shown some progress on reasoning tasks, but prior work has focused only on English and simple reasoning problems. This provides a limited assessment of LLMs' actual reasoning capabilities.

- The authors introduce a new multilingual structured reasoning dataset called xSTREET that covers 6 languages (English, Spanish, Russian, Chinese, Japanese, Arabic) and 4 reasoning tasks (science, arithmetic, logic). 

- Experiments show LLMs struggle on non-English xSTREET problems compared to English, exposing a gap in reasoning abilities.

Proposed Solutions:
- Leverage the hypothesis that LLMs trained on code are better at reasoning. Apply techniques at both train time and inference time.

Train time:
- Create a dataset called TCC by machine translating comments in a source code dataset into 5 languages, keeping code intact.
- Further pre-train a multilingual LLM on TCC using parameter-efficient fine-tuning.

Inference time:  
- Design a novel code prompt format called Sim that incorporates step-by-step function calls and multilingual comments to map better to the reasoning task structure.

Main Contributions:
- Release first comprehensive multilingual structured reasoning benchmark (xSTREET)
- Show training on multilingual code comments enhances reasoning in all languages 
- Propose Sim code prompts that improve multilingual reasoning performance by making model better adhere to reasoning format
- Achieve gains on xSTREET tasks using proposed techniques without losing performance on general NLU benchmarks

The solutions aim to elicit better underlying reasoning from LLMs in a multilingual setting by leveraging code as an intermediate representation. The paper demonstrates improved reasoning across languages is possible without requiring huge proprietary models.
