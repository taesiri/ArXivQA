# [Is Fast Adaptation All You Need?](https://arxiv.org/abs/1910.01705)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether incorporating robustness to catastrophic interference in the meta-learning objective leads to representations that are more suitable for incremental learning, compared to just maximizing fast adaptation like MAML does. The key hypothesis is that directly minimizing interference during meta-training will result in representations that are more conducive for incremental learning than those learned through fast adaptation alone.In summary, the paper investigates if a meta-learning objective that incorporates robustness to catastrophic interference is better than simply maximizing fast adaptation for learning representations suitable for incremental learning. The main hypothesis is that directly minimizing interference is crucial.


## What is the main contribution of this paper?

The main contribution of this paper is investigating whether incorporating robustness to interference in the meta-learning objective improves performance on incremental learning benchmarks. Specifically, the paper compares two meta-learning objectives:1) MAML - Which maximizes fast adaptation and generalization.2) MRCL - Which additionally minimizes interference between updates. The authors find that representations learned by MRCL significantly outperform MAML on incremental learning benchmarks that require online learning from correlated streams of data. This suggests that minimizing interference is important for meta-learning representations suitable for incremental learning, and fast adaptation alone is not sufficient.The key novelty is empirically demonstrating the benefit of the MRCL objective over the popular MAML objective on incremental learning tasks. The paper also proposes a new continual learning prediction task setup for comparing meta-learning algorithms.
