# [I Can't Believe There's No Images! Learning Visual Tasks Using only   Language Supervision](https://arxiv.org/abs/2211.09778)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is whether it is possible to learn high-level visual skills, such as parsing questions, comparing semantics, and generating descriptions, by transferring those skills from natural language training data to computer vision tasks without using any visual training data. 

The key hypothesis is that the high-level semantics needed for many vision and language tasks have significant overlap, and therefore models can learn those skills from natural language data and then apply them to visual inputs if both modalities are encoded into a shared semantic space.

Specifically, the paper proposes an approach called CLOSE that exploits jointly trained contrastive vision-language models like CLIP to embed visual and textual inputs into a common semantic vector space. It then studies whether models trained on textual training data alone in this space can effectively transfer to visual inputs at test time.

The authors evaluate this cross-modal transfer hypothesis on a range of vision and language tasks including image captioning, visual question answering, and visual entailment. Their results generally show a surprisingly small drop in performance compared to models trained with visual data, demonstrating efficient transfer is possible for high-level semantics between vision and language using this approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method for zero-shot cross-modal transfer learning using contrastive models. Specifically:

- They introduce CLOSE, a method that can train models on textual data alone and then apply them to visual inputs at test time. This allows transferring skills learned from text data to visual tasks. 

- They show CLOSE is effective on a range of vision and language tasks including image captioning, visual question answering, visual entailment, and visual news captioning. The text-only CLOSE models achieve results surprisingly close to models trained directly on images.

- They demonstrate an application of CLOSE for stylistic image captioning by training on diverse text sources like books, web data, and language model generations. This allows captioning in different styles without using any labeled image data.

- They analyze CLOSE's robustness to the "modality gap" between text and image vectors in contrastive models. They show it is insensitive to constant offsets and study more complex adapters to align the modalities.

- Overall, the ability to transfer from text data to visual tasks using contrastive models is shown to be not only achievable, but produce strong results. This could enable training on text data which is often cheaper to collect than visual data.

In summary, the key contribution is enabling zero-shot cross-modal transfer for vision tasks by exploiting contrastive models trained on large amounts of text and image data. The effectiveness of learning complex vision skills purely from text data is demonstrated.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a method called CLOSE that allows models to learn visual skills like image captioning, visual entailment, and visual question answering using only natural language training data by leveraging contrastively trained visual-language models like CLIP to map images and text into a shared embedding space.
