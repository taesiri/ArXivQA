# [I Can't Believe There's No Images! Learning Visual Tasks Using only   Language Supervision](https://arxiv.org/abs/2211.09778)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is whether it is possible to learn high-level visual skills, such as parsing questions, comparing semantics, and generating descriptions, by transferring those skills from natural language training data to computer vision tasks without using any visual training data. 

The key hypothesis is that the high-level semantics needed for many vision and language tasks have significant overlap, and therefore models can learn those skills from natural language data and then apply them to visual inputs if both modalities are encoded into a shared semantic space.

Specifically, the paper proposes an approach called CLOSE that exploits jointly trained contrastive vision-language models like CLIP to embed visual and textual inputs into a common semantic vector space. It then studies whether models trained on textual training data alone in this space can effectively transfer to visual inputs at test time.

The authors evaluate this cross-modal transfer hypothesis on a range of vision and language tasks including image captioning, visual question answering, and visual entailment. Their results generally show a surprisingly small drop in performance compared to models trained with visual data, demonstrating efficient transfer is possible for high-level semantics between vision and language using this approach.
