# ["Which LLM should I use?": Evaluating LLMs for tasks performed by   Undergraduate Computer Science Students in India](https://arxiv.org/abs/2402.01687)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT, Google Bard, GitHub Copilot, and Microsoft Copilot show promise for assisting undergraduate CS students on diverse tasks. 
- However, there is a lack of comprehensive research evaluating and comparing different LLMs on tasks commonly performed by these students.

Methodology:  
- The authors systematically evaluated 4 LLMs - ChatGPT, Google Bard, GitHub Copilot, Microsoft Copilot across 5 common undergraduate CS tasks:
   1. Code explanation and documentation
   2. Solving class assignments 
   3. Preparing for technical interviews
   4. Learning new CS concepts/frameworks
   5. Writing emails
- For each task, a relevant dataset was created and both quantitative and qualitative analyses were performed to judge the LLMs' strengths and weaknesses.

Key Findings:
- No single LLM performed the best across all tasks. Each LLM showed varying competencies for different tasks:
  - Microsoft Copilot was best for code explanation, solving theoretical assignments and writing humanities assignments
  - GitHub Copilot Chat was best for solving programming assignments and technical interview preparation
  - ChatGPT was best for writing emails and on par with GitHub Copilot for technical interviews
  - Google Bard was best for explaining new CS concepts and frameworks
- The strengths and limitations of each LLM on every task were highlighted through quantitative ratings and qualitative analyses.

Recommendations:
- Guidelines provided for students and instructors to select the most appropriate LLM based on their specific needs and task type, rather than using any single LLM blindly. 
- Suggestions given for constructive usage of LLMs in line with academic integrity and avoiding plagiarism.

Main Contributions:  
- First comprehensive study evaluating and comparing multiple LLMs strictly in the context of undergraduate CS education across both technical and non-technical tasks.
- Quantitative scores and qualitative insights that can guide students and instructors in effectively leveraging the capabilities of different LLMs.
- Recommendations for judiciously using LLMs to enhance teaching and learning while exercising caution regarding accuracy.


## Summarize the paper in one sentence.

 This paper systematically evaluates and compares ChatGPT, Microsoft Copilot, GitHub Copilot Chat, and Google Bard across diverse tasks relevant for undergraduate computer science students to provide guidance on selecting the most suitable LLM for specific tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is a comprehensive evaluation and comparison of four large language models (LLMs) - ChatGPT, Microsoft Copilot, GitHub Copilot Chat, and Google Bard - across a diverse set of tasks commonly performed by undergraduate computer science students in India. 

Specifically, the key contributions are:

1) Systematic assessment of the capabilities and limitations of these LLMs for tasks like code explanation, solving class assignments, technical interview preparation, learning new concepts/frameworks, and writing emails.

2) Development of tailored datasets for evaluating performance on these tasks.

3) Quantitative analysis showing which LLM performs the best for each task type. No single LLM is the top performer across all tasks.

4) Qualitative analysis providing insights into the strengths and weaknesses of each LLM's responses.

5) Recommendations and suggestions for both students and instructors on effectively leveraging these LLMs.

In summary, this is the first comprehensive study focused specifically on guiding the usage of LLMs by undergraduate computer science students in India across their diverse academic needs. The comparative evaluation and recommendations are the key contributions.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the main keywords or key terms associated with this paper include:

- Large language models (LLMs)
- ChatGPT
- Microsoft Copilot
- GitHub Copilot Chat  
- Google Bard
- Undergraduate computer science education
- Code explanation and documentation
- Class assignments
- Technical interview preparation  
- Learning new concepts and frameworks
- Writing emails
- Strengths and weaknesses
- Comparative evaluation
- Recommendations
- India

The paper evaluates and compares the capabilities of ChatGPT, Microsoft Copilot, GitHub Copilot Chat, and Google Bard across diverse tasks relevant to undergraduate computer science students. It assesses the models' effectiveness in areas like providing code explanations, solving programming assignments and theoretical problems, technical interview preparation, comprehending complex concepts, and writing emails. A key focus is identifying each model's strengths and limitations when addressing the needs of undergraduate students in the Indian context. The comparative analysis aims to guide students and instructors in selecting suitable LLMs based on the requirements of specific tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methodology proposed in this paper:

1. The paper evaluates 4 LLMs on 5 different tasks. Do you think there is benefit in evaluating more LLMs or more tasks? What other LLMs or tasks would be useful to include in future work?

2. The authors use both quantitative analysis (ratings/scores) and qualitative analysis to evaluate the LLMs. What are the relative merits and limitations of each evaluation approach? How could the evaluation methodology be improved?  

3. For the code explanation task, what other metrics beyond reasoning, placement and clarity could be used to rate the LLM responses? How would using additional metrics impact the results?

4. In the class assignments section, assignments from operating systems and algorithms courses are used. What insights could be gained by using assignments from other core CS courses? Would the relative LLM performances change?

5. For the interview prep task, 150 Leetcode questions across difficulty levels were used. Would the results change if more questions were used or if the distribution of difficulties was altered?

6. The learning concepts/frameworks section uses a custom dataset of 13 prompts. How was this dataset created? What steps were taken to reduce bias in prompt design? How could the dataset be expanded?

7. For the email writing task, what considerations went into designing 'scenarios faced by CS undergrads'? Could any scenarios have been overlooked that would provide additional insights?

8. The authors provide LLM selection recommendations for students/instructors. How were these recommendations formulated? What additional evidence is needed to validate them fully?

9. The limitations mention updating evaluations as new LLMs emerge. What challenges does this present for producing timely and relevant recommendations? How can evaluations keep pace?

10. Beyond evaluating accuracy, what other criteria could be assessed regarding LLM responses, such as originality, creativity, logical coherence or relevance? Would adding assessments of these qualities alter the conclusions?
