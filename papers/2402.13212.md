# [Soft Self-Consistency Improves Language Model Agents](https://arxiv.org/abs/2402.13212)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) can generate multiple candidate solutions to improve performance over greedy decoding, a technique known as "sample and select". 
- A popular approach is self-consistency (SC), which samples multiple solutions via chain-of-thought prompting and selects the majority vote.  
- However, SC runs into issues in interactive domains like bash program generation, online shopping, and text games, where the action space is very large. Identical actions are unlikely to be sampled multiple times, causing voting to fail.

Proposed Solution: 
- The authors propose Soft Self-Consistency (Soft-SC), which relaxes the hard voting criterion of SC into a continuous score based on token probabilities from the LLM.
- For each sampled action, Soft-SC aggregates the probabilities of each token into a score using min, mean or product. The highest scoring action is selected.
- An adaptive variant stops sampling more actions when a confidence threshold is reached based on minimum token probabilities.

Main Contributions:
- Soft-SC outperforms SC for the same number of samples, with gains of 1.3-6.6% on bash, shopping and text games. 
- It exhibits better sample efficiency than SC, achieving comparable or better performance with fewer samples.
- It continues providing gains as model size increases, allowing smaller LMs with Soft-SC to match larger LMs with SC.
- It can score outputs from black box models like GPT-3.5 and GPT-4 to give 4% gains over SC.
- Calibration is not needed for good Soft-SC performance.
