# [Unmasked Teacher: Towards Training-Efficient Video Foundation Models](https://arxiv.org/abs/2303.16058)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that an image foundation model like CLIP can be used as an "unmasked teacher" to train a video transformer from scratch in a data-efficient and computationally-efficient manner. 

The key ideas are:

- Using CLIP as a teacher provides semantic guidance to help the video model learn effective spatiotemporal representations from limited data.

- Masking out most video tokens reduces computational costs. Selectively aligning unmasked tokens with CLIP provides supervision while avoiding pixel-level reconstruction.

- A two-stage progressive pre-training framework enables handling both video-only tasks and video-language tasks.

- Compared to prior video models relying on image foundation models or reconstruction, this approach is more data-efficient, time-efficient, and results in better performance on various benchmarks.

In summary, the central hypothesis is that utilizing CLIP as an unmasked teacher can overcome limitations of prior work and enable better video foundation models with less data and computation. The experiments aim to validate the effectiveness and efficiency of this proposed learning paradigm.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposes a training-efficient method for temporal-sensitive video foundation models. It integrates the benefits of existing methods by using an image foundation model (CLIP) as an unmasked teacher to train a video transformer (ViT) from scratch.

2. Presents a progressive pre-training framework with two stages: 

- Stage 1 pre-trains only with videos using masked modeling and unmasked teacher alignment, resulting in a model good at video-only tasks. 

- Stage 2 continues pre-training with large-scale image-text data using multiple objectives like masked language modeling, enabling video-language understanding.

3. Achieves state-of-the-art results on various downstream tasks including action recognition, spatiotemporal localization, video-text retrieval and video QA. The model is trained efficiently using only public sources within 6 days on 32 A100 GPUs.

4. The simple and reproducible framework significantly reduces training costs compared to prior arts. It also enables building environmentally friendly video foundation models.

In summary, the key contribution is an efficient training paradigm that integrates the benefits of existing methods to learn temporal-sensitive and multimodal-friendly video representations from scratch. The resulting model achieves strong performance on diverse video tasks with low training requirements.
