# [ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models](https://arxiv.org/abs/2312.01305)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Generating novel views of objects from a single image is an important problem in computer vision. It requires understanding the 3D structure of an object from a 2D image and rendering high-quality, spatially consistent new views from desired camera poses. Recent diffusion model-based methods can generate high-quality images but often lack consistency in terms of both content and viewing angle. Making them more consistent requires costly retraining or combining them with 3D methods, which can hurt image quality.

Method: 
The key idea is to formulate novel view synthesis as generating a video where the camera moves around the object from the input view to the target view. This allows leveraging powerful priors from pre-trained video diffusion models to improve consistency without any training. 

Specifically, the method:
1) Creates a smooth camera path from input to target view using spherical linear interpolation
2) Initializes latents for each frame and denoises them using both a view-conditioned diffusion model (Zero-1-to-3 XL) and a video diffusion model (ZeroScope)
3) Linearly reduces the video diffusion model's influence over time to prevent over-smoothing

The combined guidance from both models results in novel views that are more consistent in content and viewing angle compared to using only the view-conditioned model.

Contributions:
- Shows how video diffusion can improve novel view synthesis from single images without any training
- Proposes a simple and effective way to combine view-conditioned and video diffusion models 
- Introduces a new optical flow based metric to better evaluate consistency
- Outperforms recent state-of-the-art methods, especially as target view deviates more from the input

The method provides significantly improved consistency while retaining competitive image quality by exploiting video diffusion as a strong prior. It demonstrates the promise of leveraging pre-trained models in complementary ways without costly training.
