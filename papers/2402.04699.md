# [EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World   Illusions](https://arxiv.org/abs/2402.04699)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions":

Problem:
Deep neural networks are vulnerable to adversarial attacks - small perturbations to inputs that cause misclassification. Prior approaches for generating adversarial examples often rely on white-box access to models, alter image distributions compared to training data, or are tailored to specific classifiers. This limits their applicability in black-box real-world scenarios.  

Proposed Solution:
The paper proposes EvoSeed, a novel framework to generate natural adversarial examples using evolutionary strategies. EvoSeed requires only black-box access to an auxiliary conditional diffusion model $G$ and classifier model $F$. It leverages CMA-ES to optimize an initial seed vector $z$ such that when processed by $G$, results in an image $x$ misclassified by $F$. An $L_\infty$ constraint on $z$ ensures $x$ remains close to $G$'s training distribution.

Key Contributions:

- Black-box framework to generate natural adversarial examples using conditional diffusion models and evolutionary optimization 

- Demonstrates EvoSeed can find high quality adversarial examples that: 
   - Fool robust and non-robust classifiers
   - Have Fr√©chet Inception Distance and Inception Scores close to clean data
   - Transfer to different classifier architectures

- Analysis reveals inherent adversarial patterns between labels exist across classifiers and training distributions

- First application of evolutionary strategies for generating natural adversarial examples in conditional diffusion models

The work unveils vulnerabilities of classifiers to natural distribution shifts. Further research into enhancing model robustness against such real-world adversarial attacks is motivated.
