# [Imagination is All You Need! Curved Contrastive Learning for Abstract   Sequence Modeling Utilized on Long Short-Term Dialogue Planning](https://arxiv.org/abs/2211.07591)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses that this paper explores are:1) Do chit-chat conversations have planning capability? (RQ1) The paper hypothesizes that even chit-chat conversations can be considered goal-oriented, based on literature from psychology showing conversation is a goal-directed process. The authors test this by evaluating the planning capability of chit-chat dialogues over multiple turns using their proposed evaluation techniques.2) What characteristics make dialogue planning possible? (RQ2)The paper hypothesizes there may be significant differences in the plannability of different intents (e.g. requests vs informing). The authors analyze the planning capability on dialogues with different intents to investigate what factors influence plannability.The main technical contribution is proposing Curved Contrastive Learning (CCL) to learn utterance embeddings that capture conversational position and directional awareness. CCL is evaluated on three dialogue planning tasks: short-term planning, next utterance selection, and long-term planning. A key hypothesis seems to be that the curved embeddings can enable more effective planning compared to prior approaches.In summary, the two main research questions focus on analyzing the planning capability and characteristics of chit-chat dialogues. The curved contrastive learning method is proposed as a technical approach to improve planning, and is evaluated on planning tasks to test its effectiveness.


## What is the main contribution of this paper?

This paper introduces Curved Contrastive Learning (CCL), a novel technique for learning semantic sentence embeddings that capture relative turn distances between utterance pairs in dialogues. The key contributions are:- CCL trains embeddings to be "curved" based on the turn distance between utterance pairs, inspired by the curvature of spacetime in physics. The cosine similarity between embeddings indicates the relative distance between utterances. - CCL embeddings can be used for zero-shot goal-oriented dialogue without full sequence encoding, just by encoding the current and goal utterances. This enables more efficient sequence modeling.- The embeddings are applied to short-term planning (guiding response generation to goals), next utterance selection, and long-term planning (ordering future goals). Strong performance is shown on all tasks.- Analysis provides evidence that even chit-chat dialogues exhibit planning capabilities, with the ability to plan utterances over multiple turns. Certain characteristics like request intents make conversations more "plannable".- CCL shows higher efficiency than baselines like DialogRPT, by encoding only the last utterance during inference thanks to the relativistic curvature.In summary, the key innovation is the curved training technique to induce useful properties in sentence embeddings for dialogue tasks, enabling more efficient sequence modeling and planning capabilities. The analysis also provides new insights into planning in chit-chat dialogues.
