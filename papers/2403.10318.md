# [Anytime Neural Architecture Search on Tabular Data](https://arxiv.org/abs/2403.10318)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The increasing demand for tabular data analysis calls for transitioning from manual to automated neural architecture search (NAS). This requires an efficient and responsive "anytime NAS" approach that can return a good architecture within any time budget and continue refining it as more budget becomes available. However, anytime NAS for tabular data remains unexplored. Existing NAS approaches rely on computationally expensive training-based evaluation or training-free methods designed for vision tasks. How to combine their strengths and enable anytime NAS on tabular data is an open challenge.

Proposed Solution:
The paper proposes ATLAS, the first anytime NAS tailored for tabular data. It introduces a novel two-phase filtering-and-refinement scheme with joint optimization that combines training-free and training-based evaluation. 

In the filtering phase, ATLAS uses a new zero-cost proxy called ExpressFlow to efficiently estimate performance and explore the search space. ExpressFlow captures both trainability and expressivity of architectures.

In the refinement phase, ATLAS accurately evaluates the most promising architectures using training. It uses a budget-aware scheduler to allocate more budgets to more promising candidates.

A coordinator jointly optimizes the two phases to balance exploration and exploitation and meet the time budget constraint.

Main Contributions:

1) Constructs a NAS benchmark for tabular data called NAS-Bench-Tabular with 160,000 architectures.

2) Proposes ExpressFlow, a new zero-cost proxy tailored for tabular data that outperforms existing proxies.

3) Develops ATLAS, the first anytime NAS for tabular data using a novel filtering-and-refinement scheme and joint optimization.

4) Empirically shows ATLAS reduces search time by up to 82.75x and achieves anytime NAS on tabular datasets, outperforming state-of-the-art approaches.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes ATLAS, the first anytime neural architecture search approach tailored for tabular data, which introduces a novel two-phase filtering-and-refinement optimization scheme combining training-free and training-based evaluations to efficiently explore the search space and accurately identify high-performing architectures within any predefined time budget.


## What is the main contribution of this paper?

 This paper proposes ATLAS, an anytime neural architecture search approach tailored for tabular data. The main contributions are:

1. It constructs a comprehensive NAS benchmark for tabular data called NAS-Bench-Tabular, which contains over 160,000 neural architectures along with their training statistics.

2. It proposes a new zero-cost proxy called ExpressFlow to efficiently estimate neural architecture performance for tabular data by capturing both trainability and expressivity.

3. It introduces a novel two-phase optimization scheme combining efficient filtering based on the ExpressFlow proxy and effective refinement using training-based evaluation. The two phases are jointly optimized via a budget-aware coordinator.  

4. It empirically demonstrates that ATLAS significantly outperforms existing NAS approaches, reducing architecture search time by up to 82.75x and supporting anytime NAS for tabular data.

In summary, the key contribution is the proposal of ATLAS, the first anytime neural architecture search approach for tabular data, which leverages a tailored proxy and two-phase optimization scheme to efficiently search neural architectures within a given time budget.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Neural Architecture Search (NAS): The process of automating architecture design to find optimal neural network architectures for a dataset.

- Anytime NAS: A type of NAS that can return good architectures within any predefined time budget and continue refining results as more budget becomes available.

- Training-free evaluation: Quickly estimating an architecture's performance by calculating statistics without expensive full training. Also called zero-cost proxies.

- Training-based evaluation: Accurately assessing an architecture's performance by fully training it.

- Filter phase: Efficiently exploring a large set of candidate architectures using training-free evaluation to filter out less promising ones. 

- Refine phase: Accurately evaluating the most promising architectures using training-based evaluation.

- Coordinator: A component that balances and jointly optimizes the filter and refine phases to enable anytime NAS. 

- Expressivity: The complexity of functions an architecture can represent. Related to depth and width.

- Trainability: The degree an architecture can be effectively optimized via gradient descent. Related to parameter importance.

- Neuron saliency: A measure proposed in this work to characterize both trainability and expressivity by aggregating neuron importance.

- NAS-Bench-Tabular: A NAS benchmark dataset for tabular data introduced in this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) The paper proposes a new zero-cost proxy called ExpressFlow for evaluating neural network architectures on tabular data. How does ExpressFlow capture both the trainability and expressivity of an architecture compared to prior proxies? 

2) The paper introduces a two-phase optimization scheme with a filtering phase and a refinement phase. What are the strengths and weaknesses of each phase and how does the proposed method balance them?

3) How does the proposed budget-aware coordinator work to enable anytime neural architecture search? What constraints and objective function does it employ? 

4) The paper constructs a new benchmark called NAS-Bench-Tabular with over 160,000 neural networks. What statistics does this benchmark provide for each architecture and what insights did the authors gain from analyzing it? 

5) How does the neuron saliency used in ExpressFlow differ from the commonly used synaptic saliency? What theoretical justification is provided for using neuron saliency instead?

6) What modifications need to be made to ExpressFlow when using the ReLU activation function? How does this make ExpressFlow data-agnostic?

7) What ablation studies did the authors perform for hyperparameters of ExpressFlow like parameter positivity, initialization schemes, and batch sizes? What were the key findings?

8) The paper compares the proposed approach against several baselines using both training-based and training-free evaluations. What combination strategies did they benchmark and how much speedup does the proposed method achieve?

9) One-shot NAS methods have gained popularity recently. How well does weight sharing perform on the tabular datasets compared to ExpressFlow and why does it fail to work?

10) The paper claims anytime NAS capability for the proposed method. How does search time versus accuracy tradeoff compare against training-based methods and across different time budgets? What trend is observed?
