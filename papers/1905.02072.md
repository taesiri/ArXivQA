# [Characterizing the invariances of learning algorithms using category   theory](https://arxiv.org/abs/1905.02072)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we use concepts from category theory to formally characterize the invariances possessed by different learning algorithms?In particular, the paper aims to provide a framework for specifying:- The types of predictor and target spaces a learning algorithm can accept (the objects in categories X and Y). - The transformations of these spaces that the algorithm must be invariant to (the morphisms in X and Y).- Invariance to permutations of the training examples (captured by a category I). - How training datasets and learned functions should transform under these invariances (formalized by functors D and P).- The definition of an invariant learning algorithm as a natural transformation between D and P.The research then applies this framework to characterize and contrast the invariances of two specific algorithms - linear regression and ridge regression. The goal is to provide a generalizable approach for formalizing algorithmic invariances using category theory concepts.


## What is the main contribution of this paper?

The main contribution of this paper is to formalize the invariances of learning algorithms using concepts from category theory. Specifically:- It defines how the invariances an algorithm possesses can be formalized by categories of predictor and target spaces (whose morphisms represent the algorithm's invariances) and an index category (whose morphisms represent permutations of the training examples). - It shows that an invariant learning algorithm can be formalized as a natural transformation between functors representing training datasets and learned functions. The naturality conditions encode the requirement for the algorithm to transform predictably under changes to the training data.- It illustrates the framework by characterizing and contrasting the invariances of linear regression and ridge regression. The analysis shows that linear regression is invariant under arbitrary invertible linear maps of the predictors, while ridge regression is only invariant under orthogonal transformations. - The authors suggest the framework provides a useful way to characterize and compare the invariances possessed by different learning algorithms more generally.In summary, the key contribution is the categorical framework for formally characterizing invariance of learning algorithms, which is illustrated through application to linear and ridge regression. This formalizes and extends intuitive notions about invariances these algorithms possess.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using concepts from category theory to formalize the invariances of learning algorithms, that is, the ways training data can be transformed without changing the learned function.
