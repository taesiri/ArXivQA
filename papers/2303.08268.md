# Chat with the Environment: Interactive Multimodal Perception Using Large
  Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question this paper addresses is:How can large language models (LLMs) be leveraged to enable robots to perform interactive multimodal perception and make reasonable decisions by examining objects to uncover their latent properties needed to complete a task?The key hypothesis appears to be that LLMs can provide high-level planning and reasoning skills to control interactive robot behavior in a multimodal environment, while multimodal perception modules help ground the LLMs in the environmental state and extend their processing capabilities.The paper develops an interactive perception framework with an LLM backbone that allows the robot to take epistemic actions to gather multimodal sensory information (vision, sound, haptics, proprioception) needed to resolve uncertainty and successfully execute instructed tasks. Experiments demonstrate the approach on an object picking task requiring the robot to uncover latent object properties through interactive perception guided by the LLM's reasoning.In summary, the central research question is how LLMs can enable robots to perform interactive multimodal perception for uncovering latent environmental properties needed for task completion. The key hypothesis is that the reasoning abilities of LLMs combined with multimodal perception modules can achieve this effectively.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It establishes a robot manipulation scenario with multimodal sensory data and language descriptions for interactive perception.2. It proposes Matcha, a multimodal interactive agent that uses an LLM backbone to "chat" with the environment. The LLM instructs epistemic actions to gather information, reasons over multimodal feedback, and plans task execution.  3. It demonstrates that LLMs can perform interactive multimodal perception and provide behavior explanations, enabling a robot to make reasonable decisions by examining objects to clarify latent properties needed to complete tasks.In summary, the key contribution is showing how LLMs can provide high-level planning and reasoning skills to control interactive robot behavior in a multimodal environment, while multimodal perception modules help ground the LLM. The framework is flexible and can scale to new actions, modalities, and scenarios.
