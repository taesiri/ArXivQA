# [Frugal LMs Trained to Invoke Symbolic Solvers Achieve   Parameter-Efficient Arithmetic Reasoning](https://arxiv.org/abs/2312.05571)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) exhibit mathematical reasoning abilities, but this requires exorbitantly large models with billions of parameters. 
- Smaller LMs struggle with multi-step reasoning for math word problems.
- Fully finetuning smaller LMs for math reasoning leads to loss of generalizability and is computationally expensive.

Proposed Solution:
- Propose SYReLM, which uses a small frozen LM with a lightweight adapter module. 
- The LM acts as a translator to map natural language math questions into a formal language (FL) description that is executed by a symbolic solver to get the answer.
- Use policy gradient reinforcement learning to train the adapter module, with rewards based on correctness of the symbolic solver's output and coverage of variables from the original question.

Key Contributions:
- Shows that much smaller LMs can achieve strong arithmetic reasoning performance when posed as a formalize-then-solve task instead of entirely reasoning within the LM.
- Adapter-based finetuning provides parameter efficiency compared to full finetuning of small LMs.
- Reinforcement learning with a reasoning-specific reward function enables the model to learn better generalizability across diverse reasoning steps.  
- Achieves large performance gains over base LMs and recent methods like Toolformer, while keeping the setup interpretable and accessible.
- Establishes the validity of applying grade-school pedagogical principles to teaching AI models.

The key insight is to detach linguistic, analytical and logical processing into an LM, formal solver and symbolic solver, while using an adapter-based approach to enable smaller LMs to achieve strong arithmetic reasoning. This shows promise for frugal LMs compared to large opaque LLMs.
