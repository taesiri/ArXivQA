# [Tracking Everything Everywhere All at Once](https://arxiv.org/abs/2306.05422)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to estimate long-range, dense motion trajectories for all pixels in a video that are globally consistent and can track points through occlusions. The key hypothesis is that representing a video using a quasi-3D canonical volume mapped to per-frame local volumes through neural network parameterized bijections can capture complex scene and camera motions while ensuring cycle consistency. This representation allows tracking all pixels through occlusions and consolidation of noisy, incomplete input correspondence into complete, accurate motion trajectories spanning entire videos.In summary, the core research question is how to produce dense, long-range, and globally consistent motion trajectories for videos. The key hypothesis is that a quasi-3D canonical representation with learned neural bijective mappings can achieve this goal.


## What is the main contribution of this paper?

This paper presents a new method called OmniMotion for estimating dense, long-range motion trajectories in videos. The key ideas and contributions are:- Introduces a global motion representation called OmniMotion that uses a canonical 3D volume mapped to per-frame local volumes through neural network parametrized bijections. This allows representing complete motion trajectories for all pixels.- Can track pixels through occlusions by mapping pixels to 3D and reasoning about visibility and depth ordering.- Optimization based approach that consolidates noisy input correspondences (e.g. optical flow) into a globally consistent motion representation for the full video. - Evaluated on the TAP-Vid benchmark and shows state-of-the-art performance in tracking accuracy, occlusion handling, and temporal coherence compared to prior dense tracking methods.- Qualitative results on real videos demonstrate the ability to track complex nonrigid motions through long occlusions.In summary, the key contribution is a new representation and optimization method for estimating complete, globally consistent, long-range dense motion trajectories for all pixels in a video, even through occlusions. This addresses a major limitation of prior optical flow and tracking methods. The experiments demonstrate significant improvements in tracking performance compared to previous state-of-the-art techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called OmniMotion that represents videos using a quasi-3D canonical volume mapped to local volumes through neural network bijections, allowing dense, long-range pixel motion trajectories to be estimated while ensuring global cycle consistency and enabling tracking through occlusions.In short, the paper introduces a novel motion representation that can track all pixels in videos over the full sequence length, even through occlusions, while maintaining globally consistent motion estimates.
