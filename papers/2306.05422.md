# [Tracking Everything Everywhere All at Once](https://arxiv.org/abs/2306.05422)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to estimate long-range, dense motion trajectories for all pixels in a video that are globally consistent and can track points through occlusions. The key hypothesis is that representing a video using a quasi-3D canonical volume mapped to per-frame local volumes through neural network parameterized bijections can capture complex scene and camera motions while ensuring cycle consistency. This representation allows tracking all pixels through occlusions and consolidation of noisy, incomplete input correspondence into complete, accurate motion trajectories spanning entire videos.In summary, the core research question is how to produce dense, long-range, and globally consistent motion trajectories for videos. The key hypothesis is that a quasi-3D canonical representation with learned neural bijective mappings can achieve this goal.


## What is the main contribution of this paper?

This paper presents a new method called OmniMotion for estimating dense, long-range motion trajectories in videos. The key ideas and contributions are:- Introduces a global motion representation called OmniMotion that uses a canonical 3D volume mapped to per-frame local volumes through neural network parametrized bijections. This allows representing complete motion trajectories for all pixels.- Can track pixels through occlusions by mapping pixels to 3D and reasoning about visibility and depth ordering.- Optimization based approach that consolidates noisy input correspondences (e.g. optical flow) into a globally consistent motion representation for the full video. - Evaluated on the TAP-Vid benchmark and shows state-of-the-art performance in tracking accuracy, occlusion handling, and temporal coherence compared to prior dense tracking methods.- Qualitative results on real videos demonstrate the ability to track complex nonrigid motions through long occlusions.In summary, the key contribution is a new representation and optimization method for estimating complete, globally consistent, long-range dense motion trajectories for all pixels in a video, even through occlusions. This addresses a major limitation of prior optical flow and tracking methods. The experiments demonstrate significant improvements in tracking performance compared to previous state-of-the-art techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called OmniMotion that represents videos using a quasi-3D canonical volume mapped to local volumes through neural network bijections, allowing dense, long-range pixel motion trajectories to be estimated while ensuring global cycle consistency and enabling tracking through occlusions.In short, the paper introduces a novel motion representation that can track all pixels in videos over the full sequence length, even through occlusions, while maintaining globally consistent motion estimates.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on video motion estimation:- It proposes a new global representation for dense, long-range motion estimation called OmniMotion. This is different from prior work like optical flow that estimates motion locally, or particle video/tracking that produces sparse motion. - The OmniMotion representation uses a quasi-3D canonical space and learnable 3D bijections between local and canonical frames. This provides a flexible way to model complex combinations of camera and object motion. In contrast, other video representations like layered models use explicit layered decompositions.- The paper presents a test-time optimization approach that consolidates noisy input correspondence into a globally consistent motion representation for each video. This is different from learning-based methods that estimate correspondence in a feedforward manner.- Evaluations on the TAP-Vid benchmark show the paper's method outperforms prior state-of-the-art techniques by a large margin in tracking accuracy and coherence. This demonstrates the benefits of the global optimization approach.- The paper focuses on the problem of dense, long-range motion estimation. This sets it apart from other works using similar neural representations that target novel view synthesis or video decomposition.In summary, the key innovations are the OmniMotion representation for global dense motion estimation, the test-time optimization approach, and demonstrations of significant improvements over existing methods on a challenging benchmark. The global optimization framework and flexible quasi-3D representation are novel compared to prior learning-based and classical techniques in this space.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Improving the scalability of the flow collection process. The current approach of exhaustively computing all pairwise flows scales quadratically with sequence length. The authors suggest drawing inspiration from structure from motion and SLAM methods to develop more efficient alternatives like using vocabulary trees or keyframe-based matching.- Accelerating the optimization process. Like other methods using neural implicit representations, the optimization process is relatively slow. The authors suggest leveraging recent work like Instant-NGP and Block-NeRF to speed up this optimization and enable scaling to longer sequences.- Addressing limitations in handling complex non-rigid motion and thin structures. The current method struggles with highly non-rigid motion and thin structures where the input flow methods fail to provide reliable correspondence. Exploring ways to make the optimization more robust in these cases is suggested. - Improving robustness to initialization and avoiding sub-optimal local minima. Due to the non-convex optimization problem, the results can sometimes be sensitive to initialization. The authors suggest this could potentially be addressed through better initialization strategies.- Extending the approach to jointly optimize multiple videos and leverage more context. The current approach optimizes each video independently. Exploring ways to jointly optimize over multiple videos to take advantage of more shared context is proposed.- Applying the representation to other applications like novel view synthesis. In addition to tracking, the global scene representation could potentially enable novel view rendering and interpolation.In summary, the main future directions relate to improving scalability, robustness, and generalization of the approach to handle more complex videos and applications. The optimization process and handling of challenging non-rigid motion are called out as specific areas for improvement.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a new test-time optimization method for estimating dense, long-range motion trajectories for all pixels in a video sequence. The key idea is representing the video using a quasi-3D canonical volume mapped to per-frame local volumes through neural network parameterized bijections. This representation allows propagating information across the entire video to produce complete trajectories even through occlusions while maintaining global consistency. The method takes noisy pairwise correspondences like optical flow as input and optimizes the mapping networks to consolidate these into accurate long-range trajectories. Experiments on benchmark datasets demonstrate state-of-the-art performance in trajectory accuracy and temporal coherence. The proposed OmniMotion representation is shown to be effective at tracking all pixels over full video lengths, handling complex scenarios with both camera and object motion.
