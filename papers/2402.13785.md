# [Synthesis of Hierarchical Controllers Based on Deep Reinforcement   Learning Policies](https://arxiv.org/abs/2402.13785)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Addressed:
- Developing control policies for environments modeled as Markov Decision Processes (MDPs). Two main approaches have complementary strengths and weaknesses: 
1) Reinforcement learning (RL) allows training policies in large, unknown environments but struggles with sparse rewards.  
2) Reactive synthesis provides formal guarantees and supports logical objectives but does not scale.
- The paper considers hierarchical MDPs - modeled as a graph where vertices are populated with "room" MDPs. Goal is obtaining a hierarchical controller: low-level RL policies for room objectives and a high-level synthesized planner for overall reachability.

Proposed Solution:
- Apply RL (specifically deep Q-networks) to obtain low-level policies for room objectives. This scales but provides no guarantees.
- To enable synthesis, train a latent MDP model for each room plus a mapping from true states to latent states. Provide PAC bounds on difference in values between true and latent MDPs.
- Novel unified RL procedure called WAE-DQN that alternates between DQN and representation learning based on Wasserstein auto-encoders, outputting the latent model.
- Construct a high-level MDP based on room values predicted by latent models. Synthesize optimal planner using standard algorithms.  

Main Contributions:
- Novel WAE-DQN procedure to obtain latent policies with PAC guarantees, removing the need for separate model distillation.
- Analysis showing room policies can be reused across environments with same subtasks under assumptions on entrance state distributions.
- Construct compact high-level MDP enabling efficient planner synthesis preserving value bounds.
- Demonstration on grid world with moving adversaries - HQ controller succeeds where DQN fails due to sparse rewards. Values close to model's predictions.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel approach for constructing hierarchical controllers that combine deep reinforcement learning to obtain reusable low-level policies for rooms in a hierarchical environment and reactive synthesis to obtain a high-level planner, overcoming challenges like sparse rewards and formally reasoning about neural network policies.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a novel approach for designing policies (controllers) for environments modeled as Markov decision processes (MDPs) that have a hierarchical structure. Specifically:

- They consider an environment modeled as a high-level graph ("map") where each vertex is an MDP ("room"). 

- They first apply deep reinforcement learning (DRL) to obtain low-level policies that complete room-specific tasks, like safely navigating to an exit. This scales to large rooms of unknown structure.

- They then synthesize a high-level "planner" policy based on the low-level policies that chooses which policy to execute when transitioning between rooms in order to optimize a global task. 

- A key challenge is obtaining guarantees on the low-level policies to enable feasible synthesis. They develop a DRL procedure that learns a small "latent" MDP with policy and PAC bounds.

- They demonstrate the approach on a grid world with moving adversaries where the task is to reach a target location while avoiding adversaries that move between rooms. Their hierarchical controller succeeds where DRL alone fails due to sparse rewards.

In summary, the main contribution is a novel approach to hierarchical policy design that combines low-level DRL with high-level synthesis by developing a technique to extract verifiable latent models from neural policies.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Hierarchical control - The paper focuses on constructing hierarchical controllers, consisting of high-level planners and low-level policies, for environments modeled as Markov decision processes (MDPs).

- Deep reinforcement learning (DRL) - DRL, specifically deep Q-networks (DQNs), is used to obtain the low-level policies that operate in "rooms" of the hierarchical MDP environments.

- Reactive synthesis - Formal reactive synthesis techniques are used to obtain the high-level planners based on abstract models of the low-level DRL policies. 

- Reach-avoid properties - The objectives considered are reach-avoid properties, where the goal is to reach a target set of states while avoiding "bad" states.

- PAC guarantees - The DRL procedure outputs probably approximately correct (PAC) guarantees on the quality of the learned abstraction mapping states to a latent space.

- Latent policies - The DRL procedure trains concise policies over a latent, abstract state space, rather than directly over the large concrete state spaces.

- Model abstraction - A key challenge is developing abstractions, specifically latent models and policies, to enable formal synthesis of high-level planners.

- Hierarchical navigation - The case study demonstrates the approach for hierarchical navigation in grid worlds with moving adversaries.

Some other relevant terms include WAE-DQN, the specific DRL algorithm; stationary policies; discounted reachability; value functions; model distillation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel unified DRL procedure called WAE-DQN that trains a latent policy directly while learning a latent MDP model. How exactly does this procedure work and alternate between policy optimization and representation learning? What are the specific advantages compared to prior distillation techniques?

2. The paper provides probably approximately correct (PAC) guarantees on the value of the latent policy learned by WAE-DQN. What is the formal definition of these PAC bounds and how are they derived? What assumptions are needed?

3. How exactly is the compact latent MDP plan constructed for synthesizing the high-level planner? What is the intuition behind aggregating the room values based on the graph structure to enable planning? 

4. What is the formal definition of the entrance loss for a room? How is this estimated during training and what role does it play in enabling reuse of the trained low-level policies?

5. What assumptions are needed (projection of BSCCs, support of initial distributions etc.) for the low-level policies to be reusable? Explain the stationarity and ergodicity arguments used.

6. The paper claims the method addresses challenges of sparse rewards and reusability in hierarchical DRL problems. What specifically about the method enables overcoming these challenges?

7. How does the constructed hierarchical controller provide guarantees on the probability of satisfying the high-level reach-avoid specification? What theoretical bound relates the controller's value to latent values?

8. What encodings are used for representing the grid world state space? How do convolutions handle the complexity and why doesn't a tabular representation suffice?

9. What hyperparameter tuning was performed for algorithms like WAE-DQN and MADE? What worked best? Did ablations justify design choices empirically?  

10. The evaluation aims to demonstrate applicability in a challenging scenario. What made this domain challenging for regular DRL algorithms? How did the hierarchical approach specifically overcome the difficulties?
