# [Synthesis of Hierarchical Controllers Based on Deep Reinforcement   Learning Policies](https://arxiv.org/abs/2402.13785)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Addressed:
- Developing control policies for environments modeled as Markov Decision Processes (MDPs). Two main approaches have complementary strengths and weaknesses: 
1) Reinforcement learning (RL) allows training policies in large, unknown environments but struggles with sparse rewards.  
2) Reactive synthesis provides formal guarantees and supports logical objectives but does not scale.
- The paper considers hierarchical MDPs - modeled as a graph where vertices are populated with "room" MDPs. Goal is obtaining a hierarchical controller: low-level RL policies for room objectives and a high-level synthesized planner for overall reachability.

Proposed Solution:
- Apply RL (specifically deep Q-networks) to obtain low-level policies for room objectives. This scales but provides no guarantees.
- To enable synthesis, train a latent MDP model for each room plus a mapping from true states to latent states. Provide PAC bounds on difference in values between true and latent MDPs.
- Novel unified RL procedure called WAE-DQN that alternates between DQN and representation learning based on Wasserstein auto-encoders, outputting the latent model.
- Construct a high-level MDP based on room values predicted by latent models. Synthesize optimal planner using standard algorithms.  

Main Contributions:
- Novel WAE-DQN procedure to obtain latent policies with PAC guarantees, removing the need for separate model distillation.
- Analysis showing room policies can be reused across environments with same subtasks under assumptions on entrance state distributions.
- Construct compact high-level MDP enabling efficient planner synthesis preserving value bounds.
- Demonstration on grid world with moving adversaries - HQ controller succeeds where DQN fails due to sparse rewards. Values close to model's predictions.
