# [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and   Two-Phase Partition](https://arxiv.org/abs/2402.15220)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition":

Problem:
- Self-attention is a core component of large language models (LLMs) but also a significant source of inference latency and memory usage, especially for long input sequences. 
- In multi-tenant LLM serving scenarios, multiple requests often share identical system prompts inserted by application developers, leading to redundancy in the key/value (KV) caches used by self-attention.
- Existing self-attention implementations are unable to leverage this redundancy to improve efficiency.

Proposed Solution:
- Introduce a prefix-aware KV (PAKV) cache implemented as a prefix tree structure that can detect matching prompts across requests and share KV tensors at runtime. This increases memory utilization.
- Design a two-phase partition (TPP) self-attention kernel that improves computation and data locality during self-attention using the context from the prefix tree structure. 
   - Phase 1 partitions over shared chunks and batches attention between sequences.
   - Phase 2 partitions over sequences and processes any non-shared chunks.
- Additional optimizations to hide overheads of the prefix tree management.

Main Contributions:
- Demonstrate via experiments that system prompts can be long in practice (1000+ tokens) with high redundancy across requests.
- Propose and implement the PAKV cache using a prefix tree structure that can efficiently detect and leverage this redundancy.
- Design a TPP kernel tailored for the PAKV cache that significantly accelerates self-attention computation.
- Empirically validate feasibility and quantify performance gains of the approach under various system configurations.
- Show 3.2-4.8x kernel speedups and 1.6-2.3x end-to-end throughput gains over state-of-the-art implementations as system prompt length increases.

The key innovation is effectively managing KV cache memory and computation based on online discovered redundancy, enabled by the prefix tree structure and two-phase partition kernel.
