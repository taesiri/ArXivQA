# Scaling Instruction-Finetuned Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we improve the performance and generalization abilities of large language models through instruction finetuning?The key aspects the paper explores regarding this question are:- Scaling the number of instructional tasks used in finetuning- Scaling the size of the model being finetuned - Adding chain-of-thought style reasoning tasks to the finetuning dataThe overall hypothesis seems to be that combining all three of these factors - using a very large model, finetuning it on a large and diverse set of tasks including reasoning tasks, and posing those tasks as natural language instructions - will produce significant gains in the model's capabilities on a variety of NLP benchmarks, especially ones requiring reasoning.The experiments aim to validate whether each factor individually, and their combination, does indeed improve performance and generalization. The results generally support the hypothesis, showing gains from scaling model size, scaling number of tasks, and incorporating reasoning-style tasks.So in summary, the central research question is how to improve language models via instruction finetuning, and the key factors explored are scale of model, scale of data, and inclusion of reasoning tasks. The hypothesis is that combining all these factors will produce the best results.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:- Scaling instruction-finetuning of language models by increasing the number of finetuning tasks and the size of the model. The paper shows that both increasing model size and number of finetuning tasks improves performance on a variety of held-out evaluation benchmarks. - Studying the effect of including chain-of-thought (CoT) examples in the finetuning data. The paper finds that including CoT data improves performance on reasoning tasks while maintaining gains on non-CoT tasks. It also enables zero-shot reasoning without exemplars.- Showing the generality of instruction finetuning by applying it to models of different sizes and architectures, including T5, PaLM, and U-PaLM. Instruction finetuning improves all models tested.- Releasing Flan-T5 models that achieve strong few-shot performance compared to much larger models like PaLM 62B.- Demonstrating the improved usability of instruction-finetuned models via human evaluations on open-ended generation.In summary, the main contributions are showing that instruction finetuning is a general method to improve model performance and usability, especially when scaling the number of tasks, scaling model size, and including chain-of-thought data. The results also underscore the compute-efficiency of instruction finetuning.
