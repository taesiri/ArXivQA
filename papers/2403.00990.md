# [Formulation Comparison for Timeline Construction using LLMs](https://arxiv.org/abs/2403.00990)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Timeline construction from text requires identifying the chronological order of events, which is challenging as events are often not described sequentially.  
- Prior approaches have limitations: time anchoring methods fail when events anchor to the same time range; pairwise ordering methods are expensive to annotate all pairs.
- There lacks a unified framework to compare models and task formulations for timeline construction.

Proposed Solution:
- Develops TimeSET, a new timeline construction benchmark, featuring saliency-based event selection and partial ordering to enable practical annotation.

- Proposes an evaluation framework that casts TimeSET into 4 formulations (NLI, Pairwise, MRC, Timeline) to prompt LLMs, enabling cross-model and cross-formulation comparisons.  

- Benchmarks LLMs on existing event ordering datasets to gain robust insights into their capabilities.

Key Findings:
- NLI formulation with Flan-T5 shows strongest performance on TimeSET among others, while more compositional formulations perform better on average. Performance varies across models and formulations.

- Timeline construction and event ordering are still challenging for few-shot LLMs, underperforming smaller fine-tuned models. Larger LLMs perform generally better.

Main Contributions:
- New evaluation dataset TimeSET for timeline construction 

- Novel cross-model/formulation evaluation framework 

- Benchmarking of open LLMs' event ordering capabilities

The work provides a testbed and insights to facilitate future research into better timeline construction systems using LLMs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new timeline construction dataset called TimeSET, introduces a novel framework to compare different task formulations using this dataset and large language models, and benchmarks language models on existing event ordering datasets to understand their capabilities on the core subtask of timeline construction.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) The authors propose a new evaluation dataset called TimeSET for timeline construction consisting of single-document timelines with document-level pairwise temporal order annotations. TimeSET features saliency-based event selection and partial ordering to enable a practical annotation workload.

2) They propose an evaluation framework that compares multiple task formulations (NLI, Pairwise, MRC, Timeline) with TimeSET by prompting open large language models (LLMs) like Llama 2 and Flan-T5. This allows cross-model and cross-formulation comparisons for timeline construction.

3) The authors benchmark open LLMs on existing event temporal ordering datasets in a few-shot learning setting and compare them to smaller fine-tuned models to gain a robust understanding of the capabilities of LLMs for the core subtask in timeline construction.

In summary, the main contributions are: (1) New dataset TimeSET for timeline construction (2) Novel evaluation framework for cross-formulation comparison using TimeSET (3) Benchmarking of open LLMs on event ordering datasets to understand their capabilities.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Timeline construction - Building timelines that structure events in chronological order from textual input. A core focus of the paper.

- TimeSET - A new timeline construction evaluation dataset proposed in the paper, featuring saliency-based event selection and partial ordering annotation.

- Formulation comparison - Comparing different task formulations (NLI, pairwise, MRC, timeline) for timeline construction using large language models. A novel evaluation framework proposed in the paper.  

- Benchmarking - Evaluating the capability of large language models on existing event temporal ordering datasets through few-shot learning.

- Event temporal ordering - Identifying and structuring the temporal order of events, a key subtask of timeline construction.

- Salient events - Significant events worth including in a timeline summary. TimeSET focuses on salient event annotation.

- Partial ordering - Annotating a preceding and following event for each event to reduce annotation workload. Used in TimeSET.

- In-context learning - Few-shot learning approach for large language models, with models making predictions based on a small number of demonstrations.

So in summary, the key topics are timeline construction, the proposed TimeSET dataset, comparing task formulations, benchmarking large language models, and concepts like salient events and partial ordering that are used in TimeSET.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper introduces two key features in the TimeSET dataset - saliency-based event selection and partial-order annotation. Can you explain the motivation behind introducing these features and how they enable practical annotation workload?

2. The paper proposes an evaluation framework to compare multiple task formulations using TimeSET. Can you walk through the different formulations like NLI, Pairwise, MRC etc. and how a given document-timeline pair is converted into prompts under each formulation? 

3. The NLI formulation with Flan-T5 shows the strongest performance. What characteristics of Flan-T5 do you think contribute to this result? Does the NLI formulation play an important role as well?

4. For the same timeline construction task, why do you think different models perform differently under different formulations according to the results? What does this tell us about evaluating LLMs capabilities?

5. The paper analyzes model performance based on factors like model size, number of demonstrations, document creation time etc. Can you summarize the key findings from these analyses? 

6. How exactly is the timeline construction task converted into a code prompt? What motivated this attempt and were there any gains observed over other prompt types?

7. Apart from model and formulation comparisons, the paper also benchmarks LLMs on existing event ordering datasets. Can you contrast the relative performances of fine-tuned models versus few-shot learning models?

8. The paper focuses only on news articles in English within a single document. What are some ways the authors can expand the scope of TimeSET and the overall evaluation framework in future work?

9. What are some limitations of the study as acknowledged by the authors? Could issues with demonstration selection strategies, evaluation metrics etc. impact experimental conclusions?

10. The paper performs extensive experiments with open LLMs like Llama 2 and Flan-T5. Do you think findings from such models can generalize to other closed LLMs as well? Why or why not?
