# [Sequential training of GANs against GAN-classifiers reveals correlated   "knowledge gaps" present among independently trained GAN instances](https://arxiv.org/abs/2303.15533)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Do independently trained instances of the same GAN architecture exhibit correlated "knowledge gaps" (i.e. consistent artifacts across samples) that can be exploited by GAN classifiers? And how do the knowledge gaps change when the GANs are iteratively trained to fool the classifiers?

The key hypotheses appear to be:

- GAN classifiers can generalize to new GAN instances, implying consistent knowledge gaps across GAN instances.

- Iteratively training GANs to fool classifiers will reveal new insights into the optimization process and artifact spaces of GANs. 

- The architecture of the classifier impacts the subset of artifacts it learns to exploit.

So in summary, the paper investigates the existence, consistency, and evolution of knowledge gaps in GANs when iteratively trained against GAN classifiers. The goal is to better understand the underlying artifact spaces and optimization dynamics.
