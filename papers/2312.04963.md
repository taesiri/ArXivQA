# [Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D   priors](https://arxiv.org/abs/2312.04963)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Existing 3D generative models have limitations in either texture quality or geometric correctness due to relying solely on either 2D or 3D priors. Methods utilizing 2D priors (e.g. optimizing 2D model outputs) generate high-quality textures but inconsistent geometry. Methods using only 3D priors have better geometry but lower texture quality and diversity due to small datasets. 

Proposed Solution:
- Propose Bidirectional Diffusion (BiDiff), a unified framework incorporating both a 3D and a 2D diffusion process for high quality textured 3D generation.

- Uses a hybrid 2D-3D representation of multi-view images and an implicit 3D field to capture both texture and shape. 

- Pretrained 2D and 3D foundation models provide robust priors. The diffusion models are then jointly fine-tuned using novel bidirectional guidance to align generative directions.

- Allows independent texture and geometry control by separately adjusting the influence of 2D and 3D priors.

- Can serve as initialization for optimization methods for further refinement. Significantly reduces optimization time and avoids quality issues.


Main Contributions:

- First framework to seamlessly integrate both 2D and 3D diffusion models for textured 3D generation

- Novel bidirectional guidance through multi-view projection and back-projection to align generative processes  

- Enables separate manipulation of shape and texture

- Demonstrates state-of-the-art high quality 3D generations that are scalable and robust  

In summary, the proposed Bidirectional Diffusion elegantly marries 2D and 3D generative diffusion models through a unified framework and bidirectional guidance, achieving previously unmatched geometry correctness alongside high texture quality and diversity in 3D object generation.
