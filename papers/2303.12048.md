# [Vox-E: Text-guided Voxel Editing of 3D Objects](https://arxiv.org/abs/2303.12048)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses that this paper addresses are:- How can powerful pretrained 2D diffusion models be leveraged to edit existing 3D objects according to textual prompts? - Can a coupled volumetric representation regularized directly in 3D space allow flexibility to conform to text guidance while preserving the input object structure?- Can 2D cross-attention maps from diffusion models be elevated to 3D for spatially localizing text-guided edits on 3D objects?More specifically, the central hypothesis appears to be:By combining a generative text-guided objective with volumetric regularization and 3D cross-attention localization, existing 3D objects can be edited to match target text prompts through localized changes in geometry and appearance.The key ideas seems to be:- Using a score distillation loss to match a diffusion model's text guidance signal.- Coupling input and output voxel grids with a volumetric correlation regularizer. - Lifting 2D attention maps to 3D grids to refine the spatial extent of edits.The paper aims to demonstrate that this approach can produce consistent text-guided edits to 3D objects that prior works struggle with. The experiments analyze the approach on synthetic objects and real scenes for various prompts.In summary, the core research questions revolve around leveraging diffusion models to edit 3D objects through a combination of generative modeling and volumetric regularization, enabled by a voxel-based representation. The paper hypothesizes and evaluates whether this allows high fidelity text-guided editing of geometry and appearance.
