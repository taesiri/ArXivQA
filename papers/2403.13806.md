# [RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS](https://arxiv.org/abs/2403.13806)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent advances in neural radiance fields have achieved unprecedented quality for view synthesis tasks like novel view generation. However, they are computationally expensive to render due to reliance on volumetric rendering. On the other hand, methods based on rasterization and splatting like 3D Gaussian Splatting (3DGS) can render novel views in real-time but have difficulty optimizing complex real-world scenes. There is a need for a method that combines the optimization stability and quality of radiance fields with the runtime efficiency of splatting-based approaches.

Proposed Solution:
The paper proposes RadSplat, a novel method that uses radiance fields to supervise the training of a 3D gaussian splatting scene representation for high-quality, real-time rendering. 

Key ideas:
1) Leverage state-of-the-art radiance field model ZipNeRF as a prior and source of supervision. This provides a robust signal for optimizing the splatting model, handling complexities like lighting variation.

2) Novel ray contribution-based pruning during training to reduce splat count by up to 10x while improving quality. Achieves smaller, better models.

3) Viewpoint clustering and visibility-based filtering at test time to skip invisible splats and further accelerate rendering without quality loss. Enables scaling to large scenes.

Contributions:
1) Using radiance fields for initialization and supervision when optimizing splatting models. Enables optimizing complex real-world scenes.  

2) Pruning technique to achieve models with 10x fewer splats without quality degradation. Also faster to render.

3) Viewpoint filtering for accelerating large scene rendering up to 45% without quality loss.

Results:
- State-of-the-art quality on MipNeRF and ZipNeRF datasets while rendering 900+ FPS, over 3000x faster than ZipNeRF.

- Robust performance on challenging real-world scenes with lighting variation where 3DGS struggles.

- Up to 10x point count reduction through pruning while improving metrics over 3DGS.

The key insight is supervising splatting models with more robust radiance field representations can unlock both quality and speed. The contributions combine to enable high-fidelity view synthesis in real-time even for complex captures.
