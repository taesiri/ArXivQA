# [Dynamic Corrective Self-Distillation for Better Fine-Tuning of   Pretrained Models](https://arxiv.org/abs/2312.07028)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces Dynamic Corrective Self-Distillation (DCS), a novel framework to enhance the fine-tuning of pretrained language models (PLMs) on downstream tasks using limited labeled data. DCS is inspired by adaptive boosting and involves iterative adjustment of sample weights, emphasizing instances where the teacher and student models disagree. Specifically, a teacher model guides a student model of identical architecture via knowledge distillation. At each epoch, DCS reweights samples, assigning greater importance to those with divergent teacher-student predictions. This self-corrective mechanism significantly improves student model performance. Extensive experiments on diverse GLUE tasks and PLMs like BERT, RoBERTa, XLNet, and ELECTRA demonstrate over 2% average gains versus vanilla fine-tuning. Notably, DCS achieves substantial improvements on smaller datasets. It also consistently outperforms existing fine-tuning techniques. Ablation studies validate that both distillation and dynamic reweighting components synergistically boost effectiveness. Analysis shows prioritizing discordant samples is beneficial. By incorporating distillation and self-correction, DCS enhances model generalization and accuracy during fine-tuning with limited labels. The simple yet powerful DCS framework has broad applicability in advancing NLP.
