# [How does Multi-Task Training Affect Transformer In-Context Capabilities?   Investigations with Function Classes](https://arxiv.org/abs/2404.03558)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent work has shown that large language models (LLMs) can perform unseen tasks well when provided with few-shot examples, known as in-context learning (ICL). However, little work has explored training strategies to improve multi-task ICL capabilities. 
- Multi-task learning (MTL) has shown benefits for model performance on individual tasks, but has been difficult to apply to LLMs on natural language tasks. The new paradigm of learning function classes in context provides an easier way to study MTL for ICL.

Methods:
- Explore MTL for ICL by training a Transformer model on multiple function classes and input distributions. 
- Investigate three curriculum learning strategies to schedule tasks during training: sequential, mixed, and random.
- Compare curriculum learning models to single-task baseline models.
- Analyze model attention to identify "retrospective heads" that attend to input-output pairs.

Results:
- Mixed curriculum learning led to the best ICL performance - more stable training, transfer learning ability, and data efficiency.
- Curriculum learning enabled convergence on tasks that single-task models failed on.  
- Retrospective heads were consistent across tasks and integral for ICL capability.

Contributions:
- First systematic analysis of MTL for improving ICL with curriculum learning strategies.
- Demonstrated curriculum learning enables more reliable convergence, greater data efficiency, and transfer learning.
- Identified role of retrospective attention heads for ICL across multiple tasks.
- Results motivate pre-training strategies and analysis for multi-task ICL with natural language.

In summary, this paper showed curriculum learning is a promising approach for MTL with ICL, overcoming limitations of single-task learning. The analysis also revealed insights into how attention enables ICL across tasks.
