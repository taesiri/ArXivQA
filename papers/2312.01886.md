# [InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language   Models](https://arxiv.org/abs/2312.01886)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes an instruction-tuned targeted attack method called InstructTA for fooling large vision-language models (LVLMs). The attack operates under a practical gray-box threat model where the adversary only has access to the victim LVLM's visual encoder, without knowledge of the underlying language model or prompts used. To achieve high attack transferability, InstructTA leverages text-to-image models and language models like GPT-4 to generate reasonable target images and prompt instructions tailored to the attacker's chosen target text response. It then extracts instruction-aware features from the target image and adversarial example using a local surrogate model, and minimizes the distance between these features during optimization. Further transferability improvements come from paraphrasing the inferred prompts using ChatGPT. Experiments against models like BLIP-2, InstructBLIP and MiniGPT-4 demonstrate InstructTA's superior targeted attack success rate over baselines. The work highlights LVLMs' vulnerability to adversarial attacks, and the importance of instruction tuning and fusing visual-linguistic information to enable more transferable attacks.
