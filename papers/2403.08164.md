# [EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight   Text-to-Speech](https://arxiv.org/abs/2403.08164)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training deep learning based text-to-speech (TTS) models is computationally expensive and time-consuming, especially for low-resource languages like Mongolian. 
- Existing TTS models rely on RNNs and transformers which have a large number of parameters.

Proposed Solution:
- The authors propose EM-TTS, an efficient Mongolian TTS model based entirely on CNNs rather than RNNs or transformers.
- It uses a two-stage training strategy:
    1. Text2Spectrum: Encodes phonemes to a coarse mel spectrogram
    2. SSRN: Synthesizes a fine-grained spectrum from the coarse mel spectrogram
- Several data augmentation techniques are used to expand the limited Mongolian training data:
    - Noise suppression 
    - Time warping, frequency masking, time masking (SpecAugment)
    - Spectrogram resize by compressing/stretching mel spectrograms

Main Contributions:
- EM-TTS significantly reduces model size and training time compared to RNN/transformer TTS models
- The two-stage CNN architecture ensures quality while being lightweight
- Data augmentation expands limited training data and improves model robustness
- Subjective and objective evaluations show EM-TTS generates speech with good quality and similarity while requiring far fewer resources to train.

In summary, the paper proposes a fast and lightweight CNN-based TTS model to address resource-intensive training for low-resource Mongolian speech synthesis. Data augmentation and two-stage training strategy improves quality despite the model simplicity.
