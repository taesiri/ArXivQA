# [A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them   In Zero Shot](https://arxiv.org/abs/2305.09758)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we leverage recent advances in large language models (LLMs) to improve video understanding, given the lack of large annotated training datasets for complex story videos?The key hypothesis appears to be:By verbalizing complex story videos into coherent natural language descriptions, we can take advantage of the powerful reasoning and zero-shot capabilities of LLMs to perform video understanding tasks directly on the generated text, without requiring large amounts of video training data.In particular, the paper proposes:1) A framework to convert long, multimodal videos into textual "stories" by extracting and combining modalities like speech, text, visual frames using a LLM. 2) Using these generated stories to perform video understanding tasks in a zero-shot manner by formulating the tasks as natural language prompts.3) Demonstrating that this approach outperforms video-based supervised baselines on tasks like topic classification, emotion recognition, and persuasion strategy identification for story videos.4) Releasing a new dataset for studying persuasion strategies in video ads.So in summary, the core hypothesis is that representing videos as text stories can allow LLMs to reason about them effectively without large labeled video datasets. The paper aims to validate this via experiments on several video understanding benchmarks.
