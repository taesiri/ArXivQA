# [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video   Generation](https://arxiv.org/abs/2309.15818)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we combine pixel-based and latent-based video diffusion models to efficiently generate high-quality videos with accurate text-video alignment?The key hypotheses appear to be:1) Pixel-based video diffusion models can produce low-resolution videos with more natural motion and better text-video alignment compared to latent-based models. 2) Latent-based video diffusion models can effectively act as super-resolution models to upscale low-resolution videos to high-resolution while maintaining text-video alignment, if provided with a good low-resolution guide video. 3) By combining pixel-based models for low-resolution generation and latent-based models for super-resolution, it is possible to create an efficient text-to-video model that produces high-quality, high-resolution videos with precise text-video alignment.The central goal of the paper seems to be developing an integrative model architecture, called Show-1, that combines the strengths of pixel and latent video diffusion models to efficiently generate high-fidelity videos well-aligned to textual prompts. The key hypotheses focus on the specialized capacities of pixel vs latent models and how combining them can lead to better overall performance.
