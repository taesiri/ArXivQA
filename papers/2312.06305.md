# [A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space   Reduction in AutoML](https://arxiv.org/abs/2312.06305)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a meta-learning algorithm called Sequential Hyper-parameter Space Reduction (SHSR) for reducing the search space of AutoML hyper-parameter configuration optimization. SHSR analyzes past performance data of algorithm configurations on multiple datasets to learn which configurations can be safely eliminated for a new dataset, based on its metadata, without significantly impacting predictive performance. This allows focusing the search on more promising configurations. SHSR recursively applies this elimination step, resulting in exponential reductions of the discrete configuration search space and computational savings. The paper evaluates SHSR on a large corpus of 284 classification and 375 regression datasets. The results demonstrate substantial computational savings of 30-50% with minimal performance drops of less than 0.1% for tight thresholds. More relaxed thresholds achieve even higher savings exceeding 90% but with higher performance drops. SHSR also performs well even with only 20% partial input data. Comparisons to a KNN meta-learner and random elimination show SHSR's superiority in filtering unpromising configurations. Overall, SHSR provides an effective data-driven approach to constrain the discrete hyperparameter search space in AutoML while preserving performance.
