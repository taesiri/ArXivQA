# [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982v5)

## What is the central research question or hypothesis that this paper addresses?

 This paper presents a new open-source benchmarking framework for evaluating and comparing graph neural networks (GNNs). The key components and objectives of the framework are:

- It comprises a diverse collection of 12 medium-scale graph datasets, including both real-world graphs (e.g. molecules, social networks) and synthetic graphs based on mathematical models. The goal is to have datasets that are representative of different domains and learning tasks.

- It enables fair comparison of GNN models by enforcing the same parameter budget. This allows evaluating the impact of model architecture rather than just model size. 

- It provides a modular and extensible codebase in PyTorch/DGL for training and evaluating GNNs on the datasets. This facilitates rapid prototyping and reproducible experiments.

- The overall goal is to develop a community benchmark to drive progress in GNN research by allowing systematic evaluation of new methods and ideas. The framework aims to identify architectural principles and components that generalize across domains.

The central hypothesis is that such a rigorous graph ML benchmark is essential, but has been lacking so far, for reliably evaluating and advancing graph neural networks. The paper discusses how the proposed benchmark has enabled researchers to gain new insights, such as the use of graph positional encodings. It demonstrates the utility of the benchmark through sample studies on architectural components like using edge features and attention.

In summary, the key research question is how to develop an open benchmarking framework that can systematically drive progress and fundamental understanding of graph neural networks through fair and reproducible evaluation of new ideas. The paper presents one such carefully designed framework with both data and code, and demonstrates its usefulness.


## What is the main contribution of this paper?

 This paper presents a comprehensive benchmarking framework for evaluating and comparing graph neural networks (GNNs). The key contributions are:

1. Proposes a diverse collection of 12 medium-scale graph datasets, spanning both real-world domains like chemistry, social networks, etc. and synthetically generated mathematical graphs. Compared to existing small datasets, these are better suited for statistically separating GNN model performance. 

2. Provides standardized codebase and protocols for training and evaluating GNNs on these datasets in a fair and consistent manner. This includes using fixed parameter budgets, same train/val/test splits, training hyperparameters, and hardware. 

3. Demonstrates how the benchmark can be leveraged to gain new insights into GNN designs. As a case study, they show how using Laplacian eigenvectors as positional encodings in GNNs can overcome limitations of message-passing models.

4. Compares popular GNN architectures like GCN, GAT, GraphSage, etc. as well as recent models like 3WLGNN and RingGNN. Key findings are that attention-based GCNs perform the best, while 3WLGNN/RingGNN don't scale beyond shallow depths.

5. Makes both the datasets and reference implementations publicly available to facilitate future research. The benchmark has gained wide adoption evidenced by 2000+ GitHub stars.

In summary, the paper's main value is in providing the community with an open-source, comprehensive benchmark to track progress, gain insights, and prototype new ideas for advancing graph neural networks. The proposed datasets, codebase and experiments highlight current challenges like training stability, model scalability, etc. to drive further research.
