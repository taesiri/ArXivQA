# [Improve Robustness of Reinforcement Learning against Observation   Perturbations via $l_\infty$ Lipschitz Policy Networks](https://arxiv.org/abs/2312.08751)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel robust reinforcement learning method called SortRL to improve the robustness of policies against observation perturbations. SortRL employs a new policy network architecture based on an Lipschitz neural network called SortNet, which provides global Lipschitz continuity to bound the change in outputs given perturbed inputs. A convenient robustness evaluation method is introduced based on the output margin between top action scores. Moreover, a training framework is designed involving policy distillation on clean observation-action pairs from an expert teacher policy, while optimizing for task performance and robustness simultaneously via a cross entropy loss and a hinge-based robustness loss. Experiments on control tasks and Atari/ProcGen games demonstrate state-of-the-art robustness of SortRL against different perturbation strengths. The performance decays much more slowly compared to prior methods as perturbation strength increases. Even on nominal environments without perturbations, SortRL achieves comparable performance to state-of-the-art methods. The results showcase the effectiveness of architectural choices and training methodology of SortRL for improving policy robustness.
