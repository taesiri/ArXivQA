# [Learning Vision-based Pursuit-Evasion Robot Policies](https://arxiv.org/abs/2308.16185)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn vision-based robot policies that can reason strategically through partially observable physical state and latent intent in multi-agent interactions?Specifically, the authors aim to develop an approach for learning robot policies that can exhibit strategic behaviors like information gathering, intent prediction, and anticipation when interacting with other agents in the real world under sensing constraints (i.e. using only onboard sensors like cameras). The key challenges are handling the partial observability of the physical state and modeling the latent intent of other agents.To tackle this, the main idea is to use a privileged learning approach where a fully observable policy generates supervision for training a partially observable policy. The future trajectory of the other agent is used as a novel type of privileged information to infer their latent intent. The authors study how different models for the fully observable supervisor policy and different agent policies affect the quality of the distillation.The central hypothesis is that by leveraging privileged information about latent intent from future trajectories, they can transform the intractable planning problem into a supervised learning problem. This allows the robot policy to gather information, make predictions, and act strategically despite real-world sensing constraints.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method to learn pursuit-evasion robot policies that can reason strategically through partial observability of the physical state and latent intent of other agents. The key ideas are:- Using a fully-observable teacher policy with access to future opponent trajectories to generate supervision for a partially-observable student policy. This converts the intractable Dec-POMDP formulation into a more tractable supervised learning problem.- Learning a latent representation of the opponent's intent from future trajectories that captures goal direction, policy class, etc. The student policy must estimate this latent intent from noisy observations.- Analyzing the importance of diversity vs optimality in the opponent behavior during training. Opponents that are too optimal (e.g. game theoretic) provide poor supervision. - Demonstrating the approach on a physical quadruped robot playing pursuit-evasion games against humans and other robots using only onboard proprioception and vision. The robot exhibits interesting behaviors like information gathering, intent prediction, and anticipation.So in summary, the main contribution is using privileged learning to enable a robot to learn strategic planning behaviors for decentralized partially-observable multi-agent interactions purely from onboard sensing and interaction experience.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a method to train a robot policy for pursuit-evasion games that can reason strategically through partial observability by using a fully-observable policy to generate supervision.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other related work in multi-agent robot learning:- This paper focuses specifically on pursuit-evasion games between two mobile robots with onboard sensing, which is a relatively underexplored setting compared to simulation environments or with global state information. Most prior work in multi-agent RL has been in simulated environments like video games.- The approach uses privileged learning to train a partially observable policy, with the privileged information being the future trajectory of the evader agent. This is a novel application of privileged learning. Other related work learns latent intent models but assumes more state observability.- They find that the quality of the supervision signal depends heavily on the diversity and optimality of the evader behavior during training. Game theoretic opponents that are very optimal under perfect state assumptions provide weak supervision. This insight on the role of the opponent model is a key contribution.- They demonstrate the approach on a physical quadruped robot playing pursuit-evasion games in the real world using only onboard sensing and compute. There has been very little prior work showing decentralized multi-agent interactions on real robotic systems.- Limitations include not explicitly modeling environmental affordances and a limited field of view assumption. The approach is also currently specialized to the pursuit-evasion setting.Overall, this paper makes progress on a very challenging robot learning problem of decentralized multi-agent interaction using only onboard sensing. The insights on opponent modeling, applicability to the real-world, and method of using privileged learning help advance the field.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors are:- Incorporating affordances of the environment into the robot's planning, such as using obstacles strategically. The current approach does not explicitly model the environment geometry.- Exploring different sensor modalities like high-resolution 360 degree cameras to alleviate the limited field-of-view assumption. However, this introduces new challenges like higher computational burden.- Training the policy end-to-end with reinforcement learning instead of using distillation. The authors tried using PPO directly on the partially observable setting but it failed to learn well. New RL algorithms could potentially learn the estimation, control, and strategy jointly.- Scaling up the approach to settings with multiple pursuers and evaders interacting. The current work focuses on 1v1 interactions.- Deploying the approach on more dynamic and agile robots like quadrupeds to showcase more sophisticated strategic maneuvers. - Experimenting with different types of interactions beyond pursuit-evasion, such as collaborative tasks. The core ideas could extend to other strategic multi-agent scenarios.- Evaluating performance against human evaders with more diverse and strategic behavior compared to the teleoperated robot. This would better stress test the robot's capabilities.In summary, the key directions are developing more sophisticated reasoning about the environment, improving the learning approach, scaling up the number of agents, experimenting on more agile robots, testing on new tasks beyond pursuit-evasion, and benchmarking against humans.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents a method for learning vision-based robot policies for strategic multi-agent interaction. The authors focus on pursuit-evasion games between a quadruped robot (pursuer) and a human or another robot (evader). They take a privileged learning approach where a fully-observable robot policy that has access to the evader's future trajectory provides supervision to train a partially-observable policy. The partially-observable policy must reason strategically from visual observations and state uncertainty. They find the quality of the supervision signal depends on the diversity and optimality of the evader's behavior during training. When deployed on a physical robot, the learned policy exhibits strategic behaviors like information gathering, intent prediction, and anticipation despite real-world sensing constraints. The key limitation is the method does not model environmental affordances for strategic planning. Overall, this is a promising approach for enabling strategic interaction behaviors on real embodied agents with only onboard sensing.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper presents a method for learning vision-based robot policies that can act strategically in pursuit-evasion interactions under real-world sensing constraints. The key idea is to leverage a fully-observable teacher policy to supervise a partially-observable student policy. The teacher policy gets access to privileged information about the future state trajectory of the evader agent, enabling it to quickly learn a latent representation of the evader's intent and take optimal actions. This teacher policy is then used to supervise a student policy which must operate under real-world sensing constraints, only having access to noisy state estimates from an onboard RGB-D camera. Specifically, the student policy learns to estimate the latent intent representation from its observations and mimic the teacher's actions. Through experiments in simulation and on physical quadruped robots, the authors demonstrate that this approach can enable strategic behaviors like information gathering, intent prediction, and anticipation. They analyze how the quality of the supervision signal depends on properties of the evader behavior during training as well as assumptions in the teacher policy. The student policy exhibits creative strategies when deployed on a physical robot interacting with humans or other robots, despite real-world noise and out-of-distribution opponents. Overall, this work takes steps towards building vision-based robot policies that can reason strategically about other agents, an essential capability for real-world multi-agent interaction.


## Summarize the main method used in the paper in one paragraph.

The paper presents an approach for learning vision-based robot policies for pursuit-evasion interactions in the wild. The key idea is to leverage a fully-observable policy to generate supervision for a partially-observable policy. The fully-observable policy gets access to privileged information - the true relative state between the robot and the evader agent, as well as the future trajectory of the evader agent. This enables it to quickly learn a latent representation of the evader's intent and take actions that account for the evader's behavior. The partially-observable policy only has access to onboard RGB-D observations and estimates the relative state via a Kalman filter. It is trained via DAGGER using the fully-observable policy as a teacher - supervising both the action at each timestep as well as the latent intent estimate. This allows the partially-observable policy to learn information-gathering behaviors to resolve physical state uncertainty as well as generate predictions about the evader's intent for strategic planning.The approach is deployed on a physical quadruped robot playing pursuit-evasion games with humans or other legged robots in the wild using only onboard sensing and computation.
