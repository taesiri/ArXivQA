# [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness   Constants](https://arxiv.org/abs/2402.10774)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Distributed machine learning methods like distributed gradient descent often use communication compression techniques to alleviate the communication bottleneck. However, powerful compressors like TopK and RankK are biased and the theoretical understanding of algorithms using them is weak compared to unbiased compressors. In particular, the best known rate for the EF21 algorithm matches vanilla gradient descent without compression. There is a need to improve the theory for methods like EF21 that work with general contractive compressors.

Proposed Solution and Contributions:

1. The paper proposes to clone problematic machines with higher smoothness constants $L_i$, which allows replacing the quadratic mean (QM) of the $L_i$'s with their arithmetic mean (AM) in the EF21 rate. However, this requires more devices. 

2. A weighted EF21 algorithm (EF21-W) is proposed where server aggregates client gradients using weights $w_i = L_i/(\sum_j L_j)$. This allows improving the convergence rate dependence from QM to AM of smoothness constants, without cloning devices.

3. Further analysis shows that the weights can be pushed from the EF21-W algorithm into the analysis. This allows showing that even the original EF21 algorithm has an improved dependence on the AM instead of the QM.

4. The improvements apply to many EF21 variants including stochastic gradients and partial participation. The method also improves the known EF21 theory in the rare features regime.

5. Experiments on logistic regression with synthetic and real datasets demonstrate that EF21-W offers faster convergence compared to vanilla EF21 when there is high variance in client smoothness constants. The gains can be significant in practice.

In summary, the paper provides the first known improvement in the convergence rate of EF21 methods compared to baseline gradient descent, when used with general contractive compressors. This is done via a weighted analysis technique without any change to the algorithm. Practical gains are shown on nonconvex optimization tasks.
