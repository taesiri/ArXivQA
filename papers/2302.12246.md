# Active Prompting with Chain-of-Thought for Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the key research question addressed in this paper is:How to select the most helpful and informative questions from a task-specific pool to annotate with reasoning chains in order to improve the performance of large language models (LLMs) on complex reasoning tasks through chain-of-thought prompting?The paper proposes a new method called "Active Prompting with Chain-of-Thought for Large Language Models" (Active-Prompt) to address this question. The key ideas are:1) Introduce an uncertainty-based active question selection strategy to determine the most uncertain questions from a pool of task-specific questions for annotation. This reduces the need for extensive human engineering effort in constructing effective prompts.2) Characterize the uncertainty using metrics like disagreement, entropy, variance, and self-confidence among the LLM's multiple predictions for each question. Questions with highest uncertainty are selected for annotation.3) Involve humans to annotate the selected questions with reasoning chains and answers to create new exemplars. 4) Use the new annotated exemplars to prompt the LLM and achieve superior performance on complex reasoning tasks compared to baselines.In summary, the paper focuses on how to judiciously select the most informative questions from a pool to annotate with reasoning chains in order to adapt LLMs more effectively to new complex reasoning tasks through active prompting. The core hypothesis is that reducing the model's uncertainty by annotating the most uncertain questions will improve prompting performance.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Active Prompting with Chain-of-Thought (Active-Prompt) to adapt large language models (LLMs) to different tasks using task-specific example prompts annotated with human-designed chain-of-thought reasoning steps. Specifically, the key contributions are:1. Proposing a solution to judiciously select the most helpful and informative questions from a pool of task-specific queries for human annotation. This reduces the human engineering workload compared to manually selecting questions. 2. Introducing an effective uncertainty-based question selection strategy with several metrics like disagreement, entropy, variance, and self-confidence to characterize the uncertainty.3. Demonstrating the proposed method's effectiveness by surpassing competitive baselines on multiple complex reasoning tasks like arithmetic, commonsense, and symbolic reasoning.In summary, the main contribution is developing an uncertainty-based active annotation strategy to select the most useful task-specific examples for chain-of-thought prompting of LLMs, which achieves new state-of-the-art performance on several reasoning benchmarks. This is the first work to show the benefits of active question selection in chain-of-thought prompting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new method called Active Prompting with Chain-of-Thought that judiciously selects the most uncertain and helpful questions to annotate with reasoning steps, in order to improve the performance of large language models on complex reasoning tasks when using example-based prompting.
