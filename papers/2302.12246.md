# Active Prompting with Chain-of-Thought for Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the key research question addressed in this paper is:How to select the most helpful and informative questions from a task-specific pool to annotate with reasoning chains in order to improve the performance of large language models (LLMs) on complex reasoning tasks through chain-of-thought prompting?The paper proposes a new method called "Active Prompting with Chain-of-Thought for Large Language Models" (Active-Prompt) to address this question. The key ideas are:1) Introduce an uncertainty-based active question selection strategy to determine the most uncertain questions from a pool of task-specific questions for annotation. This reduces the need for extensive human engineering effort in constructing effective prompts.2) Characterize the uncertainty using metrics like disagreement, entropy, variance, and self-confidence among the LLM's multiple predictions for each question. Questions with highest uncertainty are selected for annotation.3) Involve humans to annotate the selected questions with reasoning chains and answers to create new exemplars. 4) Use the new annotated exemplars to prompt the LLM and achieve superior performance on complex reasoning tasks compared to baselines.In summary, the paper focuses on how to judiciously select the most informative questions from a pool to annotate with reasoning chains in order to adapt LLMs more effectively to new complex reasoning tasks through active prompting. The core hypothesis is that reducing the model's uncertainty by annotating the most uncertain questions will improve prompting performance.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Active Prompting with Chain-of-Thought (Active-Prompt) to adapt large language models (LLMs) to different tasks using task-specific example prompts annotated with human-designed chain-of-thought reasoning steps. Specifically, the key contributions are:1. Proposing a solution to judiciously select the most helpful and informative questions from a pool of task-specific queries for human annotation. This reduces the human engineering workload compared to manually selecting questions. 2. Introducing an effective uncertainty-based question selection strategy with several metrics like disagreement, entropy, variance, and self-confidence to characterize the uncertainty.3. Demonstrating the proposed method's effectiveness by surpassing competitive baselines on multiple complex reasoning tasks like arithmetic, commonsense, and symbolic reasoning.In summary, the main contribution is developing an uncertainty-based active annotation strategy to select the most useful task-specific examples for chain-of-thought prompting of LLMs, which achieves new state-of-the-art performance on several reasoning benchmarks. This is the first work to show the benefits of active question selection in chain-of-thought prompting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new method called Active Prompting with Chain-of-Thought that judiciously selects the most uncertain and helpful questions to annotate with reasoning steps, in order to improve the performance of large language models on complex reasoning tasks when using example-based prompting.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper tackles an important problem in improving prompting and reasoning abilities for large language models (LLMs). Developing better prompting techniques is an active area of research as LLMs become more powerful.- The key idea of actively selecting the most informative examples to annotate with reasoning chains is novel. Most prior work on chain-of-thought prompting relies on fixed sets of human-annotated examples. Actively choosing questions to annotate is an intelligent way to get the most value from limited human effort.- The proposed active learning approach based on uncertainty sampling is logically motivated and theoretically grounded. Using uncertainty to select datapoints for annotation is an established technique in active learning. Adapting this strategy to prompting is clever.- The experiments comprehensively evaluate performance on 8 diverse reasoning tasks spanning arithmetic, commonsense, symbolic reasoning. This demonstrates the broad utility of the approach. The ablations also provide useful insights into design choices. - The gains over strong baselines like self-consistency prompting are compelling. The state-of-the-art results highlight the benefits of active annotation over human-picked or random examples.Overall, this paper makes an important contribution in adapting active learning to prompting LLMs for reasoning. The uncertainty-based annotation strategy is intuitive and works well empirically. The comprehensive experiments and analyses are strengths. This approach seems promising to augment LLMs with reasoning abilities in an efficient data-driven manner.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Combining diversity and uncertainty for question selection. The paper notes that both diversity and uncertainty are useful criteria for selecting the most informative questions for annotation. The authors suggest exploring ways to combine diversity-based methods like Auto-CoT with their uncertainty-based approach. - Transferring prompts more effectively across tasks. The results show reduced gains on datasets that required transferring prompts from other tasks compared to annotating questions directly from the target task's training set. The authors highlight better prompt transfer as an important direction.- Exploring different annotators and annotation strategies. The paper analyzes the impact of different annotators. The authors suggest further work on understanding annotation effects and optimizing the annotation process.- Prompt engineering and task-specific templates. For symbolic reasoning tasks, the authors found coding-style prompts can significantly improve performance. They suggest prompt engineering as a promising direction, including designing prompts tailored to different reasoning skills.- Applying the method to other LLMs and tasks. The experiments focus on Codex and mathematical/commonsense reasoning tasks. The authors propose testing the approach on other large models and more complex reasoning datasets.- Combining with other methods like self-consistency and zero-shot prompting. The paper integrates some of these techniques but suggests further exploration of jointly applying active annotation selection with other advances.In summary, the key suggestions are developing better annotation transfer, combining diversity and uncertainty, customized prompting strategies, and integrating active selection with other advances in few-shot learning for LLMs. The authors frame active annotation of examples as a promising approach to efficiently adapt LLMs to complex reasoning tasks.
