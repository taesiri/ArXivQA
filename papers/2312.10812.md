# [Learning to Act without Actions](https://arxiv.org/abs/2312.10812)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) models typically require action labels or rewards during training. However, most real-world demonstration data (e.g. videos) does not contain such labels. Existing methods for learning from unlabeled videos require some labeled data to train an inverse dynamics model for labeling the unlabeled data. This limits the ability to leverage the vast amounts of unlabeled video data available on the web to learn powerful RL policies.

Proposed Solution:
The paper proposes a method called Latent Action Policies from Observation (LAPO) to learn latent action representations and policies purely from observation data, without any action or reward labels. 

Key ideas:
- An inverse dynamics model (IDM) is trained to predict a continuous latent action vector between two observations.  
- A forward dynamics model (FDM) tries to predict the next observation given past observation and latent action from IDM.
- Vector quantization is used to force the latent action to be a useful information bottleneck between IDM and FDM.
- The latent actions from IDM can label unlabeled data to train a latent action policy via behavior cloning.
- The latent policy can be adapted to true actions via fine-tuning or training an action decoder.

Main Contributions:
- Proposes first fully unsupervised approach to learn useful policies from unlabeled videos, enabling leveraging web-scale data.
- Shows latent space captures interpretable action-related structure despite no action labels during training.
- Demonstrates latent policies can be quickly adapted to recover or exceed expert performance after fine-tuning.
- Serves as an important stepping stone towards unlocking unsupervised pre-training paradigms seen in other domains for RL.

In summary, the key innovation is an unsupervised objective based on IDM and FDM consistency that recovers latent action structure from observations alone. This enables pre-training policies on unlabeled data in a scalable way not possible with prior methods.
