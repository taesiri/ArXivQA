# [Words are all you need? Language as an approximation for human   similarity judgments](https://arxiv.org/abs/2206.04105)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: To what extent can language capture human sensory experiences and similarity judgments across different modalities (images, audio, video)? The authors approach this question by:1) Collecting human similarity judgments for stimuli across 3 modalities (images, audio, video). 2) Collecting two types of textual descriptors for the stimuli: free-form captions and concise semantic tags.3) Using word and language embedding models to generate predictions of human similarity judgments based solely on the textual descriptors. 4) Comparing these text-based similarity predictions to similarity judgments produced by a wide array of sensory/perceptual models, including 611 pre-trained deep neural networks across the 3 modalities.5) Evaluating whether language provides a good approximation of human similarity judgments across modalities and how it compares to sensory models.So in summary, the central hypothesis is that language can serve as an effective predictor or approximation of human similarity judgments across diverse modalities. The authors test this hypothesis by comparing text-based similarity predictions to human judgments and sensory model predictions.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Introducing a novel pipeline for efficiently collecting human similarity judgments that scales linearly (O(N)) rather than quadratically (O(N^2)) in the number of stimuli. This is done by collecting textual descriptions (captions and tags) for stimuli and then using language models to approximate similarity judgments based on those descriptions. 2. Proposing and validating STEP-Tag, an adaptive tagging procedure for crowdsourcing meaningful and diverse tags that describe salient aspects of stimuli.3. Conducting a large-scale comparison of over 600 pre-trained deep neural networks (DNNs) across 3 modalities (images, audio, video) for approximating human similarity judgments. The results show a gap between DNNs and classical methods requiring O(N^2) judgments.4. Demonstrating that the proposed language-based methods using pre-trained language models (LLMs) provide better approximations of human similarity compared to the DNNs, while still being much cheaper than classical methods.5. Introducing stacked methods that combine both DNN and LLM embeddings to generate the overall best approximations of human similarity judgments across modalities.6. Providing a guide for researchers summarizing the best practices for scalable human similarity judgment collection or approximation based on the results.7. Releasing a large dataset of human similarity judgments and text descriptions, comprising over 200,000 ratings, to accompany the guide.So in summary, the key contribution is providing an efficient pipeline for approximating human similarity judgments at scale by using semantic textual descriptions and validating its effectiveness compared to a wide array of alternative methods.
