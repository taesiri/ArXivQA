# [Words are all you need? Language as an approximation for human   similarity judgments](https://arxiv.org/abs/2206.04105)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: To what extent can language capture human sensory experiences and similarity judgments across different modalities (images, audio, video)? The authors approach this question by:1) Collecting human similarity judgments for stimuli across 3 modalities (images, audio, video). 2) Collecting two types of textual descriptors for the stimuli: free-form captions and concise semantic tags.3) Using word and language embedding models to generate predictions of human similarity judgments based solely on the textual descriptors. 4) Comparing these text-based similarity predictions to similarity judgments produced by a wide array of sensory/perceptual models, including 611 pre-trained deep neural networks across the 3 modalities.5) Evaluating whether language provides a good approximation of human similarity judgments across modalities and how it compares to sensory models.So in summary, the central hypothesis is that language can serve as an effective predictor or approximation of human similarity judgments across diverse modalities. The authors test this hypothesis by comparing text-based similarity predictions to human judgments and sensory model predictions.
