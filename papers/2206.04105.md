# [Words are all you need? Language as an approximation for human   similarity judgments](https://arxiv.org/abs/2206.04105)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: To what extent can language capture human sensory experiences and similarity judgments across different modalities (images, audio, video)? 

The authors approach this question by:

1) Collecting human similarity judgments for stimuli across 3 modalities (images, audio, video). 

2) Collecting two types of textual descriptors for the stimuli: free-form captions and concise semantic tags.

3) Using word and language embedding models to generate predictions of human similarity judgments based solely on the textual descriptors. 

4) Comparing these text-based similarity predictions to similarity judgments produced by a wide array of sensory/perceptual models, including 611 pre-trained deep neural networks across the 3 modalities.

5) Evaluating whether language provides a good approximation of human similarity judgments across modalities and how it compares to sensory models.

So in summary, the central hypothesis is that language can serve as an effective predictor or approximation of human similarity judgments across diverse modalities. The authors test this hypothesis by comparing text-based similarity predictions to human judgments and sensory model predictions.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

1. Introducing a novel pipeline for efficiently collecting human similarity judgments that scales linearly (O(N)) rather than quadratically (O(N^2)) in the number of stimuli. This is done by collecting textual descriptions (captions and tags) for stimuli and then using language models to approximate similarity judgments based on those descriptions. 

2. Proposing and validating STEP-Tag, an adaptive tagging procedure for crowdsourcing meaningful and diverse tags that describe salient aspects of stimuli.

3. Conducting a large-scale comparison of over 600 pre-trained deep neural networks (DNNs) across 3 modalities (images, audio, video) for approximating human similarity judgments. The results show a gap between DNNs and classical methods requiring O(N^2) judgments.

4. Demonstrating that the proposed language-based methods using pre-trained language models (LLMs) provide better approximations of human similarity compared to the DNNs, while still being much cheaper than classical methods.

5. Introducing stacked methods that combine both DNN and LLM embeddings to generate the overall best approximations of human similarity judgments across modalities.

6. Providing a guide for researchers summarizing the best practices for scalable human similarity judgment collection or approximation based on the results.

7. Releasing a large dataset of human similarity judgments and text descriptions, comprising over 200,000 ratings, to accompany the guide.

So in summary, the key contribution is providing an efficient pipeline for approximating human similarity judgments at scale by using semantic textual descriptions and validating its effectiveness compared to a wide array of alternative methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes and evaluates language-based methods for efficiently approximating human similarity judgments across modalities, finding they outperform deep neural networks and provide a scalable alternative to collecting exhaustive pairwise comparisons.


## How does this paper compare to other research in the same field?

 This paper offers a comprehensive comparison of methods for approximating human similarity judgments, evaluating over 600 pre-trained models across images, audio, and video. It makes several key contributions:

- Proposes efficient language-based methods ($O(N)$) for approximating similarity that are more generalizable and scalable than relying solely on domain-specific models ($O(1)$). This allows for estimating similarity without needing in-domain DNNs.

- Introduces STEP-Tag, a novel adaptive pipeline for crowdsourcing concise and meaningful word tags. This provides an efficient way to collect textual descriptors. 

- Tests stacked models combining language and perceptual information, finding they consistently perform the best across modalities. This suggests they capture complementary aspects of human judgments.

- Releases a large behavioral dataset with human similarity ratings and textual descriptors across three modalities, enabling further research. 

Compared to prior work like Peterson et al. 2018 and Marjieh et al. 2022 which studied images, this paper evaluates many more models (600+ vs â‰¤100) and modalities (image/audio/video vs just images). It also proposes methods that do not require domain-specific DNNs, unlike some prior work that relied solely on perceptual embeddings. The scale of the behavioral data collected and released significantly exceeds prior efforts.

Overall, this paper provides one of the most extensive comparisons of human similarity approximation methods. The proposed techniques and comprehensive empirical evaluations across modalities will serve as a guide for future research needing to estimate human similarity judgments. The public data will also facilitate further studies on this topic.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Exploring other techniques for pre-processing captions and aggregating representations from multiple captions in ways that could improve correlation with human similarity judgments. The authors tried simple averaging and concatenation but noted there may be better ways to combine multiple captions into a single representation.

- Further analysis to determine what specific architectural components of deep learning models contribute to more human-like performance on similarity judgments. The authors found number of parameters and architecture were better predictors than performance on ImageNet, but more analysis is needed to understand exactly what aspects make some architectures better.

- Expanding the study of representational similarity between humans and machines to more languages and diverse cultures using the proposed methods. The authors suggest their text-based approach and STEP-Tag procedure could enable efficient crowdsourcing of textual descriptors across languages.

- Incorporating human similarity judgments into the training objectives of deep neural networks, for example through contrastive learning. The authors suggest this could help improve model representations and alignment with human judgments.

- Addressing the risk of bias amplification when using text-based similarity proxies in model training. The authors note care must be taken regarding what biases exist in the textual descriptors.

- Further prompt engineering with large language models like GPT-3 to increase performance in predicting similarity from text descriptions. The authors found initial promising results using GPT-3 prompting.

- Closing the remaining gap between model predictions and inter-rater reliability bounds on similarity. The authors observe there is still room for improvement to achieve human-level performance.

In summary, the main future directions focus on improving text-based similarity approximation methods, expanding to new datasets and languages, incorporating similarity into model training objectives, and further alignment between human and machine representations.


## Summarize the paper in one paragraph.

 The paper introduces a comprehensive comparison of methods for approximating human similarity judgments. It evaluates pre-trained deep neural networks, large language models, and word frequency analysis techniques on their ability to predict human similarity ratings for images, audio clips, and videos. The key findings are:

1) Stacked methods that combine sensory embeddings (from domain-specific models like convolutional neural networks) with textual embeddings (from large language models) consistently provide the best approximation of human similarity across modalities. Interestingly, these outperform jointly trained multi-modal models like CLIP. 

2) Methods based purely on textual features (tags or captions) are more efficient, requiring only O(N) judgments to collect descriptions for N stimuli. Despite not having access to sensory inputs, language-based methods still reach high performance, sometimes exceeding domain-specific models.

3) Classical word frequency analysis techniques are competitive with neural methods, even without access to embeddings. This suggests their potential value for settings like low-resource languages. 

4) Across modalities, all methods exhibit a gap compared to human inter-rater reliability. This indicates room for improvement in capturing human notions of similarity.

The paper introduces a novel tag mining procedure (STEP-Tag) and releases new behavioral datasets. Overall, it provides an extensive empirical evaluation and clear recommendations for cost-effective approximations of human similarity judgments.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper explores whether language can be used to approximate human similarity judgments across different modalities such as images, audio, and video. The authors point out that collecting a complete dataset of human similarity judgments requires an unfeasibly large number of comparisons, so they investigate cheaper alternatives. First, they evaluate using deep neural networks (DNNs) to extract embeddings and predict similarity, testing over 600 pre-trained models. However, they find DNNs do not perform as well as true human judgments. Next, they propose using text data such as tags and captions to predict similarity through word embeddings and frequency analysis. To collect tags, they introduce a new crowdsourcing pipeline called STEP-Tag. They show language-based methods can approximate human judgments nearly as well as DNNs, despite requiring no pre-trained models. The best performance comes from "stacked" models combining both DNN and language embeddings. Based on their comprehensive comparison, the authors provide concrete guidelines for how researchers can cheaply and accurately approximate human similarity judgments. They also release the behavioral data they collected, including text-predicted similarity matrices for 1,000 audio clips and videos.

In summary, this paper shows that language can efficiently predict human similarity judgments across modalities, though the best performance requires combining language with sensory representations. The authors' proposed methods and comprehensive experiments provide researchers concrete tools and insights on approximating human similarity at scale. Their work combines machine learning and cognitive science to better understand similarities between human and machine perception.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes using language (specifically, text embeddings of tags and captions) to efficiently approximate human similarity judgments. The key idea is that collecting textual descriptions for stimuli scales linearly ($O(N)$) with the number of stimuli, whereas collecting full human similarity matrices scales quadratically ($O(N^2)$). To generate the text data, the authors introduce a novel crowdsourcing pipeline called STEP-Tag for collecting concise and meaningful tags. They then use pre-trained language models to embed the tags and captions into vector representations, and compute cosine similarity between these vectors to generate similarity scores. As a comparison, they evaluate the performance of over 600 pre-trained computer vision, audio, and video models on predicting human similarity judgments. They find that the text-based methods are cheaper to collect yet competitive in performance with these perceptual models. The text methods are also more general since they do not require access to domain-specific models. Finally, the authors propose combining the best text embeddings with the best perceptual embeddings to create "stacked" representations that yield the overall highest correlation with human similarity judgments.
