# [Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to   Identify Trajectory Prediction Vulnerabilities for Autonomous Driving   Security](https://arxiv.org/abs/2401.10313)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Trajectory prediction models are being integrated into autonomous vehicles, but they may be vulnerable to adversarial attacks. Prior work has shown attacks on state history inputs, but the impacts of perturbations on other inputs like maps is not well studied. 
- It's also not clear how adversarial perturbations to the prediction model may affect downstream vehicle planning and control.

Methods:
- The authors conduct a sensitivity analysis on two trajectory predictors - Trajectron++ and AgentFormer - to understand how perturbations on different input features (state histories, maps, graph encoding) impact prediction error.
- They measure sensitivity using the percent increase in average displacement error (ADE) between the original and perturbed predictions. 
- They demonstrate undetectable image perturbations that can greatly increase ADE, revealing a potential vulnerability.
- Using an optimization-based planner, they show how perturbations informed by the sensitivity analysis can cause sudden stopping behaviors.

Key Contributions:
1) A framework for attributing a trajectory predictor's sensitivity to different input features 
2) Findings that both models are most sensitive to recent position/velocity perturbations
3) Examples of small, undetectable image perturbations causing large ADE increases  
4) Identification of a vulnerability - small perturbations accumulating over less important features
5) Demonstration that input perturbations can propagate to the vehicle planner, causing unsafe behaviors

In summary, the authors perform an analysis to understand trajectory predictor sensitivities, reveal a potential vulnerability to well-designed perturbations, and show how this could impact vehicle planning and control. The results have implications for the security and robustness of these prediction models.


## Summarize the paper in one sentence.

 This paper conducts a sensitivity analysis on trajectory prediction models Trajectron++ and AgentFormer, demonstrates the impact of small but carefully crafted image perturbations, and shows how these perturbations propagate to downstream vehicle planning.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1) A sensitivity analysis framework for trajectory prediction models that preserves outlier information and attributes perturbation sensitivities to different input features (Contribution 1 in Section 1). The analysis shows that Trajectron++ is highly sensitive to perturbations on the most recent position and velocity state history inputs, while preliminary results suggest AgentFormer has sensitivities more spread out across state history inputs over time.

2) An illustration of how small, undetectable image perturbations can exploit a vulnerability in trajectory predictors by accumulating and causing significant increases in prediction error, even if the image inputs do not contribute much to the predictions (Contributions 2 and 3 in Section 1). Examples are shown for both Trajectron++ and AgentFormer.

3) A demonstration of how the sensitivity analysis can inform targeted perturbations that, when fed into an optimization-based planner utilizing Trajectron++, can cause the planner to make the vehicle come to a sudden stop from moderate driving speeds (Contribution 4 in Section 1).

In summary, the main contributions focus on analyzing perturbation sensitivities of trajectory predictors, revealing and demonstrating a potential vulnerability exploiting small image perturbations, and showing impacts on downstream vehicle planning. The sensitivity analysis method itself to attribute perturbation effects to inputs is also presented as a main contribution.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it are:

- Adversarial Machine Learning
- Sensitivity Analysis  
- Trajectory Forecasting
- Autonomous Driving
- Security of Cyber-Physical Systems

These keywords are listed directly after the abstract in the paper. The paper conducts a sensitivity analysis on trajectory prediction models Trajectron++ and AgentFormer. It examines the effects of perturbations on different input features like state histories and map images. It also demonstrates how adversarial perturbations informed by the sensitivity analysis can impact downstream vehicle planning when using Trajectron++. Overall, the key themes relate to understanding model sensitivities, adversarial attacks, trajectory prediction, and impacts on autonomous driving systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a sensitivity attribution function to quantify how much a specific perturbation on each input feature contributes to a change in the performance of the prediction model. What are some limitations of using percent increase in average displacement error as the measure of model performance change? Could other metrics like likelihood better capture sensitivity?

2. The choice of perturbation functions seems to be based on the inductive biases of the models' architectures. However, are there other potential perturbation types that could reveal additional sensitivities not explored? For example, could perturbing acceleration or higher order derivatives reveal anything new? 

3. For the depth analysis in Figure 3, are the sensitivities concentrated on only the most recent state because of how the loss function is designed? Could a multi-step loss attenuate this recency bias in the sensitivities? 

4. While extreme outliers for image sensitivities are shown, what proportion of the dataset exhibited nontrivial image perturbation sensitivities? Could more analysis be done to characterize what conditions lead image perturbations to be impactful?

5. The image perturbations demonstrate potential model vulnerabilities, but what defenses could make the models more robust to these perturbations? Adversarial training, regularization, or encoding invariances?

6. For the downstream planning analysis, were any constraints added to ensure safety or feasibility after perturbations? What effect would constraints have on the propagation of bad predictions?

7. What other planning or control algorithms could be tested to analyze the effects of perturbed predictions? How do model-predictive control, rapidly-exploring random trees, or reinforcement learning controllers propagate erroneous predictions?

8. Are the perturbation functions proposed reflective of real-world attacks that could be mounted on autonomous vehicles? What practical attack vectors should be considered?

9. How sensitive are the results to hyperparameters like prediction horizon, history length, and perturbation magnitudes? Could insights change with different experimental settings?

10. For real-time autonomous driving systems, what failure modes occur if predictions suddenly change drastically? Could fail-safe behaviors handle cases where perturbations cause mode collapse?
