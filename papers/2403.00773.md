# [Misconduct in Post-Selections and Deep Learning](https://arxiv.org/abs/2403.00773)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper analyzes the problem of "post-selection" in machine learning, where authors train multiple models but only report performance of the luckiest model on a validation set. This leads to inflated performance estimates. 

- Specifically, the paper examines "deep learning" methods where multiple neural networks with different weight initializations and architectures are trained, but only the luckiest network is selected and reported.

- The paper argues that simply using cross-validation on the input data is insufficient to address the statistical issues with post-selection. Even with cross-validation, biased performance estimates can result.

Proposed Solution:
- The paper proves a "Lost Luck Theorem" which states that the expected performance of the luckiest validated model will be the same as the average performance of all trained models.

- Therefore, proper evaluation requires reporting performance metrics over all trained models, not just the luckiest. The mean and percentile locations of the error distribution should be reported.

- The paper also examines using "nest" cross-validation with folds at both input and output. But proves that tricks like NNWT and PGNN can still exploit this to give perfect validated performance, while generalizing poorly.

Main Contributions:
- Formalizes the problem of post-selection bias in deep learning and proves theoretically why it leads to inflated performance estimates

- Demonstrates weaknesses of using cross-validation in resolving the issues with post-selection 

- Provides specific recommendations for proper experimental methodology like reporting mean and percentile performance over all trained models

- Discusses implications in fields like social sciences where similar issues with selection bias arise (e.g. selective reporting of metrics for national development)

In summary, the paper provides a rigorous statistical analysis of post-selection bias and demonstrates limitations of techniques like cross-validation in addressing it. The key insight is that performance must be reported over all models, not just the luckiest.


## Summarize the paper in one sentence.

 This paper theoretically proves that post-selection of machine learning models based on validation set performance, even with cross-validation, is statistically invalid for estimating expected performance on a future test set.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It generalizes cross-validation to apply not only to random weights but also to architecture hyperparameters that are typically manually tuned or searched. This means Post-Selection methods perform much worse than reported.

2) It proves that traditional cross-validation for data splits, even with nesting at both input and output ends, does not rescue Post-Selection from misconduct. The performance on a future test is still the average, much worse than authors report. 

3) It relates the statistical invalidity of Post-Selection to issues in social sciences like evaluating national development. Selecting only positive national pride metrics based on a limited validation set is not optimal for future tests.

4) It strengthens previous theoretical results on Deep Learning misconduct by establishing traditional cross-validation cannot rescue Post-Selection, and performance estimates should be based on the average of all trained networks, not just the luckiest few.

In summary, the paper theoretically establishes the statistical invalidity of Post-Selection even with cross-validation, and relates this to flawed reasoning in complex domains like social policy and national development evaluations.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and concepts associated with this paper include:

- Post-Selection - Selecting the best or luckiest models from multiple trained models based on validation set performance. The paper argues this is statistically invalid.

- Deep Learning misconduct - Alleged cheating and hiding of bad results that is common in deep learning research through post-selection.

- Cross-validation - Validating model performance by splitting data into different folds and averaging results across folds. The paper discusses limitations of traditional cross-validation for rescuing post-selection. 

- Lost Luck Theorem - Key theorem stating that the luckiest model on a validation set will have the same expected error as other less lucky models on a future test set.

- General cross-validation - Proposed cross-validation approach that averages performance across not only data splits but also across model architectures and weight initializations. 

- Nest cross-validation - Performing cross-validation at both the input data level and output data level. The paper argues this still does not rescue post-selection.

- Super Learner - Existing ensemble method that gives weights to different models to minimize cross-validated error. The paper shows how it can be fooled by methods like NNWT and PGNN.

So in summary, the key topics revolve around post-selection, misconduct in machine learning, cross-validation, and statistical validity.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a "general cross-validation" principle that extends traditional cross-validation for data splits to also include cross-validation across model parameters like architecture hyperparameters and neural weights. What are the key advantages and disadvantages of using general cross-validation compared to traditional cross-validation?

2. The Lost Luck Theorem states that the luckiest model on the validation set will have the same expected error as any other less lucky model in a future test set. What are the implications of this result for common practices in deep learning where only the top few "lucky" models are reported? 

3. The paper shows that methods like NNWT and PGNN can achieve zero error on cross-validated data splits. However, they are impractical methods. What specific issues make these methods impractical, despite their ability to fit the validation data perfectly?

4. The Super Learner ensemble method weights multiple models to minimize cross-validated error. However, the paper argues that methods like NNWT and PGNN could "fool" the Super Learner. How could such methods fool the Super Learner and why does cross-validation fail to prevent this?

5. The paper discusses the high cost of methods like NNWT and PGNN in terms of computational resources required. How do we properly account for developmental costs like storage, computation time, etc. when evaluating machine learning methods?

6. When applying the results to social issues, the paper argues against "post-selecting" metrics like national pride while ignoring costs. What other key social science issues are subject to biases from post-selection and how can a statistical view help?

7. The mismatch between performance on the validation set and expectations for the test set occurs because the post-selection step contaminates the validation set metrics. What modifications could make post-selection more statistically valid? 

8. For national development, the paper argues against short-sighted selection of suboptimal systems that look good on internal metrics only. What insights does this provide for long-term planning in complex policy domains?

9. What open questions remain in formally characterizing the types of methods and data where we could expect post-selection to show real improvements on unseen test data?

10. How do the theoretical results extend to other areas like reinforcement learning where an agent "post-selects" beneficial policies based on simulated experience? What constraints need to be satisfied there?
