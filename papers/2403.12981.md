# [Beyond Inference: Performance Analysis of DNN Server Overheads for   Computer Vision](https://arxiv.org/abs/2403.12981)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep neural network (DNN) inference has become critical for many datacenter workloads, especially computer vision applications. This has driven optimize efforts on inference accelerators like GPUs and TPUs.
- However, a full end-to-end DNN application contains more than just inference, including tasks like data decompression, resizing, normalization etc. The impact of these "DNN inference overheads" on overall system performance is not well understood.

Goal: 
- Quantify the performance impact of server overheads like data movement, preprocessing etc. for computer vision inference requests on a throughput-optimized system.

Methodology:
- Implement and profile image classification, segmentation, detection etc. workloads on a CPU-GPU server running state-of-the-art optimization software like TensorRT and Triton.  
- Vary parameters like image size, model complexity, hardware used for preprocessing etc. to expose bottlenecks.
- Also study more complex pipelines with multiple DNNs and message brokers in between.

Key Results:
- Non-inference tasks can account for over 50% of latency even on a GPU server.  Preprocessing time increases for larger image sizes.
- At high concurrency, queuing latency can be ~60% of total latency.
- Preprocessing is a throughput bottleneck, especially for small models and large images. End-to-end throughput can be 5x worse than inference-only throughput.
- Accelerating preprocessing on the GPU helps but hitting limits due to memory capacity at high loads. Multi-GPU scaling efficiency also reduced.
- For multi-DNN systems, in-memory message brokers have much lower overhead than prior estimates.  

Contributions:
- First paper to thoroughly analyze the impact of preprocessing and data movement overheads for vision workloads.
- Achieved 2.25x higher throughput compared to prior work on multi-DNN systems.
- Results pave the way for more holistic optimization of end-to-end deep learning systems, not just the DNN execution.
