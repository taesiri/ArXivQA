# [Epistemic Exploration for Generalizable Planning and Learning in   Non-Stationary Settings](https://arxiv.org/abs/2402.08145)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the challenge of continual planning and learning in non-stationary stochastic environments, where the dynamics of the environment can change arbitrarily over time. This is a common challenge in real-world deployment of planning systems, where things like spills, blocked paths, or changing inventory can alter the dynamics of a robot's environment. Existing approaches struggle to efficiently adapt their models and plans when such unpredictable changes occur.

Proposed Solution:
The paper proposes a new framework for continual learning and planning that can detect when the environment dynamics no longer match the agent's learned model, focus exploration on the inconsistent parts of the model to improve it, use the updated model to replan for the current goal, and leverage the relational structure of the learned models to enable transfer across problems. 

Key ideas include:
- Learning relational PPDDL models that can generalize across problems with different objects/goals
- Active learning to resolve model uncertainties and handle non-stationarity 
- Monitoring execution to detect transitions inconsistent with learned model
- Updating just the inconsistent predicates rather than relearning the full model
- Using model to replan when changes detected instead of relying on model-free RL
- Theoretical guarantees on convergence when environment is stationary

The algorithm interleaves active model learning, monitoring for inconsistencies, selectively updating the inconsistent parts of the model, and replanning using the updated model.

Contributions:
- A new paradigm for continual planning and learning under non-stationarity by coupling active learning, focused model updating, and replanning
- Significantly increased sample efficiency compared to model-free RL and model-based baselines
- Ability to automatically adapt model and plan when environment dynamics change
- Leveraging relational models for improved generalizability across problems
- Theoretical results on convergence in stationary settings

Experiments in several stochastic planning domains demonstrate the approach can quickly adapt to changes, solve more tasks given a fixed budget, and learn policies comparable to an oracle with full knowledge of the true dynamics.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments by modeling gaps in the agent's knowledge to conduct focused investigative explorations and use the collected data to learn generalizable probabilistic models for solving current and future tasks despite continual changes in the environment dynamics.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

"Our main contribution is the first known approach for using information about epistemic uncertainty of a logic-based internal probabilistic model to create exploration strategies, learn better models, and then compute plans even as transition systems change."

In more detail, the key contributions are:

1) A new framework for continual learning and planning under non-stationarity for relational Markov decision processes (RMDPs). The framework can detect and adapt to changes in the transition dynamics of the environment.

2) Solution algorithms that conduct focused, investigative explorations to resolve inconsistencies between the learned model and the actual environment dynamics. The data collected is used to learn more accurate probabilistic models.

3) Empirical evaluations showing the approach significantly outperforms planning and reinforcement learning baselines in terms of sample complexity.

4) Theoretical results showing the system reverts to exhibit desirable convergence properties when stationarity holds.

In summary, the main novelty is using epistemic uncertainty in a learned symbolic model to guide explorations that improve the model and enable continual planning even as the dynamics change over time.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms appear to be:

- Non-stationary settings
- Relational Markov decision processes (RMDPs)  
- Probabilistic planning 
- Model learning
- Active learning
- Exploratory policies
- Sample complexity
- Continual planning
- Transfer learning
- Goodness of fit tests

The paper introduces an approach for continual planning and model learning in non-stationary stochastic environments represented using relational models. Key aspects include using active learning to guide exploratory policies for resolving uncertainties in the learned model, monitoring execution to detect changes in dynamics, and selectively updating the model to adapt it to changes. Evaluations analyze the sample complexity and ability of the approach to transfer knowledge across tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the approach handle trade-offs between selectively updating the model versus re-learning the full model from scratch when changes in the transition system are detected? What heuristics could help determine which approach is more efficient? 

2. The goodness-of-fit testing procedure checks for distributional shifts after changes are detected. Could more sophisticated statistical tests like KL-divergence be used instead? How would this impact sample complexity?

3. The approach uses random walks during exploration - could more directed exploration methods like intrinsic motivation help discover transitions more efficiently? How can exploration be optimized for sample complexity?  

4. What role could interleaved hierarchical reinforcement learning using temporal abstractions play in improving sample efficiency during exploration?

5. How sensitive is the variational distance based convergence guarantee to the choice of sample size? Could PAC-style bounds be derived instead that rely less on obtaining representative samples?  

6. How does the approach scale to more complex, larger domains with many predicates and actions? Are there ways to decompose the learning task that preserve performance?

7. Instead of detecting discrepancies at the level of individual transitions, can discrepancies be detected at a higher level using statistical tests on aggregated trajectories? Would this make the system more robust?  

8. What modifications are needed for the system to work in partially observable settings where the full state is not observed?

9. The goals are provided as input - what mechanisms can enable autonomous goal formulation from high-level task specifications?

10. How can lifelong learning of the model over a non- stationary stream of tasks be conducted to minimize forgetting of previously learned parts of the model?
