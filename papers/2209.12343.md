# [Paraphrasing Is All You Need for Novel Object Captioning](https://arxiv.org/abs/2209.12343)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of novel object captioning (NOC), where the goal is to generate descriptive captions for images containing objects that were not seen in the training data. The key research question is how to generate fluent, accurate, and adequate captions for novel objects without having ground truth captions available during training. The central hypothesis of this paper is that incorporating paraphrasing capabilities into image captioning models can help improve the fluency, fidelity (accuracy), and adequacy of novel object captions. Specifically, the authors propose a two-stage framework called "Paraphrasing-to-Captioning" (P2C) with the following main ideas:1. Use a pretrained language model to paraphrase generated captions to improve fluency.2. Perform self-paraphrasing using image-text alignment models as critics to improve fidelity and adequacy. 3. Use repetition penalties during self-paraphrasing to maintain fluency.So in summary, the central hypothesis is that learning to paraphrase, guided by language and image-text alignment models, can allow captioning models to generate better captions for novel objects without ground truth caption supervision. The paper aims to demonstrate the effectiveness of this P2C framework.


## What is the main contribution of this paper?

The main contribution of this paper is a novel framework for novel object captioning (NOC) called "Paraphrasing-to-Captioning" (P2C). The key ideas are:- Using paraphrasing capabilities to improve the linguistic fluency of generated captions for novel objects not seen during training. This is done by first distilling knowledge from a pretrained language model to expand the captioning model's word bank, and then enforcing self-paraphrasing objectives during training.- Leveraging cross-modality association models like CLIP to provide "pseudo-supervision" rewards when paraphrasing, to ensure the generated captions have high fidelity and adequacy in describing the visual content. - A two-stage learning framework that trains the NOC model to: 1) paraphrase and describe novel objects fluently using a language model, and 2) self-paraphrase to improve fidelity and adequacy using CLIP's image-text association scores as rewards.- Achieving state-of-the-art performance on nocaps and COCO Caption datasets, while also generating captions with better linguistic fluency, fidelity and adequacy compared to prior methods.In summary, the key contribution is a flexible NOC framework P2C that uses paraphrasing and association models to improve caption fluency, fidelity and adequacy in a label-free pseudo-supervised manner during training.
