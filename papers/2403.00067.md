# [Query-OPT: Optimizing Inference of Large Language Models via Multi-Query   Instructions in Meeting Summarization](https://arxiv.org/abs/2403.00067)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Using large language models (LLMs) for query-based meeting summarization requires calling the LLM inference API/endpoint for each new query, even if the context (meeting transcript) is the same. This significantly increases costs.
- Techniques like knowledge distillation and quantization can optimize costs but have limitations (e.g. reduced accuracy) and don't work for closed-source models like GPT-4.

Proposed Solution:  
- Investigate if combining multiple queries for the same meeting transcript into a single prompt can minimize API calls and reduce costs.
- Created a multi-query version of the QMSUM dataset and evaluated ability of various LLMs to follow multi-query instructions.

Models Evaluated:
- Closed-source: GPT-4, PaLM-2
- Open-source (default & fine-tuned): LLaMA-2, Mistral-7B, Mixtral-8x7B, FLAN-T5

Key Findings:
- Most models fail to generate outputs in the required format despite responding to multi-query prompts.
- GPT-4 stands out in its ability to reliably follow instructions and generate formatted outputs summarizing multiple queries.  
- Fine-tuned open-source models perform better on single queries than multi-queries.
- Proposed new ROUGE-based evaluation approach to compare models fairly.

Main Contributions:
- Insights on ability of different LLMs to follow multi-query prompts to optimize costs.
- Identification of GPT-4's superior performance on multi-query meeting summarization. 
- Evaluation methodology for multi-query summarization when output format varies across models.

Limitations and Future Work:
- Limited human evaluation of quality of generated summaries.
- Effects of training data size need further investigation.
- More prompt engineering could further improve multi-query performance.
