# [Did Translation Models Get More Robust Without Anyone Even Noticing?](https://arxiv.org/abs/2403.03923)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural machine translation (NMT) models are widely believed to be brittle and sensitive to noisy input text, such as spelling errors or informal language. This has motivated work on making models more robust through specialized training or architectures.

- However, recent NMT paradigms have shifted towards large pretrained models and instruction tuning, which are more opaque and may make existing robustness techniques difficult. So it's unclear if these new huge models are inherently more robust or if explicit robustness methods are still needed.

Methodology:
- The authors evaluate several NMT models on synthetic character noise and social media texts to compare robustness:
  - Conventional transformer models trained on clean parallel text (OPUS-MT) 
  - Large multilingual models (M2M-100, NLLB)
  - Instruction-tuned language models (Tower-Instruct)
  - Proprietary models (ChatGPT)

- They introduce a metric called "COMET-slope" to quantify robustness to different noise types based on the rate of decline in scores.

- They also evaluate on the social media corpus MTNT and propose using the lexical normalization dataset MultiLexNorm for reference-free MT evaluation.

Key Findings:

- Large models are much more robust to synthetic noise than conventional models, even without explicit robustness techniques. ChatGPT is the most robust across settings.

- The large models also perform better on real-world noisy social media text. However, source correction pipelines don't help much there, likely because there aren't enough genuine errors.

- Analysis shows the large models continue producing fluent output despite noise, whereas conventional models show rapidly declining fluency and adequacy.

- Surprisingly, some large models show inherent robustness despite lacking architectural features for handling character-level noise. The reasons are still unclear.

Conclusion:
- Recent large NMT models have become more robust to noise, mitigating the need for some explicit robustness methods. But better understanding is still needed of why and how large scale training leads to this implicit robustness.
