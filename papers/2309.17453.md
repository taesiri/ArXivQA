# [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:Can we deploy a large language model (LLM) for infinite-length text inputs without sacrificing efficiency and performance?The key challenges the authors identify in deploying LLMs for streaming applications with long interactions are:1) Caching previous tokens' key-value states during decoding consumes a lot of memory.2) Popular LLMs cannot generalize well to longer texts than their pre-training sequence length. To address these challenges, the paper introduces the concept of "attention sinks" in LLMs, which are initial tokens that absorb a large amount of attention regardless of their relevance. The authors propose a method called StreamingLLM that retains these attention sinks alongside a rolling cache of recent tokens to enable stable and efficient language modeling over infinite lengths.The central hypothesis is that keeping the key-value states of a few initial tokens as attention sinks, combined with a sliding window, can allow LLMs pre-trained on finite lengths to generalize to infinite sequence lengths without fine-tuning or sacrificing efficiency. The paper aims to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing StreamingLLM, an efficient framework that enables pre-trained language models to handle inputs of effectively infinite length in a streaming manner, without sacrificing performance or requiring fine-tuning. The key ideas are:- Identifying the phenomenon of "attention sinks" in autoregressive LMs, where certain tokens like initial tokens absorb a disproportionate amount of attention, acting as anchors for the attention distribution. - Leveraging attention sinks by caching the key/value states of a few initial tokens to stabilize attention, while using a rolling cache of recent tokens for language modeling. This allows pre-trained LMs to reliably handle inputs orders of magnitude longer than their training sequence length.- Showing that adding a dedicated learnable "sink token" during pre-training further improves streaming performance, by consolidating excess attention to a single token.- Demonstrating stable perplexity and QA accuracy for inputs up to 4 million tokens using Llama, MPT, Falcon, Pythia models, with up to 22x speedup over sliding window recomputation.In summary, StreamingLLM enables the efficient deployment of pre-trained LMs for streaming applications like dialog systems, without expensive retraining or degradation in performance. The key insight is the existence and importance of attention sinks for long context modeling.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes StreamingLLM, an efficient framework that enables Large Language Models trained with a finite attention window to generalize to infinitely long sequence lengths without fine-tuning. It does so by exploiting "attention sinks", a small number of initial tokens that absorb excess attention, which allows stable streaming generation when combined with a rolling cache of recent tokens. The key insight is that preserving these attention sinks can anchor model performance, while the rolling cache provides relevant context.
