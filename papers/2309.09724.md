# [Robust Geometry-Preserving Depth Estimation Using Differentiable   Rendering](https://arxiv.org/abs/2309.09724)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we train monocular depth estimation models to predict geometry-preserving depth up to scale for accurate 3D scene reconstruction, without requiring extra datasets or annotations, through mix-dataset training?

The key hypothesis is that by rendering novel views of reconstructed scenes and designing losses to promote consistency of depth prediction across views, the model can learn to produce undistorted 3D structures from depth. This allows mix-dataset training without extra data/annotations.

In summary, the paper focuses on enabling robust 3D scene structure recovery from monocular images by developing depth estimators that can generalize well across diverse scenes while preserving geometric integrity, which is a challenge for existing mix-dataset trained models. The core ideas are around using differentiable rendering and multi-view consistency losses to achieve this goal in a data-efficient manner.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a learning framework to train depth estimation models to predict geometry-preserving depth without requiring extra datasets or annotations. The key ideas are:

- Using differentiable rendering to reconstruct 3D point clouds from predicted depth maps and render novel views. 

- Designing consistency losses between rendered views and original views to promote geometry-preserving depth predictions.

- Showing the consistency losses can also recover domain-specific scale/shift coefficients and estimate focal length in a self-supervised manner.

In summary, the main contribution is developing a method to enable robust 3D scene reconstruction from monocular images using mix-dataset trained depth estimators, without needing extra data or supervision. Experiments show it outperforms prior arts in recovering geometrically accurate depth and 3D structures.
