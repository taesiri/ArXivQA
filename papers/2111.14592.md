# GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with   Semi-Supervised Learning and Explicit Policy Injection

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How to effectively inject explicit dialog policy knowledge into pre-trained conversational models, in order to improve their performance on downstream task-oriented dialog tasks?Specifically, the paper proposes a new pre-training approach called GALAXY that incorporates dialog act prediction as an auxiliary task during pre-training, to explicitly model dialog policy. The key ideas and contributions are:- Designs a unified dialog act taxonomy and collects a new labeled dataset UniDA for pre-training.- Proposes a semi-supervised pre-training paradigm that combines consistency regularization on unlabeled data and supervision from dialog act prediction on labeled data. This allows incorporating policy knowledge from limited labeled data while leveraging large unlabeled corpora. - Implements a gating mechanism to automatically select high-quality unlabeled dialog samples for consistency training.- Achieves new state-of-the-art results on several task-oriented dialog benchmarks like MultiWOZ 2.0/2.1. Shows stronger few-shot ability than previous models.In summary, the central hypothesis is that incorporating explicit dialog policy modeling via semi-supervised pre-training can improve the performance of conversational models on downstream task-oriented dialog applications. The GALAXY model with the proposed pre-training approach is presented as a method to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes GALAXY, a novel pre-trained dialog model that can explicitly learn dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. 2. It designs a unified dialog act (DA) taxonomy and collects a new labeled dataset UniDA for dialog policy modeling in the pre-training stage.3. It introduces a consistency regularization loss on unlabeled dialog data to facilitate better representation learning. A gating mechanism is also proposed to weigh suitable unlabeled samples.4. Experiments show GALAXY achieves new state-of-the-art results on several task-oriented dialog benchmarks like MultiWOZ 2.0 and 2.1. It also has stronger few-shot ability than previous models under low-resource settings.In summary, the main contribution is proposing a novel semi-supervised pre-training approach called GALAXY to inject explicit dialog policy knowledge into pre-trained conversation models, which improves the performance on downstream task-oriented dialog tasks. The new labeled dataset UniDA and regularization method for unlabeled data are also contributions.
