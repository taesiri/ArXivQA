# [Interpreting Black Box Models via Hypothesis Testing](https://arxiv.org/abs/1904.00045)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can we provide statistical guarantees on the error rate when interpreting black box machine learning models, especially for high-stakes applications like science and medicine where false discoveries can have serious consequences?The paper argues that most existing methods for interpreting machine learning models are ad hoc and lack rigorous statistical guarantees on the rate of false discoveries. This is especially problematic for applications like healthcare where model interpretations may guide medical decisions and false discoveries could potentially lead to patient harm. To address this issue, the authors propose reframing model interpretability as a multiple hypothesis testing problem. Specifically, they test whether replacing certain input features with randomly sampled counterfactual values significantly changes the model output. If the output changes significantly, those features can be considered "important" for that prediction. The main contribution is a principled framework and methods (the Interpretability Randomization Test and One-Shot Feature Test) that provide statistical control on the false discovery rate when determining important features. This allows the reliability of reported model interpretations to be quantified, avoiding an excessive number of false discoveries.In summary, the key hypothesis is that by casting black box interpretability as a hypothesis testing problem, they can derive procedures that offer rigorous control on the finite-sample error rate of interpretations - something lacking in most existing interpretability methods but crucial for high-stakes applications. The paper aims to demonstrate this hypothesis through the proposed methods.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Framing black box model interpretability as a multiple hypothesis testing problem in order to provide statistical guarantees on the error rate of interpretations. The paper proposes testing whether model predictions change significantly when features are replaced with samples from a counterfactual distribution.- Proposing two hypothesis testing methods - the Interpretability Randomization Test (IRT) and One-Shot Feature Test (OSFT) - that can control the false discovery rate when determining important features. The IRT provides strict FDR control but requires many model evaluations, while the OSFT is faster but only controls FDR approximately.- Demonstrating the utility of these methods on vision and language models, where they are able to select intuitively important features to explain model predictions. The methods leverage recent advances in deep generative models to sample counterfactual inputs.- Providing a general framework for model interpretation that could be applied to different counterfactual distributions and test statistics depending on the application. The framework aims to bring more statistically rigorous techniques to bear on the problem of interpreting black box machine learning models.In summary, the main contribution is a hypothesis testing framework for interpretable machine learning that enables statistical control on the error rate of explanations by testing model predictions against counterfactual inputs. This is demonstrated on vision and language tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework to interpret black box machine learning models by framing model interpretability as a multiple hypothesis testing problem, with the goal of discovering important features while controlling the false discovery rate.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of black box model interpretability:- The key novelty of this paper is framing interpretability as a hypothesis testing problem, which allows statistical control over the false discovery rate when selecting important features. Most prior work has focused on optimization-based or gradient-based approaches, without statistical guarantees on selected features.- By casting the problem as hypothesis testing, the authors are able to leverage ideas like knockoffs from the statistical/conditional independence testing literature. Making this connection is an innovative way to bring more rigorous statistics into interpretability.- The paper proposes two new testing procedures - the Interpretability Randomization Test (IRT) and One-Shot Feature Test (OSFT). The IRT controls the FDR but is computationally intensive, while OSFT is fast but only controls FDR in limited cases.- The authors demonstrate the utility of their methods on state-of-the-art vision and language models. Applying interpretability methods to complex neural networks trained on real-world data represents the current research frontier.- Compared to optimization-based methods like LIME and SHAP, a key advantage is error control on selected features. Compared to saliency and gradient methods, the proposed tests are model-agnostic.- A limitation is the need for conditional sampling, though the rapid progress in deep generative models makes this increasingly feasible. The reliance on counterfactuals is similar to some recent work like that of Goyal et al. 2019.In summary, this paper makes a strong connection between interpretability and rigorous statistics that opens up many possibilities for future work and represents an advance over current methods. The proposed hypothesis testing viewpoint is novel and promising.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Improving the generative/conditional models used for generating counterfactuals in the proposed framework. The authors note that as these models continue to improve, methods like the IRT and OSFT will become more robust and practical. They mention recent work on probabilistic image inpainting models as an example.- Exploring the application of the framework to other interpretability questions beyond just feature importance, such as testing for issues related to fairness, reliance on texture cues, etc. The authors suggest constructing suitable generative models for the counterfactuals depending on the specific question.- Investigating other potential test statistics beyond the one-sided and two-sided statistics discussed. The authors mention statistics that could test more nuanced questions like whether the model uses a feature for a specific class or in general. - Automating the selection of feature subsets to test, rather than manual selection. They mention using techniques like object detection, image segmentation, dependency parsing, etc. to automatically identify candidate features in images, text, and other data types.- Applying the framework beyond deep classifiers to other models like regressors. The authors also mention sample-level or group-level variable selection.- Considering other types of statistical guarantees for interpretability methods beyond FDR control. For example, providing confidence intervals or error bars for feature importance values.- Generally, continuing work to make interpretability methods more statistically rigorous and reliable. The authors view their framework as an initial step in this direction.In summary, the main suggestions are around improving the conditional models for counterfactuals, expanding the framework to new interpretability settings and models, automating feature selection, and developing more rigorous statistical guarantees for interpretations.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper presents a framework for interpreting black box machine learning models by casting model interpretability as a multiple hypothesis testing problem. The key idea is to test whether replacing subsets of input features with randomly sampled counterfactuals significantly changes the model's prediction. If so, those features are deemed "important" for that prediction. The authors propose two testing procedures within this framework - the Interpretability Randomization Test (IRT) and One-Shot Feature Test (OSFT) - that provide statistical control of the false discovery rate when determining important features. In simulations, these tests have high power compared to existing methods. When applied to image and text classifiers, the OSFT selects features that intuitively explain model predictions. Framing interpretability as hypothesis testing allows for more reliable explanations with finite sample guarantees.


## Summarize the paper in two paragraphs.

Here is a summary of the paper in two paragraphs:This paper presents a statistical framework for interpreting predictions from black box machine learning models, with a focus on applications like science and medicine where false discoveries can have serious consequences. The key idea is to frame interpretability as a hypothesis testing problem, where features are tested for whether replacing them with samples from a counterfactual distribution significantly changes the model's output. This allows control over the false discovery rate when determining important features. The authors propose two testing methods: the Interpretability Randomization Test (IRT), which provides provable FDR control but is computationally expensive, and the One-Shot Feature Test (OSFT), a faster approximate test. In experiments, both have higher power than existing methods while controlling FDR. On real vision and language tasks, OSFT tends to select intuitive features that explain model predictions. An advantage of this approach is one can inspect the counterfactual inputs to verify selections, providing reassurance. Overall, this statistical framework and the proposed testing methods represent a principled approach to model interpretation that prevents false discoveries.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework for interpreting black box machine learning models by casting model interpretability as a multiple hypothesis testing problem. Given a trained model and an input sample, the goal is to determine which features of the input are "important" for the model's prediction. This is done by testing whether replacing the original features with randomly drawn counterfactual features significantly changes the model output. If so, those features can be deemed important. Specifically, the null hypothesis is that the model output comes from the same distribution whether using the original or counterfactual features. The paper introduces two testing procedures: the Interpretability Randomization Test (IRT), which provides statistical control of the false discovery rate but is computationally intensive, and the One-Shot Feature Test (OSFT), which is much faster but only approximately controls the false discovery rate. These methods are evaluated on image classification and text sentiment analysis tasks. Overall, the proposed framework enables interpreting black box models while rigorously controlling the error rate of the identified important features.


## What problem or question is the paper addressing?

The paper appears to be addressing the need for more rigorous and reliable interpretation methods for complex machine learning models, particularly in high-stakes domains like science and medicine. The key points seem to be:- Many existing interpretability methods for machine learning models are ad hoc and hard to evaluate, and don't provide any statistical guarantees on the error rate of the interpretations. This lack of reliability is problematic when using models to guide scientific research or medical treatments.- The paper proposes reframing model interpretability as a hypothesis testing problem, where the goal is to determine which input features significantly influence the model output compared to a "null" distribution where those features are replaced by counterfactuals. - They introduce two testing procedures (IRT and OSFT) within this framework that enable control of the false discovery rate, providing statistical rigor.- The methods are evaluated on synthetic data and on image classification and text sentiment models, demonstrating their ability to discover important features while controlling the FDR.- The framework provides visually interpretable explanations by showing how the prediction changes when features are replaced by counterfactuals. This allows users to sanity check the selected important features.So in summary, the key focus is developing more statistically principled and reliable methods for interpreting complex black box models, with applications like science and medicine in mind where avoiding false discoveries is critical. The paper proposes a hypothesis testing approach to achieve this.
