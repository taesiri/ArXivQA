# [An Infinite-Width Analysis on the Jacobian-Regularised Training of a   Neural Network](https://arxiv.org/abs/2312.03386)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper extends the recent infinite-width analysis of deep neural networks to studying the input-output Jacobian of a multilayer perceptron (MLP). Theoretically, it is shown that at initialization, an MLP and its Jacobian jointly converge to a Gaussian process as the MLP widths go to infinity. The kernel characterizing the joint limiting Gaussian process is derived inductively. It is also proved that in the infinite-width limit, training an MLP to minimize a loss function regularized by the Jacobian norm is described by a linear ordinary differential equation. Experimentally, the relevance of the theoretical claims to wide finite MLPs is confirmed, and the robustness properties of the kernel regression solution of the ODE are analyzed. Overall, through analysis in the infinite-width regime, new insights are provided into training MLPs to output smooth functions that generalize well under input perturbations.
