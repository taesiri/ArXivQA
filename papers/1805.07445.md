# [DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors](https://arxiv.org/abs/1805.07445)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis appears to be that relaxations of Boltzmann machine priors can improve the performance of discrete variational autoencoders (DVAEs). Specifically, the authors propose two approaches for relaxing Boltzmann machines to continuous distributions that allow training DVAEs with tighter importance-weighted bounds. The key questions/hypotheses seem to be:

- Can continuous relaxations of Boltzmann machine priors enable training discrete VAEs with tighter importance-weighted bounds compared to previous methods like DVAE and DVAE++? 

- Will the proposed relaxations based on overlapping transformations and the Gaussian integral trick outperform previous DVAE methods with Boltzmann priors?

- Can they develop more general overlapping transformations beyond previous work to provide better approximations of discrete variables? 

- Will their proposed power-function overlapping transformation provide lower variance gradients and improved performance compared to things like exponential transformations?

So in summary, the central hypothesis is around developing better continuous relaxations of Boltzmann machine priors to improve training of discrete VAEs, especially with tighter variational bounds. The key questions focus on whether their proposed relaxations can outperform previous DVAE methods and whether properties like lower variance gradients from the power-function transformation translate to better performance.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Introducing two continuous relaxations of Boltzmann machines that allow them to be used as priors in variational autoencoders (VAEs) and trained with tighter importance-weighted bounds. The two relaxations are based on overlapping transformations and the Gaussian integral trick.

2. Generalizing overlapping transformations to any pair of distributions with computable PDF and CDF. This allows for new smoothing transformations like mixtures of Gaussians and power functions. 

3. Applying these relaxations and new transformations to train discrete VAEs with Boltzmann machine priors using importance weighting. The proposed models, called DVAE#, outperform previous discrete VAEs with Boltzmann priors like DVAE and DVAE++ on MNIST and OMNIGLOT datasets.

4. Demonstrating that power-function overlapping transformations provide lower variance gradient estimates and improved test log-likelihoods compared to exponential transformations, especially when the inverse temperature is large.

In summary, the main contribution is developing continuous relaxations of Boltzmann machines to enable training discrete VAEs with tighter bounds, proposing more general overlapping transformations, and showing improved performance of the resulting DVAE# models compared to prior discrete VAEs. The key innovations are the relaxations and new transformations that enable tighter training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper: 

The paper proposes two methods for relaxing discrete Boltzmann machine distributions to continuous distributions that can be used as priors in variational autoencoders, allowing tighter variational bounds and improved performance compared to previous discrete VAE methods.
