# [A Multi-Label Dataset of French Fake News: Human and Machine Insights](https://arxiv.org/abs/2403.16099)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fake news is a complex, multidimensional issue including fabrication, satire, mistaken reports, biased/partisan information. 
- Existing fake news detection datasets have limitations, such as only using 2 coarse-grained labels ("biased" vs "legitimate"), lacking detail on the specific type of "fakeness".
- Need datasets with more precise, fine-grained labels to build better fake news classifiers and understand human annotation cues.

Proposed Solution:
- Created a new multi-label fake news dataset called OBSINFOX with 100 French articles from unreliable sources.
- Articles annotated by 8 people using 11 labels including "Fake News", "False Information", "Opinions", "Exaggeration", etc. 
- Analyzed topics, genres and persuasion techniques using GATE tool. About half the articles written in a "satire-like" style.
- Measured inter-annotator agreement and correlation between labels. Highest agreement for "Facts" label.
- Used automated subjectivity analyzer VAGO to relate linguistic cues like opinions and exaggeration to human annotations.

Main Contributions:
- New fine-grained multi-label fake news dataset for French
- Analysis of topics, genres and persuasion techniques showing prevalence of satire
- Measurement of inter-annotator agreement on 11 annotation labels  
- Relating automated linguistic subjectivity scores to human annotations, showing closer ties to "Subjective" than "Fake News" label
- Can use dataset for further analysis of how "Fake News" label relates to other labels

Limitations: 
- Small dataset size.
- Annotators not necessarily representative of larger population.
