# [Evaluation and Improvement of Interpretability for Self-Explainable   Part-Prototype Networks](https://arxiv.org/abs/2212.05946)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we quantitatively and objectively evaluate the interpretability of part-prototype networks?

The key points are:

- Part-prototype networks (e.g. ProtoPNet, ProtoTree) are recently proposed deep self-explainable models for image classification. They define prototypes to represent object parts and make predictions by comparing parts across images. 

- However, current evaluations of part-prototype networks rely on limited visualization examples, which can be misleading. There is a need for formal quantitative evaluation metrics.

- This paper proposes two metrics - consistency score and stability score - to quantitatively measure how consistent and stable the prototype explanations are across different images.

- The consistency score evaluates if a prototype maps to the same object part in different images. The stability score measures if a prototype maps to the same part in original vs perturbed images.

- Experiments show current methods have low consistency and stability. The metrics are positively correlated with accuracy.

- The paper also proposes modules to enhance consistency and stability of part-prototypes, and achieves state-of-the-art performance on accuracy and interpretability.

In summary, the central hypothesis is that the proposed consistency and stability metrics can objectively quantify the interpretability of part-prototype networks. The experiments validate this hypothesis and show the metrics reconcile accuracy and interpretability.
