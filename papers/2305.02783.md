# [Automated Code generation for Information Technology Tasks in YAML   through Large Language Models](https://arxiv.org/abs/2305.02783)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can large language models be applied to generate Ansible YAML code from natural language prompts, and can models customized for Ansible outperform general code generation models?More specifically, some key aspects the paper investigates are:- Developing transformer-based models specialized for generating Ansible YAML code from natural language, through pretraining on YAML and Ansible YAML data. - Proposing novel performance metrics tailored for evaluating Ansible YAML generation.- Comparing models pretrained on Ansible/YAML data against existing general code generation models like CodeGen and Codex in few-shot settings.- Analyzing the effects of various factors like prompt formulation, context window size, model size, and training data size when fine-tuning for the Ansible generation task.- Demonstrating that their customized Ansible Wisdom models can achieve better performance on Ansible YAML generation compared to Codex and CodeGen, especially when fine-tuned on Ansible data.So in summary, the main hypothesis seems to be that large language models customized for Ansible through pretraining and fine-tuning will outperform general code generation models on the task of generating Ansible YAML. The paper presents experiments and results to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- The authors explore the application of large language models (LLMs) to generating Ansible YAML code from natural language prompts. Ansible YAML is widely used for IT automation but has received little attention in terms of AI-assisted coding. - They build a new dataset containing Ansible YAML code for both pretraining and finetuning tasks. The dataset comes from sources like GitHub, Google BigQuery, GitLab, and Ansible Galaxy.- They develop transformer-based models called "Ansible Wisdom" for generating Ansible YAML from natural language. These models are pretrained on their YAML dataset and then finetuned for the natural language to Ansible YAML task.- They propose two new evaluation metrics tailored for assessing Ansible YAML generation: Ansible Aware and Schema Correct. - Their experiments show that their Ansible Wisdom models can generate Ansible YAML accurately from natural language prompts, outperforming existing code generation models like Codex and CodeGen in few-shot settings.- After finetuning, their best model (BLEU of 66.67) can even surpass a much larger Codex model (BLEU of 50.4), which was evaluated in few-shot mode. In summary, the key contribution is demonstrating the feasibility of applying LLMs to generate Ansible YAML code from natural language, including creating Ansible-specific models, metrics and datasets. The results show strong performance on this task, outperforming general code generation models.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related research on using large language models for code generation:- It focuses specifically on generating Ansible YAML code, whereas most prior work has focused on general programming languages like Python, Java, C++, etc. Ansible YAML is an important domain-specific language, but has received relatively little attention so far in code generation research.- The paper introduces two new metrics tailored to evaluating Ansible YAML generation - Ansible Aware and Schema Correct. Most prior work uses more general metrics like BLEU, exact match accuracy, etc. The new metrics allow for more meaningful evaluation on the nuances of Ansible YAML.- It explores both pre-training and fine-tuning transformer models on YAML data. Many recent papers have focused just on pre-training or just on fine-tuning. Looking at both allows for insights into the benefits of each approach. - Both generic YAML data and Ansible-specific YAML data are used during pre-training. Most prior work uses only data from the target domain during pre-training. Using the additional generic YAML seems to provide a useful boost in performance.- The paper systematically compares pre-training vs fine-tuning, model sizes, context window sizes, etc. This ablation study provides insights into what factors matter most for generating high-quality Ansible YAML specifically.- The proposed Ansible Wisdom model outperforms Codex and baseline CodeGen models on Ansible YAML generation after pre-training and fine-tuning. This highlights the benefits of customization for this domain.Overall, the paper makes excellent progress in adapting code generation techniques to the important but understudied domain of Ansible YAML. The rigorous experiments and new evaluation metrics advance the state-of-the-art in this domain specifically.
