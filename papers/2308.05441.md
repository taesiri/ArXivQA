# [Benchmarking Algorithmic Bias in Face Recognition: An Experimental   Approach Using Synthetic Faces and Human Evaluation](https://arxiv.org/abs/2308.05441)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question that this paper addresses is: How can we develop an experimental method to measure biases in face recognition systems using synthetic faces and human evaluation? The key points are:- The paper proposes a new experimental approach to measure bias in face recognition algorithms by generating synthetic faces where attributes like race, gender, age etc. can be controlled independently. - This allows causally attributing any measured bias to specific attributes, overcoming limitations of using observational datasets where attributes are naturally correlated.- The synthetic faces are generated using a pretrained neural face generator by traversing its latent space.- Since there are no true identities behind the synthetic faces, the paper uses extensive human evaluation to annotate attributes of individual faces and perceived identity similarities between face pairs. - This human consensus is used as the ground truth to benchmark several face recognition models, revealing biases related to race, gender and age attributes.- The experimental synthetic approach allows cheaper, faster and privacy-preserving measurement of face recognition biases compared to curating large diverse benchmark datasets.In summary, the key hypothesis is that synthetic faces with independent attribute control along with human evaluation of identity can enable precise experimental measurement of algorithmic biases in face recognition. The paper presents evidence to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an experimental framework to measure bias in face recognition systems using synthetically generated faces and human evaluation. Specifically:- They generate synthetic face images where race, gender and other attributes like pose, age, expression etc. can be controlled independently. This allows changing one attribute at a time to study its causal effect on recognition accuracy. - They collect extensive human annotations on the synthetic faces. Annotators label attributes of individual images, and also provide identity comparisons between pairs of images. - The human annotations provide "ground truth" labels to benchmark face recognition systems. Using these, the authors evaluate several popular academic models and show that they have lower accuracy on Black and East Asian subgroups compared to Caucasian subgroups.- Their framework allows bias measurement in a controlled experimental setup by generating synthetic data. This avoids limitations of using biased real-world datasets where multiple attributes are entangled. It also reduces privacy concerns and economic costs.In summary, the key contribution is demonstrating that synthetic faces combined with human evaluation can reliably estimate and analyze bias in face recognition algorithms in a causal, controlled and economical manner. The authors provide an extensive dataset and empirical demonstration on real algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an experimental method to measure face recognition algorithm bias by generating synthetic faces with attributes modified independently and using human annotations as ground truth for identity.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on measuring algorithmic bias in face recognition systems:- Uses synthetic faces rather than real images to construct the benchmark dataset. Most prior work uses real-world face datasets collected "in the wild", which have limitations like unbalanced sampling across subgroups. Using synthetic faces allows more control over balancing attributes.- Focuses on an experimental approach to estimating causal effects of attributes on bias. The synthetic image generation process modifies one attribute at a time. This is different from observational studies on real datasets where attributes are correlated. - Obtains "ground truth" labels on face identity using human perception rather than true identity labels. Synthetic faces don't have a real identity, so the authors use consensus from multiple human judgments as the proxy for identity.- Validates the approach by evaluating bias in three research face recognition models. Prior work has mainly evaluated commercial systems. The results do show lower accuracy for darker skin tones.- Makes the dataset and annotations public to enable further research. Many previous benchmark datasets are proprietary.Overall, this paper makes a nice contribution in using synthetic faces and an experimental approach to make face recognition benchmarking more controllable, flexible and focused on causality. The reliance on human annotations for identity is an interesting idea. The results largely validate known trends about lower accuracy for minorities. The public dataset is also a valuable asset for the research community.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Testing additional state-of-the-art generative models (GANs and diffusion models in particular) to compare their capabilities in precisely controlling one or more facial attributes during image synthesis. The current latent space traversal method has some limitations.- Improving the annotation process, such as by ensuring annotators come from diverse demographic groups themselves, comparing responses on synthetic vs real images, and investigating if annotators focus on unintended factors. - Incorporating additional non-protected attributes like hairstyle, eyeglasses, etc. in the analysis. The current work looks at pose, lighting, age and expression. - Developing metrics to enable analyzing the effect of multiple attributes in parallel. The current work looks at one non-protected attribute at a time. - Comparing results using improved image quality datasets like the recent DIGIFACE dataset. The image quality affects analysis.- Releasing the dataset and code to enable reproducibility and promote further research in this area of fairness for face recognition systems.In summary, the main future directions are around improving the image generation and annotation processes, expanding the attributes considered, developing multi-attribute analysis, leveraging improved datasets, and releasing resources to enable further research in this important area.
