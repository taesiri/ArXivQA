# [Unsupervised Representation Learning by InvariancePropagation](https://arxiv.org/abs/2010.11694)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to learn useful visual representations from unlabeled images through propagating invariance at the category level. Specifically, the paper proposes a novel unsupervised learning method called "Invariance Propagation" (InvP) that aims to learn representations invariant to intra-class variations across different instances of the same category. The key ideas and hypotheses are:- Semantically similar images reside in connected high-density regions in the representation space. By discovering neighbors and propagating local invariance along the nearest neighbor graph, semantically consistent positive samples can be obtained.- Learning representations invariant to the diverse positive samples leads to invariance at the category level, as opposed to only instance-level invariance.- A hard sampling strategy that focuses on hard positives can provide more intra-class variations and help capture abstract invariance more effectively.So in summary, the central hypothesis is that by propagating invariance from the local neighborhood to broader positive samples across instances, category-level invariant representations can be learned from unlabeled images in an unsupervised manner. The proposed InvP method is designed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper can be summarized as:- It proposes Invariance Propagation, a novel unsupervised learning method that exploits relations between different instances to learn representations invariant to category-level variations. - It introduces a positive sample discovery algorithm that recursively propagates local invariance through a kNN graph to find semantically consistent samples residing in high-density regions.- It demonstrates a hard sampling strategy to concentrate on maximizing agreement between the anchor and its hard positive samples, providing more intra-class variations.- It achieves state-of-the-art results on extensive downstream tasks, including ImageNet classification, semi-supervised learning, transfer learning, and object detection.- It provides both quantitative results and qualitative analysis like similarity distribution and neighborhood visualization to demonstrate the effectiveness of the proposed method.In summary, the key contribution is the proposed Invariance Propagation method that can effectively learn semantic representations from unlabeled data by discovering positive samples exhibiting intra-class variations and adopting a hard sampling strategy. Both algorithm design and comprehensive empirical results validate its superiority over previous methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new unsupervised learning method called Invariance Propagation that learns representations invariant to intra-class variations by recursively propagating local invariance through nearest neighbors to discover semantically consistent positive samples across different instances of the same category.
