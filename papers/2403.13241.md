# [Tackling Noisy Labels with Network Parameter Additive Decomposition](https://arxiv.org/abs/2403.13241)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Learning with noisy labels remains an open challenge in deep learning. Over-parameterized deep networks have large learning capacities and can eventually memorize all noisy training data, leading to poor generalization. Although early stopping exploits the memorization effect to stop training before the model overfits noisy data, it cannot distinguish between memorization of clean and mislabeled data in early training. This causes the model to still inevitably overfit some mislabeled data.  

Proposed Solution: 
The paper proposes a new method called TNLPAD that tackled noisy labels by performing additive decomposition of network parameters into two parts - parameters for desired memorization of clean data ($\bm{\sigma}$), and parameters for undesired memorization of mislabeled data ($\bm{\gamma}$). Based on the memorization effect where models first learn clean data before noisy data, constraints are imposed on $\bm{\sigma}$ and $\bm{\gamma}$ dynamically over training epochs to control their updates. $\bm{\sigma}$ updates are encouraged initially to fully memorize clean data, then discouraged later to reduce overfitting noisy data. $\bm{\gamma}$ updates are constrained initially but relaxed later to absorb effects of noisy data. Only $\bm{\sigma}$ is used at test time for predictions.

Main Contributions:
- Novel idea of additively decomposing network parameters into ideal parameters ($\bm{\sigma}$) for memorizing clean data and noise parameters ($\bm{\gamma}$) for absorbing effects of mislabeled data.
- Leveraging memorization effect to dynamically control updates of $\bm{\sigma}$ and $\bm{\gamma}$ via time-varying loss, so $\bm{\sigma}$ can focus on clean data and be less influenced by noise. 
- Experiments on simulated and real-world noisy datasets show clear improvements over state-of-the-art methods. Visualizations also demonstrate better inter-class separation.
- Simple method that is model-independent and combines strengths of parameter decomposition and early stopping to achieve enhanced robustness over early stopping alone.
