# [4M: Massively Multimodal Masked Modeling](https://arxiv.org/abs/2312.06647)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces 4M, a novel framework for training scalable and versatile foundation models for computer vision using a multimodal masked modeling objective. The method trains a single Transformer encoder-decoder on multiple modalities including images, text, geometry, and semantics. It tokenizes all modalities into discrete tokens, allowing the model to map between modalities by predicting tokens. A key aspect is the training objective, which randomly masks subsets of tokens from all modalities as inputs and targets. This allows scaling to multiple modalities while keeping costs low. Experiments demonstrate 4M's ability to perform diverse vision tasks out-of-the-box and transfer well to unseen tasks when fine-tuned. The learned representations also enable flexible conditional generation between modalities, allowing various multimodal editing capabilities. Thorough ablations provide insights into model design choices and show 4M scales well with data, compute, and model size. The simplicity and strong performance of the approach demonstrates potential for 4M to serve as versatile foundation models for vision.
