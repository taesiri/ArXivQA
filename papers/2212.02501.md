# [SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance   Fields](https://arxiv.org/abs/2212.02501)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we perform 3D scene reconstruction from a single monocular RGB image in a completely self-supervised manner, without requiring any depth supervision during training?

The key ideas and contributions appear to be:

- Using a neural radiance field (NeRF) framework and optimizing it in a self-supervised way from posed image sequences, to learn an implicit 3D scene representation. 

- Making several custom design choices to enable the NeRF to explicitly predict depth, such as a reprojection loss and probabilistic sampling strategy.

- Introducing a spherical U-Net to hallucinate features beyond the input image's field of view.

- A scheme to reconstruct the full 3D scene from the single input image by sampling and fusing novel predicted depth views at various poses.

So in summary, the central hypothesis is that by carefully designing and training an image-conditioned NeRF in a self-supervised manner, they can perform monocular 3D scene reconstruction from a single RGB image, without requiring any depth supervision. The paper aims to demonstrate this is feasible and can even outperform depth-supervised baselines.


## What is the main contribution of this paper?

 This paper proposes SceneRF, a method for self-supervised monocular 3D scene reconstruction using neural radiance fields (NeRFs). The main contributions are:

- Explicitly optimizing depth in the NeRF using a novel reprojection loss, without requiring depth supervision. This allows SceneRF to synthesize high-quality novel depth views for scene reconstruction.

- A probabilistic ray sampling strategy that efficiently samples points near scene surfaces to handle large scenes, modeling the scene density as a mixture of Gaussians.

- A spherical U-Net decoder that extends the input view's field of view, enabling hallucination of the scene beyond the input image. 

- Demonstrating the first self-supervised monocular method for reconstructing full 3D scenes, outperforming even depth-supervised baselines on complex indoor and outdoor datasets.

In summary, the key contribution is a self-supervised framework to learn neural radiance fields suitable for novel view synthesis and 3D reconstruction from a single RGB image input. This is achieved through customized components like explicit depth optimization, efficient sampling, and view hallucination that enable handling complex scenes without 3D supervision.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes SceneRF, a self-supervised monocular 3D scene reconstruction method that uses image sequences and poses to train an image-conditioned neural radiance field which can then synthesize novel views and depths from a single input image to reconstruct a 3D mesh of the scene.
