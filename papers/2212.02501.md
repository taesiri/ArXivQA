# [SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance   Fields](https://arxiv.org/abs/2212.02501)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we perform 3D scene reconstruction from a single monocular RGB image in a completely self-supervised manner, without requiring any depth supervision during training?The key ideas and contributions appear to be:- Using a neural radiance field (NeRF) framework and optimizing it in a self-supervised way from posed image sequences, to learn an implicit 3D scene representation. - Making several custom design choices to enable the NeRF to explicitly predict depth, such as a reprojection loss and probabilistic sampling strategy.- Introducing a spherical U-Net to hallucinate features beyond the input image's field of view.- A scheme to reconstruct the full 3D scene from the single input image by sampling and fusing novel predicted depth views at various poses.So in summary, the central hypothesis is that by carefully designing and training an image-conditioned NeRF in a self-supervised manner, they can perform monocular 3D scene reconstruction from a single RGB image, without requiring any depth supervision. The paper aims to demonstrate this is feasible and can even outperform depth-supervised baselines.
