# [Follow Anything: Open-set detection, tracking, and following in   real-time](https://arxiv.org/abs/2308.05737)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis is:Can an end-to-end robotic system be developed to enable real-time detection, tracking and following of any object specified via multimodal queries (text, image, click), without being restricted to a fixed set of objects seen during training?The key hypotheses appear to be:1) Foundation models like CLIP, DINO and SAM can be combined and optimized to enable open-vocabulary, multimodal detection and segmentation of novel objects specified via text, image or clicks. 2) The detected objects can be robustly tracked in real-time video, accounting for occlusion, loss of tracking, and re-emergence.3) The system can be deployed on a robotic platform like a drone to follow the detected object in real-time by integrating the perception pipeline with a control loop.4) The complete system can operate efficiently on commodity hardware like a lightweight GPU laptop to enable real-time throughput for detection, tracking and following.In summary, the central hypothesis is that an end-to-end open-vocabulary object following system can be developed by combining optimized foundation models, robust tracking, and real-time control - enabling multimodal specification and following of objects without any prior training on those specific objects. The experiments and results aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is the \FAM{} system for open-vocabulary object following. The key aspects of \FAM{} are:- It is an open-set, multimodal system that can detect and follow arbitrary objects specified via text, images, or clicks at inference time. This provides flexibility to adapt to novel objects not seen during training.- It combines state-of-the-art vision models like SAM, DINO, and CLIP in an optimized pipeline for real-time object detection, segmentation, and tracking from video streams. Tracing and quantization are used to speed up inference.- A lightweight tracking and visual servoing controller allows deploying the system on a quadrotor for following objects in the real world.- Re-detection mechanisms handle occlusion or loss of tracking, either automatically using stored features or with human-in-the-loop clicks.- The system is comprehensively evaluated on a physical quadrotor following objects like drones, cars, and bricks. It achieves 6-20 FPS throughput on a lightweight GPU.In summary, the key contribution is a real-time multimodal open-vocabulary object following system that can leverage state-of-the-art vision models to detect and track arbitrary objects for robotic applications. The optimization techniques and modular design allow deployment on resource-constrained platforms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one-sentence summary of the paper:The paper presents a real-time robotic system called Follow Anything (FAM) that leverages foundation models like CLIP, DINO, and SAM to detect, track, and follow arbitrary objects specified via text, image, or click queries, and demonstrates it on a quadrotor tracking objects like cars and drones.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on the Follow Anything system compares to other research on open-set object detection and tracking for robotics:- It proposes an end-to-end system that unifies state-of-the-art models into a real-time pipeline for detection, segmentation, tracking, and following. Other works tend to focus on individual components rather than the full system.- The system is open-set and multimodal, taking text, image, click, or box queries as input to detect and track novel objects at test time. Most prior work is closed-set or restricted to class labels only. - It leverages recent vision transformer models like SAM, DINO, and CLIP in a lightweight and optimized way for robotics. These models enable open-set generalization but are often not designed for real-time use.- Re-detection mechanisms are incorporated to handle occlusion and lost tracks. Many papers overlook this practical aspect of tracking in robotics applications.- Live demonstration on a quadrotor platform shows the system works in real-world conditions in real-time. Some works are simulated or offline.- An emphasis is placed on efficient and accessible implementation for robotics. The code is open-source and uses common robotics tools like PX4 and OpenCV.Overall, this paper integrates a lot of disparate research ideas into one applied system for robotic object following. The open-set capabilities and focus on real-time multimodal interaction also help push the boundaries of what is possible in this domain.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions the authors suggest are:- Improving robustness and efficiency of the re-detection mechanisms. The paper proposes some initial re-detection methods, but notes there is room for improvement, especially in terms of robustness and speed.- Exploring different aggregation methods when creating region-level descriptors from pixel-level features. The paper uses average pooling for efficiency, but other methods could be explored.- Adapting the system to work on more constrained robotic platforms with lower compute. The current system uses a powerful desktop GPU, but adapting it to work on more limited onboard robot compute could enable more applications.- Incorporating temporal information more extensively. The system currently operates on individual frames, but leveraging more temporal/sequence-based methods could improve performance. - Expanding the modalities supported as queries. The system allows text, image, bounding box, and click queries. Adding support for other modalities like audio or video could be interesting.- Testing the approach on more dynamic objects and environments. The experiments focused on indoor scenes with simple objects - testing on more complex real-world scenarios could reveal areas for improvement.- Combining the approach with navigation and planning for fully autonomous operation. The current system requires a person to move the camera - combining it with autonomous robot navigation and planning could enable more applications.- Exploring semi-supervised or self-supervised learning to reduce reliance on large datasets. The foundation models require lots of data, but semi-supervised or self-supervised methods could help.So in summary, some key directions are improving the efficiency and robustness, adapting the system toconstrained platforms, incorporating more temporal information, supporting more query modalities, testing on more complex scenarios, combining it with navigation, and reducing the data dependence.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:The paper presents Follow Anything (FAM), an end-to-end system for open-vocabulary object detection, tracking, and following using a robotic platform like a drone. FAM leverages recent advances in foundation models like CLIP, DINO, and SAM to enable real-time, multimodal (text, image, click) detection and segmentation of novel objects not seen during training. These objects are tracked across frames using lightweight tracking models, accounting for occlusion, and can be re-detected after loss using stored features. FAM runs in real-time on a lightweight GPU, enabling deployment on an aerial robot for following dynamic objects specified via flexible user queries. Experiments validate the approach on various objects like cars and drones. The open-source system bridges state-of-the-art vision and robotics for versatile real-world tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper presents Follow Anything (FAM), an open-set, real-time system for detecting, tracking, and following arbitrary objects using a robotic platform. FAM leverages recent advances in vision and vision-language foundation models like CLIP, DINO, and SAM to enable multimodal queries using text, images, or clicks for specifying objects of interest. It uses a lightweight semantic segmentation and detection scheme by combining DINO or CLIP features with class-agnostic instance segmentations from SAM. For real-time performance, FAM optimizes models through quantization and tracing and uses a fast detection scheme solely based on grouping DINO features. Once objects are detected, FAM tracks them across frames using state-of-the-art trackers like SiamMask and AOT. It accounts for occlusion and object re-emergence using automated re-detection mechanisms. The authors demonstrate FAM on a micro aerial vehicle following various objects like drones, cars, and bricks in real-time, achieving 6-20 FPS throughput. In summary, FAM contributes an open-vocabulary, multimodal system for robotic agents to detect, track, and follow arbitrary objects specified via text, images, or clicks. Through optimized foundation models and real-time processors, it achieves efficient object following on resource-constrained platforms. FAM represents an important step towards more generalizable and flexible vision-based robotic systems.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper presents a robotic system called Follow Anything (FAM) for real-time open-vocabulary object detection, tracking, and following. FAM leverages state-of-the-art vision transformers like SAM for segmentation, DINO and CLIP for visual features, and lightweight tracking models like SiamMask. It extracts pixel-level features from the DINO and CLIP models and matches them to multimodal queries like text, images, or clicks to detect objects of interest in the input video frames. These objects are tracked across frames, accounting for occlusion, and a visual servoing controller issues velocity commands to a robotic platform like a quadrotor to follow the object. Model optimization techniques like quantization and tracing are used to improve inference speed. Overall, FAM provides an end-to-end pipeline for specifying arbitrary objects using multimodal queries, detecting and segmenting them in real-time video, tracking them across frames, and producing control commands to physically follow them with a robotic system.


## What problem or question is the paper addressing?

 The paper presents a robotic system called Follow Anything (\FAM{}) for detecting, tracking, and following any object in real-time. The key aspects of the system are:- It is an open-set system that can detect and track novel objects not seen during training, specified via text, images, or clicks. This provides more flexibility compared to closed-set systems restricted to a fixed set of classes. - It leverages recent advances in vision and vision-language models like CLIP, DINO, and SAM to enable open-set detection and segmentation by matching multimodal queries to input images.- It tracks objects robustly across frames, handling occlusion and re-detection of lost objects.- The system is designed for real-time performance by optimizing and integrating models like SAM, CLIP, DINO, and incorporating fast trackers. It can run at 6-20 FPS on a lightweight GPU.- It is a unified system that can be deployed on robot platforms like quadrotors for following objects using visual servoing. The paper shows autonomous following of various objects.- The code is open-sourced to enable adoption, deployment, and extension of the system.In summary, the key focus is developing a flexible open-vocabulary object following system by harnessing recent advances in vision models, optimizing them for real-time robotics use cases, and providing capabilities like re-detection that are crucial for robustness. The system aims to advance the state-of-the-art in robotic perception and control.
