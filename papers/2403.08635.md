# [Human Alignment of Large Language Models through Online Preference   Optimisation](https://arxiv.org/abs/2403.08635)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent work has proposed two algorithms for optimizing language model outputs to align with human preferences: Identity Policy Optimization (IPO) and Nash Mirror Descent Policy Gradient (Nash-MD-PG). 
- IPO is an offline, contrastive loss method that optimizes a fixed dataset. Nash-MD-PG is an online method that finds a Nash equilibrium via policy gradient updates against a regularised policy.
- These seem quite different, but the paper aims to unify and bridge the gap between them.

Key Contributions:
1) Identified factors that vary between IPO and Nash-MD-PG:
   - Contrastive vs non-contrastive loss
   - Offline vs online data  
   - Different equilibrium concepts
2) Proposed "Online IPO", an online variant of IPO, and showed its expected update is equivalent to self-play in the Nash-MD-PG game.
3) Proposed "IPO-MD", an IPO method with regularised data sampling like Nash-MD-PG. Showed IPO-MD's stationary points match those of Nash-MD-PG.

Experiments:
- Evaluated IPO, Nash-MD-PG and proposed methods on a summarization task with T5X models.
- IPO and IPO-MD performed best overall. Performance was statistically indistinguishable, but both clearly beat other methods.

Conclusion:
- Identified connections between recent preference optimization methods IPO and Nash-MD-PG.
- Proposed online IPO and IPO-MD that combine strengths of both to align language models with human preferences.
- Empirical evaluation shows proposed methods are state-of-the-art on a summarization task.
