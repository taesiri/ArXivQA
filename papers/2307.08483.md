# [Differentiable Transportation Pruning](https://arxiv.org/abs/2307.08483)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to develop an accurate and efficient pruning technique to compress deep neural networks for deployment on resource-constrained edge devices. Specifically, the paper proposes a novel pruning method called Differentiable Transportation Pruning that allows precise control over the output network size while achieving high accuracy. 

The key hypotheses are:

1. Posing the pruning problem as an optimal transport problem, where importance scores are aligned to a target distribution defined by the desired sparsity level, will allow jointly learning soft masks and network parameters in an end-to-end differentiable manner.

2. Making the transportation problem efficient and differentiable using entropic regularization and approximate bilevel optimization will enable precise control over the sparsity level while retaining accuracy.

3. The temperature parameter of the entropic regularization can be automatically annealed during training to transition soft masks into discrete masks.

4. The proposed Differentiable Transportation Pruning method will achieve state-of-the-art pruning performance across different models, datasets and sparsity levels compared to prior techniques.

In summary, the paper hypothesizes that their proposed optimal transport based pruning formulation, made computationally tractable through approximations, will enable highly accurate and adjustable network pruning critical for edge deployment. The experiments aim to validate these hypotheses across diverse settings.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing a differentiable pruning method based on entropy regularized optimal transportation that allows precise control over the output network size. 

2. Formulating an efficient optimal transportation algorithm that reduces the computational complexity compared to prior work using optimal transport for pruning. 

3. Introducing an automatic temperature annealing mechanism that gradually turns soft masks into discrete masks, instead of needing to manually choose a decay schedule.

4. Demonstrating state-of-the-art pruning results on 3 datasets using 5 different models, across a range of pruning ratios and with different types of pruning granularity (structured and unstructured).

In summary, the key ideas seem to be making optimal transport more computationally feasible for pruning by using only a single Sinkhorn iteration per update, and increasing the ease-of-use by automatically annealing the temperature instead of needing to tune a schedule. The experimental results then show these contributions lead to improved accuracy compared to prior pruning techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from this paper:

This paper presents a new differentiable pruning method based on entropy-regularized optimal transportation that achieves precise control over the pruned network size while allowing joint optimization of soft masks and network parameters, resulting in state-of-the-art performance compared to prior pruning techniques.


## How does this paper compare to other research in the same field?

 Based on the abstract, this paper proposes a novel pruning technique for deep neural networks using differentiable transportation and optimal transport. Here are some key ways this relates to other prune research:

- Pruning methods aim to remove redundant or less important parts of a neural network to make the models smaller and more efficient. This work focuses on "filter pruning", which removes entire filters from convolutional layers. This is a form of structured pruning that can lead to practical benefits like reduced memory usage and compute requirements.

- Many pruning techniques assign an importance score to each part of the network, then prune low importance elements. A key difference in this work is that they learn the importance scores directly through a differentiable process, rather than using pre-defined criteria like weight magnitude. This allows the importance scores to be optimized end-to-end along with the network training.

- Optimal transport provides a principled way to match one distribution to another. By framing the pruning as an optimal transport problem between importance scores and a sparse target distribution, they can directly control the sparsity level. This differs from typical pruning methods that require extra hyperparameter tuning to meet a sparsity target.

- To make the discrete optimal transport problem trainable, they propose a smoothed relaxation using entropic regularization. The temperature parameter is automatically annealed during training to transition from soft to hard pruning.

- They further improve computational efficiency by using only a single Sinkhorn iteration per update step rather than requiring full inner convergence. This approximate bi-level optimization technique is analogous to methods like DARTS for neural architecture search.

Overall, this work combines differentiable training of importance scores, optimal transport for direct sparsity control, and approximate bi-level optimization for efficiency. The results demonstrate state-of-the-art pruning across multiple models, datasets, and sparsity levels compared to prior arts. The proposed innovations could potentially advance the field of deep neural network pruning.
