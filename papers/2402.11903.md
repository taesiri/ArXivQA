# [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) still face challenges with complex logical reasoning tasks. Using LLMs to parse problems and offload reasoning to symbolic solvers has limitations - parsing errors lead to solver failures, and solver capabilities constrain LLM performance. 

- Industrial SAT problems involving large arithmetic circuits pose significant challenges for state-of-the-art symbolic solvers like Z3 and Kissat. They struggle with problems like high bit-width multipliers, taking weeks to solve.

Proposed Solution: 
- The paper proposes Solver-Layer Adaptation (SoLA) to enhance LLM's logical reasoning by introducing a differential solver layer. 

- In SoLA, the LLM parses the natural language problem description and generates an initial solution. A MaxSAT solver layer then refines this solution towards satisfiability.

- Forward and backward transfer gradients are defined between LLM and MaxSAT layer enabling the model to converge to a satisfied solution or prove unsatisfiability.

Main Contributions:

- First work to bridge the gap between LLM's natural language understanding and solver's symbolic computation for enhanced end-to-end performance.

- First to achieve polynomial time complexity for solving UNSAT instances. This enables solving previously intractable industrial UNSAT cases within reasonable time. 

- Compared to learning-aided SAT techniques using graph neural networks, SoLA uniquely uses LLM to comprehend symbolic expressions.

- Experiments show SoLA significantly improves LLM accuracy to 100% on problems with 200 variables, and outperforms state-of-the-art solvers on industrial circuit problems.
