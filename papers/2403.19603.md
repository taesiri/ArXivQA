# [Semantic Map-based Generation of Navigation Instructions](https://arxiv.org/abs/2403.19603)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Vision-and-language navigation (VLN) requires large amounts of annotated data which is costly to obtain. Generated navigation instructions can help improve VLN models and reduce annotation needs. 
- Existing methods for navigation instruction generation (VL-GEN) use sequences of panoramic RGB images as input which is computationally expensive to process. Images also contain irrelevant details.

Proposed Solution:
- Use a top-down semantic map as the main visual input instead of panoramic images. This fuses information from multiple viewpoints into a single image and removes unnecessary details.
- Frame VL-GEN as an image captioning task taking the semantic map with overlaid path as input and generating a textual navigation instruction as output.
- Additionally provide supplementary input modalities - region names, navigation actions, panoramic images.
- Develop a multimodal transformer model based on BLIP that encodes these varied inputs.

Main Contributions:
- Create a new benchmark dataset based on Room-to-Room by extracting semantic maps and overlaying navigation paths.
- Demonstrate feasibility of using semantic maps for VL-GEN instead of panoramic image sequences.
- Show that providing additional input modalities (regions, actions) alongside the semantic map improves performance.
- Human evaluation finds model with all supplementary modalities achieves best results.
- Analysis reveals common error types - incorrect actions, hallucinations, redundancies. Prompting helps reduce some errors.
- Performance is on par or better than prior LSTM-based sequence models for this task.

In summary, this paper proposes reframing navigation instruction generation as a multimodal image captioning task using semantic maps as the primary visual input. Experiments show promising results and analysis provides insights into model errors.
