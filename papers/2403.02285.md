# [Detection of Non-recorded Word Senses in English and Swedish](https://arxiv.org/abs/2403.02285)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Dictionaries can become outdated as language evolves and new senses of words emerge or old senses become obsolete. However, manually identifying such gaps in dictionary entries is challenging for lexicographers. 
- The paper focuses on the task of automatic "Unknown Sense Detection" - determining if the meaning expressed by a particular usage of a word is missing from the dictionary entry. 
- This is difficult because dictionary entries only provide limited contextual information about word senses (e.g. a short gloss/definition, few example sentences). Standard Word Sense Disambiguation models require more training data.

Proposed Solution:
- Uses a pre-trained Word-in-Context (WiC) embedder to create vector representations of both the target word usages (from corpora) and dictionary senses. 
- Compares these vectors using similarity metrics to determine if the usage matches any recorded sense. If similarity to closest sense is below a learned threshold, predicts the usage as unassigned/unknown sense.
- Tests different sense representations (gloss, examples) and modifications (inserting headword, replacements) to deal with missing context.
- Performs human annotation on random and predicted samples to select optimal models and evaluate quality.

Main Contributions:
- Demonstrates automatic identification of missing dictionary senses by comparing corpus usages and limited dictionary context vectors.
- Systematically tests different contextualization techniques for representing senses from incomplete dictionary entries.
- Provides new human-annotated data for Unknown Sense Detection in English and Swedish.
- Analysis shows models can considerably increase discovery of unknown senses compared to random baseline.
- Identifies non-recorded usages that will be integrated into dictionaries. Discusses remaining challenges around headword matching.

In summary, the paper presents a novel application of Word-in-Context models to assist dictionary updating by pinpointing new and obsolete word senses in corpora. The vector comparison approach works despite limited available context and shows promising results.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper develops a model to detect non-recorded word senses in English and Swedish by comparing vector representations of dictionary sense entries and corpus word usages using a pre-trained contextual embedding model, in order to provide support for dictionary maintenance.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is developing and evaluating methods for automatically detecting word senses that are not recorded in dictionaries, in order to support dictionary maintenance. Specifically:

- They propose models that compare vector representations of word usages from corpora to vector representations of sense entries from dictionaries, using a pre-trained word-in-context embedder, to determine if the usage expresses an unrecorded sense.

- They conduct experiments on English and Swedish, using WordNet and a Swedish dictionary as sense inventories. Their models are able to considerably increase the number of detected word usages with non-recorded senses compared to a random sample.

- They perform two phases of human annotation to adapt the models and evaluate their predictions. The models successfully predict a large number of unrecorded senses that can potentially be added to the dictionaries.

- They analyze the results in depth, identifying strengths and weaknesses of the approach, such as issues with multiword expressions and finding appropriate headword usages to compare against the sense inventory.

In summary, the key contribution is developing and demonstrating a practical computational method to help identify missing dictionary senses by comparing corpus usages to existing sense inventories. The experiments and analyses provide insight into the feasibility of using such models to assist lexicographers in updating dictionaries.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Unknown Sense Detection
- Word Sense Disambiguation (WSD)
- Word Sense Induction (WSI)
- Word-in-Context (WiC) models
- Contextualized word embeddings
- SentenceBERT (SBERT)
- XL-LEXEME
- Few-shot learning
- Dictionary maintenance
- WordNet
- Svensk ordbok (SO)
- Gloss and example sentences
- Thresholding similarity scores
- Annotation and evaluation

The paper focuses on detecting word senses that are not yet recorded in dictionaries like WordNet and the Swedish dictionary SO. It treats this as a binary classification task, using contextualized word embeddings from a WiC model called XL-LEXEME to represent word usages and dictionary senses. The model is tuned in a few-shot learning setup and aims to support dictionary maintenance. Key methods include comparing embedding similarities to a threshold and leveraging human annotations for model selection and evaluation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using a pre-trained Word-in-Context (WiC) model to create contextualized word embeddings for representing word senses and usages. What are the advantages and disadvantages of using a WiC model compared to other contextualized embedding techniques like BERT?

2. The paper experiments with different strategies for representing dictionary senses when example sentences are missing (e.g. inserting the headword, using the gloss). What is the intuition behind these different strategies and why do you think some performed better than others? 

3. The paper uses a threshold-based approach to determine if a usage is unassigned. What are some alternative outlier detection techniques that could have been used instead of a threshold, and what might be their advantages?

4. The inter-annotator agreement scores are relatively low in the human evaluations. What could be some reasons for the disagreement and how might the annotation process be improved to increase agreement? 

5. The paper evaluates on both contemporary and historical corpus data. Why is it useful to evaluate on historical data and what unique challenges does historical data present?

6. The paper observes different model performance on modern vs historical data. What might account for these differences? How could the model be adapted to work better on historical texts?

7. The paper manually analyzes some of the true positive predictions. What are some of the common error cases identified in these examples and how could they be addressed? 

8. How exactly does the paper simulate unknown senses during the cross-validation by randomly masking assigned senses? What is the motivation behind this data augmentation technique?

9. The paper uses a limited dictionary extract for Swedish. How might having the full dictionary have impacted the model's performance and what is lost by using only a subset?

10. The paper frames this as an "unknown sense detection" task. How is this different from related tasks like novel sense detection and lexical semantic change detection? What distinguishes unknown sense detection as its own task?
