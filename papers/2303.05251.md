# [Masked Image Modeling with Local Multi-Scale Reconstruction](https://arxiv.org/abs/2303.05251)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new method for masked image modeling called Local Multi-Scale Reconstruction. The key ideas are:- Conducting reconstruction tasks not just at the top layer of the encoder like most MIM models, but at multiple local layers including both lower and upper ones. This helps guide and accelerate learning at these layers.- Using multi-scale supervisions calculated from the input image rather than just single-scale. The lower layers reconstruct fine-scale signals while upper layers reconstruct coarse-scale signals. This facilitates multi-scale understanding. - Using tiny decoders so the local tasks don't add much computation.The central hypothesis is that applying these ideas - local reconstructions with multi-scale supervisions - will improve masked image modeling by accelerating representation learning, especially in pyramidal architectures, while adding little extra computation.The key research questions are:- Does conducting reconstruction tasks at multiple local layers help guide and accelerate learning compared to only global reconstruction? - Is using multi-scale supervisions more effective than single-scale for these local tasks?- Can tiny decoders handle these local reconstruction tasks effectively?- Will this approach improve performance and efficiency compared to prior MIM methods, especially for pyramidal architectures?So in summary, the main goal is to improve masked image modeling, particularly its efficiency, via the proposed local multi-scale reconstruction approach. The key hypotheses are that local tasks, multi-scale supervisions, and tiny decoders will help achieve this goal.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel pretext task called "local multi-scale reconstruction" for masked image modeling. The key ideas are:- Conducting reconstruction tasks not only at the top layer but also at multiple local layers of the encoder, including both lower and upper layers, to explicitly guide them. - Using multi-scale supervisions, where the lower layers reconstruct fine-scale signals and the upper layers reconstruct coarse-scale signals. The multi-scale supervisions are obtained by dividing the input image into regions of different scales.- The local multi-scale reconstruction accelerates the representation learning process and facilitates multi-scale semantic understanding of the input image.In summary, this method can train masked image modeling models like MAE and BEiT more efficiently and achieve better performance on downstream tasks like classification, detection and segmentation. The ablation studies show local reconstruction outperforms feature fusion, and multi-scale supervisions are better than single-scale ones. This novel pretext task provides a new direction to improve masked image modeling.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a 1 sentence summary of the key points in the paper:The paper proposes a novel masked image modeling approach called LocalMIM that conducts reconstruction tasks at multiple local layers of the encoder using multi-scale supervisions from the input, achieving comparable performance to state-of-the-art models but with significantly improved training efficiency.
