# [HOP to the Next Tasks and Domains for Continual Learning in NLP](https://arxiv.org/abs/2402.18449)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Continual learning (CL) aims to learn a sequence of tasks or domains incrementally over time. This is challenging because when learning a new task, catastrophic forgetting (CF) of previous tasks often occurs due to the non-stationary data distribution. Existing CL methods have mainly focused on computer vision applications and have limitations when applied to natural language processing (NLP) tasks. Specifically, prior work has mostly addressed either task-incremental learning (TIL) or domain-incremental learning (DIL) in NLP separately, but not jointly in a unified framework.

Proposed Solution:
This paper proposes a new method called HOP that can handle both TIL and DIL continually in NLP applications. The key ideas are:

(1) Use adapter modules and Multi-Layer Perceptrons (MLPs) specialized for each task/domain to enable efficient transfer learning. 

(2) Compute high-order statistical moments (mean, variance, skewness etc) over the distributions of embedded text tokens. This captures richer sentence-wide information compared to using just the [CLS] token.  

(3) Model the changing distributions over tasks/domains better to encourage knowledge transfer and reduce forgetting.

Main Contributions:

- First continual learning method that jointly handles TIL and DIL settings for multiple NLP applications like text classification, aspect-based sentiment analysis etc.

- Introduction of high-order pooling over token embeddings to extract richer representations for overcoming non-stationary distributions.

- Outperforms state-of-the-art approaches like Adaptor-BERT and CAN in various metrics like accuracy, forward/backward transfer, forgetting etc across benchmarks.

- Adds minimal computational overhead over fine-tuning BERT, making it efficient.

- Can be combined with other CL methods to consistently improve their performance.

In summary, HOP pushes state-of-the-art in continual learning for NLP by handling both task and domain shift, extracting high-order statistics from token embeddings in an efficient yet effective manner.
