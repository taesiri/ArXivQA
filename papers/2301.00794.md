# [STEPs: Self-Supervised Key Step Extraction and Localization from   Unlabeled Procedural Videos](https://arxiv.org/abs/2301.00794)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question/hypothesis of this paper is:

Can we extract key steps from unlabeled procedural videos in a self-supervised manner to enable automatic augmented reality content creation?

The authors propose a two-stage approach:

1) Self-supervised representation learning to produce discriminative features for each frame without any labels. They use a multi-cue contrastive loss (BMC2) to enforce temporal consistency and leverage multiple modalities like optical flow, depth, gaze etc. 

2) Extract key steps by clustering the learned representations and sampling representative frames.

The key hypotheses seem to be:

- Using multiple modalities and long-range temporal context is crucial for learning good representations of key steps from unlabeled video.

- Their proposed self-supervised loss BMC2 enables learning these discriminative representations from unlabeled procedural video in a data-efficient manner.

- The learned representations can be effectively clustered to extract meaningful and succinct key steps for complex tasks.

So in summary, the main research question is around developing a self-supervised approach to extract key steps from unlabeled procedural video by exploiting multiple modalities and temporal context, to enable downstream AR applications. The key hypothesis is that their proposed approach can learn sufficiently discriminative representations to extract meaningful key steps without manual labels.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes an approach called STEPs (Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos) to extract key steps from unlabeled procedural videos. 

2. It introduces a novel training objective called Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn discriminative representations of video frames without any labels. This loss uses raw features from multiple modalities (e.g. RGB, optical flow) to bootstrap a temporal window around each frame and enforce consistency across modalities. 

3. Unlike prior works that rely on aligning multiple videos, STEPs is designed to work with only a few recordings of expert demonstrations. This makes it suitable for practical AR/VR applications where large datasets are unavailable.

4. It demonstrates superior performance over prior methods on key step localization and phase classification tasks on several first-person and third-person video datasets.

5. The two-stage design with decoupled representation learning and key step extraction provides flexibility. The features can be used with different clustering algorithms to extract key steps based on application needs.

In summary, the main contribution is a self-supervised learning approach to extract key steps from unlabeled procedural videos, designed specifically for few-shot learning from expert demonstrations as commonly needed for AR/VR applications. The proposed BMC2 loss and flexibility of the overall framework are the key novel aspects.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a two-stage approach for extracting key steps from unlabeled procedural videos, first using a self-supervised contrastive loss to learn discriminative representations from multiple modalities without labels, and then sampling key steps via clustering the learned representations.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of unsupervised key step extraction from procedural videos:

- It proposes a new training objective called the Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn discriminative representations of video frames without any labels. This is a novel contribution compared to prior work. 

- It focuses on learning from only a few expert recordings of complex tasks. Many prior methods rely on having multiple recordings of the same tasks from different camera viewpoints. This makes the method more practical for real-world applications where only limited video data may be available.

- It leverages multiple modalities like optical flow, depth, and gaze during training. Integrating multi-modal signals is less common in this field, but the results show it helps significantly. Most prior work uses only RGB frames.

- It trains a lightweight temporal module on top of off-the-shelf features rather than end-to-end training a large model. This allows efficiently training on very long videos (1000s of frames) which capture more temporal context.

- It does not require video alignment or correspondence between multiple videos, unlike some recent methods. This also makes it applicable in cases where only a single long demonstration is available.

- The two-stage approach separating representation learning from key step extraction provides flexibility. Different clustering or selection algorithms could be substituted in the second stage.

- It achieves superior performance on standard benchmarks for related tasks like key step localization and phase classification. The learned features seem generally useful.

Overall, the proposed approach makes contributions in effectively leveraging multiple modalities, long temporal contexts, and only a few videos. The results demonstrate state-of-the-art performance on several datasets.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Collecting a detailed dataset of procedural task recordings from AR headsets to better evaluate key step extraction methods. The authors note that their approach focuses on extracting key steps for AR/VR applications where many modalities are available, but there is a lack of suitable datasets to properly evaluate this.

- Incorporating recent advances in meta-learning and few-shot learning to potentially improve key step extraction while reducing the amount of required training data. The authors note that their approach can work with few videos, but see potential for further improvements by leveraging meta-learning and few-shot techniques.

- Validating extracted key steps through device usage experiments and appropriately tuning models. The authors note that determining key steps is subjective and may need validation through real-world testing on AR devices along with model tuning.

- Exploring the use of natural language modalities along with other sensor-based and vision-derived modalities. The authors focused on modalities like gaze, depth, motion etc. but note language could also provide useful weak alignment.

- Applying the feature learning approach to other related tasks such as video summarization. The authors suggest their learned features could benefit video summarization and other high-level vision tasks.

In summary, the main future directions are around collecting better procedural datasets, incorporating meta-learning and few-shot techniques, real-world validation and tuning of models, use of multiple modalities including language, and applications to other high-level vision tasks. The key focus seems to be on improving and validating real-world applicability.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a two-stage approach for automatically extracting key steps from unlabeled procedural videos to assist with augmented reality (AR) job training. First, they learn discriminative frame-level representations without labels using a novel self-supervised contrastive loss called Bootstrapped Multi-Cue Contrastive (BMC2) loss. This allows using multiple synchronized cues like optical flow and on-device sensors. Unlike prior works, they train a lightweight temporal module on top of off-the-shelf features from pretrained models, enabling long temporal contexts. For key step extraction, representations are clustered and steps are sampled - all without requiring manual labels. Experiments on multiple datasets show significant gains over prior work in key step localization and phase classification. Qualitative results demonstrate the extracted key steps succinctly capture sub-tasks in complex procedures. The approach is practical, fast, and applicable to both multi-modal AR recordings and regular videos.
