# [STEPs: Self-Supervised Key Step Extraction and Localization from   Unlabeled Procedural Videos](https://arxiv.org/abs/2301.00794)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question/hypothesis of this paper is:

Can we extract key steps from unlabeled procedural videos in a self-supervised manner to enable automatic augmented reality content creation?

The authors propose a two-stage approach:

1) Self-supervised representation learning to produce discriminative features for each frame without any labels. They use a multi-cue contrastive loss (BMC2) to enforce temporal consistency and leverage multiple modalities like optical flow, depth, gaze etc. 

2) Extract key steps by clustering the learned representations and sampling representative frames.

The key hypotheses seem to be:

- Using multiple modalities and long-range temporal context is crucial for learning good representations of key steps from unlabeled video.

- Their proposed self-supervised loss BMC2 enables learning these discriminative representations from unlabeled procedural video in a data-efficient manner.

- The learned representations can be effectively clustered to extract meaningful and succinct key steps for complex tasks.

So in summary, the main research question is around developing a self-supervised approach to extract key steps from unlabeled procedural video by exploiting multiple modalities and temporal context, to enable downstream AR applications. The key hypothesis is that their proposed approach can learn sufficiently discriminative representations to extract meaningful key steps without manual labels.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes an approach called STEPs (Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos) to extract key steps from unlabeled procedural videos. 

2. It introduces a novel training objective called Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn discriminative representations of video frames without any labels. This loss uses raw features from multiple modalities (e.g. RGB, optical flow) to bootstrap a temporal window around each frame and enforce consistency across modalities. 

3. Unlike prior works that rely on aligning multiple videos, STEPs is designed to work with only a few recordings of expert demonstrations. This makes it suitable for practical AR/VR applications where large datasets are unavailable.

4. It demonstrates superior performance over prior methods on key step localization and phase classification tasks on several first-person and third-person video datasets.

5. The two-stage design with decoupled representation learning and key step extraction provides flexibility. The features can be used with different clustering algorithms to extract key steps based on application needs.

In summary, the main contribution is a self-supervised learning approach to extract key steps from unlabeled procedural videos, designed specifically for few-shot learning from expert demonstrations as commonly needed for AR/VR applications. The proposed BMC2 loss and flexibility of the overall framework are the key novel aspects.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a two-stage approach for extracting key steps from unlabeled procedural videos, first using a self-supervised contrastive loss to learn discriminative representations from multiple modalities without labels, and then sampling key steps via clustering the learned representations.
