# [STEPs: Self-Supervised Key Step Extraction and Localization from   Unlabeled Procedural Videos](https://arxiv.org/abs/2301.00794)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question/hypothesis of this paper is:Can we extract key steps from unlabeled procedural videos in a self-supervised manner to enable automatic augmented reality content creation?The authors propose a two-stage approach:1) Self-supervised representation learning to produce discriminative features for each frame without any labels. They use a multi-cue contrastive loss (BMC2) to enforce temporal consistency and leverage multiple modalities like optical flow, depth, gaze etc. 2) Extract key steps by clustering the learned representations and sampling representative frames.The key hypotheses seem to be:- Using multiple modalities and long-range temporal context is crucial for learning good representations of key steps from unlabeled video.- Their proposed self-supervised loss BMC2 enables learning these discriminative representations from unlabeled procedural video in a data-efficient manner.- The learned representations can be effectively clustered to extract meaningful and succinct key steps for complex tasks.So in summary, the main research question is around developing a self-supervised approach to extract key steps from unlabeled procedural video by exploiting multiple modalities and temporal context, to enable downstream AR applications. The key hypothesis is that their proposed approach can learn sufficiently discriminative representations to extract meaningful key steps without manual labels.
