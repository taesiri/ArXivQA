# What Matters in Training a GPT4-Style Language Model with Multimodal
  Inputs?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are the key factors (network structures, training data, prompts/instructions) that affect the performance of GPT4-style large language models adapted for multi-modal (image/video) understanding and generation?The authors aim to systematically study these factors through controlled experiments and propose an improved model Lynx based on their findings. Specifically, some of the key questions they try to answer are:- How do different LLM backbones and adaptation methods (cross-attention vs prefix-tuning) impact multi-modal performance? - How does the quantity and quality of image-text training data affect multi-modal language generation?- What is the influence of diversified prompts/instructions on the instruction-following ability? - How to balance multi-modal understanding accuracy and open-ended text generation ability in evaluation and training?The central hypothesis seems to be that carefully designed network architecture, high-quality training data, and diversified instructions are crucial for developing high-performing multi-modal LLMs like GPT4. The authors try to verify this through systematic ablation studies.
