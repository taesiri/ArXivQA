# [VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections](https://arxiv.org/abs/2403.16030)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most graph transformer models rely on dense attention mechanisms that require quadratic computational complexity, making them unscalable for large graphs. This poses challenges for mini-batch training of graph transformers, as the limited node samples in each batch cannot support effective dense attention to encode informative node representations. 

Proposed Solution:
1) The authors first propose graph tokenization based on personalized PageRank (PPR), where each node is assigned a token list consisting of its top PPR-ranked neighbors. This allows decoupling complex graph topology from model training, enabling offline graph feature engineering and mini-batch loading. 

2) To address limitations of using only PPR, the authors introduce virtual connections to rewire the graph - including structure-aware connections via super nodes connecting clusters, and content-aware connections via super nodes connecting same-label nodes. This allows the PPR token lists to encode comprehensive local, global, long-range and heterophilous graph information.

3) By wrapping the PPR graph tokenization and virtual connection graph rewiring as the Virtual Connection Ranking based Graph Transformer (VCR-Graphormer), the model only needs self-attention on each node's token list to encode expressive embeddings, with complexity $O(m+k\log k)$ compared to previous $O(n^3)$.

Main Contributions:
- Formalized graph tokenization via PPR as a fixed polynomial filter GCN with jumping knowledge, proving its viability.  
- Proposed criteria (encoding topology, long-range interactions, heterophily, efficiency) for constructing informative token lists.
- Developed techniques of structure- and content-aware virtual graph connections to meet proposed criteria.
- Designed the end-to-end VCR-Graphormer model for effective and scalable graph representation learning.
- Demonstrated state-of-the-art performance of VCR-Graphormer on 13 datasets with different scales and heterophily levels.

In summary, the paper makes significant contributions in developing a mini-batch ready graph transformer that matches or exceeds previous dense attention models in effectiveness while being efficient and scalable. The virtual connection based graph rewiring broadens the graph information encoded in each node's token list to support comprehensive representation learning.
