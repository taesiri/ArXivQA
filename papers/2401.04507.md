# [TechGPT-2.0: A large language model project to solve the task of   knowledge graph construction](https://arxiv.org/abs/2401.04507)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown strong performance on NLP tasks, but have limited domain-specific expertise for specialized tasks like knowledge graph construction. Specifically, existing models struggle with named entity recognition (NER) and relation triple extraction (RTE) which are crucial for knowledge graph creation.

- There is a need for capable open-source Chinese LLMs specialized for knowledge graph construction to advance research in integrating knowledge graphs and large models.

Methodology:  
- The authors introduce TechGPT-2.0, a project with 2 x 7B model weights and a QLoRA weight tailored for lengthy texts to address knowledge graph construction.

- Models are derived from Chinese-optimized LLAMA2, pre-trained on large Chinese corpora and fine-tuned on 4 million diverse instructions encompassing 15W general RTE, 15W general NER, 6.8W medical RTE, and 20W domain NER examples.

- Training data includes value alignment, hallucination data and focuses on medicine, law and geographical domains while retaining general capabilities.

- Models trained on Huawei Ascend servers using MindSpore/Mindformer. The ChatGLM architecture was selected after assessing adaptability.

Contributions:

- Release of TechGPT-2.0 models focused on knowledge graph construction that can serve as valuable open-source Chinese LLMs.

- Documentation of complete fine-tuning details on Ascend servers including debugging experiences to support LLM training on this platform. 

- Introduction of long-text handling capacity to TechGPT using the QLoRA model with position interpolation method.

- Future plans to conduct comprehensive experiments and utilize models for retrieval augmentation, agent and multimodal research.


## Summarize the paper in one sentence.

 This paper introduces TechGPT-2.0, a project with two 7B large language models and a QLoRA model specialized for knowledge graph construction tasks like named entity recognition and relationship triple extraction, providing insights into data processing, Ascend server debugging, and model training experiences.


## What is the main contribution of this paper?

 Based on the content of the paper, the main contributions seem to be:

1) The release of two 7B-scale instruction fine-tuned models (TechGPT-2.0-alpaca-hf and TechGPT-2.0-atom-hf) and a QLoRA model weight specialized for processing lengthy texts, focused on knowledge graph construction tasks such as named entity recognition (NER) and relationship triple extraction (RTE).

2) Providing insights and experiences related to data collection/processing, debugging the Ascend server, and model training, aimed at guiding future research efforts on training large-scale language models on Ascend servers. 

3) Addressing the challenge of handling long texts through the use of the position interpolation method with the QLoRA model, enhancing the capacity to process lengthy conversations/documents while maintaining performance on shorter texts.

4) Releasing Chinese LLAMA2-based models (alpaca-hf and atom-hf) fine-tuned on a large instructional dataset to serve as a substantial Chinese open-source model capable of knowledge graph construction, intended for the research community.

In summary, the main contribution is furnishing Chinese open-source models focused on knowledge graph construction, alongside experiences to assist research on large model training using Ascend servers.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Large language models (LLMs)
- Knowledge graphs (KGs) 
- Named entity recognition (NER)
- Relationship triple extraction (RTE)
- Instruction tuning/fine-tuning
- Full fine tuning (FFT) 
- Parameter-efficient fine tuning (PEFT)
- Prompt tuning
- Prefix tuning 
- LoRA
- QLoRA
- Ascend server 
- Mindspore
- Mindformer
- Data processing
- Model training
- Long text processing

The paper introduces the TechGPT-2.0 project for enhancing large language models for knowledge graph construction tasks like NER and RTE. It provides details on model selection, data collection/processing, debugging the Ascend server, and insights from model training, including handling long texts. So the key terms reflect this focus on LLMs, knowledge graphs, instruction tuning approaches, and the technical details around implementing the project.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I have formulated about the methods proposed in the paper:

1. The paper mentions employing the RoPE method for addressing long text issues. Can you elaborate on why direct extrapolation was avoided and how exactly the position interpolation method enhances the model's capacity for longer texts? 

2. The paper states that the Ascend server firmware, Mindspore version and Mindformer version underwent rapid updates during development. Can you expand on some of the key challenges faced in version control and how you ensured stability for full fine-tuning of the LLAMA2 model?

3. When discussing the data collection process, it is mentioned that the KnowLM-IE dataset introduced considerable noise. Can you delve deeper into why directly incorporating this dataset was risky and how you handled filtering and adding portions of it?

4. The paper highlights sporadic Loss surges during instruction fine-tuning of the Chinese-Alpaca-2 (7B) model. What are some potential reasons for this non-convergence and how did you troubleshoot this issue?

5. You mention that using the full instruction fine-tuning dataset degraded model performance and capabilities over time. After extensive controlled experiments, what core insight did you gain into why secondary training leads to poorer output?

6. When elaborating on the model settings, it is stated that PanGuAlpha, Baichuan and Baichuan2 encountered debugging issues. Can you expand on the specific equipment and code challenges faced with these models?

7. The paper indicates preference for a 4 server configuration over 5 servers despite having 5 Ascend 910A servers available. What motivated this decision and what issues had you observed during training with 5 servers?

8. When discussing data processing, it is mentioned that noise from the KnowLM-IE dataset was handled by filtering and adding portions as new triple data. Can you elaborate on the specifics of the filtering criteria and data transformation process?

9. The paper states that initial annotation data caused confusion between NER and RTE tasks post-training. How exactly did the annotation process lead to this problem and how was it addressed?

10. When comparing the Chinese-Alpaca-2 and Atom-7B-Chat models, what aspects account most significantly for their disparity in performance across different tasks?
