# [Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning](https://arxiv.org/abs/2402.11690)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing vision-language models (VLMs) lack task diversity in pretraining and instruction tuning, leading to issues like poor generalizability, hallucination, and catastrophic forgetting. 
- Most instruction tuning datasets are synthetically generated via GPT-4 by repurposing existing annotations. This can introduce spurious correlations, long-form outputs and bias that exacerbate the aforementioned issues.

Proposed Solution:  
- Construct Vision-Flan, the most diverse public visual instruction tuning dataset with 187 tasks and 1.6M instances sourced from academic datasets, with expert-written instructions for each task.
- Introduce a two-stage instruction tuning framework:
   1) Finetune pretrained LLaVA model on Vision-Flan to gain diverse capabilities  
   2) Further finetune on minimal GPT-4 synthesized data (1K instances) to align outputs with human preferences

Key Contributions:
- Vision-Flan significantly enhances model capabilities and reduces hallucination/catastrophic forgetting
- Two-stage tuning achieves superior alignment using far less GPT-4 data than single-stage tuning  
- Analysis reveals:
   1) Increasing # of human-labeled tasks substantially boosts capabilities
   2) Minimal GPT-4 data (1K) suffices to align responses  
   3) More GPT-4 data introduces hallucination/bias, no proportional gains
   4) Tuning enhances LLMs' ability to process visual features

In summary, the paper introduces Vision-Flan, the most diverse public visual instruction tuning dataset to date, and a two-stage tuning framework that achieves SOTA performance with significantly less GPT-4 data than existing methods. Analysis provides meaningful insights into the roles and impacts of human-labeled vs GPT-4 synthesized data.
