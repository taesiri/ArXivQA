# [TEncDM: Understanding the Properties of Diffusion Model in the Space of   Language Model Encodings](https://arxiv.org/abs/2402.19097)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper proposes a novel text diffusion model called Text Encoding Diffusion Model (TEncDM) for high-quality text generation. 

Problem: Existing text diffusion models underperform compared to large autoregressive language models. They operate in the token embedding space which lacks contextual information. The decoding process is also simplistic by just rounding to the nearest embedding. The commonly used cosine and sqrt noise schedules also do not introduce enough noise and difficulty to the denoising task during training.

Proposed Solution: 
- Train diffusion model in the encoding space of a pretrained language model encoder (e.g. BERT) which contains contextual information. This boosts performance over using raw token embeddings.

- Use a Transformer decoder that considers context when converting the latents to text. This is more robust to inaccuracies compared to just rounding latents.

- Introduce a new tan-d noise schedule that adds more noise across timesteps to increase denoising difficulty. 

- Leverage self-conditioning which is found to increase prediction magnitude and confidence, allowing less steps during inference.

Main Contributions:
- Thorough analysis and insights on components like encodings, decoder, noise schedule and self-conditioning to understand and improve text diffusion models

- Proposal of TEncDM framework that trains diffusion model in encoding space and uses Transformer decoder

- Introduction of tan-d noise schedule that adds more noise for harder denoising

- Demonstrating state-of-the-art performance over previous diffusion and autoregressive models on summarization and paraphrasing tasks
