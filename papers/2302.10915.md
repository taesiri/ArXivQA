# [Conformers are All You Need for Visual Speech Recogntion](https://arxiv.org/abs/2302.10915)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether complex visual front-ends like convolutional neural networks or vision transformers are necessary for visual speech recognition, or if a simpler linear projection front-end paired with a Conformer encoder can achieve state-of-the-art performance. The key hypothesis is that allocating model capacity to a bigger Conformer encoder rather than a sophisticated visual front-end will result in better performance, lower latency, and more efficient memory usage for visual speech recognition models.Specifically, the authors hypothesize that:- A linear projection front-end paired with a bigger Conformer encoder will outperform models with complex visual front-ends like VGGs or ViTs.- The increased training efficiency of the linear projection model will allow it to better take advantage of large amounts of training data.- The linear projection's improved memory efficiency will allow fitting larger encoders into memory.- The linear projection will have lower latency compared to CNN or ViT front-ends.So in summary, the central hypothesis is that a simple linear projection front-end with a bigger Conformer encoder is all you need for state-of-the-art visual speech recognition, rather than complex hierarchical visual feature extractors. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is showing that complex visual front-ends, like convolutional neural networks or vision transformers, are not necessary for visual speech recognition. Instead, the authors propose using a simple linear projection front-end paired with a Conformer encoder, which achieves state-of-the-art results on the TED-LRS3 benchmark. Specifically, the key contributions are:- Conducting profiling experiments that show sophisticated visual front-ends like VGGs and ViTs are inefficient in terms of memory usage, latency, and scaling compared to a linear projection front-end.- Proposing a Conformer model with a linear projection front-end that trains twice as fast and fits twice as many parameters in memory compared to prior work.- Achieving a new state-of-the-art word error rate of 12.8% on the TED-LRS3 visual speech recognition benchmark using the linear projection Conformer, which rivals the performance of audio-only models from a few years ago.- Showing a 15% diarization error rate improvement on an audio-visual diarization task compared to a model with a VGG front-end.- Demonstrating the linear projection Conformer model is robust to missing video, with performance degrading gracefully as more frames are dropped.In summary, the key insight is that for visual speech recognition, it is more important to model the relationships between video frames rather than between pixels within a frame, and thus a simple linear projection is sufficient as a visual front-end. The computational savings can then be allocated to a larger Conformer encoder.
