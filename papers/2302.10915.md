# [Conformers are All You Need for Visual Speech Recogntion](https://arxiv.org/abs/2302.10915)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether complex visual front-ends like convolutional neural networks or vision transformers are necessary for visual speech recognition, or if a simpler linear projection front-end paired with a Conformer encoder can achieve state-of-the-art performance. The key hypothesis is that allocating model capacity to a bigger Conformer encoder rather than a sophisticated visual front-end will result in better performance, lower latency, and more efficient memory usage for visual speech recognition models.Specifically, the authors hypothesize that:- A linear projection front-end paired with a bigger Conformer encoder will outperform models with complex visual front-ends like VGGs or ViTs.- The increased training efficiency of the linear projection model will allow it to better take advantage of large amounts of training data.- The linear projection's improved memory efficiency will allow fitting larger encoders into memory.- The linear projection will have lower latency compared to CNN or ViT front-ends.So in summary, the central hypothesis is that a simple linear projection front-end with a bigger Conformer encoder is all you need for state-of-the-art visual speech recognition, rather than complex hierarchical visual feature extractors. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is showing that complex visual front-ends, like convolutional neural networks or vision transformers, are not necessary for visual speech recognition. Instead, the authors propose using a simple linear projection front-end paired with a Conformer encoder, which achieves state-of-the-art results on the TED-LRS3 benchmark. Specifically, the key contributions are:- Conducting profiling experiments that show sophisticated visual front-ends like VGGs and ViTs are inefficient in terms of memory usage, latency, and scaling compared to a linear projection front-end.- Proposing a Conformer model with a linear projection front-end that trains twice as fast and fits twice as many parameters in memory compared to prior work.- Achieving a new state-of-the-art word error rate of 12.8% on the TED-LRS3 visual speech recognition benchmark using the linear projection Conformer, which rivals the performance of audio-only models from a few years ago.- Showing a 15% diarization error rate improvement on an audio-visual diarization task compared to a model with a VGG front-end.- Demonstrating the linear projection Conformer model is robust to missing video, with performance degrading gracefully as more frames are dropped.In summary, the key insight is that for visual speech recognition, it is more important to model the relationships between video frames rather than between pixels within a frame, and thus a simple linear projection is sufficient as a visual front-end. The computational savings can then be allocated to a larger Conformer encoder.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper shows that a simple linear projection visual front-end paired with a Conformer encoder achieves state-of-the-art visual speech recognition performance on the TED-LRS3 benchmark, outperforming more complex visual front-ends like VGGs and vision transformers.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in visual speech recognition:- This paper proposes using a simple linear projection (LP) front-end instead of more complex visual front-ends like VGG, ResNet, 3D CNNs, etc. Most prior work has focused on increasingly sophisticated visual front-ends. Using a simple LP front-end is a departure from this trend.- They show state-of-the-art results on the TED-LRS3 benchmark by pairing the LP front-end with a Conformer encoder. Prior work typically used Transformers or LSTMs instead of Conformers.- The LP Conformer model achieves 12.8% WER on TED-LRS3 for video-only speech recognition. This is comparable to good audio-only models from a few years ago. Prior lipreading models had much higher WERs in the 20-30% range.- They demonstrate the LP Conformer's effectiveness for audio-visual diarization, obtaining lower DER and WDER compared to models with VGG or ResNet front-ends. Most prior work focused only on speech recognition.- The paper analyzes model efficiency, showing the LP Conformer trains faster and uses less memory than ViT+Conformer models. Other papers do not include similar profiling.- Robustness analysis shows the LP Conformer degrades gracefully like prior models when video frames are dropped at test time. Model robustness to missing inputs is a key consideration.In summary, using a simple LP front-end with Conformers for VSR is novel compared to prior visual front-ends. The efficiency and state-of-the-art results demonstrate this is an effective approach for VSR and related tasks. The analysis is more comprehensive than most papers in this field.


## What future research directions do the authors suggest?

Based on my reading of the paper, some future research directions suggested by the authors are:- Exploring other simple yet efficient visual front-ends besides linear projection. The authors showed linear projection works very well, but there may be other simple visual front-ends that are equally effective.- Applying the efficient LP Conformer model to other visual speech tasks like audio-visual speaker identification. The authors demonstrated strong results on visual speech recognition and diarization, but the model could likely achieve state-of-the-art on other tasks too.- Leveraging the computational efficiency of the LP Conformer to scale up model and dataset size even further. The authors suggest their efficient model can take better advantage of large datasets, so scaling up in terms of data and model parameters is a clear next step.- Deploying the model in a production setting and testing robustness to real-world conditions. The authors already tested robustness to missing video, but evaluating real-world performance could reveal new challenges.- Exploring whether similar findings apply in other multimodal domains like audio-visual scene analysis. The authors suggest their one-patch model may generalize, so testing it on non-speech modalities could be impactful.In summary, the main future directions relate to exploring other efficient front-ends, applying the model to new tasks, leveraging its efficiency to scale further, and evaluating real-world deployment and generalization to other modalities. The authors convincingly demonstrate the potential of simple and efficient models for visual speech recognition.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes using a simple linear projection front-end paired with a Conformer encoder architecture for visual speech recognition, rather than more complex visual front-ends like convolutional neural networks or vision transformers. Through profiling experiments, they find that complex visual front-ends have high latency and memory usage compared to the encoder, limiting model size and training efficiency. Their proposed model with just a linear projection front-end and Conformer encoder achieves state-of-the-art results on the TED-LRS3 benchmark, reaching 12.8% WER for video-only and 0.9% WER for audio-visual speech recognition. This demonstrates that large Conformer encoders are more important than complex visual front-ends for modeling relationships between video frames in VSR. The linear projection front-end also enables building robust and performant models for audio-visual diarization. Overall, the work shows simple is better for visual front-ends in VSR when paired with powerful Transformers like the Conformer.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes using a simple linear projection (LP) as the visual front-end instead of more complex architectures like VGGs or vision transformers for visual speech recognition (VSR). The authors show through profiling experiments that complex visual front-ends like VGG and ViT have poor latency scaling and memory efficiency compared to a Conformer encoder. By replacing a ViT front-end with an LP, they are able to fit a Conformer model twice as large into memory and achieve twice the training speed. The LP Conformer model achieves state-of-the-art word error rate of 12.8% on the TED-LRS3 benchmark for VSR. This demonstrates that complex visual front-ends are not needed, since attending to relationships between video frames is more important than relationships between pixels within a frame for VSR. The LP Conformer also improves audio-visual diarization performance by 15% DER over an equivalent VGG-based model on the MEET360 dataset.In summary, this paper shows that a simple LP visual front-end paired with a Conformer encoder is sufficient to achieve state-of-the-art results for VSR on TED-LRS3. The LP Conformer is more parameter and memory efficient compared to models with VGG or ViT front-ends, allowing for bigger Conformer encoders to be trained. The effectiveness of the LP Conformer highlights that modeling relationships between video frames over time matters more than relationships between pixels within a frame for visual speech recognition.
