# [Experimenting with Large Language Models and vector embeddings in NASA   SciX](https://arxiv.org/abs/2312.14211)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper describes an experiment by the NASA SciX team to utilize large language models (LLMs) to improve information retrieval and data augmentation for their scientific literature database. The key problem they aimed to solve was the issue of "hallucination" - when LLMs generate fictional responses because they lack the relevant knowledge or context. 

To address this, they implemented a retrieval augmented generation (RAG) system. This involves prompting the LLM with the user's question along with relevant text snippets from the literature. The snippets provide context to ground the LLM's response in reality. They experimented with different strategies to select good snippets, including:

1) Using a separate LLM to translate questions into queries for the SciX search engine. This retrieves relevant abstracts. 

2) Creating semantic vector embeddings of paragraphs from full text articles. Closest vectors to the question vector are retrieved as snippets.

They built a web interface and API to interact with the LLMs. Based on subjective human evaluation, RAG with full text snippets produced the best quality responses with less hallucination. 

The experiment demonstrated the potential for LLMs to improve SciX's capabilities while respecting data copyright and user privacy. It also highlighted the importance of grounding the models in relevant data to reduce fictional responses. Further work is needed to design additional SciX features leveraging this technology. But overall it opens up new possibilities for better serving users via LLMs.


## Summarize the paper in one sentence.

 This paper describes an experiment at NASA SciX using retrieval augmented generation with large language models to reduce hallucination and improve response quality when answering questions about scientific literature.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper describes an experiment carried out by the NASA SciX team to use large language models (LLMs) and retrieval augmented generation (RAG) to provide responses to user questions that are grounded in the scientific literature. Specifically, they developed a web interface and API to interact with open-source LLMs, and implemented two strategies to retrieve relevant text snippets from the literature to provide context and reduce the chance of hallucinations:

1) Using a LLM to translate natural language questions into queries for the NASA SciX search engine, retrieving relevant abstracts. 

2) Generating vector embeddings for paragraphs from full-text articles, and finding semantically similar snippets to the user's question.

The paper concludes that RAG with full-text snippets led to better quality and more detailed, specific responses from the LLMs compared to without retrieval or only using abstracts. The experiment helped NASA SciX explore potential use cases for LLMs while respecting data copyright and user privacy.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the key terms and concepts associated with this work include:

- Large language models (LLMs)
- OpenAI's chatGPT
- Retrieval augmented generation (RAG) 
- NASA SciX/Astrophysics Data System
- Semantic search
- Vector embeddings/semantic vectors
- Information retrieval
- Data augmentation
- User privacy
- Hallucination reduction
- Text snippets
- Abstracts
- APIs

The paper discusses an internal experiment by the NASA SciX team using open-source LLMs and a large corpus of scientific articles. It focuses on using RAG and semantic search with snippets of text to reduce hallucinations and provide more grounded, relevant responses from the LLMs to user queries. Overall, it explores how LLMs could be utilized in projects like NASA SciX for tasks like data enrichment while respecting data copyright and user privacy.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The authors mention using both a traditional search approach and a modern semantic search approach to find relevant snippets. How do these two approaches differ, and what are the relative advantages and disadvantages of each? 

2. The authors tested several open-source large language models before settling on Zephyr as one of the best 7 billion parameter models. What specific criteria did they use to evaluate the models and why did Zephyr stand out?

3. The authors utilized a prompt formatting strategy that included simulated conversations between a user and assistant. What was the rationale behind structuring the prompts this way rather than just providing the user question and snippets? 

4. What complications did the authors encounter when trying to translate natural language questions into structured queries for the NASA SciX search API? How did they attempt to overcome these?

5. The authors mention the risk of allowing multiple parallel requests to their LLM web interface due to GPU constraints. What solutions could they implement to scale up the number of supported concurrent requests?  

6. How large was the semantic search corpus created from the open access full text articles? What considerations went into determining the paragraph segmentation and embedding strategies?

7. What specific evaluation was done to determine if the LLM responses based on retrieval augmentation contained less hallucination? What metrics were used?

8. How did the authors balance providing enough snippet context to reduce hallucination while avoiding snippets that contain misleading or irrelevant information?  

9. What data augmentation and knowledge extraction applications are the authors considering building on top of the LLM API and interface created through this experimentation?

10. What additional safeguards would need to be put in place before considering deploying these LLM services openly rather than just for internal NASA SciX use?
