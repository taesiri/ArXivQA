# [FRL: Federated Rank Learning](https://arxiv.org/abs/2110.04350)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the key points from this paper are:- The paper proposes a new federated learning algorithm called Federated Rank Learning (FRL) to address two major challenges in federated learning: robustness against poisoning attacks and communication efficiency. - The main hypothesis is that the large continuous space of model update values that clients can send in standard federated learning algorithms makes them susceptible to poisoning attacks. - FRL aims to address this by having clients send rankings of model parameters rather than the parameter values themselves. This converts the continuous space of updates to a discrete space of rankings.- The intuition is that the discrete ranking space makes it harder for attackers to craft malicious updates, thereby improving robustness. The ranking vectors are also smaller than full model updates, improving communication efficiency.- FRL has clients collaboratively find a highly performing subnetwork within a fixed, randomly initialized "supernetwork" by ranking the importance of the edges in the supernetwork. The global edge rankings are computed via a novel voting-based aggregation of local client rankings.- Experiments demonstrate that FRL achieves competitive utility compared to FedAvg but with significantly improved robustness against poisoning attacks and lower communication costs.In summary, the main hypothesis is that using a discrete space of rankings rather than continuous model update values can simultaneously improve robustness and communication efficiency in federated learning. FRL is proposed to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:1. It proposes a new federated learning algorithm called Federated Rank Learning (FRL) that aims to improve robustness against poisoning attacks and reduce communication costs compared to existing federated learning algorithms like FedAvg. 2. FRL trains clients to collaboratively rank the edges of a randomly initialized neural network (called the supernetwork) based on importance, rather than directly training the weights like FedAvg. This ranking is done by having clients run the edge popup algorithm locally. 3. FRL's server aggregates client rankings using a voting mechanism to determine the globally important edges. This results in a discrete space of updates (rankings) rather than a continuous space like in FedAvg, which restricts what malicious clients can do to poison the model.4. Theoretical analysis shows FRL is robust to poisoning attacks by design under certain assumptions. Experiments also demonstrate superior robustness compared to state-of-the-art defenses like Multi-Krum and Trimmed-Mean.5. FRL achieves similar utility to FedAvg on tasks like CIFAR-10 classification but with much lower communication costs. For example, 35% lower communication than FedAvg on CIFAR-10 with similar accuracy.6. Proposes Sparse-FRL extension to further reduce communication costs by having clients only send top-k fractional ranks rather than full rankings.In summary, the key ideas are using rankings rather than weights for training, a voting procedure for aggregation, and showing these ideas improve robustness and communication efficiency in federated learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes Federated Rank Learning (FRL), a new federated learning approach where clients collaborate by ranking the importance of edges in a neural network instead of sharing weight updates, which improves robustness to poisoning attacks and reduces communication costs.In more detail, the key points are:- Existing federated learning algorithms are vulnerable to poisoning attacks where malicious clients send manipulated model updates. A key reason is the large continuous space of possible weight updates clients can send. - FRL has clients rank the importance of edges in a neural network to find a good subnetwork. Clients send integer rankings rather than continuous weight updates, severely restricting the space for malicious updates.- FRL uses a voting method to aggregate client rankings into a global ranking and subnetwork. This makes poisoning attacks much harder as each client only gets one vote.- FRL significantly reduces communication costs as rankings are smaller than full weight updates.- Experiments show FRL matches accuracy of FedAvg on CIFAR10 but with 35% lower communication cost. FRL is also far more robust to poisoning attacks.So in summary, FRL improves federated learning by using client rankings rather than weight updates, enhancing robustness and efficiency. The key innovation is the discrete ranking-based update space.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in federated learning:- The main goal of the paper is to improve the robustness of federated learning against poisoning attacks while also reducing communication costs. This is an important research direction since poisoning attacks and communication efficiency are two major challenges facing real-world deployment of federated learning.- The key idea proposed is using edge rankings instead of raw model parameters for client updates. This converts the continuous update space to a discrete space, limiting what attackers can do. It also compresses the updates. Both help achieve the robustness and efficiency goals.- Using rankings/supermasks for model training has been explored before in areas like lottery ticket hypothesis and supermasks training. But the paper makes a nice connection to improve federated learning specifically. The voting-based aggregation for global rankings is also novel.- For robustness, the paper compares against existing defense methods like Multi-Krum and Trimmed Mean. The experiments demonstrate improved attack resistance. The theoretical analysis also quantifies the robustness gain. - For efficiency, the paper compares against common compression techniques like SignSGD and Top-K sparsification. The experiments show FRL can match or exceed efficiency while maintaining accuracy.- The focus is on untargeted attacks to maximize impact, as those degrade performance for most clients. Targeted or backdoor attacks are relevant too but help showcase the general robustness.In summary, the paper makes good contributions in adapting supermask-style ranking training for the federated setting and demonstrating nice improvements in two important metrics - robustness and efficiency. The comparisons against relevant baselines are also useful additions to the literature.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring different aggregation mechanisms in Federated Rank Learning (FRL) beyond majority voting. The authors mention that the voting mechanism in FRL provides inherent robustness, but other aggregation rules could potentially improve utility. - Investigating how to further reduce the communication costs in FRL, such as through compression techniques for communicating the rankings. The authors propose Sparse-FRL to reduce upload communication costs. Additional techniques could help reduce download costs as well.- Evaluating FRL in different threat models, such as where attackers can collude or have knowledge of the global model. The authors' analysis makes assumptions about independent attackers with only local knowledge. Testing FRL under different assumptions could reveal strengths/weaknesses.- Applying FRL to other domains beyond image classification, such as language tasks, recommendation systems, etc. The authors demonstrate FRL on image datasets, but the approach could potentially translate to other data types and applications.- Comparing FRL to personalized federated learning methods like Federated Distillation. The authors suggest FRL is complementary, so directly comparing performance would be interesting.- Developing theoretical understanding of how ranking distributions impact FRL convergence and accuracy. The authors provide an initial analysis, but more precise characterizations could guide algorithm design.In summary, the main directions are enhancing utility of FRL through improved aggregation and reductions in communication costs, evaluating security under broader threat models, and expanding FRL to new domains and theoretical insights.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes a new federated learning approach called Federated Rank Learning (FRL) that is robust to poisoning attacks and communication-efficient. FRL reduces the space of client updates from continuous model parameter updates in standard federated learning to discrete rankings of model parameters. In FRL, clients rank the parameters of a randomly initialized neural network provided by the server based on their local data. The FRL server aggregates these rankings using a voting mechanism to obtain a global ranking and corresponding binary mask, which is applied to the random network to obtain the final subnetwork for downstream tasks. By limiting client updates to rankings rather than continuous parameters, FRL restricts the choices available to poisoning clients and enables the use of voting for robust aggregation. Experiments demonstrate FRL's superior robustness over existing algorithms and comparable communication efficiency. Key results show FRL maintains high accuracy under poisoning attacks that significantly degrade the performance of other methods, while reducing communication costs by 26-35% compared to standard federated averaging.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:The paper proposes a new federated learning framework called Federated Rank Learning (FRL) that is designed to be robust against poisoning attacks while also reducing communication costs. FRL argues that a core weakness in existing federated learning algorithms is the large continuous space of model update values that clients can send, which allows malicious clients flexibility to craft highly damaging updates. Instead, in FRL, clients rank the importance of edges in a random neural network and send these discrete rankings to the server. The server aggregates client rankings using a voting mechanism to determine a global edge ranking and corresponding subnetwork for each round. By restricting updates to rankings rather than continuous values, FRL limits the choices available to attackers. Experiments show FRL is significantly more robust to poisoning compared to existing defenses while also reducing communication costs. For example, on CIFAR-10 with 10% attackers, FRL retains 79% accuracy versus 56-58% for other defenses. FRL also reduces communication cost by 35% compared to standard Federated Averaging.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a novel federated learning algorithm called Federated Rank Learning (FRL) that aims to improve robustness against poisoning attacks and reduce communication costs. In FRL, clients collaborate to find a subnetwork within a randomly initialized neural network (called the supernetwork) by ranking the importance of the network edges based on their local data. Clients share these local edge rankings with the FRL server, which uses a voting-based aggregation mechanism to combine the rankings into a global edge ranking. This global ranking represents the most important edges that will form the final subnetwork, which is equivalent to the global model in standard federated learning. By using discrete rankings rather than continuous model weight updates, FRL restricts the space available to adversaries to craft malicious updates and makes the aggregation more robust. Communicating rankings also reduces bandwidth usage compared to sending full model weight updates. The key intuition is that ranking-based federated learning can mitigate issues in standard federated learning related to poisoning attacks and communication efficiency.


## What problem or question is the paper addressing?

 Here are the key points from the paper:- The paper proposes a new federated learning algorithm called Federated Rank Learning (FRL) to address two major challenges in federated learning: robustness to poisoning attacks and communication efficiency. - Existing federated learning algorithms like FedAvg are vulnerable to poisoning attacks where malicious clients send manipulated model updates to degrade the performance of the global model. FRL aims to improve robustness by having clients send rankings of model parameters rather than the parameter values directly. - FRL reduces the continuous space of possible model updates in FedAvg to a discrete space of rankings. This restricts the choices available to a poisoning adversary, making it harder to manipulate the global model.- FRL also aims to reduce communication costs compared to FedAvg by only transmitting rankings rather than full model parameters. Rankings can be encoded more efficiently than float parameter values.- In FRL, clients collaborate to find a highly performing subnetwork within a larger randomly initialized "supernetwork", without changing the supernetwork weights. Clients rank the importance of edges in the supernetwork based on their local data.- The FRL server aggregates client rankings using a voting scheme to determine the globally best subnetwork. This subnetwork with the top ranked edges is then used as the final global model.- Experiments on MNIST, CIFAR10, and FEMNIST datasets show FRL is more robust to poisoning attacks and achieves similar accuracy to FedAvg with 35% lower communication.In summary, the key novelty is using client rankings rather than model parameters to improve robustness and efficiency of federated learning. The paper demonstrates the viability of this ranking-based approach.
