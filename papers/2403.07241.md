# [Calibrating Multi-modal Representations: A Pursuit of Group Robustness   without Annotations](https://arxiv.org/abs/2403.07241)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Spurious correlations are a key challenge for vision-language models like CLIP, where the model relies on superficial patterns rather than intended features for prediction. This causes issues like reduced robustness and poor generalizability. 
- Mitigating spurious correlations usually requires either full fine-tuning (computationally expensive) or access to group annotations (labeling cost).
- The paper explores an efficient way to improve the group robustness of CLIP without access to group annotations.

Method: 
- First, the paper analyzes CLIP and shows clear evidence of spurious correlations using t-SNE visualizations and GradCAM.
- Then, a lightweight representation recalibration method called Contrastive Feature Recalibration (CFR) is proposed. 
- CFR has two main steps:
   1. Construct a calibration set by selecting samples misclassified by a CLIP model tuned with ERM.
   2. Recalibrate representations of anchors in this set using contrastive learning, pulling them closer to class centroids.
- Two sample selection strategies are explored for contrastive learning: Dynamic Positive Centroid Sampling (DPS) and Random/Nearest Neighbor Negative Sampling (RNS/NNS).

Main Contributions:
- Comprehensive analysis showing spurious correlations in pretrained CLIP models. 
- Lightweight CFR method to recalibrate CLIP's features without any group annotations, using contrastive learning on a calibrated set.
- Extensive experiments on multiple datasets demonstrating state-of-the-art performance of CFR among semi-supervised methods in improving group robustness. 
- Detailed visualizations and ablations providing insights into the effectiveness of the recalibrated representations.

In summary, the paper makes notable contributions in analyzing and mitigating spurious correlations for efficient fine-tuning of vision-language models like CLIP.
