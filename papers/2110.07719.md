# [Certified Patch Robustness via Smoothed Vision Transformers](https://arxiv.org/abs/2110.07719)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how vision transformers (ViTs) can be used to enable better certified patch robustness compared to convolutional neural networks (CNNs), in terms of higher standard accuracy, increased robustness guarantees, and faster inference. Specifically, the paper investigates whether ViTs are better suited than CNNs as the base classifier model within the derandomized smoothing framework for certifying patch robustness. The key hypotheses are:1) ViTs will have higher accuracy than CNNs on the image ablations used in derandomized smoothing. This is because ViTs can dynamically attend to the visible regions and do not require gradually increasing receptive fields like CNNs.2) ViTs can be easily modified to avoid computation on masked image regions, directly speeding up the smoothing process. This stems from their token-based architecture which allows dropping irrelevant masked tokens.3) Using ViTs within derandomized smoothing will thus lead to higher standard accuracy, better robustness guarantees, and faster inference compared to using CNNs.The paper aims to demonstrate these hypotheses through empirical analysis and proposes smoothed vision transformers that achieve improved certified patch robustness over prior work based on CNNs. The suitability of ViTs for handling image ablations is the key motivation and theoretical basis for the proposed approach.In summary, the central question is whether changing the model architecture to ViTs can enable better certified defenses in terms of accuracy, robustness and speed compared to existing CNN-based approaches. The paper hypothesizes ViTs are better suited for smoothing-based certified patch defenses and empirically verifies this claim.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is demonstrating how using Vision Transformers (ViTs) as the base model in certified defenses against adversarial patches can significantly improve robustness guarantees and maintain standard accuracy and inference times comparable to non-robust models. Specifically, the key contributions are:1. Showing that ViTs are much better suited than CNNs for classifying the masked image ablations used in smoothing-based certified defenses like derandomized smoothing. Replacing CNNs with ViTs leads to substantial gains in certified patch robustness.2. Modifying the ViT architecture and smoothing procedure to enable much faster inference times. This includes dropping masked tokens from ablated images and using strided ablations. Together these allow smoothed ViTs to have inference times close to non-robust ResNets. 3. Achieving state-of-the-art certified robustness to patch attacks on ImageNet while maintaining standard accuracy comparable to non-robust models (>70%). Prior work suffered from much lower standard accuracy.4. Providing an in-depth analysis on how choices in the smoothing procedure like ablation size and striding affect the tradeoffs between robustness, standard accuracy, and inference time. This provides guidance on how to optimize smoothed ViTs.In summary, the key innovation is leveraging the properties of ViTs to enable certified patch defenses that are much faster, more accurate on natural images, and more robust compared to prior work based on CNNs. This makes certified robustness against patches more viable and practical.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper demonstrates that using vision transformers as the base classifier in derandomized smoothing substantially improves the robustness guarantees, standard accuracy, and inference time of certified defenses against adversarial patches.In summary, the paper shows that leveraging vision transformers can enable certified patch defenses that are faster, more accurate, and more robust compared to using convolutional neural networks. The improvements stem from the capability of transformers to effectively handle the image ablations arising in derandomized smoothing.


## How does this paper compare to other research in the same field?

Here is a brief summary of how this paper on smoothed vision transformers compares to other related research:- The paper focuses on certifiable defenses against adversarial patch attacks, an important problem in robust ML. Prior work like randomized smoothing and derandomized smoothing provide certified defenses, but suffer from poor accuracy and slow inference. - The key novelty is using vision transformers (ViTs), instead of standard CNNs, as the base model within the smoothing framework. The paper shows ViTs are better suited to handle the cropped/masked images needed for smoothing.- Compared to prior certified defenses like derandomized smoothing with CNNs, using ViTs provides higher robustness guarantees (e.g. 13% higher certified accuracy on ImageNet). The standard accuracy also remains high and close to non-robust models.- The ViT architecture is modified to avoid computation on masked tokens, making the smoothing procedure much faster (up to 100x on ImageNet). This makes the defense practically viable compared to prior work.- Overall, the paper demonstrates significant improvements in certified robustness against patch attacks by adapting the model architecture. The insights on ViTs being well-suited for smoothed defenses are novel.- The work is compared extensively to recent smoothing-based defenses like derandomized smoothing and PatchGuard. The experiments convincingly show the benefits of using ViTs over standard CNN architectures for this task.In summary, this paper advances the state-of-the-art for provable defenses against patch attacks by leveraging ViTs in a novel way. The architectural insights and modifications contribute new ideas to this area.


## What future research directions do the authors suggest?

Here are some of the main future research directions suggested in the paper:- Develop more efficient training methods for vision transformers (ViTs) to make them more applicable for a wider range of computer vision tasks. The authors suggest techniques like knowledge distillation could help improve training efficiency.- Explore how to make ViTs more sample efficient compared to convolutional neural networks. ViTs typically require large datasets like ImageNet to reach full performance. Methods to reduce the data requirements could make ViTs more accessible.- Study the effects of different tokenization approaches and patch sizes for ViTs. The authors mention the common practice of splitting images into fixed 16x16 patches may not be optimal. Adaptive tokenization could improve efficiency and performance.  - Analyze the theoretical properties of self-attention and relate it to convolutions to better understand the trade-offs between these architectures. This could help guide architecture designs in the future.- Develop techniques to scale up ViTs to handle higher resolution images and videos efficiently. The quadratic complexity of self-attention limits their direct application for high-resolution data.- Explore ViT architectures specialized for sparse data like 3D point clouds, graphs, etc. Rather than flattening the data into tokens, designing ViT variants that directly operate on non-euclidean data could be beneficial.- Understand if certain visual tasks can provably benefit from the global processing of self-attention compared to local operations like convolutions. This theoretical analysis could reveal which domains are best suited for ViT models.In summary, the key future directions focus on improving training efficiency, sample complexity, understanding trade-offs with convolutions, scaling to high-resolution and sparse data, and developing specialized ViT architectures for different vision domains. Advances in these areas could help increase the applicability of vision transformers.
