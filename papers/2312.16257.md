# [More than Correlation: Do Large Language Models Learn Causal   Representations of Space?](https://arxiv.org/abs/2312.16257)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
Recent work showed high correlation between learned representations of large language models (LLMs) like LLaMa and geospatial properties of input text. However, it is unclear if there is a causal relation between the internal spatial representations and model behaviors. This paper investigates whether LLMs learn and use a spatial world model that governs their behaviors.

Methods: 
The authors analyzed the spatial representations learned by DeBERTa-v2 and GPT-Neo on a dataset of city names mapped to latitudes/longitudes using:

- Representational similarity analysis between distance metrics of country representations in LLM activation space and actual geographic space.

- Linear and non-linear probing by training regressors to predict latitude/longitude of cities from LLM activations. A new GeoDist loss is proposed to address limitations of MSE loss.

- Causal intervention experiments where intermediate LLM activations of city names were edited using probe gradients and changes in model prediction were measured for a country classification task and next word prediction.

Key Results:

- RSA and probing analyses showed DeBERTa and GPT-Neo learn spatial representations of city names, with higher layers capturing more geospatial information.

- Intervening on activations to enhance encoded geospatial info improved country classification accuracy, establishing causal influence of spatial representations.

- Similar interventions also causally impacted next word prediction performance that relies on geospatial knowledge.

Main Contributions:

- Provided strong evidence for causal (not just correlative) learned spatial world model in smaller LLMs using targeted activation editing.

- Proposed a novel GeoDist loss for more accurate supervision of latitude/longitude regression probes.

- Showed spatial representations govern model behaviors on geospatial tasks rather than simply reflecting training data statistics.

In summary, this paper clearly establishes a causal spatial world model in LLMs through detailed analysis and intervention experiments, advancing our understanding of these models. The proposed GeoDist loss also helps build more powerful probes.
