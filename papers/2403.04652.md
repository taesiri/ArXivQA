# [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is still a significant gap between the capabilities of existing open-source language models like LLaMA and proprietary models like GPT-3.5 in areas like reasoning, math, and coding. 
- Scaling up model size alone often leads to diminishing returns without corresponding improvements in data quality and finetuning methodology.

Proposed Solution - Yi Model Series:
- Pretrain transformer-based 6B and 34B parameter language models from scratch on 3.1 trillion high-quality English and Chinese tokens cleaned with a sophisticated cascaded filtering pipeline.
- Finetune on a small (<10K) but carefully hand-crafted dataset over multiple iterations based on engineer and user feedback, focusing on quality over quantity.  
- Extend capabilities to 200K context via lightweight continued pretraining, equip models with visual understanding via a vision transformer encoder, and depth-upscale base models.
- Enable efficient deployment via model quantization and optimizations like dynamic batching and paged attention.

Main Contributions:
- Yi-34B matches GPT-3.5 performance on benchmarks while being deployable on consumer GPUs for local use.
- Show strong performance gains from training on larger but cleaner datasets compared to model size alone.
- Demonstrate competitive performance can be achieved with small, high-quality finetuning datasets.  
- Present methods to extend model capabilities to areas like vision and provide infrastructure to enable efficient deployment.
- Overall, provide a strong open-source model for the community with analysis on effective methods for scaling and adapting language models.
