# [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is still a significant gap between the capabilities of existing open-source language models like LLaMA and proprietary models like GPT-3.5 in areas like reasoning, math, and coding. 
- Scaling up model size alone often leads to diminishing returns without corresponding improvements in data quality and finetuning methodology.

Proposed Solution - Yi Model Series:
- Pretrain transformer-based 6B and 34B parameter language models from scratch on 3.1 trillion high-quality English and Chinese tokens cleaned with a sophisticated cascaded filtering pipeline.
- Finetune on a small (<10K) but carefully hand-crafted dataset over multiple iterations based on engineer and user feedback, focusing on quality over quantity.  
- Extend capabilities to 200K context via lightweight continued pretraining, equip models with visual understanding via a vision transformer encoder, and depth-upscale base models.
- Enable efficient deployment via model quantization and optimizations like dynamic batching and paged attention.

Main Contributions:
- Yi-34B matches GPT-3.5 performance on benchmarks while being deployable on consumer GPUs for local use.
- Show strong performance gains from training on larger but cleaner datasets compared to model size alone.
- Demonstrate competitive performance can be achieved with small, high-quality finetuning datasets.  
- Present methods to extend model capabilities to areas like vision and provide infrastructure to enable efficient deployment.
- Overall, provide a strong open-source model for the community with analysis on effective methods for scaling and adapting language models.


## Summarize the paper in one sentence.

 This paper introduces the Yi model family, a series of language and multimodal models trained on over 3 trillion tokens of engineered English and Chinese data that demonstrate strong capabilities across various tasks while being deployable on consumer hardware.


## What is the main contribution of this paper?

 This paper introduces the Yi model family, which includes 6B and 34B language models as well as extensions to chat, long context, vision-language, and depth-upscaled variants. The key contributions summarized in the paper are:

1. Yi-34B matches GPT-3.5 performance on many benchmarks while being more efficient for deployment thanks to quantization. This makes it suitable for local deployment on consumer devices.

2. The pretraining procedure emphasizes using a large, high quality dataset - 3.1 trillion tokens that have gone through extensive filtering and deduplication. This is more data than the conventional optimal regime.

3. The finetuning procedure focuses on carefully curating a small (<10k) but high quality set of instructions, polished iteratively by ML engineers based on user feedback. This deviates from existing approaches that scale up instruction datasets to millions of examples.

4. Extensions are introduced for 200k context via lightweight continued pretraining, vision-language capabilities via adapting a vision transformer, and depth upscaling by duplicating layers.

5. The overall results demonstrate Yi's strong performance across many benchmarks and human evaluations, matching GPT-3.5 in quality while being more deployable. The key conclusion is that continuing to scale model size along with optimized data leads to better frontier models.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Yi model family - The series of language and multimodal models introduced in the paper, including Yi-6B, Yi-34B, Yi-Chat, Yi-VL, etc.

- Pretraining - The process of training the base Yi models on a large corpus of 3.1 trillion tokens of English and Chinese text data. A key aspect is the sophisticated data cleaning pipeline used.

- Finetuning - Training the models on a small (<10K instances) but high-quality dataset to adapt them for conversational chat abilities. Focuses on iterative refinement of data based on user feedback rather than scale. 

- Model architectures - Uses a standard Transformer architecture with modifications like GQA attention, SwiGLU activation. Extends to vision-language models by integrating a vision encoder.

- Long context modeling - Extending models to support 200K token context lengths using continual pretraining and finetuning approaches.

- Depth upscaling - Making models deeper, e.g. increasing Yi-6B from 32 to 48 layers to create Yi-9B, using continual pretraining.

- Inference optimizations - Quantization, dynamic batching, paged attention to reduce serving costs.

- Data engineering - Emphasis on data quality via filtering, clustering, deduplication. Cascaded pipeline to clean 3.1T token pretrain corpus.  

- Performance - Matches GPT-3.5 scores on many benchmarks. Human preference rates comparable to GPT-3.5 on evaluation platforms.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The paper mentions using a sophisticated data cleaning pipeline for pretraining data. Can you elaborate more on the specific techniques used in this pipeline, especially the learned filters like perplexity scorer and quality scorer? How were they trained and evaluated?

2. When choosing the model scale, the paper aims for a balance between performance and cost-effectiveness. What were the specific considerations and trade-offs when settling on the 34B parameter size? Why not smaller or larger? 

3. The paper takes a different approach from other works by emphasizing finetuning data quality over quantity. Can you expand more on the iterative process of constructing and refining the finetuning dataset? What was the evaluation and user feedback loop like? 

4. The document mentions optimizing finetuning hyperparameter selection through approximate grid search. What was the search space and how many configurations were evaluated? Were there any insightful findings?

5. For long context modeling, what modifications were made to enable training with sequence lengths up to 200K tokens? What schemes were used for computing self-attention efficiently?

6. When adapting the model to vision-language tasks, the paper leverages both labeled and unlabeled image-text data across multiple stages. Can you detail the objectives and learning dynamics at each stage?

7. With depth upscaling from 6B to 9B parameters, how was the decision made regarding which layers to duplicate? What insights guided this architecture search?

8. What software frameworks, libraries or other infrastructure elements were crucial for enabling efficient scale-up of pretraining and finetuning? 

9. The paper mentions safety considerations during data filtering and model alignment. What are some examples of safety issues discovered and what mitigations were put in place?

10. For real-world deployment, what optimizations mentioned in this paper were most critical for reducing latency and cost to serve user traffic? What scope exists for further optimizations?
