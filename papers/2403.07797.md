# [Joint Selection: Adaptively Incorporating Public Information for Private   Synthetic Data](https://arxiv.org/abs/2403.07797)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Generating differentially private synthetic data is an important task that allows sensitive data to be shared while preserving privacy. Many existing methods are based on an "adaptive measurements" framework where noisy measurements are made to iteratively refine a model. However, one limitation of these methods is the inability to incorporate public data, which could potentially improve accuracy. Simply pre-training models on public data is incompatible with methods like PGM that do not fix model structure a priori.  

Proposed Solution:
This paper develops a "Joint Adaptive Measurements" (JAM) framework that expands adaptive measurements to jointly select between measuring public data and private data. This allows public data to be selectively incorporated even when model structure evolves over iterations.

Specifically, the paper develops JAM-PGM which uses graphical models in PGM to represent the data distribution. The selection step is augmented to include public and private versions of candidate queries. A score function estimates the error reduction from making each measurement. This allows the method to decide when public data can accurately proxy for private data. A "frugal budgeting" scheme rolls privacy savings over to later rounds when public data is used.

Main Contributions:
- Develops the JAM framework to incorporate public data selection into iterative methods for differentially private synthetic data 
- Introduces joint selection between public and private measurements based on an expected error reduction score
- Proposes JAM-PGM method that can leverage public data even when model structure changes across iterations
- Empirically evaluates JAM-PGM against state-of-the-art baselines on multiple datasets
- Shows improved accuracy compared to public-data assisted baselines across varying levels of public data bias

In summary, the paper makes important contributions in effectively and adaptively incorporating public data to improve differentially private synthetic data generation. The JAM framework and JAM-PGM method advance the state-of-the-art in this area.


## Summarize the paper in one sentence.

 The paper develops a method called Joint Adaptive Measurements with PGM (JAM-PGM) for differentially private synthetic data generation that adaptively selects between using noisy measurements from private data or bias but noiseless measurements from public data in order to maximize accuracy.


## What is the main contribution of this paper?

 The main contribution of this paper is the development of "Joint Adaptive Measurements (JAM)", a framework for incorporating public data into differentially private synthetic data generation. Specifically, the paper proposes:

1) Expanding the candidate set of queries to include both private versions (measured with noise) and public versions (measured from public data). This allows the mechanism to privately select between using noisy private measurements or proxy public measurements.

2) A scoring function to select candidates that quantifies the expected improvement from making either a private or public measurement. This aims to balance accuracy gains with privacy costs.  

3) A "frugal budgeting" strategy that rolls over unused privacy budget from public measurements to later rounds, allowing more budget for private measurements when needed.

4) The method "JAM-PGM" which implements joint adaptive measurements using the PGM model. Experiments show it outperforms prior public data techniques in a range of scenarios, automatically determining when to use public vs. private measurements.

In summary, the key contribution is the joint adaptive measurements framework that expands the design space for privately incorporating public data via query selection. This is shown to improve accuracy of differentially private synthetic data.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper are:

- Differential privacy
- Synthetic data generation
- Adaptive measurements 
- Graphical models
- Marginals
- Public data
- Joint selection
- Query workload
- Measurement error
- Expected improvement score
- Frugal budgeting

The paper develops a differentially private mechanism called "Joint Adaptive Measurements with PGM" (JAM-PGM) for synthesizing data using both public and private data. It incorporates public data into the selection and measurement steps of the adaptive measurements framework. Key ideas include jointly selecting between public or private data to measure marginals, using an expected improvement score to guide selection, and a frugal budgeting strategy to allocate privacy budget. The goal is to minimize error on an input query workload while satisfying differential privacy. Graphical models and estimating measurement error play an important role.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the joint selection framework allow $\ourmethod$ to adaptively choose between using public or private data to measure marginals? What are the tradeoffs between these two types of measurements? 

2. Explain the expected improvement score function used by $\ourmethod$ to select queries. How does it balance reducing current error and minimizing future measurement error?

3. The "frugal budgeting" aspect of $\ourmethod$ rolls privacy savings from public measurements into future rounds. How does this impact the privacy analysis? Does it provide any concrete benefits?

4. In the experiments, what causes the performance difference between methods that use public data versus those that do not? When does utilizing public data help and when does it hurt accuracy?

5. What assumptions does the analysis in this paper make about the relationship between the public and private data distributions? How realistic are these assumptions and what would happen if they were violated?

6. How does the adaptive nature of $\ourmethod$'s joint selection alleviate some of the practical difficulties of using public data? Does it completely resolve them?

7. What modifications would need to be made to apply the joint selection framework to other iterative, select-measure-generate style synthetic data algorithms besides $\ourmethod$?

8. The paper identifies some limitations of $\ourmethod$ such as lack of public data error estimates. What approaches could address these? How impactful are those limitations in practice?

9. Under what conditions can pre-training on public data outperform joint selection? When is the reverse true? What causes the difference?

10. How does the performance of $\ourmethod$ change across different amounts of private data, sizes of workload queries, and data dimensionality? What factors have the biggest influence?
