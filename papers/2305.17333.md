# [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is: How can we efficiently fine-tune large pre-trained language models on downstream tasks using only forward passes, without requiring memory-intensive backpropagation? The key hypothesis appears to be that by carefully adapting classical zeroth-order optimization methods like ZO-SGD to operate in an extremely memory-efficient fashion, it is possible to fine-tune enormous language models with hundreds of billions of parameters using only a small constant amount of extra memory beyond what is needed for inference. This is in sharp contrast to standard fine-tuning via backpropagation, which requires caching many intermediate activations and gradients and thus scales very poorly in terms of memory.The paper proposes a method called MeZO that adapts ZO-SGD to work in-place on language models with minimal memory overhead. It hypothesizes that despite classic analyses suggesting ZO methods will be catastrophically slow for high-dimensional problems, MeZO will successfully fine-tune large LMs due to their special structure resulting from pre-training. The paper aims to validate this hypothesis empirically across a range of models and tasks, and also provide some theoretical justification.In summary, the main question is whether very large LMs can be fine-tuned efficiently with only forward passes, and the key hypothesis is that a properly designed memory-efficient ZO method can succeed at this despite traditional wisdom suggesting it should fail. The paper empirically and theoretically investigates this hypothesis.


## What is the main contribution of this paper?

Based on the abstract, the main contributions of this paper appear to be:1. Proposing a memory-efficient zeroth-order optimizer (MeZO) that adapts the classical ZO-SGD algorithm to operate in-place, thereby fine-tuning large language models with the same memory footprint as inference. 2. Comprehensive experiments demonstrating that MeZO consistently outperforms zero-shot, in-context learning, and linear probing across a variety of model types, scales, and tasks. MeZO achieves comparable performance to standard fine-tuning while reducing memory consumption substantially (e.g. 12x for OPT-13B).3. Experiments showing MeZO is compatible with both full-parameter tuning and parameter-efficient methods like LoRA and prefix tuning.4. Demonstrating MeZO can optimize non-differentiable objectives like directly maximizing accuracy or F1 score.5. Providing theoretical analysis suggesting adequate pre-training ensures MeZO's convergence rate depends on the Hessian's condition number, not the number of parameters, contradicting classical ZO results.In summary, the main contribution appears to be proposing, implementing, and empirically evaluating a memory-efficient version of ZO-SGD that enables fine-tuning massive language models using just forward passes and negligible memory overhead beyond inference. Theoretical and experimental results suggest this approach is effective despite classical ZO limitations.
