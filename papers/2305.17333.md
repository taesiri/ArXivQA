# [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is: How can we efficiently fine-tune large pre-trained language models on downstream tasks using only forward passes, without requiring memory-intensive backpropagation? The key hypothesis appears to be that by carefully adapting classical zeroth-order optimization methods like ZO-SGD to operate in an extremely memory-efficient fashion, it is possible to fine-tune enormous language models with hundreds of billions of parameters using only a small constant amount of extra memory beyond what is needed for inference. This is in sharp contrast to standard fine-tuning via backpropagation, which requires caching many intermediate activations and gradients and thus scales very poorly in terms of memory.The paper proposes a method called MeZO that adapts ZO-SGD to work in-place on language models with minimal memory overhead. It hypothesizes that despite classic analyses suggesting ZO methods will be catastrophically slow for high-dimensional problems, MeZO will successfully fine-tune large LMs due to their special structure resulting from pre-training. The paper aims to validate this hypothesis empirically across a range of models and tasks, and also provide some theoretical justification.In summary, the main question is whether very large LMs can be fine-tuned efficiently with only forward passes, and the key hypothesis is that a properly designed memory-efficient ZO method can succeed at this despite traditional wisdom suggesting it should fail. The paper empirically and theoretically investigates this hypothesis.


## What is the main contribution of this paper?

Based on the abstract, the main contributions of this paper appear to be:1. Proposing a memory-efficient zeroth-order optimizer (MeZO) that adapts the classical ZO-SGD algorithm to operate in-place, thereby fine-tuning large language models with the same memory footprint as inference. 2. Comprehensive experiments demonstrating that MeZO consistently outperforms zero-shot, in-context learning, and linear probing across a variety of model types, scales, and tasks. MeZO achieves comparable performance to standard fine-tuning while reducing memory consumption substantially (e.g. 12x for OPT-13B).3. Experiments showing MeZO is compatible with both full-parameter tuning and parameter-efficient methods like LoRA and prefix tuning.4. Demonstrating MeZO can optimize non-differentiable objectives like directly maximizing accuracy or F1 score.5. Providing theoretical analysis suggesting adequate pre-training ensures MeZO's convergence rate depends on the Hessian's condition number, not the number of parameters, contradicting classical ZO results.In summary, the main contribution appears to be proposing, implementing, and empirically evaluating a memory-efficient version of ZO-SGD that enables fine-tuning massive language models using just forward passes and negligible memory overhead beyond inference. Theoretical and experimental results suggest this approach is effective despite classical ZO limitations.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to related work in the field:- The paper adapts a classical zeroth-order optimization method (ZO-SGD) to large language model fine-tuning, which is a novel application area. Previous work has focused more on using ZO methods for adversarial example generation or small neural networks. Applying ZO to optimize billions of parameters in LLMs is largely unexplored.- The memory-efficient in-place implementation of ZO-SGD is a key contribution, allowing it to be feasible for large models. This implementation insight is crucial for the method's viability.- The paper shows strong empirical results, demonstrating ZO-SGD can match or exceed standard backprop fine-tuning performance across various models and datasets. This is surprising given theoretical results suggesting ZO methods will be catastrophically slow.- The theoretical analysis provides insights into why ZO-SGD succeeds, relating it to low effective rank structure in the loss landscape. This contrasts with prior work deriving dimension-dependent lower bounds for ZO methods.- Compared to other approaches for gradient-free tuning of LLMs like black box tuning or discrete prompt search, ZO-SGD directly updates model parameters and does not require restrictive projections.- The compatibility with parameter-efficient methods like LoRA and prefix tuning is another notable finding, since classical analyses suggest ZO benefits from tuning fewer parameters.Overall, I'd say the paper makes excellent progress advancing ZO methods as a promising direction for memory-efficient LLM fine-tuning. The empirical results are strong and theory provides supporting insights. It opens up an interesting research direction compared to prior work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Exploring the applicability of their memory-efficient ZO optimizer (MeZO) to other areas like pruning, distillation, saliency, interpretability, and dataset selection for fine-tuning. They highlight non-differentiable objectives and tuning models based on human feedback as particularly promising applications.- Conducting more theoretical analysis to understand how the efficient gradient estimates from MeZO impact performance in different applications like pruning, distillation, etc. - Exploring combinations of MeZO with other memory-efficient methods like gradient checkpointing, FlashAttention, and quantization training.- Studying if the proposed variance-reduced MeZO algorithm (VR-MeZO) can help tune language models on more complex objectives, even though it does not improve efficiency in the base setting.- Developing more advanced MeZO algorithms, perhaps inspired by the novel preconditioned algorithm briefly mentioned.- Empirically verifying the assumption on low effective Hessian rank made in the theory when fine-tuning large language models.- Extending the theory to cover the practical setting of using Gaussian perturbation vectors instead of sampling from a sphere.In summary, the main suggestions are to explore applications of MeZO beyond standard fine-tuning, combine MeZO with other efficient methods, develop more advanced theoretical understanding and algorithms, and provide more empirical evidence to support the assumptions made in analyzing MeZO.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a memory-efficient zeroth-order optimizer (MeZO) for fine-tuning large language models that requires only the memory needed for inference. MeZO adapts the classical ZO-SGD algorithm to operate in-place on arbitrarily large models with almost no memory overhead. Comprehensive experiments are conducted on masked language models like RoBERTa-large and autoregressive models like OPT up to 66 billion parameters on classification, multiple choice, and generation tasks. MeZO consistently outperforms zero-shot learning, in-context learning, and linear probing across tasks and scales. It achieves performance comparable to fine-tuning for certain tasks while using substantially less memory (e.g. 12x less for OPT-13B). MeZO works well with both full parameter tuning and parameter-efficient methods like LoRA and prefix tuning. Additional experiments optimize non-differentiable objectives like accuracy and F1 score directly. Theoretical analysis provides insight into why MeZO succeeds despite classical ZO results suggesting optimization should slow catastrophically with model size. Overall, the paper demonstrates that MeZO is an effective memory-efficient method for fine-tuning large language models.
