# [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is: How can we efficiently fine-tune large pre-trained language models on downstream tasks using only forward passes, without requiring memory-intensive backpropagation? The key hypothesis appears to be that by carefully adapting classical zeroth-order optimization methods like ZO-SGD to operate in an extremely memory-efficient fashion, it is possible to fine-tune enormous language models with hundreds of billions of parameters using only a small constant amount of extra memory beyond what is needed for inference. This is in sharp contrast to standard fine-tuning via backpropagation, which requires caching many intermediate activations and gradients and thus scales very poorly in terms of memory.The paper proposes a method called MeZO that adapts ZO-SGD to work in-place on language models with minimal memory overhead. It hypothesizes that despite classic analyses suggesting ZO methods will be catastrophically slow for high-dimensional problems, MeZO will successfully fine-tune large LMs due to their special structure resulting from pre-training. The paper aims to validate this hypothesis empirically across a range of models and tasks, and also provide some theoretical justification.In summary, the main question is whether very large LMs can be fine-tuned efficiently with only forward passes, and the key hypothesis is that a properly designed memory-efficient ZO method can succeed at this despite traditional wisdom suggesting it should fail. The paper empirically and theoretically investigates this hypothesis.
