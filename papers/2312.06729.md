# [RGNet: A Unified Retrieval and Grounding Network for Long Videos](https://arxiv.org/abs/2312.06729)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper addresses the problem of long video temporal grounding (LVTG). LVTG involves locating precise moments described by natural language queries from long, untrimmed videos (several hours or longer). This is challenging as the target moments constitute only a tiny fraction of the full video. Prior methods follow a two-stage approach - first generating proposal clips likely to contain the moment, and then applying a grounding model to locate the precise boundaries. However, the proposal selection stage is disjoint and lacks shared optimization with the grounding model, limiting overall performance.

Proposed Solution:
The paper proposes a unified network called RGNet that jointly handles proposal selection and grounding in an end-to-end manner. The core of RGNet is the RG-Encoder which extracts cross-modal features between proposals and text query for both retrieval and grounding. It employs a differentiable sampler to select relevant frames based on moment supervision. The encoder also uses a learnable token to summarize the proposal context and contrast positive and negative proposal-text pairs. The top-ranked proposals are passed to a grounding decoder to locate precise boundaries. Intra-proposal sampling and inter-proposal contrastive losses enable end-to-end optimization.

Main Contributions:
- Formulates proposal selection as a video-text retrieval task and designs a unified cross-modal RG-Encoder to share features between both stages.
- Incorporates a trainable sparse sampler to focus only on relevant frames based on ground-truth moments.  
- Employs losses to enable end-to-end training - intra-proposal sampling loss to optimize sampler, and inter-proposal contrastive loss to align proposals with query.
- Achieves new state-of-the-art performance on MAD and Ego4D datasets, demonstrating the effectiveness of the unified modeling approach.

In summary, the key idea is to unify the typically disjoint proposal selection and grounding stages via shared cross-modal encodings and end-to-end optimization for improved long video temporal grounding.
