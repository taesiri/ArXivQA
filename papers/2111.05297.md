# [Sliced Recursive Transformer](https://arxiv.org/abs/2111.05297)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve the parameter utilization and efficiency of vision transformers without increasing model size or computational cost? The authors are motivated by trying to enhance the representation ability and accuracy of vision transformers like ViT while keeping the model compact. They propose using a recursive operation within the transformer blocks to repeatedly refine and compress the feature representations. The key hypothesis appears to be that sharing weights recursively can help extract stronger features and improve accuracy without increasing parameters. The paper introduces "sliced recursion" - approximating the self-attention via multiple sliced group attentions - as a way to reduce the extra computation caused by recursion.In summary, the central research question is how to design a parameter-efficient vision transformer using recursive weight sharing. The hypothesis is that recursion and sliced self-attention can improve accuracy and efficiency without model size growth. The authors aim to develop compact yet accurate vision transformers.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes a recursive operation on vision transformers that can improve parameter utilization without increasing the number of parameters. This is achieved by sharing weights across layers in the transformer network. - It introduces an approximating method through multiple sliced group self-attentions across recursive layers to reduce the computational overhead caused by recursion, while maintaining accuracy. This method can reduce FLOPs by 10-30% without compromising performance.- It presents a new vision transformer model called Sliced Recursive Transformer (SReT) that integrates the proposed sliced recursive operation. SReT establishes significant improvement over state-of-the-art methods on ImageNet while containing fewer parameters and FLOPs.- It demonstrates the generalization ability of the proposed techniques by applying them to transformer architectures beyond vision, including an all-MLP transformer variant and neural machine translation models. Improvements are shown across domains.- It provides design principles and extensive ablation studies on factors like the recursive operation, group self-attention, non-linear projection layers, and learnable residual connections. This offers guidelines for future research.- It shows the proposed weight sharing mechanism enables building transformers with over 100 or 1000 layers easily while keeping the model compact, avoiding optimization difficulties with extremely large models.In summary, the key contribution is introducing recursive operation with sliced group self-attention into vision transformers in a parameter-efficient way, leading to improved accuracy and compact model size. The methods are broadly applicable across modalities and transformer architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a sliced recursive transformer architecture that improves parameter efficiency and representation power in vision transformers through weight sharing across depth by approximating the full self-attention with multiple sliced group self-attentions.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper on Sliced Recursive Transformer (SReT) to other research on transformers for computer vision:- The main goal is improving parameter efficiency and utilization in vision transformers without increasing model size. Many other works like Bottleneck Transformers, Tokens-to-Token Transformers, etc. also aim to improve efficiency but do so by modifying the transformer structure more heavily. SReT takes a simpler approach through weight sharing.- The core ideas are recursive operation and sliced group self-attention. Recursive operation shares weights across transformer depth to reuse representations. Sliced self-attention reduces computational cost of recursion. These ideas are relatively new for vision transformers compared to other techniques explored.- SReT does not rely on extra training data or input information like some other methods. It is also compatible with many existing vision transformer designs. This makes it a more flexible and simple approach to integrate.- Experiments show SReT achieves significantly better accuracy and efficiency than DeiT and other recent vision transformers. The improvements are demonstrated systematically over a variety of model sizes and datasets.- Analysis provides insights into how recursion and weight sharing help transform optimization and feature learning. Visualizations of learned features show SReT representations are more hierarchical compared to baseline models.Overall, SReT introduces techniques like recursion and sliced attention that are less explored for vision transformers. The paper shows these simple ideas can be highly effective for improving vision transformers. The approach is modular and compatible with many other transformer innovations. The thorough experiments and analysis provide convincing evidence for the benefits of SReT.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Further exploration of recursive operation in vision transformers: The authors propose using recursive operation in vision transformers as a way to improve parameter efficiency. However, they note that this approach has not yet been extensively studied for vision transformers. They suggest further research could be done to better understand the optimal ways to incorporate recursion into vision transformer architectures.- Scaling up models using weight sharing mechanisms: The authors show that their sliced recursive approach allows scaling up transformers to over 100 layers while keeping model size compact. They suggest this opens up possibilities for exploring extremely deep vision transformers using weight sharing techniques.- Applying recursive operation to other modalities/tasks: The authors demonstrate promising results applying their sliced recursive approach not just to image classification but also to neural machine translation. They suggest exploring the generalization of this technique to other modalities beyond vision/language and other tasks beyond classification/translation. - Theoretical analysis of benefits of recursion: The authors provide some initial theoretical analysis of how recursion may aid optimization and accumulation of gradients. But they note formal theoretical analysis of the benefits of recursion is still an open challenge for future work.- Efficient approximation of self-attention: The authors propose approximating global self-attention via grouped self-attention to reduce computational costs of recursion. They suggest further exploring efficient approximations of self-attention to enable recursion with lower overhead.- Optimal designs for residual connections: The authors observe interesting patterns in how their learned residual connection coefficients evolve during training. They suggest these observations could inspire future work into optimal designs for residual connections in vision transformers.So in summary, the main future directions relate to further exploration of recursion for transformers across modalities, tasks, and model scales, along with theoretical analysis and efficient approximation techniques to enable the benefits of recursion.
