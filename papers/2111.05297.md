# [Sliced Recursive Transformer](https://arxiv.org/abs/2111.05297)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve the parameter utilization and efficiency of vision transformers without increasing model size or computational cost? The authors are motivated by trying to enhance the representation ability and accuracy of vision transformers like ViT while keeping the model compact. They propose using a recursive operation within the transformer blocks to repeatedly refine and compress the feature representations. The key hypothesis appears to be that sharing weights recursively can help extract stronger features and improve accuracy without increasing parameters. The paper introduces "sliced recursion" - approximating the self-attention via multiple sliced group attentions - as a way to reduce the extra computation caused by recursion.In summary, the central research question is how to design a parameter-efficient vision transformer using recursive weight sharing. The hypothesis is that recursion and sliced self-attention can improve accuracy and efficiency without model size growth. The authors aim to develop compact yet accurate vision transformers.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes a recursive operation on vision transformers that can improve parameter utilization without increasing the number of parameters. This is achieved by sharing weights across layers in the transformer network. - It introduces an approximating method through multiple sliced group self-attentions across recursive layers to reduce the computational overhead caused by recursion, while maintaining accuracy. This method can reduce FLOPs by 10-30% without compromising performance.- It presents a new vision transformer model called Sliced Recursive Transformer (SReT) that integrates the proposed sliced recursive operation. SReT establishes significant improvement over state-of-the-art methods on ImageNet while containing fewer parameters and FLOPs.- It demonstrates the generalization ability of the proposed techniques by applying them to transformer architectures beyond vision, including an all-MLP transformer variant and neural machine translation models. Improvements are shown across domains.- It provides design principles and extensive ablation studies on factors like the recursive operation, group self-attention, non-linear projection layers, and learnable residual connections. This offers guidelines for future research.- It shows the proposed weight sharing mechanism enables building transformers with over 100 or 1000 layers easily while keeping the model compact, avoiding optimization difficulties with extremely large models.In summary, the key contribution is introducing recursive operation with sliced group self-attention into vision transformers in a parameter-efficient way, leading to improved accuracy and compact model size. The methods are broadly applicable across modalities and transformer architectures.
