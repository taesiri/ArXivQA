# [Contextual Code Switching for Machine Translation using Language Models](https://arxiv.org/abs/2312.13179)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Code-switching (mixing multiple languages in conversation) poses challenges for language models in machine translation tasks.  
- Large multilingual models struggle with code-switching translation due to limited mixed-language data and complexity of fine-tuning.  
- The paper focuses on Hinglish (Hindi-English) to English translation.

Proposed Solution
- Compare small and large language models for Hinglish-English translation using custom datasets.
- Smaller models easier to train end-to-end on limited data.  
- Employ LoRA method to fine-tune large models on small dataset.
- Add noise to training data to handle informal spelling variations in Hinglish.
- Evaluate models using BLEU automated metric and manual inspection.

Key Results
- Smaller Flan-T5 model (77M parameters) outperformed larger models when fully trained on dataset.  
- Multilingual models struggled with limited Hinglish data compared to dedicated training.
- Best BLEU score of 65.81 achieved on test dataset with Flan-T5 small model.
- Qualitative inspection also showed Flan-T5 small model provided most accurate translations.

Main Contributions  
- First study comparing small vs large models on Hinglish-English translation.
- Demonstrated smaller tailored models can outperform larger multilingual models.
- Created new noisy Hinglish dataset for model training.
- Quantitative and qualitative evaluation of model performance.

In summary, the paper shows smaller dedicated models can achieve state-of-the-art performance on code-switched translation tasks, outperforming larger multilingual models. The results highlight the need for specialized model training using limited available mixed-language datasets.
