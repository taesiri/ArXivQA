# [Towards Robots That Know When They Need Help: Affordance-Based   Uncertainty for Large Language Model Planners](https://arxiv.org/abs/2403.13198)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) show promise for robot planning but suffer from hallucinations where they make confident but incorrect predictions. This is problematic for robot assistants that interact with humans.
- Prior work in using LLMs for robot planning does not properly handle when robots should ask humans for help on ambiguous or uncertain instructions. Robots should minimize unnecessary requests for human assistance. 

Proposed Solution - LAP:  
- Introduces affordance scores that measure if actions are possible/safe given the environment context. Affordance scores help ground LLM predictions and mitigate hallucinations. 
- Calculates affordance scores in 3 ways: from scene context, from perceptual algorithms, or from LLM prompts that check if actions are possible and safe. Can combine into one score.
- Multiplies LLM next-token probabilities with affordance scores to get calibrated confidence scores for each candidate action. Uses these to construct a prediction set of options over a threshold. Asks human for help if prediction set contains multiple options.

Key Contributions:
- Significantly increases success rate and decreases human help requests compared to prior method KnowNo, reducing help requests by over 33% for same 70% success rate
- Explores different affordance scoring methods, including novel prompt-based approach that allows flexibility in scoring actions on safety etc.
- Shows using latest LLMs out-of-box with LAP outperforms fine-tuning older LLMs, with performance improving as LLM size/capability increases
- Evaluates in simulation and real-world experiments with mobile robot and demonstrates improved uncertainty alignment across range of environments/tasks


## Summarize the paper in one sentence.

 This paper presents LAP, a novel approach that uses large language models and affordances to better align model confidence with task success for robotic planners by calculating and leveraging a scene affordance score to mitigate hallucinations in LLM predictions.


## What is the main contribution of this paper?

 The main contributions of this paper include:

1. Introducing LAP, a novel approach for uncertainty alignment that uses Large Language Models and affordances to better align model confidence with task success for high-level robotic planners. 

2. Evaluating the approach in both simulation and on hardware in the real world using a suite of language-instructed manipulation tasks. Showing that LAP significantly increases the success rate and reduces the amount of help needed compared to prior art across different environments and LLMs.

3. Exploring and evaluating different ways of extracting an affordance value for use in LAP, including a novel prompt-based method that allows scoring actions on safety and other criteria. 

4. Showing that using LAP on the newest, most powerful language models out-of-the-box outperforms fine-tuning LAP and prior art on the best models available for fine-tuning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Large language models (LLMs): The paper focuses on using large pre-trained language models like GPT-3/GPT-4 for robotic planning tasks.

- Hallucinations: A key challenge with LLMs that the paper tries to address is their tendency to hallucinate or generate confident but incorrect predictions. 

- Affordances: The concept of affordances, referring to the set of possible actions in an environment, is core to the paper's proposed approach. Affordance scores are used to ground the LLM's predictions.

- Uncertainty alignment: Aligning the LLM's confidence estimates with actual success probability is framed as an uncertainty alignment problem. The goal is to minimize harmful hallucinations and enable asking for help when needed.

- Simulation experiments: Experiments are done both in simulation and real-world settings with a mobile robot manipulator.

- Prediction sets: Multiple choice questions are used to generate prediction sets over possible next actions. Affordance scores help refine these sets.

- Prompt-based scoring: One affordance scoring method involves prompting the LLM about whether a possible action is safe/grounded.

- Success rate vs help rate: Key evaluation metrics look at task success rate tradeoffs with amount of human help needed.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces three different methods for calculating the affordance score - context-based, perception-based, and prompt-based. How do these methods complement each other when used together? What are the limitations of using each one independently?

2. The prompt-based affordance scoring allows incorporating additional criteria like safety into the score. What other criteria could be included in the prompt to make the affordance score more flexible and customizable? How would that change the performance? 

3. Fine-tuning older LLMs like GPT-3.5 Turbo led to overfitting and poorer generalization compared to using GPT-4 out-of-the-box. What tweaks could be made to the fine-tuning process to prevent overfitting and improve generalization capability?

4. The paper shows that combining multiple affordance scores works better than using them independently. What machine learning techniques could be used on top of these scores to learn an optimal combination automatically from data?

5. The method relies on ground truth perception information being provided to the LLM. How can the uncertainty and errors from real-world perception systems be accounted for in the affordance scoring?

6. Beyond safety and possibility, what other criteria need to be considered for home/consumer robots to ensure their actions align well with user preferences and expectations? How can those factors be quantified?

7. How sensitive is the performance of LAP to the choice and quality of the few-shot examples provided? What guidelines should be followed for picking effective few-shot examples? 

8. The paper focuses on object manipulation tasks. How should the affordance scoring be adapted for other types of robotic tasks like navigation or human-robot interaction?

9. The affordance scoring helps mitigate LLM hallucinations about objects in the given scene/context. What other complementary techniques can reduce other types of factual hallucinations?

10. The paper shows the latest LLMs outperforming fine-tuned older models. Do you think this trend will continue with future generations of LLMs? What challenges need to be solved to make fine-tuning more effective?
