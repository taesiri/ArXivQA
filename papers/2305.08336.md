# [Inverse Rendering of Translucent Objects using Physical and Neural   Renderers](https://arxiv.org/abs/2305.08336)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes an inverse rendering framework for estimating the 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination of translucent objects from a pair of flash and no-flash images. 

The key research questions and hypotheses are:

- Can we jointly estimate shape, reflectance, subsurface scattering, and illumination for translucent objects from limited input images? Previous works have tackled these properties separately or made simplifying assumptions. This paper hypothesizes that jointly estimating all factors is possible with suitable input images and model design.

- How to handle the inherent ambiguity in inverse rendering? The appearance of translucent objects depends on complex interactions of shape, reflectance, scattering, and illumination. The paper hypothesizes that using complementary flash and no-flash images along with physically-based and learned rendering can help resolve ambiguity.

- How to obtain training data and supervise a model to estimate subsurface scattering parameters? The paper hypothesizes that differentiable rendering with an augmented loss computed on edited subsurface scattering parameters can provide effective supervision, while a large-scale synthetic dataset can enable training.

In summary, the central hypothesis is that the proposed inverse rendering framework with appropriate inputs, differentiable rendering, and training data can effectively decompose translucent object appearance into shape, reflectance, subsurface scattering, and illumination components. The research aims to tackle a challenging inverse problem with practical applications.
