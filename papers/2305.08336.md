# [Inverse Rendering of Translucent Objects using Physical and Neural   Renderers](https://arxiv.org/abs/2305.08336)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes an inverse rendering framework for estimating the 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination of translucent objects from a pair of flash and no-flash images. 

The key research questions and hypotheses are:

- Can we jointly estimate shape, reflectance, subsurface scattering, and illumination for translucent objects from limited input images? Previous works have tackled these properties separately or made simplifying assumptions. This paper hypothesizes that jointly estimating all factors is possible with suitable input images and model design.

- How to handle the inherent ambiguity in inverse rendering? The appearance of translucent objects depends on complex interactions of shape, reflectance, scattering, and illumination. The paper hypothesizes that using complementary flash and no-flash images along with physically-based and learned rendering can help resolve ambiguity.

- How to obtain training data and supervise a model to estimate subsurface scattering parameters? The paper hypothesizes that differentiable rendering with an augmented loss computed on edited subsurface scattering parameters can provide effective supervision, while a large-scale synthetic dataset can enable training.

In summary, the central hypothesis is that the proposed inverse rendering framework with appropriate inputs, differentiable rendering, and training data can effectively decompose translucent object appearance into shape, reflectance, subsurface scattering, and illumination components. The research aims to tackle a challenging inverse problem with practical applications.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. They propose an inverse rendering model to jointly estimate 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination from only a pair of flash and no-flash images of a translucent object. 

2. They develop a novel model that combines a physically-based renderer and a neural renderer to explicitly separate surface reflectance and subsurface scattering (SSS). The physically-based renderer handles direct illumination and surface reflectance while the neural renderer creates the SSS effect.

3. They introduce an augmented loss using multiple edited images with altered SSS parameters to enhance the supervision of the neural renderer. This helps the neural renderer learn the relationship between SSS parameters and rendered images.

4. They construct a large-scale photorealistic synthetic dataset consisting of over 117K scenes of translucent objects with ground truth shape, reflectance, SSS and illumination.

5. They demonstrate simultaneous shape, reflectance, SSS and illumination estimation on both synthetic and real datasets. They also show an application of SSS parameter editing.

In summary, the key innovation is the combined physically-based and neural renderer model along with the augmented loss to tackle the highly ill-posed inverse rendering problem for translucent objects. The large-scale synthetic dataset is also a contribution for this challenging task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes an inverse rendering framework to jointly estimate 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination from a pair of flash and no-flash images of a translucent object, using a physically-based renderer and a neural renderer to address the ambiguity problem.


## How does this paper compare to other research in the same field?

 This paper makes several novel contributions to the field of inverse rendering of translucent objects:

1. It is the first work to jointly estimate 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination from only a pair of flash and no-flash images. 

Prior works have typically made simplifying assumptions like ignoring subsurface scattering entirely and assuming Lambertian materials. Other recent works on translucent inverse rendering have focused only on estimating subsurface scattering parameters, not the full set of intrinsic scene properties.

2. The proposed two-renderer architecture combining a physical and a neural renderer is innovative. Using the physical renderer for direct illumination and the neural renderer for subsurface scattering effects allows rendering high-quality results while remaining efficient.

Other inverse rendering methods typically use only a physical or a neural renderer. The two-renderer design here provides complementary strengths.

3. The augmented loss and editing of subsurface parameters provides extra supervision for the neural renderer. This helps address the issue of "information hiding" in neural networks and improves estimation of scattering parameters.

Augmented losses have not been explored much in inverse rendering. The editing of subsurface parameters to generate extra training data is also novel.

4. The paper contributes a large-scale dataset of 117K synthetic translucent object scenes with ground truth shape, reflectance, scattering, and illumination. 

Previous translucent inverse rendering works have lacked sufficient training data. This dataset will enable further research.

Overall, this paper pushes the state-of-the-art in translucent inverse rendering significantly forward. The joint estimation of shape, reflectance, and scattering from limited input is an important step towards practical inverse rendering of real-world translucent objects. The novel two-renderer design and augmented loss method could also inspire new techniques for tackling ambiguities in inverse problems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Extending the method to handle translucent objects with a wider range of refractive indices (IoR). The current method assumes a constant IoR, but varying IoR affects light transmission at object boundaries and subsurface scattering. Developing techniques to estimate spatially-varying IoR could expand the applicability of the method.

- Enabling novel view synthesis and relighting applications. The current method focuses on inverse rendering from a single viewpoint. Estimating a complete 3D shape representation rather than just depth/normals could allow relighting and view synthesis. This is challenging for translucent objects though. 

-Considering more complex non-homogeneous subsurface scattering. The current method models homogeneous subsurface scattering parameters. Modeling spatially-varying scattering could better represent many real translucent materials.

- Improving the surface estimation, especially for very transparent objects where the background affects the appearance significantly. Developing techniques to disentangle background influence could improve surface estimation.

- Extending the framework to model and estimate additional effects like interreflections, occlusion, and dynamic lighting. This could expand the applicability to more complex real-world scenes.

- Applying the translucent material acquisition framework to graphics/vision applications like material editing, segmentation, recognition, etc.

- Investigating unsupervised or self-supervised training methods that rely less on large labeled datasets. 

In summary, the main future directions are improving the material representation, enhancing the shape estimation, expanding to more complex scenes, and developing practical applications. Advancing any of these areas could build on the capabilities of the current method for inverse rendering of translucent objects.
