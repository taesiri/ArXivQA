# [Inverse Rendering of Translucent Objects using Physical and Neural   Renderers](https://arxiv.org/abs/2305.08336)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes an inverse rendering framework for estimating the 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination of translucent objects from a pair of flash and no-flash images. 

The key research questions and hypotheses are:

- Can we jointly estimate shape, reflectance, subsurface scattering, and illumination for translucent objects from limited input images? Previous works have tackled these properties separately or made simplifying assumptions. This paper hypothesizes that jointly estimating all factors is possible with suitable input images and model design.

- How to handle the inherent ambiguity in inverse rendering? The appearance of translucent objects depends on complex interactions of shape, reflectance, scattering, and illumination. The paper hypothesizes that using complementary flash and no-flash images along with physically-based and learned rendering can help resolve ambiguity.

- How to obtain training data and supervise a model to estimate subsurface scattering parameters? The paper hypothesizes that differentiable rendering with an augmented loss computed on edited subsurface scattering parameters can provide effective supervision, while a large-scale synthetic dataset can enable training.

In summary, the central hypothesis is that the proposed inverse rendering framework with appropriate inputs, differentiable rendering, and training data can effectively decompose translucent object appearance into shape, reflectance, subsurface scattering, and illumination components. The research aims to tackle a challenging inverse problem with practical applications.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. They propose an inverse rendering model to jointly estimate 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and environment illumination from only a pair of flash and no-flash images of a translucent object. 

2. They develop a novel model that combines a physically-based renderer and a neural renderer to explicitly separate surface reflectance and subsurface scattering (SSS). The physically-based renderer handles direct illumination and surface reflectance while the neural renderer creates the SSS effect.

3. They introduce an augmented loss using multiple edited images with altered SSS parameters to enhance the supervision of the neural renderer. This helps the neural renderer learn the relationship between SSS parameters and rendered images.

4. They construct a large-scale photorealistic synthetic dataset consisting of over 117K scenes of translucent objects with ground truth shape, reflectance, SSS and illumination.

5. They demonstrate simultaneous shape, reflectance, SSS and illumination estimation on both synthetic and real datasets. They also show an application of SSS parameter editing.

In summary, the key innovation is the combined physically-based and neural renderer model along with the augmented loss to tackle the highly ill-posed inverse rendering problem for translucent objects. The large-scale synthetic dataset is also a contribution for this challenging task.
