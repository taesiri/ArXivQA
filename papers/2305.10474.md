# [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](https://arxiv.org/abs/2305.10474)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes a new method to train high-quality text-to-video diffusion models by finetuning a pretrained text-to-image diffusion model. - The key hypothesis is that naively extending the image diffusion noise prior (i.i.d. Gaussian noise) to video by adding a temporal dimension is not ideal. The noise maps between video frames are correlated.- To address this, the paper proposes two new video diffusion noise priors - mixed noise and progressive noise - that are designed to better capture the correlations between video frames.- The progressive noise prior in particular is shown to work well when finetuning a text-to-image diffusion model for video synthesis. It leads to improved video quality and consistency.- The main experiments show state-of-the-art results on text-to-video synthesis benchmarks like UCF-101 and MSR-VTT by finetuning the E-Diff-I image model with the proposed progressive noise prior.So in summary, the central hypothesis is that using a temporally correlated noise prior is better suited for finetuning an image diffusion model for video synthesis compared to naively using i.i.d. noise. The paper provides strong empirical evidence to validate this hypothesis through careful experiments and analysis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new video diffusion noise prior that is tailored for finetuning pretrained text-to-image diffusion models for video generation. The key idea is to model correlations between noise maps of different frames from the same video. This is done via a mixed noise model or a progressive noise model.2. Performing comprehensive experiments that validate the effectiveness of the proposed video diffusion noise prior. Experiments on the UCF-101 dataset show that finetuning an image diffusion model with the proposed prior outperforms training from scratch and finetuning with independent frame noise.3. Developing a large-scale text-to-video diffusion model by finetuning a pretrained EDiff-I model with the proposed noise prior. This model achieves state-of-the-art results on the UCF-101 and MSR-VTT datasets for zero-shot text-to-video generation.In summary, the main contribution is proposing a better noise prior for finetuning image diffusion models on video data. This enables building large-scale text-to-video diffusion models efficiently by reusing image models. The proposed approach is comprehensively analyzed and achieves state-of-the-art video generation quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new video diffusion noise prior tailored for finetuning pretrained image diffusion models on video data that better captures temporal correlations, leading to improved video generation quality.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of text-to-video generation:- This paper builds off recent work exploring diffusion models for text-to-video generation, including Video Diffusion Model (VDM) and Imagen Video. The key novelty is proposing a new video noise prior tailored for finetuning an image diffusion model for video generation. - Most prior work trains video diffusion models from scratch with only video data. This paper explores finetuning an image diffusion model. Some other recent works like Make-A-Video and Video LDM have also explored finetuning image models, but use simpler/naive video noise priors.- Compared to training from scratch approaches like VDM and Imagen Video, this paper is much more computationally efficient and scalable by leveraging an existing powerful image model. The total training time and compute needed is an order of magnitude lower.- The proposed model, PYoCo, achieves state-of-the-art results on the UCF-101 dataset, outperforming Make-A-Video, Video LDM, and other concurrent works in metrics like IS, FVD, and FID.- On the larger scale MSR-VTT dataset, PYoCo also achieves a new state-of-the-art in the CLIP-FID metric compared to Make-A-Video and other methods.- Qualitatively, the paper shows PYoCo can generate high fidelity videos well matched to text descriptions, with improved quality over Make-A-Video and Imagen Video.- A limitation is the evaluation is still constrained to relatively small academic datasets. Performance on a diverse large-scale video dataset is still an open challenge.Overall, by efficiently leveraging an image diffusion model and proposing a better suited video noise prior, this paper pushes state-of-the-art in zero-shot text-to-video generation, while being much more scalable than training from scratch approaches. The simple but effective idea of finetuning image models for video shows promising results.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring different design choices for the video diffusion noise prior. The authors propose two approaches (mixed noise and progressive noise), but note there may be other effective ways to model noise correlations across frames. More work could be done to design video-specific noise priors.- Scaling up to higher-resolution videos. The authors demonstrate results on 256x256 and 1024x1024 videos, but note that scaling further to resolutions like 2K and 4K remains an open challenge. Architectural improvements and larger models may help.- Leveraging more unlabeled video data. The authors use a combination of labeled and unlabeled image data, but only labeled video data. Making use of larger unlabeled video datasets could further improve results.- Video generation conditioned on other modalities. The current work focuses on text-to-video generation. An interesting direction is exploring generation conditioned on other inputs like audio, sketches, etc.- Controllable video generation. Allowing more fine-grained control over factors like motion, style, viewpoint etc. during generation remains an open problem.- Evaluation metrics for video. The authors note issues with current evaluation metrics like IS and FVD. Developing better quantitative metrics aligned with human judgments is an important direction.- Applications like video editing, interpolation etc. The authors demonstrate video synthesis, but the model could potentially be adapted for editing and other applications as well.So in summary, some of the key future directions are improving the video-specific components like the noise prior and scaling to higher resolutions, using more unlabeled video data, exploring conditional generation, improving control and evaluation, and applying the approach to additional video tasks beyond synthesis.
