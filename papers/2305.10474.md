# [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](https://arxiv.org/abs/2305.10474)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes a new method to train high-quality text-to-video diffusion models by finetuning a pretrained text-to-image diffusion model. - The key hypothesis is that naively extending the image diffusion noise prior (i.i.d. Gaussian noise) to video by adding a temporal dimension is not ideal. The noise maps between video frames are correlated.- To address this, the paper proposes two new video diffusion noise priors - mixed noise and progressive noise - that are designed to better capture the correlations between video frames.- The progressive noise prior in particular is shown to work well when finetuning a text-to-image diffusion model for video synthesis. It leads to improved video quality and consistency.- The main experiments show state-of-the-art results on text-to-video synthesis benchmarks like UCF-101 and MSR-VTT by finetuning the E-Diff-I image model with the proposed progressive noise prior.So in summary, the central hypothesis is that using a temporally correlated noise prior is better suited for finetuning an image diffusion model for video synthesis compared to naively using i.i.d. noise. The paper provides strong empirical evidence to validate this hypothesis through careful experiments and analysis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new video diffusion noise prior that is tailored for finetuning pretrained text-to-image diffusion models for video generation. The key idea is to model correlations between noise maps of different frames from the same video. This is done via a mixed noise model or a progressive noise model.2. Performing comprehensive experiments that validate the effectiveness of the proposed video diffusion noise prior. Experiments on the UCF-101 dataset show that finetuning an image diffusion model with the proposed prior outperforms training from scratch and finetuning with independent frame noise.3. Developing a large-scale text-to-video diffusion model by finetuning a pretrained EDiff-I model with the proposed noise prior. This model achieves state-of-the-art results on the UCF-101 and MSR-VTT datasets for zero-shot text-to-video generation.In summary, the main contribution is proposing a better noise prior for finetuning image diffusion models on video data. This enables building large-scale text-to-video diffusion models efficiently by reusing image models. The proposed approach is comprehensively analyzed and achieves state-of-the-art video generation quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new video diffusion noise prior tailored for finetuning pretrained image diffusion models on video data that better captures temporal correlations, leading to improved video generation quality.
