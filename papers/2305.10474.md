# [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](https://arxiv.org/abs/2305.10474)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes a new method to train high-quality text-to-video diffusion models by finetuning a pretrained text-to-image diffusion model. - The key hypothesis is that naively extending the image diffusion noise prior (i.i.d. Gaussian noise) to video by adding a temporal dimension is not ideal. The noise maps between video frames are correlated.- To address this, the paper proposes two new video diffusion noise priors - mixed noise and progressive noise - that are designed to better capture the correlations between video frames.- The progressive noise prior in particular is shown to work well when finetuning a text-to-image diffusion model for video synthesis. It leads to improved video quality and consistency.- The main experiments show state-of-the-art results on text-to-video synthesis benchmarks like UCF-101 and MSR-VTT by finetuning the E-Diff-I image model with the proposed progressive noise prior.So in summary, the central hypothesis is that using a temporally correlated noise prior is better suited for finetuning an image diffusion model for video synthesis compared to naively using i.i.d. noise. The paper provides strong empirical evidence to validate this hypothesis through careful experiments and analysis.
