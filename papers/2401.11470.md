# [Exploring Missing Modality in Multimodal Egocentric Datasets](https://arxiv.org/abs/2401.11470)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Multimodal transformer (MMT) models suffer performance degradation when one of the modalities is missing during inference time. The paper focuses on making MMT models robust to missing modalities.

Proposed Solution: 
The paper proposes Modality Masking Training (MMT), where the model is trained on inputs with missing modalities simulated by replacing modality tokens with special MMT tokens. Two strategies are explored - fixed-replace, where a fixed percentage of training samples have a missing modality, and random-replace, where modalities are randomly dropped.

Contributions:

1. Analyzes the impact of the percentage of samples with missing modalities during training ($r_{train}$) and the modality drop probability ($p$) in the two strategies. Shows higher values result in better handling of missing modalities during inference, but too high values hurt overall performance.

2. Finds random-replace more effective than fixed-replace. Randomly seeing the same samples as both complete and incomplete helps the model learn to handle missing modalities.

3. Shows MMT makes models robust to missing modalities across different fusion layers, addressing limitations of prior work.

4. Evaluates on multiple egocentric datasets - Epic-Kitchens, Epic-Sounds, Ego4D-AR. MMT matches or exceeds unimodal performance even with 100% modality missing during inference.

5. Compares with existing method of using trainable prompts. Outperforms prompts and establishes a new SOTA for handling missing modalities.

In summary, the paper provides an extensive analysis of training MMT models to be robust to missing modalities using the proposed MMT strategies and makes them adaptable across fusion layers.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a modality masking training strategy to make multimodal transformers robust to missing modalities at test time, analyzing different proportions of masked inputs and random vs fixed masking.


## What is the main contribution of this paper?

 Based on the paper, the main contribution seems to be proposing two strategies called "fixed-replace" and "random-replace" for training multimodal transformer models to be robust to missing modalities at test time. Specifically:

- The "fixed-replace" strategy replaces a fixed proportion $r_{train}$ of the training samples with missing modalities, by removing either the audio or visual stream.

- The "random-replace" strategy randomly removes modalities from training samples with probability $p$. 

The paper analyzes these two strategies on multiple audio-visual egocentric datasets, showing that they can match or exceed the performance of unimodal models when one modality is missing at test time. The "random-replace" strategy in particular is shown to regularize training and achieve slightly better performance.

Additionally, the paper demonstrates that models trained with these strategies are more robust to the choice of fusion layer used, addressing an issue with prior multimodal transformer models.

In summary, the main contribution is proposing and evaluating two simple but effective strategies to train multimodal transformers to handle missing modalities during inference.


## What are the keywords or key terms associated with this paper?

 Based on the content of the paper, some of the key terms and keywords that I would associate with it include:

- Multimodal transformer (MMT)
- Missing modality/modalities
- Modality masking training (MMT)
- Fixed-replace strategy
- Random-replace strategy  
- Fusion layers
- Egocentric action recognition
- Epic-Kitchens dataset
- Epic-Sounds dataset
- Ego4D-AR dataset
- Training with complete data
- Training with missing data
- Modal-incomplete inputs
- Missing rate ($r_{train}$, $r_{test}$)  

The paper proposes modality masking training (MMT) strategies called fixed-replace and random-replace to make multimodal transformers robust to missing modalities during inference. It analyzes the impact of the proportion of modal-incomplete inputs and the fusion layers on the model performance. The experiments are conducted on egocentric action recognition datasets - Epic-Kitchens, Epic-Sounds, and Ego4D-AR. The key focus is handling missing modalities and training multimodal models that can maintain decent performance even with incomplete inputs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes two strategies for training the model with missing modalities: fixed-replace and random-replace. What are the key differences between these two strategies and what are the relative advantages/disadvantages of each? 

2. When using the fixed-replace strategy, the paper examines the impact of varying $r_{train}$, the proportion of modally incomplete inputs in the training set. What trends do they observe regarding the relationship between $r_{train}$ and performance at different levels of $r_{test}$? How does this relationship differ across the three datasets studied?

3. For the random-replace strategy, the impact of varying the modality drop probability $p$ is analyzed. How does the effect of $p$ compare to that of $r_{train}$ in the fixed-replace strategy? What explanations do the authors provide for the differences observed?

4. The paper finds that the random-replace strategy is more effective overall than the fixed-replace strategy. What evidence do they provide to support this conclusion? Are there any cases where fixed-replace outperforms random-replace?

5. When training with missing modalities already present in the training data (the training-with-missing scenario), both $r_{train}$ and $p$ are used. What trends can you observe regarding the interplay between these two variables and their joint impact on performance? 

6. Section 4.4 examines the impact of the fusion layer depth $L_f$ with and without modality masking training. What differences do they observe regarding sensitivity to $L_f$ between adapted and non-adapted models? How does MMT enhance robustness to missing modalities across different values of $L_f$?

7. The paper compares the proposed modality masking training strategies to a baseline method from Lee et al. 2023. What backbone model do the authors base their implementation on and why? How do both fixed-replace and random-replace compare to this baseline?

8. Across the three datasets, how does the performance of models trained with MMT at $r_{test}=100\%$ generally compare to the unimodal performance? Are there any cases where the unimodal performance is exceeded?

9. The authors use three different downstream datasets in their experiments - Epic-Kitchens, Epic-Sounds, and Ego4D-AR. What are the key differences between these three datasets in terms of annotations, modalities, and naturally missing data? 

10. The pre-training procedure uses a masked autoencoder objective. What hyperparameters are used during pre-training in terms of masking ratios, number of epochs, etc.? What motivates these choices? How important is pre-training to the overall success of modality masking training?
