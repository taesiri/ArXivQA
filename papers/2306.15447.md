# [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: Are aligned neural network models "adversarially aligned"?In other words, the paper investigates whether neural network models that have been trained or fine-tuned to be "helpful and harmless" (aligned) remain robustly aligned when presented with adversarial examples - inputs that are intentionally designed to cause the model to produce harmful or toxic outputs. The key hypotheses seem to be:1) Existing NLP attack methods may not be powerful enough to reliably evaluate the adversarial robustness of aligned language models. The paper provides evidence that current attacks often fail to find adversarial inputs even when it's possible to guarantee their existence.2) Aligned multimodal text-vision models can be more easily attacked by perturbing the input image. The paper shows these models can be induced to generate toxic or harmful outputs through small adversarial image perturbations. 3) The failure of current NLP attacks does not prove text-only models are adversarially aligned; the conjecture is that improved attacks may succeed in inducing harmful behaviors.So in summary, the central research question is evaluating whether aligned models remain robustly aligned under adversarial attacks, especially focused on limitations of current NLP attacks and vulnerabilities in multimodal models.
