# [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: Are aligned neural network models "adversarially aligned"?In other words, the paper investigates whether neural network models that have been trained or fine-tuned to be "helpful and harmless" (aligned) remain robustly aligned when presented with adversarial examples - inputs that are intentionally designed to cause the model to produce harmful or toxic outputs. The key hypotheses seem to be:1) Existing NLP attack methods may not be powerful enough to reliably evaluate the adversarial robustness of aligned language models. The paper provides evidence that current attacks often fail to find adversarial inputs even when it's possible to guarantee their existence.2) Aligned multimodal text-vision models can be more easily attacked by perturbing the input image. The paper shows these models can be induced to generate toxic or harmful outputs through small adversarial image perturbations. 3) The failure of current NLP attacks does not prove text-only models are adversarially aligned; the conjecture is that improved attacks may succeed in inducing harmful behaviors.So in summary, the central research question is evaluating whether aligned models remain robustly aligned under adversarial attacks, especially focused on limitations of current NLP attacks and vulnerabilities in multimodal models.


## What is the main contribution of this paper?

This paper studies the adversarial robustness of large language models (LLMs) that have been aligned to be helpful and harmless, especially in response to user interactions. The main contributions are:1. It shows that existing NLP optimization attacks often fail to generate adversarial examples causing aligned LLMs to emit harmful content, even when solutions are known to exist. This suggests current attacks are not powerful enough to properly evaluate adversarial robustness.2. It demonstrates successful adversarial attacks on multimodal (text + image) aligned LLMs by perturbing the image input. This indicates potential vulnerabilities exist even in aligned LLMs. 3. Based on the results, it conjectures that improved future NLP attacks may be able to induce harmful behaviors in text-only aligned LLMs as well. It calls for more research into stronger attacks to properly evaluate adversarial robustness.4. The results suggest current alignment techniques may not make models robustly aligned under adversarial conditions. More foundational work on attacks/defenses is needed to develop techniques that remain reliable against adversarial inputs.In summary, the key contribution is highlighting limitations in current methods for evaluating and ensuring adversarial robustness of aligned LLMs, especially against optimized malicious inputs. The results motivate further research into stronger attacks and defenses for robust alignment.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper studies whether aligned language models trained to be helpful and harmless via techniques like reinforcement learning from human feedback are still aligned when presented with adversarial examples, finding existing NLP attacks fail but image attacks easily trick multimodal models.
