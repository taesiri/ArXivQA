# [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/abs/2303.09752)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:How can we design a Transformer model architecture that enables fast processing of long document inputs by selectively applying more computation to the most important tokens?In particular, the paper proposes a model called \slic (Conditional LongT5) that aims to achieve the following:1. Reduce the quadratic computational cost of self-attention over long inputs, as in LongT5. 2. Further reduce the linear cost of applying feedforward and projection layers to every token, which becomes prohibitive for long inputs.3. Do the above while maintaining or improving model quality compared to LongT5.The key ideas are:- Employ conditional computation in both the attention and feedforward components, with a 'light' branch applied to all tokens and a 'heavy' branch applied only to important tokens.- Use learned routing modules to select the important tokens in each layer and component. - Add other optimizations like multi-query attention and a new pretraining objective.The paper hypothesizes that this approach will enable processing extremely long inputs both efficiently and effectively, by focusing computation on the small fraction of tokens likely to be most important. Experiments aim to validate whether \slic achieves these goals compared to LongT5.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing SLIC, a new family of models for efficiently processing long documents. SLIC builds on top of LongT5 and employs conditional computation to devote more resources to important tokens for both feedforward and attention layers. Specifically, the key ideas are:- SLIC divides each feedforward layer into a light branch applied to all tokens and a heavy branch applied to a subset of important tokens. The light branch has lower dimensionality while the heavy branch has higher dimensionality.- Similarly, SLIC divides each attention layer into a light branch with fewer heads and local attention, and a heavy branch with more heads and global attention over important tokens. - Important tokens for the heavy branches are selected separately for each layer and component using learned routing modules.- SLIC also incorporates multi-query attention for faster inference and is pre-trained on UL2 objective for improved in-context learning.The paper shows SLIC matches or exceeds LongT5 quality with much faster training and inference, achieving SOTA on the SCROLLS benchmark. SLIC also scales effectively to extremely long inputs up to 64k tokens. The main novelty is the conditional computation architecture to devote disproportionate computation to important tokens in both feedforward and attention.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes Slic, a new Transformer model that speeds up processing of long documents by selectively applying more computation to important tokens in both attention and feedforward layers.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on SLiC (Conditional LongT5) compares to other related work on efficient Transformers and processing long inputs:- Builds on LongT5 architecture but makes further improvements to reduce computation for both attention and feedforward components. LongT5 focused mainly on reducing self-attention cost for long inputs. - Employs conditional computation to devote more capacity to important tokens. This is a common technique in efficient Transformers, also used by models like Switch Transformers, Mixture-of-Experts, and Sparse Transformers.- Combines sparse attention with conditional feedforward/attention layers. Most prior work focuses on just one of those techniques. CoDA is another very recent model that combines sparse attention with conditional computation.- Shows strong performance on summarization and QA tasks with long inputs. Achieves state-of-the-art on the SCROLLS benchmark dataset designed for evaluating long document modeling.- Demonstrates that SLiC can make effective use of extremely long inputs (up to 64K tokens), with gains beyond other models like LongT5.- Incorporates modifications like multi-query attention and UL2 pre-training objective to further improve speed and capability.Overall, SLiC builds on a lot of recent advances in efficient Transformers and long document modeling, combining techniques like sparse attention, conditional computation, and optimized pre-training in a unified architecture. The paper shows these different components are complementary and together enable stronger and faster performance on tasks involving very long text.


## What future research directions do the authors suggest?

 The paper proposes the \slic model for efficient processing of long inputs using conditional computation. The authors suggest some potential future research directions:- Further study extremely long inputs beyond 64k tokens, as \slic has shown strong gains going from 16k to 64k tokens. The authors hypothesize that gains may continue for even longer inputs as the fraction of focus tokens needed diminishes.- Explore different conditional computation architectures and mechanisms for selecting "important" tokens, beyond the learned routing approach used in \slic. - Investigate techniques to reduce memory usage and enable processing inputs longer than model context length, which was a limitation encountered in the few-shot learning experiments.- Apply the conditional computation approach to other domains like images, video, and speech that also exhibit varying importance across different parts of the input.- Combine \slic with other optimizations like mixture-of-experts to further improve efficiency.- Study the learned routing behavior in more detail to better understand which tokens are considered important and how that evolves during training.- Evaluate the tradeoffs between routing a fixed fraction of tokens versus dynamically adjusting based on model confidence.- Extend the approach to encoder-decoder models and investigate conditional computation in the decoder as well.In summary, the main future direction is to continue pushing the envelope on long document modeling by studying model architectures, routing mechanisms, training techniques, and applications to other domains. There are still many open questions around effectively leveraging extremely long contexts.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes SLiC (Conditional LongT5), a new family of models for processing long text inputs. SLiC builds on LongT5 by employing conditional computation to devote more resources to important tokens in both the feedforward and attention layers. Specifically, SLiC divides each layer into a light branch applied to all tokens and a heavy branch applied only to selected important tokens. The heavy branch uses higher model capacity. SLiC selects important tokens using a learned routing mechanism. The paper shows that SLiC achieves stronger performance than LongT5 on summarization and question answering with much faster training and inference. SLiC also effectively handles extremely long inputs up to 64K tokens by routing a diminishing fraction of tokens. The paper demonstrates state-of-the-art results on the SCROLLS benchmark and shows the advantages of conditional computation for tractable modeling of long documents.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes \slic, a long-input Transformer model that employs conditional computation to reduce the cost of processing long documents. \slic builds on LongT5 by dividing each feedforward layer and attention layer into a light branch applied to all tokens and a heavy branch applied only to important tokens selected by a learned router. This allows \slic to devote more computation to salient parts of long inputs. The light feedforward branch has lower dimension than standard LongT5, while the heavy branch has higher dimension. Similarly, the light attention branch uses local attention with fewer heads, while the heavy branch performs full attention with more heads over separately selected query and key/value tokens. Experiments show that \slic performs much faster training and inference while achieving similar or better performance compared to LongT5. \slic matches LongT5 quality on the SCROLLS benchmark using 35-75% less training time and 50-100% less inference time. The paper also demonstrates \slic's ability to leverage extremely long inputs up to 64K tokens through tractable routing. Ablations verify the importance of conditional computation and learned routing. Analysis shows the router learns to select semantic units like questions, answers, and key phrases. Overall, \slic advances the state of the art in efficient modeling of long documents.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes \slic (Conditional LongT5), a new family of models for processing long text inputs. \slic employs conditional computation to devote more computation to important tokens. Each feedforward and attention layer is split into a light branch applied to all tokens and a heavy branch applied to a subset of tokens selected by a learned router. The light feedforward branch has reduced hidden size while the heavy branch has increased hidden size. The light attention does local attention with fewer heads while the heavy branch attends globally. By routing a fraction of tokens to the heavy branches, \slic reduces computation compared to applying the full transformer uniformly. Experiments show that \slic matches or exceeds the performance of \longt with significant speedups. \slic also demonstrates the ability to effectively leverage very long inputs up to 64k tokens.


## What problem or question is the paper addressing?

 The paper is presenting a new model called SLIC (Conditional LongT5) for processing long text inputs efficiently. The key problems it is trying to address are:- Processing long documents with Transformers is computationally expensive, due to the quadratic scaling of attention and having to apply feedforward layers to every token. This makes it difficult to leverage very long documents.- Not all tokens are equally important, especially in longer documents. So it is inefficient to devote equal computation to every token.To address these issues, the main question the paper seems to be asking is:How can we build a Transformer model that selectively applies more computation to the important tokens in long documents, thereby improving quality and computational efficiency?The paper proposes a conditional computation approach in SLIC to route important tokens in both the feedforward and attention layers to higher capacity components, while applying lighter components to all tokens. This allows focusing computation on the small fraction of salient tokens.In summary, the key problem is efficiently leveraging long documents in Transformers, and the main question is how to apply conditional computation to devote higher capacity to important tokens. SLIC provides a way to do this for both feedforward and attention layers.
