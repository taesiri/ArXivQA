# [MultiMAE: Multi-modal Multi-task Masked Autoencoders](https://arxiv.org/abs/2204.01678)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to extend masked autoencoders (MAEs) to effectively leverage multiple input modalities and output tasks during pre-training. Specifically, the authors propose a new pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE) that can optionally accept additional input modalities besides RGB images and has training objectives that include predicting multiple output tasks. 

The key hypotheses tested are:

1) Adding more input modalities like depth maps and semantics during pre-training will allow MultiMAE to better transfer to downstream tasks, especially when those extra modalities are available.

2) Training with multiple output tasks beyond just reconstructing the RGB image will lead to learning more general representations that transfer better across different downstream tasks.

3) Masking across modalities and tasks is crucial for making the multi-modal multi-task pre-training computationally tractable and promoting cross-modal predictive coding.

4) Pseudo-labeling can be effectively used to create a large-scale multi-modal multi-task dataset for pre-training without needing aligned ground truth data.

The experiments aim to validate these hypotheses by pre-training MultiMAE models on pseudo-labeled data and evaluating their transfer performance to various downstream tasks using different input modalities. The results generally confirm the hypotheses, showing benefits of the proposed multi-modal multi-task pre-training approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Multi-modal Multi-task Masked Autoencoders (MultiMAE), a pre-training strategy for Vision Transformers. The key aspects of MultiMAE are:

1. It can optionally accept multiple modalities as input besides just RGB images, making it "multi-modal". For example, the paper explores using depth maps and semantic segmentation along with RGB.

2. The pre-training objective includes reconstructing multiple output modalities/tasks besides just RGB images, making it "multi-task". In this case, the tasks are RGB, depth, and semantic segmentation. 

3. Masking is used (across image patches and input modalities) during pre-training to make it tractable and to enforce cross-modality predictive coding.

4. The same pre-trained model can be flexibly used for downstream tasks, whether additional modalities are available or not. This is shown to improve results compared to baselines.

5. MultiMAE is trained using pseudo-labeling, avoiding the need for aligned multi-modal datasets. This makes the framework widely applicable.

In summary, the main contribution is proposing a multi-modal multi-task extension to masked autoencoders that is shown to learn better representations for downstream tasks while remaining simple and efficient to train. The use of masking and pseudo-labeling are key aspects that enable training at scale.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes Multi-modal Multi-task Masked Autoencoders (M3AE), a self-supervised pre-training method for Vision Transformers that trains the model to reconstruct masked image patches from multiple modalities (e.g. RGB, depth, segmentation maps) and outputs multiple prediction tasks, showing strong performance on downstream transfer tasks when additional modalities are available.
