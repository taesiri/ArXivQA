# [PEFTDebias : Capturing debiasing information using PEFTs](https://arxiv.org/abs/2312.00434)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces PEFTDebias, a novel debiasing approach for foundation models like BERT that leverages parameter-efficient fine-tuning (PEFT) techniques. It consists of an upstream phase where debiasing PEFT parameters are learned by training on axis-specific corpora using counterfactual data augmentation, and a downstream phase where these parameters are injected into a trainable model and kept frozen during task fine-tuning. Experiments demonstrate that techniques like Prompt Tuning effectively capture debiasing information and mitigate biases on various datasets across gender and racial axes, both in-domain and out-of-domain, with minimal impact on task performance. The transferability of the debiasing parameters to new tasks validates that they learn generalizable representations. By evaluating comprehensive intrinsic and extrinsic bias metrics, the authors systematically analyze the efficacy of different PEFT methods for bias mitigation. The overall effectiveness of PEFTDebias highlights the viability of specialized subnetworks to sustainably debias foundation models.
