# [MixCE: Training Autoregressive Language Models by Mixing Forward and   Reverse Cross-Entropies](https://arxiv.org/abs/2305.16958)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to train autoregressive language models to generate more human-like text during open-ended generation. The hypothesis is that combining forward and reverse cross-entropy as a training objective will result in language models that produce higher quality and more diverse text compared to standard maximum likelihood estimation.The key points are:- Standard MLE training struggles to penalize bad or non-human-like generations from the model. It also tends to overgeneralize and cover noise in the training data.- Reverse cross-entropy evaluates model generations by the human distribution, focusing on text quality. Forward cross-entropy ensures diversity. Combining them helps balance quality and diversity. - Approximating and optimizing the intractable reverse cross-entropy results in a self-reinforced objective that encourages high-confidence generations.- Experiments show models trained with the proposed MixCE objective generate text more preferred by humans compared to MLE, without needing modified decoding methods.In summary, the paper hypothesizes and provides evidence that optimizing a mixture of forward and reverse cross-entropy can improve language model training towards more human-like text generation.
