# [MixCE: Training Autoregressive Language Models by Mixing Forward and   Reverse Cross-Entropies](https://arxiv.org/abs/2305.16958)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to train autoregressive language models to generate more human-like text during open-ended generation. The hypothesis is that combining forward and reverse cross-entropy as a training objective will result in language models that produce higher quality and more diverse text compared to standard maximum likelihood estimation.The key points are:- Standard MLE training struggles to penalize bad or non-human-like generations from the model. It also tends to overgeneralize and cover noise in the training data.- Reverse cross-entropy evaluates model generations by the human distribution, focusing on text quality. Forward cross-entropy ensures diversity. Combining them helps balance quality and diversity. - Approximating and optimizing the intractable reverse cross-entropy results in a self-reinforced objective that encourages high-confidence generations.- Experiments show models trained with the proposed MixCE objective generate text more preferred by humans compared to MLE, without needing modified decoding methods.In summary, the paper hypothesizes and provides evidence that optimizing a mixture of forward and reverse cross-entropy can improve language model training towards more human-like text generation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new training objective called MixCE for autoregressive language models. The key ideas are:- MixCE combines forward and reverse cross-entropy losses during training. Forward cross-entropy focuses on diversity of generations while reverse cross-entropy focuses more on quality. - MixCE approximates a mixture of forward and reverse KL divergences, which helps balance two complementary driving forces - reverse KL narrows down the model distribution when it is too broad while forward KL broadens the model distribution when too narrow.- They derive an approximation for reverse cross-entropy that encourages the model to generate text it is already confident about. This acts like a self-reinforcing objective.  - Experiments show MixCE improves over standard maximum likelihood estimation (which uses just forward cross-entropy) in both synthetic and real settings. Models trained with MixCE generate more human-like, higher quality text according to both automatic metrics and human evaluation.In summary, the main contribution is proposing the MixCE objective to improve open-ended text generation from autoregressive language models by combining forward and reverse cross-entropy losses. The method is shown to generate better quality text without needing complex decoding strategies.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new training objective called MixCE that mixes forward and reverse cross-entropy to improve the quality and diversity of text generated by autoregressive language models like GPT-2.
