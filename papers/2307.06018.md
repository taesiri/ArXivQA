# [PolyLM: An Open Source Polyglot Large Language Model](https://arxiv.org/abs/2307.06018)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question/hypothesis seems to be:How can we develop an open-source multilingual large language model (LLM) that achieves strong performance across diverse languages, especially lower-resource non-English languages?The key gaps this paper aims to address are:1) The lack of an open-source multilingual LLM, especially one with a large 13B parameter version. 2) The inadequate availability of multilingual instruction data for fine-tuning LLMs.3) The lack of a unified multilingual evaluation benchmark.To address these gaps, the paper introduces:- PolyLM, an open-source multilingual LLM trained on 640B tokens covering over 30% non-English data in 18 major languages. Models are released in 13B and 1.7B sizes.- MultiAlpaca, a multilingual instruction dataset with 132k samples in 11 languages, generated via a self-instruct method.- A multilingual benchmark with 10 tasks across 15 languages for model evaluation.The central hypothesis is that PolyLM, trained with a curriculum strategy and multilingual instruction data, will outperform prior open-source models on this diverse multilingual benchmark, especially for non-English languages, while maintaining strong English proficiency.In summary, the key research question is how to develop an open-source multilingual LLM with broad capabilities, and the paper proposes PolyLM along with associated training strategies, datasets, and benchmarks to address this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The release of PolyLM, an open-source multilingual large language model trained from scratch. The key features highlighted are:- It is available in two sizes - 1.7B and 13B parameters, to facilitate usage. - Trained on a 640B token dataset covering 18 major languages. The data incorporates over 30% non-English text.- Employs a curriculum learning strategy to transfer knowledge from high to low resource languages. 2. Construction of a multilingual instruction dataset called MultiAlpaca with 132K samples in 11 languages. This aims to provide diverse instruction data for multilingual model fine-tuning.3. Introduction of a new multilingual benchmark for evaluating capabilities like language understanding, knowledge grasping, generation and translation across multiple languages.4. Extensive experiments showing PolyLM outperforms comparable open-source models on non-English tasks, while maintaining English proficiency. Analyses also demonstrate the benefits of curriculum learning and multilingual instruction tuning.In summary, the main contribution appears to be the release of an open-source multilingual model PolyLM that aims to enhance LLM research and usage for non-English languages. The other contributions like MultiAlpaca and the benchmark support and validate the capabilities of PolyLM across diverse multilingual tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review of the paper, here is a one sentence summary:The paper presents PolyLM, a new open-source multilingual language model of 1.7B and 13B parameters, along with a multilingual instruction dataset and benchmark for evaluating multilingual capabilities.


## How does this paper compare to other research in the same field?

Based on my reading, here is a high-level comparison of this paper to other related research:- Dataset: This paper introduces a new large-scale multilingual dataset for pretraining language models. The dataset contains 640B tokens covering 18 languages, with 32% being non-English data. This is a larger and more diverse multilingual dataset compared to what has been used to train other open-source multilingual LLMs like BLOOM and LLaMA.- Model Architecture: The paper presents two Transformer-based language models with 1.7B and 13B parameters. These model sizes are comparable to existing models like LLaMA-13B and BLOOM-7B. The models use a vocabulary of 256K tokens with BPE, which provides better coverage of non-English languages compared to models like LLaMA with smaller vocabularies.- Training Methodology: A key contribution is the curriculum learning strategy during pretraining that progressively increases the amount of high-quality multilingual data. This is different from prior work that generally uses a fixed data distribution throughout training. The paper also uses techniques like mixed-precision training that have been explored in other recent LLMs.- Multilingual Evaluation: The paper conducts a rigorous evaluation of multilingual capabilities on a diverse set of NLU, NLG, QA, and MT tasks. This provides a more comprehensive assessment of multilingual performance compared to prior models that were often evaluated only on English or a smaller subset of tasks/languages.- Multilingual Instruction Tuning: Introducing the new MultiAlpaca dataset for multilingual instruction tuning is a novel contribution not explored by other models. This allows tailored tuning for multilingual zero-shot abilities.Overall, the model, training approach, evaluation, and instruction tuning in this paper provide important innovations beyond prior multilingual LLMs, while building on a foundation of similar model architectures and pretraining techniques explored in other recent work. The comprehensive multilingual benchmark evaluation demonstrates these improvements over existing models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Exploring techniques to further optimize multilingual performance, such as combining diverse data sources, evolutionary methods to refine instructions, and strategies to increase instructional diversity. For example, the authors mention potential areas like incorporating ROTARY position encoding or the ALiBi attention mechanism to expand the model's context window size.- Releasing additional PolyLM models of varying sizes to refine the scaling laws and gain more insight into the relationship between model scale and multilingual capabilities.- Further analysis and possible improvements to the curriculum learning strategy, as well as the self-instruction fine-tuning process, to strengthen the model's ability to acquire and apply multilingual knowledge. - Extending PolyLM's capabilities to more low-resource languages beyond the current set, to advance multilingual NLP research and applications.- Continuing to assess potential deficiencies in PolyLM related to biases, hallucination, factuality, etc. and exploring techniques to improve alignment with human values.- Using PolyLM and the MultiAlpaca dataset as a foundation for follow-on work aimed at advancing multilingual LLMs and evaluating their instruction following abilities.In summary, the key directions focus on scaling and analyzing PolyLM, refining the training methodology, expanding multilingual support, evaluating model deficiencies, and leveraging PolyLM to enable further research innovations in this space.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes PolyLM, a new multilingual large language model trained on 640 billion tokens covering 18 major languages. To enhance its multilingual capabilities, two key strategies are employed during pretraining: 1) Bilingual data is integrated into the training data to improve cross-lingual representation. 2) A curriculum learning approach is adopted that increases the proportion of non-English data from 30% initially to 60% in later stages. This enables transferring knowledge from high-resource languages like English to low-resource languages. The pretrained PolyLM is then fine-tuned using Multilingual Alpaca, a new dataset of 132,701 diverse multilingual instructions automatically generated via a self-instruct method. To assess PolyLM's capabilities, the authors collect a multilingual benchmark with tasks spanning question answering, natural language understanding, generation and translation across 15 languages. Experiments demonstrate PolyLM's stronger performance compared to existing models like LLaMA and BLOOM on non-English languages, while maintaining English proficiency. The curriculum strategy and multilingual instruction tuning are shown to notably enhance PolyLM's multilingual abilities.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces PolyLM, a new multilingual language model developed by Alibaba DAMO Academy. PolyLM is available in two sizes - 1.7 billion parameters and 13 billion parameters. It is trained on 640 billion tokens covering over 30% non-English data across 18 languages. To enhance its multilingual capabilities, the authors use two techniques: 1) Integrating parallel bilingual data into the training data, and 2) A curriculum learning strategy that increases the proportion of non-English data from 30% initially to 60% in later stages of pre-training. The authors also propose a multilingual self-instruct method to automatically generate 132,701 diverse multilingual instructions for model fine-tuning. To evaluate PolyLM, the authors collect existing multilingual tasks and build a benchmark covering language understanding, question answering, generation and translation across 15 languages. Extensive experiments demonstrate PolyLM's superiority over other open-source models like LLaMA and BLOOM on non-English languages, while maintaining comparable performance in English. Analyses show the curriculum strategy boosts multilingual performance without compromising English proficiency. The multilingual instruction data also markedly improves PolyLM's ability to tackle zero-shot multilingual tasks. The models, instruction data and benchmark are publicly released to facilitate research in multilingual LLMs.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces PolyLM, a new open-source multilingual large language model (LLM) developed by Alibaba's DAMO Academy. PolyLM comes in 1.7 billion and 13 billion parameter sizes and is trained on 640 billion tokens covering over 30% non-English data in 18 major languages. To improve multilingual capabilities, PolyLM employs curriculum learning which gradually increases the ratio of non-English data during training to facilitate knowledge transfer. The authors also construct a 132,701 sample multilingual instruction dataset MultiAlpaca using self-supervision to enhance PolyLM's ability to follow multilingual instructions. To evaluate PolyLM, the authors gather multilingual tasks into a benchmark covering language understanding, QA, generation and translation across 15 languages. Experiments show PolyLM outperforms other open-source models like BLOOM and LLaMA on non-English tasks while maintaining English proficiency. The paper makes models, instruction data and benchmark available to facilitate multilingual LLM research and usage.
