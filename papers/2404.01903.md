# [Activation Steering for Robust Type Prediction in CodeLLMs](https://arxiv.org/abs/2404.01903)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Contemporary language models pretrained on code (CodeLLMs) are very useful for programming tasks, but they are sensitive to small syntactical changes in code that do not change semantics (e.g. variable renaming). This makes them unreliable in production environments.

- The paper focuses on the task of neural type prediction in gradually typed languages like Python and TypeScript. The goal is to predict a type annotation for a variable binding in a partially typed program. CodeLLMs do well on natural data but small syntactical changes often lead them to mispredict types.

Methodology:
- The paper proposes using activation steering to make CodeLLMs more robust to syntactic distractors. Activation steering involves modifying the internal activations of a model to steer it towards a desired prediction. 

- Steering vectors are constructed using semantics-preserving code edits that lead to mispredictions. Edits include variable renames, type renames, and removing annotations. Pairs of related code snippets are created - one which elicits a correct prediction, and one (with edits) which does not. 

- The mean difference between activations of these code pairs is used as the steering vector. The intuition is that this difference encodes the transformation to correct to the right type. Steering is applied by adding this vector to activations at layers 10-14 of StarCoderBase-1B.

Contributions:
- A novel method to construct steering vectors for code using semantics-preserving edits inspired by mutation testing.

- Steering significantly improves type prediction accuracy over baselines, correcting up to 90% of mispredictions. Vectors generalize to unseen programs.

- Steering is more lightweight than fine-tuning, which degrades unrelated abilities like code completion.

- Steering vectors surprisingly transfer between Python and TypeScript, suggesting CodeLLMs learn a shared representation of types across programming languages.


## Summarize the paper in one sentence.

 This paper presents an inference-time technique to make CodeLLMs more robust to syntactic distractors for neural type prediction by steering internal model activations using semantics-preserving program transformations.


## What is the main contribution of this paper?

 According to the abstract, the main contribution of this paper is an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Specifically, the authors contribute a novel way to construct steering vectors for the activation steering method by taking inspiration from mutation testing. They apply this approach to the task of type prediction for the gradually typed languages Python and TypeScript.

So in summary, the main contribution is a new technique for constructing steering vectors that can make CodeLLMs more robust at predicting types, even in the presence of irrelevant syntactic changes to the code. This is done using activation steering at inference time.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- CodeLLMs (Code Large Language Models) - Transformer models pretrained on large corpora of source code to perform various programming tasks.

- Activation steering - An inference-time technique to modify a model's internal activations to steer behavior towards desired predictions.

- Type prediction - The task of predicting type annotations in gradually typed languages like Python and TypeScript. 

- Distractors - Semantics-preserving syntactic code changes like variable renamings that cause models to mispredict.

- Mutation testing - Method for evaluating test suites by introducing small semantic-changing edits to code. Contrasted with the semantics-preserving edits used to construct steering vectors.

- Residual stream - The inputs/outputs of each transformer layer which steering vectors are added to. 

- Task vectors - Latent directions in model representation space encoding high-level notions of tasks. Successful steering meant to perform transformations over these.

- Language transfer - Finding that steering vectors for type prediction transfer across Python and TypeScript, suggesting the model learns a shared representation of types across languages.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper constructs steering vectors by taking the difference between activations of positive and negative prompts. What is the intuition behind using this difference to steer the model's predictions? How does this relate to the idea of "task vectors" that represent high-level concepts like types?

2. The paper finds that steering over multiple adjacent layers leads to better performance than steering at individual layers. Why might steering over multiple layers be more effective? How does this relate to ideas about how transformers build up representations over layers?

3. When constructing negative prompts, the paper applies semantics-preserving code edits like renaming variables and types. Why are these edits useful for constructing steering vectors? What threat model or types of robustness do you think these edits target?

4. The paper shows that steering vectors transfer surprisingly well between Python and TypeScript. What does this suggest about how the CodeLLM represents types, even across different programming languages? How might you design an experiment to further probe this representation?

5. The paper compares steering to fine-tuning and shows a tradeoff - fine-tuning improves type prediction but harms code completion. Why does this happen and why might steering vectors avoid this tradeoff? When might fine-tuning still be preferred over steering?

6. How confident are you that the performance gains from steering truly come from the steering vectors, rather than just triggering backup circuits or other mechanisms in the model? What further analyses could you do to increase confidence? 

7. The steering vectors are constructed from natural data instead of synthetic examples. Why is this important? What threats to validity does this mitigate compared to using synthetic data?

8. The paper focuses specifically on steering type annotations. What other prediction tasks in code could you apply steering vectors to? Would the methodology need to change?

9. The paper hypothesizes steering vectors target an underlying "task vector" that represents the high-level task. What other interventions could you design to test whether these task vectors exist and can be manipulated?

10. The paper studies steering's robustness to different distractors, but doesn't deeply analyze what types are still the hardest to correct. What further analysis could you do to better understand the limitations of this approach?
