# [SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural   Architecture Search](https://arxiv.org/abs/2402.13204)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Hardware-aware Neural Architecture Search (HW-aware NAS) aims to automate the design of neural networks (NNs) that are optimized for both performance (e.g. accuracy) and hardware efficiency (e.g. latency, energy). However, the search space is large and evaluating NN architectures is expensive. 
- Conventional evolutionary NAS relies on random mutation and crossover operators to generate new NN architectures. But this can lead to wasting resources exploring low-quality architectures.
- There is untapped potential to leverage the search history data to guide the search more strategically. 

Proposed Solution:
- The authors propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS. 
- SONATA uses the search history to train machine learning models to guide the mutation and crossover operators to focus on the most important NN design parameters. This aims to generate better candidate architectures over time.
- An XGBoost model is used to estimate the accuracy of architectures. Look-up tables are used to estimate hardware metrics. This accelerates the evaluation.
- A tree-based model called Î¸ is trained to estimate the "optimality" and "diversity" of architectures on the Pareto front tradeoff. The importance scores of design parameters from this model guide the mutation and crossover. 
- A reinforcement learning agent assigns dynamic evolution probabilities to architectures.

Main Contributions:
- Novel framework to leverage search history to learn importance of design parameters and guide evolutionary operators in NAS.
- Self-adaptive mutation, crossover, and evaluation components.
- Validation across CNN and Transformer architectures and hardware.
- Accuracy gains up to 0.25% and latency/energy improvements up to 2.42x over baseline methods.
- Up to 93.6% more Pareto dominance over standard NSGA-II algorithm.
