# [Training Class-Imbalanced Diffusion Model Via Overlap Optimization](https://arxiv.org/abs/2402.10821)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Diffusion models have shown promising results for high-quality image generation. However, when trained on real-world imbalanced datasets, they perform worse on tail/rare classes compared to head/majority classes.  
- Images synthesized for rare classes often lack fidelity and exhibit an overlap in appearance with images from majority classes. Diffusion models exhibit bias towards generating stereotypical images of majority classes.

Proposed Solution: 
- The paper proposes a novel method called DiffROP to minimize the overlap between distributions of synthetic images from different classes. This is done via a probabilistic contrastive learning (PCL) loss.
- The PCL loss penalizes the KL divergence between conditional image distributions for different classes. It is shown that this KL divergence can be simply computed using the estimated noises for two images from two different classes.
- Several variants of the PCL loss using hinge-based functions are explored to ensure the original diffusion model loss function optimum is preserved while minimizing distribution overlap.
- The overall framework simply adds this PCL loss as a regularization term to the original diffusion model objective. No specific sampling schemes are required.

Main Contributions:
- Proposes a general and easy to implement probabilistic contrastive learning loss to handle class imbalance in diffusion models. Can work with any conditional diffusion model.
- Cleverly estimates the KL divergence between conditional distributions using predicted noises to enable efficient optimization.
- Shows superior quantitative and qualitative generation results compared to baselines, especially for tail classes on CIFAR and derivatives.
- Demonstrates improved classification accuracy when using DiffROP augmented minority class samples, proving usefulness for downstream tasks.

In summary, the paper introduces an effective data-efficient technique to enhance conditional diffusion models for imbalanced datasets via distribution regularization. The modular PCL loss provides explicit control over overlap between distributions for different classes.
