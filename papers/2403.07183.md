# [Monitoring AI-Modified Content at Scale: A Case Study on the Impact of   ChatGPT on AI Conference Peer Reviews](https://arxiv.org/abs/2403.07183)

## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 suggested in-depth questions about the method proposed in this paper:

1. The paper proposes using maximum likelihood estimation to estimate the fraction of AI-generated text in a corpus. What are the key assumptions behind this approach and when might they be violated in practice?

2. The vocabulary is restricted to adjectives when estimating text distributions P and Q. What is the justification for this choice and have the authors explored using other parts of speech or other semantic features? 

3. The method splits the training data 80/20 into train/validation. Has any sensitivity analysis been done regarding this split? For example, how does performance change if 90/10 split is used instead?

4. Have the authors experimented with any semi-supervised approaches to leverage unlabeled review data and improve estimation of P and Q? If not, what challenges do you foresee in applying semi-supervised learning in this setting?

5. One of the advantages claimed is computational efficiency. But have actual runtimes been benchmarked relative to baseline methods? And how does the method scale as the number of documents grows to millions?

6. Theoretical analysis is provided for the sample complexity. But how tight are these bounds? And could the analysis be strengthened by incorporating assumptions on the separability of P and Q? 

7. The method estimates the fraction of sentences generated by AI. How might the accuracy change if applied at the document level instead? Could any adjustments help improve document-level performance?

8. Prompts used to generate the AI training data play an important role. Is there any analysis regarding the sensitivity of results to changes in prompts? 

9. External language models like GPT-3 are used to generate text. But might better performance be achieved by fine-tuning on the target distribution of documents?

10. The method focuses solely on English text. What adaptations would be needed to apply this approach to a multilingual corpus containing translations or code-switching?
