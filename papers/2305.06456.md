# [Perpetual Humanoid Control for Real-time Simulated Avatars](https://arxiv.org/abs/2305.06456)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research contributions seem to be:1) Proposing a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can imitate a large corpus of human motions with high fidelity and recover from failures like falls, without needing resets. 2) Introducing a progressive multiplicative control policy (PMCP) that allows efficient scaling when learning from large motion datasets and adding new capabilities like fail-recovery, without catastrophic forgetting.3) Demonstrating that the controller can work with just 3D keypoint inputs instead of full joint rotations, which makes it compatible with vision-based 3D pose estimators.4) Showing applications like real-time avatar control from video pose estimates and disjoint motion clips generated from language prompts, enabled by PHC's ability to perpetually control the avatar without resets.So in summary, the main research focus seems to be developing a robust and scalable physics-based controller that can mimic diverse human motions and drive simulated avatars perpetually from imperfect/noisy inputs like video or language, without needing resets. The key ideas are the PMCP for scalability and fail-recovery, as well as the ability to work with just 3D keypoints as input.
