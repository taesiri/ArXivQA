# [Perpetual Humanoid Control for Real-time Simulated Avatars](https://arxiv.org/abs/2305.06456)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research contributions seem to be:1) Proposing a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can imitate a large corpus of human motions with high fidelity and recover from failures like falls, without needing resets. 2) Introducing a progressive multiplicative control policy (PMCP) that allows efficient scaling when learning from large motion datasets and adding new capabilities like fail-recovery, without catastrophic forgetting.3) Demonstrating that the controller can work with just 3D keypoint inputs instead of full joint rotations, which makes it compatible with vision-based 3D pose estimators.4) Showing applications like real-time avatar control from video pose estimates and disjoint motion clips generated from language prompts, enabled by PHC's ability to perpetually control the avatar without resets.So in summary, the main research focus seems to be developing a robust and scalable physics-based controller that can mimic diverse human motions and drive simulated avatars perpetually from imperfect/noisy inputs like video or language, without needing resets. The key ideas are the PMCP for scalability and fail-recovery, as well as the ability to work with just 3D keypoints as input.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:- Proposing a Perpetual Humanoid Controller (PHC) that can imitate human motion with high fidelity and recover from failure states like falling down, without needing resets.- Introducing the Progressive Multiplicative Control Policy (PMCP) which allows efficient scaling to large motion databases and adding new capabilities like fail-state recovery without catastrophic forgetting. - Demonstrating the effectiveness of PHC for real-time avatar control using noisy pose estimates from video and disjoint motion clips from language-based generators.- Showing that the controller can be keypoint-based, only requiring 3D joint positions instead of full joint rotations, making it easier to integrate with vision-based pose estimators.- Achieving state-of-the-art performance on physics-based motion imitation, recovering from fail states, and driving real-time avatars without needing resets. PHC imitates 98.9% of the large AMASS dataset without external forces.- Presenting qualitative results like controlling simulated avatars in real-time from webcam video and generating motions from language prompts.In summary, the main contribution seems to be proposing PHC, a robust and perpetual physics-based controller for simulated characters, and demonstrating its effectiveness for tasks like real-time avatar control even with noisy observations. The PMCP and keypoint-only formulations are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can mimic a large corpus of human motion with high fidelity and recover from failure cases like falling down, enabling perpetual control of simulated avatars from video or text inputs without needing resets.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related research:- This paper focuses on physics-based motion imitation and controlling simulated humanoids. Other work in this area includes methods like DeepMimic, SIMS, and Unconstrained Humanoids Control (UHC). A key distinction of this paper is the goal of perpetual control without needing resets, even when the humanoid falls or receives noisy input data. Most prior work requires resetting the pose when failures occur.- The proposed Progressive Multiplicative Control Policy (PMCP) allows efficient training on large motion datasets like AMASS without catastrophic forgetting. This overcomes limitations of prior approaches that struggle to scale training to such diverse and large datasets with a single policy. The concepts of training primitive networks on progressively harder tasks and combining them with a composer network draws inspiration from progressive neural networks and multiplicative control policies.- For handling fail states, this paper uniquely aims to learn natural recoveries and return to the reference motion, compared to prior work that uses pre-defined recovery policies or resetting. Learning the recovery capability alongside the main imitation policy is enabled by the PMCP formulation.- The controller is designed to be robust to noisy input, which sets it apart from methods that assume clean motion capture data. Tests on estimated poses from video and language-generated motions demonstrate this real-world applicability.- The capability to perform rotation-free imitation with only 3D joint positions as input is also novel, since most prior physics-based methods require full pose information. This could allow integration with more vision-based pose estimators.- For real-time avatar control, this paper shows strong results on driving simulated characters with live video and text inputs. This is a promising application area enabled by the perpetual control and noise robustness of the method.In summary, the key innovations of perpetual control, scaling to large datasets, fail recovery, and noisy input robustness help advance the state-of-the-art in physics-based motion imitation and controlling digital human models. The real-time avatar experiments also showcase promising practical applications.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some potential future research directions the authors suggest include:- Improving imitation capability to achieve 100% success rate on imitating all motion sequences in the training set. The current method still struggles with very dynamic motions like backflips.- Incorporating terrain and scene awareness to enable human-object interactions. The current method focuses on imitation in empty environments. Enabling interaction with objects and uneven terrain could expand the applicability.- Tighter integration with downstream tasks like pose estimation and motion generation. The current pipeline uses off-the-shelf components for pose estimation and motion generation. A deeper integration could improve real-time performance and capabilities.- Applying the progressive reinforcement learning framework to other tasks like embodied agents and language grounding. The PMCP approach could enable scaling and transfer learning for other hard RL problems.- Exploring sequence-level information or future motion prediction to help learn highly dynamic motions within the overall dataset. The current per-frame policy struggles to learn certain motions that require planning. - Improving naturalness and smoothness during fail-state recovery. The current method can get up and resume tracking but exhibits some unnatural behaviors in this phase.- Enabling simulation of soft-body dynamics for more realism. The current method uses simplified capsule and mesh humanoids.In summary, potential future directions focus on improving imitation fidelity, integrating with other systems, applying the methods to new domains, and enhancing fail-state recovery behaviors. Overall it seems like an promising and generalizable approach that could enable many future applications.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can mimic human motion and naturally recover from falls to resume motion tracking. PHC uses a progressive training procedure called progressive multiplicative control policy (PMCP) to train multiple subnetworks/primitives on increasingly harder motion sequences and tasks (like fall recovery) without forgetting previously learned skills. This allows PHC to scale up imitation learning to large motion datasets like AMASS (10k clips) and achieve 98.9% success rate without using any external stabilization forces. For real-time avatar control, PHC only requires 3D joint positions as input which can be provided by off-the-shelf video pose estimators. PHC can perpetually control simulated avatars from webcam video without requiring resets after falls. Experiments show PHC can imitate both clean MoCap data and noisy estimated poses, mimic prompts from text-based motion generators, and naturally recover from fall states.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can imitate human motion with high fidelity and recover from failures like falls. PHC uses a goal conditioned reinforcement learning framework where the goal is to track reference human motion. It incorporates an adversarial motion prior throughout the pipeline to ensure natural human-like behavior. To scale up imitation learning to large datasets like AMASS, PHC uses a progressive multiplicative control policy (PMCP) which allocates new network capacity to learn harder motion sequences without forgetting easier ones or requiring external stabilizing forces. This allows recovering from fail states like falls without compromising imitation ability. Experiments show PHC can imitate 98.9% of the AMASS dataset and naturally recover from fail states. PHC is also robust to noisy input like video pose estimates, enabling real-time avatars driven by webcam video.The main contributions are: (1) PHC controller that achieves high success imitating AMASS and recovers from failures without external forces; (2) PMCP method that scales imitation learning to large datasets without catastrophic forgetting; (3) PHC works with noisy pose estimates like video, enabling real-time avatars driven by webcam video. The method is task-agnostic so works with various pose estimators and motion generators. Experiments validate high fidelity imitation, fail recovery, and driving real-time avatars from video and language-generated motion. Limitations are imperfect imitation success rate and reduced performance on noisy video input vs offline. Future work involves tighter integration with pose estimation and motion generation.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a physics-based humanoid controller called the Perpetual Humanoid Controller (PHC) that can imitate human motion with high fidelity and recover from failures like falls without needing resets. It uses a progressive reinforcement learning approach called the progressive multiplicative control policy (PMCP) where the model is trained on increasingly difficult motion data by allocating new network capacity to learn harder tasks without forgetting easier ones. This allows efficient scaling to large motion datasets like AMASS. The controller is trained using adversarial imitation learning with an additional energy regularization term for stability. During inference, the controller takes in either joint rotations or just 3D keypoints as input and outputs actions to track the motion. It can perpetually control simulated avatars by pairing it with off-the-shelf video pose estimators or text-based motion generators. The controller does not need any external forces for stabilization.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- It addresses the challenge of controlling simulated humanoid avatars to imitate reference motions, especially in the presence of noisy input (e.g. from video pose estimation) and unexpected falls. - Current methods struggle to handle noisy observations and cannot perpetually control avatars without resetting/stabilizing forces when falls happen. This limits their applicability for real-time avatar control.- The paper proposes a physics-based controller called Perpetual Humanoid Controller (PHC) that can imitate motions with high fidelity and naturally recover from falls to resume tracking the reference.- PHC uses a novel progressive training procedure called progressive multiplicative control policy (PMCP) to efficiently learn from large motion datasets without catastrophic forgetting. - PMCP allows adding new skills like fall recovery without compromising prior imitation capabilities.- PHC works with just 3D keypoints as input, instead of full joint rotations, making it easy to connect with vision-based pose estimators.- Experiments show PHC can imitate almost all sequences in AMASS dataset without any external forces, and perpetually control avatars from video/language inputs by recovering from failures.In summary, the key contribution is a robust and perpetual physics-based controller for simulated humanoid avatars, applicable to real-time avatar control from imperfect vision/language inputs. The progressive training method is also a general framework for multi-task RL.
