# [Perpetual Humanoid Control for Real-time Simulated Avatars](https://arxiv.org/abs/2305.06456)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research contributions seem to be:1) Proposing a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can imitate a large corpus of human motions with high fidelity and recover from failures like falls, without needing resets. 2) Introducing a progressive multiplicative control policy (PMCP) that allows efficient scaling when learning from large motion datasets and adding new capabilities like fail-recovery, without catastrophic forgetting.3) Demonstrating that the controller can work with just 3D keypoint inputs instead of full joint rotations, which makes it compatible with vision-based 3D pose estimators.4) Showing applications like real-time avatar control from video pose estimates and disjoint motion clips generated from language prompts, enabled by PHC's ability to perpetually control the avatar without resets.So in summary, the main research focus seems to be developing a robust and scalable physics-based controller that can mimic diverse human motions and drive simulated avatars perpetually from imperfect/noisy inputs like video or language, without needing resets. The key ideas are the PMCP for scalability and fail-recovery, as well as the ability to work with just 3D keypoints as input.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a Perpetual Humanoid Controller (PHC) that can imitate human motion with high fidelity and recover from failure states like falling down, without needing resets.- Introducing the Progressive Multiplicative Control Policy (PMCP) which allows efficient scaling to large motion databases and adding new capabilities like fail-state recovery without catastrophic forgetting. - Demonstrating the effectiveness of PHC for real-time avatar control using noisy pose estimates from video and disjoint motion clips from language-based generators.- Showing that the controller can be keypoint-based, only requiring 3D joint positions instead of full joint rotations, making it easier to integrate with vision-based pose estimators.- Achieving state-of-the-art performance on physics-based motion imitation, recovering from fail states, and driving real-time avatars without needing resets. PHC imitates 98.9% of the large AMASS dataset without external forces.- Presenting qualitative results like controlling simulated avatars in real-time from webcam video and generating motions from language prompts.In summary, the main contribution seems to be proposing PHC, a robust and perpetual physics-based controller for simulated characters, and demonstrating its effectiveness for tasks like real-time avatar control even with noisy observations. The PMCP and keypoint-only formulations are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a physics-based humanoid controller called Perpetual Humanoid Controller (PHC) that can mimic a large corpus of human motion with high fidelity and recover from failure cases like falling down, enabling perpetual control of simulated avatars from video or text inputs without needing resets.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper focuses on physics-based motion imitation and controlling simulated humanoids. Other work in this area includes methods like DeepMimic, SIMS, and Unconstrained Humanoids Control (UHC). A key distinction of this paper is the goal of perpetual control without needing resets, even when the humanoid falls or receives noisy input data. Most prior work requires resetting the pose when failures occur.- The proposed Progressive Multiplicative Control Policy (PMCP) allows efficient training on large motion datasets like AMASS without catastrophic forgetting. This overcomes limitations of prior approaches that struggle to scale training to such diverse and large datasets with a single policy. The concepts of training primitive networks on progressively harder tasks and combining them with a composer network draws inspiration from progressive neural networks and multiplicative control policies.- For handling fail states, this paper uniquely aims to learn natural recoveries and return to the reference motion, compared to prior work that uses pre-defined recovery policies or resetting. Learning the recovery capability alongside the main imitation policy is enabled by the PMCP formulation.- The controller is designed to be robust to noisy input, which sets it apart from methods that assume clean motion capture data. Tests on estimated poses from video and language-generated motions demonstrate this real-world applicability.- The capability to perform rotation-free imitation with only 3D joint positions as input is also novel, since most prior physics-based methods require full pose information. This could allow integration with more vision-based pose estimators.- For real-time avatar control, this paper shows strong results on driving simulated characters with live video and text inputs. This is a promising application area enabled by the perpetual control and noise robustness of the method.In summary, the key innovations of perpetual control, scaling to large datasets, fail recovery, and noisy input robustness help advance the state-of-the-art in physics-based motion imitation and controlling digital human models. The real-time avatar experiments also showcase promising practical applications.


## What future research directions do the authors suggest?

Based on my reading of the paper, some potential future research directions the authors suggest include:- Improving imitation capability to achieve 100% success rate on imitating all motion sequences in the training set. The current method still struggles with very dynamic motions like backflips.- Incorporating terrain and scene awareness to enable human-object interactions. The current method focuses on imitation in empty environments. Enabling interaction with objects and uneven terrain could expand the applicability.- Tighter integration with downstream tasks like pose estimation and motion generation. The current pipeline uses off-the-shelf components for pose estimation and motion generation. A deeper integration could improve real-time performance and capabilities.- Applying the progressive reinforcement learning framework to other tasks like embodied agents and language grounding. The PMCP approach could enable scaling and transfer learning for other hard RL problems.- Exploring sequence-level information or future motion prediction to help learn highly dynamic motions within the overall dataset. The current per-frame policy struggles to learn certain motions that require planning. - Improving naturalness and smoothness during fail-state recovery. The current method can get up and resume tracking but exhibits some unnatural behaviors in this phase.- Enabling simulation of soft-body dynamics for more realism. The current method uses simplified capsule and mesh humanoids.In summary, potential future directions focus on improving imitation fidelity, integrating with other systems, applying the methods to new domains, and enhancing fail-state recovery behaviors. Overall it seems like an promising and generalizable approach that could enable many future applications.
