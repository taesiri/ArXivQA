# [Deep ReLU Networks Preserve Expected Length](https://arxiv.org/abs/2102.10492)

## What is the central research question or hypothesis that this paper addresses?

This paper proves bounds on the expected length/volume distortion of deep ReLU networks at initialization. The main results are:1) The expected length distortion does not grow exponentially with depth, contrary to prior belief. In fact, it decreases slightly with depth. 2) Higher moments of the length distortion, as well as volume distortion of higher dimensional manifolds, are also bounded and well-behaved.3) These theoretical bounds are tight and accurately predict the empirical behavior of networks at initialization, unlike previous looser lower bounds.The central hypothesis is that deep ReLU networks at standard initialization have low complexity in terms of how they distort lengths/volumes. This is in contrast to the belief that length distortion grows exponentially with depth, which was based on prior theoretical lower bounds that are shown here to be loose. The paper proves tight upper bounds showing length/volume distortion is well-controlled.


## What is the main contribution of this paper?

This paper proves upper bounds on the expected length and volume distortion of deep ReLU networks at initialization. The key contributions are:- It shows that the expected length distortion of a 1D curve does not grow with network depth, contrary to prior belief. In fact, it decreases slightly with depth. Previous lower bounds suggesting exponential growth were loose. - It generalizes this to prove upper bounds on higher moments of length distortion, showing they are well controlled. For example, the variance is bounded by the ratio of output to input dimension.- It extends the analysis to consider distortion of higher dimensional manifolds, proving analogous upper bounds on expected volume distortion. - It provides experiments confirming that the theoretical predictions accurately capture subtle dependencies on network architecture, while prior bounds fail to do so.Overall, this rigorous analysis shows that with proper initialization, the functions computed by deep ReLU networks have low complexity according to length/volume distortion measures. This counters the common intuition that depth necessarily leads to exponential complexity, providing a piece towards demystifying why deep networks can generalize well in practice despite high parameter counts.
