# [Understanding Self-supervised Learning with Dual Deep Networks](https://arxiv.org/abs/2010.00578)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we theoretically understand and analyze modern self-supervised learning (SSL) methods involving deep neural networks, in order to gain insight into how they learn meaningful representations starting from random initialization?Specifically, the paper focuses on analyzing SSL methods like SimCLR that use dual/Siamese networks during training. The key contributions are:- Proving that the weight updates in SimCLR amplify certain components based on a covariance operator that captures variability across data samples that survives averaging over data augmentations. This provides a principled framework to study feature learning in SSL.- Analyzing the covariance operator under different data distributions and augmentations, including a hierarchical latent tree model (HLTM) representing compositionality in images. The paper shows how hidden neurons can learn latent variables in the HLTM through the covariance operator, despite no direct supervision.- Providing experiments on CIFAR-10 and STL-10 that support the theoretical findings, including emergence of hierarchical features in synthetic HLTM data.In summary, the central research question is a theoretical characterization of modern deep SSL methods in order to understand the mechanisms by which they learn useful representations from scratch. The key insight is the role of the covariance operator in amplifying certain components based on variation across augmented data samples.


## What is the main contribution of this paper?

This paper proposes a theoretical framework for understanding contrastive self-supervised learning methods like SimCLR that employ dual pairs of deep ReLU networks. The main contributions are:1. It proves that in each SGD update of SimCLR, the weights at each layer are updated by a covariance operator that amplifies initial random selectivities that vary across data samples but survive averaging over data augmentations. 2. It models data generation and augmentation with a hierarchical latent tree model (HLTM) and shows that hidden neurons in deep ReLU networks can learn the latent variables in this model, despite no direct supervision from them. This demonstrates the emergence of hierarchical features through amplification of initial randomness.3. Experiments on CIFAR-10 and STL-10 support the theoretical findings. For example, experiments show that intermediate layers of deep ReLU networks trained on data from the HLTM learn to represent the latent variables, even without being directly supervised.In summary, this work provides a theoretical framework to understand how contrastive self-supervised learning like SimCLR can learn meaningful intermediate representations starting from random initialization, as a function of the data distribution and augmentation procedure. The covariance operator and analysis with the HLTM model are the key theoretical contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper provides a theoretical analysis of contrastive self-supervised learning methods like SimCLR that use dual deep neural networks, proving that the weight updates amplify initial random selectivity in directions that vary across data samples but are invariant to data augmentations, and this leads to the emergence of useful hierarchical representations despite no direct supervision.
