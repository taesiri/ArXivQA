# [Understanding Self-supervised Learning with Dual Deep Networks](https://arxiv.org/abs/2010.00578)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we theoretically understand and analyze modern self-supervised learning (SSL) methods involving deep neural networks, in order to gain insight into how they learn meaningful representations starting from random initialization?Specifically, the paper focuses on analyzing SSL methods like SimCLR that use dual/Siamese networks during training. The key contributions are:- Proving that the weight updates in SimCLR amplify certain components based on a covariance operator that captures variability across data samples that survives averaging over data augmentations. This provides a principled framework to study feature learning in SSL.- Analyzing the covariance operator under different data distributions and augmentations, including a hierarchical latent tree model (HLTM) representing compositionality in images. The paper shows how hidden neurons can learn latent variables in the HLTM through the covariance operator, despite no direct supervision.- Providing experiments on CIFAR-10 and STL-10 that support the theoretical findings, including emergence of hierarchical features in synthetic HLTM data.In summary, the central research question is a theoretical characterization of modern deep SSL methods in order to understand the mechanisms by which they learn useful representations from scratch. The key insight is the role of the covariance operator in amplifying certain components based on variation across augmented data samples.
