# [Grid-guided Neural Radiance Fields for Large Urban Scenes](https://arxiv.org/abs/2303.14001)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to achieve high-fidelity rendering of large urban scenes while being computationally efficient. 

The key idea is to combine the strengths of NeRF-based methods and feature grid representations in a unified framework to effectively model both global and local scene information. The two main hypotheses are:

1) A feature grid representation can coarsely yet reliably capture local scene contents like geometry and appearance. This can guide NeRF's sampling to focus on scene surfaces and provide intermediate features to enrich its pure coordinate inputs.

2) Jointly optimizing the feature grid representation along with the NeRF branch will further refine the grid features to be more accurate and compact. The NeRF branch acts as an implicit regularization to smooth noisy grid features.

Specifically, the paper proposes a two-branch model with a grid branch for compactly encoding multi-resolution ground plane features, and a NeRF branch that takes both grid features and positional encodings as input. The grid branch is first pretrained and then jointly optimized with the NeRF branch. This allows utilizing the strengths of both approaches - efficient and reliable local scene encoding through feature grids, and inherent continuity and detail recovery with NeRF's positional encoding and MLP network.

So in summary, this work aims to enable high-fidelity novel view synthesis for large urban scenes by unifying the feature grid and NeRF-based scene representations in a jointly optimized framework. The key hypotheses are on how the two branches can complement each other's limitations through joint training.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new framework for large urban scene rendering that combines a grid-based representation with a MLP-based neural radiance field (NeRF). 

2. It introduces a compact multi-resolution ground feature plane representation to capture coarse scene geometry and appearance. This allows efficiently modeling large urban scenes.

3. It proposes a two-branch model where one branch is the grid feature representation and the other is a light-weight NeRF that takes the grid features as input. This allows combining the benefits of both approaches.

4. It shows that jointly optimizing the two branches leads to high quality novel view synthesis results. The grid features help guide the NeRF sampling and provide local geometry/appearance information. The NeRF supervision refines the grid features.

5. Extensive experiments on large real-world urban datasets demonstrate the effectiveness of the approach over strong baselines like NeRF, MegaNeRF, and TensoRF. The method achieves photorealistic rendering quality even for very large scenes.

In summary, the key innovation is the joint grid-NeRF model and training process that enables high-fidelity novel view synthesis for large urban scenes by effectively combining the complementary strengths of grid and NeRF representations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel neural radiance field framework for photorealistic rendering of large urban scenes, combining a multi-resolution ground feature plane representation to capture coarse scene information with a lightweight NeRF branch that takes the grid features as input to refine details and encourage global consistency.
