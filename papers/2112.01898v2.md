# [Linear algebra with transformers](https://arxiv.org/abs/2112.01898v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question of this paper is:

Can transformers learn to perform numerical computations on matrices, such as matrix operations and decompositions, from examples only?

The paper investigates whether transformers can be trained on datasets of random matrices to accurately compute solutions to various linear algebra problems, including matrix transposition, addition, multiplication, inversion, and eigenvalue/singular value decomposition. The goal is to assess the capability of transformers to learn complex numerical computations, without being provided explicit algorithms or equations. The paper examines different encoding schemes to represent real numbers as tokens, model architectures, the impact of training set distribution, and the ability to generalize outside the training distribution.

Overall, the main hypothesis seems to be that transformers can in fact learn to accurately solve problems in linear algebra by training only on examples, despite some prior results showing transformers struggled with basic arithmetic. The author aims to demonstrate this capability and understand its limitations.
