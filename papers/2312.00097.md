# [SparseDC: Depth Completion from sparse and non-uniform inputs](https://arxiv.org/abs/2312.00097)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Mainstream depth completion methods perform well on benchmark datasets with fixed input depth distributions, but struggle with sparse and non-uniform real-world depth inputs. For example, they fail to maintain scene structure when given only 5 depth points or depth from shifting grid patterns. This is because they are not robust to varying sparsity and spatial patterns of input depths.

Proposed Solution (SparseDC):
The paper proposes a new depth completion framework called SparseDC focused on handling sparse and non-uniform inputs. The key ideas are:

1) Sparse Feature Filling Module (SFFM): Explicitly fills unstable depth features with stable image features to improve robustness. Helps network rely more on stable cues when depth is sparse/non-uniform.

2) Two-branch feature extraction: A CNN branch for precise local geometry and a Vision Transformer branch for accurate global structure. Handles tradeoff in relying on local vs global information. 

3) Uncertainty-Guided Feature Fusion Module (UFFM): Predicts per-pixel uncertainty to determine relative contribution of CNN and ViT branches based on depth sparsity patterns. Allows dynamically adjusting fusion.

Together these components enable extracting robust features to retain precise local geometry in regions with depth while guessing depth in missing regions from global structure.

Main Contributions:
- Analyze limitations of existing methods on sparse and non-uniform depth inputs 
- Propose the SparseDC framework with SFFM, two-branch architecture and UFFM to address limitations
- Demonstrate SparseDC outperforms state-of-the-art in depth completion across spatial patterns and sparsity levels on NYU Depth and KITTI datasets (17% better REL metric)
- Show increased robustness on real sensor data like LIDAR scans with varying vertical resolution
