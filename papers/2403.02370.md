# [adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource   Languages with Integrated LLM Playgrounds](https://arxiv.org/abs/2403.02370)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multilingual language models (MLLMs) have great potential to improve machine translation (MT), but their impact on low-resource languages is under-explored. 
- There is no open-source application dedicated to fine-tuning MLLMs and managing the full MT workflow for low-resource languages.

Proposed Solution:
- The authors develop "adaptMLLM", an open-source application to streamline all processes of fine-tuning MLLMs for MT. 
- It has an intuitive interface for customizing hyperparameters and model evaluation/deployment.
- It is designed as a cloud-based Google Colab notebook for easy sharing and configuration.

Main Contributions:
- Used adaptMLLM to fine-tune MLLM models for English-Irish and English-Marathi translation.
- Achieved significant BLEU score improvements over top models from LoResMT 2021 shared task.  
- For English-Irish, got 14% and 117% relative BLEU gains in the two translation directions.
- English-Marathi also saw major gains, especially 68% BLEU increase for Marathi-to-English.
- Performed detailed human evaluation using MQM and SQM metrics to analyze translation quality. 
- Overall, showed adaptMLLM can effectively fine-tune MLLMs and get state-of-the-art results for low-resource language pairs.
- The application and models are open-sourced to help MT community, especially newcomers.

In summary, the key innovation is the development of adaptMLLM to streamline fine-tuning MLLMs for low-resource MT, with impressive empirical gains demonstrated for sample languages. The open-source release would benefit the wider community.
