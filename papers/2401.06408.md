# [AboutMe: Using Self-Descriptions in Webpages to Document the Effects of   English Pretraining Data Filters](https://arxiv.org/abs/2401.06408)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are built using web data, but how this data is filtered and transformed is not well understood. In particular, assumptions around notions of "quality" and English language content can disproportionately exclude certain groups.

- There is a lack of large-scale language data tied to social and contextual metadata. Web data like Common Crawl lacks metadata about content creators. 

Methodology:  
- The authors create a new dataset, "AboutMe", by extracting 10.3 million "about me" style webpages from Common Crawl to understand text provenance.

- They extract information about webpages' topical interests, whether they were created by individuals or organizations, individuals' social roles, and geographic locations. This allows them to analyze effects of filters.

- They investigate 10 quality and English ID filters from prior work, using two scenarios: pages retained at high vs low score percentiles.

Key Findings:
- Quality filters behave differently, but tend to act as weak topical classifiers, favoring news or academic content. Some relate to occupational prestige.  

- English ID systems struggle with multilingual content and non-standard English, disproportionately filtering Asian regions.

- Individuals are preferred over organizations. Role-specific language increases polarization of scores.

Contributions:
- New dataset linking language to social contexts at an unprecedented scale
- First analysis investigating effects of common data filtering practices 
- Results reveal preferences and caveats around notions of “quality” and “English” during data curation
- Framework and analyses enable better documentation of data curation processes


## Summarize the paper in one sentence.

 This paper introduces a new dataset linking 10.3 million webpages to social attributes of their creators extracted from "about me" pages, and uses it to study the effects of common data filtering practices in large language model development on text from different social groups and geographic regions.


## What is the main contribution of this paper?

 The main contribution of this paper is:

1) Introducing a new dataset, \texttt{AboutMe}, that links 10.3 million webpages to information about the creators of those webpages, including their topical interests, social roles, and geographic locations. This allows for examining how data filters used in large language model pretraining affect text grounded to social and geographic contexts.

2) Conducting the first study investigating how ten common "quality" and English language identification filters impact webpages that vary along dimensions like creators' occupations, interests, and geography. The experiments uncover implicit preferences within data curation steps, showing for example that some quality classifiers resemble topical domain filters, while English ID can overlook English content from certain global regions.

3) Providing a framework and set of analyses that enable better understanding and documentation of the potential social implications of decisions made during the data preprocessing stages of large language model development. The authors argue that it is important to scrutinize the assumptions and tradeoffs in data curation pipelines, in order to build more responsible and appropriate language technologies.

In summary, the key innovation is in linking text to social context, which then allows for a novel investigation into the effects of data filtering on text from different populations and domains. The overall goal is to encourage more careful consideration of data curation practices.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, some of the key terms and concepts associated with this paper include:

- Large language models (LLMs)
- Pretraining data curation
- Data filtering 
- "Quality" filters
- English language identification (langID) filters
- Web text 
- Social dimensions of website creators (e.g. topical interests, social roles, geographic locations)
- Effects of data filters on varying types of web content
- Dataset of website "About Me" pages
- Implicit preferences in data curation
- Social implications of pretraining data curation

The paper introduces a new dataset called "AboutMe" which contains over 10 million website "About Me" pages. The authors use this dataset to study how common data filtering practices used in developing large language models may implicitly favor or disfavor certain types of web content based on the social dimensions of the website creators. The key variables investigated are creators' topical interests, social roles, and geographic locations. Overall, the paper aims to document the effects of different data curation decisions to encourage more careful consideration of their potential social impacts.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How did the authors identify "about me" webpages at scale from Common Crawl data? What preprocessing steps were involved and what challenges did they encounter?

2. The authors propose a new approach for social role extraction from "about me" pages using fine-tuned RoBERTa. What were the key ideas behind this approach and why was it more suitable than other information extraction methods? 

3. The paper demonstrates a creative way to obtain geographic information by geoparsing locations mentioned in the text. What NLP techniques did they leverage to associate websites with countries and regions? How did they handle disambiguation between locations with the same name?

4. What motivated the authors to analyze the effects of quality filters and English language ID systems? Why are these important elements in large language model development that warrant further investigation?  

5. How exactly did the authors simulate different data filtering scenarios to determine which websites were most/least affected? What cutoff criteria did they use for the various filters examined?

6. One interesting finding was that text specifically expressing one's social role seems to amplify model preferences during filtering. What might explain this phenomenon at a technical level?

7. Certain quality filters appear to optimize for topical domain fit - how might this impact the knowledge imparted to LLMs during pretraining? Could it exacerbate existing biases?

8. The analysis revealed subtle geographic preferences in filtering systems. What societal impacts might emerge if certain world regions are systemically excluded from pretraining data?  

9. How could the proposed framework and dataset enable future work to further scrutinize and improve data curation practices? What other analyses are now possible with linked demographic and text data?

10. What are some limitations of the methodology and evaluation presented in the paper? How can the robustness and reliability of findings be improved in follow-up work?
