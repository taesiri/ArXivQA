# [Effective Gradient Sample Size via Variation Estimation for Accelerating   Sharpness aware Minimization](https://arxiv.org/abs/2403.08821)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Sharpness Aware Minimization (SAM) is a recently proposed technique that improves model generalization by finding flat minima of the loss landscape. However, SAM requires calculating gradients twice per optimization step, which doubles the computation compared to standard SGD. This is inefficient especially for large models.

Proposed Solution:
- The key idea is that the gradient update in SAM can be decomposed into the SGD gradient and the Projection of Second-order gradient matrix onto First-order gradient (PSF). 
- It is observed that the PSF term exhibits increasing amplitude during training. This indicates its varying importance. 
- An adaptive sampling strategy called vSAM is proposed. It samples the PSF term adaptively based on its variance and reused previous PSF values during non-sampling steps.

Main Contributions:
- Show that SAM gradient can be decomposed into SGD and PSF terms, with PSF showing increasing amplitude during training
- Propose vSAM method which adaptively samples PSF based on its variance and reuses old PSF values to avoid double gradient calculations
- Achieve 40% speedup over SAM on CIFAR image classification while maintaining accuracy. Also show strong results on network quantization task.

In summary, the paper analyzes SAM updates, identifies the varying PSF term and proposes an efficient adaptive sampling technique called vSAM to accelerate SAM by avoiding redundant computations of PSF. Extensive experiments validate that vSAM can significantly improve optimization efficiency of SAM while preserving model accuracy across tasks.
