# [Random Sub-Samples Generation for Self-Supervised Real Image Denoising](https://arxiv.org/abs/2307.16825)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an effective self-supervised learning method for real image denoising that does not require paired noisy/clean training data? The key points are:- Supervised deep learning methods require paired noisy/clean images for training, which are difficult to obtain for real-world images. - Existing self-supervised methods like Noise2Void make assumptions (e.g. pixel-wise independent noise) that are not valid for real images.- The authors propose a new self-supervised framework called SDAP that uses random sub-samples generation and a cyclic sampling difference loss to achieve state-of-the-art results in real image denoising without needing paired training data.So in summary, the main research question is how to do effective self-supervised real image denoising without paired training data, which SDAP aims to address through its proposed methods.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- The paper proposes a new self-supervised real image denoising framework called Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) with a cyclic sample difference loss. - It provides a new perspective that adding appropriate perturbations to the training data can improve the performance of Blind-Spot Network (BSN) for self-supervised denoising. The paper suggests using the difference between sub-samples generated by sampling as a type of perturbation.- It proposes the random sub-samples generation (RSG) strategy to obtain more diverse sub-samples and make the sampling difference closer to random perturbation compared to pixel-shuffle downsampling (PD).- A new cyclic sampling difference BSN loss is proposed to make full use of the sub-samples generated by RSG.- Extensive experiments show the proposed method outperforms state-of-the-art self-supervised denoising methods on real-world datasets like SIDD and DND.In summary, the key novelty is proposing the idea of using sampling difference as perturbation for self-supervised real image denoising, as well as the associated techniques like RSG and the new cyclic loss function. The proposed SDAP framework achieves new state-of-the-art results on benchmark datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new self-supervised real image denoising framework called Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) and a cyclic sample difference loss, which outperforms state-of-the-art self-supervised denoising methods on real-world datasets without requiring paired training data.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in self-supervised image denoising:- This paper focuses on real image denoising, which is more challenging than denoising synthetic noisy images. Many previous self-supervised methods like Noise2Void are designed for synthetic noise and don't work as well on real images. - The paper points out limitations of prior self-supervised methods like AP-BSN that make assumptions about independent pixel noise. Real noise is spatially correlated so these methods don't translate well. This paper adapts the blind spot network idea to handle real image characteristics better.- The proposed Sampling Difference as Perturbation (SDAP) method introduces novel components like random sub-sample generation and a cyclic sampling difference loss. These provide more varied training data and help the network generalize better to real images.- Experiments show SDAP outperforms recent state-of-the-art self-supervised denoising methods on standard real image datasets like SIDD and DND. The gains over methods like AP-BSN and CVF-SID are significant.- SDAP achieves results competitive with some supervised methods that require paired noisy/clean training data which is difficult to obtain for real images. This demonstrates the promise of self-supervised techniques.Overall, this paper pushes self-supervised denoising capabilities forward, especially for real images. The sampling strategies and cyclic loss provide useful insights that could benefit other self-supervised approaches as well. The results demonstrate self-supervised methods can reach performance levels comparable to supervised techniques for this task.


## What future research directions do the authors suggest?

The authors suggest a few promising future research directions:1. Reviving more traditional self-supervised objectives in the deep learning era: The authors argue that many traditional self-supervised objectives like context prediction and solving jigsaw puzzles could be revisited and may achieve compelling performance with modern deep architectures. Exploring how to adapt these ideas to deep networks could lead to advances.2. Scaling self-supervision to more data and larger models: Self-supervision has made good progress but has not yet matched the performance of supervised learning on large benchmark datasets and models. Scaling up self-supervised techniques to work well with more data and larger models is an important direction.3. Combining self-supervised learning with semi-supervised learning: The authors argue that semi-supervised techniques that make use of limited labeled data together with unlabeled data are complementary to self-supervision. Finding optimal ways to combine these approaches is a promising direction.4. Utilizing recent contrastive learning insights: Contrastive self-supervised methods have shown marked improvements, suggesting value in better understanding and extending contrastive techniques. For example, exploring multi-view contrasting beyond image transforms.5. Improving transfer learning from self-supervised representations: While some methods show promising transfer results, transfer is less well understood for self-supervised techniques. Advancing transferability is key for widespread practical use.6. Combining self-supervision with different supervision types: Beyond combinations with standard supervised learning, exploring integrations with weak supervision forms like image tags or video narration could be fruitful.So in summary, some key future directions involve revisiting traditional pretext tasks, scaling up self-supervision, combining it with semi-supervised and transfer learning, utilizing contrastive techniques, and integrating it with varied supervision signals. Advances in these areas could help progress self-supervised learning.


## Summarize the paper in one paragraph.

The paper proposes a novel self-supervised real image denoising framework called Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) with a cyclic sample difference loss. It first analyzes the limitations of the blind spot network (BSN) when applied to real-world denoising and finds that adding appropriate perturbations to the training data can improve BSN's performance. The paper suggests using the difference between sub-samples generated by sampling as a type of perturbation. To obtain more random perturbations and sub-samples, it proposes an RSG strategy. Based on this, it introduces a new cyclic sampling difference loss for training BSN and proposes a new BSN framework combined with RSG. Experiments on real-world datasets like SIDD and DND show the proposed method significantly outperforms other state-of-the-art self-supervised denoising methods.
