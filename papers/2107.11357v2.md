# [Joint Shapley values: a measure of joint feature importance](https://arxiv.org/abs/2107.11357v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question of this paper is: how can we extend the Shapley value, a popular approach for measuring the importance of individual features, to measure the joint importance of sets of features? 

The Shapley value assigns a value to each feature that represents its average marginal contribution to a model's predictions. However, when features are correlated, looking at them individually can be misleading. The authors propose extending the axioms behind the Shapley value to sets of features to derive "joint Shapley values" that directly measure the contribution of feature sets. 

The key novelty is in how they extend the null axiom: in the original Shapley value, a null feature contributes nothing to any set it belongs to. For joint Shapley values, the authors propose that a null set contributes nothing to any set it belongs to, which allows capturing the importance of a set beyond its individual elements.

The paper introduces the joint axioms, proves the uniqueness of joint Shapley values satisfying those axioms, derives the joint Shapley value formula, and shows how joint Shapley values provide different insights than existing interaction indices in both game theory models and machine learning attribution problems.

In summary, the central hypothesis is that extending Shapley values to sets of features through appropriate axioms can produce a useful measure of joint feature importance in machine learning models. The paper introduces joint Shapley values and validates their utility theoretically and empirically.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It introduces the concept of "joint Shapley values", which directly extends the standard Shapley values to measure the importance of sets of features rather than individual features. 

2. It proves the uniqueness of joint Shapley values for any order of explanation k. Specifically, it shows that for each k there is a unique set of constants q_r that define the joint Shapley values.

3. It provides an axiomatic derivation of joint Shapley values by extending Shapley's original axioms to sets of features. This includes new axioms like "joint null" and "joint symmetry".

4. It illustrates joint Shapley values on game theory examples and machine learning attribution problems. The results demonstrate that joint Shapley values can provide complementary insights to existing interaction indices.

5. For models with binary features, it introduces a "presence-adjusted" global Shapley value that is more consistent with local feature attributions than the standard approach of averaging absolute local values.

In summary, the paper presents a novel and principled approach for extending feature attribution methods to sets of features, with theoretical results on uniqueness and applications demonstrating the usefulness of the approach. The joint Shapley values provide a new tool for understanding model predictions based on groups of correlated features.
