# [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of   Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a novel inference augmentation method called Attention Buckets to enhance the context awareness and tool use capabilities of large language models (LLMs) based on Transformers and rotary position embeddings (RoPE). The authors observe that the upper bound of the attention scores in these LLMs exhibits a waveform pattern, with peaks and troughs across sequence positions. They hypothesize and verify that placing critical information at attention troughs can degrade performance. To address this, Attention Buckets runs the LLM context through parallel processes with different RoPE bases, resulting in varied attention patterns that interleave peaks and troughs across positions. By aggregating the output distributions, Attention Buckets ensures attention peaks from one process can compensate for troughs in another. Experiments on a large-scale tool use benchmark show Attention Buckets augmented LLMs competitive with GPT-4 despite much smaller parameters. Exploratory experiments on open-domain QA also demonstrate accuracy improvements. The approach enhances context awareness for better tool use without changing model parameters or training.
