# [Model Interpretability through the Lens of Computational Complexity](https://arxiv.org/abs/2010.12265)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of formal notions to compare and quantify the interpretability of different machine learning models. Terms like "linear models are more interpretable than neural networks" are used loosely without a precise definition.

- The paper aims to propose a formally grounded definition of interpretability to allow comparing model classes.

Proposed Solution: 
- The paper relates interpretability of a model class to the computational complexity of answering "local post-hoc explainability queries". These queries try to explain the classification of a single input, e.g. "why was this input classified in this way?"

- The framework defines a model class C1 to be "more interpretable" than C2 if the complexity of answering such queries is provably lower for C1 than for C2. So higher complexity implies lower interpretability.

Main Contributions:
- Instantiation of the framework for queries like "minimum change required", "minimum sufficient reason", "counting completions" over model classes like decision trees, linear models, and MLPs.

- proofs that under this notion, decision trees and linear models are more interpretable than MLPs, formalizing common wisdom.

- Comparisons between decision trees and linear models are more nuanced - neither strictly dominates the other in terms of interpretability.

- Use of parameterized complexity to prove a difference in interpretability between shallow and deep MLPs - the first complexity-theoretic evidence that deeper models are harder to interpret.

In summary, the paper's main contribution is a complexity-theoretic framework to formally compare interpretability across model classes, which matches and provides theoretical grounding for some common beliefs about model interpretability.


## Summarize the paper in one sentence.

 This paper studies the computational complexity of answering local post-hoc explainability queries for different classes of machine learning models, in order to formally compare their interpretability.


## What is the main contribution of this paper?

 This paper presents a framework to formally compare the interpretability of different classes of machine learning models using concepts from computational complexity theory. The key ideas are:

1) The paper defines a notion of one class of models being "more interpretable" than another in terms of the complexity of answering certain "explainability queries" about the models. Specifically, if the complexity of answering queries for a class C1 is lower than for C2, then C1 is deemed more interpretable. 

2) The paper focuses on studying three important explainability queries - MinimumChangeRequired, MinimumSufficientReason, and CountCompletions - over three widely compared model classes in terms of interpretability: decision trees/FBDDs, linear models/perceptrons, and MLPs.

3) Under common complexity assumptions, the paper shows that decision trees/FBDDs are more interpretable than MLPs for all three queries, perceptrons are more interpretable than MLPs for two of the three queries, and the comparison between perceptrons and decision trees is ambiguous. 

4) The paper also applies techniques from parameterized complexity theory to show that shallow MLPs are theoretically more interpretable than deep MLPs. This required a finer-grained complexity analysis.

In summary, the main contribution is a complexity-theoretic framework to compare interpretability across model classes, along with an in-depth analysis that provides theoretical evidence for several folklore beliefs about model interpretability.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it include:

- Interpretability of machine learning models
- Explainability queries
- Computational complexity
- Local post-hoc explainability
- Binary decision diagrams (FBDDs)
- Perceptrons
- Multilayer perceptrons (MLPs)
- Activation functions (ReLU)
- Parameterized complexity
- Depth of neural networks
- Counting complexity
- Sufficient reasons
- Minimum changes required

The paper studies the computational complexity of answering different types of "explainability queries" on three classes of models - FBDDs, perceptrons, and MLPs. It aims to formally compare the interpretability of these models by relating it to the complexity of these queries. Key concepts include different notions of complexity classes, the depth of MLPs, counting problems, and types of local explanation methods like sufficient reasons and minimum changes. The use of parameterized complexity to separately analyze shallow and deep MLPs also seems like a key contribution. Overall, the paper provides a novel, complexity-theoretic perspective on model interpretability.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using computational complexity as a way to compare the interpretability of different classes of models. What are some limitations of using worst-case computational complexity as a measure of interpretability? For instance, does high worst-case complexity necessarily imply that a model is difficult to interpret in practice?

2. When proving the NP-hardness of the MinimumSufficientReason problem for MLPs, the authors reduce from the Shortest Implicant Core problem. Can you explain this reduction and the intuition behind it? What makes it nontrivial?

3. The paper utilizes the parameterized complexity framework and the W[t] hierarchy to separate the complexity of interpreting shallow vs deep MLPs. Can you explain the key ideas behind parameterized complexity and why it allows making finer-grained complexity distinctions compared to classical complexity classes? 

4. What is the high-level intuition behind the proof showing that interpreting an rMLP with 3t+3 layers is W(Maj)[t]-hard? Can you summarize the key steps in the reduction?

5. The paper proves that the CountCompletions problem is #P-complete for perceptrons but admits an FPRAS. What is an FPRAS and why does having one make the CountCompletions problem easier for perceptrons?

6. What modification did the authors make to the majority circuit construction by Goldmann and Karpinski to make it compatible with the W(Maj) hierarchy used in this paper? Can you explain why this modification was necessary?

7. The paper assumes the W(Maj) hierarchy is strict. What would be the implications for the results if this assumption turned out to be false?

8. The notion of c-interpretability refers to complexity-based interpretability. What are some ways this formal notion could be extended or improved to better capture model interpretability?

9. The paper focuses on local, post-hoc explanations for model interpretability. What are some other types of explanations that could be studied through the lens of complexity?

10. The paper instantiates the framework on binary classifiers. What are some challenges in extending the complexity analysis to other model types like regressors or classifiers with non-Boolean outputs?
