# [Extending Context Window of Large Language Models via Positional   Interpolation](https://arxiv.org/abs/2306.15595)

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, this paper presents Position Interpolation (PI) to extend the context window sizes of pretrained large language models (LLMs) like LLaMA up to 32768 tokens with minimal fine-tuning. The key hypothesis seems to be that interpolating position encodings to match the original context window range is more stable and easier for models to adapt to than directly extrapolating position encodings beyond the trained context length. The paper aims to demonstrate the effectiveness and efficiency of Position Interpolation for enabling longer context in LLMs.The main research questions/goals addressed in this paper appear to be:- Can Position Interpolation effectively extend LLM context window sizes to very long lengths (e.g. 32768 tokens) with minimal fine-tuning cost?- Can models extended via Position Interpolation make effective use of the much longer context for tasks like language modeling and long document summarization? - Does Position Interpolation preserve model quality relatively well on tasks within the original context window size?- Is Position Interpolation more stable and efficient than directly extrapolating position encodings beyond the trained context length?The central hypothesis seems to be that interpolating position encodings to match the original pre-training distribution is easier for models to adapt to than extrapolating to unseen positions, allowing efficient extension to much longer context windows. The paper aims to demonstrate this both theoretically and empirically.
