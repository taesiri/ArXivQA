# [PARIS: Part-level Reconstruction and Motion Analysis for Articulated   Objects](https://arxiv.org/abs/2308.07391)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we jointly perform part-level reconstruction and motion analysis for articulated objects from only multi-view RGB images observing the object in two different articulation states?In particular, the paper aims to address the coupled tasks of:1) Reconstructing the shape and appearance of the articulated parts of an object.2) Estimating the articulation motion parameters (joint type, axis, state) between the two observed states. The key ideas are to:- Learn separate implicit neural fields for the static and movable parts of the object.- Transform points sampled along camera rays between the two states using the estimated motion parameters in order to establish correspondence and extract the movable component. - Supervise the reconstruction and motion estimation in a self-supervised manner using only the input multi-view RGB images, without any 3D labels or motion supervision.The goal is to develop a method that can generalize to novel object categories in a category-agnostic manner, using only easily obtainable multi-view observations of the object in two states as input.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing PARIS, an end-to-end neural architecture for joint part-level reconstruction and motion analysis of articulated objects from multi-view RGB images. - A self-supervised approach that learns implicit shape and appearance models for the static and movable parts of an articulated object, while also estimating the motion parameters, without requiring any 3D supervision or motion/semantic annotation.- Demonstrating that the proposed method generalizes well across object categories, outperforming baselines and prior work that take 3D point clouds as input. PARIS achieves significant improvements in reconstruction quality and motion estimation accuracy.Specifically, the key aspects that differentiate PARIS seem to be:- Using only multi-view RGB images of an object in two static articulation states as input.- Learning separate neural fields for static and movable parts, compositing them using estimated motion parameters for self-supervision.- Being category-agnostic and not relying on any 3D data, motion parameters, or semantic supervision signals.- Reconstructing both shape and appearance at the part-level, while also estimating articulation motion parameters in an end-to-end manner.- Significantly outperforming baselines in quantitative evaluation on shape reconstruction and motion analysis, demonstrating good generalization across object categories.In summary, the main contribution appears to be proposing and demonstrating a self-supervised neural approach for jointly tackling the intertwined tasks of reconstruction and motion analysis for articulated objects from only RGB images.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes PARIS, a self-supervised end-to-end architecture that learns part-level implicit shape and appearance models and jointly optimizes motion parameters for articulated objects from multi-view images in two static states, without requiring any 3D supervision, motion, or semantic annotation.
