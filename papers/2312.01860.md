# [Unveiling Objects with SOLA: An Annotation-Free Image Search on the   Object Level for Automotive Data Sets](https://arxiv.org/abs/2312.01860)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper presents SOLA, a method for searching for specific objects within images in large automotive datasets. SOLA allows developers to query for rare or challenging objects to use for testing automated driving systems, without needing any manual annotation. It works by first using a panoptic segmentation model to detect objects in all images and extract them into cropped views. These object crops are then embedded into vectors using CLIP. At search time, the user provides a textual description of the desired kind of object. This text is also embedded with CLIP. Then, images are retrieved based on the similarity of their object embeddings to this text embedding. Experiments on automotive datasets demonstrate SOLA's ability to effectively retrieve rare objects like stretch limousines, fire trucks, and distracted pedestrians. The authors show it finds 31.6 more objects on average compared to searching full images with CLIP. By preprocessing the dataset and only running CLIP on the query text at search time, SOLA also has reasonable runtime. The paper discusses some limitations around false positives and the dependency on the object detector, but overall demonstrates the value of SOLA for accelerating development of automated driving systems by enabling precise search for challenging test cases.
