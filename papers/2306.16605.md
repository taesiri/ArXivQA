# KITE: Keypoint-Conditioned Policies for Semantic Manipulation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is how to enable robots to perform semantic manipulation - instruction following with both scene-level and object-level awareness. Specifically, the paper aims to tackle two key challenges:1) Enabling the robot to reason about what object/object part to manipulate based on natural language instructions that may reference objects or features in a visual scene (i.e. scene semantics) or parts of a particular object instance (i.e. object semantics).2) Determining how the robot can actually execute the desired manipulation behavior in 6D space after interpreting the instruction semantically. To address these challenges, the paper proposes KITE, a framework that grounds natural language instructions into 2D keypoints that identify objects or object parts, and then executes parameterized skills conditioned on those keypoints to perform precise 6D actions.The key hypothesis is that using 2D keypoints as an intermediate representation between language and low-level actions will allow for sample-efficient learning of semantic manipulation behaviors that generalize well to new scenes and objects compared to prior end-to-end approaches. The experiments on real-world tabletop, grasping, and coffee-making scenarios aim to validate whether KITE can effectively exhibit scene and object semantic awareness and precision manipulation in practice.In summary, the core research question is how robots can perform complex instruction following that requires disambiguating semantics at both the scene level across objects and at the object level across parts, which KITE aims to solve through grounded keypoints and skills.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is the KITE framework for semantic manipulation. Specifically:- KITE proposes a two-step approach consisting of a grounding module to map natural language instructions and images to 2D keypoints, and an acting module that executes keypoint-conditioned skills to carry out 6-DOF actions. - The grounding module leverages a convolutional neural network with CLIP embeddings to predict precise 2D keypoints corresponding to objects/object parts mentioned in the language input. - The acting module refines these 2D keypoints into 6-DOF actions using a library of parameterized skills. Each skill consists of a waypoint policy and controller. The waypoint policies are PointNet++ models trained on demonstrations to output waypoint poses given a point cloud and 2D keypoint.- This combined pipeline of precise keypoint grounding and keypoint-conditioned skills enables KITE to follow free-form instructions with both scene-level semantics (distinguishing objects) and object-level semantics (identifying object parts).- KITE is shown to be sample-efficient, requiring only hundreds of examples to train the grounding model and less than 50 demos per skill. It generalizes well to new scenes and objects.- It is demonstrated on challenging real-world manipulation tasks like 6-DOF tabletop rearrangement, semantic grasping, and precise coffee making. KITE outperforms prior methods without keypoints or skills on these tasks.In summary, the key contribution is a new approach to semantic manipulation that leverages 2D keypoints and skills to efficiently follow free-form language instructions with precision and generalization.
