# [Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models](https://arxiv.org/abs/2402.14977)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Foundation models like CLIP and BERT have become the backbone of many AI systems and are deployed as cloud services. However, they are vulnerable to backdoor attacks where attackers can inject triggers so that the model produces adversary-desired outputs for inputs containing triggers. This makes downstream tasks built using the foundation model inherit vulnerabilities. Prior defenses either prevent backdoor injection before deployment or detect triggers at inference, but do not fix already deployed vulnerable models. 

Proposed Solution:
This paper proposes Mudjacking, the first method to patch already deployed foundation models to remove backdoors. It formulates the patching as an optimization problem with three goals:

1) Effectiveness: Fix the detected misclassification caused by the trigger. 

2) Locality: Minimize impact on correct predictions for other inputs.

3) Generalizability: Remove the influence of the detected trigger on new inputs.  

To achieve these, three loss terms are proposed:

1) Effectiveness loss: Quantifies similarity of embeddings for the misclassified input and a clean reference input.

2) Locality loss: Quantifies similarity of embeddings output by pre- and post-patching models for clean validation inputs.  

3) Generalizability loss: Quantifies robustness of post-patching model to the detected trigger.

The optimization minimizes a weighted sum of these losses using gradient descent based patching. A key challenge is identifying the trigger from the detected misclassified input. This is addressed by using interpretability methods to identify input components (pixels/words) that contribute the most to the misclassification.

Main Contributions:

1) First defense to patch foundation models to remove backdoor vulnerabilities using optimization based patching.

2) Novel formulation with effectiveness, locality and generalizability goals tailored to foundation models.  

3) Trigger reverse engineering method leveraging interpretability techniques.

4) Comprehensive evaluation on multiple foundation model types, datasets and attacks showing effectiveness against adaptive attacks.

The method can patch foundation models to effectively mitigate backdoor attacks, while maintaining utility for benign inputs. It also fixes normal misclassifications as a side-effect.
