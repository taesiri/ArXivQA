# [IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks](https://arxiv.org/abs/2312.01771)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper introduces IMProv, an inpainting-based multimodal prompting model for in-context learning of computer vision tasks. The key idea is to train a masked autoencoder transformer on a new dataset of 268K figures from computer vision papers paired with captions, combined with the LAION-400M image-text dataset. During inference, the model receives a textual and/or visual prompt describing a task (e.g. "Left: input, Right: segmentation") and must fill in the output for a given input image. This allows adapting the model to new vision tasks without fine-tuning. Experiments demonstrate clear improvements over vision-only prompting, with over 10% higher AP on foreground segmentation, 5% on detection, and 20% lower LPIPS on colorization. The results also reveal an interesting trade-off between visual and textual prompts - high-quality text can reduce the need for curated image examples. Comparisons to few-shot learning approaches reveal the model trains in a more generalizable self-supervised fashion, closing over 40% of the gap from no base training data. The emergent in-context learning shows promise towards adaptable computer vision systems. Key limitations are needing high-quality textual prompts and not matching 1-shot supervised performance. Overall, the work makes excellent progress blending vision, language and in-context learning.
