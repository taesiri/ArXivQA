# Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large   Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can large language models (LLMs) be enhanced to better capture the non-sequential, graph-like nature of human reasoning and problem solving? The key hypothesis is that modeling reasoning processes as graphs rather than linear chains will allow LLMs to perform more complex, human-like deductive reasoning and generate more accurate answers to reasoning tasks.In particular, the paper proposes representing reasoning as a "Graph-of-Thought" (GoT) rather than just a Chain-of-Thought (CoT). The GoT models reasoning steps as connected nodes in a graph structure rather than a simple linear chain. The authors hypothesize that integrating GoT graph representations along with textual and visual features will improve the performance of LLMs on reasoning tasks compared to only using CoT chains or textual/visual features alone.The paper aims to test this central hypothesis by implementing GoT reasoning in LLMs and evaluating on text-only and multimodal reasoning datasets. The goal is to demonstrate that modeling reasoning as graph structures rather than chains enables more human-like deductive reasoning and stronger performance on complex reasoning tasks.In summary, the core research question is whether Graph-of-Thought can enhance reasoning abilities in LLMs beyond current Chain-of-Thought and multimodal methods. The central hypothesis is that the graph structure will better capture human reasoning.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing Graph-of-Thought (GoT) reasoning, a new approach to model human reasoning processes as graphs instead of simple chains. GoT represents thought units as nodes and connections between them as edges, capturing the non-sequential nature of human thinking. 2. Implementing GoT reasoning by constructing thought graphs with an Extract-Cluster-Coreference process and encoding the graphs with a graph attention network. The graph features are fused with text and visual features using a gated fusion mechanism.3. Evaluating GoT reasoning on the GSM8K and ScienceQA benchmarks. GoT significantly outperforms strong baselines like GPT-3, Multimodal-CoT, and even exceeds human performance on ScienceQA.4. Conducting comprehensive ablation studies and analysis to demonstrate the efficacy of GoT in improving reasoning and deductive abilities. The results show incorporating thought graph features and graph encoding enhances model performance, especially on complex, higher-level reasoning tasks.5. Establishing new state-of-the-art results on GSM8K and ScienceQA datasets using GoT reasoning, while using significantly fewer parameters than models like GPT-3 and GPT-4.In summary, the key contribution is proposing Graph-of-Thought to model reasoning as graphs instead of chains, and showing this approach effectively improves reasoning abilities of large language models like T5 with fewer parameters. The results demonstrate the potential of GoT in unlocking stronger reasoning for LLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Graph-of-Thought (GoT) reasoning, which models human thought processes as graphs rather than just chains, and shows this improves the reasoning abilities of large language models like T5 on complex tasks involving text, images, and logical deductions.
