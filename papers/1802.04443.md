# [On Characterizing the Capacity of Neural Networks using Algebraic   Topology](https://arxiv.org/abs/1802.04443)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we characterize and quantify the capacity of different neural network architectures to express topological complexity, and can this be used to guide architecture selection? The key hypothesis seems to be:The topological capacity of a neural network architecture, as measured by its ability to express the persistent homology of a dataset, is directly related to its ability to generalize well on that dataset. Architectures that lack the topological capacity to capture the complexity of a dataset will be fundamentally limited in how well they can learn to model the data.So the paper aims to formalize the notion of topological capacity using persistent homology, empirically characterize how it varies with architecture depth/width, and explore whether it can be used to select good architectures for a given dataset by matching topological complexity. The core idea is that architecture selection should be driven by understanding how complex an architecture needs to be to capture the inherent topological structure in the data.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:1. Framing the problem of neural network architecture selection in terms of understanding the "topological capacity" of different neural networks. The paper proposes using algebraic topology, specifically persistent homology, as a way to measure the complexity of a dataset. 2. Providing an empirical characterization of the topological capacity of different neural network architectures. The authors train networks of varying depth and width on synthetic datasets with increasing topological complexity. They analyze the ability of the networks to learn and express the topological features of the data.3. Using the empirical findings to make conjectures about bounds on the expressivity of neural networks, tightening known theoretical results.4. Introducing a method called "topological architecture selection" which uses persistent homology of a dataset to predict a suitable neural network architecture. This is validated on several real datasets.5. Demonstrating that common machine learning benchmark datasets have non-trivial topological complexity that can be captured using persistent homology.In summary, the main contribution seems to be proposing a novel perspective and set of techniques for neural architecture selection, grounded in algebraic topology and based on empirically characterizing the topological capacity of networks. The paper makes both theoretical and practical contributions in this direction.
