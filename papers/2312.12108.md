# [Knowledge Graph Error Detection with Contrastive Confidence Adaption](https://arxiv.org/abs/2312.12108)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Knowledge graphs (KGs) often contain noise and errors. Detecting these errors can improve the quality of KGs.  
- Existing KG error detection models rely mainly on graph structure and struggle to discriminate noise from semantically-similar correct triplets.
- Real-world noise is often semantically confusing and similar to correct triplets. Existing models do not handle such realistic noise effectively.

Proposed Solution:
- The paper proposes a new model called Contrastive Confidence Adaption (CCA) for KG error detection.
- CCA encodes both textual descriptions and graph structure to extract features and detect noise patterns.
- It uses a pre-trained language model (BERT) to encode textual information. A Transformer encoder is used for graph structure.
- Triplet reconstruction loss is used to evaluate triplet confidence by reconstructing head/tail entities from textual and structural features.
- An interactive contrastive learning module aligns textual and structural latent spaces and recognizes errors based on their differences.
- Knowledge fusion adaptively aggregates reconstruction and contrastive scores into confidence scores. These guide model training to alleviate noise interference.

Main Contributions:
- Proposes an end-to-end model to jointly leverage textual and structural information for KG error detection using reconstruction and contrastive learning.
- Designs interactive contrastive learning to align textual and structural latent spaces and detect anomalies. 
- Constructs semantically-similar noise and adversarial noise to evaluate model robustness.
- Outperforms state-of-the-art methods on benchmark datasets, especially on semantically-similar and adversarial noise.
