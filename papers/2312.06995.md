# [Transformer-based No-Reference Image Quality Assessment via Supervised   Contrastive Learning](https://arxiv.org/abs/2312.06995)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes SaTQA, a new no-reference image quality assessment (NR-IQA) method that combines supervised contrastive learning and Transformer architecture. The key ideas are: (1) Use supervised contrastive learning on a large-scale synthetic dataset (KADIS) to learn degradation features corresponding to different distortion types and levels. This allows the model to generalize better to real-world datasets. (2) Propose a Multi-Stream Block (MSB) module that combines CNN and Transformer features to leverage CNN's localization capability and Transformer's global modeling ability for robust distortion extraction. (3) Introduce a Patch Attention Block (PAB) to fuse the learned degradation features with the perceptual distortion features extracted by MSB. This further enhances quality prediction. Experiments on 7 IQA datasets, including challenging authentic distortion datasets, demonstrate state-of-the-art performance. The proposed innovations of contrastive pre-training, MSB heterogeneous architecture, and PAB feature fusion enable the model to effectively assess image quality for both synthetic and real-world distortions.
