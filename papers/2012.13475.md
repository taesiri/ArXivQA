# [Evolution Is All You Need: Phylogenetic Augmentation for Contrastive   Learning](https://arxiv.org/abs/2012.13475)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that using molecular evolution to generate augmented views of biological sequences for contrastive learning can produce useful sequence representations that capture functionally important properties. Specifically, the paper proposes using homologous sequences related through molecular evolution as positive pairs in a contrastive learning framework like SimCLR. This is analogous to using different augmentations of the same image as positive pairs in visual contrastive learning. The key hypotheses are:- Molecular evolution provides a good source of "views" for contrastive learning of sequence representations, similar to how image augmentations provide views of the same visual content. - Maximizing agreement between representations of homologous sequences will encourage the model to capture sequence properties important for conserved biological function.- This approach aligns well with principles of comparative genomics and the concept of evolutionary conservation of functional elements.So in summary, the main hypothesis is that contrastive learning using phylogenetic sequence relationships as natural data augmentation will produce useful biological sequence representations that capture functional properties, due to the underlying comparative genomics principles. Evaluating this hypothesis would involve empirical tests showing that representations learned this way capture meaningful biological properties compared to other self-supervised approaches.


## What is the main contribution of this paper?

This appears to be a draft paper that proposes using molecular evolution as a form of data augmentation for self-supervised contrastive learning of biological sequence representations. The key ideas are:- Contrastive learning frameworks like SimCLR can be adapted for biological sequences by treating homologous sequences related by evolution as different "views" of the same ancestral sequence. Maximizing agreement between these views encourages learning sequence representations that capture conserved functional properties.- Molecular evolution provides a natural form of augmentation that is well-suited for contrastive learning of biological sequences, both from an information theoretic perspective (evolution as a noisy channel) and a biological perspective (leveraging comparative genomics). - Theoretical results suggest evolutionary augmentation is a good choice to produce views that maximize mutual information about the ancestral sequence while minimizing shared nuisance factors.So in summary, the main contribution is proposing the use of phylogenetic augmentation during self-supervised pretraining as a way to learn informative sequence representations that capture evolutionary constraints. This provides a principled way to adapt contrastive learning approaches from vision/NLP to sequence biology.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using molecular evolution to generate augmented views of biological sequences for contrastive representation learning, which aligns with biological principles and theoretically optimizes mutual information between sequences and their conserved functions.


## How does this paper compare to other research in the same field?

This paper presents a novel method for self-supervised learning of biological sequence representations using phylogenetic relationships and contrastive learning. Here are a few key ways it compares to other research in this field:- Uses evolution/phylogeny for data augmentation: Most prior work uses augmentations borrowed from computer vision or generic noise injection. Leveraging homologs as natural augmentations is more tailored for biology.- Maximizes mutual information between views: Connects to information theory and the noisy channel coding analogy for molecular evolution. Contrasts with methods imported from NLP that lack an information theoretic grounding.- Does not require downstream task labels: Avoids expensive annotation by using evolutionary conservation as a proxy for functional properties of interest. Makes the method widely applicable. - Theoretical justification for evolutionary views: Beyond the biological motivation, evolutionary augmentation satisfies theoretical requirements for ideal views that maximize information about the input while minimizing shared information.- Illustrates with SimCLR framework: Adapts a leading contrastive self-supervised algorithm to biology with phylogenetic augmentation. Prior works have not examined latest contrastive methods.Overall, this paper makes important connections between evolutionary principles, information theory, and representation learning that can inspire more biologically-grounded self-supervised methods. The phylogenetic augmentation approach seems promising compared to existing techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing better mutual information estimators for use in contrastive learning frameworks. The authors note that the empirical success of methods like InfoNCE is not fully explained by their use as mutual information estimators. They suggest further research into estimators that provide tighter bounds on mutual information and help explain the success of contrastive losses.- Applying contrastive learning with evolutionary augmentation to real biological datasets. The authors propose the idea of using molecular evolution to generate views for contrastive learning, but do not implement or test it themselves. They suggest applying this approach in practice to evaluate its effectiveness.- Exploring other potential choices of "views" for contrastive learning of biological sequences. The authors focus on evolution as a view augmentation strategy, but note that other options could be explored as well.- Developing weakly-supervised or unsupervised strategies for selecting views based on the InfoMin principle. The InfoMin principle suggests selecting views to minimize mutual information while preserving task-relevant information. The authors suggest ways to approximate this without access to supervised labels.- Designing encoders and other architectural choices to best leverage evolutionary relationships in a contrastive learning framework. The overall approach is agnostic to encoder design, leaving room to experiment.- Evaluating whether contrastive learning with phylogenetic augmentation leads to useful representations for diverse downstream tasks. Assessing the general utility of the learned representations.- Exploring how ideas from contrastive learning can be integrated into other self-supervised objectives for biological sequences. Combining the strengths of multiple approaches.In summary, the authors lay out a vision and rationale for contrastive learning using phylogenetics, but suggest many open avenues for refining the approach and applying it to real biological data.
