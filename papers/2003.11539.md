# [Rethinking Few-Shot Image Classification: a Good Embedding Is All You   Need?](https://arxiv.org/abs/2003.11539)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether sophisticated meta-learning algorithms are actually needed for few-shot image classification, or if a simple baseline of learning good representations is sufficient. The key hypothesis is that a linear classifier trained on top of learned embeddings can match or exceed the performance of complex meta-learning methods on few-shot classification benchmarks.The paper tests this hypothesis through extensive experiments, showing that their proposed baseline of just using a linear model on learned embeddings sets new state-of-the-art results on several few-shot classification datasets. This suggests that much of the gains attributed to sophisticated meta-learning algorithms may actually just come from learning a good representation on the meta-training data, rather than the algorithms themselves. The paper argues this finding should motivate a rethinking of few-shot learning benchmarks and the role of meta-learning.In summary, the central hypothesis is that meta-learning algorithms provide little benefit over a simple baseline for few-shot image classification, and the key experiments support this by showing strong performance using just linear models on learned embeddings. The paper makes a case that representation learning, not complex metalearning methods, deserves more focus in few-shot image classification research.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes an extremely simple baseline for few-shot image classification that achieves state-of-the-art performance. The baseline involves learning a linear classifier on top of embeddings from a model pre-trained on the meta-training set. 2. It shows that self-distillation can further boost the performance of this simple baseline. Self-distillation involves distilling knowledge from a teacher model into an identical student model.3. The combined approach achieves around 3% average improvement over previous state-of-the-art on standard few-shot learning benchmarks. On the Meta-Dataset benchmark, it outperforms previous best by over 7% on average.4. It demonstrates that representations learned with self-supervised methods can match the performance of supervised methods for this baseline. This shows that good embeddings for few-shot learning can be learned without using labels.In summary, the key contribution is proposing an extremely simple but very effective baseline for few-shot learning that relies primarily on learning a good embedding model, rather than a complicated meta-learning algorithm. The results suggest that much of the recent progress on sophisticated meta-learning methods for few-shot image classification has brought limited gains over this simple approach.
