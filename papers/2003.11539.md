# [Rethinking Few-Shot Image Classification: a Good Embedding Is All You   Need?](https://arxiv.org/abs/2003.11539)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether sophisticated meta-learning algorithms are actually needed for few-shot image classification, or if a simple baseline of learning good representations is sufficient. The key hypothesis is that a linear classifier trained on top of learned embeddings can match or exceed the performance of complex meta-learning methods on few-shot classification benchmarks.The paper tests this hypothesis through extensive experiments, showing that their proposed baseline of just using a linear model on learned embeddings sets new state-of-the-art results on several few-shot classification datasets. This suggests that much of the gains attributed to sophisticated meta-learning algorithms may actually just come from learning a good representation on the meta-training data, rather than the algorithms themselves. The paper argues this finding should motivate a rethinking of few-shot learning benchmarks and the role of meta-learning.In summary, the central hypothesis is that meta-learning algorithms provide little benefit over a simple baseline for few-shot image classification, and the key experiments support this by showing strong performance using just linear models on learned embeddings. The paper makes a case that representation learning, not complex metalearning methods, deserves more focus in few-shot image classification research.
