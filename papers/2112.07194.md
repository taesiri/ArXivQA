# MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue   Evaluation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions addressed in this paper are:1. How can an automatic dialogue evaluation metric (ADM) learn the skill to accurately discriminate between good and bad dialogue responses? 2. How can an ADM generalize this evaluation skill across multiple dialogue domains?The paper proposes a framework called MDD-Eval to tackle these research questions. The key ideas are:- First train a "teacher" model on human-annotated data from a single domain to acquire a good rating skill for response quality discrimination. - Then use this teacher model to label a large amount of augmented multi-domain data. - Finally, train a "student" model on this pseudo-labeled multi-domain data to learn to generalize the rating skill across domains.So in summary, the central hypothesis is that by combining learning from human annotations on a small in-domain dataset and pseudo-labeled multi-domain data, an ADM can acquire both an accurate rating skill as well as the ability to generalize this skill across dialogue domains. MDD-Eval is proposed as a framework to achieve this.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a framework called MDD-Eval for automatic dialogue evaluation across multiple domains. The key ideas are:- Train a teacher model on human-annotated data from a single domain to acquire a rating skill to discriminate good and bad dialogue responses. - Augment a large multi-domain dialogue dataset using techniques like back-translation, generative models, etc. - Train a student model on the augmented dataset using self-training and consistency regularization to generalize the rating skill across domains.- The student model serves as the basis for a robust automatic dialogue evaluation metric applicable to multiple domains.In summary, the main contribution is proposing a self-training framework called MDD-Eval to learn a generalized dialogue evaluation skill from a teacher model trained on limited annotated data and a large augmented multi-domain dataset. Experiments show MDD-Eval achieves much better correlation with human judgments compared to prior automatic dialogue evaluation metrics on multiple benchmarks.
