# [FLAP: Flow Adhering Planning with Constrained Decoding in LLMs](https://arxiv.org/abs/2403.05766)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies the problem of faithful planning in task-oriented dialog systems to resolve user queries. Specifically, the goal is to generate a sequence of actions (APIs) that follows predefined natural language workflows and preserves API dependencies, in order to fulfill a user's request. This is challenging for large language models (LLMs) as they tend to deviate from instructions due to bias from their pretraining data. Additionally, workflows and APIs change frequently in real-world systems, requiring efficient adaptation.

Proposed Solution - FLAP Algorithm:
The paper proposes FLAP, a novel decoding algorithm that uses constrained beam search to generate faithful plans. FLAP works by constructing dependency graphs of workflows and APIs. During decoding, it looks ahead to score candidate tokens based on whether the corresponding workflow steps and APIs are valid given the dependency graphs. Additional alignment scores are computed between thoughts, APIs, workflows and user query to improve coherence. Structural constraints are added to enforce a specific plan format. FLAP does not require finetuning on domain data.

Contributions:
- Formulates a new task of zero-shot, faithful planning in task-oriented dialog systems using predefined workflows and APIs 
- Proposes FLAP, a constrained decoding algorithm using lookahead heuristics that improves plan faithfulness without finetuning
- Constructs a dataset with rich workflows, APIs and dependencies across 4 domains and 260 test queries
- Achieves strong improvements over baselines in plan correctness. Comparable performance to much larger LLMs using only 7B parameter models with FLAP
- Provides ablation studies analyzing different decoding methods and components of FLAP

Overall, the paper addresses an important real-world problem of following customized business workflows and API dependencies in dialog systems, and provides an effective solution through constrained decoding. The proposed FLAP algorithm and analysis advance the state-of-the-art in controlled text generation from LLMs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel flow adhering planning algorithm, FLAP, based on constrained decoding with lookahead heuristic that generates faithful plans resolving user intents by following predefined natural language workflows and preserving API dependencies in task-oriented dialog systems, outperforming prompting-based baselines.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FLAP, a novel Flow Adhering Planning algorithm using constrained decoding of large language models (LLMs) with lookahead heuristic. Specifically:

- The paper studies the novel problem of zero-shot faithful planning in task-oriented dialogues to adhere to predefined natural language flows and API dependencies. 

- It proposes FLAP, a constrained decoding algorithm that uses lookahead heuristic to enforce faithfulness of the generated plans to the given flows and API dependencies. FLAP does not require any finetuning of LLMs.

- The paper creates a new dataset with flows, APIs and user queries for evaluating faithful planning. 

- Experiments show that FLAP outperforms prompting-based baselines with standard decoding methods. By applying FLAP on top of smaller LLMs, the paper is able to achieve comparable performance to larger LLMs for faithful planning.

In summary, the key contribution is proposing a novel constrained decoding algorithm FLAP to generate faithful plans from LLMs without requiring finetuning, along with creating a new dataset and benchmark for evaluating the task.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Task oriented dialogs (TODs)
- Faithful planning
- Predefined workflows/instructions
- API dependencies
- Lookahead heuristic
- Constrained decoding
- Large language models (LLMs)
- Zero-shot planning
- Thoughts
- API hallucination
- Flow adherence

The paper introduces the novel problem of "zero-shot faithful planning with LLMs to predefined NL flows and API dependencies in TODs". The key goal is to generate plans to fulfill user requests by following predefined natural language workflows/instructions and preserving dependencies between APIs. The proposed approach is a constrained decoding algorithm called FLAP that uses lookahead heuristics to enforce faithfulness during text generation from LLMs. Key aspects evaluated are adhering to workflows, avoiding API hallucinations, and handling task decomposition properly according to API dependencies. The approach does not require finetuning the base LLMs.

In summary, the key focus is on utilizing large language models for faithful planning in task-oriented conversational scenarios by adhering to provided instructions and dependencies.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a novel constrained decoding algorithm called FLAP for generating faithful plans. Can you explain in detail how the lookahead heuristic works in FLAP to enforce faithfulness constraints during decoding?

2. The paper constructs dependency graphs for flows and APIs which are used to determine permitted steps/APIs during plan generation. What are the key benefits of maintaining and updating such dependency graphs dynamically?

3. One of the evaluation metrics used is the percentage of inconsistent APIs in the generated plans. Why is this an important metric in assessing how well FLAP can preserve API dependencies compared to other baselines?

4. The paper finds that larger LLMs don't necessarily perform better than smaller LLMs when equipped with FLAP. What explanations are provided in the paper for this finding?

5. How exactly does FLAP align the generated thoughts to various elements like flows, APIs and user intent using semantic similarity? What is the motivation behind adding these different alignment objectives?

6. The paper studies the problem in a zero-shot prompting based setting without any finetuning. What are the practical challenges finetuning poses that a zero-shot approach could alleviate?

7. What simplifying assumptions does the paper make in defining the problem formulation and dataset creation? How could these be enhanced in future work for more complex, real-world scenarios?

8. The paper leaves dynamic planning based on user changes as future work. What modifications would need to be made to FLAP to adapt the plans dynamically based on user feedback or other signals? 

9. What practical deployment challenges in terms of runtime and parallelization does FLAP face currently? What solutions are discussed to address these challenges?

10. The paper uses only open-source LLMs for experiments. What additional experiments could be done with large proprietary LLMs to further analyze FLAP's performance?
