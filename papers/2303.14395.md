# [MDQE: Mining Discriminative Query Embeddings to Segment Occluded   Instances on Challenging Videos](https://arxiv.org/abs/2303.14395)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central research question this paper tries to address is:

How to improve video instance segmentation performance on challenging videos with occluded objects and crowded scenes? 

The key hypothesis is that mining discriminative query embeddings can help distinguish occluded instances in such challenging videos, leading to better segmentation performance compared to prior arts.

Specifically, the paper proposes two main ideas:

1. Initialize object queries with spatial-temporal priors by using grid-guided query selection and inter-frame query association. This provides better query embeddings to start with.

2. Use an inter-instance mask repulsion loss during training to force each query to not only match its target instance pixels but also suppress nearby non-target instance pixels. This teaches the model to distinguish between occluded instances.

The main evaluation validates these ideas on the challenging OVIS dataset containing occluded objects. The proposed MDQE method achieves significantly higher performance than prior per-frame and per-clip video instance segmentation methods, especially on occluded objects. This supports the hypothesis that mining discriminative query embeddings helps segment occluded instances.

In summary, the central hypothesis is around improving segmentation of occluded objects by better modeling query embeddings, which is validated through state-of-the-art results on a challenging dataset. The main ideas are query initialization and the proposed inter-instance loss.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing a method called MDQE (Mining Discriminative Query Embeddings) to improve video instance segmentation (VIS) on challenging videos with occluded objects and crowded scenes. 

2. Improving the initialization of object queries in VIS by proposing a grid-guided query selection method and inter-frame query association to specify discriminative spatial-temporal priors.

3. Proposing an inter-instance mask repulsion loss to teach the query-based segmenter to distinguish occluded instances by suppressing pixels of nearby non-target instances.

4. Achieving state-of-the-art VIS results on the challenging OVIS dataset using per-clip input, outperforming prior per-frame methods. MDQE achieves 33.0% mask AP on OVIS with ResNet50, which is the best result so far.

5. Obtaining competitive VIS performance on simple videos of YouTube-VIS dataset compared to state-of-the-art methods while using less parameters and faster speed. MDQE achieves 44.5% mask AP on YouTube-VIS 2021.

In summary, the main contribution is developing the MDQE method to effectively segment occluded instances in challenging videos by mining discriminative query embeddings using improved query initialization and inter-instance mask repulsion loss. MDQE sets new state-of-the-art results for per-clip VIS methods on the OVIS dataset.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a video instance segmentation method called MDQE that mines discriminative query embeddings to better segment occluded instances in challenging videos by initializing object queries with spatio-temporal priors and using an inter-instance mask repulsion loss for contrastive learning.
