# [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine learning models, especially large language models, are difficult to interpret and lack transparency into their reasoning. This is an issue for human-in-the-loop use cases where trust in model predictions is critical.

Proposed Solution: 
- The authors introduce "Retrieve to Explain" (R2E), a retrieval-based language model that scores possible answers to a query based on relevant evidence extracted from a document corpus. 
- R2E uses Shapley values to attribute the prediction to the most important pieces of evidence, enabling explainability. 
- The approach allows new evidence to be incorporated without retraining, and can leverage both free text and structured data templated into text.

Use Case & Results:
- The authors apply R2E to identifying promising drug targets from biomedical literature, to treat diseases. This is used as a test case for assessing model explainability and performance.
- R2E outperforms standard genetics-based and language model baselines at predicting outcomes of historical clinical trials, demonstrating practical usefulness. 
- Explanations are shown to correlate with expert and LLM assessments of evidence relevance.
- Explanations also enable auditing false positive predictions using LLM feedback, further improving performance.

Main Contributions:
- A new retrieval-based language model architecture optimized for explainability via evidence attribution.
- State-of-the-art performance at an important real-world task of drug target identification using scientific literature.
- Demonstration of how model explanations can be leveraged to improve predictions in a human- or LLM-in-the-loop setup.
