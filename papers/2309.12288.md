# [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org/abs/2309.12288)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Do large language models (LLMs) fail to generalize logical reversals of statements they are trained on? In particular, if an LLM is trained that "A is B", will it automatically be able to infer that "B is A"?The paper hypothesizes that LLMs suffer from what they term the "Reversal Curse", where models trained on facts in the forward direction ("A is B") fail to generalize to the reverse direction ("B is A"). The experiments in the paper aim to test for the existence of this hypothesized Reversal Curse across different LLM architectures and training regimes.In summary, the central research question is: Do LLMs suffer from the inability to automatically reverse factual statements they are trained on, a phenomenon dubbed the Reversal Curse? The paper presents experiments to test for this phenomenon.


## What is the main contribution of this paper?

Based on my reading, the key contributions of this paper seem to be:1. Identifying and characterizing a phenomenon called the "Reversal Curse" where language models trained on statements of the form "A is B" fail to automatically generalize to the reversed statement "B is A".2. Providing evidence for the Reversal Curse through finetuning experiments on synthetic datasets, where models trained on name-description pairs in one order struggle to answer questions with the reverse order. Models show near 0% accuracy on reversed questions.3. Demonstrating the Reversal Curse holds across model sizes, architectures, and training setups. Data augmentation and auxiliary training data does not seem to help models overcome it.4. Giving tentative evidence that the Reversal Curse impacts practical generalization, by testing ChatGPT on real celebrity-parent facts. The model answers questions correctly much more often in one direction vs the reversed direction.5. Raising questions about why auto-regressive LLMs suffer from the Reversal Curse, whether other model families exhibit it, and whether it reflects a similarity to ordering effects seen in human recall.In summary, the paper identifies and provides evidence for an intriguing failure of generalization in LLMs, which they term the Reversal Curse. It highlights issues with bidirectional reasoning and integration of logically equivalent facts.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in its field:- The paper focuses on studying the ability of large language models (LLMs) like GPT-3 to generalize facts bidirectionally. This is a relatively new line of research investigating the generalization capabilities and limitations of LLMs. - It provides direct empirical evidence for a phenomenon the authors term the "Reversal Curse", where LLMs fail to generalize factual statements from "A is B" to "B is A". This complements other recent work using influence functions to study bidirectional generalization.- The experiments are carefully designed using synthetic datasets to isolate the reversal generalization issue. This is a strength compared to analyzing real-world datasets where many confounds can obscure the effect.- The authors test several techniques like data augmentation and multitask learning to try to alleviate the Reversal Curse, but find the effect persists. This helps rule out some potential solutions.- The paper investigates the effect across model sizes, architectures, and tasks, demonstrating it is quite robust. This thoroughness in exploring the scope of the phenomenon is a contribution.- While other work has shown LLMs can exhibit inconsistencies or fail on negated statements, this paper uniquely identifies the bidirectionality failure, which is surprising given the logical equivalence of forward and reverse statements.- The experiments focus on factual knowledge, but the authors discuss how the issue may extend more broadly to other types of relations. Studying generalization for non-factual knowledge is an interesting direction for future work.Overall, this paper makes a significant contribution by carefully identifying and characterizing an intriguing generalization failure in LLMs. The scope and rigor of the experiments distinguishes this work and advances our understanding of these powerful models. It opens up many questions for further investigation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Studying other types of relations to see if models also fail to reverse them, such as logical implications, spatial relationships, or n-place relations.- Using entity linking to find real examples in large pretrained language models' training sets where information only occurs in one direction, in order to analyze the practical impact of the reversal curse. - Further analyzing the mechanics behind the reversal failures in transformers to understand what properties prevent them from generalizing in this way.- Exploring whether the reversal curse hinders model performance on downstream tasks by preventing integration of information learned in different orders.- Testing whether non-autoregressive language models also suffer from the reversal curse or if they can avoid it. - Comparing to humans to see if some analogous effect occurs, though the authors note the complete failure to reverse they find seems more extreme than in humans.- Explaining the reversal curse - why it occurs in the training process of autoregressive LLMs.In summary, the authors suggest further probing the scope, impact, mechanical causes, and potential solutions to the reversal curse across model architectures, tasks, and in comparison to human learning. Their results open up many avenues for better understanding generalization in LLMs.
