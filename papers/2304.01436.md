# [Learning Personalized High Quality Volumetric Head Avatars from   Monocular RGB Videos](https://arxiv.org/abs/2304.01436)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we learn high-quality 3D head avatars from monocular RGB videos that can be rendered with controllable expressions and viewpoints? The key points related to this question seem to be:- Existing methods for high-quality avatar generation require extensive hardware setups or manual intervention. The authors aim to generate avatars purely from short monocular RGB videos.- The authors propose a hybrid pipeline that combines a 3D morphable face model (3DMM) with a neural radiance field representation. This allows leveraging the 3DMM for geometry and tracking while using the neural radiance field for photorealistic rendering.- A core contribution is predicting expression-dependent features on the 3DMM mesh vertices to capture fine details. The features are predicted using a convolutional neural network in UV space for better generalization.- Experiments demonstrate they can reconstruct high-quality avatars from monocular video that have accurate expressions, details like wrinkles and hair, and can render novel views well. Comparisons to other state-of-the-art methods show improved quality.In summary, the main hypothesis is that by combining a 3DMM with a neural radiance field and using a convolutional network to predict localized expression-dependent features on the 3DMM vertices, they can learn controllable high-quality head avatars from monocular RGB video. The experiments aim to validate this approach.
