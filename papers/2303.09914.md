# [Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize More and   Forget Less](https://arxiv.org/abs/2303.09914)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:- It addresses the problem of face anti-spoofing (FAS) under the setting of domain continual learning (DCL). The goal is to enable FAS models to continually evolve when encountering new data domains, while avoiding catastrophic forgetting of previous domains.- The main research questions/hypotheses are:1) How to efficiently adapt FAS models to new domains with only a small amount of new training data, without accessing previous data? 2) How to improve the model's generalization capability to unseen domains?3) How to alleviate catastrophic forgetting of previous domains during continual learning, without storing previous data?- To address these questions, the main contributions are:1) Proposing a Dynamic Central Difference Convolutional Adapter (DCDCA) to efficiently adapt Vision Transformer models to new domains by extracting fine-grained features.2) A Proxy Prototype Contrastive Regularization method to improve generalization and reduce forgetting by constraining the model with "pseudo" prototypes from previous domains. 3) Extensive experiments on 15 datasets showing the proposed method generalizes better on unseen domains and forgets less compared to baselines.In summary, the core hypothesis is that by designing a more adaptive and generalizable model architecture, along with a regularization method using proxy prototypes, one can achieve better generalization and less forgetting in the challenging problem setting of rehearsal-free DCL for FAS. The experiments seem to validate these hypotheses.
