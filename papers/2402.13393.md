# [Fairness Risks for Group-conditionally Missing Demographics](https://arxiv.org/abs/2402.13393)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Most existing fair machine learning models require full knowledge of sensitive demographic features like gender or race. However, in many real-world applications, these features may be unavailable or only partially available due to privacy concerns, legal issues, or individuals' reluctance to share such information. The key challenge addressed in this paper is that the unavailability of demographics can depend on the demographic group itself. For example, people of certain age groups may be more reluctant to reveal their age for certain applications like hiring. 

Proposed Solution:
The paper proposes a semi-supervised variational autoencoder model called Fair-SS-VAE to address the problem of group-conditionally missing demographics in fair classification. The key ideas are:

1) Model the group-conditional missing mechanism using a probabilistic model P(observed demo | true demo). Add constraints on this model to prevent solutions like swapping groups.

2) Design a differentiable fairness risk measure that handles probabilistic demographic imputations, allowing it to be integrated with general semi-supervised learners. Address potential issues like manipulating imputed demographics to reduce fairness risk.  

3) Instantiate the semi-supervised learner as a VAE, with separate encoder and decoder networks for class label and demographics. Allow conditioning missing data on observed values.

4) Estimate expectations in fairness risk using Monte Carlo sampling for efficiency. Provide analysis of sample complexity.

Main Contributions:

1) Novel formulation and model for fair classification with group-conditionally missing sensitive demographics.

2) Techniques to avoid pathological solutions when jointly learning representations and missing data in VAE framework.

3) Efficient Monte Carlo approach to compute expectations in fairness risk measures.

4) Empirical demonstration that the proposed Fair-SS-VAE model outperforms state-of-the-art methods significantly in terms of fairness metrics on image and tabular datasets, while maintaining competitive accuracy.

In summary, the paper makes important contributions towards making fair ML models applicable to practical settings where sensitive demographic information can be partially missing in a group-dependent manner.
