# [Mercury: An Efficiency Benchmark for LLM Code Synthesis](https://arxiv.org/abs/2402.07844)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing NL2Code benchmarks focus on code correctness but overlook efficiency, which is critical for real-world usage. 
- They lack comprehensive test case coverage to fully validate functional correctness.
- They lack diversity and complexity in tasks, leading to performance ceiling where models yield indistinguishable results.

Proposed Solution - Mercury Benchmark:  
- First benchmark for assessing code efficiency of NL2Code models.
- Contains 1,889 programming tasks covering 3 difficulty levels.
- Integrates test case generators to produce unlimited test cases per task. 
- Introduces novel metric "Beyond@K" to measure normalized code efficiency based on historical submissions.

Key Contributions:
- Dataset: Introduces Mercury benchmark with task description, solutions, test case generators.
- Metric: Establishes "Beyond@K" as first efficiency metric for NL2Code.
- Optimization: Shows code efficiency gap remains despite advances in functional correctness.
- Analysis: Identifies generation, execution and test case errors contributing to benchmark failures.

Overall, the paper presents Mercury as the first dedicated benchmark for evaluating code efficiency for neural code synthesis models, highlighting it as an overlooked but critical dimension beyond just functional correctness. The benchmark and efficiency metric reveal significant room for improvement in state-of-the-art models.


## Summarize the paper in one sentence.

 This paper introduces Mercury, the first benchmark for evaluating code efficiency of large language models on code synthesis tasks, using a novel metric called Beyond@K that measures the runtime percentile of generated solutions compared to historical submissions.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Dataset: Introduction of Mercury, a novel dataset created for fine-tuning and assessing code efficiency in NL2Code tasks, along with an extensible open-source collection framework for enriching Mercury with more tasks and languages.

2) Metric: Establishment of the first efficiency-focused metric called $Beyond@K$, and evaluation of leading Code-LLMs using this criterion. 

3) Optimization: A comparative analysis of two baselines (Supervised Fine-Tuning and Direct Preference Optimization) aimed at enhancing code efficiency while maintaining functional correctness. The result shows that while LLMs excel in functional correctness, there is still significant potential to improve code efficiency.

So in summary, the main contribution is the proposal of Mercury - the first benchmark dataset and metric focused specifically on evaluating and optimizing the code efficiency of language models for code synthesis tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Code efficiency - The paper introduces a new benchmark called "Mercury" to specifically assess the code efficiency of language models for code synthesis tasks. Evaluating code efficiency is a main focus. 

- $Beyond@K$ metric - A novel evaluation metric proposed in the paper to measure normalized code efficiency by comparing a model's performance to historical human submissions.

- Code synthesis - The task of automatically generating executable code from natural language descriptions, which models like CodeT5, Codex, and CodeLlama are designed for.

- Functional correctness - Existing code synthesis benchmarks primarily measure whether the generated code is functionally/logically correct, which Mercury argues is insufficient.

- Test case generators - Mercury contains dedicated test case generators for each programming task, allowing the automated generation of unlimited test cases to thoroughly evaluate model-generated code.

- Code-LLMs - Large language models specialized for code, such as CodeT5, Codex, AlphaCode, and CodeLlama, which are evaluated in the paper.

- Supervised fine-tuning - One of the techniques explored in the paper to optimize code-LLMs for better efficiency while preserving functionality.

- Direct preference optimization (DPO) - Another technique explored in the paper to improve code efficiency by directly mapping human preferences without separate reward modeling.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new metric called "Beyond@K" to evaluate code efficiency. How is this metric calculated? What are the advantages and disadvantages of using this normalized metric compared to directly measuring runtime?

2. The paper employs a test case generator for each programming task to enable automatic generation of unlimited test cases. What considerations and techniques did the authors use when developing these generators? How does this approach for comprehensive test case coverage compare to other methods?

3. The paper finds that while language models can generate functionally correct code, there is still a significant gap in efficiency output. What underlying deficiencies in the models might be causing this gap? How can models be improved to generate more optimized code? 

4. The Direct Preference Optimization (DPO) method outperformed supervised fine-tuning (SFT) on larger models in both functional correctness and efficiency. Why might DPO be better suited to large models? What are the tradeoffs between these two optimization approaches?

5. The failure analysis shows execution errors increased after SFT while DPO maintained logical correctness better. What might explain these differential effects? Does this suggest potential drawbacks of SFT to be mindful of?

6. The benchmark comprises tasks filtered by number of solutions, restricted data structures, and unique outputs. How do these filters improve dataset quality and evaluation? What impact might they have on task difficulty and diversity?  

7. The paper employed 4-bit model quantization and other optimizations for training. How do these methods work and what advantages did they provide? What hyperparameters were used?

8. The benchmark difficulty levels correlated with lower code efficiency scores. Is this expected? How should model benchmarks consider balancing difficulty with efficiency evaluation?

9. The paper focuses on Python but the framework supports multiple languages. What considerations are needed to expand the benchmark to other languages? Would the efficiency principles transfer?

10. The historical solution data comes from Leetcode submissions. How might the distribution of solutions impact efficiency modeling? Could other data sources complement this?
