# [On the Effectiveness of Machine Learning-based Call Graph Pruning: An   Empirical Study](https://arxiv.org/abs/2402.07294)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Constructing precise static call graphs is challenging due to over-approximation leading to false edges. Recent ML-based call graph pruning approaches suffer from: (1) lack of suitable benchmark datasets, (2) imbalanced training data due to over-approximation, (3) significant recall drop after pruning, and (4) no comparison with context-sensitive call graph algorithms.

Proposed Solution:
- Introduced NYXCorpus benchmark by combining 3 datasets - NJR-1, XCorpus and YCorpus (added new dataset YCorpus)
- Use conservative strategy during training and inference to improve recall 
- Compared pruned 0-CFA call graphs with 1-CFA call graphs
- Evaluated pruned call graphs on a vulnerability analysis use case

Main Contributions:
- Created new benchmark dataset NYXCorpus with Java programs of various sizes including real-world ones
- Adapted ML models to support weighted training and customizable pruning via confidence levels
- Presented an empirical study evaluating effectiveness of ML-based call graph pruning, studying current issues, proposing solutions like conservative strategies, and evaluating their effects
- Showed pruned call graphs have comparable quality to 1-CFA analysis while being 3.5x faster with 69% smaller size, with virtually unchanged vulnerability analysis results

In summary, the paper identifies issues with current ML-based call graph pruning approaches, proposes enhancements to address them, introduces a new benchmark dataset, and empirically evaluates the solutions, demonstrating improved precision while maintaining high recall and use case applicability.


## Summarize the paper in one sentence.

 This paper empirically studies machine learning-based call graph pruning techniques, identifying key issues with current approaches, proposing solutions, and evaluating their effects on precision, recall, and practical usage.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Creation of a new benchmark dataset, NYXCorpus, from pre-existing datasets and tailored it to the call graph pruning task. It has Java programs of various sizes including real-world ones.

2. Adaptation of existing ML models to support weighted training and customizable pruning through confidence levels.

3. Presentation of an empirical study on the effectiveness of ML-based call graph pruning, which studies current issues, proposes solutions, and evaluates their effects.

In summary, the paper introduces a new dataset, adapts ML models for call graph pruning, and conducts an in-depth empirical study to assess the performance of ML-based call graph pruning techniques. The study identifies issues with existing approaches and proposes enhancements to address them.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms associated with this paper include:

- Call graphs: Represent function calls within programs and a crucial component for static program analysis. Constructing precise call graphs is challenging.

- Machine learning: Used to build data-driven models to prune false edges from static call graphs as a post-processing step, improving precision.

- Pruning: Removing unnecessary edges from static call graphs using machine learning models to improve precision while retaining true edges. 

- Benchmark dataset: The paper introduces NYXCorpus, combining three existing Java program datasets - NJR-1, XCorpus, and YCorpus for evaluation.

- Conservative pruning: Strategies like weighted model training and configurable inference thresholds to favor retaining more edges, improving recall of pruned call graphs. 

- Context sensitivity: Comparison of pruned call graphs from context-insensitive analysis (0-CFA) vs more advanced context-sensitive analysis (1-CFA).

- Security analysis: Evaluating practical impact of pruned call graphs on a vulnerability propagation use case, showing reduced graph size and faster analysis time.

- Empirical study: Comprehensive investigation of machine learning based call graph pruning using multiple experiments and evaluation of research questions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new dataset called NYXCorpus by combining 3 existing datasets. What is the motivation behind creating this new dataset and how is it more suitable for training and evaluating ML-based call graph pruning models compared to using the individual datasets?

2. The paper talks about two strategies - weighted training and confidence thresholding - to make the pruning more conservative so as to favor recall over precision. Can you explain the intuition behind both these strategies and how they help in retaining more true edges?  

3. The models use both structural and semantic features for the ML-based pruning task. Can you explain what each of these features capture and why using them together can be more effective than using them individually?

4. Context-sensitive call graph construction algorithms like 1-CFA have higher precision than simpler algorithms like 0-CFA. How does the performance of ML-based pruning on top of 0-CFA based call graphs compare with just using 1-CFA based call graphs in terms of quality and runtime?

5. The vulnerability analysis use case highlights the practical benefits of using pruned call graphs. Can you explain the tradeoffs involved between the conservative and paranoid pruning strategies in terms of analysis accuracy and performance?

6. What are some of the challenges faced while training ML models for the call graph pruning task as highlighted in the discussion section? How can these challenges be addressed in future work?

7. The paper filters certain edges like ones involving GUI components before the training process. What could be the limitations of this filtering strategy? How can more robust filtering be performed in the future?  

8. The features used currently are based on the entire source code of the caller and callee methods. Why may this be suboptimal? What are some ideas discussed to engineer more useful semantic features?

9. One interesting direction suggested is to integrate ML-based techniques into static analysis instead of having them as separate steps. Can you explain with examples how this could lead to improvements?

10. What are some of the threats to validity highlighted for the empirical study presented in this paper? How have the authors tried to address them?
