# [Truly No-Regret Learning in Constrained MDPs](https://arxiv.org/abs/2402.15776)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of safe reinforcement learning in constrained Markov decision processes (CMDPs). CMDPs model safety constraints by having multiple expected cumulative reward signals that need to lie above respective thresholds. Most state-of-the-art algorithms for CMDPs are based on primal-dual optimization. However, their regret bounds allow for "error cancellations", where a constraint violation in one round can be compensated by a strict satisfaction in another. This makes the learning process unsafe, as safety is only guaranteed for the final policy but not during learning. It has been an open question whether primal-dual algorithms can achieve sublinear regret without allowing such cancellations.

Proposed Solution:
The paper proposes a regularized primal-dual algorithm based on adding an entropy term to the Lagrangian of the CMDP. This induces strong convexity and concavity to enable last-iterate convergence. The algorithm maintains optimistic estimates of the unknown environment and applies truncated policy evaluation to compute regularized value functions. 

Main Contributions:
- Proves last-iterate convergence of a regularized primal-dual scheme in CMDPs with an arbitrary number of constraints.
- Proposes the first primal-dual algorithm that achieves sublinear strong regret in unknown CMDPs, without allowing error cancellations. This provides the first positive answer to the open question.
- Establishes an $\tilde{O}(T^{0.93})$ strong regret bound.
- Empirically confirms that vanilla primal-dual methods can suffer linear strong regret, validating that error cancellations are not merely a theoretical issue.

By addressing a long-standing open problem, the paper makes an important step toward designing primal-dual algorithms that provide safety guarantees during learning. The analysis introduces new proof techniques that may inspire further improvements.
