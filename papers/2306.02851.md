# [Scene as Occupancy](https://arxiv.org/abs/2306.02851)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:

Can 3D occupancy serve as an effective unified scene representation to enhance both perception and planning in autonomous driving systems? 

The key hypotheses appear to be:

1) 3D occupancy can capture finer details of the physical world compared to alternatives like 3D boxes. This can benefit perception tasks like detection and segmentation.

2) More accurate perception translates to improved planning performance downstream. So a better scene representation like 3D occupancy can improve planning. 

3) Dense 3D occupancy supervision is more effective than sparse supervision for training perception models.

To evaluate these hypotheses, the paper proposes OccNet to generate an occupancy descriptor from images, and OpenOcc as a benchmark dataset with dense occupancy labels. Experiments demonstrate advantages of the occupancy representation and OccNet framework for multiple tasks.

In summary, the central thrust is examining if a dense 3D occupancy representation of the full scene can enhance perception and planning accuracy compared to prior more limited or sparse representations. The method and experiments provide evidence towards confirming this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes OccNet, a multi-view vision-centric pipeline to reconstruct 3D occupancy from images using a cascade voxel decoder. OccNet generates an occupancy descriptor that supports multiple driving tasks like detection, segmentation and planning.

2. It introduces OpenOcc, the first large-scale 3D occupancy benchmark built on nuScenes with dense annotations. OpenOcc has over 1.4 billion annotated voxels across 16 classes and also provides flow annotations.

3. It demonstrates through experiments that 3D occupancy is a better scene representation compared to alternatives like 3D boxes and BEV segmentation. Key results:

- OccNet outperforms prior work like TPVFormer on semantic occupancy prediction by a relative improvement of 14%. 

- Detection models pre-trained on OccNet show ~10 points gain when fine-tuned on small data.

- Motion planning based on OccNet's occupancy reduces collision rate by 15-58% over planning with BEV or boxes.

In summary, the main contribution is presenting OccNet and OpenOcc to show 3D occupancy is a superior representation for perception and planning in autonomous driving. The cascade voxel decoder and dense benchmark enable this analysis.
