# [Federated Full-Parameter Tuning of Billion-Sized Language Models with   Communication Cost under 18 Kilobytes](https://arxiv.org/abs/2312.06353)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel federated learning approach called FedKSeed that enables full-parameter tuning of billion-sized language models on end devices. The key innovation is the use of zeroth-order optimization with a limited set of random seeds for gradient estimation, which allows transmitting only the scalar gradients and seeds between server and clients instead of the full model parameters. This drastically reduces communication overhead to under 18 kilobytes per round. FedKSeed introduces a seed reuse paradigm so clients only need to perform a small number of steps to obtain the latest model state. An enhanced version called FedKSeed-Pro further prioritizes more impactful seeds via non-uniform sampling probabilities calculated from the scalar gradients. Experiments on multiple datasets and models demonstrate FedKSeed and FedKSeed-Pro achieve higher accuracy than existing federated tuning methods, with thousands times less communication cost and equal memory usage to inference. The work makes full-parameter language model tuning feasible on devices in a privacy-preserving federated setting.
