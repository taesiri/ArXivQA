# [Federated Full-Parameter Tuning of Billion-Sized Language Models with   Communication Cost under 18 Kilobytes](https://arxiv.org/abs/2312.06353)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel federated learning approach called FedKSeed that enables full-parameter tuning of billion-sized language models on end devices. The key innovation is the use of zeroth-order optimization with a limited set of random seeds for gradient estimation, which allows transmitting only the scalar gradients and seeds between server and clients instead of the full model parameters. This drastically reduces communication overhead to under 18 kilobytes per round. FedKSeed introduces a seed reuse paradigm so clients only need to perform a small number of steps to obtain the latest model state. An enhanced version called FedKSeed-Pro further prioritizes more impactful seeds via non-uniform sampling probabilities calculated from the scalar gradients. Experiments on multiple datasets and models demonstrate FedKSeed and FedKSeed-Pro achieve higher accuracy than existing federated tuning methods, with thousands times less communication cost and equal memory usage to inference. The work makes full-parameter language model tuning feasible on devices in a privacy-preserving federated setting.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

\textbf{Problem:}
Pre-trained large language models (LLMs) require fine-tuning to improve their responsiveness to natural language instructions. Federated learning (FL) provides a way to perform fine-tuning using the abundant data on end devices without compromising data privacy. However, most existing federated fine-tuning methods rely on parameter-efficient fine-tuning techniques, which may not achieve the performance of full-parameter tuning. Full-parameter tuning incurs massive communication overhead and is infeasible for billion-sized LLMs on devices.

\textbf{Proposed Solution: }
The paper proposes \app, a novel federated learning approach that enables full-parameter tuning of billion-sized LLMs directly on devices. The key ideas are:

(1) Use zeroth-order optimization (ZOO) to estimate gradients, avoiding backpropagation and its high memory cost. 

(2) Limit perturbation seeds to a small set of $K$ candidate seeds and accumulate the corresponding scalar gradients on the server. This allows model updates to be replicated using just the seed and scalar gradient, reducing communication to a few KB per round.

(3) Assess significance of perturbations by the average amplitude of their scalar gradients. Assign higher sampling probabilities to more significant seeds. This further reduces the number of required seeds without sacrificing accuracy.

\textbf{Main Contributions:}

(1) Propose \app that enables federated full-parameter tuning of billion-sized LLMs with communication cost under 18 KB per round and memory usage equal to that for inference.

(2) Introduce a strategy to differentiate seed importance based on scalar gradient information. Probability-differentiated seed sampling further boosts efficiency and accuracy.  

(3) Extensive experiments on two datasets and two LLMs demonstrate that \app outperforms existing federated LLM tuning methods in accuracy, communication efficiency and memory usage. The improvements are more significant on larger LLMs.

In summary, the paper makes federated full-parameter tuning practical for billion-sized LLMs on devices through an innovative ZOO-based approach and probability-differentiated seed sampling. The experimental results validate the superiority of the proposed solutions.
