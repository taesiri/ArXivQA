# [Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity   Tracking](https://arxiv.org/abs/2402.14811)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Studied:
The paper investigates how fine-tuning affects the internal mechanisms and computations within large language models (LLMs). Specifically, it explores the effect of fine-tuning on the models' ability to perform "entity tracking" - tracing entities and their associated properties across long contextual sequences. 

Approach Taken:  
The authors use entity tracking as a case study task. They compare the base LLaMA model to fine-tuned versions on user conversations (Vicuna) and arithmetic tasks (Goat, FLoat). To understand the mechanisms, they identify the core "entity tracking circuit" within each model using path patching and evaluate if the same circuit exists across models. They then use desiderata-based component masking (DCM) to reveal the functionality of circuit components. Finally, they introduce a new technique - cross-model activation patching (CMAP) - to attribute improved performance to specific steps in the mechanism.

Key Findings:
1) The same entity tracking circuit is present in both base and fine-tuned models, with fine-tuned models having additional components.

2) The circuit implements the same core functionality in all models - tracking entities via their position and resolving positions to values. 

3) Fine-tuning enhances the ability of this mechanism to handle position and value information rather than fundamentally altering computations.

4) Specifically, CMAP reveals improved encoding of positions and retrieval of associated values within the common circuit underlies performance gains from fine-tuning.

Main Contributions:  
In summary, the key contributions are:

(i) Demonstrating mechanism consistency for entity tracking across base and tuned models 

(ii) Introducing techniques like DCM and CMAP to study mechanisms

(iii) Providing evidence that fine-tuning enhances existing capabilities in models rather than radically changing them

The findings suggest fine-tuning boosts performance by refining mechanisms in pretrained models versus introducing wholly different computations.
