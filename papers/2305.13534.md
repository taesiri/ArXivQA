# [How Language Model Hallucinations Can Snowball](https://arxiv.org/abs/2305.13534)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Do language models hallucinate incorrect claims not due to lack of knowledge, but rather to maintain consistency with previous hallucinations? In other words, do language models "snowball" hallucinations even when they have the capability to recognize those claims as false in isolation?The key hypothesis is that language models are susceptible to a phenomenon the authors term "hallucination snowballing", where a language model makes an initial mistake or hallucination, and then continues generating more incorrect claims in an attempt to justify the initial mistake and maintain coherence/consistency. The authors hypothesize that this occurs even for simple factual claims that the model has the knowledge to recognize as false when presented alone, demonstrating that hallucinations are not solely due to knowledge gaps.To test this hypothesis, the authors construct three QA datasets designed to elicit incorrect answers and justifications from the models, and then test whether the models can recognize the false claims made in the justifications when presented separately. The primality testing, senator search, and graph connectivity datasets are designed to probe this snowballing behavior.So in summary, the central research question is whether LMs hallucinate not just due to lack of knowledge, but also a tendency to snowball even simple mistakes - a phenomenon the authors term "hallucination snowballing". The hypotheses and experiments center around demonstrating and quantifying this effect.
