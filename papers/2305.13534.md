# [How Language Model Hallucinations Can Snowball](https://arxiv.org/abs/2305.13534)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Do language models hallucinate incorrect claims not due to lack of knowledge, but rather to maintain consistency with previous hallucinations? In other words, do language models "snowball" hallucinations even when they have the capability to recognize those claims as false in isolation?The key hypothesis is that language models are susceptible to a phenomenon the authors term "hallucination snowballing", where a language model makes an initial mistake or hallucination, and then continues generating more incorrect claims in an attempt to justify the initial mistake and maintain coherence/consistency. The authors hypothesize that this occurs even for simple factual claims that the model has the knowledge to recognize as false when presented alone, demonstrating that hallucinations are not solely due to knowledge gaps.To test this hypothesis, the authors construct three QA datasets designed to elicit incorrect answers and justifications from the models, and then test whether the models can recognize the false claims made in the justifications when presented separately. The primality testing, senator search, and graph connectivity datasets are designed to probe this snowballing behavior.So in summary, the central research question is whether LMs hallucinate not just due to lack of knowledge, but also a tendency to snowball even simple mistakes - a phenomenon the authors term "hallucination snowballing". The hypotheses and experiments center around demonstrating and quantifying this effect.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating the phenomenon of "hallucination snowballing" in large language models like ChatGPT and GPT-4. The key points are:- The paper shows that LMs often make incorrect claims not just due to lack of knowledge, but in order to remain consistent with an earlier mistake. - The authors construct 3 QA datasets where the LMs first state an incorrect answer, then justify it with an explanation containing false claims. Crucially, when presented alone, the LMs can identify those false claims as incorrect.- This "hallucination snowballing" happens because LMs are trained to maintain coherence, so once they commit to a wrong answer, they continue generating false facts to support it. - The authors recommend modifications to prompting, decoding, and training strategies to reduce snowballing. The core problem is that conditioning on faulty context leads LMs to produce simple mistakes they otherwise wouldn't.In summary, the key contribution is demonstrating and analyzing the specific phenomenon of hallucination snowballing, showing how consistency pressures cause LMs to compound early mistakes with further falsities. The authors provide insights into this failure mode and suggestions to mitigate it.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper demonstrates that large language models like ChatGPT and GPT-4 often make incorrect claims to justify previously generated incorrect answers, even though they can identify those claims as wrong when presented in isolation - a phenomenon the authors call "hallucination snowballing".
