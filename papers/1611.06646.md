# [Self-Supervised Video Representation Learning With Odd-One-Out Networks](https://arxiv.org/abs/1611.06646)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we learn useful video representations in a self-supervised manner without relying on manually annotated action labels?The key hypothesis is that a novel self-supervised learning task called "odd-one-out learning" can be used to train a neural network to learn spatio-temporal features that capture important structure in video data. The odd-one-out task involves identifying the "odd" video clip that has the wrong temporal order from a set of otherwise related clips. This requires the model to compare video clips, identify regularities, and detect irregular temporal structure. The authors propose training an "odd-one-out network" (O3N) on this task, without any manual annotations, as a way to learn representations that transfer well to action recognition. Their hypothesis is that features learned via analogical reasoning on the odd-one-out task will generalize better than other self-supervised approaches.So in summary, the main research question is how to learn good video representations from unlabeled video in a self-supervised manner, with the central hypothesis being that the proposed odd-one-out learning task is an effective approach for this goal.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new self-supervised CNN pre-training technique called "odd-one-out learning". In this technique, the model is trained to identify the unrelated or "odd" video clip from a set of otherwise related clips sampled from the same video. The key ideas are:- Generate training examples (questions) where each question contains N+1 video clips/elements. N clips are sampled to have the correct temporal order from a video while 1 clip (odd) has the wrong temporal order.- Train a multi-stream CNN (called odd-one-out network O3N) end-to-end to predict which of the N+1 clips is the odd one.- The O3N learns useful spatio-temporal features without manual annotations by doing this analogical reasoning task. - The learned features can be transferred to other video analysis tasks like action recognition by fine-tuning.- Experiments show O3N significantly outperforms previous self-supervised methods on action classification, achieving 60.3% on UCF101 using only UCF101 training data (no external data).So in summary, the key contribution is proposing the odd-one-out self-supervised learning approach and an O3N model to implement it for video representation learning without manual labels.
