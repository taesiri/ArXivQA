# [AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning](https://arxiv.org/abs/2308.03526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is whether offline reinforcement learning algorithms can learn competitive policies purely from human replays in a complex game like StarCraft II. The paper introduces a new benchmark called "AlphaStar Unplugged" using a dataset of millions of human StarCraft II replays. It aims to evaluate the capability of offline RL algorithms to learn from this fixed dataset of imperfect human gameplay without any environment interaction.The key hypothesis is that breakthroughs on this benchmark could demonstrate that superhuman performance can be achieved purely using human-generated datasets. This would prove offline RL as a promising technique for real-world problems like robotics, healthcare, and industrial control where interactively collecting data is infeasible.To test this, the paper establishes tools to train and evaluate agents on this dataset without any online interaction. It presents several baseline agents using behavior cloning, offline actor-critic, and offline MuZero. The results demonstrate these agents can exceed 90% win rate against a prior AlphaStar agent trained with supervision, showing progress towards the goal of learning from offline human data.In summary, the central hypothesis is that offline RL can achieve competitive performance on a complex game like StarCraft II using purely static human datasets. The AlphaStar Unplugged benchmark is introduced to test this hypothesis and the baseline agent results provide initial evidence in support of it.


## What is the main contribution of this paper?

This paper introduces AlphaStar Unplugged, a new benchmark for offline reinforcement learning based on StarCraft II. The key contributions are:- Proposes AlphaStar Unplugged as a challenging new benchmark for offline RL. It uses a dataset of millions of human StarCraft II games. The complex, partially observable, and long time horizon nature of StarCraft II makes it uniquely suited to advance offline RL algorithms.- Provides an evaluation protocol, metrics, baseline implementations, and open source code for the benchmark. This includes baseline agents like behavior cloning, offline actor-critic, and offline MuZero.- Shows that offline RL can achieve over 90% win rate against the AlphaStar Supervised agent from previous work, using only the offline human dataset. This demonstrates offline RL can learn competitive policies from human demonstrations without online environment interactions.- Provides an analysis of design choices and insights into what methods work best on this large-scale offline RL problem. Successful approaches follow a two-step approach - first imitate the behavior policy and value function, then improve the policy using the estimated value function.In summary, the paper establishes a challenging new benchmark to advance offline RL, especially for complex partially observable environments. It provides tools and baseline agents, and demonstrates offline RL can reach high performance on this task. The insights from this large-scale setting could potentially translate to real-world offline RL problems.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of offline reinforcement learning:- This paper introduces a new benchmark, StarCraft II Unplugged, for evaluating offline RL algorithms. Other popular offline RL benchmarks include D4RL, RL Unplugged, and Procgen Benchmark, but they lack complex, partially observable environments like StarCraft II. So this paper fills an important gap.- The paper shows that behavior cloning forms a strong baseline on this benchmark. Many other offline RL papers compare to behavior cloning, and often aim to outperform it. But on this benchmark, several standard offline RL algorithms fail to beat behavior cloning. This highlights the challenge of this new benchmark. - The paper finds offline actor-critic and MuZero variants achieve the best performance. Other recent offline RL papers have also found success with constrained policy optimization and planning methods like MuZero. So the top performing approaches align with the current state-of-the-art.- However, there is still a substantial gap between the offline methods here and online RL results like AlphaStar. Closing this gap remains an open challenge. The dataset size and complexity likely contribute to this gap.- The paper releases code, datasets, and reference agents to standardize training and evaluation. This is an important contribution that will help advance research in this area. Other benchmarks like D4RL and Procgen Benchmark have also open-sourced elements to spur research.Overall, this paper makes a significant contribution by proposing a new challenging offline RL benchmark, revealing limitations of current algorithms, providing strong baselines and metrics, and open-sourcing key elements to advance research in this domain. The results highlight open challenges for offline RL on complex partially observable environments that will likely inspire future work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing better methods for offline policy evaluation during training. The paper notes that online policy evaluation is currently needed for hyperparameter tuning, but this is not feasible in many real-world offline RL settings. Better offline methods for evaluating policies during training could help with tuning hyperparameters and model selection.- Scaling up model sizes and compute. The paper notes their standard model size was chosen based on hardware limitations at the time. Larger models trained with more compute may lead to better performance.- Improving sample efficiency and reducing overfitting to the dataset. The paper shows performance improves logarithmically with more data, suggesting there is room for algorithms that can learn more efficiently from limited offline datasets.- Adapting more online RL algorithms to the offline setting. Many standard online RL methods like Q-learning and policy gradient methods did not perform well, but modifying them to constrain the policy and value function near the behavior policy may make them more effective.- Developing model-based offline RL methods. The paper suggests model-based approaches may help address the challenges of limited coverage and extrapolation in the complex state and action spaces of StarCraft.- Adding memory to handle partial observability. The paper found transformers can improve performance given enough training time, so integrating memory into offline RL agents is another direction.- Expanding the set of opponents for evaluation. The paper notes even their best agents are still far from the online RL performance of AlphaStar, suggesting expanding the set of opponents, including human players, would better measure progress.
