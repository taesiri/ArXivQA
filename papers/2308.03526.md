# [AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning](https://arxiv.org/abs/2308.03526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is whether offline reinforcement learning algorithms can learn competitive policies purely from human replays in a complex game like StarCraft II. The paper introduces a new benchmark called "AlphaStar Unplugged" using a dataset of millions of human StarCraft II replays. It aims to evaluate the capability of offline RL algorithms to learn from this fixed dataset of imperfect human gameplay without any environment interaction.The key hypothesis is that breakthroughs on this benchmark could demonstrate that superhuman performance can be achieved purely using human-generated datasets. This would prove offline RL as a promising technique for real-world problems like robotics, healthcare, and industrial control where interactively collecting data is infeasible.To test this, the paper establishes tools to train and evaluate agents on this dataset without any online interaction. It presents several baseline agents using behavior cloning, offline actor-critic, and offline MuZero. The results demonstrate these agents can exceed 90% win rate against a prior AlphaStar agent trained with supervision, showing progress towards the goal of learning from offline human data.In summary, the central hypothesis is that offline RL can achieve competitive performance on a complex game like StarCraft II using purely static human datasets. The AlphaStar Unplugged benchmark is introduced to test this hypothesis and the baseline agent results provide initial evidence in support of it.


## What is the main contribution of this paper?

This paper introduces AlphaStar Unplugged, a new benchmark for offline reinforcement learning based on StarCraft II. The key contributions are:- Proposes AlphaStar Unplugged as a challenging new benchmark for offline RL. It uses a dataset of millions of human StarCraft II games. The complex, partially observable, and long time horizon nature of StarCraft II makes it uniquely suited to advance offline RL algorithms.- Provides an evaluation protocol, metrics, baseline implementations, and open source code for the benchmark. This includes baseline agents like behavior cloning, offline actor-critic, and offline MuZero.- Shows that offline RL can achieve over 90% win rate against the AlphaStar Supervised agent from previous work, using only the offline human dataset. This demonstrates offline RL can learn competitive policies from human demonstrations without online environment interactions.- Provides an analysis of design choices and insights into what methods work best on this large-scale offline RL problem. Successful approaches follow a two-step approach - first imitate the behavior policy and value function, then improve the policy using the estimated value function.In summary, the paper establishes a challenging new benchmark to advance offline RL, especially for complex partially observable environments. It provides tools and baseline agents, and demonstrates offline RL can reach high performance on this task. The insights from this large-scale setting could potentially translate to real-world offline RL problems.
