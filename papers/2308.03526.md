# [AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning](https://arxiv.org/abs/2308.03526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is whether offline reinforcement learning algorithms can learn competitive policies purely from human replays in a complex game like StarCraft II. The paper introduces a new benchmark called "AlphaStar Unplugged" using a dataset of millions of human StarCraft II replays. It aims to evaluate the capability of offline RL algorithms to learn from this fixed dataset of imperfect human gameplay without any environment interaction.The key hypothesis is that breakthroughs on this benchmark could demonstrate that superhuman performance can be achieved purely using human-generated datasets. This would prove offline RL as a promising technique for real-world problems like robotics, healthcare, and industrial control where interactively collecting data is infeasible.To test this, the paper establishes tools to train and evaluate agents on this dataset without any online interaction. It presents several baseline agents using behavior cloning, offline actor-critic, and offline MuZero. The results demonstrate these agents can exceed 90% win rate against a prior AlphaStar agent trained with supervision, showing progress towards the goal of learning from offline human data.In summary, the central hypothesis is that offline RL can achieve competitive performance on a complex game like StarCraft II using purely static human datasets. The AlphaStar Unplugged benchmark is introduced to test this hypothesis and the baseline agent results provide initial evidence in support of it.
