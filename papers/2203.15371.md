# [mc-BEiT: Multi-choice Discretization for Image BERT Pre-training](https://arxiv.org/abs/2203.15371)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we improve image BERT pre-training by easing the strict mapping between masked patch predictions and unique token ids used in prior work like BEiT? 

The key hypothesis is that using multi-choice vision token ids, rather than unique hard labels, for the masked patches will lead to better pre-training performance. Specifically, the authors hypothesize that:

1) There is no single "correct" way to discretize visual patches, so assigning them unique labels is suboptimal. 

2) Semantically similar image patches should have shared/overlapping predictions, rather than being forced to map to distinct token ids.

3) By using soft probability vectors over possible token ids and incorporating inter-patch similarity, they can create better multi-choice training objectives for masked patches.

The proposed mc-BEiT method explores this hypothesis by introducing eased multi-choice vision token predictions for masked patches based on both the tokenizer outputs and inter-patch feature similarities. The experiments aim to validate whether this approach improves pre-training and downstream task performance compared to prior single-choice classification used in BEiT.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes an improved BERT-style image pre-training method called mc-BEiT, which uses multi-choice discretization objectives instead of mapping masked patches to unique token IDs. 

2. It introduces a multi-choice supervised signal formed by soft probability vectors predicted by an off-the-shelf image tokenizer. These are further refined using inter-patch semantic similarities estimated from the vision transformer features.

3. The multi-choice objectives provide more diverse and semantically meaningful target signals compared to BEiT's hard classification targets. This improves the model's ability to capture visual semantics.

4. Extensive experiments show mc-BEiT outperforms BEiT and other self-supervised methods on image classification, detection, segmentation across ImageNet, COCO and ADE20K datasets.

In summary, the key contribution is proposing a more effective multi-choice objective for masked image modeling, which improves visual semantics and outperforms prior arts in various vision tasks. The core ideas are using soft probability vectors and inter-patch similarities to create better supervision for the masked patches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes an improved BERT-style image pre-training method called mc-BEiT, which uses multi-choice discretization objectives with soft probability vectors to allow semantically similar image patches to share predictions instead of assigning them unique hard token ids.
