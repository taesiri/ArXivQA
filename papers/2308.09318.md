# [Towards Attack-tolerant Federated Learning via Critical Parameter   Analysis](https://arxiv.org/abs/2308.09318)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new defense strategy called FedCPA (Federated learning with Critical Parameter Analysis) to make federated learning more robust against poisoning attacks, especially under non-IID data settings. 

The central hypothesis is that benign local models tend to have similar sets of top and bottom critical parameters in terms of importance ranks, whereas poisoned local models tend to have different sets of critical parameters compared to benign models. Based on this hypothesis, the authors propose a new metric for measuring model similarity using the top and bottom critical parameters, and use this to detect and filter out likely malicious model updates during federated learning.

The key research questions addressed in this paper are:

1) Do benign local models exhibit common patterns in how parameter importance changes during training? 

2) Are there differences in parameter importance changes between benign and poisoned local models?

3) Can these patterns be exploited to develop a robust similarity metric to detect malicious updates in federated learning, especially under non-IID data?

4) How does the proposed defense method, FedCPA, compare against existing defense strategies for federated learning in terms of attack tolerance?

In summary, the central hypothesis is that analyzing patterns of critical parameters can enable more robust detection of poisoning attacks in federated learning. The key contribution is the proposal and evaluation of the FedCPA defense strategy based on this idea.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new defense method called FedCPA (Federated learning with Critical Parameter Analysis) to make federated learning systems more robust against poisoning attacks. The key ideas are:

- Analyzing the importance and patterns of change in model parameters during training. The paper shows empirically that benign models tend to have similar sets of top and bottom important parameters after training, while adversarial models exhibit larger disruptions. 

- Leveraging this observation, FedCPA defines a new metric to measure model similarity based on the parameters' importance patterns. This allows detecting potentially malicious updates that deviate from normal patterns.

- Using the model similarity measure, FedCPA assigns a normality score to each client's update. Then during aggregation, it filters out the effect of likely malicious updates via weighted averaging, where the weight depends on the normality score.

- Experiments demonstrate FedCPA's superior defense performance over existing methods against both untargeted and targeted attacks under non-IID settings. The success rate of attacks is reduced by a factor of 2-4.

In summary, the key contribution is a new parameter analysis-based defense strategy tailored for federated learning that is more robust against model poisoning attacks compared to prior approaches. The method provides new insights into detecting anomalies based on parameters' roles during training.
