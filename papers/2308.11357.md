# [Exemplar-Free Continual Transformer with Convolutions](https://arxiv.org/abs/2308.11357)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop an effective approach for continual learning with vision transformers that does not require storing exemplars and can operate without explicit task identifiers during inference?The key hypotheses appear to be:1) Convolutional manipulation of transformer weights can enable effective adaptation to new tasks with low overhead.2) An entropy-based criterion for inferring task identity from augmented views of the test image can allow task-agnostic inference. The authors propose a new continual learning approach called ConTraCon that aims to address the lack of research on continually learning vision transformers without exemplar storage or task identifiers. Their method leverages convolution on transformer weights for task adaptation and an augmentation-based entropy criterion for task inference. The experiments aim to validate if this approach can enable competitive continual learning performance in transformers without the usual requirements.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes ConTraCon, a new approach for continual learning with vision transformers. ConTraCon allows the model to expand and adapt to new tasks while maintaining performance on previous tasks, without needing to store examples from previous tasks.- It introduces a method to adapt the weights of a pretrained transformer to new tasks using convolution operations. This allows adapting to new tasks with minimal increase in parameters. - It uses a learnable temporal skip gating to balance between retaining previous knowledge and adapting to the new task. - It proposes an entropy-based approach to predict the task identity at test time without requiring explicit task labels. This allows the method to work in class incremental learning scenarios.- It evaluates ConTraCon extensively on benchmark datasets like CIFAR-100, ImageNet-100, TinyImageNet and compares with state-of-the-art approaches. The results show ConTraCon outperforms previous continual learning approaches for vision transformers while using fewer parameters.- It performs ablation studies to analyze the contribution of different components of ConTraCon like the convolutional kernels, skip gating, task prediction with augmentation, etc.In summary, the key contribution is a new continual learning approach for vision transformers that can expand to learn new tasks efficiently while retaining previous knowledge, without needing exemplar storage or task labels at test time. The convolutional weight adaptation and entropy-based task prediction are the main novel aspects.
