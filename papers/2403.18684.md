# [Scaling Laws For Dense Retrieval](https://arxiv.org/abs/2403.18684)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Prior work has uncovered scaling laws relating model performance to factors like model size and data size for language models. However, such laws have not been explored for dense retrieval models.  
- Key challenges include: 1) limited annotated training data, and 2) discrete evaluation metrics like NDCG are not sensitive enough.  

Proposed Solution
- Use a new evaluation metric called "contrastive perplexity" that is continuous and correlates well with ranking metrics.
- Conduct extensive experiments varying model size, data size, and annotation quality to uncover scaling laws.
- Show performance follows a power-law scaling w.r.t. model size and data size. Laws hold for data of different annotation quality.
- Derive joint scaling function relating performance to both model and data size.

Key Contributions  
- First study to uncover scaling laws for dense retrieval relating performance to model size, data size and annotation quality.
- New continuous evaluation metric more suitable than discrete IR metrics.
- Precise power-law relationships identified through extensive experiments.
- Joint scaling function derived that can predict performance and guide resource allocation.
- Analysis shows optimal model size is much smaller when including inference cost.
- Findings provide meaningful insights to guide future research on dense retrieval models.

In summary, this is the first work to uncover precise scaling laws governing the performance of dense retrieval models w.r.t. key factors like model size and data size. The laws are identified through extensive experiments using a new evaluation metric. The joint scaling function has promising applications for predicting model performance and allocating resources.
