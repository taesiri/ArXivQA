# [Attention Is Not All You Need Anymore](https://arxiv.org/abs/2308.07661)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. It formulates the sequence prediction problem in text generation using variable-length discrete-time Markov chains. 2. It proposes a family of layers called "Extractors" to replace the self-attention mechanism in Transformers. The Extractors aim to extract unified features from the input sequence and adjust them based on sequence length. 3. It evaluates four types of Extractors (SHE, HE, WE, ME) on a text generation task. Results show they can match or outperform standard self-attention, while reducing complexity.4. It analyzes the computational complexity of the Extractors compared to self-attention. The critical path is shorter for Extractors, suggesting potential to run faster.5. It reviews and explains the Transformer architecture based on the authors' understanding. Overall, the central hypothesis seems to be that replacing self-attention with the proposed Extractor layers can improve Transformer performance and/or reduce complexity. The key research contributions are introducing the Extractors and evaluating their capabilities compared to self-attention.
