# [Attention Is Not All You Need Anymore](https://arxiv.org/abs/2308.07661)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. It formulates the sequence prediction problem in text generation using variable-length discrete-time Markov chains. 2. It proposes a family of layers called "Extractors" to replace the self-attention mechanism in Transformers. The Extractors aim to extract unified features from the input sequence and adjust them based on sequence length. 3. It evaluates four types of Extractors (SHE, HE, WE, ME) on a text generation task. Results show they can match or outperform standard self-attention, while reducing complexity.4. It analyzes the computational complexity of the Extractors compared to self-attention. The critical path is shorter for Extractors, suggesting potential to run faster.5. It reviews and explains the Transformer architecture based on the authors' understanding. Overall, the central hypothesis seems to be that replacing self-attention with the proposed Extractor layers can improve Transformer performance and/or reduce complexity. The key research contributions are introducing the Extractors and evaluating their capabilities compared to self-attention.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a family of sublayers called the Extractors to replace the self-attention mechanism in the Transformer architecture. The paper introduces four types of Extractors:- Super High-Performance Extractor (SHE): Significantly outperforms self-attention but requires more computations.- Higher-Performance Extractor (HE): Outperforms multi-head self-attention with fewer computations. - Worthwhile Extractor (WE): Outperforms single-head self-attention with fewer computations.- Minimalist Extractor (ME): Requires very few computations while maintaining performance close to single-head self-attention.The key idea of the Extractors is to extract unified features from the input sequence as a constant-length representation that can be fed to a feedforward network. This avoids the quadratic complexity of self-attention. Experiments on text generation show the performance gains of using Extractors over standard self-attention Transformers. The paper also provides complexity analysis showing the tradeoffs between performance and computations for the different Extractor variants.In summary, the main contribution is proposing and evaluating a family of drop-in replacements for self-attention that can improve Transformer performance and/or reduce complexity. The Extractors offer flexibility in trading off performance vs efficiency.
