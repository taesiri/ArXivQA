# [Noise in the reverse process improves the approximation capabilities of   diffusion models](https://arxiv.org/abs/2312.07851)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper analyzes the approximation capabilities of neural stochastic differential equations (SDEs) as reverse processes in score-based generative models. It shows that neural SDEs can achieve stronger trajectory approximation in the $L^2$ norm compared to deterministic models like neural ODEs, even without assuming the reference vector field is Lipschitz continuous. This is enabled by the regularizing effect of noise. The paper characterizes the class of distributions that can be effectively sampled using this approach, relaxing assumptions on the smoothness of data distributions made in prior work. Notably, when network width is limited to the input dimension, weights act as control inputs and the analysis views trajectory approximation as a controllability problem. This perspective reveals how noise helps “steer” the system to track desired solutions. Overall, the theoretical analysis offers insight into why stochastic reverse processes empirically outperform deterministic counterparts in generative modeling, highlighting the benefits of noise for improving approximation capabilities.
