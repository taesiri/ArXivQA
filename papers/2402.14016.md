# [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on   Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) are increasingly used as zero-shot assessors to evaluate text quality, such as for benchmarking systems or grading exams. However, the adversarial robustness of LLM assessment methods has not been studied.  

Proposed Solution
- This paper investigates the vulnerability of LLM scoring and LLM comparative assessment to simple concatenative adversarial attacks. They find both methods are susceptible, with LLM scoring being particularly vulnerable.

- A greedy search algorithm is used to generate universal attack phrases of 4 words or less that can be appended to texts to trick LLM assessment models into providing higher scores, irrespective of true quality.

Key Findings
- Both LLM scoring and comparative assessment see reduced average predicted ranks when attack phrases are concatenated, but absolute scoring exhibits greater susceptibility.

- With just a 4 word phrase, LLM scoring consistently yields maximal scores regardless of input text. Comparative assessment shows some robustness but still sees inflated scores.

- Attack phrases transfer across model families and sizes. Phrases crafted on smaller models trick larger models like GPT-3.5, demonstrating pervasive vulnerabilities.

- As a defence, perplexity detection shows promise in distinguishing clean and attacked examples. Comparative assessment also demonstrates more inherent robustness over scoring.

Main Contributions  
- First adversarial attack analysis of LLM zero-shot assessment models.
- Demonstrates susceptibility of scoring/comparative assessment to simple concatenation attacks.
- Attack transferability across model sizes and families.
- Analysis of defences including comparative assessment and perplexity detection.

The paper underscores risks in using LLM assessment for critical scenarios without addressing vulnerabilities. Future work should further study attacks and defences.
