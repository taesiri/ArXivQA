# [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on   Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) are increasingly used as zero-shot assessors to evaluate text quality, such as for benchmarking systems or grading exams. However, the adversarial robustness of LLM assessment methods has not been studied.  

Proposed Solution
- This paper investigates the vulnerability of LLM scoring and LLM comparative assessment to simple concatenative adversarial attacks. They find both methods are susceptible, with LLM scoring being particularly vulnerable.

- A greedy search algorithm is used to generate universal attack phrases of 4 words or less that can be appended to texts to trick LLM assessment models into providing higher scores, irrespective of true quality.

Key Findings
- Both LLM scoring and comparative assessment see reduced average predicted ranks when attack phrases are concatenated, but absolute scoring exhibits greater susceptibility.

- With just a 4 word phrase, LLM scoring consistently yields maximal scores regardless of input text. Comparative assessment shows some robustness but still sees inflated scores.

- Attack phrases transfer across model families and sizes. Phrases crafted on smaller models trick larger models like GPT-3.5, demonstrating pervasive vulnerabilities.

- As a defence, perplexity detection shows promise in distinguishing clean and attacked examples. Comparative assessment also demonstrates more inherent robustness over scoring.

Main Contributions  
- First adversarial attack analysis of LLM zero-shot assessment models.
- Demonstrates susceptibility of scoring/comparative assessment to simple concatenation attacks.
- Attack transferability across model sizes and families.
- Analysis of defences including comparative assessment and perplexity detection.

The paper underscores risks in using LLM assessment for critical scenarios without addressing vulnerabilities. Future work should further study attacks and defences.


## Summarize the paper in one sentence.

 This paper investigates adversarial attacks on large language model assessment systems, finding both comparative and absolute scoring approaches vulnerable to simple universal phrase concatenation attacks that can manipulate quality scores, with absolute scoring more susceptible.


## What is the main contribution of this paper?

 Based on my review of the paper, the main contribution is:

The paper presents the first study on the adversarial robustness of assessment LLMs, demonstrating that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenative adversarial attacks. In particular, the paper shows that LLM-scoring is highly susceptible and universal attack phrases learned on smaller LLMs can transfer to larger closed-source models. The findings underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.

In summary, the key contributions are:

1) Demonstrating vulnerabilities in LLM assessment systems to adversarial attacks
2) Showing LLM-scoring is particularly susceptible compared to comparative assessment 
3) Establishing attack transferability from smaller to larger LLM models
4) Underscoring the need to address these vulnerabilities especially for high-stakes applications


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Adversarial attacks
- Large language models (LLMs)
- LLM-as-a-judge
- Zero-shot assessment
- Comparative assessment
- Absolute assessment 
- Universal adversarial phrases
- Concatenation attacks
- Transferability of attacks
- Robustness of assessment methods
- Defence strategies against attacks

The paper investigates the vulnerability of large language models (LLMs) when used for zero-shot assessment of text quality. It looks at both comparative assessment and absolute scoring methods, and shows they are susceptible to simple concatenative adversarial attacks, where a universal phrase appended to texts can trick LLMs into providing higher assessment scores. A key finding is that comparative assessment demonstrates more robustness than absolute scoring. The paper also examines the transferrability of attacks from smaller LLMs to larger models, and provides an initial investigation into using perplexity as a defence for detecting adversarial attacks. Overall, it raises concerns around the reliability of LLM assessment systems if deployed in real-world scenarios without addressing vulnerabilities.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a greedy search algorithm to learn the universal attack phrase. What are some potential limitations of using a greedy search rather than a more sophisticated optimization method? Could the phrases found by greedy search be suboptimal?

2. The paper evaluates attack transferability from a small model (FlanT5-3B) to larger models. What factors do you think contribute most to attack transferability? Could transferability be improved by using a larger model to learn attack phrases?

3. The authors find comparative assessment is more robust to attacks than absolute assessment. What core properties of comparative assessment make it harder to attack effectively? Could tweaking the comparative prompt further improve robustness? 

4. How well do you think the greedy search algorithm could scale to much larger datasets and vocabulary sizes? Would computational complexity pose issues? Could the search process be made more efficient?

5. The attack objective focuses on manipulating the ranking of responses. Could other attack objectives, such as directly optimizing the scores, be more effective? What are the tradeoffs?

6. The paper explores using perplexity for attack detection. What other detection methods could be viable for defending LLM assessment systems? What are some challenges in designing good detection systems?

7. What impact could adversarial training have on improving robustness? Could it protect well against the attacks seen or would adaptive attacks still succeed? What difficulties exist in adversarially training LLM assessment models?

8. Do you think prompts could be designed that are inherently more robust to concatenative attacks? What prompt properties would contribute to improved robustness?

9. The authors use a fixed vocabulary for the greedy search. How could the vocabulary be optimized during search to improve attack phrase quality? Would vocabulary selection help?

10. The paper focuses on universal phrase attacks. What other simple attacks might be effective against LLM assessment systems, such as inserting typos/grammar errors or substituting rare words? How could these be defended against?
