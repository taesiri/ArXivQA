# [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to efficiently adapt large pre-trained vision-language models like CLIP for downstream image recognition tasks. Specifically, the paper identifies prompt engineering as a major challenge that hinders the deployment efficiency of such models in practice. It proposes a simple approach called Context Optimization (CoOp) to automate prompt engineering for CLIP-like models. The key idea is to model a prompt's context words with learnable vectors, which can be optimized via standard supervised learning while keeping the pre-trained parameters fixed.The main hypothesis is that the proposed CoOp approach can turn pre-trained vision-language models into data-efficient learners for downstream tasks, outperforming hand-crafted prompts and baseline methods like the linear probe. Experiments on 11 image classification datasets validate this hypothesis and show that CoOp brings significant improvements over prompt engineering, especially when more training shots are available.In summary, the central research question is how to efficiently adapt large vision-language models to new datasets through automated prompt learning, and the key hypothesis is that the proposed CoOp approach can achieve this goal and outperform manual prompt engineering baselines.
