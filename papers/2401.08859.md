# [Shabari: Delayed Decision-Making for Faster and Efficient Serverless   Functions](https://arxiv.org/abs/2401.08859)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Serverless platforms today lack performance guarantees for function invocations, leading to high variability (up to 6x slowdown). This limits their use for performance-critical applications.
- Providers lack visibility into user functions, making it hard to right-size resource allocations. This leads to high underutilization (up to 80%).
- The root cause is that allocation decisions are made early without information about function semantics and inputs. Performance and resource utilization depend heavily on these factors.

Proposed Solution: 
- Introduce a system called Shabari that makes resource allocation decisions as late as possible, only after the function input is available.  
- It has two main components:
   - Resource Allocator: Uses online learning to predict the CPU and memory allocation needed to meet the user-specified SLO based on input features. Makes independent predictions per resource type.
   - Scheduler: Prioritizes routing invocations to warm containers of the right size. Launches smaller containers in background to reduce future cold starts. Considers both CPU and memory utilization for placement.

Main Contributions:
- Shows importance of delayed, independent allocation decisions per resource type 
- Introduces a performance-centric interface for users to specify SLOs per invocation
- Uses online learning tailored to function semantics to predict allocations
- Scheduling policies to mitigate cold starts introduced by per-invocation decisions
- Evaluation shows Shabari reduces SLO violations by 11-73% and wasted memory by 64-94%, compared to state-of-the-art systems

In summary, Shabari is a resource management system for serverless that leverages delayed decision making based on function inputs to provide performance guarantees and improve resource efficiency.


## Summarize the paper in one sentence.

 Shabari is a resource management framework for serverless systems that delays resource allocation decisions until function inputs are available in order to meet per-invocation performance objectives while improving resource utilization.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is a resource management framework for serverless systems called Shabari that makes resource allocation decisions as late as possible in order to account for the needs of each function invocation. Specifically:

- Shabari introduces a performance-centric interface that allows users to specify service-level objectives (SLOs, i.e. execution time targets) per invocation. 

- It uses an online learning agent to predict the required resource configuration (vCPUs and memory) to meet the SLO for each invocation, based on characteristics of the function inputs. 

- Shabari's scheduler mitigates cold starts introduced by the delayed per-invocation resource allocations by prioritizing routing invocations to warm containers and launching perfectly sized containers in the background.

In essence, Shabari delays binding resource types and allocation decisions until the function inputs are available in order to right-size each invocation and meet user-specified SLOs, while also improving resource utilization compared to baseline systems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Serverless computing
- Resource management
- Performance variability
- Resource underutilization 
- Function semantics
- Function inputs
- Delayed decision-making
- Independent resource allocations
- Online learning agent
- Service-level objectives (SLOs)
- Scheduler
- Cold starts
- Resource contention
- Apache OpenWhisk

The main focus of the paper is on a new resource management framework called Shabari that makes delayed and independent resource allocation decisions per invocation in order to meet specified SLOs while improving resource utilization. Key components include the online learning agent that predicts resource requirements based on function inputs and semantics, and the scheduler that mitigates cold starts. The evaluations are done on top of Apache OpenWhisk and compare against several baseline systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does Lachesis's resource allocator use online machine learning specifically to make resource allocation decisions? What algorithm does it use and why? 

2. What are the key differences in Lachesis's scheduler compared to default OpenWhisk's scheduler? How does it try to mitigate cold starts introduced by the resource allocator?

3. Why does Lachesis argue for delaying resource allocation decisions until inputs are available? What limitations does it highlight with early, input-agnostic resource allocation?

4. What are the tradeoffs Lachesis makes with creating one machine learning model per function versus per input type or across functions? What did the authors find in their evaluation?

5. How does Lachesis handle variability in execution times and resource usage across inputs to the same function? What safeguards has it built into its resource allocator?  

6. How does Lachesis configure its cost function to balance under-predictions and over-predictions of resource allocations? What techniques did the authors explore?

7. What scheduling algorithm does Lachesis use to place containers on servers? How is this different from default OpenWhisk and why?

8. What overheads are introduced by Lachesis's components, especially input featurization? How do the authors argue that improvements outweigh these overheads?

9. How does Lachesis configure its vCPU oversubscription limit per server and confidence thresholds for its online learning models? What did the sensitivity analysis reveal?

10. Why can't existing tools for profiling resource requirements be used instead of Lachesis's online approach? What are some of their limitations highlighted in the paper?
