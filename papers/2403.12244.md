# [Zero-Shot Multi-task Hallucination Detection](https://arxiv.org/abs/2403.12244)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Natural language generation (NLG) models often produce fluent but inaccurate text, exhibiting a problem known as "hallucination". 
- Current evaluation metrics prioritize fluency over accuracy. There is a need to assess model "truthfulness" by evaluating alignment of outputs with inputs/training data.

Definitions of Hallucination:
- Definition modeling: Output does not entail target output, reducing detection to NLI
- Paraphrase & Translation: Output is not semantically equivalent to input. Detection uses bidirectional entailment.  
- General: Output is not faithful to inputs or training data. Contradicts inputs or training data.

Proposed Solution:  
- Leverage entailment relationship between model output and input/targets to detect "detached" hallucinating samples.
- For definition modeling, check if output entails targets.  
- For translation/paraphrase, check bidirectional entailment between outputs and inputs.

Key Results:
- Accuracy of 0.78 on model-aware and 0.61 on model-agnostic SHROOM validation sets
- Confirm hypothesis that hallucinations violate entailment constraints.
- Computationally efficient NLI-based approach, unlike expensive LLMs.

Main Contributions:
- Concrete definitions of hallucination enabling quantitative detection
- Efficient NLI-based framework for hallucination detection across NLG tasks

The summary covers the key details on the problem being addressed, the specific definitions and approaches proposed, the main results achieved, and highlights the core contributions made in this paper related to detecting hallucinations.
