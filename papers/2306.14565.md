# Aligning Large Multi-Modal Model with Robust Instruction Tuning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses that this paper aims to address are:1. How can we build a large-scale and diverse visual instruction dataset to help align large multi-modal language models (LMMs) and reduce their tendency to hallucinate? The paper hypothesizes that creating a dataset with more diverse vision-language tasks, both positive and negative instructions, and different linguistic styles will help make LMMs more robust and reduce hallucination.2. How can we develop an automated evaluation approach for assessing visual instruction tuning that does not require human-labeled ground truth answers?The paper proposes a novel GPT4-assisted evaluation approach called GAVIE to assess model outputs on accuracy and relevancy without needing predefined formats or human annotations. 3. Will fine-tuning LMMs like MiniGPT4 on the proposed robust visual instruction dataset mitigate hallucination and improve performance compared to state-of-the-art methods?The paper hypothesizes that fine-tuning on their proposed dataset will reduce hallucination and boost LMM performance even with less training data than current state-of-the-art techniques. Their experiments aim to validate this.In summary, the key goals are developing a diverse visual instruction dataset to reduce LMM hallucination, proposing an automated evaluation approach, and showing improved LMM performance when fine-tuned on this data. The paper aims to demonstrate the value of robust instruction tuning for aligning LMMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The authors construct a new dataset called LRV-Instruction, which contains 120k visual instructions covering 16 vision-language tasks. The key features of this dataset are:- It includes both positive and negative instructions, unlike most existing datasets that focus just on positive examples. The negative instructions are designed to mitigate hallucination issues in models.- The instructions and answers are open-ended and generated by GPT4, rather than following pre-defined templates created by humans. This provides more diversity.- The negative instructions are generated at two different semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation.2. The authors propose a new evaluation approach called GPT4-Assisted Visual Instruction Evaluation (GAVIE) to assess model performance on visual instruction following. Key aspects are:- It does not require human-annotated groundtruth answers.- It evaluates model outputs on two criteria - Relevancy and Accuracy.- It is flexible to different instruction formats, unlike prior work that uses specific templates.3. Comprehensive experiments are conducted investigating the hallucination behavior of existing large multimodal models (LMMs) when tested on the new benchmark dataset LRV-Instruction. The results demonstrate:- Existing LMMs exhibit significant hallucination, especially on the negative instructions.- Existent Element Manipulation instructions are more challenging than Nonexistent Element Manipulation. - Finetuning a model on the more balanced LRV-Instruction dataset mitigates hallucination and boosts performance with less training data.- Balanced ratios of positive/negative data produce the most robust models.In summary, the key contribution appears to be the new dataset, evaluation approach, and experiments exposing weaknesses of current LMMs, while also showing how the proposed techniques can help make these models more robust.
