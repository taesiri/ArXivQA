# Aligning Large Multi-Modal Model with Robust Instruction Tuning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses that this paper aims to address are:1. How can we build a large-scale and diverse visual instruction dataset to help align large multi-modal language models (LMMs) and reduce their tendency to hallucinate? The paper hypothesizes that creating a dataset with more diverse vision-language tasks, both positive and negative instructions, and different linguistic styles will help make LMMs more robust and reduce hallucination.2. How can we develop an automated evaluation approach for assessing visual instruction tuning that does not require human-labeled ground truth answers?The paper proposes a novel GPT4-assisted evaluation approach called GAVIE to assess model outputs on accuracy and relevancy without needing predefined formats or human annotations. 3. Will fine-tuning LMMs like MiniGPT4 on the proposed robust visual instruction dataset mitigate hallucination and improve performance compared to state-of-the-art methods?The paper hypothesizes that fine-tuning on their proposed dataset will reduce hallucination and boost LMM performance even with less training data than current state-of-the-art techniques. Their experiments aim to validate this.In summary, the key goals are developing a diverse visual instruction dataset to reduce LMM hallucination, proposing an automated evaluation approach, and showing improved LMM performance when fine-tuned on this data. The paper aims to demonstrate the value of robust instruction tuning for aligning LMMs.
