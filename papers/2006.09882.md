# [Unsupervised Learning of Visual Features by Contrasting Cluster   Assignments](https://arxiv.org/abs/2006.09882)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be:Unsupervised image representations can achieve performance competitive with supervised pretraining by using a contrastive learning approach that enforces consistency between cluster assignments for different augmentations of the same image, rather than directly comparing image features as in standard contrastive learning.The key claims being made are:1) A "swapped prediction" approach where cluster assignments are predicted between views of the same image can serve as an effective alternative to direct feature comparison for contrastive learning. 2) This method can scale to large datasets since it does not require storing a large memory bank of features or using a momentum encoder.3) A "multi-crop" augmentation strategy that uses a mix of views at different resolutions improves performance without increasing compute/memory requirements. 4) Combining the swapped prediction loss and multi-crop augmentation allows their method (SwAV) to surpass supervised pretraining on several transfer learning benchmarks, a first for an unsupervised method.So in summary, the central hypothesis is around whether their proposed swapped prediction and multi-crop techniques can push unsupervised contrastive learning to exceed supervised pretraining, which the results seem to validate.


## What is the central research question or hypothesis that this paper addresses?

The main contributions of this paper are:1. Proposing an online clustering method called SwAV (Swapping Assignments between Views) for self-supervised visual representation learning. The key idea is to enforce consistency between cluster assignments predicted from different augmented views of the same image, instead of directly comparing image features as in contrastive learning.2. Introducing a "multi-crop" data augmentation strategy that uses a mix of crops at different resolutions to increase the number of views of an image without increasing compute/memory costs. 3. Showing that SwAV improves over prior contrastive and clustering methods, achieving 75.3% top-1 accuracy on ImageNet using a ResNet-50, which is state-of-the-art for self-supervised methods using a standard ResNet-50.4. Demonstrating that features learned by SwAV transfer very well to other downstream tasks, outperforming a supervised ResNet-50 pretrained on ImageNet on several transfer learning benchmarks.So in summary, the central hypothesis is that enforcing consistency between cluster assignments from different views can be an effective alternative to contrasting image features directly for self-supervised learning. The multi-crop strategy further improves results. Together, SwAV achieves new state-of-the-art transfer performance for a self-supervised method.
