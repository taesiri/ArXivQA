# [Unsupervised Learning of Visual Features by Contrasting Cluster   Assignments](https://arxiv.org/abs/2006.09882)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be:Unsupervised image representations can achieve performance competitive with supervised pretraining by using a contrastive learning approach that enforces consistency between cluster assignments for different augmentations of the same image, rather than directly comparing image features as in standard contrastive learning.The key claims being made are:1) A "swapped prediction" approach where cluster assignments are predicted between views of the same image can serve as an effective alternative to direct feature comparison for contrastive learning. 2) This method can scale to large datasets since it does not require storing a large memory bank of features or using a momentum encoder.3) A "multi-crop" augmentation strategy that uses a mix of views at different resolutions improves performance without increasing compute/memory requirements. 4) Combining the swapped prediction loss and multi-crop augmentation allows their method (SwAV) to surpass supervised pretraining on several transfer learning benchmarks, a first for an unsupervised method.So in summary, the central hypothesis is around whether their proposed swapped prediction and multi-crop techniques can push unsupervised contrastive learning to exceed supervised pretraining, which the results seem to validate.
