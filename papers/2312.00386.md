# [Local monotone operator learning using non-monotone operators: MnM-MOL](https://arxiv.org/abs/2312.00386)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from the paper:

This paper proposes extensions to the monotone operator learning (MOL) framework for recovering images from undersampled MRI measurements. MOL uses deep equilibrium models to constrain the CNN as a monotone operator, ensuring theoretical guarantees on convergence and robustness. However, the constraints also degrade performance compared to unrolled methods without such constraints. The authors introduce two novel relaxations of these constraints to enhance the effectiveness of MOL while retaining its benefits: 1) Motivated by convex-nonconvex methods, they constrain the sum of the data term and score function gradients to be monotone rather than the score function alone. This allows the score to be non-monotone, improving performance. 2) They also replace the global Lipschitz constraint with a localized version that only requires monotonicity in a neighborhood around the solution manifold. This further relaxes constraints while still ensuring convergence given proper initialization near the true solution. Theoretical analysis proves uniqueness, convergence guarantees, and robustness to perturbations for the localized model. Experiments demonstrate improved performance rivaling unrolled methods, 10x lower memory use, strong robustness to perturbations, and correctness of the theoretical results. The relaxations enhance MOL performance while retaining its practical advantages.
