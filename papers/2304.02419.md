# [TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration](https://arxiv.org/abs/2304.02419)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:How can we generate realistic 3D dance movements that integrate guidance from both music and text modalities? The key hypotheses appear to be:1) By utilizing a VQ-VAE architecture, motions from separate music-dance and text-motion datasets can be projected into a shared latent space to enable training a model on both datasets. 2) A cross-modal transformer architecture can effectively translate both audio and text features into dance motion tokens, allowing the fusion of guidance from both modalities.3) The proposed methods will generate coherent and realistic 3D dance motions guided by both music and text inputs, while maintaining performance on single modality tasks.In summary, the paper proposes a pipeline to enable 3D dance generation utilizing bimodal guidance from music and text. The central hypothesis is that the proposed VQ-VAE representation and cross-modal transformer will enable high-quality dance generation driven by both modalities.
