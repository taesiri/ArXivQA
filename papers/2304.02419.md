# [TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration](https://arxiv.org/abs/2304.02419)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we generate realistic 3D dance movements that integrate guidance from both music and text modalities? 

The key hypotheses appear to be:

1) By utilizing a VQ-VAE architecture, motions from separate music-dance and text-motion datasets can be projected into a shared latent space to enable training a model on both datasets. 

2) A cross-modal transformer architecture can effectively translate both audio and text features into dance motion tokens, allowing the fusion of guidance from both modalities.

3) The proposed methods will generate coherent and realistic 3D dance motions guided by both music and text inputs, while maintaining performance on single modality tasks.

In summary, the paper proposes a pipeline to enable 3D dance generation utilizing bimodal guidance from music and text. The central hypothesis is that the proposed VQ-VAE representation and cross-modal transformer will enable high-quality dance generation driven by both modalities.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposes a new task of generating 3D dance movements conditioned on both music and text modalities. This enables generating richer dance motions guided by text instructions in addition to music. 

2. Proposes a method to tackle the lack of paired music-text-dance data by utilizing separate music-dance and text-motion datasets. A VQ-VAE is used to project motions from both datasets into a shared latent space of discrete tokens. 

3. Proposes a cross-modal transformer architecture that translates audio and text features into motion tokens in a sequence-to-sequence manner. This allows integrating text instructions efficiently without degrading music-conditioned dance generation performance.

4. Introduces two new evaluation metrics - Motion Prediction Distance (MPD) and Freezing Score (FS) - to better measure the coherence and amount of freezing in generated motions.

5. Demonstrates successful dance generation based on both music and text instructions while maintaining comparable performance on single modality tasks. The method generates realistic and coherent dance motions.

In summary, the key contribution is proposing the novel bimodal (music+text) 3D dance generation task and an effective method to tackle it by mixing separate unimodal datasets and using a cross-modal transformer architecture. The new metrics also allow better evaluation of the generated motions.
