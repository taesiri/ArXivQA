# [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741)

## What is the central research question or hypothesis that this paper addresses?

This paper does not appear to have an explicit central research question or hypothesis stated. However, based on my reading, the main focus seems to be developing and demonstrating a method for training deep reinforcement learning agents to perform complex tasks based on human preferences rather than explicitly programmed reward functions. Some key points about the research goals/contributions:- The authors aim to show that human feedback can be used to train deep RL agents without access to the true reward function, including solving complex tasks like Atari games and simulated robotics tasks. - They want to show their approach can work with minimal human feedback, making it practical to apply to state-of-the-art deep RL algorithms that require huge amounts of experience.- They demonstrate their method can learn complex novel behaviors (like backflips) that would be difficult to specify reward functions for.- Their approach scales up prior work on learning from human preferences to much more complex domains by using deep neural networks and modern deep RL algorithms.So in summary, this paper focuses on developing and empirically evaluating a practical method for training deep RL agents from human feedback rather than hardcoded rewards, with the goal of enabling RL to be applied to complex real-world tasks where defining rewards is difficult. The central hypothesis is that this approach can work efficiently enough to scale to large modern RL systems and complex behaviors.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- Proposes a method for deep reinforcement learning from human preferences instead of a predefined reward function. The approach involves training a separate reward model using supervised learning on preferences, and using that learned reward for policy optimization.- Demonstrates that this approach can scale up to complex deep RL tasks including Atari games and simulated robotics tasks. The method is able to solve these tasks with relatively little human feedback (e.g. 15 minutes to 5 hours).- Shows that the approach can be used to learn novel and complex behaviors that would be difficult to specify a reward function for, such as performing backflips in simulation or driving alongside cars in Enduro. Only about an hour of human feedback is needed to learn these novel behaviors.- Provides ablation studies analyzing the impact of different algorithm design choices like using clip comparisons vs absolute scores, online vs offline training, etc. This provides insights into what factors contribute most to the success of the approach.- Compares performance using real human feedback vs synthetic oracles, showing that real human feedback can be almost as effective as an idealized synthetic oracle in many tasks.In summary, the key contribution is showing that reinforcement learning algorithms can be successfully trained from human preferences to solve complex tasks, instead of needing hand-specified reward functions. This helps address the challenge of specifying rewards for real-world applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a method for training reinforcement learning agents to perform complex tasks using only human feedback on pairs of short video clips, without access to the true reward function.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper focuses on using human feedback for reinforcement learning (RL), which is an active area of research. Other papers like Christiano et al. (2017), Ibarz et al. (2018), and Sadigh et al. (2017) have also explored human feedback for RL. - A key contribution of this paper is showing that human feedback can scale up to complex deep RL tasks like Atari games and simulated robotics. Prior work on human feedback for RL typically focused on smaller or simpler domains. This paper demonstrates the feasibility on much more challenging tasks.- The approach follows a similar framework to some prior work like Akrour et al. (2012) of learning a separate reward model from human feedback. However, the implementation deals with the complexities of deep neural network policies and environments.- For eliciting feedback, the paper uses comparisons of short clips which is similar to Wilson et al. (2012). The clip length and asynchronous training appear to be important innovations for scaling up. Other work has focused more on trajectories or individual states.- The environments and behaviors learned seem significantly more complex than in any prior work on learning from human feedback. For example, learning simulated robotic backflips or driving in Enduro with dense traffic appear well beyond what had been shown previously.- Overall, this paper represents an important advance in scaling up human feedback for RL to challenging deep RL problems. It demonstrates results on a level of complexity not shown before in prior work on learning from human judges or preferences. The innovations on clip comparisons and training regimes seem crucial to the successes.In summary, this paper takes a similar high-level approach as some prior works but demonstrates substantially more impressive results by tackling more complex, high-dimensional RL environments. The results significantly expand the frontier of what's possible for learning behaviors through human feedback.


## What future research directions do the authors suggest?

The paper suggests the following future research directions:- Improve the efficiency of learning from human preferences further, to make it as easy as learning from a programmatic reward signal. This would allow powerful RL systems to be applied to complex human values rather than just simple goals.- Expand the range of tasks that can be learned from human preferences. The paper shows results on Atari games and simulated robotics, but applying the approach to more complex real-world behaviors would be an impactful direction.- Investigate better methods for eliciting preferences, such as asking more informative questions. The paper uses a simple disagreement-based heuristic for selecting pairs of clips to query about, but more principled active learning approaches may be much more sample efficient. - Scale up the amount of human feedback that can be incorporated, for example by training on preferences from many human raters in parallel. This could accelerate learning.- Apply the approach to partially observable environments, where the reward may depend on the history of observations. The paper assumes the reward depends only on current state, but modeling rewards with RNNs could extend it.- Study the theoretical sample complexity of learning from human preferences. The empirical results show it can be very efficient, but formal guarantees could lend insight.In summary, the main future directions are 1) making human preference learning practically as easy as standard RL 2) expanding the scope of tasks it can handle 3) improving the active querying algorithms and 4) theoretical analysis of the approach.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper explores using human preferences to guide reinforcement learning agents in complex environments. The authors propose an algorithm that fits a neural network reward predictor to comparisons of short video clips made by humans, while simultaneously optimizing a policy with reinforcement learning to maximize predicted reward. They show this approach can solve a range of deep RL benchmark tasks and learn novel complex behaviors in Atari games and simulated robotics, using only a small amount of human feedback. Key to the approach is the efficiency gained by interleaving reward learning and policy optimization, and eliciting comparisons of short clips rather than individual states. The authors demonstrate their method can learn behaviors not easily specified by a reward function, like performing backflips, with only about an hour of human time. Overall, this work provides evidence that human preferences can be leveraged to train deep RL agents economically even in complex state spaces, which could expand applications of RL to real-world problems.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents an algorithm for deep reinforcement learning from human preferences. Rather than requiring a pre-defined reward function, the algorithm learns a reward function from human feedback. Humans are asked to compare pairs of short video clips showing the agent's behavior in the environment. Their preferences are used to fit a reward function, which is then optimized using standard deep RL algorithms like A2C and TRPO. The method is evaluated on a range of benchmark tasks in the MuJoCo simulator and Atari games. On most tasks, it is able to learn policies that perform nearly as well as policies trained directly on the true reward, using only a modest amount of human feedback (e.g. 700 comparisons in MuJoCo tasks). The algorithm can also be used to train novel complex behaviors specified in natural language, like doing backflips in MuJoCo or driving alongside cars in Enduro. The key contribution is demonstrating that preference-based deep RL can scale to modern deep RL systems and complex behaviors, reducing the amount of human oversight needed by orders of magnitude compared to naive applications of human feedback to RL.


## Summarize the main method used in the paper in one paragraph.

The paper proposes an algorithm for deep reinforcement learning from human preferences. The key ideas are:- Maintain a policy network and a reward predictor network. The policy network interacts with the environment to collect trajectories. - Show pairs of short video clips from the trajectories to a human, who indicates which clip they prefer. These preferences are used as training data to fit the reward predictor network.- Use the predicted rewards from the reward network to train the policy network with standard deep RL algorithms like A2C or TRPO.- Train the policy, reward predictor, and collect human preferences asynchronously and continually throughout training. This allows the agent to adapt as the human provides more labels.- Use an ensemble of predictors and adversarial clip selection to make the reward prediction more robust.So in summary, they intertwine human preference comparisons, reward function prediction, and RL policy optimization in an asynchronous loop to allow an RL agent to learn complex behaviors from modest human feedback. The key novelty is scaling this approach up to modern deep RL algorithms.
