# [Not All Attention is Needed: Parameter and Computation Efficient   Transfer Learning for Multi-modal Large Language Models](https://arxiv.org/abs/2403.15226)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Multi-modal large language models (MLLMs) have high computational complexity due to the introduction of visual modalities, making their tuning to downstream tasks parameter and computationally expensive. 
- Existing methods like adapters add latency during inference which limits efficiency.

Proposed Solution:
- The paper proposes Efficient Attention Skipping (EAS) to reduce both parameters and computation for tuning MLLMs.
- EAS evaluates and skips redundant multi-head attentions (MHAs), which are less important than feedforward networks (FFNs) for retaining performance.
- A novel Propagation-of-Information Adapter (PIA) is proposed to replace skipped MHAs. PIA exchanges information like MHAs and has no latency as it is reparameterized into FFNs after training.

Main Contributions:
- EAS reduces both parameters and computation for tuning MLLMs by selectively skipping redundant MHAs using PIA.
- PIA exchanges information during training and is reparameterized into FFNs to avoid latency during inference.
- Experiments on LaVIN model and ScienceQA dataset show EAS retains performance while speeding up inference by 2.2x compared to default LaVIN and 1.9x over a strong baseline.
- Experiments on METER model show EAS matches performance of state-of-the-art while reducing computations significantly.

In summary, the paper proposes an efficient attention skipping method to reduce parameters and computation for tuning large multi-modal models using a novel propagate-information adapter, providing up to 2x speedups without performance loss.
