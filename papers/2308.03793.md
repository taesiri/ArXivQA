# [ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free   Domain Adaptation](https://arxiv.org/abs/2308.03793)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is:How can we adapt vision-language models like CLIP to new target domains in a source-free, unsupervised manner to improve their classification accuracy, when only unlabeled target data is available?The key hypotheses explored are:1) The visual and text embeddings of CLIP contain redundant, class-agnostic information that leads to misalignment between modalities and hurts target domain accuracy. 2) Aligning the embeddings by removing this redundant info, generating pseudo-labels, and performing cross-modality self-training will mitigate the gaps between visual and text domains as well as their misalignment.3) This approach can effectively adapt CLIP in a source-free, unsupervised manner to new target domains using just unlabeled target data, improving its classification accuracy.The paper proposes a method called ReCLIP to address this question and evaluate these hypotheses. ReCLIP aligns the visual and text embeddings using projections, assigns pseudo-labels via propagation, and performs iterative self-training on visual and text encoders to refine embeddings and labels. Experiments on 22 datasets demonstrate ReCLIP significantly improves CLIP's classification performance in the target domains.
