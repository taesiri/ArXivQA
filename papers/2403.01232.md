# [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) suffer from over-smoothing and over-squashing issues, limiting their expressive power. 
- Graph transformers (GTs) are more expressive but have at least quadratic complexity, making them unscalable. 
- Existing linear GTs still underperform GNNs on several datasets, questioning their practical expressivity.

Proposed Solution:
- The paper proposes Polynormer, a linear GT model with high polynomial expressivity. 
- A base attention model is introduced that explicitly learns high-degree polynomial functions on node features. The coefficients of polynomials are controlled by attention scores.
- By incorporating graph topology and node features into coefficients separately, local and global equivariant attention modules are derived from the base model.
- Polynormer adopts a linear local-to-global attention scheme, where local features are first transformed by the local module, followed by the global module. This captures both local and global structures.

Main Contributions:
- Polynormer achieves polynomial expressivity with linear complexity, balancing expressivity and scalability.
- Without using activation functions, Polynormer outperforms GNN/GT baselines on multiple datasets, showcasing its high expressivity. 
- When combined with ReLU activation, Polynormer improves accuracy over baselines by up to 4.06% on 11 out of 13 datasets, including both homophilic and heterophilic graphs.
- Polynormer scales to large graphs with millions of nodes and consistently ranks among the top fastest models.
