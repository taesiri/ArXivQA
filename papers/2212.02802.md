# [Diffusion Video Autoencoders: Toward Temporally Consistent Face Video   Editing via Disentangled Video Encoding](https://arxiv.org/abs/2212.02802)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to edit face attributes in videos while maintaining temporal consistency across frames. The key hypothesis is that decomposing videos into disentangled latent representations for identity and motion will enable consistent video editing by modifying only the identity feature.

Specifically, the paper proposes a novel framework called "Diffusion Video Autoencoders" to encode videos into a time-invariant identity feature and per-frame motion features. It hypothesizes that editing the identity feature and decoding it with the original motion features will result in temporally consistent video editing. The experiments aim to validate whether:

1) The proposed autoencoder can decompose videos into disentangled identity and motion latents.

2) Editing the identity latent leads to consistent video editing results. 

3) The diffusion autoencoder framework enables editing of real-world videos robustly, unlike GAN inversion methods.

4) The edited results are temporally more consistent compared to previous video editing methods.

In summary, the central hypothesis is consistent video editing via disentangled identity/motion representations, enabled by a novel diffusion autoencoder framework. The experiments aim to validate the decomposition, editing consistency, robustness and improvements over previous methods.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. They propose a novel video editing framework called Diffusion Video Autoencoders based on diffusion autoencoders. This allows decomposing a video into a time-invariant identity feature and time-variant per-frame features. 

2. Using the decomposed representation from the diffusion video autoencoder, they can perform face video editing by only editing the single time-invariant identity feature and decoding it with the original time-variant features. This allows temporally consistent editing.

3. Thanks to the near perfect reconstruction ability of diffusion models, their framework can handle editing of unusual cases like occluded faces, not just normal faces. 

4. They propose a text-based identity editing method based on CLIP guidance. This allows optimizing the identity feature based on a text prompt to get the desired editing effect. To make this more efficient, they propose using intermediate latent states rather than fully generated images for computing the CLIP loss.

In summary, the key ideas are leveraging diffusion autoencoders to decompose videos into identity and time-variant features, editing just the identity feature for consistency, using diffusion models for robustness to unusual cases, and CLIP-guided editing. The disentangled representation and editing only the identity are key for the improved consistency.
