# [Diffusion Video Autoencoders: Toward Temporally Consistent Face Video   Editing via Disentangled Video Encoding](https://arxiv.org/abs/2212.02802)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to edit face attributes in videos while maintaining temporal consistency across frames. The key hypothesis is that decomposing videos into disentangled latent representations for identity and motion will enable consistent video editing by modifying only the identity feature.

Specifically, the paper proposes a novel framework called "Diffusion Video Autoencoders" to encode videos into a time-invariant identity feature and per-frame motion features. It hypothesizes that editing the identity feature and decoding it with the original motion features will result in temporally consistent video editing. The experiments aim to validate whether:

1) The proposed autoencoder can decompose videos into disentangled identity and motion latents.

2) Editing the identity latent leads to consistent video editing results. 

3) The diffusion autoencoder framework enables editing of real-world videos robustly, unlike GAN inversion methods.

4) The edited results are temporally more consistent compared to previous video editing methods.

In summary, the central hypothesis is consistent video editing via disentangled identity/motion representations, enabled by a novel diffusion autoencoder framework. The experiments aim to validate the decomposition, editing consistency, robustness and improvements over previous methods.
