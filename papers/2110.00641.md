# [Batch size-invariance for policy optimization](https://arxiv.org/abs/2110.00641)

## What is the central research question or hypothesis that this paper addresses?

The central research questions addressed in this paper are:1) Can the proximal policy be decoupled from the behavior policy in PPO, and is this beneficial? 2) Can PPO and PPG be made batch size-invariant by decoupling the proximal policy and using an EWMA to control its "age"?3) Does using an EWMA for the proximal policy provide any outright benefits compared to the standard algorithms?The key hypothesis underlying these questions is that the proximal policy only needs to be a recent policy, and does not specifically need to be the behavior policy used for collecting experience. The paper provides empirical evidence to support this hypothesis through experiments on decoupling the policies and achieving batch size-invariance.Some key findings from the experiments are:- Decoupling allows PPO to work well with surprisingly stale data by preventing incorrect importance sampling ratios.- Adjusting hyperparameters, especially the EWMA decay rate, allows near-perfect batch size scaling for PPO and PPG. - Using an EWMA for the proximal policy provides a small but consistent benefit to sample efficiency compared to the normal algorithms.So in summary, the central hypothesis is that the proximal policy can be decoupled, which enables better use of stale data and batch size-invariance. The paper provides empirical support for this hypothesis through careful experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing the concept of decoupling the proximal policy from the behavior policy in policy optimization algorithms like PPO. The key insight is that the proximal policy controls the size of policy updates, while the behavior policy is only needed for importance sampling corrections. These can be treated as separate policies.2. Proposing PPO-EWMA and PPG-EWMA, variants of PPO and PPG that use an exponentially weighted moving average of the policy parameters as the proximal policy. This allows stale data to be used more efficiently.3. Showing how PPO-EWMA and PPG-EWMA can be made "batch size-invariant", meaning the batch size can be adjusted without changing the overall algorithm behavior, by modifying hyperparameters like the learning rate and EWMA decay rate. This is useful for adapting to computational constraints.4. Providing empirical evidence to support the claims, including experiments on using stale data, achieving batch size invariance, and comparing PPO/PPG to the EWMA variants.5. Discussing how their findings provide evidence that PPO can be viewed as approximating natural policy gradients, rather than as a trust region method. The proximal policy controls the speed of policy updates, not proximity to the behavior policy.In summary, the main contributions are introducing decoupled policy objectives, proposing the EWMA variants to enable this, achieving batch size invariance, and providing empirical support along with theoretical discussion/implications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes decoupling the proximal and behavior policies in PPO to achieve batch size-invariance, allow efficient use of stale data, and improve performance.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on policy optimization algorithms in reinforcement learning:- It provides a new perspective on understanding PPO through the lens of natural policy gradients rather than just trust regions. This contrasts with how PPO has typically been motivated and analyzed.- It decouples the proximal policy from the behavior policy in PPO's objectives. This is a novel modification that allows PPO to work better with stale data and to achieve batch size invariance. Other work has not explored or proposed this decoupling.- It introduces PPO-EWMA and PPG-EWMA, modified versions of PPO and PPG that use an EWMA of the policy parameters as the proximal policy. The use of an EWMA in this way is novel.- It provides an analysis of how to achieve batch size invariance for policy optimization algorithms like PPO. Much prior work has studied batch size invariance for SGD, but less work has looked specifically at policy optimization.- The experiments thoroughly test the effects of decoupling the proximal policy, achieving batch size invariance, and using an EWMA. They provide strong empirical evidence for the benefits of these ideas.- The paper situates the work well within the broader context of research on policy optimization, trust regions, natural gradients etc. It builds nicely on a foundation of prior algorithmic innovations in RL.Overall, this paper makes several novel contributions to better understanding and improving policy optimization algorithms like PPO. The ideas are well motivated theoretically and validated experimentally. It represents an incrementally innovative step forward in RL research.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Exploring other ways to decouple the proximal policy from the behavior policy beyond EWMAs, such as using a target network like in DQN.- Extending the analysis of batch size-invariance to algorithms like SAC and TD3 that use clipped double Q-learning.- Studying whether the insights about controlling policy update speeds rather than trusting behavior policies could be applied in model-based RL.- Exploring whether batch size-invariance allows more efficient use of very large batches, in addition to very small batches.- Investigating other dimensions of invariance, like invariance to different network architectures or invariance when transferring policies to new environments. - Using the proximal variance reduction effect of EWMAs to improve sample efficiency of policy optimization algorithms.- Understanding the theory behind why controlling policy update speeds matters more than trusting behavior policies.In summary, the main suggestions are around exploring other ways to decouple proximal and behavior policies, extending the analysis to other algorithms and settings like model-based RL, studying other dimensions of invariance, and better understanding the theory behind the empirical results.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes decoupling the proximal policy from the behavior policy in policy optimization algorithms like PPO, where the proximal policy is used to control the size of policy updates while the behavior policy is used for off-policy corrections. The key insight is that the proximal policy does not need to equal the behavior policy - it only matters how old the proximal policy is. The authors introduce PPO-EWMA and PPG-EWMA variants that use an exponentially-weighted moving average of the policy parameters as the proximal policy network. This allows stale data to be used more efficiently and enables batch size-invariance by maintaining a consistent proximal policy age. Experiments demonstrate the benefits of decoupling in terms of handling artificial staleness and achieving batch size-invariance. The EWMA modification also slightly improves performance, likely by reducing proximal policy variance. Overall, the work provides both theoretical and practical insights into policy optimization.
