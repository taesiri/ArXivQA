# [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement Learning from Human Feedback (RLHF) is an effective technique to align large language models (LLMs) with human preferences. However, running RLHF is computationally expensive as it requires training a separate reward model and tuning an LLM policy using reinforcement learning. This complexity limits the adoption of RLHF. 

Proposed Solution: 
- The authors propose "Parameter Efficient Reinforcement Learning" (PERL) which uses a parameter-efficient method called Low-Rank Adaptation (LoRA) to train the reward model and policy instead of fully tuning all the parameters. This greatly reduces memory usage and speeds up training.

Key Contributions:
- Extensive experiments comparing PERL to conventional RLHF on 7 datasets across summarization, dialog, and other tasks
- Finds PERL matches performance of RLHF while using 50% less memory to train reward model and 20% less for policy tuning. Also trains faster.  
- Analyzes impact of model size and number of trainable parameters on PERL
- Releases two new dialog datasets - "Taskmaster Coffee" and "Taskmaster Ticketing" to promote RLHF research

In summary, the paper demonstrates PARL can make RLHF much more efficient and scalable while maintaining performance. This enables wider adoption of RLHF for aligning LLMs to human preferences. The efficiency gains and new datasets are the main contributions.
