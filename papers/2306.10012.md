# [MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image   Editing](https://arxiv.org/abs/2306.10012)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is developing a large-scale, manually annotated dataset called MagicBrush for instruction-guided real image editing. The authors aim to create a comprehensive benchmark for both training and evaluating text-guided image editing models. The main motivation is that current datasets in this domain are either limited to specific categories like flowers/birds, or rely on automatic synthesis which can introduce noise. There is a lack of large-scale, high-quality data with manual annotation to really push text-guided image editing research forward. To address this gap, the authors design the MagicBrush dataset comprising over 10K manually annotated real image editing examples that cover diverse scenarios like single-/multi-turn, mask-provided/free etc.The central hypothesis is that such a high-quality dataset can contribute to building more advanced text-guided image editing models, compared to current methods that are either zero-shot or trained on noisy synthetic data. The authors conduct experiments to showcase the challenging nature of MagicBrush and that fine-tuning models on it can improve performance.In summary, the key research question is whether a large-scale, manually curated dataset like MagicBrush can drive progress in instruction-guided real image editing. The hypothesis is that it can lead to more capable models compared to the status quo. The paper presents MagicBrush and initial experiments to demonstrate its potential.
