# [Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit   Models for High-dimensional Gaussian Mixtures](https://arxiv.org/abs/2402.02697)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Deep equilibrium models (DEQs) are a type of implicit neural network that have shown strong empirical performance, but there is limited theoretical understanding of their connections to standard explicit neural networks. 

- Key open questions:
    - Do DEQs provide advantages over explicit networks? 
    - Can we design an equivalent explicit network for a given DEQ to avoid the computational overhead of DEQs during training and inference?

Proposed Solution:
- The authors perform an in-depth analysis of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices of DEQs using random matrix theory, for the case when input data follows a Gaussian mixture model (GMM). 

- They show the CK and NTK depend only on 4 key parameters relating to the DEQ activation function and variance. 

- They prove a shallow explicit network can be carefully constructed to produce the same CK or NTK as the DEQ. This establishes an equivalence between implicit DEQs and shallow explicit networks.

Main Contributions:
- Precise characterization of CK and NTK matrices of implicit DEQs, showing dependence only on 4 nonlinear equations.

- Methodology to construct equivalent shallow explicit networks matching the DEQ's CK or NTK. Demonstrated for commonly used ReLU and Tanh DEQs.  

- Empirical evidence on synthetic and real datasets (MNIST, FashionMNIST, CIFAR10) showing performance match between constructed explicit networks and original DEQs.

- First theoretical evidence showing implicit DEQ models exhibit similar spectral properties to shallow explicit networks. Provides fundamentals for further analyses of training dynamics and generalization.

In summary, the key insight is that despite the conceptual difference between infinitely deep implicit DEQs and shallow explicit networks, carefully constructed shallow networks can provably achieve equivalent predictive performance. This suggests potential for computational savings.
