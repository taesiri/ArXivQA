# There and Back Again: Revisiting Backpropagation Saliency Methods

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: How can we develop a unified framework to better understand, analyze, and improve backpropagation-based saliency methods for explaining deep neural network predictions?The key aspects related to this question are:- Proposing a principled two-phase "Extract & Aggregate" framework that unifies several existing saliency methods like Grad-CAM, gradients, and linear approximation. The framework separates spatial contribution extraction and aggregation into saliency maps.- Using the framework to develop a new saliency method called NormGrad that aggregates spatial contributions using the Frobenius norm.- Analyzing the ability of different saliency methods to extract complementary information from multiple network layers by combining layer-wise saliency maps.- Introducing a metric to quantify class sensitivity of saliency methods and a meta-learning inspired technique to improve it.So in summary, the main research question is about developing a unifying framework for explaining and improving backpropagation-based saliency methods through systematic analysis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The paper proposes a principled framework to unify and help explain various backpropagation-based saliency methods for deep neural networks. The key idea is to view the process of generating a saliency heatmap in two phases: extracting the contribution of each pixel to the gradient of network parameters, and aggregating this spatial information into a 2D map. 2. Using this framework, the paper introduces a new saliency method called NormGrad which uses the Frobenius norm to aggregate spatial gradient information from convolutional layers.3. The paper analyzes the ability of different saliency methods to extract complementary information from multiple network layers. It finds that combining maps across layers consistently improves performance for certain methods like linear approximation and NormGrad.4. The paper proposes a novel meta-learning inspired technique to improve the class sensitivity of any saliency method by adding an inner SGD step before computing gradients.5. The paper conducts extensive experiments analyzing and comparing various backpropagation saliency methods on image classification models trained on ImageNet and PASCAL VOC datasets.In summary, the main contribution is the unified framework for understanding and improving backpropagation-based saliency methods, along with the introduction of NormGrad and the meta-learning based technique to enhance class sensitivity. The analysis across network layers and comprehensive comparative experiments are also valuable additions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a framework to unify and analyze backpropagation-based saliency methods for deep neural networks, proposes a new saliency method called NormGrad, and introduces techniques to improve saliency map quality by combining maps across layers and increasing class sensitivity.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- The paper proposes a unified framework for backpropagation-based saliency methods by dividing heatmap generation into an extraction and aggregation phase. This provides a theoretical way to compare and understand different methods instead of just empirically. Other works have focused more on empirical comparisons or only unified certain methods.- The paper introduces NormGrad, a new saliency method based on aggregating spatial gradients using the Frobenius norm. This provides an alternative aggregation approach compared to prior methods like summing or maximizing.- The paper analyzes combining saliency maps from different layers, finding that some methods benefit while others do not. Most prior works have focused on saliency at just the input or last convolutional layer.- A new class sensitivity metric and meta-saliency technique are proposed to improve sensitivity to the target class. Other works have identified issues with class sensitivity but not proposed ways to improve it.- The paper focuses specifically on gradient-based saliency methods that only require one forward/backward pass. Some related works have proposed more complex perturbation-based approaches.Overall, this paper provides a more rigorous theoretical understanding, proposes new techniques, and conducts novel analyses compared to prior work on backpropagation saliency methods. The unified framework, new metrics, and findings around multi-layer combinations seem to be unique contributions.
