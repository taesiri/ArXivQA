# There and Back Again: Revisiting Backpropagation Saliency Methods

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: How can we develop a unified framework to better understand, analyze, and improve backpropagation-based saliency methods for explaining deep neural network predictions?The key aspects related to this question are:- Proposing a principled two-phase "Extract & Aggregate" framework that unifies several existing saliency methods like Grad-CAM, gradients, and linear approximation. The framework separates spatial contribution extraction and aggregation into saliency maps.- Using the framework to develop a new saliency method called NormGrad that aggregates spatial contributions using the Frobenius norm.- Analyzing the ability of different saliency methods to extract complementary information from multiple network layers by combining layer-wise saliency maps.- Introducing a metric to quantify class sensitivity of saliency methods and a meta-learning inspired technique to improve it.So in summary, the main research question is about developing a unifying framework for explaining and improving backpropagation-based saliency methods through systematic analysis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The paper proposes a principled framework to unify and help explain various backpropagation-based saliency methods for deep neural networks. The key idea is to view the process of generating a saliency heatmap in two phases: extracting the contribution of each pixel to the gradient of network parameters, and aggregating this spatial information into a 2D map. 2. Using this framework, the paper introduces a new saliency method called NormGrad which uses the Frobenius norm to aggregate spatial gradient information from convolutional layers.3. The paper analyzes the ability of different saliency methods to extract complementary information from multiple network layers. It finds that combining maps across layers consistently improves performance for certain methods like linear approximation and NormGrad.4. The paper proposes a novel meta-learning inspired technique to improve the class sensitivity of any saliency method by adding an inner SGD step before computing gradients.5. The paper conducts extensive experiments analyzing and comparing various backpropagation saliency methods on image classification models trained on ImageNet and PASCAL VOC datasets.In summary, the main contribution is the unified framework for understanding and improving backpropagation-based saliency methods, along with the introduction of NormGrad and the meta-learning based technique to enhance class sensitivity. The analysis across network layers and comprehensive comparative experiments are also valuable additions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a framework to unify and analyze backpropagation-based saliency methods for deep neural networks, proposes a new saliency method called NormGrad, and introduces techniques to improve saliency map quality by combining maps across layers and increasing class sensitivity.
