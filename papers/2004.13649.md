# [Image Augmentation Is All You Need: Regularizing Deep Reinforcement   Learning from Pixels](https://arxiv.org/abs/2004.13649)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be that a simple data augmentation technique involving input perturbations and regularization of the value function can enable robust reinforcement learning directly from pixels, without needing auxiliary losses or pretraining. The key ideas proposed are:1) Applying standard image augmentation techniques (like random shifts) to the pixel observations during training. This acts as a regularizer and reduces overfitting of the vision encoder.2) Regularizing the Q-function learned by the critic so that different augmented versions of the same input have similar Q-values. This exploits the MDP structure. 3) Combining these data augmentation and regularization techniques with off-the-shelf RL algorithms like SAC and DQN. No other changes to the RL methods are needed.The central claim is that this approach, termed DrQ, can achieve state-of-the-art results on continuous control tasks as well as Atari, surpassing prior methods like auxiliary losses, world models, and contrastive learning. The effectiveness and simplicity of data regularization is the main hypothesis.In summary, the paper proposes that data augmentation and regularization alone is sufficient for robust reinforcement learning from pixels, without needing other modifications to the underlying RL algorithms. The experiments aim to validate this hypothesis across various benchmark tasks.
