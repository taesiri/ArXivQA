# [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Pretrained large language models (LLMs) like OPT and LLaMA demonstrate exceptional capabilities in natural language processing. However, their immense parameter sizes (billions or tens of billions) lead to substantial demands on memory and computation for deployment. Existing post-training quantization (PTQ) methods for compressing LLMs face severe performance degradation when quantizing to ultra-low bits like 1-2 bits. Therefore, there is a need for highly accurate and efficient methods to binarize LLMs while preserving their capabilities.

Proposed Solution - BiLLM:
The paper proposes a novel framework called BiLLM to binarize LLMs to 1 bit with minimal impact on performance. The key ideas are:

1) Analyze weight distribution in LLMs to find - a) a small fraction of salient weights with high impact, b) majority of redundant, bell-shaped distributed weights.  

2) For salient weights - Identify and select them structurally, binarize via a residual approximation to maximize restoration.

3) For non-salient weights - Design an optimal segmentation strategy to divide them into groups and separately binarize each group to minimize error.

Additionally, incorporate block-wise error compensation for further accuracy.

Main Contributions:

- Propose BiLLM, the first framework to successfully binarize large LLMs to 1.08-1.11 bits without significant performance loss. 

- Achieve state-of-the-art results over methods like GPTQ and PB-LLM, across various LLM families (OPT, LLaMA etc.) and metrics.

- Demonstrate the potential for extreme model compression of LLMs, enabling their deployment on resource-constrained devices while retaining capabilities.

- Provide detailed analysis of weight distributions in LLMs revealing redundancy and opportunities for accurate aggressive quantization.

- Establish benchmark results for future research into compression of large language models using model quantization techniques.
