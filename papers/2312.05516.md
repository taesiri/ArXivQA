# [Stateful Large Language Model Serving with Pensieve](https://arxiv.org/abs/2312.05516)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing large language model (LLM) serving systems are stateless across requests and do not cache conversation history between requests. 
- In multi-turn conversations, each new request must provide the full conversation history, which leads to redundant and costly recomputation of the conversation history with each new request.

Proposed Solution:
- The paper proposes Pensieve, a stateful LLM serving system that maintains conversation state across requests by caching previously processed history to avoid duplicate processing.

Key Ideas:
- Multi-tier GPU-CPU caching strategy to store conversation history embeddings (KV-tokens) for reuse across requests. Recently used conversations cached in GPU, older conversations swapped to CPU.
- Handle scenarios when part of conversation history is swapped out from GPU cache by restoring from CPU cache or recomputing dropped history.  
- New multi-query attention GPU kernel to allow attention computation between new input tokens and conversation history KV-tokens scattered in non-contiguous GPU memory.

Main Contributions:
- Identify major inefficiency of stateless LLM serving systems in multi-turn conversation scenarios.
- Design a stateful serving system Pensieve optimized for multi-turn conversation serving using multi-tier caching.
- Develop new multi-query attention GPU kernel to support non-contiguous caches.
- Evaluation shows 1.5-2x higher throughput and 60-75% lower latency compared to stateless baseline.

In summary, the paper tackles the problem of redundant computation in stateless LLM serving for multi-turn conversations, by designing a stateful serving system Pensieve that maintains conversation state across requests using multi-tier caching and a new multi-query attention GPU kernel. Key innovations include the multi-tier caching strategy and handling non-contiguous caches for efficiency.
