# [Image-Caption Encoding for Improving Zero-Shot Generalization](https://arxiv.org/abs/2402.02662)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- State-of-the-art vision-language models like CLIP achieve high accuracy on in-distribution data but struggle to generalize out-of-distribution. Specifically, their zero-shot classification performance lags behind supervised fine-tuning.
- Existing methods for improving zero-shot accuracy either require large unlabeled text data or access to model internals which may not be available. 

Proposed Solution: 
- The paper proposes a new zero-shot classification method called Image-Caption Encoding (ICE) that leverages generated image captions at test time to improve predictions.
- Key ideas:
    - Observe that correct label is usually in the top-5 predicted classes for misclassified images. ICE tries to steer the prediction towards the correct class within the top-K predictions.
    - Use caption embeddings which encode spatial image information and some reasoning ability from pre-training. 
    - Aggregate predictions from both image and multiple caption embeddings to make a more informed prediction. Adaptively set weighting on caption prediction based on relative confidences.

Main Contributions:
- Introduce using captions from decoder models to improve zero-shot classification, an under-explored direction.
- Propose ICE method that combines predictions from images and captions at test time only. Easily integrates with existing models.
- Show consistent improvements of ~0.5% on average and up to 3% on challenging datasets when applying ICE to various state-of-the-art baselines. 
- Provide detailed analysis on benefits of using captions and ablation studies on ICE parameters.

In summary, the paper presents Image-Caption Encoding as a straightforward yet effective approach to boost zero-shot classification performance of vision-language models by exploiting additional signals from generated captions.
