# [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](https://arxiv.org/abs/2303.13744)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to achieve conditional image-to-video generation that can simultaneously produce realistic spatial details and temporally coherent motions. Specifically, the paper proposes a new approach called "latent flow diffusion models" (LFDM) to tackle the conditional image-to-video generation task. The key hypothesis is that disentangling the generation of spatial contents and temporal dynamics can lead to better results on this task. To test this hypothesis, the paper designs a two-stage training framework for LFDM:1) In stage one, it trains a latent flow auto-encoder in an unsupervised fashion to learn a mapping between image frames and latent representations. This auto-encoder can estimate optical flow between frames and warp images in the latent space.2) In stage two, it trains a diffusion model to generate coherent latent flow sequences conditioned on the input image and class label. This diffusion model only needs to model a low-dimensional latent flow space containing motion/shape information.By evaluating on multiple datasets, the paper shows that the proposed LFDM approach consistently outperforms previous state-of-the-art methods for conditional image-to-video generation. The results support the hypothesis that disentangling spatial and temporal generation through latent flow modeling leads to better performance on this task.In summary, the central hypothesis is on using latent flow diffusion models for disentangled spatial-temporal modeling to achieve high-quality conditional image-to-video generation. The experiments confirm the advantage of this approach over previous methods.
