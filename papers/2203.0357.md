# [MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning](https://arxiv.org/abs/2203.0357)

## What is the central research question or hypothesis that this paper addresses?

The key research goal of this paper is to develop a self-supervised pre-training method via contrastive learning to enhance the logical reasoning ability of neural models, in order to reduce the reliance on labeled data and improve generalization. The authors propose two main novel components to achieve this:1. A meta-path guided strategy to automatically construct positive and negative training instances from unlabeled text data in a way that reveals logical structure. 2. A counterfactual data augmentation method to eliminate shortcuts induced by pre-trained language models' world knowledge during contrastive pre-training.By combining these two techniques in a self-supervised contrastive learning framework, the paper aims to show that the resulting pre-trained model can achieve superior performance on logical reasoning tasks compared to strong baselines, without requiring additional labeled data. The ablation studies are designed to demonstrate the contribution of each proposed component. Overall, the central hypothesis is that explicitly incorporating logical structure into contrastive pre-training in this way will improve logical reasoning capabilities.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a new self-supervised pre-training method called MERIT to enhance logical reasoning abilities of language models, using contrastive learning on unlabeled text data. 2. It introduces two key strategies as part of MERIT:- A meta-path based strategy to automatically discover and extract logical structures from raw text to construct positive training instance pairs.- A counterfactual data augmentation method to eliminate shortcuts based on world knowledge in pre-trained models.3. The proposed MERIT method achieves new state-of-the-art results on two logical reasoning benchmark datasets - ReClor and LogiQA, outperforming previous methods relying on task-specific model architectures or augmentation of original training data.4. Ablation studies demonstrate the contribution of the two key strategies - meta-path and counterfactual augmentation - to the overall performance of MERIT.5. The method shows stronger generalization capability on less training data and on other complex reasoning tasks like DREAM.In summary, the key novelty is in developing a self-supervised pre-training approach using contrastive learning to enhance logical reasoning, along with strategies to automatically construct useful training data from raw text and overcome world knowledge shortcuts. The method reduces reliance on annotated data and shows strong empirical performance.
