# [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding   Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Existing large language models (LLMs) pre-trained on sequences with restricted maximum lengths cannot generalize well to longer sequences. This is due to two key issues - (1) out-of-domain: the longer sequences are out of the distribution of sequences seen during pre-training, and (2) distraction: the additional noisy tokens in the longer sequences distract the LLM from accurately understanding the sequence. However, enhancing LLMs to process long streaming sequences is crucial for applications like dialog agents. Continual training of LLMs on longer sequences incurs substantial costs. Thus, improving length generalizability of LLMs without training is an important open problem.

Proposed Solution: This paper proposes a training-free memory-based approach called \modelname{} to improve the ability of LLMs to process extremely long sequences. The key ideas are:
(1) Use a sliding window attention to restrict the context per step. 
(2) Build an external context memory module to store distant context. 
(3) Lookup relevant memory units to provide useful context.
Specifically, the distant context is split into semantic blocks and stored in the memory. Only a few relevant blocks are selected via an efficient lookup mechanism to supplement the limited context window. This allows capturing long-range dependencies while avoiding distraction from noisy contexts.

Main Contributions:
(1) A training-free memory-based approach to improve length generalizability of LLMs without any continual training.
(2) A block-level memory design and efficient lookup mechanism that provides useful long-range context while reducing computational overhead.
(3) Empirical evaluation showing LLMs extended by \modelname{} match or outperform competitive baselines that continually train the LLMs on longer sequences.
(4) Demonstration of capturing dependencies effectively on sequences with up to 1 million tokens.

Overall, the paper makes significant contributions towards enabling LLMs to process extremely long streaming sequences in a training-free manner via efficient context modeling.
