# [Language-Specific Neurons: The Key to Multilingual Capabilities in Large   Language Models](https://arxiv.org/abs/2402.16438)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 demonstrate impressive multilingual capabilities without explicit multilingual training, but the underlying mechanisms are not well understood. 

- It remains an open question on how LLMs process texts in different languages at the neural composition level.

Method:
- The paper proposes the concept of "language-specific neurons" in LLMs that are responsible for processing specific languages. 

- They introduce a new method called "language activation probability entropy (LAPE)" to identify neurons that preferentially activate for 1-2 languages over others.

- LAPE works by measuring each neuron's activation probability when fed texts from different languages. Neurons with low entropy (high activation for fewer languages) are deemed language-specific.

Key Findings:
- A tiny fraction (1%) of neurons are language-specific and deactivating them significantly harms the model's proficiency in that language.

- Language-specific neurons concentrate in the top and bottom layers. Bottom layers map different languages into a shared space while top layers project back into each language's vocabulary space.  

- The output language can be "steered" by selectively activating/deactivating language-specific neurons, mitigating issues like off-target language.

Contributions:
- First study investigating the existence and impact of language-specific neurons to explain multilingual abilities of LLMs.

- A new method (LAPE) to identify such neurons.

- Findings that language proficiency depends on very few neurons, concentrated in certain key layers.

- Demonstrating the ability to control output language by manipulating language-specific neurons.
