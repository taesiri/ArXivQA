# [Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How verifiable are the responses generated by existing commercial generative search engines?

The paper focuses on evaluating the verifiability of four popular generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. Specifically, it aims to assess these systems across two key dimensions of verifiability:

1) Citation recall - The proportion of generated statements that are fully supported by citations. This measures how comprehensively the systems cite sources to back up their generated claims.

2) Citation precision - The proportion of generated citations that actually support their associated statement. This measures how accurately the citations align with the content being cited. 

The overarching goal is to audit these commercial systems to gain a better understanding of their capabilities and limitations when it comes to producing verifiable responses. The paper conducts human evaluation on a diverse set of queries to assess the systems' citation recall and precision. The central hypothesis is that while these systems may produce fluent and seemingly informative responses, they likely lack sufficient verifiability in terms of comprehensive and accurate sourcing of their generated claims.

In summary, the key research question is evaluating and quantifying the verifiability of existing commercial generative search engines through analysis of their citation recall and precision. The goal is to audit these rapidly adopted systems to surface potential issues around trustworthiness.


## What is the main contribution of this paper?

 The main contribution of this paper is conducting a human evaluation study to audit the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. The key findings are:

- Existing generative search engines often produce fluent and seemingly useful responses, but frequently contain unsupported statements and inaccurate citations. On average, only 51.5% of generated sentences are fully supported by citations, and just 74.5% of citations actually support their associated sentence.

- Citation recall and precision are inversely correlated with fluency and perceived utility. Responses that appear more helpful tend to have lower citation recall and precision. This suggests existing systems achieve fluency partly by generating unsupported or incorrect statements. 

- Analysis indicates this inverse correlation arises because some systems heavily copy or paraphrase text from their cited sources. This inflates precision and recall but reduces fluency and relevance to the user's query.

- The authors argue these low recall and precision rates are concerning for systems rapidly gaining popularity as an information seeking tool. The facade of usefulness combined with unsupported statements increases their potential to mislead users.

In summary, the key contribution is conducting an in-depth audit of verifiability in existing generative search engines. The results highlight the need for systems that combine fluency and usefulness with reliable sourcing of statements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my understanding, here is a 1 sentence TL;DR summary of the key points from the paper:

The paper evaluates the verifiability (citation recall and precision) of four popular generative search engines on diverse queries, finding that while responses appear useful, they frequently contain unsupported claims and inaccurate citations, highlighting shortcomings in existing systems.


## How does this paper compare to other research in the same field?

 This paper provides a helpful analysis of the verifiability of current generative search engines by conducting a thorough human evaluation on their citation recall and precision. Here is how I see it comparing to other related work:

- It builds on prior work like Nakano et al. (2021) and Menick et al. (2022) that trained models to provide evidence for their responses, by shifting focus to evaluating commercial systems that are already deployed. This provides valuable insight into the current state of real-world systems.

- The evaluation protocol is more comprehensive than in some prior work. For example, Bohnet et al. (2023) evaluated only on NaturalQuestions queries with paragraph answers, whereas this paper uses 12 diverse query distributions. The coverage of different answer types is useful.

- The introduction of citation recall and precision metrics provides a standardized way to quantify verifiability. This is more granular than just using overall human judgments of plausibility or faithfulness to evidence as in some prior work.

- Analyzing the correlation between verifiability and fluency/perceived utility provides an interesting finding about current systems trading off these attributes. This highlights a concerning potential to mislead users.

- The analysis of systems copying from citations explains one reason for the observed inverse correlation. The hypothesis that some systems struggle with content selection seems plausible.

Overall, I think this paper makes an important contribution in thoroughly evaluating verifiability of existing systems. The metrics and findings provide a benchmark for future improvement. The analysis also yields useful insights about current limitations and challenges.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Developing generative search engines that can handle queries that do not have clear extractive answers on the Internet. The authors found that existing systems struggle on queries like the open-ended AllSouls essay questions or NaturalQuestions queries that require aggregating information across table cells or multiple sources. Improving performance on queries lacking extractive answers is an important direction.

- Improving content selection and relevance ranking in generative search engines. The authors observe that current systems struggle to appropriately weigh and select citations of varying relevance. For example, when responding to historical ELI5 queries, systems only lightly utilize the original source ELI5 webpage even when it is retrieved. Better weighing sources and citations of differing relevance is a direction for improvement.

- Combining the strengths of extraction and abstraction. The authors believe it is possible to build systems with both high verifiability (through comprehensive and accurate citations) as well as high fluency and utility (through abstraction). Existing systems tend to tradeoff between extraction (improving verifiability metrics like citation recall and precision) and abstraction (improving fluency and perceived utility). New techniques could aim to get the best of both worlds.

- Developing improved evaluation protocols and benchmarks. The authors propose the citation recall and precision metrics to encourage verifiable systems, but also discuss their limitations (e.g. dependence on sentence segmentation). Coming up with better automatic verifiability metrics is an area for future work. Expanding the diversity of evaluation queries is another direction.

In summary, key future directions include better handling queries lacking clear extractive answers, improving content selection and citation weighting, combining extraction and abstraction, and developing improved evaluation methods for verifiability. The authors frame their work as an audit of existing systems to motivate progress on these fronts.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper evaluates the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. It conducts human evaluation across a diverse set of 1450 queries to measure the fluency, perceived utility, citation recall, and citation precision of responses from these systems. The key findings are that while existing generative search engines produce fluent responses that appear useful and informative, they frequently generate unsupported statements and inaccurate citations. On average only 51.5% of sentences are fully supported by citations (recall) and 74.5% of citations support their associated sentence (precision). Furthermore, fluency/utility are inversely correlated with citation recall/precision, likely because some systems copy text from citations, inflating precision/recall but reducing fluency/utility. Overall the paper provides concerning results about the reliability of existing generative search engines, and argues improved verifiability is critical as these systems become more mainstream.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper evaluates the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. Verifiability refers to whether the search engines comprehensively cite sources to support their generated statements (citation recall) and whether the cited sources accurately support the statements (citation precision). 

The authors conduct a human evaluation study across a diverse set of 1450 queries requiring different types of knowledge. They find that while the search engines generate fluent and seemingly useful responses, the citation recall and precision are quite low - on average only 51.5% of statements are fully supported by citations and 74.5% of citations support the associated statement. The authors observe an inverse correlation between verifiability metrics and fluency/usefulness, hypothesizing that some engines copy verbatim from sources, inflating precision/recall while reducing fluency. Overall, the concerningly low verifiability indicates these rapidly adopted search engines could potentially mislead users despite their veneer of reliability. The authors call for developing more trustworthy search engines and for better understanding of current commercial systems.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper "Evaluating Verifiability in Generative Search Engines":

The authors conducted a human evaluation to audit the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. They evaluated these systems on a diverse set of 1450 queries from sources like historical Google user queries, open-ended questions from Reddit, etc. that required different types of knowledge. For each query-response pair, they had human annotators evaluate the fluency, perceived utility, citation recall, and citation precision. Citation recall measured the proportion of sentences that were fully supported by citations, while citation precision measured the proportion of citations that supported their associated sentence. By combining citation recall and precision, they calculated a citation F1 score to evaluate systems' overall verifiability. Through this process, they found concerningly low citation recall and precision for existing systems, suggesting that their responses frequently contain unsupported statements and inaccurate citations despite appearing useful. The main method was thus a comprehensive human evaluation protocol focused on verifiability metrics like citation recall and precision.


## What problem or question is the paper addressing?

 The paper is addressing the issue of evaluating verifiability in generative search engines. Generative search engines directly generate responses to user queries, along with inline citations. A key trait of a trustworthy generative search engine is verifiability - the system should cite sources comprehensively (high citation recall) and accurately (high citation precision). 

The paper conducts an audit of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat - to evaluate their verifiability across a diverse set of queries. Specifically, the paper examines whether generated responses contain unsupported statements or inaccurate citations by measuring citation recall (proportion of statements supported by citations) and citation precision (proportion of citations that support their associated statement). 

The key question the paper is investigating is: are existing commercial generative search engines producing verifiable responses that comprehensively and accurately cite their sources, or do they frequently generate unsupported claims and inaccurate citations? Evaluating verifiability is important for understanding if these rapidly adopted systems can be trusted to provide users with reliable information.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords related to this work include:

- Generative search engines - The paper focuses on evaluating verifiability in generative search engines. These are systems that directly generate responses to user queries.

- Verifiability - A key concept examined in the paper is verifiability, which refers to systems citing sources comprehensively (high citation recall) and accurately (high citation precision). 

- Citation recall - The proportion of generated statements about the external world that are fully supported by citations.

- Citation precision - The proportion of generated citations that support their associated statements.

- Citation F1 - A metric combining citation precision and recall.

- Unsupported statements - The paper finds many generated statements are unsupported, meaning they lack comprehensive citations.

- Inaccurate citations - Many citations do not actually support their associated statements.

- Fluency - How coherent and understandable the generated text responses are.

- Perceived utility - Whether responses seem helpful and informative for the query.

- Inverse correlation - Fluency/utility tend to be inversely correlated with citation recall/precision.

- Algorithmic auditing - Evaluating AI systems to understand strengths/weaknesses.

- Commercial systems - The paper specifically examines Bing Chat, NeevaAI, perplexity.ai, YouChat.

- Diverse queries - Queries are drawn from various sources to enable broad understanding.

In summary, the key focus is on evaluating citation quality and verifiability in existing commercial generative search engines using diverse queries.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key points of the paper:

1. What is the purpose of the study? What gap is it trying to address?

2. What systems/models are evaluated in the study? 

3. What metrics are used to evaluate the systems? How are they calculated?

4. What datasets/queries are used for evaluation? Why were they chosen?

5. What are the key findings regarding fluency and perceived utility of the systems?

6. What are the key findings regarding citation recall and precision of the systems? 

7. How do the results for citation recall/precision correlate with fluency and perceived utility? What does this suggest?

8. How does the analysis of copying/paraphrasing provide insights into the inverse correlation results?

9. What are the limitations of the study? What future work is suggested?

10. What are the key contributions and takeaways of the study? How might it influence future work on generative search engines?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using human evaluation to measure citation recall and precision. What are some potential limitations or challenges with relying on human judgments for evaluation? For example, could humans introduce biases or inconsistencies when judging if a statement is supported by a citation? 

2. The paper finds an inverse correlation between citation recall/precision and fluency/perceived utility. However, correlation does not imply causation. What are some alternative hypotheses that could potentially explain this inverse correlation?

3. The paper hypothesizes that the inverse correlation occurs because some systems copy or closely paraphrase from cited webpages, improving citation recall/precision at the cost of fluency/utility. How could the authors further validate this hypothesis? For example, could they quantify or directly measure the degree of copying?

4. The paper uses BLEU and BERTScore to measure the similarity between generated statements and supporting citations. What are some limitations of these automatic metrics, especially BLEU, for this task? Could other metrics like ROUGE or semantic textual similarity metrics be more suitable?

5. The paper finds that citation recall is much lower on open-ended queries like AllSouls that lack clear extractive answers on the internet. What modifications could be made to the paper's proposed evaluation protocol to better handle such open-ended queries?

6. The paper evaluates four commercial systems, but many other models exist. How could the authors scale up the evaluation to cover more systems, while still collecting high-quality human judgments?

7. The paper uses a 3-step annotation process. Could variations of this process, like combining steps, improve efficiency or agreement rates? How could the interface be improved?

8. The paper uses a diverse set of queries, but they are still samples. How could evaluation be done in a more naturalistic setting, like with real user queries and clicks? What biases might the paper's samples introduce?

9. The paper focuses on English systems. How could the evaluation protocol be adapted to examine verifiability for systems in other languages? What challenges might arise?

10. The paper proposes citation recall and precision as metrics of verifiability. Are there other metrics of trustworthiness that could complement these? For example, metrics for coherence, factuality, or transparency?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper evaluates the verifiability of four popular generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat - using human evaluation. Verifiability refers to the extent to which generated statements are supported by comprehensive and accurate citations. The evaluation considers fluency, perceived utility, citation recall (proportion of statements supported by citations), and citation precision (proportion of accurate citations). The results show that while fluency and perceived utility are high, citation recall and precision are low - only 51.5% of statements were fully supported and 74.5% of citations were accurate. This indicates existing systems frequently generate unsupported or misattributed claims, despite appearing helpful. Notably, perceived utility was inversely correlated with citation precision, suggesting paraphrased content boosts apparent utility while lowering precision. Overall, the concerningly low verifiability demonstrates limitations of current commercial systems, especially as they gain popularity, and underscores the need for improved trustworthiness. The paper contributes new evaluation metrics focused on verifiability and provides insights into deficiencies of existing systems.


## Summarize the paper in one sentence.

 The paper evaluates the verifiability of four generative search engines by measuring citation precision and recall, finding that responses are fluent and appear useful but frequently contain unsupported claims and inaccurate citations.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper evaluates the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat - by having human annotators measure the fluency, perceived utility, citation recall, and citation precision of generated responses across diverse queries. The results show that while existing systems produce fluent responses that appear informative, they frequently generate unsupported statements and inaccurate citations - on average just 51.5% of sentences were fully supported by citations (recall) and only 74.5% of citations supported their associated sentence (precision). Furthermore, citation precision was found to be inversely correlated with perceived utility, likely because systems tend to copy text from citations even when it is not relevant, inflating precision at the cost of utility. Overall, the concerningly low recall and precision despite high fluency and perceived utility increases the potential for existing systems to mislead users. The authors hope these results will further motivate developing truly trustworthy and verifiable generative search engines.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes citation recall and citation precision as metrics for evaluating verifiability in generative search engines. What are some potential limitations or drawbacks of using these metrics? Could they overlook certain important aspects of verifiability?

2. The paper finds that citation precision is negatively correlated with perceived utility. Why might this be the case? What are some potential explanations for this inverse relationship that goes beyond the hypothesis stated in the paper? 

3. For the citation precision metric, partially supporting citations are counted if they contribute to the full verifiability of a statement when combined with other citations. What are some potential issues with this approach? When might it overestimate true citation precision?

4. What are some potential issues that could arise when using sentence-level segmentation for evaluating citation recall and precision? When might this level of segmentation fail to capture important nuances?

5. The paper acknowledges that not all sentences make distinct factual claims that require verification. What might be some principles or guidelines for more precisely identifying factual claims within sentences during evaluation?

6. What are some alternative metrics that could complement citation recall and precision for evaluating verifiability? What aspects of verifiability might they better capture?

7. How might the performance on citation recall and precision change if evidence sentences were extracted automatically rather than by human annotators? What errors could arise?

8. The paper focuses evaluation on single-turn responses. How might verifiability metrics need to be adapted for multi-turn conversational scenarios?

9. What are some potential differences in how verifiability should be evaluated in open-domain vs narrow-domain generative search engines? Do the metrics generalize?

10. Beyond improved metrics, what are some model architecture designs, training techniques, or retrieval mechanisms that could improve verifiability? How might future systems better optimize for citation recall and precision?
