# [Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How verifiable are the responses generated by existing commercial generative search engines?The paper focuses on evaluating the verifiability of four popular generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. Specifically, it aims to assess these systems across two key dimensions of verifiability:1) Citation recall - The proportion of generated statements that are fully supported by citations. This measures how comprehensively the systems cite sources to back up their generated claims.2) Citation precision - The proportion of generated citations that actually support their associated statement. This measures how accurately the citations align with the content being cited. The overarching goal is to audit these commercial systems to gain a better understanding of their capabilities and limitations when it comes to producing verifiable responses. The paper conducts human evaluation on a diverse set of queries to assess the systems' citation recall and precision. The central hypothesis is that while these systems may produce fluent and seemingly informative responses, they likely lack sufficient verifiability in terms of comprehensive and accurate sourcing of their generated claims.In summary, the key research question is evaluating and quantifying the verifiability of existing commercial generative search engines through analysis of their citation recall and precision. The goal is to audit these rapidly adopted systems to surface potential issues around trustworthiness.


## What is the main contribution of this paper?

 The main contribution of this paper is conducting a human evaluation study to audit the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. The key findings are:- Existing generative search engines often produce fluent and seemingly useful responses, but frequently contain unsupported statements and inaccurate citations. On average, only 51.5% of generated sentences are fully supported by citations, and just 74.5% of citations actually support their associated sentence.- Citation recall and precision are inversely correlated with fluency and perceived utility. Responses that appear more helpful tend to have lower citation recall and precision. This suggests existing systems achieve fluency partly by generating unsupported or incorrect statements. - Analysis indicates this inverse correlation arises because some systems heavily copy or paraphrase text from their cited sources. This inflates precision and recall but reduces fluency and relevance to the user's query.- The authors argue these low recall and precision rates are concerning for systems rapidly gaining popularity as an information seeking tool. The facade of usefulness combined with unsupported statements increases their potential to mislead users.In summary, the key contribution is conducting an in-depth audit of verifiability in existing generative search engines. The results highlight the need for systems that combine fluency and usefulness with reliable sourcing of statements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my understanding, here is a 1 sentence TL;DR summary of the key points from the paper:The paper evaluates the verifiability (citation recall and precision) of four popular generative search engines on diverse queries, finding that while responses appear useful, they frequently contain unsupported claims and inaccurate citations, highlighting shortcomings in existing systems.


## How does this paper compare to other research in the same field?

 This paper provides a helpful analysis of the verifiability of current generative search engines by conducting a thorough human evaluation on their citation recall and precision. Here is how I see it comparing to other related work:- It builds on prior work like Nakano et al. (2021) and Menick et al. (2022) that trained models to provide evidence for their responses, by shifting focus to evaluating commercial systems that are already deployed. This provides valuable insight into the current state of real-world systems.- The evaluation protocol is more comprehensive than in some prior work. For example, Bohnet et al. (2023) evaluated only on NaturalQuestions queries with paragraph answers, whereas this paper uses 12 diverse query distributions. The coverage of different answer types is useful.- The introduction of citation recall and precision metrics provides a standardized way to quantify verifiability. This is more granular than just using overall human judgments of plausibility or faithfulness to evidence as in some prior work.- Analyzing the correlation between verifiability and fluency/perceived utility provides an interesting finding about current systems trading off these attributes. This highlights a concerning potential to mislead users.- The analysis of systems copying from citations explains one reason for the observed inverse correlation. The hypothesis that some systems struggle with content selection seems plausible.Overall, I think this paper makes an important contribution in thoroughly evaluating verifiability of existing systems. The metrics and findings provide a benchmark for future improvement. The analysis also yields useful insights about current limitations and challenges.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:- Developing generative search engines that can handle queries that do not have clear extractive answers on the Internet. The authors found that existing systems struggle on queries like the open-ended AllSouls essay questions or NaturalQuestions queries that require aggregating information across table cells or multiple sources. Improving performance on queries lacking extractive answers is an important direction.- Improving content selection and relevance ranking in generative search engines. The authors observe that current systems struggle to appropriately weigh and select citations of varying relevance. For example, when responding to historical ELI5 queries, systems only lightly utilize the original source ELI5 webpage even when it is retrieved. Better weighing sources and citations of differing relevance is a direction for improvement.- Combining the strengths of extraction and abstraction. The authors believe it is possible to build systems with both high verifiability (through comprehensive and accurate citations) as well as high fluency and utility (through abstraction). Existing systems tend to tradeoff between extraction (improving verifiability metrics like citation recall and precision) and abstraction (improving fluency and perceived utility). New techniques could aim to get the best of both worlds.- Developing improved evaluation protocols and benchmarks. The authors propose the citation recall and precision metrics to encourage verifiable systems, but also discuss their limitations (e.g. dependence on sentence segmentation). Coming up with better automatic verifiability metrics is an area for future work. Expanding the diversity of evaluation queries is another direction.In summary, key future directions include better handling queries lacking clear extractive answers, improving content selection and citation weighting, combining extraction and abstraction, and developing improved evaluation methods for verifiability. The authors frame their work as an audit of existing systems to motivate progress on these fronts.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper evaluates the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. It conducts human evaluation across a diverse set of 1450 queries to measure the fluency, perceived utility, citation recall, and citation precision of responses from these systems. The key findings are that while existing generative search engines produce fluent responses that appear useful and informative, they frequently generate unsupported statements and inaccurate citations. On average only 51.5% of sentences are fully supported by citations (recall) and 74.5% of citations support their associated sentence (precision). Furthermore, fluency/utility are inversely correlated with citation recall/precision, likely because some systems copy text from citations, inflating precision/recall but reducing fluency/utility. Overall the paper provides concerning results about the reliability of existing generative search engines, and argues improved verifiability is critical as these systems become more mainstream.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper evaluates the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. Verifiability refers to whether the search engines comprehensively cite sources to support their generated statements (citation recall) and whether the cited sources accurately support the statements (citation precision). The authors conduct a human evaluation study across a diverse set of 1450 queries requiring different types of knowledge. They find that while the search engines generate fluent and seemingly useful responses, the citation recall and precision are quite low - on average only 51.5% of statements are fully supported by citations and 74.5% of citations support the associated statement. The authors observe an inverse correlation between verifiability metrics and fluency/usefulness, hypothesizing that some engines copy verbatim from sources, inflating precision/recall while reducing fluency. Overall, the concerningly low verifiability indicates these rapidly adopted search engines could potentially mislead users despite their veneer of reliability. The authors call for developing more trustworthy search engines and for better understanding of current commercial systems.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper "Evaluating Verifiability in Generative Search Engines":The authors conducted a human evaluation to audit the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. They evaluated these systems on a diverse set of 1450 queries from sources like historical Google user queries, open-ended questions from Reddit, etc. that required different types of knowledge. For each query-response pair, they had human annotators evaluate the fluency, perceived utility, citation recall, and citation precision. Citation recall measured the proportion of sentences that were fully supported by citations, while citation precision measured the proportion of citations that supported their associated sentence. By combining citation recall and precision, they calculated a citation F1 score to evaluate systems' overall verifiability. Through this process, they found concerningly low citation recall and precision for existing systems, suggesting that their responses frequently contain unsupported statements and inaccurate citations despite appearing useful. The main method was thus a comprehensive human evaluation protocol focused on verifiability metrics like citation recall and precision.


## What problem or question is the paper addressing?

 The paper is addressing the issue of evaluating verifiability in generative search engines. Generative search engines directly generate responses to user queries, along with inline citations. A key trait of a trustworthy generative search engine is verifiability - the system should cite sources comprehensively (high citation recall) and accurately (high citation precision). The paper conducts an audit of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat - to evaluate their verifiability across a diverse set of queries. Specifically, the paper examines whether generated responses contain unsupported statements or inaccurate citations by measuring citation recall (proportion of statements supported by citations) and citation precision (proportion of citations that support their associated statement). The key question the paper is investigating is: are existing commercial generative search engines producing verifiable responses that comprehensively and accurately cite their sources, or do they frequently generate unsupported claims and inaccurate citations? Evaluating verifiability is important for understanding if these rapidly adopted systems can be trusted to provide users with reliable information.
