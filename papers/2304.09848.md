# [Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How verifiable are the responses generated by existing commercial generative search engines?The paper focuses on evaluating the verifiability of four popular generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. Specifically, it aims to assess these systems across two key dimensions of verifiability:1) Citation recall - The proportion of generated statements that are fully supported by citations. This measures how comprehensively the systems cite sources to back up their generated claims.2) Citation precision - The proportion of generated citations that actually support their associated statement. This measures how accurately the citations align with the content being cited. The overarching goal is to audit these commercial systems to gain a better understanding of their capabilities and limitations when it comes to producing verifiable responses. The paper conducts human evaluation on a diverse set of queries to assess the systems' citation recall and precision. The central hypothesis is that while these systems may produce fluent and seemingly informative responses, they likely lack sufficient verifiability in terms of comprehensive and accurate sourcing of their generated claims.In summary, the key research question is evaluating and quantifying the verifiability of existing commercial generative search engines through analysis of their citation recall and precision. The goal is to audit these rapidly adopted systems to surface potential issues around trustworthiness.


## What is the main contribution of this paper?

The main contribution of this paper is conducting a human evaluation study to audit the verifiability of four popular commercial generative search engines - Bing Chat, NeevaAI, perplexity.ai, and YouChat. The key findings are:- Existing generative search engines often produce fluent and seemingly useful responses, but frequently contain unsupported statements and inaccurate citations. On average, only 51.5% of generated sentences are fully supported by citations, and just 74.5% of citations actually support their associated sentence.- Citation recall and precision are inversely correlated with fluency and perceived utility. Responses that appear more helpful tend to have lower citation recall and precision. This suggests existing systems achieve fluency partly by generating unsupported or incorrect statements. - Analysis indicates this inverse correlation arises because some systems heavily copy or paraphrase text from their cited sources. This inflates precision and recall but reduces fluency and relevance to the user's query.- The authors argue these low recall and precision rates are concerning for systems rapidly gaining popularity as an information seeking tool. The facade of usefulness combined with unsupported statements increases their potential to mislead users.In summary, the key contribution is conducting an in-depth audit of verifiability in existing generative search engines. The results highlight the need for systems that combine fluency and usefulness with reliable sourcing of statements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, here is a 1 sentence TL;DR summary of the key points from the paper:The paper evaluates the verifiability (citation recall and precision) of four popular generative search engines on diverse queries, finding that while responses appear useful, they frequently contain unsupported claims and inaccurate citations, highlighting shortcomings in existing systems.


## How does this paper compare to other research in the same field?

This paper provides a helpful analysis of the verifiability of current generative search engines by conducting a thorough human evaluation on their citation recall and precision. Here is how I see it comparing to other related work:- It builds on prior work like Nakano et al. (2021) and Menick et al. (2022) that trained models to provide evidence for their responses, by shifting focus to evaluating commercial systems that are already deployed. This provides valuable insight into the current state of real-world systems.- The evaluation protocol is more comprehensive than in some prior work. For example, Bohnet et al. (2023) evaluated only on NaturalQuestions queries with paragraph answers, whereas this paper uses 12 diverse query distributions. The coverage of different answer types is useful.- The introduction of citation recall and precision metrics provides a standardized way to quantify verifiability. This is more granular than just using overall human judgments of plausibility or faithfulness to evidence as in some prior work.- Analyzing the correlation between verifiability and fluency/perceived utility provides an interesting finding about current systems trading off these attributes. This highlights a concerning potential to mislead users.- The analysis of systems copying from citations explains one reason for the observed inverse correlation. The hypothesis that some systems struggle with content selection seems plausible.Overall, I think this paper makes an important contribution in thoroughly evaluating verifiability of existing systems. The metrics and findings provide a benchmark for future improvement. The analysis also yields useful insights about current limitations and challenges.
