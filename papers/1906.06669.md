# [One Epoch Is All You Need](https://arxiv.org/abs/1906.06669)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether training an unsupervised model like a Transformer language model for just one epoch on a large dataset can yield better performance compared to the standard practice of training for multiple epochs on a smaller dataset. The key hypotheses appear to be:- Training on more diverse data from a larger dataset for one epoch can improve model performance compared to training on a smaller dataset for multiple epochs, where samples are reused.- Eliminating regularization like dropout in one epoch training can improve performance since overfitting is less of an issue.- Adjusting model size and number of iterations appropriately for the dataset size can further optimize one epoch training.- Following these practices could allow unsupervised models like BERT and GPT-2 to be trained much more efficiently, possibly reducing costs significantly.So in summary, the central research question seems to be re-evaluating the standard multi-epoch training approach in favor of single epoch training on larger datasets to improve performance and training efficiency of unsupervised models. The paper hypothesizes and provides evidence that this could be a superior training strategy.
