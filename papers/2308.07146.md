# [CTP: Towards Vision-Language Continual Pretraining via Compatible   Momentum Contrast and Topology Preservation](https://arxiv.org/abs/2308.07146)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop effective methods for vision-language continual pretraining (VLCP) that allow vision-language models to continuously learn from non-stationary data streams? The key points are:- Existing vision-language pretraining (VLP) models are trained offline on fixed datasets and cannot continually accumulate knowledge from new data over time. - Most prior continual learning work focuses on image classification, but VLP has different challenges like fixed-dimensional embeddings and lack of contrastive samples from old tasks.- The authors propose a new dataset and method for VLCP:1) They contribute a new dataset called P9D with over 1 million image-text pairs split into 9 continual pretraining tasks based on different product domains.2) They propose a method called CTP (Compatible momentum contrast with Topology Preservation) that uses a compatible momentum model to absorb old and new knowledge for updating encoders, along with topology preservation to maintain sample relationships across tasks.So in summary, the main research question is how to develop effective VLCP methods that can continuously learn from non-stationary streams of vision-language data, which is addressed through the proposed CTP approach and new continual pretraining dataset.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper are:1. It proposes the first benchmark dataset for vision-language continual pretraining (VLCP). The dataset contains over 1 million image-text pairs from 9 different industries/domains. This allows simulating continual learning on non-stationary data streams.2. It provides a comprehensive study on the characteristics and challenges of VLCP compared to conventional class-incremental learning. Key differences highlighted are the fixed embedding dimensions, lack of contrastive samples from old tasks, and joint optimization of multi-modal encoders. 3. It proposes a new VLCP method called Compatible Momentum Contrast (CMC) with Topology Preservation (TP). CMC uses a compatible momentum model to absorb knowledge from both old and new tasks to update the encoders. TP transfers topology knowledge of old embedding while allowing flexibility.4. Extensive experiments compare proposed method with baseline approaches adapted from classical continual learning literature. The method achieves superior performance in both memory-free and memory-buffer settings without expensive training overhead.In summary, the key contribution is proposing a new VLCP setting, benchmark, and effective method to allow vision-language models to continually learn from non-stationary data streams. This is an important problem as offline pretraining paradigms are not sustainable with ever-expanding web data.
