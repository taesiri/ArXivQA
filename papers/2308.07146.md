# [CTP: Towards Vision-Language Continual Pretraining via Compatible   Momentum Contrast and Topology Preservation](https://arxiv.org/abs/2308.07146)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop effective methods for vision-language continual pretraining (VLCP) that allow vision-language models to continuously learn from non-stationary data streams? The key points are:- Existing vision-language pretraining (VLP) models are trained offline on fixed datasets and cannot continually accumulate knowledge from new data over time. - Most prior continual learning work focuses on image classification, but VLP has different challenges like fixed-dimensional embeddings and lack of contrastive samples from old tasks.- The authors propose a new dataset and method for VLCP:1) They contribute a new dataset called P9D with over 1 million image-text pairs split into 9 continual pretraining tasks based on different product domains.2) They propose a method called CTP (Compatible momentum contrast with Topology Preservation) that uses a compatible momentum model to absorb old and new knowledge for updating encoders, along with topology preservation to maintain sample relationships across tasks.So in summary, the main research question is how to develop effective VLCP methods that can continuously learn from non-stationary streams of vision-language data, which is addressed through the proposed CTP approach and new continual pretraining dataset.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper are:1. It proposes the first benchmark dataset for vision-language continual pretraining (VLCP). The dataset contains over 1 million image-text pairs from 9 different industries/domains. This allows simulating continual learning on non-stationary data streams.2. It provides a comprehensive study on the characteristics and challenges of VLCP compared to conventional class-incremental learning. Key differences highlighted are the fixed embedding dimensions, lack of contrastive samples from old tasks, and joint optimization of multi-modal encoders. 3. It proposes a new VLCP method called Compatible Momentum Contrast (CMC) with Topology Preservation (TP). CMC uses a compatible momentum model to absorb knowledge from both old and new tasks to update the encoders. TP transfers topology knowledge of old embedding while allowing flexibility.4. Extensive experiments compare proposed method with baseline approaches adapted from classical continual learning literature. The method achieves superior performance in both memory-free and memory-buffer settings without expensive training overhead.In summary, the key contribution is proposing a new VLCP setting, benchmark, and effective method to allow vision-language models to continually learn from non-stationary data streams. This is an important problem as offline pretraining paradigms are not sustainable with ever-expanding web data.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is how I would compare it to other research in the field:- This paper introduces a new dataset and method for vision-language continual pretraining (VLCP), which aims to enable vision-language models to continuously learn from non-stationary data streams. VLCP is a relatively new area of research compared to the more established fields of vision-language pretraining (VLP) and continual learning. - For datasets, this paper contributes P9D, the first large-scale dataset designed specifically for VLCP research. It contains over 1 million image-text pairs across 9 different domains/tasks. This is much larger and more diverse than existing continual learning datasets like CIFAR and ImageNet-Subset. Other VLP datasets like CC3M and CC12M are larger but not designed for continual learning. So P9D fills an important gap.- For methods, the paper proposes a new approach CTP that combines compatible momentum contrast and topology preservation to address the challenges of multi-modal optimization and forgetting in VLCP. This is novel compared to existing continual learning methods like EWC, LwF, etc. which focus on class incremental learning for a single modality. Applying and adapting such methods to VLCP is also a contribution of this paper.- Compared to other VLCP works, this paper provides a more comprehensive study and evaluation. The comparisons to various continual learning methods adapted to VLCP provide useful baselines and analysis. And CTP outperforms prior arts in memory-based and memory-free scenarios. - The evaluation metrics are also more thorough, assessing both cross-modal retrieval and multi-modal clustering which test different capabilities relevant for VLCP.- Overall, this paper makes excellent progress in introducing and formalizing the VLCP problem, providing datasets/baselines, and developing a new method CTP that pushes state-of-the-art in this nascent but important research area. The results highlight the challenges unique to VLCP vs VLP and incremental learning.In summary, I would say this paper makes significant contributions to the relatively new field of VLCP through its introduction of the P9D dataset, comprehensive benchmarking of methods, and development of the CTP algorithm which advances the state-of-the-art. It opens up many exciting avenues for future work in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some potential future research directions the authors suggest include:- Developing continual learning methods for other vision-language tasks beyond just pretraining, such as visual question answering, image captioning, etc. They suggest their CTP approach could be extended to these other settings.- Exploring different architectures for continual vision-language pretraining besides the ALBEF framework they used. They suggest studying how different model architectures impact continual learning performance.- Developing datasets better suited for continual pretraining. They note limitations of existing datasets and suggest collecting new datasets explicitly designed to test continual learning of visual concepts from non-stationary data streams.- Studying replay strategies and memory management for continual pretraining. The authors note memory replay helps but incurs storage costs for large-scale pretraining. They suggest exploring efficient replay mechanisms.- Extending continual pretraining to even larger models and datasets. The authors suggest scaling up continual pretraining as model and dataset sizes continue to grow.- Studying task-adaptive continual learning methods that can dynamically adjust to new tasks and data distributions. The preset task sequence may differ from real-world shifts.- Considering multimodal knowledge consolidation in continual pretraining. The authors suggest going beyond just vision and language to learn from additional modalities over time.So in summary, the main future directions relate to tasks, architectures, datasets, replay mechanisms, scaling, adaptation, and multimodality for advancing continual pretraining research. The authors lay out several interesting open challenges for the field.
