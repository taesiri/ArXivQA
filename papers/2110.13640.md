# s2s-ft: Fine-Tuning Pretrained Transformer Encoders for   Sequence-to-Sequence Learning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can pretrained bidirectional Transformers like BERT be effectively fine-tuned for sequence-to-sequence natural language generation tasks?The key hypothesis is that by using appropriate self-attention masks and fine-tuning approaches adapted from UniLM, the capabilities of powerful pretrained bidirectional models like BERT, RoBERTa, and UniLMv2 can be leveraged to achieve strong performance on conditional text generation tasks. Specifically, the paper proposes and compares three fine-tuning methods:1) Causal fine-tuning: Generates target text autoregressively using a causal self-attention mask, similar to a causal LM.2) Masked fine-tuning: Randomly masks some target tokens and predicts them based on context, similar to masked LM pretraining. 3) Pseudo-masked fine-tuning: Adds pseudo mask tokens for target words to enable a compromise between the above two approaches.The central hypothesis is that pseudo-masked fine-tuning will perform best by avoiding position shift discrepancies while enabling full backpropagation, which is evaluated empirically on summarization and question generation tasks.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It presents a sequence-to-sequence fine-tuning toolkit called s2s-ft that adapts pretrained bidirectional Transformers like BERT, RoBERTa, ELECTRA, and UniLM for conditional natural language generation tasks. 2. It implements three sequence-to-sequence fine-tuning algorithms - causal fine-tuning, masked fine-tuning, and pseudo-masked fine-tuning - inspired by UniLM. These allow leveraging pretrained bidirectional Transformers for generation while avoiding discrepancies between pretraining and finetuning.3. It conducts extensive experiments on abstractive summarization and question generation benchmarks, in both monolingual and multilingual settings. The results demonstrate strong performance across tasks by fine-tuning strong pretrained models like UniLMv2 and RoBERTa with s2s-ft.4. It shows s2s-ft can be easily applied to multilingual generation tasks by using multilingual pretrained models like XLM-RoBERTa, allowing it to handle both monolingual and cross-lingual NLG.5. The paper introduces the s2s-ft toolkit that enables easy application of pretrained bidirectional Transformers to sequence-to-sequence learning, available at https://github.com/microsoft/unilm/tree/master/s2s-ft.In summary, the main contribution is presenting s2s-ft, a method and toolkit for effectively adapting pretrained bidirectional Transformers to conditional natural language generation via fine-tuning, enabling leveraging their capabilities for NLG tasks. The experiments demonstrate strong performance on various benchmarks.
