# [Extreme Masking for Learning Instance and Distributed Visual   Representations](https://arxiv.org/abs/2206.04667)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we learn both distributed and holistic representations for images using only instance-level supervision from extreme random masking?

The key points are:

- The paper explores using extreme random masking (75-90% of image patches masked) as a novel form of data augmentation to train a siamese network for self-supervised representation learning. 

- This is in contrast to prior work like BEiT and MAE that use masking mainly for token-level prediction objectives. Here masking is used just for data augmentation without token-level supervision.

- The model adopts a momentum encoder and base encoder architecture. The momentum encoder sees the full image, while the base encoder sees the masked version. The objective is to train the base encoder's representation of the masked image to predict the momentum encoder's representation of the full image.

- This allows the model to learn both distributed representations over patches (from the base encoder) and a holistic image representation (from the momentum encoder's output) with just instance-level supervision. 

- The extreme masking creates a difficult prediction task that requires capturing informative details, rather than just high-level semantics.

So in summary, the central hypothesis is that extreme masking augmentation along with instance-level prediction can allow learning both detailed distributed representations and holistic representations without token-level supervision. The experiments aim to validate the effectiveness of this approach.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It presents a simple and effective approach called ExtreMA for self-supervised visual representation learning using extreme random masking as the main data augmentation. 

2. The proposed method uses a momentum encoder to process the full image and a base encoder to process a small random subset of image patches (10%-25%). The base encoder is trained to predict the representation produced by the momentum encoder in a BYOL framework.

3. Through using an extremely high masking ratio of 75%-90%, the model is forced to encode as much information as possible from the small visible subset in order to recover the full image representation. This encourages learning informative visual variations rather than invariances.

4. The model learns distributed patch-level representations as well as a holistic image-level representation simultaneously using only instance-level supervision, without any token-level objectives.

5. ExtreMA demonstrates strong performance on ImageNet classification, using 5-10x less compute than prior self-supervised methods. It also shows good transfer learning results on downstream tasks like semi-supervised learning and semantic segmentation.

6. Analysis reveals that ExtreMA learns representations sensitive to spatial/scale variations unlike methods relying on invariance, and can reconstruct masked regions indicating its learned representations capture detailed visual information.

In summary, the key contribution is a simple and efficient self-supervised approach using extreme masking that learns high-quality visual representations surpassing prior arts on various benchmarks. The representations learned are shown to be sensitive to spatial/scale details rather than just invariant features.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a scalable approach for learning spatially distributed visual representations over individual tokens and a holistic instance representation simultaneously using extreme masking ratios as the supervision signal.
