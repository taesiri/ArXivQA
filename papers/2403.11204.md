# [Partitioned Neural Network Training via Synthetic Intermediate Labels](https://arxiv.org/abs/2403.11204)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Training large and deep neural networks is very computationally intensive and limited by GPU memory constraints. 
- Existing strategies like data parallelism, model parallelism, pipeline parallelism have limitations in terms of communication overhead, memory usage and inability to customize training hyperparameters.

Proposed Solution:
- Partition the neural network into multiple segments and train each segment independently using synthetic intermediate labels instead of training the whole network end-to-end.
- The synthetic labels are randomly generated vectors that serve as targets for the intermediate layers during training.
- This approach reduces communication overhead between partitions, decreases memory usage per device and allows customization of training hyperparameters like epochs and batch size for each partition.

Main Contributions:
- Proposes a novel Partitioned Neural Network (PNN) training methodology that is more efficient than conventional training approaches.
- Reduces computational load by allowing uneven training of partitions (e.g. 5 epochs for left partition, 160 epochs for right).
- Achieves testing accuracy (71.5%) comparable to baseline model (76.2%) while using significantly less computations.
- Demonstrates the approach on a 6-layer fully connected network partitioned into 2 segments and trained on EMNIST dataset.
- Shows the effect of key hyperparameters like number of epochs and synthetic label multiplier on accuracy.
- The method paves the way for more accessible development of large neural network models.

In summary, the paper makes important contributions in efficient neural network training by proposing a partitioned training approach with synthetic intermediate labels that reduces computational requirements while maintaining accuracy.
