# [Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted   Attacks](https://arxiv.org/abs/2212.09035)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to craft highly transferable adversarial examples for black-box targeted attack. In particular, the authors aim to theoretically analyze and improve the transferability of adversarial examples from the perspective of model discrepancy. 

The key hypothesis is that minimizing the maximum model discrepancy between substitute models when crafting adversarial examples will lead to higher transferability against unknown target models.

Summarize the key contributions of this paper:


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides a theoretical analysis of black-box targeted attacks from the perspective of model discrepancy, and derives a generalization error bound based on the attack error on the substitute model and the maximum model discrepancy. 

2. Based on the theoretical analysis, it proposes a new black-box targeted attack method called M3D (Minimizing Maximum Model Discrepancy) that minimizes the maximum discrepancy between two substitute models while attacking them. This results in more transferable adversarial examples.

3. It conducts extensive experiments on ImageNet using different models. The results demonstrate that the proposed M3D approach significantly outperforms prior state-of-the-art black-box targeted attack methods, especially when the black-box model has a large architecture difference from the substitute model.

In summary, the key contribution is introducing a theoretical model discrepancy perspective for analyzing and improving black-box targeted attacks, as well as an effective new attack method M3D based on this perspective. The approach is shown to generate more transferable and robust adversarial examples compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents a theoretical analysis of black-box targeted attacks from a model discrepancy perspective, proposes a new attack method called M3D that minimizes the maximum discrepancy between substitute models to craft highly transferable adversarial examples, and demonstrates the effectiveness of M3D with extensive experiments showing significant improvements over prior state-of-the-art methods.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research on adversarial attacks:

- The key novel contribution of this paper is using the model discrepancy perspective to analyze black-box targeted attacks. The authors provide theoretical justification and generalization error bounds based on model discrepancy, which is lacking in most prior work on adversarial attacks. 

- The proposed M3D attack method is the first to explicitly minimize the maximum model discrepancy when crafting adversarial examples. This makes the examples more transferable and robust to model variations compared to prior generative attack methods like GAP, CDA, and TTP.

- Extensive experiments on ImageNet with different models demonstrate the effectiveness of the M3D approach. It outperforms prior state-of-the-art methods, especially when the model architecture differs significantly between the substitute and target model.

- The paper focuses on targeted attacks, while most prior work has looked at untargeted attacks. The theoretical analysis provides new insight specifically for the targeted attack setting.

- The proposed approach follows recent trends in using generative models over iterative methods to craft adversarial examples. Generative approaches tend to have higher efficiency for large datasets.

- Compared to defense techniques, this paper falls into the category of new attack methods. The results against robust and adversarially trained models demonstrate the strength of the proposed attack.

Overall, this paper makes solid theoretical and empirical contributions to the field of adversarial machine learning. The model discrepancy perspective and generalization analysis help better understand transferable black-box attacks. The M3D approach advances state-of-the-art in targeted attack methods.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the conclusion section:

1. They propose exploring different surrogate loss functions for the model discrepancy term besides the cross-entropy loss used in this work. Other loss functions like Kullbackâ€“Leibler divergence or maximum mean discrepancy could potentially further improve the transferability.

2. They suggest exploring different architectures for the generator and discriminators besides the standard CNNs used in this work. More sophisticated generative models like GANs could potentially synthesize even more realistic and imperceptible perturbations. 

3. They propose investigating different adversarial training formulations besides the min-max game used here. For example, exploring collaborative adversarial training methods to make the training more stable.

4. They suggest applying the proposed model discrepancy minimization idea to other transfer-based attack settings besides black-box targeted attack, such as black-box untargeted attack or evasion of adversarial detection methods.

5. They propose exploring the theoretical understanding of transferability in greater depth, such as tighter bounds on the model discrepancy term.

In summary, the main future directions are exploring different loss functions, model architectures, adversarial training schemes, attack settings, and tighter theoretical bounds related to model discrepancy minimization for improving transferable black-box attacks.
