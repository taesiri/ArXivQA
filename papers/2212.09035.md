# [Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted   Attacks](https://arxiv.org/abs/2212.09035)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to craft highly transferable adversarial examples for black-box targeted attack. In particular, the authors aim to theoretically analyze and improve the transferability of adversarial examples from the perspective of model discrepancy. 

The key hypothesis is that minimizing the maximum model discrepancy between substitute models when crafting adversarial examples will lead to higher transferability against unknown target models.

Summarize the key contributions of this paper:


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides a theoretical analysis of black-box targeted attacks from the perspective of model discrepancy, and derives a generalization error bound based on the attack error on the substitute model and the maximum model discrepancy. 

2. Based on the theoretical analysis, it proposes a new black-box targeted attack method called M3D (Minimizing Maximum Model Discrepancy) that minimizes the maximum discrepancy between two substitute models while attacking them. This results in more transferable adversarial examples.

3. It conducts extensive experiments on ImageNet using different models. The results demonstrate that the proposed M3D approach significantly outperforms prior state-of-the-art black-box targeted attack methods, especially when the black-box model has a large architecture difference from the substitute model.

In summary, the key contribution is introducing a theoretical model discrepancy perspective for analyzing and improving black-box targeted attacks, as well as an effective new attack method M3D based on this perspective. The approach is shown to generate more transferable and robust adversarial examples compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents a theoretical analysis of black-box targeted attacks from a model discrepancy perspective, proposes a new attack method called M3D that minimizes the maximum discrepancy between substitute models to craft highly transferable adversarial examples, and demonstrates the effectiveness of M3D with extensive experiments showing significant improvements over prior state-of-the-art methods.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research on adversarial attacks:

- The key novel contribution of this paper is using the model discrepancy perspective to analyze black-box targeted attacks. The authors provide theoretical justification and generalization error bounds based on model discrepancy, which is lacking in most prior work on adversarial attacks. 

- The proposed M3D attack method is the first to explicitly minimize the maximum model discrepancy when crafting adversarial examples. This makes the examples more transferable and robust to model variations compared to prior generative attack methods like GAP, CDA, and TTP.

- Extensive experiments on ImageNet with different models demonstrate the effectiveness of the M3D approach. It outperforms prior state-of-the-art methods, especially when the model architecture differs significantly between the substitute and target model.

- The paper focuses on targeted attacks, while most prior work has looked at untargeted attacks. The theoretical analysis provides new insight specifically for the targeted attack setting.

- The proposed approach follows recent trends in using generative models over iterative methods to craft adversarial examples. Generative approaches tend to have higher efficiency for large datasets.

- Compared to defense techniques, this paper falls into the category of new attack methods. The results against robust and adversarially trained models demonstrate the strength of the proposed attack.

Overall, this paper makes solid theoretical and empirical contributions to the field of adversarial machine learning. The model discrepancy perspective and generalization analysis help better understand transferable black-box attacks. The M3D approach advances state-of-the-art in targeted attack methods.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the conclusion section:

1. They propose exploring different surrogate loss functions for the model discrepancy term besides the cross-entropy loss used in this work. Other loss functions like Kullbackâ€“Leibler divergence or maximum mean discrepancy could potentially further improve the transferability.

2. They suggest exploring different architectures for the generator and discriminators besides the standard CNNs used in this work. More sophisticated generative models like GANs could potentially synthesize even more realistic and imperceptible perturbations. 

3. They propose investigating different adversarial training formulations besides the min-max game used here. For example, exploring collaborative adversarial training methods to make the training more stable.

4. They suggest applying the proposed model discrepancy minimization idea to other transfer-based attack settings besides black-box targeted attack, such as black-box untargeted attack or evasion of adversarial detection methods.

5. They propose exploring the theoretical understanding of transferability in greater depth, such as tighter bounds on the model discrepancy term.

In summary, the main future directions are exploring different loss functions, model architectures, adversarial training schemes, attack settings, and tighter theoretical bounds related to model discrepancy minimization for improving transferable black-box attacks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper studies the black-box targeted attack problem from the perspective of model discrepancy. The authors first present a generalization error bound for black-box targeted attacks based on model discrepancy, revealing that the attack error depends on the attack error on the substitute model and the model discrepancy between the substitute and target models. Based on this analysis, they propose a new approach called Minimizing Maximum Model Discrepancy (M3D) attack, which trains a generator to craft adversarial examples that can fool two substitute models with maximum discrepancy. This minimizes the bound on generalization error, improving transferability. Experiments on ImageNet show their method substantially outperforms prior arts, especially when the target model is very different from the substitute model. The key novelty is utilizing model discrepancy theory to improve transferability of targeted attacks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper studies the black-box targeted attack problem from the perspective of model discrepancy. The authors first present a theoretical generalization error bound for black-box targeted attacks. This bound shows that the attack error on a target model depends on two terms: the attack error on the substitute model used to craft the adversarial examples, and the maximum model discrepancy between models in the hypothesis space. 

Based on this analysis, the authors propose a new attack method called Minimizing Maximum Model Discrepancy (M3D). M3D uses two substitute models which are trained adversarially - one model tries to maximize the discrepancy between their outputs on adversarial examples, while a generator tries to minimize this discrepancy. By doing so, the generator crafts examples that are robust to model variation, improving transferability. Experiments on ImageNet show M3D substantially outperforms prior work, especially when the target model is very different from the substitute model. The method also transfers better to robust and real-world models.

In summary, this paper provides a theoretical justification for model discrepancy being key to transferability, and develops a practical attack method based on this theory that achieves state-of-the-art black-box targeted attack performance.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper proposes a new black-box targeted attack method called Minimizing Maximum Model Discrepancy (M3D). The key idea is to train a generator G to craft adversarial examples that can fool two substitute models D1 and D2, while simultaneously minimizing the discrepancy between D1 and D2's predictions on the adversarial examples. Specifically, G tries to generate examples that attack D1 and D2 to output a specified target class. Meanwhile, D1 and D2 are trained adversarially to maximize their prediction discrepancy on G's outputs. By playing this min-max game, G learns to craft adversarial examples that are transferable and robust to differences between substitute models. At test time, G can generate adversarial examples to attack an unseen black-box model, since its outputs are insensitive to model variation. Experiments on ImageNet show M3D substantially outperforms prior arts for black-box targeted attack.


## What problem or question is the paper addressing?

 This paper is addressing the problem of black-box targeted adversarial attacks, where the goal is to craft adversarial examples that can fool a target model into misclassifying inputs as a specified target class, without access to the model parameters or gradients. The key question the paper aims to address is how to create adversarial examples that can transfer more effectively to unknown target models. 

The main contributions are:

1) They provide a theoretical analysis of black-box targeted attacks from a model discrepancy perspective, deriving a generalization error bound that depends on two terms - the attack error on the substitute model and the maximum model discrepancy. 

2) Based on this analysis, they propose a new attack method called Minimizing Maximum Model Discrepancy (M3D) which trains a generator to craft adversarial examples that can fool two substitute models that are trained to maximize their discrepancy. This allows it to create examples robust to model variation.

3) They demonstrate strong empirical performance, with M3D outperforming prior state-of-the-art methods on attacking various models on ImageNet, including robust and real-world models, showing its ability to create highly transferable adversarial examples.

In summary, the key novelty is introducing a model discrepancy view for black-box attacks and using it to derive an effective new attack that minimizes this discrepancy by design. The result is adversarial examples that transfer much more effectively to diverse target models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Black-box targeted attack - The paper focuses on generating adversarial examples that can successfully attack an unknown "black-box" model by misleading it to output a specific target class. 

- Transferability - The property that adversarial examples crafted on one model can often fool other models. Leveraging transferability is key for black-box attacks.

- Model discrepancy - The paper provides an analysis of black-box targeted attacks based on model discrepancy, which measures the difference between model predictions. 

- Generalization error bound - The paper derives a theoretical upper bound on the attack error against a black-box model based on substitute model error and maximum model discrepancy.

- Minimizing Maximum Model Discrepancy (M3D) - The proposed attack method that trains a generator to craft adversarial examples that can fool two substitute models with maximized discrepancy, making the examples more transferable.

- Generator-discriminator framework - The M3D method is implemented using a generator to craft adversarial examples, along with two discriminator models acting as substitutes. The models are trained in an adversarial manner.

- Robustness - The paper demonstrates that the adversarial examples crafted by the proposed M3D approach are robust not only across architectures but also against various defense mechanisms.

- State-of-the-art results - Experiments show the M3D attack significantly outperforms prior arts in targeted black-box attacks on ImageNet models.

In summary, the key focus is generating transferable targeted adversarial examples by minimizing the maximum discrepancy between substitute models in a generator-discriminator framework.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a generalization error bound for black-box targeted attacks based on model discrepancy. Can you explain in more detail how this bound is derived and what key assumptions are made? 

2. The proposed M3D attack method trains a generator G and two discriminators D1 and D2. What is the intuition behind using two discriminators rather than just one? How do the two discriminators help improve transferability?

3. In the min-max objective function for M3D, what is the purpose of the term Ld that maximizes the discrepancy between D1 and D2? How does minimizing the maximum discrepancy help improve transferability? 

4. The paper shows M3D performs much better compared to prior arts when the substitute and blackbox models have very different architectures (e.g. VGG and ResNet). Why does the proposed method work better in these cases?

5. The M3D method alternates between updating the generator and discriminators. What would happen if you updated only the generator or only the discriminators? Why is the alternating update important?

6. How does M3D compare against other transferability improvement techniques like ensemble/data augmentation? What are the limitations of those techniques that M3D addresses?

7. The paper evaluates M3D on ImageNet models. How do you think the performance would change on other datasets or tasks like CIFAR or segmentation? What adaptations would be needed?

8. The proposed method focuses on targeted attacks. How suitable do you think it is for untargeted attacks? What modifications would you suggest to adapt M3D for untargeted attacks? 

9. The paper mentions hypothesis discrepancy principles used in domain adaptation. Are there any other connections between adversarial transferability and domain adaptation that could be further explored? 

10. Beyond improving transferability, what other potential benefits or applications do you see for explicitly minimizing maximum model discrepancy as done in this paper?
