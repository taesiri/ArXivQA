# [Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted   Attacks](https://arxiv.org/abs/2212.09035)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to craft highly transferable adversarial examples for black-box targeted attack. In particular, the authors aim to theoretically analyze and improve the transferability of adversarial examples from the perspective of model discrepancy. 

The key hypothesis is that minimizing the maximum model discrepancy between substitute models when crafting adversarial examples will lead to higher transferability against unknown target models.

Summarize the key contributions of this paper:


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides a theoretical analysis of black-box targeted attacks from the perspective of model discrepancy, and derives a generalization error bound based on the attack error on the substitute model and the maximum model discrepancy. 

2. Based on the theoretical analysis, it proposes a new black-box targeted attack method called M3D (Minimizing Maximum Model Discrepancy) that minimizes the maximum discrepancy between two substitute models while attacking them. This results in more transferable adversarial examples.

3. It conducts extensive experiments on ImageNet using different models. The results demonstrate that the proposed M3D approach significantly outperforms prior state-of-the-art black-box targeted attack methods, especially when the black-box model has a large architecture difference from the substitute model.

In summary, the key contribution is introducing a theoretical model discrepancy perspective for analyzing and improving black-box targeted attacks, as well as an effective new attack method M3D based on this perspective. The approach is shown to generate more transferable and robust adversarial examples compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents a theoretical analysis of black-box targeted attacks from a model discrepancy perspective, proposes a new attack method called M3D that minimizes the maximum discrepancy between substitute models to craft highly transferable adversarial examples, and demonstrates the effectiveness of M3D with extensive experiments showing significant improvements over prior state-of-the-art methods.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research on adversarial attacks:

- The key novel contribution of this paper is using the model discrepancy perspective to analyze black-box targeted attacks. The authors provide theoretical justification and generalization error bounds based on model discrepancy, which is lacking in most prior work on adversarial attacks. 

- The proposed M3D attack method is the first to explicitly minimize the maximum model discrepancy when crafting adversarial examples. This makes the examples more transferable and robust to model variations compared to prior generative attack methods like GAP, CDA, and TTP.

- Extensive experiments on ImageNet with different models demonstrate the effectiveness of the M3D approach. It outperforms prior state-of-the-art methods, especially when the model architecture differs significantly between the substitute and target model.

- The paper focuses on targeted attacks, while most prior work has looked at untargeted attacks. The theoretical analysis provides new insight specifically for the targeted attack setting.

- The proposed approach follows recent trends in using generative models over iterative methods to craft adversarial examples. Generative approaches tend to have higher efficiency for large datasets.

- Compared to defense techniques, this paper falls into the category of new attack methods. The results against robust and adversarially trained models demonstrate the strength of the proposed attack.

Overall, this paper makes solid theoretical and empirical contributions to the field of adversarial machine learning. The model discrepancy perspective and generalization analysis help better understand transferable black-box attacks. The M3D approach advances state-of-the-art in targeted attack methods.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the conclusion section:

1. They propose exploring different surrogate loss functions for the model discrepancy term besides the cross-entropy loss used in this work. Other loss functions like Kullbackâ€“Leibler divergence or maximum mean discrepancy could potentially further improve the transferability.

2. They suggest exploring different architectures for the generator and discriminators besides the standard CNNs used in this work. More sophisticated generative models like GANs could potentially synthesize even more realistic and imperceptible perturbations. 

3. They propose investigating different adversarial training formulations besides the min-max game used here. For example, exploring collaborative adversarial training methods to make the training more stable.

4. They suggest applying the proposed model discrepancy minimization idea to other transfer-based attack settings besides black-box targeted attack, such as black-box untargeted attack or evasion of adversarial detection methods.

5. They propose exploring the theoretical understanding of transferability in greater depth, such as tighter bounds on the model discrepancy term.

In summary, the main future directions are exploring different loss functions, model architectures, adversarial training schemes, attack settings, and tighter theoretical bounds related to model discrepancy minimization for improving transferable black-box attacks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper studies the black-box targeted attack problem from the perspective of model discrepancy. The authors first present a generalization error bound for black-box targeted attacks based on model discrepancy, revealing that the attack error depends on the attack error on the substitute model and the model discrepancy between the substitute and target models. Based on this analysis, they propose a new approach called Minimizing Maximum Model Discrepancy (M3D) attack, which trains a generator to craft adversarial examples that can fool two substitute models with maximum discrepancy. This minimizes the bound on generalization error, improving transferability. Experiments on ImageNet show their method substantially outperforms prior arts, especially when the target model is very different from the substitute model. The key novelty is utilizing model discrepancy theory to improve transferability of targeted attacks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper studies the black-box targeted attack problem from the perspective of model discrepancy. The authors first present a theoretical generalization error bound for black-box targeted attacks. This bound shows that the attack error on a target model depends on two terms: the attack error on the substitute model used to craft the adversarial examples, and the maximum model discrepancy between models in the hypothesis space. 

Based on this analysis, the authors propose a new attack method called Minimizing Maximum Model Discrepancy (M3D). M3D uses two substitute models which are trained adversarially - one model tries to maximize the discrepancy between their outputs on adversarial examples, while a generator tries to minimize this discrepancy. By doing so, the generator crafts examples that are robust to model variation, improving transferability. Experiments on ImageNet show M3D substantially outperforms prior work, especially when the target model is very different from the substitute model. The method also transfers better to robust and real-world models.

In summary, this paper provides a theoretical justification for model discrepancy being key to transferability, and develops a practical attack method based on this theory that achieves state-of-the-art black-box targeted attack performance.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper proposes a new black-box targeted attack method called Minimizing Maximum Model Discrepancy (M3D). The key idea is to train a generator G to craft adversarial examples that can fool two substitute models D1 and D2, while simultaneously minimizing the discrepancy between D1 and D2's predictions on the adversarial examples. Specifically, G tries to generate examples that attack D1 and D2 to output a specified target class. Meanwhile, D1 and D2 are trained adversarially to maximize their prediction discrepancy on G's outputs. By playing this min-max game, G learns to craft adversarial examples that are transferable and robust to differences between substitute models. At test time, G can generate adversarial examples to attack an unseen black-box model, since its outputs are insensitive to model variation. Experiments on ImageNet show M3D substantially outperforms prior arts for black-box targeted attack.
