# [Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams](https://arxiv.org/abs/2403.15455)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Learning from text streams poses challenges due to concept drift (changes in data distribution over time), which can degrade model performance. 
- Fine-tuning pre-trained language models like SentenceBERT (SBERT) can help address concept drift, but using all new data is costly. 
- Selecting representative text samples for fine-tuning can provide useful information while reducing computational costs.

Proposed Solution:
- Evaluate 7 text sampling methods to selectively fine-tune SBERT on new data and mitigate performance degradation:
  - Random sampling
  - Length-based sampling 
  - TF-IDF based sampling
  - Proposed WordPieceToken ratio sampling (based on ratio of wordpieces to tokens)
  - Versions of above accounting for text labels/classes
- Assess impact across 4 SBERT loss functions: BATL, CTL, OCL, SL
- Use metrics: Macro F1 Score, Elapsed time
- Evaluate in text stream classification scenario on Airbnb and Yelp datasets  

Key Contributions:
- Extensive comparison of text sampling methods for SBERT fine-tuning in streaming context
- Analysis of sampling methods' impact across loss functions 
- Evaluation of SBERT loss functions for text stream classification 
- Novel WordPieceToken ratio sampling method that weights texts by ratio of wordpieces to tokens
- WordPieceToken ratio (with classes) obtained best Macro F1-Scores in most cases
- BATL and SL most suitable loss functions; CTL and OCL insufficient
- Fine-tuning with text sampling can help address concept drift in text streams

In summary, the paper explores selective fine-tuning strategies to adapt pre-trained language models to new data in text streams, mitigating concept drift. The proposed sampling method and findings around loss functions provide insights into effective text stream classification.
