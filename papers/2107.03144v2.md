# [Neural Contextual Bandits without Regret](https://arxiv.org/abs/2107.03144v2)

## What is the central research question or hypothesis that this paper addresses?

 The key research question addressed in this paper is whether it is possible to develop neural network-based contextual bandit algorithms with provable sublinear regret bounds. 

Specifically, the authors aim to design optimistic contextual bandit algorithms that leverage neural networks to estimate the unknown reward function, and analyze their regret properties. They consider both fully-connected and convolutional neural networks.

The main hypothesis is that by analyzing regret in terms of the maximum information gain of the Neural Tangent Kernel, it should be possible to obtain sublinear regret bounds for neural contextual bandits under general assumptions on the reward function. 

The paper introduces algorithms NTK-UCB, NN-UCB and their convolutional variants, and shows that their regret grows as Õ(T^{(2d-1)/2d}) where d is the context dimension. This implies convergence to the optimal policy.

In summary, the central research question is whether neural network-based contextual bandits can achieve no-regret, and the main hypothesis is that bounding information gain for the NTK provides a pathway for proving sublinear regret. The paper makes progress on this open problem.


## What is the main contribution of this paper?

 The main contribution of the paper "Neural Contextual Bandits without Regret" is providing the first sublinear regret bound for a neural network based contextual bandit algorithm. Specifically:

- The paper introduces NN-UCB, a neural network based upper confidence bound algorithm for contextual bandits. 

- It proves that under certain assumptions, the cumulative regret of NN-UCB grows as Õ(T^{(2d-1)/2d}) after T steps, for context vectors in a d-dimensional space. This implies the regret per step goes to 0 as T increases, meaning the algorithm converges to the optimal policy.

- The paper also introduces CNN-UCB, a variant using convolutional neural networks, and proves a similar sublinear regret bound. 

- These are the first results showing neural network based contextual bandit algorithms can attain no-regret guarantees, without restrictive assumptions on the context sequence.

- A key step in the analysis is bounding the maximum information gain of the Neural Tangent Kernel, which may be of broader interest for analyzing other kernelized sequential decision making algorithms.

In summary, the main novelty is providing the first theoretical evidence that neural networks can be effectively incorporated into contextual bandits while still ensuring no-regret properties, broadening their applicability for complex and high-dimensional problems. The analysis leverages connections between neural networks and kernel methods.
