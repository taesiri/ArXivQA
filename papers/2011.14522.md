# [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper seeks to address is: How can we create a theoretical framework and parametrization of neural networks that allows for feature learning in the infinite-width limit?Specifically, the paper points out that the standard Neural Tangent Kernel (NTK) theory does not capture feature learning, which is crucial for many real-world deep learning applications like BERT. The authors propose using the Tensor Programs framework along with a new parametrization called the Maximal Update Parametrization (MUP or Î¼P) to derive infinite-width limits of neural networks that do exhibit feature learning.The key hypotheses appear to be:- The standard parametrization and NTK parametrization of neural networks do not admit infinite-width limits that can learn features.- By using the Tensor Programs framework along with modifications like the MUP parametrization, we can derive infinite-width limits that do enable maximal feature learning.- These infinite-width feature learning limits will perform better on tasks like Word2Vec and few-shot learning where feature learning is essential, compared to NTK baselines and finite-width networks.In summary, the paper seeks to develop new theoretical tools to model feature learning in infinitely wide neural networks, with the goal of better capturing real-world deep learning behavior compared to existing theory like NTK.


## What is the main contribution of this paper?

This paper presents a theoretical framework for analyzing and classifying different parametrizations of neural networks in terms of their capabilities for feature learning vs kernel regime behavior. The main contributions are:1. Formulating a natural space of neural network parametrizations called abc-parametrizations that generalizes standard, NTK, and mean field parametrizations. 2. Proving a Dynamical Dichotomy theorem that classifies abc-parametrizations into two regimes - feature learning vs kernel regime - but not both.3. Showing standard and NTK parametrizations lead to kernel regime and lack feature learning. Proposing the Maximal Update Parametrization (MUP) that enables maximal feature learning. 4. Using the Tensor Programs technique to derive explicit formulas for the infinite-width limits of neural networks under any abc-parametrization.5. Demonstrating the feature learning behavior of MUP experimentally on Word2Vec and few-shot learning tasks where it outperforms kernel regime baselines.In summary, the key contribution is a theoretical classification of parametrizations that reveals the inability of NTK theory to capture feature learning, and a proposal of MUP that does capture it, verified experimentally. The Tensor Programs technique is leveraged to compute the infinite-width limits.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper: The paper proposes a framework to classify neural network parametrizations into feature learning and kernel regimes, proves a dichotomy theorem that each parametrization admits one or the other in the infinite width limit but not both, and introduces a new parametrization that maximally performs feature learning.


## How does this paper compare to other research in the same field?

This paper presents a theoretical framework based on "Tensor Programs" to analyze infinite-width limits of neural networks under different parametrizations. It makes several key contributions:1. It introduces the notion of "abc-parametrizations", which provides a unified way to describe common parametrizations like standard, NTK, and mean field. The paper proves a "Dynamical Dichotomy" theorem that classifies abc-parametrizations into "feature learning" and "kernel regime" categories.2. It shows the standard parametrization and NTK parametrization lead to kernel regime behavior, unable to learn useful features. To enable feature learning, it proposes the "Maximal Update Parametrization" (MUP).3. It leverages the Tensor Programs framework to rigorously derive the infinite-width limit of MUP, as well as limits for any abc-parametrization. This allows calculating the feature learning behavior exactly.4. Experiments on Word2Vec and few-shot learning verify the theory and show MUP outperforms NTK baselines.Comparisons to related work:- Provides a broader classification of parametrizations than prior dichotomies like "lazy/active" training or "short/long" timescales. - Considers discrete-time gradient descent unlike most prior theoretical work on continuous-time limits. Argues discrete-time setting is more natural.- Uses weaker assumptions than common in theoretical NN literature, thanks to Tensor Programs framework.- Focuses on feature learning capabilities, unlike most prior infinite-width analysis (NNGP, NTK, mean field).Overall, the paper makes significant theoretical progress on longstanding questions of feature learning in overparametrized networks. The Tensor Programs framework is very general and likely applicable to many other settings.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions:1. Exploring other parametrizations for neural networks that allow for feature learning in the infinite-width limit. The paper focuses on the Maximal Update Parametrization (MUP), but there may be other parametrizations worth studying as well.2. Extending the analysis to other neural network architectures beyond multilayer perceptrons, such as convolutional neural networks, transformers, etc. The Tensor Programs framework used in the paper is flexible enough to handle these architectures, but the details still need to be worked out.3. Studying the training dynamics and feature learning capabilities for broader classes of optimization algorithms and loss functions. The paper focuses on SGD with squared loss, but other choices are common in practice.4. Scaling up the analytical computations and experiments to larger datasets and models, where the benefits of feature learning may be more significant. The computations become more expensive, so approximations may be needed.5. Exploring the connections between infinite-width limits and feature learning in large but finite-width neural networks. The paper shows the former can guide our understanding of the latter, but more work is needed to quantify the relationship.6. Applying the insights from feature learning in infinite-width networks to real-world tasks where representation learning is crucial, such as computer vision, natural language processing, reinforcement learning, etc.7. Using the dynamical and geometric understanding developed to design new neural network architectures, parametrizations, and training procedures tailored for high-quality feature learning.In summary, the paper opens up many exciting research directions at the intersection of deep learning theory and practice using the lens of feature learning in infinite neural networks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper studies infinite-width limits of neural networks under different parametrizations, focusing on the ability to learn features versus just kernel-based training dynamics. It introduces abc-parametrizations, characterized by how weights and learning rates scale with width, and shows a dichotomy between parametrizations that enable maximal feature learning versus those stuck in a kernel regime. After showing standard parametrizations fall in the kernel regime, the authors propose the Maximal Update Parametrization (MUP) which allows maximal feature learning. Using the Tensor Programs framework, explicit formulas are derived for such infinite-width limits. Experiments demonstrate MUP enables greater feature learning than standard parametrizations in Word2Vec and meta-learning, with finite networks approaching the MUP limit's performance as width increases. Overall, the paper provides a theory unifying mean field and Neural Tangent Kernel limits, a classification of parametrizations, and highlights the importance of feature learning in real neural networks versus kernel-only training dynamics.
