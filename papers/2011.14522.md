# [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper seeks to address is: How can we create a theoretical framework and parametrization of neural networks that allows for feature learning in the infinite-width limit?Specifically, the paper points out that the standard Neural Tangent Kernel (NTK) theory does not capture feature learning, which is crucial for many real-world deep learning applications like BERT. The authors propose using the Tensor Programs framework along with a new parametrization called the Maximal Update Parametrization (MUP or Î¼P) to derive infinite-width limits of neural networks that do exhibit feature learning.The key hypotheses appear to be:- The standard parametrization and NTK parametrization of neural networks do not admit infinite-width limits that can learn features.- By using the Tensor Programs framework along with modifications like the MUP parametrization, we can derive infinite-width limits that do enable maximal feature learning.- These infinite-width feature learning limits will perform better on tasks like Word2Vec and few-shot learning where feature learning is essential, compared to NTK baselines and finite-width networks.In summary, the paper seeks to develop new theoretical tools to model feature learning in infinitely wide neural networks, with the goal of better capturing real-world deep learning behavior compared to existing theory like NTK.


## What is the main contribution of this paper?

This paper presents a theoretical framework for analyzing and classifying different parametrizations of neural networks in terms of their capabilities for feature learning vs kernel regime behavior. The main contributions are:1. Formulating a natural space of neural network parametrizations called abc-parametrizations that generalizes standard, NTK, and mean field parametrizations. 2. Proving a Dynamical Dichotomy theorem that classifies abc-parametrizations into two regimes - feature learning vs kernel regime - but not both.3. Showing standard and NTK parametrizations lead to kernel regime and lack feature learning. Proposing the Maximal Update Parametrization (MUP) that enables maximal feature learning. 4. Using the Tensor Programs technique to derive explicit formulas for the infinite-width limits of neural networks under any abc-parametrization.5. Demonstrating the feature learning behavior of MUP experimentally on Word2Vec and few-shot learning tasks where it outperforms kernel regime baselines.In summary, the key contribution is a theoretical classification of parametrizations that reveals the inability of NTK theory to capture feature learning, and a proposal of MUP that does capture it, verified experimentally. The Tensor Programs technique is leveraged to compute the infinite-width limits.
