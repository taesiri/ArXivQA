# [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper seeks to address is: How can we create a theoretical framework and parametrization of neural networks that allows for feature learning in the infinite-width limit?Specifically, the paper points out that the standard Neural Tangent Kernel (NTK) theory does not capture feature learning, which is crucial for many real-world deep learning applications like BERT. The authors propose using the Tensor Programs framework along with a new parametrization called the Maximal Update Parametrization (MUP or Î¼P) to derive infinite-width limits of neural networks that do exhibit feature learning.The key hypotheses appear to be:- The standard parametrization and NTK parametrization of neural networks do not admit infinite-width limits that can learn features.- By using the Tensor Programs framework along with modifications like the MUP parametrization, we can derive infinite-width limits that do enable maximal feature learning.- These infinite-width feature learning limits will perform better on tasks like Word2Vec and few-shot learning where feature learning is essential, compared to NTK baselines and finite-width networks.In summary, the paper seeks to develop new theoretical tools to model feature learning in infinitely wide neural networks, with the goal of better capturing real-world deep learning behavior compared to existing theory like NTK.
