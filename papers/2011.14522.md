# [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper seeks to address is: How can we create a theoretical framework and parametrization of neural networks that allows for feature learning in the infinite-width limit?Specifically, the paper points out that the standard Neural Tangent Kernel (NTK) theory does not capture feature learning, which is crucial for many real-world deep learning applications like BERT. The authors propose using the Tensor Programs framework along with a new parametrization called the Maximal Update Parametrization (MUP or Î¼P) to derive infinite-width limits of neural networks that do exhibit feature learning.The key hypotheses appear to be:- The standard parametrization and NTK parametrization of neural networks do not admit infinite-width limits that can learn features.- By using the Tensor Programs framework along with modifications like the MUP parametrization, we can derive infinite-width limits that do enable maximal feature learning.- These infinite-width feature learning limits will perform better on tasks like Word2Vec and few-shot learning where feature learning is essential, compared to NTK baselines and finite-width networks.In summary, the paper seeks to develop new theoretical tools to model feature learning in infinitely wide neural networks, with the goal of better capturing real-world deep learning behavior compared to existing theory like NTK.


## What is the main contribution of this paper?

This paper presents a theoretical framework for analyzing and classifying different parametrizations of neural networks in terms of their capabilities for feature learning vs kernel regime behavior. The main contributions are:1. Formulating a natural space of neural network parametrizations called abc-parametrizations that generalizes standard, NTK, and mean field parametrizations. 2. Proving a Dynamical Dichotomy theorem that classifies abc-parametrizations into two regimes - feature learning vs kernel regime - but not both.3. Showing standard and NTK parametrizations lead to kernel regime and lack feature learning. Proposing the Maximal Update Parametrization (MUP) that enables maximal feature learning. 4. Using the Tensor Programs technique to derive explicit formulas for the infinite-width limits of neural networks under any abc-parametrization.5. Demonstrating the feature learning behavior of MUP experimentally on Word2Vec and few-shot learning tasks where it outperforms kernel regime baselines.In summary, the key contribution is a theoretical classification of parametrizations that reveals the inability of NTK theory to capture feature learning, and a proposal of MUP that does capture it, verified experimentally. The Tensor Programs technique is leveraged to compute the infinite-width limits.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper: The paper proposes a framework to classify neural network parametrizations into feature learning and kernel regimes, proves a dichotomy theorem that each parametrization admits one or the other in the infinite width limit but not both, and introduces a new parametrization that maximally performs feature learning.


## How does this paper compare to other research in the same field?

This paper presents a theoretical framework based on "Tensor Programs" to analyze infinite-width limits of neural networks under different parametrizations. It makes several key contributions:1. It introduces the notion of "abc-parametrizations", which provides a unified way to describe common parametrizations like standard, NTK, and mean field. The paper proves a "Dynamical Dichotomy" theorem that classifies abc-parametrizations into "feature learning" and "kernel regime" categories.2. It shows the standard parametrization and NTK parametrization lead to kernel regime behavior, unable to learn useful features. To enable feature learning, it proposes the "Maximal Update Parametrization" (MUP).3. It leverages the Tensor Programs framework to rigorously derive the infinite-width limit of MUP, as well as limits for any abc-parametrization. This allows calculating the feature learning behavior exactly.4. Experiments on Word2Vec and few-shot learning verify the theory and show MUP outperforms NTK baselines.Comparisons to related work:- Provides a broader classification of parametrizations than prior dichotomies like "lazy/active" training or "short/long" timescales. - Considers discrete-time gradient descent unlike most prior theoretical work on continuous-time limits. Argues discrete-time setting is more natural.- Uses weaker assumptions than common in theoretical NN literature, thanks to Tensor Programs framework.- Focuses on feature learning capabilities, unlike most prior infinite-width analysis (NNGP, NTK, mean field).Overall, the paper makes significant theoretical progress on longstanding questions of feature learning in overparametrized networks. The Tensor Programs framework is very general and likely applicable to many other settings.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions:1. Exploring other parametrizations for neural networks that allow for feature learning in the infinite-width limit. The paper focuses on the Maximal Update Parametrization (MUP), but there may be other parametrizations worth studying as well.2. Extending the analysis to other neural network architectures beyond multilayer perceptrons, such as convolutional neural networks, transformers, etc. The Tensor Programs framework used in the paper is flexible enough to handle these architectures, but the details still need to be worked out.3. Studying the training dynamics and feature learning capabilities for broader classes of optimization algorithms and loss functions. The paper focuses on SGD with squared loss, but other choices are common in practice.4. Scaling up the analytical computations and experiments to larger datasets and models, where the benefits of feature learning may be more significant. The computations become more expensive, so approximations may be needed.5. Exploring the connections between infinite-width limits and feature learning in large but finite-width neural networks. The paper shows the former can guide our understanding of the latter, but more work is needed to quantify the relationship.6. Applying the insights from feature learning in infinite-width networks to real-world tasks where representation learning is crucial, such as computer vision, natural language processing, reinforcement learning, etc.7. Using the dynamical and geometric understanding developed to design new neural network architectures, parametrizations, and training procedures tailored for high-quality feature learning.In summary, the paper opens up many exciting research directions at the intersection of deep learning theory and practice using the lens of feature learning in infinite neural networks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper studies infinite-width limits of neural networks under different parametrizations, focusing on the ability to learn features versus just kernel-based training dynamics. It introduces abc-parametrizations, characterized by how weights and learning rates scale with width, and shows a dichotomy between parametrizations that enable maximal feature learning versus those stuck in a kernel regime. After showing standard parametrizations fall in the kernel regime, the authors propose the Maximal Update Parametrization (MUP) which allows maximal feature learning. Using the Tensor Programs framework, explicit formulas are derived for such infinite-width limits. Experiments demonstrate MUP enables greater feature learning than standard parametrizations in Word2Vec and meta-learning, with finite networks approaching the MUP limit's performance as width increases. Overall, the paper provides a theory unifying mean field and Neural Tangent Kernel limits, a classification of parametrizations, and highlights the importance of feature learning in real neural networks versus kernel-only training dynamics.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper studies infinite-width limits of neural networks under different parameterizations, focusing on the ability to learn features from the data. The authors define a general class of parameterizations called abc-parametrizations, characterized by how weights are scaled and initialized. They show that these parametrizations fall into two regimes: either feature learning is possible, or the training dynamics are governed by kernel gradient descent, but not both simultaneously. This Dynamical Dichotomy theorem implies that limits like the Neural Tangent Kernel (NTK), which exhibits kernel-like behavior, cannot capture feature learning. As a solution, the authors propose the Maximal Update Parametrization (MUP) that admits maximal feature learning without the network becoming unstable. Using the Tensor Programs framework, explicit formulas are derived for the MUP limit on tasks like Word2Vec and few-shot learning with MAML. Experiments demonstrate that the MUP limit outperforms NTK baselines and approximates finite-width performance.In summary, this paper provides a theoretical framework to characterize when infinite-width limits of neural networks can or cannot exhibit feature learning. By proposing the MUP limit, it bridges the gap between NTK theory and practical deep learning, where feature learning is essential. The Tensor Programs technique allows deriving these limits explicitly across diverse network architectures and tasks. Experiments verify the theory and demonstrate strong performance enabled by feature learning.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a framework based on the notion of abc-parametrizations and Tensor Programs to unify the Neural Tangent Kernel (NTK) and Mean Field limits of neural networks. The key ideas are:- abc-parametrizations characterize neural network training dynamics by three hyperparameters: weight matrix scaling (a), initialization variance scaling (b), and learning rate scaling (c). - There is a dichotomy between feature learning regimes (e.g. Mean Field) where embeddings change during training, and kernel regimes (e.g. NTK) where embeddings stay fixed but the function evolves by kernel gradient descent.- The Tensor Programs framework allows mechanically taking infinite-width limits of neural network computations like SGD training by expressing them as sequences of matrix multiplications and coordinate-wise nonlinearities.- By analyzing abc-parametrizations through Tensor Programs, the paper classifies parametrizations into feature learning vs kernel regimes, and shows the standard parametrization is ineffective for feature learning. - A Maximal Update Parametrization (MUP) is proposed that enables maximal feature learning in the infinite-width limit. Experiments on Word2Vec and Meta-Learning demonstrate MUP learns useful features unlike NTK.In summary, the paper provides a unified framework to derive and understand infinite-width limits of neural network training, and shows feature learning is crucial in such limits for representation learning tasks.


## What problem or question is the paper addressing?

Based on my reading, this paper is addressing the problem that infinite-width limits of neural networks, such as the Neural Tangent Kernel (NTK), are unable to learn useful features for downstream tasks like transfer learning. Specifically, the embeddings learned during pretraining in the NTK limit are essentially random, as opposed to finite-width networks which can learn meaningful features. The main question the paper seems to be addressing is: how can we modify neural network parametrizations and training procedures so the infinite-width limit does admit feature learning, while still being analytically tractable?The key contributions in addressing this question appear to be:1. Proposing the notion of abc-parametrizations, which generalizes common parametrizations like standard, NTK, and mean field.2. Proving a "Dynamical Dichotomy" theorem showing abc-parametrizations yield either feature learning or kernel (NTK-like) regimes, but not both.3. Identifying issues with standard parametrization and proposing the Maximal Update Parametrization (MUP) which enables maximal feature learning.4. Using the Tensor Programs framework to rigorously derive the infinite-width limits of MUP networks.5. Demonstrating on Word2Vec and few-shot learning tasks that the derived MUP limits outperform NTK and finite networks, with the latter approaching the MUP limit as width increases.In summary, the paper develops theory and tools to incorporate meaningful feature learning into the analytical study of infinite neural networks.
