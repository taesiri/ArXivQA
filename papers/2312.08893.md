# [Solving Dense Linear Systems Faster than via Preconditioning](https://arxiv.org/abs/2312.08893)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes new randomized algorithms for solving dense linear systems and least squares problems with matrices that have a relatively small number of large singular values. The key idea is to use a stochastic block coordinate descent method, where blocks of rows/columns are sampled randomly in each iteration to update the solution. The main challenge is obtaining fast per-iteration running time while still ensuring strong convergence guarantees that depend on the number of large singular values $k$ rather than the matrix dimensions. To achieve this, the algorithms first apply a randomized Hadamard transform to uniformize the block sampling probabilities. Then, they solve the small sampled linear system via an approximate sketch-and-project method, relying on a preconditioner constructed by sketching the rectangular matrix formed in each iteration. For dense $n\times n$ systems, this leads to a total runtime of $\tilde{O}(n^2 + nk^{\omega-1})$ to find an approximate solution, improving over standard iterative solvers. For tall least squares, the runtime becomes $\tilde{O}(\nnz(A) + d^2 + dk^{\omega-1})$, also leveraging input sparsity. Similar guarantees are derived for dense positive semidefinite systems.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper studies the problem of solving dense linear systems $\A\x=\b$ where $\A\in\R^{n\times n}$ has a flat-tailed spectrum, meaning only $k=o(n)$ singular values are large and the rest have a fast decay. Such systems arise often from regularization or noise. The goal is to develop faster algorithms than standard methods like conjugate gradient (CG), which run in $\tilde{O}(n^2 k)$ time, or sketching methods that construct preconditioners in $\tilde{O}(n^2 k^{\omega-2})$ time.  

Proposed Solution: 
The paper proposes two stochastic optimization algorithms based on variants of randomized block Kaczmarz and block coordinate descent methods. The key ideas are:

1) Use determinantal point processes (DPPs) to sample blocks of rows/columns. This gives a provably faster convergence rate depending only on $k$. 

2) Replace expensive DPP sampling with much cheaper uniform sampling by using a randomized Hadamard transform and a coupling argument.

3) Accelerate the block updates via preconditioning instead of solving them directly. This reduces the per-iteration cost from $\tilde{O}(n k^{\omega-1})$ to $\tilde{O}(nk+k^{\omega})$.

Main Contributions:
1) The first solver obtains a runtime of $\tilde{O}((n^2 + nk^{\omega-1})\log(1/\epsilon))$ for solving the system to accuracy $\epsilon$. For $k=O(n^{1-\theta})$, this is near-linear, $\tilde{O}(n^2)$.

2) The second solver works for inconsistent systems, obtaining the same runtime w.r.t. the least squares solution. 

3) For sparse and structured systems like least squares and positive semidefinite systems, the complexity can be further improved.

4) The runtime beats state-of-the-art solvers for systems with explicit regularization (which create flat spectra), without seeing the regularization.

5) The analysis gives the first complexity guarantee for stochastic methods that scales with the number of large singular values, using tools from majorization theory and couplings.
