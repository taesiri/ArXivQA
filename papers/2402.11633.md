# [Self-seeding and Multi-intent Self-instructing LLMs for Generating   Intent-aware Information-Seeking dialogs](https://arxiv.org/abs/2402.11633)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Identifying user intents in information-seeking dialogs is important for meeting user needs, but intent prediction (IP) is challenging and requires sufficient human-labeled dialog data, which is expensive to obtain.  
- Large language models (LLMs) can generate synthetic dialog data, but there is no prior work on using LLMs to generate intent-aware information-seeking dialogs.

Proposed Solution:
- The paper proposes SOLID, a method with novel self-seeding and multi-intent self-instructing schemes to generate large-scale, open-domain, intent-aware information-seeking dialogs using LLMs in a zero-shot manner.

- Self-seeding first asks the LLM to generate seeds (user information needs, initial queries, intent sequences) using its own knowledge, ensuring familiarity. 

- Multi-intent self-instructing then prompts the LLM to sequentially generate utterances, and if multiple intents are needed, asks the LLM to adapt its own prompt instruction, significantly reducing manual engineering.

- To enhance efficiency, SOLID-RL is proposed, which is further trained on SOLID's dialogs to generate full dialogs from a single prompt. A length-based quality estimation mechanism assigns weights to dialogs during SOLID-RL's training.

Contributions:
- SOLID and SOLID-RL methods for zero-shot intent-aware dialog generation using LLMs
- SOLISpeak dataset: 316k dialogs generated by SOLID
- SOLITurbo dataset: dialogs generated by SOLID-RL  
- Analysis showing IP methods trained on SOLID/SOLID-RL datasets plus limited human data significantly outperform methods trained purely on human datasets.

In summary, the paper focuses on leveraging LLMs to automatically generate large-scale intent-aware dialog data for improved intent prediction, via novel self-supervision schemes, while addressing efficiency and quality challenges. The generated datasets demonstrate utility for downstream prediction.
