# [An Embarrassingly Simple Defense Against Backdoor Attacks On SSL](https://arxiv.org/abs/2403.15918)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Self-supervised learning (SSL) methods like SimCLR, BYOL, and MoCo have emerged as powerful paradigms for representation learning without human supervision. 
- However, recent work has shown SSL approaches to be vulnerable to backdoor attacks, where models can be secretly manipulated during training to suit an attacker's motives. 
- Specifically, the paper focuses on defending against frequency-based backdoor attacks like CTRL and FIBA, which are invisible to the human eye, augmentation-invariant, and very effective against SSL models.

Proposed Solution:
- The paper proposes two defense strategies - one before model training, and another during inference.

Defense Before Model Training:
- Leverages the invariance properties of the downstream task. Adds gaussian blur as an augmentation which increases the variance of the trigger patterns.
- Makes it difficult for the model to associate the varying trigger patterns to the target class.
- Reduces attack success rate (ASR) by over 60% while maintaining accuracy.

Defense During Inference: 
- Relies on the assumption that frequency-based attacks minimize changes to the luminance channel which is most perceptible to humans
- Uses only the luminance (Y') channel of images during inference.
- Poisoned RGB images get classified as target class, but Y' images still get classified correctly.
- Effectively mitigates attacks without retraining the poisoned model.

Main Contributions:
- Identifies and proves the relationship between blur and variance of frequency-based trigger patterns 
- Demonstrates successful defense strategies against SSL backdoor attacks without needing extra clean data
- Opens up the possibility of using invariance properties to defend against backdoor attacks in other learning paradigms

The core ideas are using blur and luminance channel to defend SSL models against stealthy frequency-based backdoor attacks, while maintaining performance on clean images.
