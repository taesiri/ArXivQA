# [Krylov Cubic Regularized Newton: A Subspace Second-Order Method with   Dimension-Free Convergence Rate](https://arxiv.org/abs/2401.03058)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem Statement
- The paper considers unconstrained convex optimization problems of the form $\min_{\vx} f(\vx)$, where $f$ is a convex function. It focuses on second-order methods like cubic regularized Newton (CRN) for solving these problems.

- CRN methods are known to have fast convergence rates of $\mathcal{O}(1/k^2)$. However, their main limitation is the high computational cost per iteration which scales as $\mathcal{O}(d^3)$ where $d$ is the problem's dimension. This makes CRN methods impractical for high-dimensional problems.

- Prior works have proposed subspace variants called stochastic subspace cubic Newton (SSCN) methods, which restrict the CRN update to a random low-dimensional subspace to reduce the per-iteration cost to $\mathcal{O}(m^3)$ where $m \ll d$ is the subspace dimension. However, SSCN suffers from slower convergence rates that scale with $d$.

Proposed Solution
- The paper proposes a novel subspace CRN method called Krylov CRN, which performs the CRN update over the Krylov subspace spanned by the gradient and Hessian at each iteration.

- By leveraging properties of the Krylov subspace, the method retains a dimension-free convergence rate of $\mathcal{O}(1/mk + 1/k^2)$ where $k$ is the iteration count. This matches the rate of full-dimensional CRN up to a factor of $1/m$.

- Each Krylov CRN iteration only requires $\mathcal{O}(md)$ operations, arising from $m$ Hessian-vector products. Moreover, the subproblem solution can be obtained in $\mathcal{O}(m)$ time.

Key Contributions
- First analysis showing that a subspace CRN method can achieve a dimension-free convergence rate, shaving a factor of $d$ from the rate of prior SSCN methods.

- Demonstrates that the Krylov subspace is a suitable choice for subspace CRN methods, allowing better convergence guarantees compared to random subspaces. 

- Shows the convergence rate can be further improved under favorable spectral conditions on the Hessians, and recovers the full CRN rate when Hessians have limited spectrum.

- Provides numerical experiments showing much faster convergence of Krylov CRN over SSCN, especially in high dimensions.


## Summarize the paper in one sentence.

 This paper proposes a novel subspace cubic regularized Newton method based on the Krylov subspace that achieves a dimension-independent convergence rate of O(1/(mk) + 1/k^2) for convex optimization problems, outperforming prior subspace second-order methods whose rates depend on the problem dimension.


## What is the main contribution of this paper?

 This paper proposes a new subspace cubic regularized Newton (CRN) method for solving high-dimensional convex optimization problems. The key innovation is to perform the CRN update over the Krylov subspace associated with the Hessian and gradient of the objective function, instead of a random subspace. 

The main contribution is proving that this Krylov CRN method achieves a dimension-independent convergence rate of O(1/(mk) + 1/k^2), where m is the subspace dimension and k is the number of iterations. This marks the first subspace second-order method with a dimension-free rate. Furthermore, when the Hessian has limited distinct eigenvalues, the method recovers the fast O(1/k^2) rate of full-dimensional CRN.

In summary, the main contribution is a novel subspace CRN method based on the Krylov subspace that attains a dimension-independent convergence guarantee. This helps resolve a key limitation of prior subspace CRN methods like SSCN, whose rate depends on the ambient dimension d. The new Krylov CRN method also exploits spectral properties of the Hessian to achieve faster convergence.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it include:

- Krylov subspace methods
- Cubic regularized Newton (CRN) method
- Subspace second-order optimization methods
- Dimension reduction
- Convergence rates
- Hessian matrices
- Eigenspectrum/eigenvalues
- Logistic regression
- Numerical experiments

To summarize, this paper proposes a novel subspace cubic regularized Newton method that uses the Krylov subspace to reduce the dimension of the subproblem solved at each iteration. It provides an analysis of the convergence rate of this method, showing it can attain a dimension-free rate under certain assumptions. The convergence analysis involves bounding terms related to the eigenspectrum of Hessian matrices. Numerical experiments on logistic regression problems demonstrate the efficacy of the proposed algorithm compared to prior subspace CRN approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper argues that using the Krylov subspace allows retaining a similar convergence guarantee as the full Hessian method. However, computing the Krylov subspace still requires $m$ Hessian-vector products. Under what conditions can these products be computed efficiently? Are there cases where the cost would still scale poorly with dimensionality?

2. Theorem 1 provides a dimension-free convergence rate for the proposed Krylov CRN method. However, the bound depends on the quantity $\rho^{(m)}_{\max}$, which in the worst case depends on the largest eigenvalue of the Hessians. Can you characterize cases where $\rho^{(m)}_{\max}$ is small, leading to faster convergence? 

3. The analysis for the strongly convex case shows an iteration complexity with better dependence on condition number compared to prior subspace methods. However, there is still linear dependence on $1/m$. Is it possible to further improve the rate as $m$ increases? How large must $m$ be to match the convergence of full Hessian methods?

4. The paper focuses on convex and strongly convex objectives. Can similar analysis be extended for nonconvex settings? What new challenges arise in ensuring convergence to approximate stationary points or local minima?

5. The computational cost per iteration for the proposed method scales as $\mathcal{O}(md)$ for general objectives. Are there special cases such as generalized linear models where the Hessian-vector products can be computed more efficiently?

6. The analysis relies on controlling the violation of the secant condition through the quantity $\rho^{(m)}$. How does this quantity relate to the convergence of conjugate gradient and other Krylov subspace methods?

7. Could the idea of using Krylov subspaces be combined with other second-order methods such as damped Newton? What changes would be required in the analysis?

8. The experiments demonstrate faster convergence on some logistic regression problems. For what other machine learning models and tasks could we expect similar speedups? Are there cases where random subspace selection would work better?

9. The analysis assumes exact computation of gradients and Hessian-vector products. How could the results be extended to allow errors, as in the stochastic or finite-sum setting? How would convergence rates degrade with noise?

10. The parameter $m$ determines the size of the Krylov subspace. Is there an adaptive way to choose $m$ based on the progress made in past iterations? Can we design restarting schemes to reset the subspace when needed?
