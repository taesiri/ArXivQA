# [Towards Optimal Learning of Language Models](https://arxiv.org/abs/2402.17759)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Towards Optimal Learning of Language Models":

Problem:
The paper studies how to improve the learning efficiency of language models (LMs) by reducing the number of training steps needed to reach good performance. Slow training of large LMs limits model exploration and hinders the democratization of LLMs. Prior works focus on practical methods like model architecture changes or data programming. This paper aims to establish theoretical principles for optimizing LM learning.

Methodology:
The paper first proposes viewing LM training as lossless data compression, where minimizing the area under the LM's loss curve (AUC) corresponds to maximizing the compression ratio of training data. Based on this view, the paper derives a "Learning Law" theorem that characterizes the property of optimal LM learning dynamics: the contributions (influence on model update) of all non-noisy training examples should be equal. This law suggests matching local (on individual samples) and global (on corpus) learning speeds by dynamic re-weighting of examples.

The Learning Law is examined in experiments on linear classification and language modeling tasks. A method is designed to find the near-optimal learning policy by explicitly minimizing the AUC of a target loss through optimizing the example weights. The dynamics of the learning process under the found policy aligns with the Learning Law.  

Main Results:
- Formula derivation and validation of the Learning Law theorem which suggests equal contributions of examples in optimal LM learning.
- Experiments showing 5.5x and 2.4x speedup by applying the near-optimal policy in linear classification and language modeling tasks, indicating the significance of optimizing learning policies.
- Evidence that optimal learning improves the coefficients in the scaling law of LMs, revealing the essence of learning acceleration.

Key Significance:
The theory and experiments provide insights into the principle of optimizing LM learning. The results highlight the great potential of speeding up LM training by designing practical learning optimization methods, which is valuable for researching the limits of LMs and enabling small models to compete with LLMs.
