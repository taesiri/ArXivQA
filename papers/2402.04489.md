# [De-amplifying Bias from Differential Privacy in Language Model   Fine-tuning](https://arxiv.org/abs/2402.04489)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) are being increasingly used in real-world applications, raising concerns about perpetuating social biases and privacy leaks. There is a trade-off between achieving privacy (e.g. using differential privacy) and fairness goals during LLM training. 

- This paper studies the effect of differentially private (DP) fine-tuning on bias in LLMs. It finds that DP exacerbates existing model biases based on gender, race and religion. The cause is identified as DP's disproportionate impact on underrepresented patterns, hampering learning of non-stereotypical associations.

Proposed Solution:
- Counterfactual data augmentation (CDA) is proposed as a technique to mitigate bias amplification from DP. CDA creates gender-swapped versions of sentences to remove skewed associations.

- Experiments using distilGPT2 models fine-tuned on Wikitext-103 show that CDA reduces various metrics of gender bias both for DP and non-DP models. CDA also lowers the relative increase in bias caused by DP.

Main Contributions:
- Showing empirically that DP fine-tuning amplifies existing biases in LLMs across various metrics
- Identifying underlying cause as DP's unequal impact on model's ability to learn from underrepresented gender associations  
- Demonstrating that counterfactual data augmentation mitigates gender bias amplification from DP
- Proposing CDA as an effective technique to simultaneously achieve privacy and fairness objectives during LLM training

The insights from studying DP's effect on fairness of generative models pave the way for developing techniques to improve both privacy and fairness for training large scale LLMs.
