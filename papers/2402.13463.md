# [RefuteBench: Evaluating Refuting Instruction-Following for Large   Language Models](https://arxiv.org/abs/2402.13463)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-4 and ChatGPT are designed to interact with users through multiple rounds of instructions and responses. Users often provide feedback on the model's responses hoping for the model to update its outputs accordingly.  
- However, it's unclear if LLMs can appropriately handle refuting instructions from users to correct their initial responses. There is a need to evaluate how "stubborn" LLMs are to modifying their outputs when given contradictory feedback.

Proposed Solution - RefuteBench Benchmark:
- The paper proposes a benchmark called RefuteBench to evaluate LLMs' stubbornness and resistance to user feedback. 
- It covers tasks like question answering, machine translation and email writing. 
- The key measure is whether models can positively accept refuting feedback and consistently adhere to updated user demands throughout the conversation.
- Both single-feedback and multi-feedback dialog settings are considered.

Key Findings:
- All evaluated LLMs demonstrate stubbornness to some degree and inclination to their own internal knowledge. GPT-4 and Claude-2 show the most flexibility.  
- Generalizing feedback to new contexts is challenging for models, with 10-20% performance drop compared to applying feedback to the same context.
- As dialog length increases, models tend to forget user feedback and revert back to their original responses.

Main Contributions:
- First comprehensive benchmark (RefuteBench) to evaluate LLMs' responsiveness to refuting instructions.
- Analysis of multiple major LLMs on the benchmark reveals their stubbornness and difficulty in adhering to updated demands. 
- Proposed a simple yet effective "recall-and-repeat" prompt strategy to significantly boost performance.

The paper takes an important step towards testing and improving LLMs' capacity to interact with users through instruction-response-feedback cycles. The findings and benchmark could enable better human-AI interaction.
