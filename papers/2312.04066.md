# [Combining inherent knowledge of vision-language models with unsupervised   domain adaptation through self-knowledge distillation](https://arxiv.org/abs/2312.04066)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method to combine the inherent knowledge of vision-language models, obtained through pre-training on large datasets, with the knowledge gained from labeled source data transferred to an unlabeled target dataset via unsupervised domain adaptation (UDA). The method employs a knowledge distillation loss using the vision-language model's zero-shot predictions to maintain its inherent knowledge. Since these predictions tend to have a flat probability distribution, they are recalibrated to accentuate the winning class before distillation. A conventional UDA method (CDAN) is simultaneously used to transfer source knowledge. Further, a gradual source domain expansion strategy is incorporated where pseudo-labels are generated based on both model predictions and recalibrated zero-shot probabilities. Experiments on multiple benchmarks with CNN and transformer backbones show state-of-the-art performance by effectively combining the complementary strengths of vision-language knowledge and UDA. Ablations validate the contributions of different components like distribution recalibration, knowledge distillation loss, data augmentation and expanded pseudo-labeling. The method demonstrates how inherent knowledge of foundations models can be maintained alongside knowledge gained from available labeled data for effective domain adaptation.
