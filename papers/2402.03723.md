# [Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos](https://arxiv.org/abs/2402.03723)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos":

Problem:
Creating controllable 3D human portraits from casual smartphone videos is very useful for AR/VR applications, but is challenging. Key difficulties include accurately modeling and disentangling head movements and facial expressions from a single-view video to achieve high-quality renderings. Prior work using neural radiance fields (NeRFs) is slow for this task. Recent 3D Gaussian splatting (3DGS) methods improve quality and efficiency but cannot represent dynamic content.

Proposed Solution:
The paper introduces Rig3DGS, which represents the entire dynamic scene using 3D Gaussians in a canonical space. Control signals like head pose and expressions are used to transform the Gaussians to a 3D space with learned deformations to generate the desired rendering. The key innovation is carefully designing the deformation method to be guided by a learnable prior derived from a 3D morphable face model. 

Specifically, the deformation of each Gaussian from canonical to deformed space is predicted as a weighted sum of the deformations of its closest vertices on the morphable model mesh. The weights are optimized via photometric loss on ground truth images. By constraining each Gaussian's deformation to lie in a subspace of vertex deformations, the otherwise ill-posed problem of learning per-point deformations without ground truth is effectively regularized.

Contributions:
- Proposes a novel deformation model learned in the subspace defined by a 3D morphable model, enabling generalization to novel expressions and poses during reanimation.

- Introduces Rig3DGS method to create reanimatable portraits from monocular videos with control over facial expressions, head poses, and view synthesis of the entire scene.

- Demonstrates significant improvement in rendering quality over prior neural portrait work, with novel expressions, poses, and view synthesis, while being 50 times faster due to 3D Gaussian scene representation.

In summary, the paper presents a way to create controllable 3D portraits from casual monocular videos by using 3D Gaussians and constraining their deformations based on a morphable face model. This achieves higher quality and faster renders than previous methods.
