# [FAST: Factorizable Attention for Speeding up Transformers](https://arxiv.org/abs/2402.07901)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Transformers have quadratic computational and memory complexity in the number of tokens N, limiting their scalability. This is due to the softmax-based attention mechanism used. 
- Prior attempts at more efficient attention mechanisms have issues with reduced expressiveness, localization, or separability.

Proposed Solution: 
- The paper proposes FAST, a novel attention metric based on a factorizable form inspired by the fast multipole method. 
- FAST attention reduces the complexity from O(N^2) to O(N) without compromising accuracy or sparsifying the attention matrix.
- It uses a polynomial similarity metric instead of exponential, but shows similarly robust properties.
- The computations are simple and support automatic differentiation.

Main Contributions:
- FAST attention mechanism with linear complexity in both computation and memory.
- Maintains full representation of attention matrix with all-to-all token relationships.
- Shows strong performance across tasks compared to softmax attention.
- Can be seamlessly integrated into any transformer architecture.
- Enables transformers for long sequence tasks not viable before due to quadratic complexity limitations.
- Analyzes properties of FAST attention - visualizations, scaling, gradients, expressiveness.

In summary, the paper introduces a novel form of attention that scales linearly while maintaining the capabilities of standard softmax attention. This makes transformers viable for much longer sequences and data modalities previously infeasible. Experiments verify the efficiency, accuracy and expressiveness across diverse tasks.
