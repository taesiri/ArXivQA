# [Backdoor Defense via Deconfounded Representation Learning](https://arxiv.org/abs/2303.06818)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is: How can we defend against backdoor attacks on deep neural networks by using causal reasoning and learning deconfounded representations? 

Specifically, the authors hypothesize that:

- Backdoor attacks act as confounders that introduce spurious correlations between inputs and target labels in poisoned datasets. 

- By constructing a causal graph and modeling the data generation process, they can identify that the backdoor attack opens an undesirable path between inputs and labels.

- They can learn "deconfounded" representations that focus on capturing the true causal relationships while ignoring the spurious correlations induced by backdoor attacks. 

- By training one model to capture the spurious correlations (intentionally backdoored) and another model to be independent of the first (deconfounded), they can obtain a clean model for reliable classification.

In summary, the central hypothesis is that by taking a causal perspective, they can identify the root cause of backdoor attacks, and use techniques from disentangled representation learning to obtain models that are robust to such attacks by ignoring the confounding factors. The key innovation is the application of causal reasoning to analyze and defend against backdoor attacks.
