# [Backdoor Defense via Deconfounded Representation Learning](https://arxiv.org/abs/2303.06818)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is: How can we defend against backdoor attacks on deep neural networks by using causal reasoning and learning deconfounded representations? 

Specifically, the authors hypothesize that:

- Backdoor attacks act as confounders that introduce spurious correlations between inputs and target labels in poisoned datasets. 

- By constructing a causal graph and modeling the data generation process, they can identify that the backdoor attack opens an undesirable path between inputs and labels.

- They can learn "deconfounded" representations that focus on capturing the true causal relationships while ignoring the spurious correlations induced by backdoor attacks. 

- By training one model to capture the spurious correlations (intentionally backdoored) and another model to be independent of the first (deconfounded), they can obtain a clean model for reliable classification.

In summary, the central hypothesis is that by taking a causal perspective, they can identify the root cause of backdoor attacks, and use techniques from disentangled representation learning to obtain models that are robust to such attacks by ignoring the confounding factors. The key innovation is the application of causal reasoning to analyze and defend against backdoor attacks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel causality-inspired defense method against backdoor attacks on deep neural networks. Specifically:

- The paper provides a causal graph to model the generation process of poisoned data, revealing that the backdoor attack acts as a confounder that creates a spurious correlation between the input and target label. 

- Motivated by this causal insight, the paper proposes a Causality-inspired Backdoor Defense (CBD) method to learn deconfounded representations. CBD employs two models - one to capture confounding effects and another to focus on causal relations by minimizing mutual information with the first model. 

- Extensive experiments show CBD effectively defends against 6 representative backdoor attacks, reducing average attack success rates to around 1% while maintaining high clean accuracy. The method is also shown to be robust under different poisoning rates and model architectures.

- Further analysis demonstrates CBD can defend against a potential adaptive attack, where the attacker tries to make the backdoor attack stealthier using adversarial training. This verifies the effectiveness and robustness of the proposed causality-inspired defense.

In summary, the key contribution is developing a novel backdoor defense method from a causal perspective, which provides new insights on leveraging causal reasoning to analyze and mitigate backdoor attacks in deep learning. The effectiveness of the proposed CBD method is thoroughly verified.
