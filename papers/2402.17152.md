# [Actions Speak Louder than Words: Trillion-Parameter Sequential   Transducers for Generative Recommendations](https://arxiv.org/abs/2402.17152)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern recommendation systems rely on deep learning models (DLRMs) trained on huge volumes of heterogeneous user data and features. However, most DLRMs fail to effectively scale with increased compute. This scaling limitation remains unsolved.

- Three main challenges need to be addressed to scale up sequential models for recommendations:
1) Lack of structure in the heterogeneous features 
2) Extremely large (billions-scale) and continuously changing vocabularies
3) Massive computational costs, as recommendation systems process orders of magnitude more user actions daily compared to tokens processed by language models over months.

Proposed Solution: 
- The paper proposes a new paradigm called Generative Recommenders (GRs) which reformulates ranking and retrieval as sequential transduction tasks in a generative framework. This enables model training in a sequential, generative manner.

- A new architecture called Hierarchical Sequential Transduction Unit (HSTU) is introduced to address large vocabularies and computational costs. HSTU modifies the attention mechanism and exploits sparsity in user sequences through innovations like Stochastic Length. 

- New training and serving algorithms like generative training and M-FALCON are proposed to further improve efficiency. With M-FALCON, a 285x more complex model can be served with the same inference budget.

Main Contributions:
- First recommendation formulation to cast ranking and retrieval as generative sequential modeling problems to unlock generative training.

- New designs like HSTU and techniques like Stochastic Length lead to 5.3-15.2x speedups over Transformers for long sequences. 

- Online deployed models based on the proposals improve metrics by 12.4% and are 285x more complex than prior DLRMs while using similar inference resources.

- For the first time, power law scaling between model quality and training compute demonstrated across three orders of magnitude in recommendations, indicating the potential for first foundational models.

The paper provides both algorithmic innovations to efficiently handle sequential modeling in recommendations, as well as empirical evidence of scaling laws and gains from significantly more complex models. The proposals represent convincing steps towards foundational models.
