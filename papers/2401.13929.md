# [Reinforcement Learning with Hidden Markov Models for Discovering   Decision-Making Dynamics](https://arxiv.org/abs/2401.13929)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Major depressive disorder (MDD) is a complex mental health condition with heterogeneous symptoms. Traditional diagnosis based on symptoms has limitations in developing effective treatments.
- Emerging evidence suggests reward processing abnormalities (e.g. reduced motivation or reward sensitivity) may serve as behavioral markers for MDD. 
- Subjects perform reward-based tasks where they make choices based on rewards/outcomes. Reinforcement learning (RL) models are used to extract parameters (e.g. reward sensitivity) to quantify reward processing.
- Recent findings indicate inadequacy of using a single RL model. Instead, subjects may switch between multiple learning strategies. An open question is - how the dynamics of switching strategies affect reward learning in MDD.

Proposed Solution:
- The paper develops a novel RL hidden Markov model (RL-HMM) approach to analyze reward-based decision making with switching between two strategies: (1) RL-based choices, and (2) Random choices.
- The RL model is extended to continuous state space with time-varying transition probabilities in the HMM.
- An efficient EM algorithm is proposed for parameter estimation. Nonparametric bootstrap is used for inference.

Contributions:
- Applied the method to Probabilistic Reward Task data in a depression study. Discovered different findings than prior RL-only models - no significant difference in reward sensitivity between MDD and healthy groups.
- Showed MDD patients spent more time in 'random' strategy than controls. Suggests reward abnormalities may be due to reduced duration of RL strategy.
- Identified significant association between engagement scores and clinician-rated concentration difficulties.
- Revealed brain-behavior association - engagement scores correlated with variability in brain circuitry regulating negative affect.

In summary, the paper develops a useful computational framework to model strategy switching in decision tasks, leading to new insights into mechanisms behind MDD.


## Summarize the paper in one sentence.

 The paper proposes a reinforcement learning and hidden Markov model framework to characterize reward-based decision-making in psychology experiments, allowing for switching between an "engaged" learning strategy and a "lapse" random choice strategy over time.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing a novel RL-HMM (reinforcement learning - hidden Markov model) framework to model reward-based decision making that incorporates switching between two learning strategies: an "engaged" strategy where decisions follow an RL model, and a "lapse" strategy with random choices. 

2. Extending existing RL models to accommodate continuous state spaces and allowing for time-varying transition probabilities in the HMM.

3. Developing an efficient EM algorithm for parameter estimation and using nonparametric bootstrapping for inference.

4. Applying the method to a large randomized clinical trial (EMBARC study) with a reward learning task. Key findings include that MDD patients spent more time in the "lapse" strategy compared to controls, suggesting an alternative explanation for reward processing abnormalities in MDD.

5. Identifying associations between engagement in the task and brain activation patterns related to emotional conflict, indicating increased variability in the negative affect circuitry is linked to poorer engagement.

In summary, the main innovations are in developing a flexible RL-HMM approach to model learning dynamics and applying this method to gain new insights into decision-making and its neural correlates in mental health conditions like depression.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Reinforcement learning (RL)
- Hidden Markov models (HMM) 
- Reward learning
- Decision making
- Major depressive disorder (MDD)
- Probabilistic reward task (PRT)
- EMBARC study
- Learning strategies
- Engaged state
- Lapse state
- Time-varying transition probabilities
- Expectation maximization (EM) algorithm
- Bootstrap inference
- Cross-validation
- Brain imaging
- fMRI
- Amygdala-anterior cingulate (ACC) circuitry
- Negative affect circuitry

The paper proposes an RL-HMM framework to model reward-based decision making and learning strategy switching in the context of a probabilistic reward task. Key aspects include accommodating continuous RL state spaces, time-varying HMM transition probabilities, an efficient EM algorithm for parameter estimation, and applications to real fMRI and behavioral data from the EMBARC study of MDD patients. The model aims to uncover differences in decision dynamics between MDD patients and healthy controls.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper assumes there are two distinct learning strategies - engaged and lapse states. What evidence or prior research supports this dichotomy versus considering a continuum of engagement levels? Could there be more than two strategies employed? 

2. In the HMM, what motivated modeling the transition probabilities as nonparametric functions with time-varying effects rather than assuming they are fixed over trials? What are the relative advantages and disadvantages?

3. For the RL component, why was the continuous state space model preferred over a simpler discrete state space? What basis functions were used for approximation and how was this choice justified? 

4. The EM algorithm requires specifying a complete data likelihood. What assumptions did the authors make about the joint distribution of the observed and hidden/missing data in order to derive this likelihood? 

5. Trend filtering penalty was used for regularization of the transition probabilities. How was the order of the difference penalty selected? What other regularization approaches were considered?

6. The nonparametric bootstrap was used for inference. What modifications or adjustments were made to account for the regularization of the transition probabilities in constructing bootstrap confidence intervals?

7. The simulation studies show underestimation of reward sensitivity and initial Q-values under model misspecification. What explanations are provided for this downward bias? Are there any theoretical results?

8. For the brain analysis, why were measures of variability in activation preferred over means? Is there any supporting neuroscience evidence or theory to explain this? 

9. What sensitivity analyses were conducted to assess the effect of key modeling choices, such as the number and specification of strategies or fixing certain parameters?

10. The current model does not account for subject-specific effects. How might the model be extended to incorporate individual heterogeneity in learning beyond the HMM states?
