# [Scattered Mixture-of-Experts Implementation](https://arxiv.org/abs/2403.08245)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Sparse Mixture-of-Experts (SMoEs) have become popular for scaling up models, but are challenging to implement efficiently on GPUs. Naive implementations are too slow.
- Existing implementations like Megablocks have limitations:
   - Require copying input embeddings when grouping tokens by expert, increasing memory overhead
   - Use padding to make expert token groups equal size, further increasing memory
   - Opaque sparse matrix format makes extending SMoEs difficult

Proposed Solution:
- Introduce ParallelLinear module that performs grouped matrix operations on scattered groups, fusing grouping & linear steps
   - Avoids copies and padding of embeddings
   - Handles various input/output grouped/scattered configurations 
   - Easy to integrate and extend, exposes tensors instead of sparse matrices

- Use ParallelLinear to implement efficient SMoE MLP and Attention layers
   - Configurations reduce copies and reuse memory in backprop
   - Retains scattered order after first layer for Attention - avoids extra group/scatter ops

Contributions:
- ParallelLinear primitive for building various MoE layers without redundancy
- Efficient implementations of SMoE MLP & Attention using ParallelLinear
- Reduces memory footprint by 33-47% compared to Megablocks
- Increases throughput by 24-38% for SMoE MLP over Megablocks
- Relative gains higher for higher module granularity and batched inference
- Worked examples to extend SMoEs to new mixtures like Attention
