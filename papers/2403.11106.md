# [Self-Supervised Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2403.11106)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep neural networks have high computational and memory requirements, making deployment on resource-constrained edge devices challenging. Quantization is used to compress models by converting weights and activations from high to low precision. 
- However, quantization leads to accuracy loss. Existing quantization-aware training (QAT) methods have accuracy inconsistencies across models. Methods combining QAT and knowledge distillation (KD) require laborious hyperparameter tuning, labeled training data, and complex training procedures.
- There is a need for a generalized QAT framework that is simple, effective, and self-supervised.

Proposed Solution:
- The paper proposes Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD). It formulates QAT as an optimization problem that minimizes the discretization error and divergence between full-precision and low-precision models.

- SQAKD only uses the KL divergence loss for distillation, eliminating the need for balancing multiple loss terms. It distills knowledge without label supervision, enabling self-supervised training.

- The framework unifies forward and backward propagation for various quantization functions, making it flexible to incorporate different QAT techniques.

Main Contributions:
- First work to benchmark 11 KD techniques for QAT and analyze loss landscape, revealing issues with combining cross-entropy and KL divergence losses.

- Proposes SQAKD that operates in a self-supervised manner without labeled data, balances no hyperparameters, and simplifies training procedures.

- Achieves state-of-the-art results by enhancing convergence speed and accuracy over existing QAT, KD, and QAT+KD methods on diverse benchmark datasets and architectures.

- Open-sources accurate low-precision models like 2-bit VGG-8 and 4-bit ResNet-32, beneficial for edge deployment.

In summary, the paper addresses key limitations of existing works and proposes an effective self-supervised QAT framework that consistently outperforms prior arts across models and datasets.
