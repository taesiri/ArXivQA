# [FoMo Rewards: Can we cast foundation models as reward functions?](https://arxiv.org/abs/2312.03881)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Designing reward functions for reinforcement learning (RL) is challenging, especially for real-world tasks. Handcrafted rewards can often be hacked by the agent. Learning rewards from offline datasets is expensive and doesn't generalize well. 

Proposed Solution:
- Use foundation models (FoMos), especially large language models (LLMs), as generic reward functions. Specifically, authors propose a pipeline that interfaces an image model with an LLM.
- Given a trajectory of observations, compute the likelihood of an instruction describing the task using the LLM. This likelihood acts as the reward.
- The key idea is that the LLM's world knowledge allows it to reason if the trajectory matches the intent described in the instruction.

Contributions:
- Propose a novel framework to cast decoder LLMs as reward functions by learning a lightweight interface model.
- Evaluate framework on perturbed trajectories and instructions in VIMABench environment.
- Show that FoMo rewards can distinguish between intended behavior and various adversarially constructed incorrect behaviors.  
- Demonstrate that contrastive training and auxiliary success detection loss enable the framework to avoid shortcuts, unlike sole maximum likelihood training.
- Overall, work provides evidence that foundation models can elicit sensible rewards for interactive RL agents. This opens possibilities for end-to-end training of generalist RL agents.

In summary, the paper explores an exciting direction of utilizing foundation models' knowledge to automatically generate rewards, instead of handcrafting them. Results are promising and motivate future work on scaling this to real-world environments and agents.
