# [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: what is the best way to adapt large pre-trained Transformer models to downstream vision tasks in terms of effectiveness and efficiency? The authors investigate different strategies for adapting pre-trained Vision Transformer (ViT) models to various downstream computer vision tasks. The key hypotheses tested are:1. Visual Prompt Tuning (VPT), which adds a small number of trainable parameters in the input space while freezing the backbone model, can outperform full fine-tuning of the entire model on downstream tasks. 2. VPT can match or exceed the performance of other parameter-efficient tuning methods like adapter modules or bias tuning, which directly modify the backbone architecture.3. VPT maintains its advantages over full fine-tuning and other methods across different model sizes, dataset scales, and vision tasks.So in summary, the central hypothesis is that VPT, which works by "prompting" the fixed backbone model through the input space, can enable efficient and effective adaptation compared to other tuning protocols. The paper tests this thoroughly across models, tasks, and data regimes.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes Visual Prompt Tuning (VPT), a new method to efficiently adapt large pre-trained vision Transformers to downstream tasks by adding a small number of trainable parameters to the input space while keeping the backbone model frozen. 2. It shows through extensive experiments that VPT outperforms other parameter-efficient tuning methods like adapter tuning and bias tuning on a wide variety of image classification datasets. 3. More importantly, VPT even exceeds the performance of full fine-tuning the entire model in many cases, while only using a fraction of the parameters. This makes VPT very practical for deploying vision Transformers on many downstream tasks.4. It provides an in-depth analysis of various design choices for VPT, shedding light on why and how visual prompting is effective. For example, it shows that inserting prompts in the latent space is better than pixel space, and that prompt depth matters more than prompt length.5. The paper demonstrates VPT's advantages across model architectures (ViT, Swin Transformer), model scales, different pre-training objectives (supervised, self-supervised), and on segmentation as well as classification tasks. This shows the general applicability of VPT.In summary, the key contribution is proposing and extensively validating Visual Prompt Tuning as an efficient and effective approach for adapting large vision Transformer models to downstream tasks, which has significant practical implications. The analysis also provides useful insights into this new tuning technique.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes Visual Prompt Tuning (VPT), a method to efficiently adapt large pre-trained vision Transformers to downstream tasks by freezing the backbone and only learning a small number of prompt vectors prepended to the input, which achieves strong performance compared to full fine-tuning while requiring significantly fewer trainable parameters.


## How does this paper compare to other research in the same field?

Here are some key ways this paper on Visual Prompt Tuning (VPT) compares to other related research:- Approach: VPT introduces a small number of trainable parameters in the input space while keeping the pre-trained vision transformer (ViT) backbone frozen. This contrasts with other transfer learning methods that update the backbone weights (full fine-tuning), classification head, or add modules inside the backbone. VPT takes inspiration from recent advances in natural language processing on prompting large language models.- Effectiveness: Through extensive experiments on image classification, VPT shows significant gains over other parameter-efficient tuning methods and even exceeds full fine-tuning performance in many cases. This suggests VPT is uniquely effective for adapting vision transformers. Other prompting work in NLP can match but not exceed full fine-tuning.- Efficiency: By only updating the small prompt parameters, VPT provides major storage savings compared to full fine-tuning, which requires saving a separate copy of all backbone weights per task. VPT reduces this to less than 1% of backbone parameters per task.- Generality: Experiments cover various vision transformer models (ViT, Swin), multiple recognition tasks, low and high data regimes, and convolutional networks. This demonstrates the wide applicability of VPT beyond a narrow setting.- Analysis: Ablations provide insights into optimal design choices like prompt length, location, shared parameters, etc. Visualizations show VPT can produce separable features without backbone updates. Overall, the paper advances our understanding of efficient transformer tuning.In summary, this paper pushes the state-of-the-art in efficiently adapting large vision models, with an extensive empirical study showing the advantages of prompting over existing methods. The visual prompt tuning approach appears highly promising based on these results.
