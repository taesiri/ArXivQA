# [Explore and Control with Adversarial Surprise](https://arxiv.org/abs/2107.07394v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on the introduction and related work sections, it seems this paper aims to address the following key research questions:

1. How can we design an unsupervised RL method that explores effectively and learns meaningful skills in high-dimensional, stochastic environments? The paper argues that prior methods either get distracted by unpredictable elements (noisy TV problem) or fail to explore at all (dark room problem). 

2. Can an adversarial competition between policies that minimize and maximize surprise lead to emergent complexity and improved exploration? The paper proposes the Adversarial Surprise (AS) algorithm as a way to balance exploration and control through multi-agent competition.

3. Does AS enable efficient exploration and coverage of the underlying state space in stochastic block MDPs? The paper provides a theoretical analysis showing AS maximizes coverage under certain assumptions.

4. Does AS learn behaviors that transfer well to downstream tasks and benchmark environments? The paper evaluates AS on zero-shot transfer in Atari, Doom, and MiniGrid.

5. Does the competition induce behaviors of increasing complexity? The paper examines phase transitions during training.

In summary, the central hypothesis is that an adversarial game between surprise minimizing and maximizing policies can avoid limitations of prior methods and lead to effective unsupervised skill discovery in stochastic, high-dimensional environments. The theoretical analysis and experiments aim to evaluate this hypothesis from different angles.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The proposal of a new unsupervised reinforcement learning method called Adversarial Surprise (AS) that is designed to balance exploration and control. AS is formulated as an adversarial game between two policies that compete over the amount of surprise (observation entropy) experienced by the agent. 

2. A theoretical analysis showing that AS can fully explore the underlying state space of stochastic block MDPs, overcoming issues like the noisy TV problem that can distract pure novelty seeking methods.

3. Empirical results demonstrating that AS leads to better state space coverage and more meaningful learned behaviors compared to prior unsupervised RL methods like RND, ASP, SMiRL, and AGAC. Key results include:

- AS more fully explores procedurally generated block MDPs compared to baselines.

- When transferred zero-shot to Atari and VizDoom environments, AS attains higher game rewards than baselines, indicating it has learned more useful skills and behaviors even without access to the true reward.

- Analysis of learned behaviors in Atari and VizDoom reveals AS agents exhibit complex, human-like skills purely through optimizing their intrinsic reward, such as hiding from enemies and shooting.

4. Theoretical and empirical evidence suggesting AS provides a more general and robust approach for unsupervised RL in stochastic, partially observed environments compared to prior novelty seeking or surprise minimizing methods alone.

In summary, the key contribution is the proposal and analysis of the AS algorithm for unsupervised RL, which combines ideas from surprise maximization, surprise minimization, empowerment and multi-agent competition to achieve more thorough exploration and meaningful skill acquisition compared to prior methods.
