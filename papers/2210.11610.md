# Large Language Models Can Self-Improve

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether a large language model can improve its own reasoning abilities without additional supervised data or human input. Specifically, the authors investigate whether an autoregressive language model with 540 billion parameters can enhance its performance on reasoning tasks by training on its own self-generated labels. Their method involves using the model to generate "high-confidence" rationale-augmented answers on unlabeled questions, selecting the most consistent answers via majority voting, and fine-tuning the model on these self-generated solutions. The key hypothesis appears to be that by leveraging techniques like chain-of-thought prompting and self-consistency during the self-labeling process, the model can effectively teach itself to better solve reasoning tasks, without relying on ground truth labeled data. Experiments aim to validate whether this self-improvement approach can boost in-domain and out-of-domain reasoning capabilities.In summary, the central research question is whether large language models are capable of self-improvement in reasoning abilities by training on their own self-generated labels from unlabeled data, analogous to human metacognition and self-reflection. The authors hypothesize this is feasible and demonstrate a method for achieving it.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be:1. Demonstrating that a large language model can improve its own performance on reasoning tasks by training on its self-generated labels (predictions), without needing any human-labeled ground truth data. 2. The proposed method, called Language Model Self-Improved (LMSI), uses the model's self-generated chain-of-thought reasoning paths and most consistent answers selected via self-consistency. It fine-tunes the model on these rationale-augmented self-generated samples.3. Empirically showing on a 540B parameter model that LMSI improves accuracy on multiple reasoning datasets like GSM8K, DROP, OpenBookQA, and ANLI. It also generalizes to out-of-domain tasks like AQUA and StrategyQA.4. Ablation studies that identify important design choices like using chain-of-thought reasoning formats and an optimal temperature for self-consistency after fine-tuning.5. Preliminary investigation of fully self-supervised approaches where the model generates its own questions and few-shot prompting examples, reducing human effort.In summary, the main contribution is demonstrating the capability of large language models to self-improve their reasoning abilities without human-labeled data, by leveraging techniques like chain-of-thought prompting and self-consistency. The empirical results and analyses help establish the effectiveness of this self-improving approach.
