# [Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an   Entity-Centric Perspective](https://arxiv.org/abs/2402.02379)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating pre-trained text-and-layout models (PTLMs) have limitations that reduce their reliability for assessing the information extraction capabilities of PTLMs. 
- Issues include: 
    - Annotations not properly tailored for semantic entity recognition (SER) evaluation (e.g. block-level instead of entity-level annotations in FUNSD).
    - Lack of diversity in layout formats and entities (e.g. primarily receipts in SROIE and CORD).  
    - Low quality layout annotations from automated OCR.
- These issues may lead to false overfitting and inaccurate evaluation of PTLMs.

Proposed Solution:
- Introduce EC-FUNSD, an entity-centric benchmark dataset for evaluating PTLMs on SER and entity linking (EL) tasks.
- Manually created by revising FUNSD annotations to have:
    - High quality word and segment-level layout annotations.
    - Continuous semantic entity annotations mapped from blocks. 
    - Diversified document formats and entities.
- Aims to provide unbiased evaluation of information extraction capabilities of PTLMs.

Main Contributions:
- Identify issues with existing VrD datasets that reduce reliability for evaluating information extraction performance of PTLMs.
- Propose standards for an ideal PTLMs evaluation benchmark.
- Introduce EC-FUNSD benchmark dataset to address limitations of existing options.
    - Includes high quality layout/entity annotations and document diversity.
- Experiments show baseline PTLMs suffer sharp performance drops on EC-FUNSD versus FUNSD, indicating overfitting. 
    - Highlights need for unbiased evaluation benchmarks.

In summary, the paper identifies issues with existing VrD datasets that may lead to inaccurate evaluation of information extraction capabilities of PTLMs. It introduces the EC-FUNSD benchmark to provide a more reliable evaluation, with experiments confirming overfitting issues with baseline PTLMs. The work highlights the need for appropriate evaluation benchmarks to assess real-world PTLMs performance.


## Summarize the paper in one sentence.

 This paper proposes EC-FUNSD, a new entity-centric dataset for evaluating information extraction abilities of pre-trained text-and-layout models, showing that current models tend to overfit on biases in existing datasets.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It points out limitations of existing datasets used to evaluate pre-trained text-and-layout models (PTLMs) for information extraction tasks. Specifically, it argues that the annotations in these datasets are not ideal for evaluating the information extraction abilities of PTLMs.

2. It introduces EC-FUNSD, a new entity-centric benchmark dataset designed specifically for evaluating semantic entity recognition and entity linking abilities of PTLMs on visually-rich documents. The dataset has high-quality layout and entity annotations.

3. Experiments conducted with EC-FUNSD reveal that state-of-the-art PTLMs exhibit overfitting tendencies on existing biased datasets. Performance of PTLMs drops significantly on EC-FUNSD compared to the existing FUNSD dataset, demonstrating issues with using FUNSD for evaluation.

In summary, the main contribution is the proposal of EC-FUNSD, a new unbiased benchmark dataset for properly evaluating information extraction capacities of PTLMs, and experiments that reveal overfitting issues with prevailing datasets.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with this paper include:

- Pre-trained text-and-layout models (PTLMs): The main focus of the paper is evaluating the information extraction abilities of these types of models that are pre-trained on visually-rich documents.

- Semantic entity recognition (SER) and entity linking (EL): The two main information extraction tasks that the paper proposes to use to evaluate PTLMs.

- Overfitting: The paper hypothesizes and shows evidence that current PTLMs tend to overfit on biased existing datasets like FUNSD.

- Entity-centric benchmark: The paper introduces a new benchmark dataset called EC-FUNSD that is designed specifically for evaluating information extraction abilities of PTLMs in an unbiased way.

- Visually-rich documents (VrDs): The paper focuses specifically on evaluating PTLMs for extracting information from documents that have rich layout and formatting information, not just text.

- FUNSD: An existing popular dataset for information extraction from documents that the authors argue has annotation issues that make it a biased benchmark.

In summary, the key focus is on benchmarking PTLMs for semantic information extraction from visually-rich documents in an unbiased evaluation framework via the new EC-FUNSD dataset.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What are the key limitations of existing datasets like FUNSD, XFUND, SROIE, etc. that motivated the authors to create a new benchmark dataset EC-FUNSD? Discuss the issues with layout annotations, semantic entity annotations, diversity of documents, etc. in detail.

2. How does the annotation process of EC-FUNSD differ from that of FUNSD? Explain the two-step revision process for layout annotations and entity annotations. What measures were taken to ensure consistency with the original mappings?  

3. What are the 4 key requirements outlined by the authors for an appropriate benchmark dataset to evaluate pre-trained text-and-layout models? Elaborate on each requirement regarding layout annotations, semantic annotations, document diversity etc.

4. How exactly does EC-FUNSD address the tendency of overfitting displayed by pre-trained models on FUNSD? Explain how the decoupling of segment and entity annotations reduces bias.  

5. Why was the vision branch of GeoLayoutLM disabled during fine-tuning on EC-FUNSD? Discuss the unavailability of block-level vision features and their direct contribution to entity features in FUNSD.

6. Analyze and discuss the comparative results of baseline models on FUNSD versus EC-FUNSD. Why is the performance degradation strongly indicative of false overfitting?  

7. What adjustments were made to the maximum sequence length of baseline models before fine-tuning experiments? Explain why this was important to ensure fair comparison.

8. How exactly do the proposed SER and EL tasks formulated in section 3.1 capture the information extraction abilities expected from pre-trained text-and-layout models?

9. Discuss some limitations of the proposed benchmark dataset and analysis. What future work can be undertaken to further strengthen evaluation?  

10. Can EC-FUNSD be adapted and extended to other multimodal document AI tasks beyond information extraction? Elaborate on the scope and potential.
