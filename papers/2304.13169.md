# [SAFE: Machine Unlearning With Shard Graphs](https://arxiv.org/abs/2304.13169)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we develop a machine unlearning method that allows removing the influence of training data from a model while minimizing the expected cost of retraining, and also maintaining high accuracy?

The key ideas and contributions in addressing this question appear to be:

- Introducing the notion of a Shard Graph to represent relations between different data shards and control what information can be shared between them during training.

- Using the Shard Graph to develop SAFE (Synergy Aware Forgetting Ensemble), an algorithm that trains an ensemble of lightweight models/adapters on the shards. The graph allows exploiting synergistic information between connected shards to improve accuracy, while limiting connections to control expected retraining cost.

- Leveraging InCA adapters that can be efficiently trained in parallel to implement SAFE at scale, enabling training with orders of magnitude more shards than prior methods. This significantly reduces retraining cost while maintaining accuracy.

- Demonstrating SAFE empirically, showing it can shard data into as many as 256 subsets with lower forgetting cost than alternatives while achieving much higher accuracy due to making better use of synergistic information between shards.

So in summary, the main hypothesis seems to be that using a Shard Graph to control sharing of information between data shards during training can enable an efficient ensemble method for machine unlearning that minimizes retraining costs without sacrificing accuracy. The results appear to support this hypothesis.


## What is the main contribution of this paper?

 This appears to be a dummy LaTeX paper template rather than an actual completed paper. The template includes many common LaTeX packages and defines document styles, but does not contain any substantive content or contributions. Some key aspects:

- It is an ICCV paper template with a defined paper ID, using common LaTeX packages like iccv, times, graphicx, amsmath, etc.

- It defines LaTeX commands like \name, \unlearn, \algo, etc. to be used throughout the paper.

- It defines document styles like theorem, definition environments. 

- It includes template text for abstract, introduction, and other standard paper sections.

- There are placeholder comments like "Include other packages here, before hyperref" to guide authors.

- It is set up for double column formatting, page numbers, references, etc.

But overall there is no actual content or described contributions in this dummy paper template. The template provides formatting guidance, but the authors would need to add the specific paper content.


## How does this paper compare to other research in the same field?

 This paper introduces SAFE, a method for machine unlearning using shard graphs. Here are some key ways it compares to other research on machine unlearning:

- Most prior work on machine unlearning focuses on either training independent models on shards/subsets of data, or approximating the effects of removing training data from a single model. This paper introduces the novel concept of a shard graph that allows limited connectivity between shards during training to improve accuracy while still enabling forgetting.

- The paper shows SAFE can handle an order of magnitude more shards (up to 256) compared to prior work like SISA while maintaining higher accuracy. This is enabled by using lightweight InCA adapters rather than full models on each shard. 

- SAFE incorporates class prototypes into its predictions, similar to the ARCANE method. However, SAFE shows significant gains over both SISA and ARCANE style prototypes through its synergistic sharding.

- The paper introduces extensions of SAFE including stochastic forgetting via differential privacy, and a la carte personalization. These extend its applicability.

- The empirical evaluations are on more complex fine-grained vision tasks compared to CIFAR-type datasets commonly used. The gains of SAFE are shown to be greater on out-of-distribution datasets.

Overall, the paper makes useful progress over prior work by formalizing the notion of shard graphs and demonstrating how forgettable machine learning can be scaled to much larger capacity models through lightweight adapters. The analysis and empirical results demonstrate the advantages of modeling synergistic relationships between data shards.
