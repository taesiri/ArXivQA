# [Multi-objective Differentiable Neural Architecture Search](https://arxiv.org/abs/2402.18213)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Finding the optimal tradeoff between predictive performance and hardware efficiency is crucial when deploying neural networks, but requires substantial manual effort and trial-and-error. Multi-objective neural architecture search (MO-NAS) aims to automate this, but most methods either simplify the problem by using constraints or need to repeat the search for different tradeoff preferences. This is computationally wasteful and still does not provide an overview of the range of possible tradeoffs.

Proposed Solution:
This paper proposes MODNAS, a novel differentiable NAS algorithm for multi-objective Pareto front profiling. MODNAS employs a meta-hypernetwork that takes as input a user preference vector and target hardware features. It outputs architectural parameters that are used to sample architectures in a one-shot supernet. The supernet estimates accuracy while additional meta-predictors estimate hardware metrics like latency. The meta-hypernet is optimized via multiple gradient descent over preferences and devices to find Pareto-optimal architectures.

Key Contributions:
- First single-run NAS method to profile the entire Pareto front of optimal accuracy-hardware tradeoffs
- Hypernetwork architecture that enables conditioning on user preference vectors and hardware features  
- Leverages multiple gradient descent for robust optimization over multiple objectives and devices
- Evaluation across CNN, Transformer spaces, 19 devices, 3 objectives - outperforms constrained methods in efficiency and quality

The key insight is to directly optimize for diverse tradeoff solutions rather than targeting a single constrained optimum. By meta-learning over user preferences and hardware, the method can generalize to new preferences and devices at test time without any additional search costs. The proposed MODNAS approach is demonstrated to be more scalable and flexible than prior constrained NAS techniques.
