# [Tabular Data: Deep Learning is Not All You Need](https://arxiv.org/abs/2106.03253)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1) Do recently proposed deep learning models for tabular data actually outperform traditional tree-based models like XGBoost when evaluated across diverse datasets? 2) How do these deep learning models for tabular data compare to XGBoost in terms of performance, computational efficiency, and ease of hyperparameter tuning?The central hypothesis appears to be that despite claims in some previous papers, deep learning models do not consistently surpass XGBoost for tabular data learning across different datasets. The authors systematically test this hypothesis by evaluating several recent deep learning models against XGBoost on a diverse set of 11 classification and regression datasets.In summary, the key research questions focus on rigorously benchmarking deep learning versus traditional methods for tabular data, in order to provide clearer conclusions about their relative strengths and weaknesses that can help guide future research and applications. The authors hypothesize that deep learning may not be better than gradient boosted decision trees despite some claims, and they test this hypothesis through extensive experiments.


## What is the main contribution of this paper?

This paper presents a systematic comparison of recently proposed deep learning models for tabular data against traditional tree-based models, specifically XGBoost. The key findings and contributions are:- The deep models (TabNet, NODE, DNF-Net) tend to perform best on the datasets they were originally tested on in their papers, but their performance significantly deteriorates on other datasets. In contrast, XGBoost generalizes better across different tabular datasets. - XGBoost overall outperforms the deep models on the datasets tested, indicating tree ensembles are still state-of-the-art for tabular data. - However, an ensemble of XGBoost and deep models performs better than XGBoost alone. This shows both types of models have complementary strengths.- XGBoost requires much less hyperparameter tuning compared to the deep models. It reaches good performance with fewer optimization iterations.- The results indicate deep learning has made progress on tabular data but still does not surpass tree ensembles like XGBoost, despite some claims in recent papers. More research is needed to develop deep models that generalize as well across diverse tabular datasets.In summary, the key contribution is a rigorous benchmarking of recent deep learning models for tabular data, providing a clearer picture of their capabilities and limitations compared to traditional methods like XGBoost. The findings highlight open challenges in this area.
