# [Model-Based Uncertainty in Value Functions](https://arxiv.org/abs/2302.12526)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers the problem of quantifying uncertainty over expected cumulative rewards (value functions) in model-based reinforcement learning. Specifically, it focuses on characterizing the variance over values induced by a distribution over MDPs representing epistemic uncertainty. Previous works have proposed methods to upper bound this variance, but the over-approximation may lead to inefficient exploration. 

Proposed Solution:
The paper proposes a new uncertainty Bellman equation whose solution converges to the true posterior variance over values. This allows propagating epistemic uncertainty more precisely through the Bellman recursion. The key insight is to define a local uncertainty reward that subtracts out the inherent aleatoric uncertainty, so that only epistemic uncertainty gets propagated.  

Main Contributions:
- Derives an uncertainty Bellman equation whose solution provides the exact posterior variance over value functions under common assumptions, characterizing the gap in previous upper bounds.

- Provides clear interpretation of the local uncertainty reward, which isolates epistemic uncertainty by removing average aleatoric uncertainty.

- Proposes a practical deep RL implementation that estimates the variance by learning the cumulative uncertainty rewards using an uncertainty neural network.

- Integrates the variance estimation within an actor-critic architecture and shows improved sample efficiency on tabular and continuous control tasks compared to prior upper bounds and baseline methods.

In summary, the paper provides theoretical and empirical evidence that more precise quantification of epistemic uncertainty in value functions can improve exploration efficiency in model-based reinforcement learning. The insights on disentangling aleatoric and epistemic uncertainty are notable.
