# Successive Prompting for Decomposing Complex Questions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can we leverage large language models to improve complex question answering with limited supervision, in a way that is modular and provides more opportunities for tailored supervision at each reasoning step?The key ideas and hypotheses appear to be:- Prompting methods that generate reasoning steps and answers in one pass discard benefits of modular and symbolic approaches by coupling supervision.- Successive prompting, which breaks down the complex question into simple QA steps, allows decoupling supervision for question decomposition and answering. - Decoupling enables providing tailored examples at each step, and independent training of the decomposition and answering modules.- Synthetic data generated from semi-structured sources can provide useful intermediate supervision when in-domain decomposition data is limited.- Successive prompting with synthetic data and limited real examples improves over non-modular prompting methods and modular methods without synthetic data.So in summary, the main hypothesis seems to be that successive prompting, combined with synthetic data, can improve complex QA with limited supervision by enabling more tailored and modular training. The experiments aim to demonstrate these benefits over alternative prompting and modular methods.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing the "successive prompting" method for complex question answering. This involves iteratively decomposing a complex question into simple question-answer pairs, and using separate prompts to get the model to decompose the question and answer each simple question. 2. Showing how successive prompting allows decoupling the supervision and examples for question decomposition from question answering. This enables providing better tailored examples at each step, as well as modular and separate training.3. Developing a method to synthetically generate complex questions paired with decompositions using semi-structured tables. This provides out-of-domain data to help bootstrap model learning.4. Demonstrating improved performance over chain-of-thought prompting and state-of-the-art models on a few-shot DROP dataset by combining successive prompting, synthetic data generation, and modules like a symbolic calculator.In summary, the key ideas are successive prompting to break down complex QA, leveraging synthetic data, and showing improvements in a low-resource setting by decoupling the question decomposition and answering parts of the task.
