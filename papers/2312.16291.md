# [Observable Propagation: A Data-Efficient Approach to Uncover Feature   Vectors in Transformers](https://arxiv.org/abs/2312.16291)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers":

Problem:
A key challenge in interpretability research for NLP is finding "linear features" (also called "feature vectors") that correspond to concepts used by transformer models in their internal computations. Existing methods require large labeled datasets which are expensive to acquire and utilize. There is a need for more data-efficient methods to find these explanatory feature vectors.

Method - Observable Propagation:
The paper introduces a new method called "observable propagation" (ObProp) to find feature vectors used by transformers for specific tasks, using minimal data. 

Key ideas:
- Introduce "observables": linear functionals on model logits that correspond to tasks of interest (e.g. predicting gendered pronouns)
- Propagate these observables backwards through model layers to get "feature vectors" capturing information used by the model relevant to the observable
- Handle nonlinearities like LayerNorms and MLPs via first-order Taylor approximations

Theoretical Analysis: 
- Prove LayerNorms do not affect feature vector directions, only magnitudes
- Introduce "coupling coefficient" between feature vectors to quantify how related the model's computations are for two observables

Experiments & Results:
- Find attention heads important for gendered pronoun prediction using path patching, correctly predicted by ObProp feature norms 
- Show model uses same features for predicting gendered pronouns and occupations â†’ evidence of gender bias
- Outperform probing methods for finding feature vectors using little data across various NLP tasks

Conclusion:
ObProp enables improved understanding of transformers' internal mechanisms and bias using orders of magnitude less data than existing methods. Could help democratize interpretability research. Limitations include handling only OV transformer circuits for now.
