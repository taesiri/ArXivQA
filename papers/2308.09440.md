# [Scope is all you need: Transforming LLMs for HPC Code](https://arxiv.org/abs/2308.09440)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis seems to be:Can smaller, domain-specific language models perform better than larger, general-purpose language models on specialized tasks? Specifically, the authors question whether large language models pre-trained on multiple programming languages unrelated to HPC are necessary for good performance on HPC-specific tasks. They hypothesize that smaller models trained only on HPC languages and codebases would be more efficient and achieve superior results. To test this, they propose a novel tokenizer called Tokompiler tailored to HPC code structure and syntax. They apply Tokompiler to pre-train two state-of-the-art models on a Fortran code corpus and evaluate against models using conventional tokenization. The key hypothesis is that their specialized tokenizer will enable smaller, domain-specific models to outperform larger general models that have not been optimized for HPC tasks and languages. The perplexity experiments seem intended to validate whether their Tokompiler tokenizer allows more efficient training and improved representation of HPC code for downstream applications.In summary, the central research question is whether smaller, domain-specific models can surpass larger, general models on specialized HPC tasks when equipped with a tailored tokenizer like Tokompiler that is designed to capture HPC structure and semantics. The authors hypothesize this will be the case.
