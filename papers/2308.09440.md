# [Scope is all you need: Transforming LLMs for HPC Code](https://arxiv.org/abs/2308.09440)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis seems to be:Can smaller, domain-specific language models perform better than larger, general-purpose language models on specialized tasks? Specifically, the authors question whether large language models pre-trained on multiple programming languages unrelated to HPC are necessary for good performance on HPC-specific tasks. They hypothesize that smaller models trained only on HPC languages and codebases would be more efficient and achieve superior results. To test this, they propose a novel tokenizer called Tokompiler tailored to HPC code structure and syntax. They apply Tokompiler to pre-train two state-of-the-art models on a Fortran code corpus and evaluate against models using conventional tokenization. The key hypothesis is that their specialized tokenizer will enable smaller, domain-specific models to outperform larger general models that have not been optimized for HPC tasks and languages. The perplexity experiments seem intended to validate whether their Tokompiler tokenizer allows more efficient training and improved representation of HPC code for downstream applications.In summary, the central research question is whether smaller, domain-specific models can surpass larger, general models on specialized HPC tasks when equipped with a tailored tokenizer like Tokompiler that is designed to capture HPC structure and semantics. The authors hypothesize this will be the case.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel tokenizer called Tokompiler that is designed specifically for preprocessing code for language model pre-training targeting HPC tasks. The key ideas behind Tokompiler are:- Anonymizing variable names, numbers, and strings in the code to avoid learning misleading human semantics. - Generating language-oriented tokens based on abstract syntax tree (AST) parsing to better capture code structure and context.- Incorporating techniques like token splitting and random number attachment to reduce reliance on specific token replacements.The paper shows through perplexity evaluations that integrating Tokompiler with existing models like PolyCoder and SPT-Code significantly improves their ability to comprehend Fortran code structure and semantics. The smaller vocabulary size also speeds up training time. Overall, the paper demonstrates the potential of designing specialized tokenizers like Tokompiler to transform language models for improved performance on domain-specific tasks like HPC code completion and optimization. It opens up future work on developing domain-specific LLMs catering to unique demands of HPC compared to using large generic pre-trained models.
