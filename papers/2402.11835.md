# [Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret   Minimization](https://arxiv.org/abs/2402.11835)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization":

Problem:
- Reinforcement learning (RL) algorithms like Boltzmann Q-Learning (BQL) work well in stationary environments like Markov decision processes (MDPs) but fail in non-stationary multi-agent settings. 
- In contrast, game theoretic algorithms like counterfactual regret minimization (CFR) guarantee convergence to equilibria in multi-agent games but are much less sample efficient than RL methods in stationary settings. 
- There lacks a unified algorithm that can leverage the strengths of both types of methods.

Proposed Solution:
- The paper proposes Adaptive Branching through Child Stationarity (ABCs), an algorithm that adaptively chooses between BQL-style updates and CFR-style updates.
- It introduces a weaker notion called "child stationarity" which requires only the transitions and rewards associated with a specific state-action pair to be stationary, even if the environment is non-stationary overall.
- ABCs tests each state-action pair for child stationarity. If satisfied, it performs a cheap BQL-style update. If not, it does a more expensive CFR-style update that branches out all possible actions.
- This allows ABCs to be efficient like BQL in stationary settings while preserving convergence guarantees like CFR in non-stationary games.

Main Contributions:
- Formalizes the notion of "child stationarity" which enables decomposing environments into independent subgames.
- Proposes ABCs algorithm that unifies BQL and CFR by adaptively switching between the two based on measuring child stationarity.
- Proves that ABCs (1) converges to optima in MDPs with only an O(A) slowdown compared to BQL and (2) converges to Nash equilibria in two-player zero-sum games.
- Shows empirically that ABCs matches BQL's performance in Cartpole and CFR's performance in non-stationary poker games.
- Demonstrates ABCs outperforms both BQL and CFR methods in a partially stationary "stacked" environment, being the only algorithm that can efficiently solve both the stationary and non-stationary parts.
