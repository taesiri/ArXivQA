# [A Configurable Multilingual Model is All You Need to Recognize All   Languages](https://arxiv.org/abs/2107.05876)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question addressed in this paper is: How can we train a single multilingual speech recognition model that can be easily configured/adapted at inference time to recognize speech from any combination of languages selected by the user?The key points are:- The paper proposes a new "configurable multilingual model" (CMM) that can be configured at runtime to recognize speech from any combination of user-selected languages. - The CMM consists of a universal multilingual module plus small language-specific modules for each language. This allows it to leverage shared information across languages while still being adaptable for specific languages.- The CMM uses language-specific embedding, layers, and vocabularies to enable it to recognize speech from the user's selected languages.- A single CMM model only needs to be trained once, but can then be deployed in many usage scenarios based on different user language selections.- Experiments show the CMM improves over baseline universal and monolingual models, especially when users select just 1-3 languages.So in summary, the main research goal is developing a single multilingual model that is highly configurable at inference time based on user language selection, while still leveraging shared multilingual information during training.
