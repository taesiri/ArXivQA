# [Limits of Transformer Language Models on Learning Algorithmic   Compositions](https://arxiv.org/abs/2402.05785)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement: 
The paper analyzes the capabilities of Transformer language models on learning discrete algorithms involving function composition. Specifically, it investigates how sample-efficient these models are at compositional learning, and whether they can effectively learn and reuse sub-tasks when solving a new composed task.

Methods:
- Introduces two new algorithmic tasks (PEN and PER Multi) with clear sub-tasks that require function composition.
- Tests in-context learning capabilities on the tasks by prompting large models like GPT-4 and Gemini.
- Trains LLaMA models from scratch on the tasks and sub-tasks to measure compositional reuse. 
- Provides a theoretical framework using complexity theory to show fundamental limitations of gradient descent trained feedforward models in learning compositional concepts sample-efficiently.

Key Results:
- GPT-4 and Gemini struggle at the tasks even with strong prompting, showing limitations in in-context compositional learning.
- LLaMA models, despite learning the sub-tasks perfectly, fail to effectively compose them for the overall tasks, needing significantly more data than re-learning the primitives.
- The paper presents a theorem proving gradient descent trained feedforward models can be extremely sample inefficient in learning tasks solvable from a single example, explaining the empirical limitations.

Main Contributions:
- Introduction of two new discrete algorithmic tasks to systematically test compositional learning.
- Empirical demonstration that state-of-the-art Transformer LMs have limited compositional capabilities, failing to reuse learned sub-tasks effectively.
- A theoretical result bounding sample efficiency of gradient descent trained feedforward networks using complexity theory.
- The paper significantly advances the understanding of the limits of neural language models in algorithmic compositional learning.
