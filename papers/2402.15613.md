# [Towards Efficient Active Learning in NLP via Pretrained Representations](https://arxiv.org/abs/2402.15613)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Fine-tuning large language models (LLMs) like BERT and RoBERTa has become very popular for text classification tasks. However, when labeled data is scarce, active learning is needed to selectively collect more labels. The standard active learning loop requires repeatedly retraining these massive LLMs, which is extremely computationally expensive. This hinders the practical application of active learning with LLMs.

Method: 
The authors propose PRepAL - Pretrained Representation Active Learning. The key idea is to precompute representations of all data using a pretrained LLM only once. Then for each active learning iteration, instead of fine-tuning the entire LLM, only a simple linear classifier like logistic regression is fitted on the pretrained representations of the currently labeled set. This makes each iteration very fast. Once sufficient labels are collected, the LLM can be fine-tuned once on all labeled data.

Main Contributions:
- PRepAL provides orders of magnitude speedup per active learning iteration compared to fine-tuning the LLM each time, enabling practical active learning.
- It achieves competitive performance - on par or better than fine-tuning the LLM during active learning. 
- Data collected via PRepAL successfully fine-tunes the original as well as other LLMs. This allows flexibility in model choice after active learning.
- Efficient retraining in PRepAL enables selecting data points one at a time instead of in batches. This further improves data diversity and quality.
- Detailed experiments validate PRepAL across datasets, LLMs like BERT and RoBERTa, and various acquisition functions.

In summary, PRepAL enables efficient and practical active learning for text classification with LLMs, while retaining high data quality and model performance. The precomputed representations crucially uncouple iterative classifier retraining from costly LLM fine-tuning.
