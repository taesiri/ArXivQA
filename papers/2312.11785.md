# [Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs](https://arxiv.org/abs/2312.11785)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fact checking models require large amounts of labeled training data, which is expensive and time-consuming to obtain. This limits their applicability to new domains.
- Supervised models also tend to exploit biases and artifacts in datasets rather than learn nuanced language understanding. They struggle on adversarial and out-of-domain datasets.

Proposed Solution:
- The paper proposes a zero-shot fact checking method using semantic triples and knowledge graphs. 
- It decomposes the claim and evidence into triples using open information extraction.
- A universal schema model augments these triples further using Wikidata and Wikipedia.
- Pretrained natural language inference (NLI) models are used to verify claim triples against evidence triples. No model training takes place.
- Finally, a rule-based system aggregates the triple-level verdicts to determine an overall verdict for the claim.

Key Contributions:
- The method achieves state-of-the-art results among zero-shot approaches on FEVER, FEVER-Symmetric and Climate-FEVER datasets.
- It demonstrates greater robustness than supervised models on adversarial datasets FEVER-Symmetric and FEVER 2.0.
- On the out-of-domain Climate-FEVER dataset, it substantially outperforms previous supervised methods.
- Ablation experiments demonstrate that improvements over NLI baselines are consistent across different variants.
- Qualitative analysis shows the method handles mutual exclusivity well and benefits from higher evidence quality.

In summary, the key highlight is a novel zero-shot fact checking approach using semantic triples and knowledge graphs. Without model training, it rivals or exceeds supervised methods in accuracy and robustness across in-domain and out-of-domain datasets.
