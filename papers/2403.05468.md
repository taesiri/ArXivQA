# [Will GPT-4 Run DOOM?](https://arxiv.org/abs/2403.05468)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper explores using large language models (LLMs) like GPT-4 for planning and reasoning in complex environments like video games. Specifically, it evaluates if GPT-4 can play the 1993 first-person shooter game Doom with minimal training, using only its reasoning and observational capabilities. 

Proposed Solution: 
The authors set up an experimental system with two main components - a Vision component (GPT-4V) that takes Doom screenshots and returns textual descriptions, and an Agent component (GPT-4 text) that decides actions based on Vision's output. For more complex strategies, additional GPT-4 models provide finer planning (Planner) or expert advice (Experts).

The Agent is prompted with allowed game actions and some rules/guidelines. Different prompting strategies are tested including: naive, walkthrough-based, plan-generating, and k-levels. Metrics like Passed Maps Average Time andDeaths-Weighted PMAT are used.

Results:
- GPT-4 is not able to directly run Doom, but can act as a proxy for the game engine using the Vision+Agent setup.
- With appropriate prompting, GPT-4 can play Doom to a reasonable degree - opening doors, moving, firing weapons. But performance is far below human level.
- Walkthrough-based prompting leads to noticeably better traversal than naive prompting. Addition of Planner and Experts further improves pathing and combat.
- Analysis shows the model has limited reasoning depth and memory. It struggles with long-term planning.

Main Contributions:
- First work exploring LLMs for playing FPS games and using visual game input.
- Demonstrates GPT-4's capability for short-term reasoning and planning in a complex game environment. 
- Introduces a modular evaluation approach using Vision and Agent components.
- Discusses prompt engineering strategies for improving LLM game playing performance.
- Highlights need for careful regulation given ease of implementing such an LLM-based game playing agent.


## Summarize the paper in one sentence.

 The paper shows that GPT-4 can play the 1993 video game Doom to a limited degree by using its reasoning and planning capabilities, without any training or fine-tuning.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The authors show that the GPT-4 language model is able to play the 1993 first-person shooter video game Doom to a reasonable degree, using only textual descriptions of screenshots from the game as input. Specifically, they demonstrate that with carefully designed prompting strategies involving multiple chained model calls, GPT-4 is capable of basic gameplay actions like opening doors, moving around the level, fighting enemies, and firing weapons. 

While it does not play the game as well as specialized AI agents trained explicitly for this purpose, GPT-4 is still able to exhibit a level of agency and planning capability without any training, instead relying only on its reasoning skills and the information provided in the textual prompts. This pushes the boundaries on general intelligence and planning abilities in large language models.

The authors also provide an analysis of GPT-4's limitations, including lack of long-term planning and memory, sensitivity to prompt formulation, and inability to finish map levels, as well as suggestions for improvements in future work. Overall, this demonstrates a novel application of large language models to gameplay in complex video game environments.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Large language models (LLMs)
- GPT-4
- Doom (video game)
- First-person shooter (FPS) games
- Reinforcement learning
- Planning
- Reasoning
- Intelligent agents
- Video game agents
- Prompting strategies
- Vision component
- Agent model 
- Performance metrics
- Pathing
- Ethical implications

The paper evaluates GPT-4's ability to play the FPS game Doom by using a Vision component to process screenshots from the game and an Agent model to decide on actions. Different prompting strategies like a naive prompt, walkthrough, planning, and k-levels are tested. Metrics like PMAT and D-PMAT are used to measure pathing and playing performance. Limitations around real-time play, metrics, reward modeling, and prompting are discussed. Ethical implications around the model's demonstrated capabilities are also raised. So these comprise some of the key terms and concepts covered in the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using GPT-4 models for both computer vision (GPT-4V) and text generation (GPT-4). What are the advantages and disadvantages of using separate models for these tasks rather than a single multimodal model? 

2. The Vision model is used to generate structured descriptions of the game state from screenshots. How robust is this approach to changes in lighting, camera angle, etc. and how could it be made more robust?

3. The paper uses a naive prompting strategy as well as three more complex strategies involving walkthroughs, plans, and expert advice. What are the tradeoffs between these strategies in terms of performance, computational complexity, and ease of implementation?

4. The hierarchical planning strategy involves a separate model call to generate finer-grained plans from the walkthrough and game state. What techniques could be used to reduce the number of separate model calls? Could the planning be integrated into the main Agent model?

5. The k-levels strategy incorporates advice from separate "Expert" models. How sensitive is the performance to the number of expert models and the diversity of their advice? Could adversarial or cooperative training improve the experts?  

6. How do the various prompting strategies compare in terms of sample efficiency and generalization to new game levels? What changes would be needed to apply the methods to completely new games?

7. The optimisations such as reduced frame rate and using a WAIT action seem quite game-specific. What general principles could be used to make the system more efficient across many different games? 

8. The metrics used, PMAT and D-PMAT, capture only play time and deaths/timeouts. What additional metrics should be used to better evaluate planning, exploration, etc?

9. The paper identifies several limitations around real-time performance, metrics, reward modeling, and prompting. Which of these limitations could be addressed most easily and which pose the biggest challenges?

10. The discussions around ethical implications focus on potential misuse. What principles and safeguards could be built into the system itself to reduce risks and promote beneficial outcomes from this line of research?
