# [Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows](https://arxiv.org/abs/2312.11681)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Large language models (LLMs) suffer from quality issues like hallucinations and brittleness when used for complex, multi-step tasks. To address this, recent work has explored "chaining" techniques where a task is decomposed into multiple LLM calls. However, designing effective LLM chains is challenging given the vast design space. 

Solution: This paper draws inspiration from over a decade of research on crowdsourcing workflows, which similarly decompose complex human work, to provide guidance on LLM chain design. The key contributions are:

1) A design space synthesized from a literature analysis of 107 crowdsourcing workflow and LLM chaining papers. The design space provides high-level "objectives" like accuracy or creativity, "strategies" to achieve them like quality control mechanisms, and low-level "tactics" like specific workflow architectures.

2) Three case studies adapting popular crowdsourcing workflows for LLMs: taxonomy creation, text shortening, and story generation. By qualitatively and quantitatively evaluating these chains versus zero-shot baselines, the authors gain insights into how tactical designs may need to differ for LLMs versus human crowd workers.

3) An analysis of what does and does not transfer from crowdsourcing to guide LLM chain development. Key high-level concepts like objectives and strategies transfer directly. However, the different capabilities of LLMs versus humans means that specific tactic designs, like the complexity of subtasks, often need to be adapted. The paper recommends tactics to explore in future LLM chaining research.

In summary, this paper contributes a literature-based LLM chaining design space, case studies applying this guidance, and an assessment of concepts that do and do not transfer from crowdsourcing to help researchers systematically build better LLM chains.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper constructs a design space bridging crowdsourcing workflows and LLM chains to guide the adaptation of decomposition strategies, finding that high-level objectives, strategies, and tactic types transfer but the tactical design choices often need to differ to account for the different capabilities of crowdworkers versus LLMs.


## What is the main contribution of this paper?

 The main contribution of this paper is threefold:

1. It constructs a design space for LLM chain development that incorporates lessons from the crowdsourcing workflow literature. This design space guides chain designers through objectives, strategies, and tactics. 

2. It implements three case studies adapting popular crowdsourcing workflows to LLM chains. These case studies explore incorporating direct manipulation for user control. Through documenting the adaptations made for using LLMs, the case studies reveal differences in optimal tactic designs between workflows powered by crowdworkers versus LLMs.

3. It synthesizes findings from the literature review and case studies. The synthesis proposes that high-level concepts like objectives and strategies transfer from crowdsourcing to LLM chaining, but that tactic designs can differ significantly between crowdworkers and LLMs. The paper gives recommendations for future work to expand approaches to user revision and creative tasks with chains, systematically study tactic designs, and improve the chain design process.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- LLM chains 
- LLM chaining 
- Crowdsourcing workflows
- Design space
- Objectives
- Strategies 
- Tactics
- Response diversity
- Quality control
- User revision
- Appropriate subtasks
- Worker instructions
- Verification steps
- Workflow flexibility
- Creative tasks
- Direct manipulation

The paper examines how techniques and insights from crowdsourcing workflow design can be adapted for designing LLM chains. It constructs a design space to guide this process, with tiers covering objectives, strategies, and tactics. Through case studies, the paper explores transferring approaches like user revision and supporting creative tasks from crowdsourcing to LLM chaining. It finds that while high-level concepts transfer between crowdsourcing and LLM chaining, tactic design details often need to be re-adapted for LLMs. The paper recommends future work to support chain design given the faster iteration cycles enabled by LLMs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes adapting techniques from crowdsourcing workflows to design LLM chains. What are some key advantages and disadvantages of this approach compared to designing LLM chains from scratch?

2. The design space consists of 3 tiers: objectives, strategies, and tactics. How does considering the interplay between these tiers lead to better LLM chain designs compared to focusing on just one tier? 

3. The paper argues that objectives, strategies, and types of tactics transfer from crowdsourcing to LLM chaining but tactical design decisions may need to be adapted. What are some key factors that necessitate differences in tactical designs between crowdsourcing workflows and LLM chains?

4. The paper highlights incorporating more approaches to user revision and supporting a broader range of creative tasks as worth exploring from crowdsourcing literature. What opportunities and challenges do you foresee with transferring these approaches to LLM chains?

5. The case studies focus on incorporating direct manipulation of outputs, which enables user revision. What other forms of user revision do you think could be useful to explore for LLM chains?

6. The paper finds that compared to crowdworkers, LLMs require different, often simpler subtasks. What are some ways the subtasks could be adapted for LLMs while still providing sufficient context?

7. Sourcing diverse responses requires more overhead with LLMs compared to crowdworkers according to the paper. What prompt engineering techniques could help increase diversity of LLM responses? 

8. The paper recommends more robust verification methods are needed for LLM chains compared to crowdsourcing workflows. What verification tactics do you think could work well for LLM chains?

9. How could the faster speed and lower cost of querying LLMs change the LLM chain design process compared to designing crowdsourcing workflows? What support could improve this design process?

10. The design space and case study implementations focus on text-based LLM chains. How could the design space and crowdsourcing inspiration be extended to multi-modal LLM chains?
