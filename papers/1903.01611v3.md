# [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper seeks to address is:

Can we find sparse, trainable subnetworks within standard neural networks by pruning early in training rather than precisely at initialization?

The paper challenges the lottery ticket hypothesis proposed by Frankle & Carbin (2019), which conjectured that dense neural networks contain sparse subnetworks that can train to similar accuracy when all connections are reset to their initial values. The authors argue that efforts to prune neural networks precisely at initialization have struggled to scale to deeper networks. 

Instead, this paper proposes modifying the iterative magnitude pruning (IMP) method to prune neural networks early in training (after 0.1-7% of steps) rather than exactly at initialization. The paper shows that this modified IMP procedure can identify much sparser subnetworks (50-99% smaller) of deeper networks like ResNet-50 and Inception-v3 that match the full networks' accuracy on ImageNet. 

The central hypothesis is that there exist iterations early in training at which sparse, trainable subnetworks emerge, before the full training process commences. Rewinding weights to these early iterations rather than initialization enables finding these sparse subnetworks that can train effectively.

In summary, the key research question is whether we can effectively prune neural networks early in training (but not necessarily at initialization) to obtain sparse, trainable subnetworks. The central hypothesis is that such effective subnetworks emerge shortly after initialization, guiding the proposal to modify IMP to prune and rewind slightly later.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It modifies the iterative magnitude pruning (IMP) method to prune neural networks a small number of iterations into training rather than precisely at initialization. This technique, termed "rewinding", allows IMP to find extremely sparse subnetworks of larger neural networks that can match the test accuracy of the original unpruned network. 

2. It demonstrates that rewinding IMP by just 0.1-7% into training enables it to find winning tickets (sparse subnetworks that train to commensurate accuracy) for deeper networks like Resnet-50 and Inception-v3 on ImageNet. In contrast, prior work struggled to find winning tickets on large networks when pruning precisely at initialization.

3. It introduces the concept of "stability" to explain why rewinding enables IMP to find winning tickets on deeper networks. Stability refers to the distance/angle between the weights of the subnetwork and full network after training. The paper shows that stability improves rapidly in early training for cases where IMP fails at iteration 0 but succeeds with rewinding.

4. Based on these findings, the paper revises the lottery ticket hypothesis to incorporate the idea of rewinding to early training rather than pruning precisely at initialization. The key insight is that opportunities for successful pruning arise shortly after initialization.

In summary, the main contribution is showing that pruning neural networks just a small way into training rather than exactly at initialization enables the discovery of extremely sparse subnetworks that match the accuracy of the original network on large-scale tasks. This challenges the notion that the ideal time to prune is precisely at iteration 0.
