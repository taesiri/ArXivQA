# Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
  Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we build a system that combines ChatGPT and visual foundation models to enable conversational interactions involving both language and visuals?The key ideas and contributions towards this goal appear to be:- Proposing Visual ChatGPT, a system that incorporates different visual foundation models (VFMs) to allow users to interact with ChatGPT using images in addition to text.- Designing a Prompt Manager module to convert visual signals into language so ChatGPT can understand and leverage the VFMs. This involves defining prompts for system principles, VFMs, user queries, and VFM outputs. - Demonstrating how Visual ChatGPT can accomplish complex visual tasks like image generation, editing, and QA through collaboratively invoking multiple VFMs in a logical chain, guided by the prompts from the Prompt Manager.- Conducting experiments on a variety of visual tasks to showcase the capabilities enabled by combining ChatGPT and VFMs through the proposed system.So in summary, the central hypothesis is that by designing appropriate prompts and chaining of VFMs, ChatGPT can be augmented to handle conversational interactions involving visuals in addition to just text. The Visual ChatGPT system is proposed and evaluated to demonstrate this hypothesis.
