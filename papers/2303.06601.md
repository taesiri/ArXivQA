# [Multi-metrics adaptively identifies backdoors in Federated learning](https://arxiv.org/abs/2303.06601)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: how can we successfully leverage distance metrics to discriminate hostile updates from benign ones in order to defend against backdoor attacks in federated learning without sacrificing model performance? 

The authors identify two key limitations with existing distance-based defense methods:

1. Euclidean distance suffers from the "curse of dimensionality" and fails to discriminate between malicious and benign gradients in high-dimensional space. 

2. A single distance metric is insufficient since backdoor attacks have diverse characteristics and defenders have no knowledge of the underlying data distributions.

To address these limitations, the authors propose using multiple distance metrics (Manhattan, Euclidean, Cosine similarity) cooperatively with dynamic weighting to identify diverse types of malicious gradients. They aim to design an efficient defense that:

1) Is effective at identifying and eliminating malicious updates 

2) Preserves the benign performance of the global model

3) Is independent of specific attack strategies or data distributions

Through extensive experiments, they demonstrate that their multi-metric defense with dynamic weighting maintains high robustness and benign performance even against stealthy backdoor attacks that evade prior defenses.

In summary, the central hypothesis is that using multiple, adaptively weighted distance metrics will enable more successful discrimination of malicious updates to defend against diverse backdoor attacks in federated learning.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a novel defense method against backdoor attacks in federated learning using multi-metrics and dynamic weighting. 

2. It shows that Manhattan distance is more meaningful than Euclidean distance in high dimensions for discriminating between malicious and benign gradients.

3. It utilizes multiple metrics (Manhattan, Euclidean, Cosine similarity) cooperatively to identify diverse malicious gradients brought by different attacks and environments. 

4. It applies a whitening transformation and generates dynamic weights to handle non-IID data distributions and different scales of metrics.

5. Through comprehensive experiments, it demonstrates the effectiveness of the proposed method in maintaining high robustness while preserving benign performance, especially against stealthy backdoor attacks that evade prior defenses. 

6. The proposed defense is shown to be applicable under generic adversary models without assumptions on attack strategies or data distributions.

In summary, the key innovation is using multi-metrics with dynamic weighting to adaptively identify backdoors in federated learning. By introducing Manhattan distance and leveraging multiple metrics cooperatively, the defense becomes more effective against stealthy attacks and varying environments. The dynamic weighting also makes the defense robust under non-IID data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new defense method for federated learning using multiple distance metrics with dynamic weighting to identify and exclude malicious backdoor updates, showing improved robustness against stealthy attacks without sacrificing model performance.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares with other related research in the field of defending against backdoor attacks in federated learning:

- Previous defenses based on statistical differences (e.g. scoring or distance metrics) are often limited against stealthy attacks where the malicious gradients are similar to benign ones, or under non-IID data distributions. This paper proposes improvements by using multiple metrics with dynamic weighting to identify diverse malicious gradients.

- Differential privacy-based defenses can resist stealthy attacks but significantly degrade model performance and convergence speed. In contrast, this paper aims to achieve high robustness without sacrificing benign performance. 

- The paper introduces Manhattan distance for the first time to alleviate the issues with Euclidean distance becoming meaningless in high dimensions. It theoretically proves and empirically validates Manhattan distance as more meaningful than Euclidean distance.

- Most existing methods make strong assumptions about attack strategies or data distributions. A key contribution here is developing a defense that is assumption-independent and adaptive to different environments.

- Comprehensive experiments demonstrate this defense maintains both high robustness and benign accuracy under challenging stealthy attacks like Edge-case PGD, outperforming previous state-of-the-art methods.

In summary, the main novelty lies in the adaptive multi-metric approach to identify diverse backdoor attacks, the introduction of Manhattan distance, and achieving robustness without sacrificing performance or making limiting assumptions. The empirical results validate the effectiveness over a wide range of scenarios.
