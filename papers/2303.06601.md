# [Multi-metrics adaptively identifies backdoors in Federated learning](https://arxiv.org/abs/2303.06601)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: how can we successfully leverage distance metrics to discriminate hostile updates from benign ones in order to defend against backdoor attacks in federated learning without sacrificing model performance? The authors identify two key limitations with existing distance-based defense methods:1. Euclidean distance suffers from the "curse of dimensionality" and fails to discriminate between malicious and benign gradients in high-dimensional space. 2. A single distance metric is insufficient since backdoor attacks have diverse characteristics and defenders have no knowledge of the underlying data distributions.To address these limitations, the authors propose using multiple distance metrics (Manhattan, Euclidean, Cosine similarity) cooperatively with dynamic weighting to identify diverse types of malicious gradients. They aim to design an efficient defense that:1) Is effective at identifying and eliminating malicious updates 2) Preserves the benign performance of the global model3) Is independent of specific attack strategies or data distributionsThrough extensive experiments, they demonstrate that their multi-metric defense with dynamic weighting maintains high robustness and benign performance even against stealthy backdoor attacks that evade prior defenses.In summary, the central hypothesis is that using multiple, adaptively weighted distance metrics will enable more successful discrimination of malicious updates to defend against diverse backdoor attacks in federated learning.
