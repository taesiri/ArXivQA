# [Linearly Constrained Weights: Reducing Activation Shift for Faster   Training of Neural Networks](https://arxiv.org/abs/2403.13833)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural networks can suffer from an "activation shift" phenomenon where the preactivation value of a neuron has a non-zero mean that depends on the angle between the weight vector and the mean of the activation vector from the prior layer. This can hurt training and cause vanishing gradients.

Proposed Solution:
- Introduce "linearly constrained weights" (LCW) which constrain the weight vectors such that their elements sum to zero. This reduces the activation shift.

- Derive how LCW impacts variance flow through the network in forward and backward passes. Show that with LCW, variance is amplified symmetrically, unlike with unmodified weights. 

- Propose an efficient reparameterization method to optimize over the constrained LCW weight space.

- Extend LCW to convolutional layers to reduce activation shift across channels.

Key Contributions:
- Identify and define the activation shift phenomenon in neural networks

- Propose LCW to mitigate activation shift and enable efficient training

- Theoretically analyze how LCW impacts variance flow in forward and backward passes, relating this to vanishing gradients

- Show LCW enables successfully training deep sigmoid MLPs by resolving vanishing gradients

- Demonstrate improved performance from adding LCW to CNNs with batch norm, such as better convergence and generalization.

In summary, the paper identifies the activation shift problem, proposes LCW as a way to address it, derives theory around how LCW helps training, and empirically shows that LCW enables deeper network training and improves performance when combined with batch normalization.
