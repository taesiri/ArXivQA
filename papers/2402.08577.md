# [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper demonstrates that multimodal large language models (MLLMs) are vulnerable to a new type of backdoor attack called "test-time backdoor attacks", which can inject backdoors and activate harmful behaviors without accessing or modifying the training data. 

Key ideas:
- Leverages the multimodality of MLLMs by using one modality (e.g. visual) to set up the backdoor, and another modality (e.g. textual) to activate it. This strategically utilizes their capacity and timeliness.  
- Employs techniques from universal adversarial attacks to craft a universal perturbation that fools MLLMs on multiple inputs.
- Can change triggers/targets by re-optimizing the perturbation, posing challenges for defense.

Method - AnyDoor:
1) Sample visual-text pairs without ground truth 
2) Optimize a universal perturbation using projected gradient descent to minimize loss for:
   - Activating target behavior when trigger appears 
   - Maintaining normal behavior on clean samples
3) Apply perturbation to test images to inject backdoor
4) Activate with textual trigger inputs 

Experiments:
- Tested AnyDoor attack on LLaVA-1.5, MiniGPT-4, InstructBLIP and BLIP-2
- Achieved high attack success rate while preserving accuracy on clean samples 
- Studied impact of attack strategies, sample sizes, loss weights etc. through ablations
- Showed attack works for different triggers, targets and is robust to corruptions
  
Key contributions:
- First demonstration of test-time backdoor attacks on MLLMs 
- AnyDoor attack method requiring no training data access
- Analysis of vulnerabilities of advanced MLLMs  
- New challenges for defending against dynamic backdoors

The paper makes notable contributions in exposing and demonstrating an intriguing new form of backdoor attack that highlights previously overlooked vulnerabilities in even state-of-the-art multimodal models.
