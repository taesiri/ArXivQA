# [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424)

## Summarize the paper in one sentence.

 This paper proposes Tastle, a novel black-box jailbreak attack framework to automatically generate effective prompts that can elicit harmful responses from large language models, in order to test and improve their safety.


## What is the main contribution of this paper?

 This paper proposes Tastle, a novel framework for automatically generating jailbreak prompts to test the robustness and safety of large language models (LLMs). The key contributions are:

1. A distraction-based approach to conceal malicious content within complex, unrelated contexts to evade LLM defenses. 

2. A memory-reframing scheme to focus the LLM's attention on the malicious query by instructing it to re-initiate its response.

3. An iterative optimization algorithm to generate and refine jailbreak prompt templates using an attacker LLM, target LLM, and judgement model.

4. Extensive experiments showing Tastle can effectively jailbreak both open-source and proprietary LLMs with superior attack success rates, scalability and transferability compared to prior work.

In summary, the main contribution is an automated framework to generate fluent, coherent jailbreak prompts to adversarial test LLMs as a crucial step towards safer and more robust models.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and concepts related to this work include:

- Large language models (LLMs): The paper focuses on analyzing and attacking large neural network models that are trained to generate natural language text, such as ChatGPT, GPT-3.5, GPT-4.

- Jailbreak attack: A type of attack against LLMs where specially crafted prompts are used to trigger the model to generate harmful, uncensored or untruthful text that it has been trained not to produce. 

- Red teaming: A strategy of proactively testing LLMs for vulnerabilities by simulating attacks against them. This is aimed at improving the models' safety and alignment.

- Attention distraction: The idea that irrelevant context in a prompt can distract LLMs in a way that degrades their reasoning and makes them more likely to fail safety checks. The paper leverages this to conceal malicious content.  

- Memory reframing: A technique introduced that uses the LLM's tendency towards over-confidence and localized attention to get it to focus only on responding to a malicious query instead of a complex cover story prompt.

- Black box attack: Attacking a target LLM without needing access to model details or gradients, simulating a more realistic adversarial threat model.

- Prompt optimization: An algorithm to iteratively improve the effectiveness of generated jailbreak prompts using an attacker LLM, target LLM and judge model.

In summary, the key focus is developing more effective prompting techniques to reveal vulnerabilities in LLMs as a way to ultimately improve their safety and reliability when deployed.
