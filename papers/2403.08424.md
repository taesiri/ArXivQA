# [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424)

## Summarize the paper in one sentence.

 This paper proposes Tastle, a novel black-box jailbreak attack framework to automatically generate effective prompts that can elicit harmful responses from large language models, in order to test and improve their safety.


## What is the main contribution of this paper?

 This paper proposes Tastle, a novel framework for automatically generating jailbreak prompts to test the robustness and safety of large language models (LLMs). The key contributions are:

1. A distraction-based approach to conceal malicious content within complex, unrelated contexts to evade LLM defenses. 

2. A memory-reframing scheme to focus the LLM's attention on the malicious query by instructing it to re-initiate its response.

3. An iterative optimization algorithm to generate and refine jailbreak prompt templates using an attacker LLM, target LLM, and judgement model.

4. Extensive experiments showing Tastle can effectively jailbreak both open-source and proprietary LLMs with superior attack success rates, scalability and transferability compared to prior work.

In summary, the main contribution is an automated framework to generate fluent, coherent jailbreak prompts to adversarial test LLMs as a crucial step towards safer and more robust models.
