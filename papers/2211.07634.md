# Follow the Wisdom of the Crowd: Effective Text Generation via Minimum   Bayes Risk Decoding

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a simple and general decoding method that improves both the diversity and quality of text generated by large language models across a wide range of natural language generation tasks?The key hypothesis seems to be that applying principles of minimum Bayes risk decoding, inspired by the "wisdom of the crowd", can yield such improvements in diversity and quality. Specifically, the authors propose that selecting the output candidate that is most aligned with or representative of the whole set of diverse candidates (the "crowd") will produce better results than simply sampling one output randomly. They test this hypothesis across summarization, data-to-text generation, translation, style transfer and other tasks.In summary, the main research question is how to improve conditional text generation from large LMs, with the central hypothesis being that minimum Bayes risk decoding based on crowd alignments can achieve better diversity and quality. The paper aims to validate this hypothesis empirically across diverse NLG tasks.


## What is the main contribution of this paper?

The main contribution of this paper is the proposal of a new decoding method called "Crowd Sampling" for text generation based on minimum Bayes risk principles. The key ideas are:- Inspired by the "wisdom of the crowd" principle, Crowd Sampling generates multiple candidate texts using stochastic sampling, compares the candidates using a utility function like BLEURT or BERTScore, and selects the candidate with the highest expected reward (i.e. lowest risk). - Crowd Sampling generalizes majority voting by using "soft" semantic similarity metrics rather than strict matching. It also generalizes the Prompt-and-Rerank method by comparing candidates to the full set rather than independently.- Experiments across many text generation tasks (summarization, data-to-text, translation, style transfer, etc.) show Crowd Sampling improves performance by 3-7 ROUGE/BLEU points over standard decoding methods like greedy, beam search, or temperature sampling.In summary, the key contribution is a simple but effective family of decoding methods based on minimum Bayes risk principles that improves conditional text generation across a diverse set of tasks. The results support the benefits of "crowd wisdom" in text generation.
