# [AutoGPT+P: Affordance-based Task Planning with Large Language Models](https://arxiv.org/abs/2402.10778)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Humans can intuitively command robots through natural language, but limitations in reasoning capabilities of current AI models constrain direct translation of language instructions into executable robot plans. 
- Current approaches face challenges in dynamically capturing the initial state or environment needed for planning.

Proposed Solution - AutoGPT+P:
- Combines an affordance-based scene representation with a planning system based on a large language model (LLM). 
- Affordances encompass the action possibilities between an agent and environment/objects. Deriving planning domain from affordances enables symbolic planning with arbitrary objects.
- System has two main stages:
   1) Perceiving environment as objects and extracting scene affordances using object detection and automatically querying ChatGPT to map object classes to affordances
   2) Task planning by selecting planning 'tools' using LLM - tools include Plan, Partial Plan, Explore, Suggest Alternative
- Allows handling incomplete information by exploring scene, suggesting alternatives or providing partial plans.

Main Contributions:
- Novel affordance-based scene representation combining object detection and automatic object-affordance mapping with ChatGPT (89% F1 score)
- Task planning approach utilizing affordance representation and LLM-based tool selection to handle missing objects, suggest alternatives, explore etc.
- Evaluation in simulation on 150 scenarios covering tasks like pick & place, pouring, chopping, sorting etc. and 79% success rate
- Validation on humanoid robots ARMAR-6 and ARMAR-DE demonstrating feasibility of approach for subset of tasks

In summary, the paper presents an affordance-based framework to enable intuitive human commands of robots using natural language. By leveraging object affordances and an LLM-based planning approach, the system can dynamically reason on the environment to generate plans even with incomplete information. Evaluations demonstrate viability and flexibility of the approach.


## Summarize the paper in one sentence.

 The paper presents AutoGPT+P, a system that combines affordance-based scene representation with a large language model-based planning system to enable natural language command of robots, handle missing objects through exploration and suggestion of alternatives, and demonstrate real-world feasibility on humanoid robots.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) A novel affordance-based scene representation combining object detection and automatic object-affordance mapping (OAM) using ChatGPT.

2) A task planning approach based on the established OAM and an LLM-based tool selection to generate plans, partial plans, explore and suggest alternatives in case of missing objects needed to achieve a task goal specified by the user in natural language. 

3) An evaluation of the approach in simulation using 150 scenarios covering a wide range of complex tasks like picking and placing, handover, pouring, chopping, heating, wiping, and sorting.

4) Real-world validation experiments with the humanoid robots ARMAR-6 and ARMAR-DE demonstrating a subset of these tasks.

In summary, the key contribution is an affordance-based planning system called AutoGPT+P that leverages object affordances and large language models to generate plans and suggest alternatives to missing objects in order to accomplish user-specified tasks. The system is evaluated extensively in simulation and on real robots.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts include:

- Affordances - The action possibilities that objects and environments provide agents. Used as the core scene representation.

- Object Affordance Detection (OAD) - Detecting objects and their affordances from an RGB image. Combines object detection and an Object Affordance Mapping (OAM).

- Object Affordance Mapping (OAM) - Mapping from object classes to their affordances. Learned automatically using ChatGPT in this work. 

- AutoGPT+P - Proposed system combining an affordance-based scene representation with an LLM-based planning approach. Selects tools to generate plans, explore scenes, and suggest alternatives.

- Tool selection loop - Core feedback loop in AutoGPT+P where an LLM selects a tool based on the current state and memory. Tools include Plan, Partial Plan, Explore, Suggest Alternative.

- Self-correction - Approach to correct syntactic and semantic errors in the LLM-generated goal state using external feedback. Improves success rate.

- Alternative suggestion - Method to suggest replacement objects based on the affordances of a missing object needed to achieve a task. 

- Validation experiments - Experiments conducted on real robots to demonstrate feasibility of generating and executing plans.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using ChatGPT to automatically generate an Object Affordance Mapping. What techniques were used to optimize the prompts provided to ChatGPT, and how much improvement in accuracy was gained by prompt engineering? 

2. The paper proposes a new set of affordances tailored for planning tasks. What was the rationale behind selecting these specific affordances compared to existing affordance sets? How extensible is this affordance set to new objects and actions?

3. The Alternative Suggestion algorithm relies on querying the LLM multiple times. How efficient is this process compared to other approaches, and could improvements in prompt engineering further optimize the computational performance?  

4. The planning approach combines an LLM-based tool selection loop with a separate module that translates user instructions to PDDL goal states. What is the motivation behind separating these components rather than using the LLM directly for planning?

5. The paper demonstrates a success rate of 98% on the SayCan instruction set. To what extent could this high success rate be attributed to the self-correction module rather than improvements in the LLM translation capabilities?

6. The validation experiments make several simplifying assumptions regarding object models, environment maps, etc. How realistic are these assumptions for practical robotic deployments, and what modifications would be required to relax them?  

7. Tool selection is identified as the primary source of failure cases. What characteristics of the LLM limit its capability for optimal tool selection, and how might the tool selection process be improved?

8. What mechanisms support fault tolerance and error handling during plan execution? To what extent can the system recognize and recover from failures compared to simply restarting execution?

9. The affordance-based scene representation is static and lacks uncertainty estimates. How might a probabilistic affordance representation enrich the planning process and make execution more robust?

10. The paper focuses exclusively on symbolic planning. What techniques could integrate geometric, subsymbolic reasoning to enable more sophisticated manipulation skills?
