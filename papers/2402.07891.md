# [Label-Efficient Model Selection for Text Generation](https://arxiv.org/abs/2402.07891)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating and comparing text generation models is important but can be very costly, as it requires getting quality or preference judgments from a human oracle/annotator. 
- The goal is to maximize the probability of identifying the better performing model between two candidates, while minimizing the number of examples that need to be labeled by the costly oracle.

Proposed Solution:
- The paper introduces DiffUse, an efficient method to make an informed decision between candidate text generation models using fewer preference annotations from an oracle.
- DiffUse selects a subset of examples that are more informative for preference decisions by clustering semantic difference embeddings between model outputs. This identifies representative differences between model behaviors.
- An iterative approach is also proposed to dynamically determine the number of annotations needed based on a reliability threshold.

Main Contributions:
- DiffUse dramatically reduces the number of oracle annotations required for reliable evaluation, saving valuable human effort and resources. Reductions of up to 75% are demonstrated.
- The method is generic, model-agnostic, and works consistently for different text generation tasks and models.
- Analysis shows DiffUse tends to select examples where the preferred model dominates, explaining its ability to correctly determine the winner with less data.
- A practical iterative algorithm enables users to get the minimal annotations for their reliability tolerance when comparing models.

In summary, the paper presents DiffUse, an efficient and reliable approach to minimize human annotation effort when evaluating and selecting text generation models. By modeling output differences, informative subsets can be annotated to decisively determine model preference.
