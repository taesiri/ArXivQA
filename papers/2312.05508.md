# [Improving Adversarial Robust Fairness via Anti-Bias Soft Label   Distillation](https://arxiv.org/abs/2312.05508)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep neural networks (DNNs) suffer from adversarial attacks, where small perturbations to inputs can cause misclassification. Adversarial training (AT) is effective for improving robustness, but encounters a robust fairness problem - the model exhibits strong robustness on some classes (easy classes) but weak robustness on other classes (hard classes). This class-wise vulnerability raises security risks.

- Existing methods to improve robust fairness focus on re-weighting sample importance. However, the effect of labels that guide model training is ignored. 

Key Idea:
- The paper first empirically observes and theoretically analyzes that the smoothness degree of soft labels for different classes affects robust fairness. Using sharper soft labels for hard classes and smoother soft labels for easy classes can improve fairness. 

- Based on this finding, the paper proposes Anti-Bias Soft Label Distillation (ABSLD) within the knowledge distillation framework to pursue fairness by adjusting class-wise smoothness of soft labels. It assigns different temperatures when distilling soft labels for different classes based on the student's class-wise error risk.

Key Contributions:
- First work exploring effect of class-wise label smoothness on robust fairness. Found connection between label smoothness and model fairness.

- Proposed ABSLD method that adapts soft label smoothness in distillation process to mitigate fairness problem and boost worst-class robustness.

- Extensive experiments show ABSLD outperforms state-of-the-art methods on CIFAR datasets. Effectively improves model's overall robustness and fairness.

In summary, the key novelty is providing both empirical and theoretical justification that adjusting class-wise smoothness of soft labels can improve model robustness fairness, and using this idea to design the ABSLD distillation procedure. Experiments demonstrate clear improvement over prior arts.
