# [Are Vision Transformers Robust to Patch Perturbations?](https://arxiv.org/abs/2111.10659v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How robust are vision transformers (ViTs) compared to convolutional neural networks (CNNs) when individual input image patches are perturbed with either natural corruptions or adversarial perturbations?

The key findings and hypotheses appear to be:

- ViTs are more robust than CNNs to naturally corrupted patches, but less robust to adversarial patch perturbations. 

- The self-attention mechanism of ViTs allows them to effectively ignore natural patch corruptions, leading to greater robustness over CNNs. 

- However, the self-attention mechanism also makes ViTs more vulnerable to adversarial patches, as attention can be easily manipulated to focus on the perturbed patches.

So in summary, the central question is examining and comparing the robustness of ViTs and CNNs to patch-wise perturbations, with a focus on how the differences can be explained by ViTs' self-attention mechanism. The hypotheses are that self-attention improves natural corruption robustness but hurts adversarial robustness relative to CNNs.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

1. The finding that vision transformers (ViTs) are more robust to natural patch corruption than convolutional neural networks (CNNs) like ResNets, but more vulnerable to adversarial patch perturbations. 

2. The analysis and understanding that the self-attention mechanism of ViTs can effectively ignore natural corrupted patches but be easily misled by adversarial patches.

3. The proposal of a simple smoothed attention method, based on temperature scaling, to improve the robustness of ViTs against adversarial patch attacks. This also helps validate the understanding about the role of attention.

In summary, the key contributions seem to be the empirical finding of different robustness behaviors of ViTs, the analysis attributing this to properties of the self-attention mechanism, and a method to improve robustness based on this understanding. The comparisons between ViTs and CNNs on patch robustness and the insights into how attention affects this are novel.
