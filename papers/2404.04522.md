# [Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text   Reranking with Large Language Models](https://arxiv.org/abs/2404.04522)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing methods for leveraging large language models (LLMs) for text reranking have limitations. Fine-tuning the entire LLM is expensive and risks losing capabilities. Parameter-efficient methods like adapters or prompt tuning make models less flexible across tasks. 

Proposed Solution: 
The paper proposes a novel query-dependent parameter-efficient fine-tuning (Q-PEFT) approach for text reranking. It introduces a new query-dependent module while freezing base LLM parameters. Two variations are proposed - Q-PEFT-R using top query tokens for clues, and Q-PEFT-A using multi-head attention over full document.

The key idea is to use the query to guide better document-specific synthetic query generation from the LLM, influencing ranking relevance scores. This enhances scores for relevant documents and reduces scores for irrelevant ones.

Main Contributions:

- First work to learn a query-dependent module in PEFT for text reranking

- Proposed two Q-PEFT methods using query to guide document-specific synthetic query generation and reranking

- Achieves improved performance over baseline retrievers and prompting methods across 4 datasets

- Query-dependent module is flexible, efficient and can generalize across ranking tasks

- Enhances ranking abilities of LLMs with minimal parameter changes, retaining capabilities

In summary, the paper introduces a novel approach to effectively adapt LLMs for text ranking while retaining model capabilities and flexibility across tasks. The query-dependent guidance of synthetic query generation is the main innovative concept.
