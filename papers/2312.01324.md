# [MABViT -- Modified Attention Block Enhances Vision Transformers](https://arxiv.org/abs/2312.01324)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Parallel transformer structures perform comparably to standard transformers at large scale due to representation collapse, where input and output values converge in deeper layers. 
- This causes issues when applying parallel structures to vision transformers, resulting in degraded performance compared to standard transformers.

Proposed Solution: 
- Introduce non-linearity within the attention module by applying GLU-based activation functions to the Value tensor. 
- Helps assign significance to the output of the multi-head attention and alleviate representation collapse.

Contributions:
- Analyze difference in performance of parallel transformer structures at different scales.
- Hypothesize representation collapse causes comparable performance at scale.
- Propose Modified Attention Block (MAB) Vision Transformer with GLU activation on Value tensor.
- MABViT GLU variants outperform standard ViT, with 0.6% boost for PR-SwiGLU S/16 using fewer parameters.
- MABViT also works well with GELU activation.
- Show MABViT captures patterns better and has more potential with increased depth.

In summary, the paper analyzes issues with applying parallel transformers to vision tasks, hypothesizes the underlying reason to be representation collapse, and proposes a novel MABViT architecture that introduces non-linearity via activation functions in the attention module. Key results demonstrate performance improvements over standard vision transformers using this technique.
