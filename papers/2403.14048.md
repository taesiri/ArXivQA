# [The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio   Benchmarks and Novel Data](https://arxiv.org/abs/2403.14048)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of large-scale, openly accessible audio datasets tailored for specialized tasks like human-computer interaction and human behavior analysis. This limits the ability of researchers to apply advanced ML techniques.

- The organizers aim to address this by providing 4 proprietary datasets for use by researchers submitting to the 2023 NeurIPS Machine Learning for Audio (MLA) workshop.

Datasets Provided:
- Hume-Prosody: 41 hours of emotional speech data from 1004 speakers, labeled for intensity of 9 emotions. For speech emotion recognition.

- Hume-VocalBurst: 36 hours of non-linguistic emotional vocal bursts from 1702 speakers, labeled for 10 emotions. 

- Modulate-Sonata: Over 6 hours of professional voice actors speaking emotional sentences in character roles. 25 emotion classes. For speech emotion recognition and generation.

- Modulate-Stream: 7000+ hours of gaming streamer commentary audio. Metadata like transcripts included. For unsupervised audio tasks.

Solutions and Contributions:
- Authors outline tasks, baselines and best results achieved so far for Hume datasets. New speech emotion recognition baseline provided for Modulate-Sonata.

- By releasing these datasets and baselines, they enable workshop participants to advance audio ML, especially for specialized tasks in human behavior modeling and human-computer interaction.

- The datasets offer broad coverage of human emotional states. Their release helps address the key problem of audio data scarcity for specialized domains.
