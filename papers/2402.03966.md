# [On dimensionality of feature vectors in MPNNs](https://arxiv.org/abs/2402.03966)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Message passing graph neural networks (MPNNs) are commonly used for graph data, but their expressive power compared to the Weisfeiler-Lehman (WL) graph isomorphism test was not well understood. 
- Prior work by Morris et al. showed MPNNs with ReLU activation require O(n)-dimensional features to match WL, where n is number of nodes.
- Later work by Aamand et al. reduced this to O(log n) dimensions but only holds in probability. 
- It was open whether constant low dimensionality is enough to get equivalence.

Proposed Solution:
- The paper gives a simple proof that MPNNs with 1-dimensional features and any non-polynomial analytic activation (e.g. sigmoid) have the same power as the WL test.
- This holds independently of the size of the input graphs.
- The key insight is to require linear independence of feature vectors over rational numbers rather than over real numbers.

Main Contributions:
1) A simple proof that 1-dimensional MPNNs with analytic non-polynomial activation perfectly simulate the WL test on any graph.
2) Experiments validating the theory - showing one fixed MPNN perfectly simulates WL on 300 random graphs. 
3) Analysis suggesting logarithmic precision bits are enough for perfect simulation.

In summary, the paper establishes that MPNN feature dimension does not need to grow with graph size, giving theoretical justification for good performance of low-dimensional MPNNs in practice. The simple constructive proof and experiments around precision requirements are also valuable contributions.
