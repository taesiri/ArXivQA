# [Learning to Retain while Acquiring: Combating Distribution-Shift in   Adversarial Data-Free Knowledge Distillation](https://arxiv.org/abs/2302.14290)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question/hypothesis of this paper is:How to effectively perform data-free knowledge distillation from a teacher neural network to a student network without suffering from the non-stationary distribution of pseudo-samples generated by the adversary/generator?Specifically, the paper aims to address the issue that arises in adversarial data-free knowledge distillation where the student network's accuracy suffers due to the changing distribution of pseudo-samples produced by the generator across training iterations. The key hypothesis is that treating knowledge acquisition (learning from new samples) and knowledge retention (retaining performance on old samples) as separate meta-learning tasks and aligning their gradients can help alleviate this issue. The core proposal is a meta-learning inspired framework that allows the student network to acquire new knowledge from the latest pseudo-samples while retaining performance on previously encountered samples, dubbed "Learning to Retain while Acquiring". The key idea is to treat knowledge acquisition and retention as meta-train and meta-test tasks respectively and align their gradients by optimizing a meta-objective. This is hypothesized to enable the student network to maintain accuracy on old samples while learning on new ones.In summary, the central research question is how to enable effective adversarial data-free distillation without the student accuracy suffering from non-stationary pseudo-sample distributions. The core hypothesis is that framing distillation as meta-learning and aligning gradient directions can help address this problem.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a novel meta-learning inspired student update strategy for Adversarial Data-Free Knowledge Distillation (DFKD) that aims to maintain the student's performance on previously encountered examples (Knowledge-Retention) while acquiring knowledge from samples of the current distribution (Knowledge-Acquisition). Specifically, the key contributions are:- They propose treating Knowledge-Acquisition and Knowledge-Retention as meta-train and meta-test tasks respectively in a meta-learning framework. This allows the student network to acquire new information while maintaining performance on previously encountered samples. - They theoretically show that the proposed student update strategy enforces an implicit gradient alignment between the Knowledge-Acquisition and Knowledge-Retention tasks. Aligning the gradients allows obtaining student parameters that have optimal performance on both objectives.- They demonstrate the effectiveness of the proposed method through experiments on multiple datasets, network architectures, and replay schemes (Memory Buffer and Generative Replay). The method shows improved student learning evolution and performance compared to prior DFKD methods.In summary, the core novelty is in the meta-learning inspired student update strategy that retains previously learned knowledge while acquiring new knowledge by implicitly aligning the gradients of the two objectives. This improves student performance in Adversarial DFKD settings.
