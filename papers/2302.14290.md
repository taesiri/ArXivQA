# [Learning to Retain while Acquiring: Combating Distribution-Shift in   Adversarial Data-Free Knowledge Distillation](https://arxiv.org/abs/2302.14290)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question/hypothesis of this paper is:How to effectively perform data-free knowledge distillation from a teacher neural network to a student network without suffering from the non-stationary distribution of pseudo-samples generated by the adversary/generator?Specifically, the paper aims to address the issue that arises in adversarial data-free knowledge distillation where the student network's accuracy suffers due to the changing distribution of pseudo-samples produced by the generator across training iterations. The key hypothesis is that treating knowledge acquisition (learning from new samples) and knowledge retention (retaining performance on old samples) as separate meta-learning tasks and aligning their gradients can help alleviate this issue. The core proposal is a meta-learning inspired framework that allows the student network to acquire new knowledge from the latest pseudo-samples while retaining performance on previously encountered samples, dubbed "Learning to Retain while Acquiring". The key idea is to treat knowledge acquisition and retention as meta-train and meta-test tasks respectively and align their gradients by optimizing a meta-objective. This is hypothesized to enable the student network to maintain accuracy on old samples while learning on new ones.In summary, the central research question is how to enable effective adversarial data-free distillation without the student accuracy suffering from non-stationary pseudo-sample distributions. The core hypothesis is that framing distillation as meta-learning and aligning gradient directions can help address this problem.
