# [Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic   Textual Similarity](https://arxiv.org/abs/2402.13130)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- BERT produces high-quality sentence embeddings but has high pre-training cost. ELECTRA is more efficient to pre-train but produces worse sentence embeddings.
- Community has stopped using ELECTRA for semantic textual similarity (STS) tasks.
- Paper notes significant performance drop when using ELECTRA discriminator's last layer for embeddings compared to earlier layers. 

Proposed Solution:
- Authors explore why ELECTRA embeddings underperform on STS.
- They propose a novel "truncated model fine-tuning" (TMFT) method:
    - Use only first l layers of ELECTRA for embedding.
    - Apply pooling on layer l embeddings. 
    - Fine-tune the truncated model on STS dataset.
- Also uncover efficacy of ELECTRA generator model for embeddings.

Key Results:
- TMFT improves ELECTRA performance on STS benchmark by over 8 points. More parameter efficient.
- ELECTRA generator surprisingly works as well as BERT for embeddings despite way fewer parameters and smaller embedding size.
- Additional gains from:
    1) First fine-tuning TMFT model on word similarity task 
    2) Or use masked language modeling to further pre-train before TMFT

Main Contributions:
- Analysis of why ELECTRA sentence embeddings underperform on STS
- Novel TMFT method to improve ELECTRA embeddings 
- Uncovering efficacy of ELECTRA generator for embeddings
- Two techniques (word similarity pre-training and MLM pre-training) to further boost gains
