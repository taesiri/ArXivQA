# [FedUV: Uniformity and Variance for Heterogeneous Federated Learning](https://arxiv.org/abs/2402.18372)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Federated learning (FL) is a distributed machine learning approach where models are trained on decentralized data located on devices like mobiles and edge servers. A key challenge in FL is dealing with non-identically distributed (non-IID) data across devices, which causes model performance to degrade significantly. Prior regularization methods in FL try to make local models less biased but do not directly emulate IID conditions. They also rely heavily on the global model, have high computational costs, and are not evaluated extensively on feature-shifted data.

Proposed Solution:
This paper proposes FedUV, a simple and efficient method to emulate IID conditions by inducing representation hyperspherical uniformity and classifier variance during local model training. 

Key ideas:
- Analyze classifier layer weights using SVD, find singular values degenerate more in non-IID settings. Motivated freezing classifiers in prior work.
- Propose classifier variance loss to make class probability distributions match those in IID settings. Uses a hinge loss formulation.
- Propose hyperspherical uniformity loss onrepresentations, uses RBF kernel based formulation. Spreads representations to allow more classifier variance.
- Combine both losses in local training objective to emulate IID conditions. Does not rely on global model.

Experiments:
Extensive experiments on multiple label-shifted and feature-shifted FL datasets like CIFAR and TinyImageNet. Settings vary dirichlet parameter, client participation rate, number of local epochs etc.

Results:
- FedUV outperforms state of the art by large margins, especially on extreme label/feature shifts. Quicker convergence.
- Ablations show both variance and uniformity are important. Match intuition in label/feature shifted cases.
- Very efficient compared to regularization baselines like FedProx, Moon.

Contributions:
- Analysis showing classifier weight degeneration in FL 
- Novel method directly emulating IID conditions via classifier variance and representational uniformity
- State of the art performance on extensive label/feature shifted settings
- Efficient, scalable approach compared to prior FL regularization techniques
