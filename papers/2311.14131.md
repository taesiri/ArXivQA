# [Exactly conservative physics-informed neural networks and deep operator   networks for dynamical systems](https://arxiv.org/abs/2311.14131)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces a method for training physics-informed neural networks (PINNs) and physics-informed deep operator networks (DeepONets) that exactly preserve conservation laws of dynamical systems. The authors note that standard PINNs and DeepONets approximate solutions to differential equations but do not guarantee the preservation of important invariant quantities like conserved quantities (first integrals) over time. They propose the use of a projection layer added to the neural network architecture that maps learned candidate solutions onto the manifold defined by the intersection of the level sets of the first integrals. This projection is done using a constrained optimization solved with Newton iterations. The training procedure begins with an unconstrained network and progressively increases projection iterations over the course of training epochs. Through examples like the rigid body problem, double pendulum, conservative Lorenz system, and point vortex dynamics, the authors demonstrate that their conservative PINNs and DeepONets vastly outperform non-conservative counterparts in long-time integration, preserving invariants to machine precision and avoiding unphysical drifts. The method works for general dynamical systems with known first integrals. The authors conclude by noting extensions to partial differential equations may require different techniques.


## Summarize the paper in one sentence.

 This paper introduces a method to train physics-informed neural networks and physics-informed deep operator networks to exactly preserve conservation laws and first integrals of dynamical systems.


## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of a method for training exactly conservative physics-informed neural networks and physics-informed deep operator networks for dynamical systems. Specifically:

- The paper proposes using a projection-based technique to map the learned solution from a neural network solver onto the invariant manifold defined by the first integrals/conservation laws of the dynamical system. This ensures the conservation laws are exactly preserved.

- Several examples are provided illustrating that the conservative neural network solvers vastly outperform non-conservative ones for long-time integration of dynamical systems. The conservative networks avoid spurious drifts and artifacts that accumulate over time when conservation laws are violated.

- The proposed technique works for arbitrary dynamical systems with first integrals, not only Lagrangian/Hamiltonian systems. This expands the applicability compared to prior symmetry-preserving neural network methods.

- The paper introduces the notion of "geometric numerical machine learning", aimed at preserving qualitative solution properties like conservation laws, not just accuracy. This is analogous to the field of geometric numerical integration for traditional numerical methods.

In summary, the key innovation is a general method to make neural network solvers for dynamical systems exactly conservative, with demonstrations of improved numerical performance. The paper argues this is an important consideration missing from much existing research on scientific machine learning.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the main keywords and key terms associated with this paper include:

- Conservation laws
- Projection method
- Dynamical systems
- Physics-informed neural networks
- Deep operator networks
- First integrals
- Invariant manifolds
- Geometric numerical integration
- Geometric numerical machine learning
- Symplectic structures
- Lie symmetries

The paper introduces a method for training exactly conservative physics-informed neural networks and physics-informed deep operator networks for dynamical systems. The key idea is to use a projection method to map learned candidate solutions onto invariant manifolds defined by first integrals or conservation laws. This allows the neural network models to preserve important geometric properties and qualitative behavior of dynamical systems over long time intervals. The method aims to bring ideas from geometric numerical integration regarding structure-preserving numerical schemes into the emerging field of scientific machine learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions that exactly conservative physics-informed neural networks and DeepONets vastly outperform their non-conservative counterparts. Can you elaborate on some of the key advantages and differences in performance that were observed? What specific metrics were used to quantify the improved performance?

2. The projection method relies on mapping a candidate neural network solution onto the invariant manifold defined by the first integrals. What are some of the assumptions behind this approach and in what scenarios could it potentially fail or not work effectively? 

3. For partial differential equations, the authors mention that pointwise projection methods are not suitable for enforcing conservation laws, which require preservation over the entire spatial domain. Can you suggest or speculate on some alternative approaches that could be used to develop conservative physics-informed neural networks for PDEs?

4. The projection schedule and soft update of the solution during training seems quite important for the success of the conservative neural network training. Can you explain the rationale behind this approach? How sensitive is the training process to the choice of projection schedule?

5. The method uses a simplified Newton iteration to solve the constrained optimization problem for projecting onto the invariant manifold. What are some of the tradeoffs in using only a few Newton iterations? Could more advanced optimization methods be beneficial?

6. For chaotic systems like the double pendulum, what are some challenges in developing conservative neural network solvers? How does the projection method need to be adapted or interpreted for such systems?

7. The method is presented for first-order ordinary differential equations. How could it be extended to systems of higher-order ODEs while preserving geometric properties?

8. For non-autonomous systems, the method proposes introducing time as an additional variable. Are there any limitations or caveats introduced with this approach in terms of conservation laws?

9. The comparisons in the paper focus on standard physics-informed neural networks. How would you expect the performance comparison to change if compared with state-of-the-art neural ODE or neural SDE solvers?

10. The method relies on access to known analytical first integrals for a given dynamical system. Is there a way to develop more general conservative neural network solvers that do not require explicit knowledge of first integrals?
