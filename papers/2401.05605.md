# [Scaling Laws for Forgetting When Fine-Tuning Large Language Models](https://arxiv.org/abs/2401.05605)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) achieve state-of-the-art performance when pre-trained on large datasets and fine-tuned on downstream tasks. However, fine-tuning can cause the model to "forget" capabilities it originally had. 
- Forgetting is problematic as it degrades model performance on unrelated tasks and can remove safety guardrails. Prior work showed larger LLMs suffer more forgetting, but less is known about common fine-tuning methods.

Methods
- The authors fine-tune Llama 2 chat 7B using low-rank adapters (LoRA) which allows modifying the number of parameters tuned. They vary adapter ranks and measure forgetting using cross-entropy loss between fine-tuned and original model predictions.

- Experiments involve fine-tuning on an instruction following dataset (OpenOrca) and recent news articles. Forgetting is evaluated on WikiText-103. Safety/reasoning effects are examined via model generations.

Key Results
- Forgetting has an inverse linear relationship with fine-tuning loss. It increases as a power law with number of parameters tuned and update steps.

- Fine-tuning performance vs. forgetting plot shows forgetting is primarily dictated by fine-tuning loss. Achieving better performance inevitability causes more forgetting.

- Inspection of model generations shows substantial forgetting of safety behaviors and reasoning abilities when fine-tuned, even with few parameters changed.

Main Contributions
- Established precise mathematical relationships between amount of forgetting and fine-tuning loss, parameters updated, training steps.

- Demonstrated inadequacy of dataset metrics for forgetting and proposed improved cross-entropy based metric. 

- Showed forgetting occurs even with parameter efficient methods like LoRA and cannot be avoided by early stopping or tuning fewer/more parameters.

- Underscored need for techniques that mitigate forgetting when fine-tuning LLMs to maintain capabilities.


## Summarize the paper in one sentence.

 This paper empirically demonstrates power law relationships between catastrophic forgetting, fine-tuning performance, number of parameters fine-tuned, and number of update steps when adapting large language models.


## What is the main contribution of this paper?

 The main contributions of this paper are listed in Section 1 and include:

1) Showing that forgetting is strongly predicted by an inverse linear relationship with fine-tuning loss when fine-tuning large language models. 

2) Obtaining precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps.

3) Highlighting inadequacies of dataset-focused evaluation scores as a measure of forgetting, and proposing a forgetting metric based on the cross-entropy between the pre-trained model and fine-tuned model.

So in summary, the paper quantifies the relationships between forgetting, fine-tuning performance, number of parameters tuned, and number of update steps. It also proposes an improved metric for measuring forgetting. The results suggest that forgetting cannot be easily avoided through early stopping or varying the number of tuned parameters.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- Fine-tuning 
- Forgetting
- Catastrophic forgetting
- Parameter-efficient fine-tuning (PEFT)
- Low-rank adaptation (LoRA)
- Scaling laws
- Power laws
- Knowledge forgetting
- Safety forgetting
- Reasoning forgetting
- Cross-entropy loss
- Fine-tuning loss
- Number of parameters 
- Number of update steps
- Mitigating forgetting

The paper studies the phenomenon of catastrophic forgetting when fine-tuning large language models, especially using parameter-efficient methods like LoRA. It quantifies relationships between forgetting, fine-tuning loss, number of parameters tuned, and number of update steps. It also examines how forgetting impacts knowledge, reasoning capabilities, and safety. Overall, the paper underscores the need to develop techniques that can mitigate forgetting during LLM fine-tuning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The authors argue that their forgetting metric based on cross-entropy between the pre-trained and fine-tuned model is the most appropriate for quantifying forgetting. What are some potential limitations of this metric? Could there be cases where it overestimates or underestimates forgetting?

2. The inverse linear relationship found between forgetting and fine-tuning loss is concerning as it suggests forgetting may be unavoidable. Can you think of any alternative fine-tuning approaches that could potentially break this relationship to achieve better performance without as much forgetting?

3. The scaling laws found for forgetting in terms of parameters tuned and steps trained are similar in form to previously found laws for pre-training loss. Does this suggest some fundamental connection between the forgetting phenomenon and the learning dynamics when scaling up model size and compute? 

4. When analyzing model generations, different levels of forgetting were observed on the ARC vs AdvBench datasets between the News and OpenOrca fine-tuned models. What underlying reasons could explain why a model fine-tuned on reasoning data forgets safety while a model fine-tuned on news articles forgets reasoning capability?

5. LoRA adapters were crucial to uniformly varying parameters tuned in these experiments. How well do you think the results generalize to other parameter-efficient fine-tuning methods? Could the relationships found look fundamentally different?

6. The authors surface potential inadequacies in using dataset-focused metrics for evaluating forgetting. What are some other proxy tasks and metrics that could be used to more directly evaluate specific types of forgetting, like of reasoning skills or language generation quality? 

7. How well do you think these results on a 7B parameter model apply to even larger language models today? Would the relationships found strengthen or weaken as model scale increases?

8. The authors fine-tuned for a relatively small number of steps. How might forgetting behaviors change if models are tuned for more steps or multiple epochs? Could overfitting effects emerge?

9. The choice of pre-trained model, adapter architecture, and fine-tuning datasets could all impact the generalizability of these results. What sensitivity studies could be done around these choices?

10. The authors surface the need for techniques to mitigate forgetting when fine-tuning LLMs. What are some promising methods from incremental learning or continual learning that you think could help address catastrophic forgetting in this domain?
