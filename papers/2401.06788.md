# [The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in   CNVSRC 2023](https://arxiv.org/abs/2401.06788)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Visual speech recognition (VSR) aims to transcribe speech from silent video containing lip movements. It is useful in scenarios where audio is unavailable. However, research in large vocabulary continuous VSR (LVCVSR) remains limited, especially for Chinese. The Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023 seeks to advance LVCVSR in Chinese, defining two tasks - Single-Speaker VSR and Multi-Speaker VSR.

Proposed Solution:
The paper proposes an end-to-end VSR system with joint CTC/attention loss, consisting of data augmentation, ResNet3D visual frontend, E-Branchformer encoder, and Transformer decoder. Multi-scale lip video and speed perturbations augment the data. Multiple encoders and scales are used to build diverse VSR systems for ROVER fusion. 

A language model rescores the output during decoding for the shallow fusion framework. Experiments are conducted in the fixed and open tracks. 

Main Contributions:
- Achieves top performance on all tracks with 34.76% CER on Single-Speaker task and 41.06% CER on Multi-Speaker task after fusion.
- Systematically explores multi-scale lip crops (48-112) and encoders like E-Branchformer, Branchformer, Conformer. 
- Employs data augmentation techniques including speed perturbation, rotation, flip and color transforms.
- Proposes a shallow fusion technique for decoder via language model re-scoring.

In summary, the paper presents a state-of-the-art end-to-end VSR system attaining top performance on the CNVSRC 2023 challenge across multiple tracks using data augmentation, multi-scale architectures and fusion techniques.


## Summarize the paper in one sentence.

 This paper describes an end-to-end visual speech recognition system with joint CTC/attention loss, using multi-scale lip motion data and various encoders, that achieves state-of-the-art performance on the CNVSRC 2023 challenge.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is the visual speech recognition (VSR) system proposed by the NPU-ASLP-LiAuto team for the Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023. Specifically:

- They propose an end-to-end VSR system comprising data augmentation, a ResNet3D visual frontend, an E-Branchformer encoder, and a Transformer decoder. 

- They build multiple systems with different encoders (E-Branchformer, Branchformer, Conformer) and different lip crop sizes to enable multi-system fusion using ROVER. 

- Their proposed approach achieves the best performance in all three tracks they participate in for the CNVSRC 2023 challenge, including the fixed and open tracks of the Single-Speaker VSR Task and the open track of the Multi-Speaker VSR Task.

- They achieve character error rates (CER) of 34.76% on the Single-Speaker task and 41.06% on the Multi-Speaker task on the final evaluation sets, outperforming all other participating systems.

In summary, the main contribution is their state-of-the-art visual speech recognition system tailored for and evaluated on the CNVSRC 2023 challenge datasets.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, the key terms and keywords associated with this paper appear to be:

Visual Speech Recognition, Lip Reading, Chinese Continuous Visual Speech Recognition Challenge (CNVSRC), Single-Speaker VSR Task, Multi-Speaker VSR Task, lip motion extractor, data augmentation, speed perturbation, random rotation, horizontal flipping, color transformation, end-to-end model, CTC/attention loss, ResNet3D, visual frontend, E-Branchformer, Branchformer, Transformer decoder, system fusion

The paper describes the visual speech recognition system developed by the NPU-ASLP-LiAuto team for the CNVSRC 2023 challenge. The key aspects include the data processing, use of data augmentation, the end-to-end model architecture comprising the visual frontend, encoder variants like E-Branchformer, and decoder, as well as system fusion to combine multiple systems. So these appear to be the main keywords and key terms associated with summarizing this paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper mentions using multi-scale lip motion video data. What were the different scales used and what was the rationale behind using multiple scales instead of a single scale? 

2. Speed perturbation is used during data augmentation. What were the different speed rates used? Why was speed perturbation useful?

3. The visual frontend uses a 5-layer ResNet3D module. What are the advantages of using 3D convolutions over 2D convolutions for processing video data? 

4. Three different encoders - E-Branchformer, Branchformer and Conformer - are experimented with. How do they differ in architecture? Why was E-Branchformer found to perform the best?

5. The loss function comprises both CTC and attention loss. Why is using both losses beneficial compared to using only one? How are the losses weighted during training?

6. What is the Transformer decoder architecture used in the experiments (number of layers, heads etc.)? How were these hyperparameters tuned?

7. The paper utilizes a language model during decoding for shallow fusion. How large is the language model and what benefits does it provide? 

8. What beam size was used during decoding? How were the weights for CTC and language model scores determined during beam search?

9. What was the rationale behind using multi-system fusion? Why didn't the paper just focus on improving a single system instead?

10. The proposed model architecture has separate modules for visual feature extraction, encoding and decoding. What were some alternative architectures explored? Why was the final choice made?
