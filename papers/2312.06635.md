# [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes gated linear attention (GLA) Transformers, which incorporate data-dependent gating mechanisms into linear attention Transformers. GLA Transformers allow for efficient parallel training like standard Transformers, while also enabling linear complexity during inference like recurrent neural networks. The authors first derive a parallel form for GLA that avoids materializing large hidden states. However, this form requires non-standard matrix operations that do not use GPU tensor cores. To address this, the authors develop a chunk-wise parallel form that combines chunk-level recurrence with mostly standard parallel matrix multiplications. This strikes a balance between computation efficiency, memory efficiency, and numerical stability. Experiments on language modeling with 340M-1.3B parameter models demonstrate that GLA Transformers are competitive with strong Transformer baselines in terms of accuracy, while having comparable or faster training speeds. The results highlight the promise of GLA Transformers as an efficient alternative to standard Transformers.
