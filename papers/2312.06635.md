# [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes gated linear attention (GLA) Transformers, which incorporate data-dependent gating mechanisms into linear attention Transformers. GLA Transformers allow for efficient parallel training like standard Transformers, while also enabling linear complexity during inference like recurrent neural networks. The authors first derive a parallel form for GLA that avoids materializing large hidden states. However, this form requires non-standard matrix operations that do not use GPU tensor cores. To address this, the authors develop a chunk-wise parallel form that combines chunk-level recurrence with mostly standard parallel matrix multiplications. This strikes a balance between computation efficiency, memory efficiency, and numerical stability. Experiments on language modeling with 340M-1.3B parameter models demonstrate that GLA Transformers are competitive with strong Transformer baselines in terms of accuracy, while having comparable or faster training speeds. The results highlight the promise of GLA Transformers as an efficient alternative to standard Transformers.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Transformers with linear attention allow efficient parallel training but require quadratic complexity for inference. Recent works like RetNet and TransNormerLLM show that adding a global decay factor to the additive RNN update rule greatly improves performance. However, a data-independent decay factor is unlikely to be optimal. Prior works added data-dependent gates but relied on the recurrent form which lacks parallelism, has high I/O cost to store states, and is dominated by memory-bound ops instead of tensor core matmuls.

Proposed Solution:
This paper proposes an efficient training strategy for linear attention Transformers with data-dependent forget gates:

1) Derives a parallel form to compute outputs in parallel without materializing all hidden states. But it requires non-standard matrix multiplications in log-space for stability which cannot use tensor cores. 

2) Develops a chunk-wise parallel form that strikes a balance - it has inter-chunk recurrence to sync information between chunks and uses the parallel form for faster intra-chunk computation. This allows leveraging tensor cores through block-parallel computations over chunks.

3) Proposes a 2-level chunking to maximize tensor core usage - first level divides the sequence into chunks, second level divides chunks into sub-chunks. Sub-chunks interact in parallel via half-precision matmuls.

Main Contributions:
1) First work to develop an efficient parallel training strategy for data-dependent gated linear attention, addressing limitations of prior recurrent form-based approaches.

2) Proposes chunk-wise parallel form to strike balance between recurrence and parallelism for efficiency.

3) Hardware-aware 2-level chunking to maximize tensor core utilization through block-parallel computations over chunks and sub-chunks.

4) Achieves strong performance compared to Transformer++ baseline and recent state-space model on language modeling. Triton implementation compares to optimized FlashAttention-2 for speed.

In summary, the paper enables efficient training of linear attention Transformers with fine-grained data-dependent gates via a hardware-friendly chunk-wise parallel form and two-level chunking. This matches performance of efficient Transformer baselines while ensuring training speed and tensor core utilization.
