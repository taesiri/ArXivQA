# [AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation](https://arxiv.org/abs/2305.09515)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing an autoregressive diffusion model for text generation. The key ideas and hypotheses explored are:- Natural language exhibits strong sequential dependencies, whereas current diffusion models for text generate all tokens concurrently in a non-autoregressive manner. This misses important position dependencies.- An autoregressive approach where tokens are generated sequentially from left to right is better suited for text generation. - A diffusion model can be adapted to capture autoregressive behavior by having variable diffusion steps for tokens based on their position. Tokens on the left undergo fewer diffusion steps so they are generated earlier and can influence later tokens.- Using position-based variable diffusion steps allows combining the benefits of autoregressive modeling with the generative power and parallel decoding speed of diffusion models.- The proposed autoregressive diffusion model called AR-Diffusion will outperform non-autoregressive diffusion models as well as autoregressive models like Transformers in text generation quality, while being faster.So in summary, the key hypothesis is that adapting diffusion models to make them autoregressive in a position-dependent way will result in improved text generation compared to both standard autoregressive and non-autoregressive models. The paper presents the AR-Diffusion method and empirically evaluates this hypothesis across different text generation tasks.
