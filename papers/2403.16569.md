# [Revealing Vulnerabilities of Neural Networks in Parameter Learning and   Defense Against Explanation-Aware Backdoors](https://arxiv.org/abs/2403.16569)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Explainable AI (XAI) methods like saliency maps are important for interpretability and trust in neural nets, but can be misled by adversarial attacks that alter inputs to change explanations while maintaining accuracy. This poses a serious reliability challenge for XAI methods.  

- The paper focuses on three types of targeted attacks - Simple Fooling (alters explanation), Red Herring (alters both prediction and explanation), and Full Disguise (alters prediction only).

- Defending against these attacks on XAI is an open challenge with few existing solutions.

Solution:
- The paper provides a statistical analysis of how adversarial fine-tuning impacts model weights, showing batch normalization (BN) layers protect core weights but their learned parameters enable attacks. 

- They propose Channel-wise Feature Normalization (CFN) to replace BN during prediction. This normalizes activations, mitigating attack artifacts without retraining.

Contributions:
- Empirical evidence that BN protects core weights during attacks but learned BN parameters facilitate attacks on explanations.

- CFN defense that significantly reduces attack success rate and explanation manipulation for three attack types, adaptable to multiple XAI methods, without extra training.

- Thorough experiments on CIFAR-10 and GTSRB datasets against state of the art attacks, showing CFN decreases attack success rate by ~99% and explanation error by ~91% while maintaining accuracy.

- Analysis showing removing or freezing BN does not protect against attacks, but normalizing activations with CFN during prediction neutralizes attack artifacts effectively.

In summary, the paper provides useful insights into how attacks impact model weights and presents a simple but effective CFN technique to defend XAI methods against advanced explanation manipulation attacks without expensive retraining.
