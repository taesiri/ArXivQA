# [Stabilizing DARTS with Amended Gradient Estimation on Architectural   Parameters](https://arxiv.org/abs/1910.11831)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: How can we improve the stability and reliability of neural architecture search using the DARTS framework?Specifically, the authors identify an "optimization gap" as a core issue leading to instability and poor performance in DARTS. This optimization gap refers to the disconnect between optimizing the validation accuracy of the super-network during architecture search versus optimizing the final performance of the sampled sub-networks after search. To address this gap, the paper proposes two main contributions:1) Amending the architectural gradient estimation in DARTS to provide a better approximation of the true gradients. This helps optimize the architecture parameters in a more stable and reliable direction during search.2) Improving hyperparameter consistency between the architecture search and retraining stages. This includes using the same network width, depth, and regularization techniques in both phases. Overall, the central hypothesis is that by bridging the optimization gap through these two modifications, DARTS-based neural architecture search can be significantly stabilized and improved to explore larger search spaces and find better architectures. The experiments aim to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:1. Identifying an "optimization gap" in DARTS between improving the validation accuracy of the super-network and finding high-quality sub-networks. The paper argues that continuing to train the DARTS super-network leads to worse sub-networks, despite better super-network accuracy. 2. Proposing a new method to approximate the architectural gradient that has bounded error compared to the true gradient. This helps "bridge" the optimization gap.3. Showing that with their amended gradient approximation, DARTS becomes much more stable. It avoids converging to low-quality "all skip-connect" architectures even after long training times.4. Demonstrating that the improved stability allows searching larger spaces and achieving better performance. Experiments on CIFAR and ImageNet show results competitive with or exceeding the state-of-the-art NAS methods.5. Advocating for consistency between the search and re-training stages. Using the same network depth, width, and training hyperparameters helps shrink the optimization gap.In summary, the key ideas are identifying the optimization gap issue in DARTS, proposing a better gradient approximation to address it, and showing this enables more stable and effective NAS with larger search spaces. The amended gradient is the core methodological contribution.


## How does this paper compare to other research in the same field?

 This paper presents an approach to stabilize DARTS, a popular differentiable neural architecture search (NAS) method. Here are some key ways it compares to other NAS research:- Motivation: The paper identifies instability in DARTS as a key weakness. This instability is evidenced by sensitivity to hyperparameters and convergence to suboptimal architectures. Improving stability is the main focus, rather than improving accuracy or efficiency.- Approach: The core idea is to amend the gradient approximation used for the architectural parameters in DARTS. This helps bridge the optimization gap between the search and retraining phases. In contrast, other works like PDARTS and PC-DARTS use modifications like progressive search and edge normalization to improve stability.- Evaluation: Experiments are done primarily on CIFAR-10 and ImageNet image classification. On CIFAR-10, the amended DARTS achieves competitive accuracy to state-of-the-art NAS methods. The focus is more on demonstrating improved stability than achieving SOTA accuracy. - Search space: The amended DARTS is able to stably search larger spaces than standard DARTS, including individual architectures per cell rather than shared parameters. Most NAS methods still use more limited cell-based search spaces.- Generality: The gradient amendment technique could likely improve stability in other NAS algorithms beyond DARTS, but this is not explored. Most papers focus on a specific NAS algorithm.In summary, this paper provides a new perspective on instability in DARTS, proposes a principled gradient amendment method to address it, and demonstrates improved stability. The focus is more on analyzing and fixing instability rather than maximizing accuracy or efficiency.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions:- Exploring larger and more complex search spaces. The authors show that with their proposed stabilized gradient estimation, it becomes feasible to search over larger and more complex spaces. They suggest this is an important future direction as larger search spaces allow more flexibility in finding optimal architectures. - Automating the search space definition. Currently, the search space needs to be manually defined. The authors suggest automating the process of constructing the search space as an area for future work.- Search space and network weight sharing. The current approach separates the search space definition and network weight optimization. The authors suggest investigating joint search space optimization and weight training.- Extending to other tasks. The current work focuses on image classification. The authors suggest expanding the stabilized search method to other tasks like object detection, semantic segmentation, etc.- Theoretical analysis. While the proposed amended gradient estimation is shown empirically to improve stability, the authors suggest further theoretical analysis on why the original gradient estimation fails and how the amended version works.In summary, the main future directions pointed out are exploring larger and more complex search spaces, automating search space construction, integrating search space optimization and weight training, applying to other tasks beyond image classification, and further theoretical analysis. The core idea is leveraging the stabilized search method to tackle more challenging and open problems in neural architecture search.


## Summarize the paper in one paragraph.

 The paper proposes an approach to stabilize DARTS, a popular neural architecture search (NAS) method, by amending the gradient estimation on the architectural parameters. The key ideas are:- DARTS suffers from instability issues, where longer search epochs lead to degenerate architectures with mostly skip connections. This is attributed to an optimization gap between improving the super-network performance and finding good architectures. - The cause is inaccurate gradient estimation on the architectural parameters. The authors propose a modified gradient approximation that guarantees bounded error.- This amended gradient bridges the optimization gap from two aspects - improving architectural gradient estimation, and unifying hyper-parameters between search and retraining.- Experiments on CIFAR and ImageNet show the proposed approach enables stable search over larger spaces, producing competitive architectures compared to prior NAS methods. The stabilized search also works well for recurrent architectures on PTB language modeling.Overall, the paper provides important insights into instability issues in differentiable NAS, and presents a principled solution through more accurate architectural gradient approximations during the bi-level optimization process.


## Summarize the paper in two paragraphs.

 Here is a 2-paragraph summary of the paper:This paper proposes a method to stabilize the neural architecture search algorithm DARTS. DARTS suffers from instability issues, often converging to trivial "dummy" architectures like all-skip connections when trained for longer epochs. The authors attribute this to an "optimization gap" between improving the validation accuracy of the super-network versus finding high-quality sub-networks. They show this gap arises from inaccurate estimation of the architectural gradient. To address this, the authors propose amending the architectural gradient estimation with a second-order term. They mathematically prove this amended gradient has bounded error compared to the true gradient, while the original DARTS estimation has unbounded error. Experiments on CIFAR-10 and ImageNet show their amended DARTS converges to reasonable networks even after 500 epochs of search, exploring larger search spaces. The found architectures achieve competitive accuracy to state-of-the-art on image classification. Thus, their gradient amendment stabilizes DARTS, enabling search over larger spaces.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to stabilize DARTS, a popular neural architecture search algorithm, by amending the gradient estimation of the architectural parameters. The key ideas are:- DARTS suffers from instability issues, where longer search epochs lead to degenerate "all skip-connect" architectures with poor performance. This is attributed to an optimization gap between improving the validation accuracy of the super-network and finding high-quality architectures. - The cause is inaccurate approximation of the architectural gradients, specifically the second-order term. An amended approximation is proposed with bounded error.- This amended gradient fixes the optimization gap. Experiments show the search process becomes more robust and can find competitive architectures after full convergence, without sensitive hyperparameters. Larger search spaces can also be explored.In summary, by amending the architectural gradient computation, the paper provides a principled way to stabilize DARTS and enable search space exploration.


## What problem or question is the paper addressing?

 The paper is addressing the issue of instability in neural architecture search methods like DARTS. Specifically, it investigates why DARTS-based methods often converge to suboptimal "dummy" architectures like all-skip connections when trained for longer epochs. The key questions the paper tries to address are:1. Why does optimizing the DARTS super-network not reliably lead to better sub-networks after discretization? The paper refers to this issue as the "optimization gap".2. What causes the inaccurate estimation of architectural gradients in DARTS, leading to poor optimization and instability?3. How can the architectural gradient estimation be improved to enable stable search over longer epochs and larger search spaces?To summarize, the main focus is understanding and addressing the instability of DARTS to enable more effective and scalable neural architecture search. The key contribution is an amended gradient estimation method to stabilize and improve search.
