# [PLANNER: Generating Diversified Paragraph via Latent Language Diffusion   Model](https://arxiv.org/abs/2306.02531)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate high-quality, diverse, and fluent long-form text while exercising global control over the generation?The paper proposes a model called PLANNER that combines latent semantic diffusion with autoregressive decoding to address this question. The key ideas are:- Learn a variational autoencoder (VAE) model to encode input text into a small set of continuous latent codes that capture paragraph-level semantics.- Apply a denoising diffusion model on these latent codes to iteratively refine them in a coarse-to-fine manner. This "planning" via latent diffusion provides global control over the generation. - Use an autoregressive decoder to convert the refined latent codes into fluent raw text. The decoder focuses on "decoding" the semantics into natural language.- The combination allows generating diverse and high-quality long-form text while leveraging the global editing capabilities of diffusion models and local fluency of autoregressive models.So in summary, the central hypothesis is that applying latent diffusion on a paragraph embedding space can enhance long-form text generation in terms of diversity and global coherence, compared to just using autoregressive or raw text diffusion models. The PLANNER model is proposed to test this hypothesis.


## What is the main contribution of this paper?

Here is a summary of the key contributions of this paper:- The paper proposes a new method called PLANNER (Paragraph-level Diffusion model for Embedding Representation) that combines latent semantic diffusion with autoregressive generation for text generation. - A variational autoencoder is used to learn paragraph embeddings that condense lengthy text into a small set of continuous codes capturing semantics. These embeddings are used with a diffusion model to generate semantics in a coarse-to-fine manner.  - An autoregressive decoder then converts the semantic codes into fluent raw text. This allows global control over paragraph semantics via diffusion while retaining fluency with the decoder.- The method is shown to generate more diverse and less repetitive text compared to autoregressive baselines across tasks like sentiment-guided generation, text completion, and summarization.- A new metric called AuBLEU is introduced to evaluate the denoising capability of text diffusion models by assessing reconstruction quality over varying noise levels.- Analysis is provided on optimizing the paragraph embedding space for accuracy, smoothness, and minimizing conversion errors. The impact of diffusion steps on generation quality is also analyzed.In summary, the key contribution is a new technique to generate long, high-quality text by combining the benefits of latent semantic diffusion and autoregressive decoding. The method improves diversity while maintaining accuracy and fluency.
