# [PLANNER: Generating Diversified Paragraph via Latent Language Diffusion   Model](https://arxiv.org/abs/2306.02531)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate high-quality, diverse, and fluent long-form text while exercising global control over the generation?The paper proposes a model called PLANNER that combines latent semantic diffusion with autoregressive decoding to address this question. The key ideas are:- Learn a variational autoencoder (VAE) model to encode input text into a small set of continuous latent codes that capture paragraph-level semantics.- Apply a denoising diffusion model on these latent codes to iteratively refine them in a coarse-to-fine manner. This "planning" via latent diffusion provides global control over the generation. - Use an autoregressive decoder to convert the refined latent codes into fluent raw text. The decoder focuses on "decoding" the semantics into natural language.- The combination allows generating diverse and high-quality long-form text while leveraging the global editing capabilities of diffusion models and local fluency of autoregressive models.So in summary, the central hypothesis is that applying latent diffusion on a paragraph embedding space can enhance long-form text generation in terms of diversity and global coherence, compared to just using autoregressive or raw text diffusion models. The PLANNER model is proposed to test this hypothesis.
