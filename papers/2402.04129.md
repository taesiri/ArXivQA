# [OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free   Class-Incremental Learning](https://arxiv.org/abs/2402.04129)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Class-incremental learning (CIL) aims to train a model on a sequence of tasks with disjoint sets of classes. A key challenge is catastrophic forgetting - the model forgets knowledge from previous tasks when trained on new tasks. Recent prompt-based rehearsal-free methods mitigate forgetting by using a large pre-trained model with learnable prompts, but still struggle to distinguish classes across tasks as they are not trained jointly. They also require complex mechanisms to select task-specific prompts from a prompt pool during inference.

Proposed Solution - OVOR:
This paper proposes OVOR, which combines a simplified one-prompt approach called OnePrompt with a novel virtual outlier regularization technique. 

OnePrompt uses a single prompt instead of a prompt pool, eliminating the need for a query function to compose prompts. Surprisingly, it outperforms prior arts like L2P and DualPrompt that use prompt pools. OnePrompt sees little representation drift as the prompt changes minimally across tasks.

The virtual outlier regularization further tightens decision boundaries between classes by generating synthetic outliers and penalizing the model if it assigns high confidence to them. This regularization is compatible with different prompt CIL methods.

Main Contributions:
1) Proposes virtual outlier regularization method to reduce inter-task confusion and boost various prompt CIL methods.

2) Shows OnePrompt with a single prompt matches or exceeds prompt pool methods, with faster inference and fewer parameters.

3) Combines the above ideas into OVOR, outperforming state-of-the-art on ImageNet-R and CIFAR-100 benchmarks while simplifying the prompt mechanism.

Overall, the paper makes prompt-based rehearsal-free CIL more effective via regularization and efficient via simplification, advancing the state-of-the-art.
