# [Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection   via Retrieval-Augmented GPT-4 and LLaMA](https://arxiv.org/abs/2402.06549)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper explores stance, target, and hate speech detection in the context of climate activism tweets, using the new ClimaConvo dataset. Rather than training customized classifiers, it investigates how well large language models (LLMs) like GPT-4 can perform in zero-shot and few-shot settings when creatively prompted.

Method:
- GPT-4 is prompted with patterns and examples to classify tweets into categories for the 3 subtasks. No finetuning of the model is done.
- Retrieval augmentation retrieves and adds relevant examples from a vector database to enhance the prompts. Models like MiniLM and mpnet create the vector embeddings.
- Re-ranking with tools like FlashRank further refines the retrieved examples. 
- The LLM output is parsed to extract the predicted class. An ablation study with LLaMA is also done.

Results: 
- The prompted GPT-4 significantly beats baseline classifiers on all subtasks, achieving 2nd place in Target Detection.
- Adding retrieval augmentation improves performance. Optimal prompts and number of examples varies per subtask.
- Re-ranking brings marginal gains over retrieval augmentation.
- The base model impacts overall performance much more than the additions, as evidenced by LLaMA's weaker scores.

Conclusions:
- With creative prompting and retrieval augmentation, LLMs can match or exceed traditional methods for tweet classification. The black-box LLM approach achieved very competitive results, demonstrating its viability for fast development of classification solutions.


## Summarize the paper in one sentence.

 This paper details an approach using GPT-4 with retrieval augmentation and re-ranking for stance, target, and hate speech detection in tweets, achieving second place performance in the target detection subtask of the CASE 2024 shared task.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

Exploring the capability of Large Language Models (LLMs), particularly GPT-4, in zero- or few-shot settings enhanced by retrieval augmentation and re-ranking for Tweet classification on the tasks of Hate Speech Detection, Hate Speech Target Identification, and Stance Detection. The goal was to determine if LLMs could match or surpass traditional methods in this context of climate activism tweets. The results indicate that the LLM-based models significantly outperformed the baselines, securing second place in the Target Detection task, demonstrating the viability of this simple yet effective approach.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Stance detection
- Target detection  
- Hate speech detection
- Climate activism
- GPT-4
- LLaMA
- Large language models (LLMs)
- Zero-shot learning
- Few-shot learning
- Retrieval augmentation
- Re-ranking
- Prompt engineering
- ClimaConvo dataset

To summarize, this paper explores using large language models like GPT-4 and LLaMA for stance, target, and hate speech detection on tweets related to climate activism, with a focus on prompt engineering techniques like retrieval augmentation and re-ranking to improve performance in low-data regimes. The models are evaluated on the new ClimaConvo dataset.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper explores using GPT-4 and LLaMA in zero-shot and few-shot settings for the stance, target, and hate detection tasks. What are the key advantages and limitations of this approach compared to traditional supervised methods like SVM or custom neural network architectures?

2. Retrieval augmentation is utilized to provide additional in-context examples to the language models. How is the retrieval database created and what sentence encoder models are experimented with? What impact does the choice of encoder have on performance?  

3. Re-ranking using the flashrank library is explored after retrieval augmentation. What is the motivation behind re-ranking and does it provide significant gains over just retrieval augmentation? What alternatives to flashrank could be viable?

4. The prompts provided to the language models are crucial to performance. How are the initial prompt templates generated? What is the process and rationale behind prompting the models?

5. What post-processing is done on the free-form language model outputs to convert them into class predictions? What are some challenges faced in parsing the outputs?

6. The error analysis reveals significant issues with wrong labels in the dataset. What percentage of errors fall under the wrong label category and how does correcting them impact performance estimates?

7. How do the results compare between GPT-4 and LLaMA across different configurations? What factors contribute the most to the performance gap observed?

8. How do the best proposed models compare to the baseline SVM and LSTM models provided? What improvements are observed across precision, recall and F1 scores?

9. The paper establishes strong performance on the target detection sub-task. Analyze the confusion matrix - where does the model make most errors and how can it be improved further?

10. The code and models are made publicly available. What are some limitations in reproducibility and how can the work be extended by future research?
