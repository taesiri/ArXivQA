# [Learning to Estimate Critical Gait Parameters from Single-View RGB   Videos with Transformer-Based Attention Network](https://arxiv.org/abs/2312.00398)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel spatio-temporal Transformer network for estimating critical gait parameters from single-view RGB videos. The model first extracts 2D pose keypoints using OpenPose. The extracted poses are then fed into a Transformer encoder consisting of spatial and temporal multi-head self-attention blocks to capture spatial relationships between joints and temporal dynamics across frames. Compared to prior state-of-the-art 1D CNN models on a cerebral palsy dataset, the proposed approach achieves better performance in predicting parameters like Gait Deviation Index, knee flexion angle, and walking speed while using fewer parameters. The flexibility of the Transformer architecture allows effective learning of intricate spatio-temporal features without requiring manual feature engineering. By eliminating specialized equipment and manual effort, this study demonstrates the feasibility of using deep learning for automated and scalable clinical gait analysis from ordinary videos. Key strengths include interpretable attention weights, strong generalization, and accessible single-camera input. Limitations relate to performance on cadence prediction. Overall, this work represents important progress towards economical and quantitative assessment of movement disorders.
