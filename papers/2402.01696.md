# [HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text   Classification](https://arxiv.org/abs/2402.01696)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Hierarchical text classification (HTC) is a complex task that involves categorizing text data into a hierarchy of predefined classes. It faces challenges like imbalanced label distributions, complex dependencies between labels, and difficulty in capturing correlation between text and label semantics.
- Existing methods use static document representations, unable to adapt to varying relevance of text sections across hierarchy levels. They also struggle to handle long-tailed classes with limited training data.

Proposed Solution:
- The paper proposes HiGen, a text generation framework that encodes dynamic text representations using language models like BART. 
- A pretraining strategy is introduced to adapt the language model to domain knowledge and address data imbalance. Weakly labeled in-domain data is used to pretrain the model.
- The training objective includes a level-guided semantic loss to align text and label semantics, along with output space and token constraint losses. 
- Decoding is auto-regressive, generating labels conditioned on input text and previous outputs, mimicking the hierarchical classification process.

Main Contributions:
- Novel pretraining strategy to inject domain knowledge into model and mitigate data imbalance issue
- Level-guided semantic loss formulation to capture evolving correlation between text and labels across hierarchy
- Overall generative framework outperforms previous methods, especially for minority classes
- Introduction of ENZYME, a large-scale dataset of 30K documents for biomedical HTC based on EC taxonomy
- Experiments conducted on ENZYME, WOS and NYT datasets prove superiority over state-of-the-art approaches

In summary, the paper presents HiGen, a novel generative model using hierarchical decoding objectives for text classification that shows remarkable performance thanks to tailored pretraining and loss formulations.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes HiGen, a hierarchical text classification framework that uses a sequence-to-sequence model with specialized pretraining and loss functions to effectively capture document-label dependencies across taxonomy levels and handle class imbalance.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposes HiGen, a text generation-based framework for hierarchical text classification (HTC) that effectively captures document-label dependencies across hierarchy levels using a level-guided semantic loss.

2. Devises an efficient pretraining strategy that leverages in-domain data to align the language model with the target HTC task and domain. This shows significant performance gains, especially for classes with limited examples.

3. Introduces the ENZYME dataset - a new HTC dataset of 30,523 PubMed articles with Enzyme Commission (EC) numbers. It has a complex 4-level hierarchy with over 4,500 classes, making it a challenging HTC benchmark. 

4. Demonstrates state-of-the-art performance of HiGen on the ENZYME, WOS, and NYT datasets. It outperforms previous approaches, especially for minority classes, showing robustness to data imbalance.

5. Shows the efficiency of the model - faster training times compared to state-of-the-art baselines owing to the simpler architecture.

In summary, the main highlights are the novel pretraining strategy, level-guided loss function, high performance on multiple HTC datasets (especially on long-tailed classes), and computational efficiency of HiGen. The introduced ENZYME dataset is also a valuable contribution for biomedical HTC.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with it are:

- Hierarchical text classification (HTC)
- Sequence generation 
- Language models
- Pretraining 
- Level-guided semantics
- Long-tailed distribution
- Data imbalance
- Label hierarchies
- Enzyme classification system
- Macro F1 score
- Micro F1 score
- BART model
- Seq2seq model
- Weak supervision
- Contrastive learning
- ENZYME dataset

The paper proposes a sequence generation framework called HiGen for hierarchical text classification. It utilizes language models and pretraining to learn robust text and label representations. A level-guided semantic loss captures dependencies between text and labels across hierarchy levels. The method handles long-tailed distributions and data imbalance effectively. Experiments are conducted on benchmark datasets like WOS, NYT as well as a newly introduced dataset called ENZYME. Both Micro and Macro F1 scores are used to evaluate the method's superiority over state-of-the-art approaches. The key components include the BART model, seq2seq architecture, pretraining strategy and the custom objective function.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The pretraining strategy in HiGen involves masking certain levels in the hierarchical label and encoding them with the input text. How does the masking process specifically work? What percentage of labels are masked and what approach is followed to determine the masking?

2. The paper mentions using a margin loss to align text semantics with positive labels at each taxonomic level while pushing negative semantics apart. What is the intuition behind having a level-specific margin parameter Î±k that increases for lower levels of the hierarchy? 

3. One of the key components of HiGen is the level-guided semantic loss function. Can you explain in detail how the positive and negative document-label name pairs are constructed for this loss calculation? What impact does this loss have on capturing nuances between text documents across hierarchy levels?

4. The pretraining dataset used for HiGen on the WOS dataset leverages ChatGPT to generate abstracts. What was the methodology and prompt design strategy followed to ensure the quality, diversity and domain-alignment of the generated abstracts? 

5. HiGen demonstrates exceptional performance on classes with limited examples in the long-tail. What aspects of the model design contribute to this? Is the pretraining strategy the primary reason or do the loss functions also assist?

6. The token constraint loss penalizes predictions outside a designated vocabulary represented by the hierarchical structure. What is the motivation behind using this loss instead of restricting the decoder's generative capabilities?

7. How does HiGen balance computation efficiency while achieving state-of-the-art performance? What modifications can be made to improve training time and resource utilization further?

8. The output space loss aims to preserve the unidirectionality of hierarchy edges. Does a similar loss function apply when adapting HiGen to datasets with multi-path hierarchies? What changes would be required?

9. The paper introduces the ENZYME dataset based on EC hierarchy containing full-text articles. What were the major challenges in parsing and processing full-text articles at scale to construct this dataset?

10. HiGen demonstrates strong performance but has certain limitations acknowledged in the paper. What augmentation techniques can be applied to the in-batch sampled negative examples to make the representations more discriminative?
