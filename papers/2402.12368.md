# [A synthetic data approach for domain generalization of NLI models](https://arxiv.org/abs/2402.12368)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Natural language inference (NLI) is an important benchmark task for language models, with applications like fact-checking. 
- However, NLI datasets have limited diversity in domains and text lengths. As a result, model performance generalizes poorly to new distributions.

Proposed Solution:
- The authors propose a method to synthetically generate a large, balanced NLI dataset (685K examples) covering ~40 domains and varying premise lengths. 
- It uses a chain of language model tasks: generate domains, generate premises in those domains, generate hypotheses and labels for the premises.

Key Contributions:
- The synthetic dataset has high-quality, creative examples with accurate labels (80% match human annotations).
- Models trained on this data outperform models trained on existing datasets by a large margin (7% for T5 small) on the unseen TRUE benchmark.  
- Especially for smaller models, training on this general synthetic dataset gives better generalization than even larger models trained on existing datasets.
- Augmenting existing datasets with this synthetic data also leads to some improvements in in-distribution performance.  
- The results demonstrate the possibility of using diverse synthetic data to make NLI models more robust to distribution shifts.

In summary, the paper presents a novel way to generate synthetic NLI data that produces a dataset leading to models that generalize much better to new distributions. This opens up the possibility of creating tailored synthetic datasets to adapt models for specific downstream applications.
