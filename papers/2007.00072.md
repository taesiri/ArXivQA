# [Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that optimizing data movement is the key to improving performance when training transformers on GPUs. The authors observe that existing implementations of transformers do not efficiently utilize GPU hardware, achieving only around 30% of theoretical peak performance. They hypothesize that the main bottleneck is data movement rather than compute, and that optimizing data movement will yield significant performance improvements.Specifically, the paper proposes that:- Much of the runtime is spent in memory-bound operators like normalization rather than the compute-heavy tensor contractions.- Existing frameworks use suboptimal data layouts, which hurts performance of operations like matrix multiplication.- A global view of the dataflow is needed to optimize data movement rather than just optimizing each operator locally.To test these hypotheses, the paper develops a recipe to systematically optimize data movement:1) Construct a dataflow graph to analyze operator dependencies and data volume.2) Identify opportunities to reduce data movement via fusion.3) Evaluate performance impact of data layouts. 4) Select optimal configurations to minimize data movement.Using this recipe, the paper demonstrates major reductions in data movement and improvements in performance over existing transformer implementations, confirming their hypothesis that optimizing data movement is the key to efficient transformer training on GPUs.
