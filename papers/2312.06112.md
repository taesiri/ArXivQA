# [Converting and Smoothing False Negatives for Vision-Language   Pre-training](https://arxiv.org/abs/2312.06112)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes COSMO, a novel approach to address the issue of false negatives in vision-language pre-training (VLP). False negatives frequently arise in VLP due to many-to-many correspondences between images and texts. COSMO has two main components: (1) An efficient connection mining (ECM) process that identifies missing positive connections between non-paired but semantically related images and texts by examining plausible hard negative candidates using a discriminator module. These connections are incorporated as additional positives when calculating losses. (2) Smoothed image-text contrastive loss (S-ITC) based on label smoothing to mitigate over-penalization of false negatives within mini-batches constructed via hard negative sampling. Experiments show COSMO significantly enhances various downstream VLP tasks. Moreover, it demonstrates better performance than methods trained on larger datasets and is compatible with recent models like BLIP2. The results highlight the importance of managing false negatives in VLP, which may outweigh the role of eliminating false positives. Overall, COSMO effectively addresses the prevalent issue of false negatives in VLP.
