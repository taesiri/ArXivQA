# [Attention Is (not) All You Need for Commonsense Reasoning](https://arxiv.org/abs/1905.13497)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether the representations produced by BERT, a recently introduced pretrained language model, can be effectively utilized for commonsense reasoning tasks like the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). The authors propose that the attention mechanisms in BERT allow it to implicitly capture complex relationships between entities, which could aid in tasks requiring commonsense reasoning like coreference resolution. However, it has been unclear if BERT's representations are actually useful for PDP and WSC specifically. The paper introduces a simple method called Maximum Attention Score (MAS) to leverage BERT's attentions for commonsense reasoning. The authors' hypothesis is that MAS applied to BERT's attention maps can achieve strong performance on PDP and WSC without extensive feature engineering or annotated knowledge bases. Their experiments aim to validate whether this simple attention-based approach enables BERT to effectively perform commonsense reasoning on these tasks.In summary, the central hypothesis is that BERT's learned representations, when exploited via a simple attention-based method like MAS, are sufficient for commonsense reasoning tasks like PDP and WSC, achieving new state-of-the-art results. The paper aims to validate this hypothesis through experiments on multiple datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a simple method that utilizes the attention maps from BERT models for commonsense reasoning tasks like the Pronoun Disambiguation Problem and Winograd Schema Challenge. Specifically, the paper introduces a technique called Maximum Attention Score (MAS) that analyzes the attention between a reference word (e.g. pronoun) and candidate words to determine their association. The authors show that by using MAS on the attention maps from an unmodified BERT model, they are able to achieve state-of-the-art results on the PDP and WSC benchmarks, outperforming previous methods that rely on annotated knowledge bases or hand-engineered features. The key insight is that the self-attention mechanism in BERT appears to implicitly learn to establish complex relationships between entities, which can be exploited for commonsense reasoning simply through the attention maps, without task-specific fine-tuning. Overall, the main contribution is presenting a straightforward way to leverage BERT's pretrained attention for challenging commonsense reasoning tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using the attention mechanisms of BERT models to achieve state-of-the-art performance on commonsense reasoning tasks like pronoun resolution and the Winograd Schema Challenge, without task-specific fine-tuning.


## How does this paper compare to other research in the same field?

Here are some key points about how this ACL 2019 paper compares to other related work on commonsense reasoning and BERT:- The paper proposes a simple method to use BERT's attention maps for commonsense reasoning tasks like the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). This is novel compared to prior work that used more complex approaches involving knowledge bases, hand-engineered features, etc.- The proposed method sets new state-of-the-art results on PDP and WSC benchmarks, outperforming prior sophisticated systems. This shows the power of repurposing BERT for commonsense reasoning through a simple technique.- The authors discuss the limitations of the unsupervised BERT approach for commonsense reasoning, suggesting supervised fine-tuning or incorporation of external knowledge may be needed. This aligns with other recent papers arguing purely unsupervised methods are insufficient.- The qualitative analysis of attention maps provides some interpretability. Other BERT analysis papers focus more on probing tasks rather than visual inspection.- The comparison to prior statistical NLP methods and neural approaches provides useful context. The gains over previous neural methods like LSTMs suggest BERT's architecture is beneficial.- The authors recognize open challenges like resolving abstract/implicit references. This matches ongoing debates about the limits of language models for advanced reasoning.In summary, this paper makes a novel connection between BERT and commonsense reasoning, while thoughtfully situating the work among prior approaches and discussing remaining challenges. The simple yet effective method and state-of-the-art results are important contributions.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Fine-tuning the BERT attentions for commonsense reasoning tasks, rather than just using the attentions from an out-of-the-box BERT model. The authors suggest adapting the self-attention maps using supervision from coreference resolution and commonsense reasoning datasets could further improve performance.- Exploring whether additional unsupervised pre-training of BERT on even larger corpora could help improve commonsense reasoning abilities. The authors note that while BERT seems to learn some useful relationships, solving commonsense reasoning may require more than just language modeling on large text corpora.- Combining BERT with more structured knowledge representations. The authors discuss that relying solely on language modeling makes it difficult to resolve abstract or implicit references requiring background knowledge. Integrating external knowledge sources could help address this.- Developing more nuanced commonsense reasoning benchmarks. The authors suggest the high performance on WSC may be partly due to not requiring abstract/implicit reasoning. New benchmarks targeting those aspects could further test model capabilities.- Exploring different attention mechanisms beyond self-attention for commonsense reasoning. The authors note BERT's reliance on self-attention contrasts with RNNs that model word order and state, yet BERT still does well, suggesting future work on attention types.In summary, key directions include adapting BERT's attentions, incorporating external knowledge, developing better benchmarks, and exploring architectural variants - to move towards more robust commonsense reasoning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes using the attention mechanisms of BERT models to perform commonsense reasoning tasks like pronoun resolution and the Winograd Schema Challenge. The authors introduce a method called Maximum Attention Score (MAS) that examines the attention maps between a pronoun and candidate referent words. By masking the attention maps to only include maximum values and then comparing the sums, they generate scores for how related the words are based on BERT's attention. Without any task-specific fine-tuning of BERT, their method achieves new state-of-the-art results on the Pronoun Disambiguation Problem dataset and the Winograd Schema Challenge, outperforming previous systems that use knowledge bases or hand-engineered features. The authors suggest their results show BERT learns some implicit commonsense reasoning abilities through pre-training objectives, but likely still falls short of more human-like abstract reasoning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new method for commonsense reasoning using the BERT language model. Commonsense reasoning refers to the ability to make inferences about events and entities based on background knowledge about the everyday world. The authors focus on two commonsense reasoning tasks - the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). Both tasks involve resolving ambiguous pronouns in sentences by selecting the most appropriate referent. The key idea is to leverage the self-attention mechanism in BERT to capture relationships between words and determine the referent. Specifically, the authors compute a Maximum Attention Score (MAS) between the pronoun and candidate referents by extracting and masking the attention matrices. This allows focusing only on the most salient attentions. Experiments show the method outperforms previous state-of-the-art on both PDP and WSC benchmarks. The authors argue BERT has implicitly learned to establish complex relationships between entities, facilitating commonsense reasoning. However, performance is still far from human-level, suggesting purely unsupervised pre-training on text may not be sufficient for strong commonsense reasoning abilities.
