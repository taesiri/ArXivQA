# [Attention Is (not) All You Need for Commonsense Reasoning](https://arxiv.org/abs/1905.13497)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether the representations produced by BERT, a recently introduced pretrained language model, can be effectively utilized for commonsense reasoning tasks like the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). The authors propose that the attention mechanisms in BERT allow it to implicitly capture complex relationships between entities, which could aid in tasks requiring commonsense reasoning like coreference resolution. However, it has been unclear if BERT's representations are actually useful for PDP and WSC specifically. The paper introduces a simple method called Maximum Attention Score (MAS) to leverage BERT's attentions for commonsense reasoning. The authors' hypothesis is that MAS applied to BERT's attention maps can achieve strong performance on PDP and WSC without extensive feature engineering or annotated knowledge bases. Their experiments aim to validate whether this simple attention-based approach enables BERT to effectively perform commonsense reasoning on these tasks.In summary, the central hypothesis is that BERT's learned representations, when exploited via a simple attention-based method like MAS, are sufficient for commonsense reasoning tasks like PDP and WSC, achieving new state-of-the-art results. The paper aims to validate this hypothesis through experiments on multiple datasets.
