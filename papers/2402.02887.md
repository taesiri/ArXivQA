# [Time-, Memory- and Parameter-Efficient Visual Adaptation](https://arxiv.org/abs/2402.02887)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Foundation models like Vision Transformers are becoming very large and need to be efficiently adapted/finetuned for downstream tasks. 
- Prior efficient adaptation methods like adapters, BitFit, LoRA etc focus only on parameter efficiency. But they still require heavy backpropagation through the full backbone, making training expensive in time and memory.

Proposed Solution
- The paper proposes a method called Low-Rank Side Adaptation (LoSA) which is efficient in parameters as well as training time and memory.
- LoSA freezes the entire backbone model and learns a lightweight parallel network that operates on the backbone features. This avoids backpropagation through the backbone.
- The parallel network uses a specifically designed low-rank mixer module, which projects activations in channel/token dimensions in an alternating manner. This models interactions efficiently.

Contributions
- LoSA outperforms prior state-of-the-art on VTAB benchmark on accuracy vs parameters tradeoff when adapting ViT-Base.
- Experiments on larger datasets and models show LoSA is consistently pareto-optimal on accuracy vs parameters, training memory and speed.
- LoSA scales to adapting a 4 billion parameter ViT-e model on a 16GB GPU for video classification, outperforming baselines.
- Detailed efficiency analysis beyond just parameter counts shows gaps between parameter-efficient methods, with LoSA being superior.

In summary, the paper presents LoSA, a highly parameter- and runtime-efficient method to adapt large vision models by learning a lightweight side network. Experiments demonstrate state-of-the-art accuracy-efficiency tradeoffs on multiple datasets and models.
