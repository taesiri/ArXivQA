# [No One Left Behind: Improving the Worst Categories in Long-Tailed   Learning](https://arxiv.org/abs/2303.03630)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the performance on worst-performing categories in long-tailed recognition. 

The key hypotheses are:

1. Focusing only on average accuracy on a balanced test set can ignore poor performance on some categories, since it incurs little penalty for very low recall values.

2. Classes in the "Few" subset do not necessarily perform worse than "Many" or "Medium", so improving "Few" accuracy alone is insufficient. 

3. Optimizing for harmonic mean of per-class recall, rather than arithmetic mean, better ensures no categories are left behind.

4. A simple fine-tuning method with a novel geometric mean loss can improve worst-case and harmonic mean accuracy.

5. Ensembling the original and fine-tuned classifiers can combine strengths of both while adding little computational cost.

In summary, the central hypothesis is that explicitly optimizing for harmonic mean recall and minimum recall will improve worst-performing categories in long-tailed recognition compared to conventional approaches. The proposed methods aim to test this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Pointing out issues with the common evaluation scheme in long-tailed recognition research, which splits classes into "Head", "Medium", and "Few" subsets and reports accuracy on each. The authors argue this can be problematic because:

- Reporting average accuracy on each subset obscures whether some classes are completely misclassified.

- It's not necessarily true that "Few" classes perform worse than "Medium" or "Head". 

2. Proposing new metrics focused on improving the worst performing classes: harmonic mean of per-class recall and lowest recall (accuracy of worst class).

3. A novel method to optimize these metrics:

- A Geometric Mean Loss (GML) function that maximizes geometric mean of recalls as a surrogate for harmonic mean.

- Fine-tuning the classifier of any existing model with GML.

- An optional ensemble technique combining predictions of original and fine-tuned model.

4. Experiments on CIFAR and ImageNet based datasets showing the proposed method improves harmonic mean and lowest recall while maintaining overall accuracy, achieving state-of-the-art results.

5. Analysis and visualizations demonstrating the method produces a more balanced distribution of per-class recall values.

In summary, the key contribution is identifying issues with common long-tailed recognition evaluation, proposing better metrics, and presenting a simple fine-tuning method to optimize these metrics and improve recall of the worst classes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes a novel method to improve the worst-performing categories in long-tailed image recognition. The key idea is to optimize the harmonic mean of per-class recall rather than just the overall accuracy, using a surrogate loss called Geometric Mean Loss (GML) that maximizes the geometric mean. The method can be applied as a simple plug-in to existing models by retraining just the classifier, and further combines predictions from the original and retrained models. In summary, the paper aims to make sure no categories are left behind in long-tailed recognition.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in long-tailed recognition:

- Motivation: The paper argues that prior work in this field focuses too much on improving average accuracy across subsets or head vs. tail classes. Instead, it advocates focusing on the worst-performing classes and improving harmonic mean of per-class recall. This is a novel perspective compared to most existing work.

- Approach: The paper proposes a simple plug-in method that can be applied to many existing models through re-training the classifier. This makes it more flexible than methods that require end-to-end re-training or modification of the architecture. The proposed loss function GML is also novel for improving worst categories.

- Evaluation: The paper uses metrics like lowest recall, geometric mean, and harmonic mean of recall to emphasize performance on minority categories. This is more fine-grained than typical subset or head/tail accuracy. The visualizations of per-class recall distribution are also unique.

- Applicability: The paper shows consistent improvements by applying the method to various existing models like CE, BSCE, MiSLAS, PaCo on three benchmark datasets. This demonstrates the wide applicability of the approach.

- Simplicity: Compared to many recent methods involving multiple complex components, this paper's approach of re-training classifier and GML loss is relatively simple and easy to implement.

Overall, the novelty lies in its new perspective on focusing on worst categories rather than just head vs. tail, the flexibility of the approach, and the more fine-grained performance evaluation. The consistent gains demonstrate the idea's merit across diverse datasets and base models.
