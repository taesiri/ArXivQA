# [Rank-without-GPT: Building GPT-Independent Listwise Rerankers on   Open-Source Large Language Models](https://arxiv.org/abs/2312.02969)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper explores building effective listwise neural passage rerankers without relying on GPT models. Listwise rerankers directly predict the ranking of a list of passages based on relevance to a query in one pass. Prior work has depended on GPT models, creating a single point of failure. This work shows for the first time that a listwise reranker based on the open-source Code-LLaMA model can outperform rerankers based on GPT-3 and GPT-3.5, and achieve 97% effectiveness of GPT-4-based rerankers. However, experiments find that current IR training data designed for pointwise ranking is insufficient for training listwise models, indicating that higher quality listwise ranking data is crucial. Overall, this work advocates for more diverse solutions in building listwise rerankers beyond reliance on GPT models. It also calls for future work on constructing human-annotated datasets specifically for the purpose of listwise ranking. The results show listwise ranking is promising but may be bounded by current training data quality rather than model capacity.
