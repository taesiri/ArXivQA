# [VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking](https://arxiv.org/abs/2303.16727)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to scale video masked autoencoders (VideoMAE) to build powerful and general video foundation models. Specifically, the paper focuses on scaling VideoMAE in both model size (to billion-level parameters) and pre-training data (to million-level videos). The key hypothesis is that by properly addressing the challenges in scaling computation, data, and model adaptation, they can train the first billion-level video transformer with VideoMAE pre-training, and demonstrate its state-of-the-art performance and generalization ability on a variety of downstream video tasks.

Some key aspects of their investigation include:

- Proposing a dual masking strategy to improve training efficiency of large VideoMAE models.

- Building a large-scale diverse video dataset by mixing multiple sources for pre-training. 

- Studying progressive training schemes to adapt the billion-level pre-trained model to different downstream tasks.

By addressing these challenges, the paper shows VideoMAE is highly scalable and can produce powerful and generalizable video foundation models when trained at billion-scale with million-level unlabeled videos. The effectiveness is demonstrated by state-of-the-art results on various downstream tasks including classification, detection and temporal localization.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. They present an efficient framework VideoMAE V2 to scale video masked autoencoders to billion-level models on million-level unlabeled video data. 

2. They propose a dual masking strategy for VideoMAE that masks both the encoder and decoder to improve pre-training efficiency. This allows scaling to larger models and datasets.

3. They use a progressive training paradigm with an initial pre-training on diverse unlabeled data, followed by post-pre-training on a labeled dataset. 

4. They successfully train a video vision transformer (ViT) with over 1 billion parameters, which is the first billion-level video transformer.

5. Their billion-parameter VideoMAE V2 model achieves new state-of-the-art results on major video understanding benchmarks like Kinetics, Something-Something, AVA, THUMOS14 etc.

In summary, the key contribution is presenting an efficient and scalable framework to pre-train billion-parameter video masked autoencoders. The proposed techniques like dual masking and progressive training enable scaling to huge models and datasets, leading to SOTA results on various downstream tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper presents VideoMAE V2, a scaled up version of VideoMAE that uses a dual masking strategy and trains on large unlabeled and labeled video datasets to learn powerful billion-parameter video foundation models. The key results are state-of-the-art performance on multiple video understanding tasks including action recognition, detection and localization.
