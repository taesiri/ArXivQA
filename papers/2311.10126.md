# [I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of   Post-Training ViTs Quantization](https://arxiv.org/abs/2311.10126)

## Summarize the paper in one sentence.

 The paper introduces I&S-ViT, a novel post-training quantization method for Vision Transformers that addresses quantization inefficiency in the log2 quantizer and loss landscape challenges through a shift-uniform-log2 quantizer and smooth multi-stage optimization strategy.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

This paper proposes I&S-ViT, a novel post-training quantization method tailored for vision transformers (ViTs). It addresses two issues in PTQ of ViTs: quantization inefficiency of the log2 quantizer for post-Softmax activations, and rugged/magnified loss landscape from coarse quantization of post-LayerNorm activations. To tackle the first issue, it introduces a shift-uniform-log2 quantizer (SULQ) that shifts the input domain before log2 quantization to fully cover the range and accurately approximate distributions. For the second issue, it proposes a three-stage smooth optimization strategy (SOS) that starts with channel-wise quantization to enable stable learning on a smooth landscape, and transitions to efficient layer-wise quantization. Experiments on image classification, object detection and segmentation validate improvements from I&S-ViT, especially in very low precision cases. The method achieves significant gains over prior PTQ techniques for ViTs.
