# [Categorical Hopfield Networks](https://arxiv.org/abs/2201.02756)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main goals of this paper are:1) To introduce a categorical formulation of dynamical Hopfield networks, where resources are modeled as objects in symmetric monoidal categories and network states are modeled by summing functors that assign resources to network nodes and edges.2) To illustrate this categorical framework concretely in a simple example where resources are computational models (DNNs) assigned to nodes in a network. 3) To show how the categorical Hopfield dynamics in this example reproduce some known neural network behaviors like backpropagation and inhibitory-excitatory balance.More specifically, the paper introduces categorical Hopfield equations that govern how resource assignments evolve over time, with dynamics specified by endofunctors on the resource category. It then focuses on an example where resources are weighted IDAGs (a type of DNN architecture) and studies the resulting dynamics and fixed points. A key result is showing how backpropagation and gradient descent arise naturally from this categorical perspective.So in summary, the main goals are to propose a new categorical perspective on dynamical neural networks, illustrate it concretely in a DNN example, and recover some known neural behaviors in this framework. The overall hypothesis is that categorical models like this can provide a useful high-level perspective on network dynamics.


## What is the main contribution of this paper?

This paper introduces a categorical framework for modeling neural networks and their dynamics. The key contributions are:- It develops a categorical model of neuronal networks where resources are modeled as objects in a symmetric monoidal category. The dynamics is governed by "categorical Hopfield equations" based on summing functors that assign resources to network nodes.- It provides a concrete example of this framework using a category of deep neural networks (DNNs) as resources. The objects are weighted directed acyclic graphs representing DNN architectures. The Hopfield dynamics updates the weights using endofunctors that incorporate a threshold nonlinearity.- This categorical Hopfield system with DNN resources is analyzed. It is shown that solutions generically diverge, except at fixed points where an inhibitory-excitatory balance condition is violated. Backpropagation via gradient descent is identified as a special case of the dynamics.- The framework offers a new perspective on neural network dynamics using categorical constructs like resources, summing functors, operad/properad structures, and endofunctor evolution. The DNN example illustrates how complex neural architectures can be modeled categorically. Overall, it develops new mathematical tools for studying neural systems and their information processing properties.In summary, the main innovation is a categorical perspective on neural networks that enables new dynamical models incorporating key structural and computational features like resources, compositionality, and learning rules. The DNN example showcases the approach.
