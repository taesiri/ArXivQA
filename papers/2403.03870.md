# [Learning to Decode Collaboratively with Multiple Language Models](https://arxiv.org/abs/2403.03870)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Learning to Decode Collaboratively with Multiple Language Models":

Problem:
The paper explores how to enable multiple large language models (LLMs) to collaborate during text generation in order to produce higher quality text. Specifically, it aims to develop an unsupervised approach where a base LLM can learn when to generate text itself versus when to defer generation to assistant LLMs with different expertise. This allows the base model to take advantage of the knowledge and capabilities of larger, more specialized assistant models.

Method: 
The key idea is to model the choice of which LLM generates each token as a latent variable during decoding. The base LLM has a learned model selector that decides whether to generate the next token itself or defer to an assistant LLM based on the current context. By optimizing the marginal likelihood of text under this latent variable model, the base model learns good deferral decisions without direct supervision. At inference time, tokens are generated by interleaving the outputs of the base and assistant models coordinated by the learned model selector.

Main Contributions:
- Proposes a latent variable framework to enable unsupervised learning of when and which assistant LLM to invoke during collaborative decoding.

- Introduces a method called Co-LLM that allows a base LLM to learn to collaborate with larger, more specialized LLMs in a flexible, task-specific way without prescription on how to combine the models.

- Empirically demonstrates that Co-LLM improves performance over individual models on instruction following, mathematical reasoning, question answering and classification tasks. Shows it learns sensible collaboration strategies like template-filling.

- Analysis indicates optimal collaboration improves all models and Co-LLM degrades gracefully as deferral amount changes, enabling inference-time control.

Overall the method enables emergent, versatile collaboration across domains and scales to improve LLM performance.
