# [Extraneousness-Aware Imitation Learning](https://arxiv.org/abs/2210.01379v2)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: how can we enable agents to learn visuomotor policies from demonstrations that contain extraneous yet locally consistent subsequences? 

The key points are:

- Standard visual imitation learning methods assume near-optimal demonstrations, which are expensive or sometimes impossible to collect. Real-world demonstrations often contain extraneous yet locally consistent segments that are irrelevant to the main task.

- This paper proposes Extraneousness-Aware Imitation Learning (EIL), a self-supervised approach that allows agents to identify and exclude extraneous subsequences from demonstrations. 

- EIL learns action-conditioned observation embeddings using temporal cycle consistency loss. Similar states across demonstrations are encoded to similar embeddings. 

- It introduces an Unsupervised Voting-based Alignment (UVA) algorithm to retrieve task-relevant observations across demonstrations while excluding extraneous ones.

- Experiments on simulated and real-world robot control tasks show EIL outperforms strong baselines and achieves comparable performance to learning from perfect demonstrations.

In summary, the key hypothesis is that by learning action-conditioned embeddings and aligning observations across demonstrations in a self-supervised manner, agents can identify and imitate only the task-relevant parts of the demonstration while ignoring extraneous segments.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Extraneousness-Aware Imitation Learning (EIL), a framework that enables agents to identify and exclude extraneous (task-irrelevant yet locally consistent) subsequences when learning from visual demonstrations. Specifically, the key ideas are:

1. Learning action-conditioned observation embeddings via temporal cycle consistency loss. This allows matching frames with similar progress across demonstrations.

2. Proposing an Unsupervised Voting-based Alignment (UVA) algorithm to align demonstrations and filter out extraneous segments when no perfect reference is available. 

3. Evaluating EIL on simulated continuous control tasks as well as real robot tasks. Results show EIL outperforms baselines and achieves comparable performance to learning from perfect demonstrations.

In summary, the paper focuses on the novel problem setting of imitation learning from visual demonstrations containing extraneous yet locally consistent subsequences. It proposes the EIL framework to address this via self-supervised representation learning and unsupervised alignment. Experiments validate EIL's ability to identify and exclude extraneous segments for more effective imitation.
