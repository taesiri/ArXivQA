# Tractable Control for Autoregressive Language Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can tractable probabilistic models (TPMs) be used to impose lexical constraints in autoregressive language generation models? The key idea is to use TPMs, which allow efficient inference, to guide autoregressive language models like GPT-2/3 during text generation, so that desired lexical constraints are satisfied. Specifically, the paper proposes GeLaTo, a framework where:- A TPM (e.g. hidden Markov model) is trained to approximate the base autoregressive LM via maximum likelihood. - During text generation, the TPM provides token-level guidance by computing the probability Pr(x_{t+1} | x_{1:t}, α) of the next token efficiently, given the prefix x_{1:t} and lexical constraint α. - This guidance from the TPM is combined with the base LM's next-token distribution to generate the constrained text autoregressively.The central hypothesis is that by using TPMs that allow tractable inference given lexical constraints, the proposed GeLaTo framework can reliably control autoregressive LMs to generate high-quality texts satisfying complex constraints. Experiments on constrained text generation benchmarks demonstrate the effectiveness of this approach.


## What is the main contribution of this paper?

The key contributions of this paper are:- Proposing GeLaTo, a framework for generating language with complex lexical constraints using tractable probabilistic models (TPMs). - Demonstrating how hidden Markov models (HMMs), as an example of TPMs, can be efficiently conditioned on conjunctive normal form constraints to provide token-level guidance for autoregressive generation. An efficient dynamic programming algorithm is presented.- Achieving state-of-the-art results on challenging lexical constrained text generation benchmarks like CommonGen, while guaranteeing 100% constraint satisfaction.- Showing the potential of using TPMs to control large neural language models. The results motivate developing more expressive TPMs that can better approximate the distributions of large pre-trained LMs.To summarize, the main novelty is in using TPMs, which support efficient logical inference, to impose lexical constraints in neural autoregressive text generation models like GPT-2. This allows controlling text generation to satisfy complex constraints, overcoming the intractability of conditioning on such constraints in neural models directly. The proposed framework GeLaTo demonstrates strong empirical results on constrained text generation tasks.
