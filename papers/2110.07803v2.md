# [ContraQA: Question Answering under Contradicting Contexts](https://arxiv.org/abs/2110.07803v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How do question answering (QA) models behave under contradicting contexts that contain both real and fake information?

The key hypotheses appear to be:

1) Existing QA models are vulnerable to being misled by contradicting contexts brought about by misinformation, whether the misinformation is manually written by humans or automatically generated by models. 

2) Integrating a misinformation detector into the QA pipeline can help mitigate this vulnerability, but only if sufficient labeled training data is available.

3) Humans are better than current models at making subtle and efficient edits to texts to create contradicting fake contexts that fool both humans and QA models.

4) The proposed BART-FG model can create more deceiving contradicting fake contexts than off-the-shelf generative models like GPT-2, but still falls short of human-written fake contexts in terms of deception capability.

In summary, the central focus seems to be analyzing how QA models behave under contradicting contexts containing misinformation, revealing their vulnerabilities, and proposing methods to make them more robust. The key novelty is constructing a new dataset to enable this analysis.
