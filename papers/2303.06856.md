# [Dynamic Neural Network for Multi-Task Learning Searching across Diverse   Network Topologies](https://arxiv.org/abs/2303.06856)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question/hypothesis of this paper is:How to design an MTL framework that can search for optimized network structures tailored to each individual task across diverse graph topologies in a single network? The paper proposes a new MTL framework to address two key challenges:1) Searching across diverse graph topologies: The paper applies DAG structure to allow searching across diverse topologies. However, DAG-based search space is too large for MTL. 2) Constructing task-specific sub-networks: Existing MTL methods using DNN or NAS cannot find optimal structures for each task. They use simple/restricted topologies which may degrade performance in heterogeneous MTL scenarios.To address these challenges, the paper proposes:- A restricted DAG-based central network with read-in/out layers to enable topology search while limiting search space. - A training procedure and algorithms to optimize the central network into compact task-specific sub-networks.The central hypothesis is that by searching across broader graph topologies and optimizing task-specific structures, the proposed MTL framework can achieve better performance compared to prior MTL methods. Experiments on benchmark datasets verify this hypothesis.In summary, the core research question is how to design an MTL framework that can search across diverse topologies to find optimized task-specific structures in a scalable way. The paper aims to address the limitations of prior MTL methods in this aspect.


## What is the main contribution of this paper?

The main contributions of this paper can be summarized as:- It presents the first MTL framework that searches for both task-adaptive structures and sharing patterns among tasks. The proposed method achieves state-of-the-art performance on all evaluated public MTL datasets.- It proposes a new DAG-based central network composed of a flow restriction scheme and read-in/out layers. This allows searching across diverse graph topologies in a reasonably restricted search space. - It introduces a new training procedure that optimizes the MTL framework to compactly construct various task-specific sub-networks in a single network.- Key components proposed include:  - Flow restriction to reduce search space while maintaining topology diversity  - Read-in/out layers to enable polytree structures and early exiting  - Three stage training procedure with squeeze loss and flow-based reduction   - Flow-based reduction algorithm to prune unimportant connectionsIn summary, the main contribution is a novel MTL framework that can search for optimal task-specific structures across diverse topologies in an efficient manner, outperforming prior state-of-the-art methods. The proposed components help restrict the search space, construct compact networks, and achieve strong performance across MTL benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents a new multi-task learning framework that searches for optimized task-specific neural network structures across diverse topologies in a single network, achieving state-of-the-art performance by dynamically controlling feature sharing.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in multi-task learning:- It proposes a new framework for searching optimal network structures tailored to each task, rather than using a shared network or fixed branching/selection of modules like other works. This allows more flexibility and potential to handle heterogeneous tasks.- It uses a DAG-based architecture search space, which is common in NAS but novel for MTL. The flow restriction scheme makes this feasible for MTL while still allowing diverse topologies.- Most prior MTL NAS methods only search over fixed cells or blocks. This searches broader network-level architectures.- It introduces a new training process with warm-up, search, and fine-tuning stages. The proposed squeeze loss and flow-based reduction algorithm help create compact final task networks.- Experiments show state-of-the-art results on common MTL benchmarks compared to both static and other NAS-based methods. The analyses provide insights into task relationships and resource needs.- Overall, it pushes MTL NAS to search more flexible architectures instead of just module selection/connectivity patterns. The innovations in search space design, training process, and network optimization enable this advancement for improved MTL performance.In summary, the key novelties are in using DAG search for MTL, the broader graph topology search, and associated techniques to make this feasible and effective. The results demonstrate improved performance and ability to handle heterogeneous tasks versus prior works.
