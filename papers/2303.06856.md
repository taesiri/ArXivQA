# [Dynamic Neural Network for Multi-Task Learning Searching across Diverse   Network Topologies](https://arxiv.org/abs/2303.06856)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question/hypothesis of this paper is:How to design an MTL framework that can search for optimized network structures tailored to each individual task across diverse graph topologies in a single network? The paper proposes a new MTL framework to address two key challenges:1) Searching across diverse graph topologies: The paper applies DAG structure to allow searching across diverse topologies. However, DAG-based search space is too large for MTL. 2) Constructing task-specific sub-networks: Existing MTL methods using DNN or NAS cannot find optimal structures for each task. They use simple/restricted topologies which may degrade performance in heterogeneous MTL scenarios.To address these challenges, the paper proposes:- A restricted DAG-based central network with read-in/out layers to enable topology search while limiting search space. - A training procedure and algorithms to optimize the central network into compact task-specific sub-networks.The central hypothesis is that by searching across broader graph topologies and optimizing task-specific structures, the proposed MTL framework can achieve better performance compared to prior MTL methods. Experiments on benchmark datasets verify this hypothesis.In summary, the core research question is how to design an MTL framework that can search across diverse topologies to find optimized task-specific structures in a scalable way. The paper aims to address the limitations of prior MTL methods in this aspect.


## What is the main contribution of this paper?

The main contributions of this paper can be summarized as:- It presents the first MTL framework that searches for both task-adaptive structures and sharing patterns among tasks. The proposed method achieves state-of-the-art performance on all evaluated public MTL datasets.- It proposes a new DAG-based central network composed of a flow restriction scheme and read-in/out layers. This allows searching across diverse graph topologies in a reasonably restricted search space. - It introduces a new training procedure that optimizes the MTL framework to compactly construct various task-specific sub-networks in a single network.- Key components proposed include:  - Flow restriction to reduce search space while maintaining topology diversity  - Read-in/out layers to enable polytree structures and early exiting  - Three stage training procedure with squeeze loss and flow-based reduction   - Flow-based reduction algorithm to prune unimportant connectionsIn summary, the main contribution is a novel MTL framework that can search for optimal task-specific structures across diverse topologies in an efficient manner, outperforming prior state-of-the-art methods. The proposed components help restrict the search space, construct compact networks, and achieve strong performance across MTL benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents a new multi-task learning framework that searches for optimized task-specific neural network structures across diverse topologies in a single network, achieving state-of-the-art performance by dynamically controlling feature sharing.
