# [EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation](https://arxiv.org/abs/2402.01422)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Generating emotional talking faces from a single image is challenging. Existing methods either require videos for training or lack fine-grained control over emotion categories and intensities. They are unable to generate unseen emotional intensities beyond the dataset's labels. 

Proposed Solution:
This paper proposes EmoSpeaker, a novel one-shot fine-grained emotion-controlled talking face generation method. The key ideas are:

1) Visual Attribute-Guided Audio Decoupler: Uses facial action units from videos as supervision to decouple emotion features from content features in the audio spectrogram. This extracts pure content features for accurate lip motion control.

2) Fine-Grained Emotion Control: Manually specifies emotion categories and intensities. Varies sliding window sizes during inference to achieve unseen emotional intensities. 

3) Emotion Face Renderer: Predicts 3DMM coefficients from content and emotion features to render emotive talking faces on the reference image via a video generation network.

Main Contributions:

1) Audio emotion decoupling with facial visual attribute guidance for better lip synchronization

2) Fine-grained emotion intensity matrix with adjustable windows for unseen emotion control 

3) One-shot talking face generation with precise lip motion and adjustable emotion categories/intensities

Experiments show state-of-the-art video quality, facial expressions and lip sync accuracy. Both seen and unseen emotional intensities can be reliably generated in a one-shot manner. The method has promising applications in digital humans, VR and movie special effects.
