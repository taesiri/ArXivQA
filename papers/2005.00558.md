# POINTER: Constrained Progressive Text Generation via Insertion-based   Generative Pre-training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is: How can we develop an effective model for constrained text generation that is able to incorporate given lexical constraints and generate coherent, fluent, and informative natural language text?The paper proposes a model called POINTER that aims to address the challenges of hard-constrained text generation, where the model must generate text containing specific required words or phrases. The key ideas proposed to address this research problem are:1) A progressive, multi-stage generative process that inserts words in order of importance, from high-level concepts to details.2) Leveraging pre-trained BERT to initialize the model parameters.3) Large-scale pre-training on Wikipedia text to create a versatile generative model. 4) A parallel, non-autoregressive generation approach to improve efficiency.5) Customized beam search to improve the coordination of the parallel generation.Through experiments on news and Yelp review datasets, the paper shows that POINTER outperforms previous models on constrained text generation by producing more fluent, coherent, and lexically diverse text. The human evaluation also indicates POINTER generates text that is generally preferred over baseline models.In summary, the key research contribution is developing a simple but effective approach for constrained text generation using insertion-based pre-training and progressive generation. The model is shown to improve over previous methods for this challenging generative task.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a novel non-autoregressive model called PTG (Progressive Text Generation) for hard-constrained text generation. Specifically, the key contributions are:1. PTG generates text in a progressive and iterative manner, starting from lexical constraints. It first generates high-level words that bridge the constraints, then uses them as pivots to insert finer details. This coarse-to-fine hierarchy allows long-term control over generation.2. PTG allows initializing from large pre-trained language models like BERT via its masked language modeling-like objective. The authors also pre-train PTG on a large Wikipedia corpus for better generalizability.3. PTG uses parallel non-autoregressive decoding, achieving logarithmic time complexity during inference, unlike previous linear-time methods. They also propose an inner-layer beam search to improve the decoding.4. Extensive experiments on News and Yelp datasets show PTG outperforming previous constrained text generation methods on relevance, fluency and diversity. The human evaluation also prefers PTG outputs.In summary, the main contribution is proposing the progressive text generation framework PTG that enables efficient non-autoregressive decoding while allowing better control over lexically constrained text generation through pre-training and hierarchical coarse-to-fine generation. The overall approach is simple, interpretable and performs well empirically.
