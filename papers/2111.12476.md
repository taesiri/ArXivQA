# [Hierarchical Modular Network for Video Captioning](https://arxiv.org/abs/2111.12476)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is how to learn effective video representations for video captioning that capture linguistic semantics at different levels. Specifically, the paper proposes a hierarchical modular network to learn video representations that associate with linguistic semantics from three levels:1) Entity level - Highlighting principal objects likely to be mentioned in the caption, supervised by entities in the caption.2) Predicate level - Learning actions conditioned on principal objects, supervised by the predicate (verb + noun phrase) in the caption. 3) Sentence level - Learning a global representation of the full video content, supervised by the embedding of the whole caption.The key hypothesis is that learning video representations associated with linguistic semantics at different granularities (entity, predicate, sentence) will allow generating more accurate and semantically relevant captions compared to prior work. The experiments aim to validate whether this hierarchical supervision approach leads to improved video captioning performance.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a hierarchical modular network for video captioning that bridges video representations and linguistic semantics at three levels: entity, predicate, and sentence. - It introduces a novel entity module implemented with a transformer encoder-decoder architecture to highlight the principal objects in a video that are most likely to be mentioned in the caption. - The entity, predicate, and sentence modules are each supervised by the corresponding linguistic components (entities, predicate, full sentence) extracted from the ground truth captions. - Extensive experiments show the proposed method achieves state-of-the-art performance on the MSVD and MSR-VTT benchmarks for video captioning.In summary, the key contribution is the hierarchical modular framework that establishes multi-level connections between visual representations and linguistic semantics to generate more accurate video captions. The entity module for selecting principal objects is also a novel component.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a hierarchical modular network for video captioning that bridges video representations and linguistic semantics at three levels - entity, predicate, and sentence - by designing modules linked to linguistic counterparts to learn multi-granularity video representations for generating more accurate captions.
