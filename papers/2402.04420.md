# [Measuring machine learning harms from stereotypes: requires   understanding who is being harmed by which errors in what ways](https://arxiv.org/abs/2402.04420)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine learning models can learn and amplify stereotypical associations (e.g. associating women with domestic roles), leading to errors that may cause harm. 
- However, current fairness metrics in ML often make assumptions about which errors reflect stereotypes and what constitutes harm without empirical evidence.

Proposed Solution:
- Use social psychology theories and survey experiments to understand which machine learning errors people perceive as stereotypical, and actually cause harm.
- Distinguish between stereotype-reinforcing, violating and neutral errors. Also distinguish between pragmatic harms (changes in beliefs/attitudes/behaviors) and experiential harms (negative emotions).   
- Conduct studies exposing people to different error types and measure resulting harms.

Key Findings:
- People do not agree on whether particular objects reflect gender stereotypes. Reasons given for stereotypical associations are varied.
- Stereotype-reinforcing errors cause more experiential harm, especially for the stereotyped group. However, little immediate pragmatic harm observed.  
- Stereotype-violating errors can also cause experiential harm due to phenomena like precarious manhood and transphobia. Harm is unequally distributed.

Main Contributions:  
- Empirically demonstrated that not all errors equally invoke stereotypes or cause harm.
- Showed experiential harm results from exposing people to stereotypical machine learning errors.
- Highlighted complexity that stereotype-violating errors cause harm too, questioning assumptions in ML fairness.
- Argued importance of measuring actual psychological harm over making assumptions when evaluating fairness.

The paper makes an important contribution in grounding machine learning fairness interventions in social contexts and actual harm experienced by people. The findings illustrate the need for nuanced perspectives in assessing and mitigating algorithmic harms.
