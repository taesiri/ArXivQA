# [SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.11896)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) suffer from the issue of over-smoothing, where token representations become increasingly similar in deeper layers. This limits model performance.
- Full fine-tuning of LLMs is computationally expensive. Recent parameter-efficient fine-tuning (PEFT) methods like adapters and LoRA allow updating only a small subset of parameters, but they still face the over-smoothing problem.

Proposed Solution:
- The paper proposes a simple, plug-and-play framework called SIBO to alleviate over-smoothing for PEFT methods. 
- The key idea is to inject an "initial residual" from the input token representation into the PEFT modules. This preserves some uniqueness of tokens.
- SIBO is applied to adapter and LoRA tuning. It only requires adding a residual connection and tuning one extra hyperparameter.

Contributions:
- SIBO significantly enhances performance over vanilla adapter/LoRA tuning across diverse tasks. On arithmetic reasoning tasks, gains are up to 15.7% over adapter and 13.6% over LoRA.
- On commonsense reasoning datasets, SIBO leads to gains of up to 7.6% over adapter and 23.5% over LoRA.
- SIBO has marginal overhead and complexity compared to baseline PEFT techniques.
- Analysis shows SIBO reduces token similarity, mitigating over-smoothing. Case studies demonstrate it improves reasoning and word discrimination.

In summary, the paper presents SIBO, a simple yet effective plugin to improve existing PEFT methods by alleviating the issue of over-smoothing in LLMs. With negligible overhead, SIBO leads to notable performance gains across multiple benchmarks.
