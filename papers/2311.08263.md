# [Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads   to Answers Faster](https://arxiv.org/abs/2311.08263)

## Summarize the paper in one sentence.

 The paper proposes FastCoT, a model-agnostic framework that uses parallel decoding to provide approximate future tokens as a glimpse to large language models, accelerating reasoning tasks like Chain-of-Thought prompting while maintaining performance.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes FastCoT, a model-agnostic framework to accelerate Chain-of-Thought reasoning tasks using large language models. FastCoT utilizes parallel decoding to provide the language model a quick "glimpse" of approximate future tokens alongside exact tokens from autoregressive decoding. This allows the model to analyze answers faster without needing to fully decode rationales. Experiments show FastCoT can provide up to 20% speedup in inference time with minimal performance drop on commonsense reasoning datasets like CSQA and StrategyQA. The framework supports batch processing and KV-cache to further enhance efficiency. Overall, this is one of the first works to introduce acceleration in CoT reasoning by providing the model approximate glimpses into the future through parallel decoding. It paves the way for future research on speeding up reasoning tasks.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the paper:

This paper proposes FastCoT, a novel method to accelerate the Chain of Thought (CoT) reasoning task using large language models. The key idea is to utilize parallel decoding to provide the model with approximate future tokens as a "glance" into the future. Specifically, FastCoT performs autoregressive decoding to generate exact tokens, while also using a parallel decoding context window to reveal approximate future tokens. These approximate tokens serve as an inner reasoning chain that guides the model to the answer faster compared to just autoregressive decoding. Experiments across multiple datasets demonstrate FastCoT achieves inference speedups up to 20% with minimal performance loss. The method includes optimizations like batch processing and caching to further enhance efficiency. A detailed analysis is provided on the quality of the approximate tokens, showing they capture important keywords for commonsense reasoning. Overall, this is one of the first works to introduce speed improvements for CoT reasoning using the concept of approximate rationale glimpses from parallel decoding. The proposed techniques provide model-agnostic acceleration while retaining strong performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes FastCoT, a model-agnostic framework that utilizes parallel decoding to provide the language model with approximate tokens as a glimpse of the future. This allows FastCoT to reduce the time overhead of autoregressive decoding for CoT reasoning tasks by up to 20%, with minimal impact on performance.


## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to accelerate the inference speed of reasoning tasks like the Chain of Thought (CoT) prompting method using large language models (LLMs). 

Specifically, the key questions and hypotheses explored in this paper are:

- Can providing the LLM with a quick "glimpse" of approximate future tokens generated through parallel decoding lead to faster answers for reasoning tasks compared to regular autoregressive decoding? 

- Will using these approximate tokens from parallel decoding as part of the input help the LLM analyze and generate the final answer faster without having to autoregressively decode the full rationale?

- Can a model-agnostic framework be designed to leverage parallel decoding to reduce the time overhead of autoregressive decoding for CoT reasoning tasks by up to 20% with minimal impact on performance?

The core hypothesis is that providing LLMs with approximate future tokens as a "glimpse" can accelerate the inference speed for CoT reasoning tasks by enabling the model to analyze and generate the final answer faster, without having to decode the complete rationale through slower autoregressive methods. The paper aims to demonstrate this through empirical experiments across different datasets and transformer sizes.
