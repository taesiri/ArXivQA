# Language Models are Multilingual Chain-of-Thought Reasoners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How well do large language models perform on tasks requiring complex reasoning in multilingual settings?More specifically, the key aspects investigated in this paper are:1. Introducing a new multilingual benchmark, MGSM, for evaluating arithmetic reasoning of language models across diverse languages. 2. Analyzing the ability of large models like GPT-3 and PaLM to perform multi-step reasoning and solve MGSM problems when provided with chain-of-thought prompting in various languages.3. Demonstrating that these models exhibit strong cross-lingual transfer and can solve reasoning problems even in low-resource languages not well represented in their training data.4. Extending the multilingual reasoning evaluation to other tasks like commonsense reasoning (XCOPA) and word sense disambiguation (XL-WiC) and showing competitive performance.5. Overall, the paper aims to comprehensively evaluate and highlight the emerging cross-lingual reasoning capabilities of large language models when presented with explicit reasoning chains, even with little or no training data in the target languages. The introduction of MGSM and the analysis help demonstrate these abilities.In summary, the core hypothesis is that large language models have acquired strong latent skills in multilingual reasoning that can be revealed through chain-of-thought prompting, even in low-resource languages. The empirical evaluations and new benchmark aim to demonstrate that.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing the Multilingual Grade School Math (MGSM) benchmark, which is the first multilingual arithmetic reasoning benchmark. This is created by manually translating a subset of the English GSM8K dataset into 10 diverse languages. 2. Comprehensively evaluating the multilingual reasoning abilities of large language models like GPT-3 and PaLM on multiple tasks, including the new MGSM benchmark. Key findings are:- Models show surprisingly good reasoning abilities even in underrepresented languages. For PaLM-540B, accuracy on MGSM is only 3% lower for underrepresented vs high-resource languages. - Intermediate reasoning steps in English lead to competitive or better performance than reasoning in the native language across settings.- Multilingual reasoning is an emergent property of scale - it substantially improves from 62B to 540B parameters for PaLM.3. Demonstrating that multilingual reasoning abilities extend beyond arithmetic to commonsense reasoning (on XCOPA dataset) and word sense disambiguation (XL-WiC dataset). Using few-shot prompting, PaLM-540B achieves new SOTA results on XCOPA, outperforming prior approaches requiring full training.In summary, the key contributions are introducing a new multilingual reasoning benchmark, and providing an extensive evaluation showing that large language models exhibit surprisingly strong multilingual reasoning abilities, which emerge with scale and transfer across tasks. The results suggest models may have some inherent cross-lingual reasoning capacities beyond what is directly observed during pretraining.
