# [Learning Interpretable Policies in Hindsight-Observable POMDPs through   Partially Supervised Reinforcement Learning](https://arxiv.org/abs/2402.09290)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Many real-world environments like robotics and autonomous driving are partially observable Markov decision processes (POMDPs). In POMDPs, the agent receives high-dimensional observations like images, but the true low-dimensional state containing key semantic information is hidden.
- Typical deep reinforcement learning methods treat observations as states and learn end-to-end policies directly from observations. This lacks interpretability as the mapping from observations to actions is complex. 
- Some methods leverage the true state available during training, but have limitations - they target specific RL algorithms like DQN or actor-critic, don't fully utilize state information, or learn policies still mapping raw observations to actions.

Proposed Solution:
- The paper proposes the Partially Supervised Reinforcement Learning (PSRL) framework that fuses supervised and reinforcement learning. 
- PSRL has two components - a state predictor network that predicts state from observations, and a policy network that maps predicted state to actions. These are composed at test time.
- During training, the state predictor is trained with supervision using true states and the policy is trained with RL, by taking predicted instead of true states as input.
- A more general framework PSRL-K also allows learning a latent representation along with predicted state for the policy. This balances interpretability and performance.

Key Contributions:
- First framework to systematically combine supervised state prediction and RL policy learning in POMDPs by leveraging state observations at training time.
- Unified approach applicable to both DQN and actor-critic RL methods. Modulating between supervised and unsupervised learning via PSRL-K.
- Policies explicitly compose state prediction and control for interpretability. Experiments show PSRL policies outperform baselines in terms of rewards and sample efficiency across many OpenAI Gym environments.
- Analysis reveals state predictions in PSRL are highly accurate unlike uninterpretable latent states learned by end-to-end methods.

In summary, the paper proposes PSRL as an effective and interpretable reinforcement learning approach for POMDPs by fusing supervised and unsupervised learning in a principled manner.
