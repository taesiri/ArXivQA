# Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does chain-of-thought prompting affect the performance of large language models on challenging reasoning tasks from the BIG-Bench benchmark?The key hypotheses appear to be:1) Chain-of-thought (CoT) prompting will improve the performance of large language models on challenging BIG-Bench tasks compared to standard few-shot prompting.2) CoT prompting will reveal emergent reasoning abilities in large language models that are not apparent with standard prompting. 3) The benefits of CoT prompting will require sufficiently large model scale (it will not help smaller models).The authors evaluate these hypotheses by curating a subset of BIG-Bench tasks that are particularly challenging for current LMs, prompting them with CoT exemplars, and measuring the resulting performance across models of varying scale. The goal is to probe the limitations of existing models through CoT prompting and determine if reasoning abilities emerge with scale when CoT is used.In summary, the central research question is whether CoT prompting can unlock stronger reasoning and task performance in large language models on complex, multi-step BIG-Bench tasks. The hypotheses focus on CoT prompting surpassing standard few-shot prompting, revealing emergent abilities, and benefiting larger models more.


## What is the main contribution of this paper?

This paper compares the performance of large language models on a set of challenging reasoning tasks from BIG-Bench using different prompting strategies. The main contributions are:1. Identification of a subset of 23 particularly challenging BIG-Bench tasks called BIG-Bench Hard (BBH) where prior language models fall short of average human-rater performance.2. Evaluation of standard "answer-only" prompting versus chain-of-thought (CoT) prompting on the BBH tasks using several language models (PaLM, InstructGPT, Codex). 3. Demonstration that CoT prompting substantially improves performance over answer-only prompting on BBH, with the Codex model surpassing average human-rater on 17 out of 23 tasks using CoT.4. Analysis showing CoT prompting enables emergent task performance on several BBH tasks that otherwise exhibit flat scaling curves. 5. Release of the BBH subset, prompts, and model outputs to facilitate further research on challenging reasoning tasks.In summary, the key contribution is using CoT prompting to show stronger reasoning capabilities of large language models on tasks in BIG-Bench Hard compared to prior answer-only prompting evaluations. The results suggest scaling models with CoT prompting unlocks improved performance on challenging reasoning tasks.
