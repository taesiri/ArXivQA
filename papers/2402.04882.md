# [LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre   Memory Units](https://arxiv.org/abs/2402.04882)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Transformers have achieved state-of-the-art performance on many sequence learning tasks but have high complexity ($O(N^2)$) during inference and cannot process streaming data, limiting their use for low-latency edge applications. In contrast, RNNs like LSTMs can process sequences sequentially but have worse performance and difficult parallel training. 

Proposed Solution:
The paper proposes a novel model called LMUFormer which combines a Legendre Memory Unit (LMU) with convolutional patch embedding and channel mixing layers inspired by transformers. The LMU allows parallel training while retaining sequential processing capability. Additional transformer components provide higher representational capacity to push LMU performance closer to transformers while retaining sequential inference.

The authors also propose a spiking version called Spiking LMUFormer which converts parts of the model to spiking neural network (SNN) layers. This further reduces computational complexity and power consumption while maintaining accuracy.

Main Contributions:

- LMUFormer outperforms prior RNN models while using 53x fewer parameters and 65x fewer FLOPs than transformer models with similar accuracy on Speech Commands dataset

- Spiking LMUFormer sets new state-of-the-art for SNNs on Speech Commands with 96.12% accuracy while needing only 30 million synaptic operations  

- Thanks to sequential processing capability, LMUFormer achieves 99% of peak accuracy on Speech Commands while processing only 68% of the input sequence duration

- First SNN model to demonstrate competitive performance on the Long Range Arena benchmark, outperforming several transformer variants

In summary, the paper presents a novel neural architecture that achieves excellent accuracy-efficiency trade-offs on streaming sequence tasks compared to both transformers and prior RNN models. The spiking version further pushes the efficiency envelope while retaining accuracy.
