# Contrastive Learning with Adversarial Perturbations for Conditional Text   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we mitigate the "exposure bias" problem in sequence-to-sequence (seq2seq) models for conditional text generation, where models are trained only with teacher forcing using ground truth labels and not exposed to incorrectly generated tokens during training?The key hypothesis proposed is that contrasting positive input-output pairs with negative pairs, to expose the model to both valid and incorrect variations of the inputs, can improve the generalization performance of seq2seq models. Specifically, the paper proposes a principled method called CLAPS (Contrastive Learning with Adversarial Perturbations for Seq2seq) to automatically generate "hard" positive and negative pairs to guide the model to better distinguish correct vs incorrect outputs. The negative pairs are generated by adding small perturbations to minimize conditional likelihood, while positive pairs are generated with larger perturbations to be far from the input embedding while preserving high likelihood.The central hypothesis is that training seq2seq models with such automatically constructed difficult positive and negative pairs within a contrastive learning framework can mitigate exposure bias and improve generalization on unseen inputs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a contrastive learning framework with adversarial perturbations to improve generalization for conditional text generation tasks like machine translation, text summarization, and question generation. Specifically, the key ideas are:- Using contrastive learning to train the seq2seq model by maximizing similarity between positive pairs (input and target text) while minimizing similarity to negative pairs. This exposes the model to various valid and incorrect outputs during training.- Generating "hard" positive and negative examples via adversarial perturbations, rather than using random non-target texts as negative examples. The adversarial examples are more difficult for the model to discriminate.- Negative examples are generated by adding small perturbations to minimize the conditional likelihood of the target text. - Positive examples are generated by adding larger perturbations to maximize distance from the input while keeping conditional likelihood high.- The adversarial positive and negative pairs guide the model to better distinguish correct vs incorrect outputs, improving generalization.- Empirically showing the method improves performance on machine translation, summarization, and question generation over baselines.So in summary, the key contribution is using principled adversarial perturbations to construct better positive/negative pairs for contrastive learning, which improves generalization for seq2seq text generation models. The gains are demonstrated on multiple text gen tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:This paper proposes a contrastive learning framework called CLAPS that generates adversarial positive and negative examples to train sequence-to-sequence models, improving their generalization by exposing them to varied valid and incorrect outputs during training.
