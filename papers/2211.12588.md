# Program of Thoughts Prompting: Disentangling Computation from Reasoning   for Numerical Reasoning Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we disentangle computation from reasoning when teaching language models to solve complex numerical reasoning tasks?In particular, the paper proposes an approach called "Program of Thoughts" (PoT) that allows language models to generate programs expressing reasoning steps, while delegating the actual computation to an interpreter. This allows decoupling complex computation from language understanding and reasoning. The key hypothesis is that by separating out the computation component, PoT will be more effective at eliciting language models' reasoning capabilities compared to having the model perform both reasoning and computation (as in the Chain of Thoughts or CoT approach).The paper evaluates this hypothesis by testing PoT on several math word problem and financial QA datasets. The main findings are that PoT outperforms CoT significantly, with average gains of around 12% across the datasets. This supports the hypothesis that disentangling reasoning from computation leads to better reasoning performance from language models.In summary, the core research question is how to elicit better reasoning from language models by separating out the computation component. PoT is proposed as a method to test this hypothesis, and the experimental results demonstrate its effectiveness versus an integrated reasoning+computation approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing "Program of Thoughts" (PoT) prompting, which uses language models to generate programs that express reasoning steps for solving numerical reasoning tasks. The key ideas are:- PoT generates programs instead of just text explanations. This allows delegating complex computations to an interpreter, decoupling computation from reasoning. - PoT aims to elicit "thoughtful" programs from language models by using semantic variable names and breaking reasoning into multiple steps.- PoT is evaluated on a diverse set of math word problem and financial QA datasets. It significantly outperforms prior work like Chain of Thoughts prompting across both few-shot and zero-shot settings.- PoT combined with self-consistency decoding achieves new state-of-the-art results on several benchmark math reasoning datasets.In summary, the main contribution is presenting PoT as an effective method for numerical reasoning that disentangles reasoning and computation. By generating "programs of thoughts", PoT takes better advantage of language models' reasoning capabilities while outsourcing complex computations. The broad experiments demonstrate the general applicability and strong performance of PoT.
