# [Sparse is Enough in Fine-tuning Pre-trained Large Language Model](https://arxiv.org/abs/2312.11875)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning large pre-trained language models (LLMs) like GPT-3 is very expensive. So methods are needed to efficiently adapt LLMs to downstream tasks.
- Several parameter-efficient fine-tuning (PEFT) methods exist, like adapters and LoRA, but their underlying principles are unclear. 

Main Contributions:

1) Reveals gradient properties of LLMs on downstream tasks:
- Visualizes transition of loss landscape from random init to pre-trained init. Finds it goes from low to high amplitude oscillations.
- Shows LLM gradients exhibit a "quasi-sparse" property - small fraction of components dominate total gradient norm. This leads to low intrinsic dimension.

2) Proposes Sparse Increment Fine-Tuning (SIFT):
- Updates only components with top x% absolute gradient values each batch. Ensures sufficient descent direction.
- More streamlined than PEFT methods like adapters/LoRA which inject small plug-ins.
- Also proposes memory-efficient implementation using hooks.

3) Validates SIFT effectiveness on GLUE and instruction tuning tasks:
- Gets competitive performance as other PEFT methods on GLUE.  
- Also works well for instruction tuning larger LLMs like 7B parameter Llama.

Overall, reveals gradient sparsity principle in LLMs that enables efficient fine-tuning. SIFT is simpler and more streamlined than other PEFT methods by just updating top gradient components. Memory-efficient implementation also enables scaling up.
