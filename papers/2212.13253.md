# [DSI2I: Dense Style for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2212.13253)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to represent and transfer style in a fine-grained, spatially-varying manner for exemplar-based image-to-image translation without paired data or semantic supervision during training. 

The key hypotheses are:

1) Modeling style densely as a spatial map instead of a global feature vector will allow for finer control over stylistic attributes and better preservation of semantics during translation. 

2) Adversarial and perceptual losses can encourage disentanglement of dense style and content representations without supervision.

3) Semantic correspondence between source and exemplar images can be used to spatially warp the dense style map to match the layout of the source image content.

4) Evaluating stylistic similarity in a localized, class-wise manner better captures the benefits of dense style modeling compared to global image-level metrics.

The overall goal is to develop an unpaired, unsupervised image translation method that can adopt the fine-grained style of a target exemplar image while preserving source content, without relying on semantic labels or ground truth pairs during training. The central hypothesis is that dense style modeling along with the other proposed techniques can achieve this effectively.
