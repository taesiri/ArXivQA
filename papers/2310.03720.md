# [HeaP: Hierarchical Policies for Web Actions using LLMs](https://arxiv.org/abs/2310.03720)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can large language models (LLMs) be taught to perform complex web tasks through hierarchical prompting and few-shot learning?More specifically, the key research questions seem to be:- How to decompose complex web tasks into modular sub-tasks that can each be solved by a specialized policy? - How to learn a shared "grammar" of reusable low-level policies from few demonstrations that can generalize across web tasks and interfaces?- Can an LLM task planner harness these modular policies to plan and execute new web tasks?The central hypothesis seems to be that by learning a hierarchical library of prompt-based policies from few examples, an LLM can generalize to perform new web tasks by planning and composing these modular building blocks. Rather than training task-specific models, this approach aims to acquire a grammar of skills that adapts to new tasks and websites.The key ideas are:- Decomposing web tasks hierarchically into high-level planning and low-level policies- Learning prompt-based policies from few examples that generalize across web interfaces - Task planning via LLM prompts to compose policies for new tasks- Achieving better generalization and sample efficiency compared to supervised approachesIn summary, the paper focuses on using hierarchical prompting and few-shot learning to teach LLMs to perform complex web tasks by acquiring a reusable library of skills.


## What is the main contribution of this paper?

This paper presents a new framework called \method (Hierarchical Policies for Web Actions using LLMs) for training large language models (LLMs) to perform complex web tasks with minimal training data. The key ideas are:- Decompose web tasks into modular sub-tasks that can each be solved by simple, reusable policies (e.g. fill text, choose date, etc). These policies act as a "shared grammar" across tasks.- Learn prompts for a high-level task planner and low-level policies from just a few demonstrations. The task planner invokes policies sequentially to complete the overall task. - Use hierarchical prompting to efficiently leverage large LLMs, avoiding the need to fine-tune models per task.The main contributions are:1) A novel hierarchical framework that enables generalization to new web tasks from a handful of examples by re-using modular policies.2) Experimental results on tasks like MiniWoB++, WebArena, live websites and a new airline CRM simulator showing improved performance over prior methods while using orders of magnitude less training data.3) Analysis indicating hierarchical prompting is more sample-efficient by allowing more focused examples per sub-task, compared to flat prompting.Overall, the key insight is to leverage LLMs to decompose tasks into reusable skills, avoiding the need for exhaustive training data covering all possible tasks. The hierarchical prompting approach enables rapid adaptation to new web tasks and interfaces.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel framework called HeaP that uses hierarchical prompting of large language models to decompose complex web tasks into modular policies, enabling generalization across tasks and websites from few demonstrations.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on using large language models (LLMs) for web tasks:- It proposes a novel hierarchical framework HeaP that decomposes complex web tasks into reusable low-level policies. This is different from prior works that train end-to-end models for specific web tasks. The modularity makes HeaP more generalizable across tasks and websites. - The method is significantly more sample efficient than prior approaches. HeaP matches state-of-the-art performance on MiniWoB++ using orders of magnitude less training data (21 examples vs millions of examples in prior works). This enables rapidly adapting the framework to new tasks with limited demonstrations.- The framework uses a hierarchical prompting approach to leverage LLMs, unlike most prior works that fine-tune LLMs for web tasks. Prompting is a lighter-weight approach compared to fine-tuning, making the system easier to update and maintain.- It evaluates the framework on a wider range of environments compared to prior works - benchmarks like MiniWoB++ and WebArena, a custom airline CRM simulator, as well as live commercial websites. This tests generalization across tasks, semantics, and UI variations.- The paper provides useful analysis and ablations on the benefits of hierarchy, few-shot learning, reasoning, and model scaling. This gives useful insights into what factors help in leveraging LLMs for web tasks.Overall, this paper pushes forward web-based LLM research through its hierarchical design, sample efficiency, and evaluations across diverse and complex environments. The modular policy-based approach seems promising for scaling LLMs to the combinatorially large space of real-world web tasks.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing more advanced compression techniques to handle complex webpages with long tables, databases, etc. The paper notes limitations in handling such webpages where picking out the salient information is challenging. The authors suggest learning dedicated saliency models as a promising direction.- Incorporating multimodal perception and reasoning, especially to handle visual-only elements in webpages that are not present in the DOM structure. The authors suggest leveraging recent advances in multimodal LLMs as a way to address this limitation.- Improving the model's ability to recover from errors and false actions, such as clicking incorrect links. The authors suggest incorporating human feedback or self-verification techniques to enable better error recovery.- Expanding the diversity and complexity of tasks, for instance by considering workflows spanning multiple websites. Generalizing across a wider range of websites and tasks is noted as an important direction.- Enhancing security, privacy and transparency, for instance by developing robust verification methods before allowing execution on live websites. The authors note the potential for misuse and emphasize the need for ethical guidelines.- Exploring alternative decomposition techniques, beyond hierarchical policies, that can flexibly solve new web tasks. The hierarchical decomposition is a key contribution but the authors suggest exploring other modular, reusable abstractions.In summary, the main future directions are developing more advanced techniques for parsing complex webpages, incorporating multimodal reasoning, enhancing generalization across diverse tasks/websites, improving error handling, and ensuring ethical deployment through verification, transparency and accountability.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a novel framework called Hierarchical Policies for Web Actions using LLMs (HeaP) that learns to perform web tasks from few-shot demonstrations. It tackles the challenges of the combinatorially large space of web tasks and variations in web interfaces by leveraging LLMs to decompose complex web tasks into modular sub-tasks that can each be solved by specialized low-level policies. These policies constitute a shared grammar across tasks. The framework consists of a high-level task planner that invokes a sequence of low-level web policies. It is trained from raw demonstrations that are auto-labeled and converted into hierarchical prompts. Experiments across a range of benchmarks like MiniWoB++, WebArena, a mock airline CRM, and live websites show the approach achieves significantly higher task success rates using orders of magnitude less training data compared to prior works. The key benefits are the ability to generalize across tasks and web interfaces from a handful of examples due to both the hierarchical decomposition and few-shot prompting.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel framework called Hierarchical Policies for Web Actions using Large Language Models (HeaP) for teaching large language models (LLMs) to perform tasks on the web from few-shot demonstrations. The key idea is to leverage LLMs to decompose complex web tasks into a set of modular sub-tasks, each solved by a specialized low-level policy. For example, booking a flight can be broken down into policies for filling departure city, destination city, travel dates, passenger details etc. The framework has two components - a high-level task planner that invokes a sequence of low-level web policies to solve a task. The policies and planner are implemented as hierarchical LLM prompts generated from human demonstrations. At inference time, the task planner generates a plan consisting of policy calls based on the task objective. Each policy then executes low-level actions like clicks and types to interact with web elements. Experiments across benchmarks like MiniWoB++, WebArena and live websites show HeaP achieves higher task success rates using orders of magnitude less training data compared to prior works. The key benefit is being able to generalize across tasks and websites from few examples by reusing modular policies.


## Summarize the main method used in the paper in one paragraph.

The paper presents a hierarchical prompting framework called HeaP for teaching large language models (LLMs) to perform web tasks using few-shot demonstrations. The key idea is to decompose complex web tasks into modular sub-tasks that can each be solved by specialized low-level web policies. These policies constitute a shared grammar that can be combined to perform new tasks. The method has two components - a high-level task planner and low-level web policies. The task planner takes in a natural language instruction and current webpage state, and generates a plan consisting of calls to different web policies needed to achieve the task objective. Each web policy is a specialized LLM prompt that can execute low-level actions like clicking, typing, choosing dates, etc. To generate the prompts, raw demonstrations of humans performing web tasks are collected and auto-labeled to identify constituent web policies. These labeled demonstrations are used to create in-context examples for the task planner and web policy prompts. At test time, the framework hierarchically invokes the LLM, first generating a plan from the task planner, and then executing each policy step in the plan to interact with the webpage. This approach allows generalizing to new tasks and websites from few examples.
