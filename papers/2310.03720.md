# [HeaP: Hierarchical Policies for Web Actions using LLMs](https://arxiv.org/abs/2310.03720)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can large language models (LLMs) be taught to perform complex web tasks through hierarchical prompting and few-shot learning?

More specifically, the key research questions seem to be:

- How to decompose complex web tasks into modular sub-tasks that can each be solved by a specialized policy? 

- How to learn a shared "grammar" of reusable low-level policies from few demonstrations that can generalize across web tasks and interfaces?

- Can an LLM task planner harness these modular policies to plan and execute new web tasks?

The central hypothesis seems to be that by learning a hierarchical library of prompt-based policies from few examples, an LLM can generalize to perform new web tasks by planning and composing these modular building blocks. Rather than training task-specific models, this approach aims to acquire a grammar of skills that adapts to new tasks and websites.

The key ideas are:

- Decomposing web tasks hierarchically into high-level planning and low-level policies

- Learning prompt-based policies from few examples that generalize across web interfaces 

- Task planning via LLM prompts to compose policies for new tasks

- Achieving better generalization and sample efficiency compared to supervised approaches

In summary, the paper focuses on using hierarchical prompting and few-shot learning to teach LLMs to perform complex web tasks by acquiring a reusable library of skills.


## What is the main contribution of this paper?

 This paper presents a new framework called \method (Hierarchical Policies for Web Actions using LLMs) for training large language models (LLMs) to perform complex web tasks with minimal training data. The key ideas are:

- Decompose web tasks into modular sub-tasks that can each be solved by simple, reusable policies (e.g. fill text, choose date, etc). These policies act as a "shared grammar" across tasks.

- Learn prompts for a high-level task planner and low-level policies from just a few demonstrations. The task planner invokes policies sequentially to complete the overall task. 

- Use hierarchical prompting to efficiently leverage large LLMs, avoiding the need to fine-tune models per task.

The main contributions are:

1) A novel hierarchical framework that enables generalization to new web tasks from a handful of examples by re-using modular policies.

2) Experimental results on tasks like MiniWoB++, WebArena, live websites and a new airline CRM simulator showing improved performance over prior methods while using orders of magnitude less training data.

3) Analysis indicating hierarchical prompting is more sample-efficient by allowing more focused examples per sub-task, compared to flat prompting.

Overall, the key insight is to leverage LLMs to decompose tasks into reusable skills, avoiding the need for exhaustive training data covering all possible tasks. The hierarchical prompting approach enables rapid adaptation to new web tasks and interfaces.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel framework called HeaP that uses hierarchical prompting of large language models to decompose complex web tasks into modular policies, enabling generalization across tasks and websites from few demonstrations.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on using large language models (LLMs) for web tasks:

- It proposes a novel hierarchical framework HeaP that decomposes complex web tasks into reusable low-level policies. This is different from prior works that train end-to-end models for specific web tasks. The modularity makes HeaP more generalizable across tasks and websites. 

- The method is significantly more sample efficient than prior approaches. HeaP matches state-of-the-art performance on MiniWoB++ using orders of magnitude less training data (21 examples vs millions of examples in prior works). This enables rapidly adapting the framework to new tasks with limited demonstrations.

- The framework uses a hierarchical prompting approach to leverage LLMs, unlike most prior works that fine-tune LLMs for web tasks. Prompting is a lighter-weight approach compared to fine-tuning, making the system easier to update and maintain.

- It evaluates the framework on a wider range of environments compared to prior works - benchmarks like MiniWoB++ and WebArena, a custom airline CRM simulator, as well as live commercial websites. This tests generalization across tasks, semantics, and UI variations.

- The paper provides useful analysis and ablations on the benefits of hierarchy, few-shot learning, reasoning, and model scaling. This gives useful insights into what factors help in leveraging LLMs for web tasks.

Overall, this paper pushes forward web-based LLM research through its hierarchical design, sample efficiency, and evaluations across diverse and complex environments. The modular policy-based approach seems promising for scaling LLMs to the combinatorially large space of real-world web tasks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Developing more advanced compression techniques to handle complex webpages with long tables, databases, etc. The paper notes limitations in handling such webpages where picking out the salient information is challenging. The authors suggest learning dedicated saliency models as a promising direction.

- Incorporating multimodal perception and reasoning, especially to handle visual-only elements in webpages that are not present in the DOM structure. The authors suggest leveraging recent advances in multimodal LLMs as a way to address this limitation.

- Improving the model's ability to recover from errors and false actions, such as clicking incorrect links. The authors suggest incorporating human feedback or self-verification techniques to enable better error recovery.

- Expanding the diversity and complexity of tasks, for instance by considering workflows spanning multiple websites. Generalizing across a wider range of websites and tasks is noted as an important direction.

- Enhancing security, privacy and transparency, for instance by developing robust verification methods before allowing execution on live websites. The authors note the potential for misuse and emphasize the need for ethical guidelines.

- Exploring alternative decomposition techniques, beyond hierarchical policies, that can flexibly solve new web tasks. The hierarchical decomposition is a key contribution but the authors suggest exploring other modular, reusable abstractions.

In summary, the main future directions are developing more advanced techniques for parsing complex webpages, incorporating multimodal reasoning, enhancing generalization across diverse tasks/websites, improving error handling, and ensuring ethical deployment through verification, transparency and accountability.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a novel framework called Hierarchical Policies for Web Actions using LLMs (HeaP) that learns to perform web tasks from few-shot demonstrations. It tackles the challenges of the combinatorially large space of web tasks and variations in web interfaces by leveraging LLMs to decompose complex web tasks into modular sub-tasks that can each be solved by specialized low-level policies. These policies constitute a shared grammar across tasks. The framework consists of a high-level task planner that invokes a sequence of low-level web policies. It is trained from raw demonstrations that are auto-labeled and converted into hierarchical prompts. Experiments across a range of benchmarks like MiniWoB++, WebArena, a mock airline CRM, and live websites show the approach achieves significantly higher task success rates using orders of magnitude less training data compared to prior works. The key benefits are the ability to generalize across tasks and web interfaces from a handful of examples due to both the hierarchical decomposition and few-shot prompting.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a novel framework called Hierarchical Policies for Web Actions using Large Language Models (HeaP) for teaching large language models (LLMs) to perform tasks on the web from few-shot demonstrations. The key idea is to leverage LLMs to decompose complex web tasks into a set of modular sub-tasks, each solved by a specialized low-level policy. For example, booking a flight can be broken down into policies for filling departure city, destination city, travel dates, passenger details etc. 

The framework has two components - a high-level task planner that invokes a sequence of low-level web policies to solve a task. The policies and planner are implemented as hierarchical LLM prompts generated from human demonstrations. At inference time, the task planner generates a plan consisting of policy calls based on the task objective. Each policy then executes low-level actions like clicks and types to interact with web elements. Experiments across benchmarks like MiniWoB++, WebArena and live websites show HeaP achieves higher task success rates using orders of magnitude less training data compared to prior works. The key benefit is being able to generalize across tasks and websites from few examples by reusing modular policies.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a hierarchical prompting framework called HeaP for teaching large language models (LLMs) to perform web tasks using few-shot demonstrations. The key idea is to decompose complex web tasks into modular sub-tasks that can each be solved by specialized low-level web policies. These policies constitute a shared grammar that can be combined to perform new tasks. 

The method has two components - a high-level task planner and low-level web policies. The task planner takes in a natural language instruction and current webpage state, and generates a plan consisting of calls to different web policies needed to achieve the task objective. Each web policy is a specialized LLM prompt that can execute low-level actions like clicking, typing, choosing dates, etc. 

To generate the prompts, raw demonstrations of humans performing web tasks are collected and auto-labeled to identify constituent web policies. These labeled demonstrations are used to create in-context examples for the task planner and web policy prompts. At test time, the framework hierarchically invokes the LLM, first generating a plan from the task planner, and then executing each policy step in the plan to interact with the webpage. This approach allows generalizing to new tasks and websites from few examples.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the title of the paper?

2. Who are the authors of the paper?

3. What conference or journal was the paper published in? 

4. What is the key problem or challenge that the paper aims to address?

5. What are the key contributions or main ideas proposed in the paper?

6. What methods, models, or frameworks are presented in the paper? 

7. What experiments were conducted and what were the main results?

8. What are the limitations of the approaches proposed in the paper?

9. How does this work compare to prior research in the field? What improvements does it offer?

10. What are potential future directions for research that builds on this work?

Asking these types of questions should help summarize the core ideas and contributions of the paper, the proposed techniques, the experiments and results, and how this work relates to the broader research area. The questions aim to distill the key information needed to provide a comprehensive overview of what the paper presents. Additional targeted questions could also be asked to elucidate technical details or results as needed.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the key research questions/hypotheses appear to be:

- Can a hierarchical framework that decomposes complex web tasks into reusable sub-tasks enable large language models (LLMs) to efficiently learn policies for performing a wide range of web tasks from limited demonstrations?

- Can representing web tasks as compositions of modular low-level policies that interact with web elements create a kind of "shared grammar" that allows generalization to new tasks and websites? 

- Does hierarchical prompting and decomposition of tasks into granular policies allow more efficient use of the limited context space compared to flat prompting approaches?

- Can an approach that leverages LLMs to auto-label demonstrations with low-level policies, and auto-generate hierarchical prompts from these, enable learning from raw demonstration data with minimal manual engineering?

- How does the proposed framework, HeaP, compare to prior works and baselines on metrics like success rates, data efficiency, and generalization across web tasks and websites?

So in summary, the key hypotheses appear to be around using hierarchical decomposition and prompting to efficiently teach LLMs policies for web tasks that can generalize broadly across tasks and websites from limited demonstrations. The proposed HeaP framework is evaluated to test these hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a novel framework called HeaP (Hierarchical Policies for Web Actions using LLMs) for teaching large language models (LLMs) to perform complex web tasks from few-shot demonstrations. 

Specifically, the key ideas and contributions are:

- Proposing a hierarchical framework with a high-level task planner that breaks down web tasks into sub-tasks, each solved by low-level web policies. This allows composability and generalization across tasks.

- Learning prompts for the high-level planner and low-level policies automatically from unlabeled demonstrations. This involves auto-labeling demonstrations and generating hierarchical prompts.

- Evaluating HeaP on a range of benchmarks including MiniWoB++, WebArena, an airline CRM simulator, and live websites. HeaP achieves higher task success rates using orders of magnitude less training data than prior works.

- Demonstrating that hierarchical prompting and few-shot learning helps LLMs effectively generalize across varying tasks, interfaces and websites. HeaP is able to adapt to new tasks and websites with only a handful of examples.

In summary, the key contribution is a novel hierarchical prompting framework that enables sample-efficient learning of complex web tasks using LLMs from few demonstrations. The composability and generalization capabilities are the main strengths demonstrated through comprehensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes a new method called HeaP that teaches large language models to perform complex web tasks by hierarchically decomposing demonstrations into reusable subtask policies, enabling generalization from very few examples.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field:

- This paper presents a novel framework called HeaP for teaching large language models (LLMs) to perform web tasks using hierarchical prompting and few-shot demonstrations. It builds on prior work in using LLMs for web tasks, but offers a more compositional and sample-efficient approach.

- Compared to prior work in using RL to train agents to navigate websites (e.g. WoB, MiniWoB, Humphreys et al.), this paper does not require environment exploration and avoids the sample inefficiency of RL. It is able to match performance with orders of magnitude less data.

- Compared to supervised approaches that fine-tune LLMs on web tasks (e.g. WebN-T5, WebGUM), this paper shows better generalization with much less training data. Fine-tuning often requires retraining with more examples of unseen tasks and websites.

- Compared to few-shot prompting without hierarchy (e.g. ReAct), the key innovation is the hierarchical decomposition into a task planner and reusable web policies. This allows packing more examples into individual prompts and handling more complex tasks.

- The idea of hierarchical task decomposition relates to prior work on hierarchical reasoning and planning. However, the application to interactive web tasks with LLMs is novel.

- The overall approach of learning from human demonstrations also relates to imitation learning. However, the method of separating high-level planning from low-level control distinguishes it from standard imitation learning formulations.

In summary, this paper pushes forward web task learning for LLMs by combining ideas of hierarchy, compositionality, and learning from demonstrations. The comparisons on multiple benchmarks demonstrate improved generalization and sample efficiency over a variety of strong prior methods.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Developing improved techniques for compressing and parsing complex webpages. The paper notes limitations in handling pages with long tables, databases, and visual-only elements. Methods like learning dedicated saliency models could help determine the most relevant elements on a complex page. 

- Incorporating verification and error recovery abilities. The paper mentions that the method can sometimes take incorrect actions like clicking the wrong links. Enabling the system to detect such errors and recover could improve robustness. Ideas like human feedback and self-verification are proposed.

- Exploring multi-modal models. The paper notes that lack of visual information is a limitation, so leveraging multi-modal models that can reason about both text and images could help.

- Scaling up the framework with more tasks. While the current work focuses on a limited set of tasks, expanding to a wider range of complex web tasks could further demonstrate the benefits of the hierarchical prompting approach.

- Improving generalization with fewer examples. Reducing the number of demonstrations needed for few-shot learning, perhaps via meta-learning or leveraging structure in the tasks and policies, could make the approach more practical. 

- Studying social impacts of action LLMs. The paper raises concerns around potential misuse of such systems that can execute arbitrary actions. Understanding societal impacts and developing solutions like verification methods is important future work.

In summary, the key directions revolve around improving webpage understanding, adding recovery abilities, incorporating multi-modality, expanding tasks, boosting few-shot generalization, and studying social impacts. Overall, the hierarchical prompting framework seems promising and enhancing it along these dimensions could enable wider practical applications.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a novel framework called HeaP (Hierarchical Policies for Web Actions using LLMs) that uses large language models (LLMs) to perform web tasks from few-shot demonstrations. HeaP decomposes complex web tasks into modular sub-tasks that can each be solved by dedicated low-level policies. At inference time, it hierarchically invokes an LLM to first generate a high-level task plan, and then execute each step in the plan by calling the appropriate low-level policy. The policies act directly on the web UI by issuing clicks, fills, etc. HeaP is trained from raw human demonstrations that are auto-labeled into policies and used to generate prompts for the task planner and policies. Experiments across benchmarks like MiniWoB++ and WebArena show HeaP achieving much higher task success rates using orders of magnitude less training data compared to prior works. A key benefit is the ability to generalize across unseen tasks and websites from a handful of examples by leveraging the hierarchy and modularity of policies.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a novel framework called HeaP: Hierarchical Policies for Web Actions using Large Language Models that enables LLMs to perform a wide range of web tasks from natural language instructions. The key idea is to leverage LLMs to decompose complex web tasks into a set of modular sub-tasks, each of which can be solved by a specialized low-level policy. For example, booking a flight can be broken down into policies for filling departure city, destination city, travel dates, passenger details etc. 

The framework consists of a high-level task planner that generates a plan comprising of calls to low-level policies. Each policy executes web actions like clicking, typing, choosing dates. At train time, raw demonstrations are collected from humans and auto-labeled to identify constituent policies. These are used to generate prompts for the task planner and policies. At test time, given a task instruction, the planner generates a policy plan which is executed step-by-step on the web page. Experiments across benchmarks like MiniWoB++, WebArena and live websites show the framework achieves higher task success with orders of magnitude less data than prior works. A key benefit is the ability to generalize across tasks and websites.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel framework called HeaP (Hierarchical Policies for Web Actions using LLMs) that learns to perform web tasks from few-shot demonstrations. HeaP decomposes complex web tasks into modular sub-tasks that can each be solved by specialized low-level policies. It has a hierarchical structure with a high-level task planner that breaks down the task into a sequence of policy calls, and low-level web policies that execute actions like clicking and typing. HeaP is trained from raw demonstrations that are auto-labeled into policies, which are then converted to prompts for the task planner and web policies. At test time, HeaP is given a task objective and uses the task planner prompt to generate a policy plan. It then sequentially executes each policy using the policy prompts, interacting with the web page until the task is completed. A key benefit is it enables generalization to new tasks and websites from very few examples.


## What problem or question is the paper addressing?

 Based on my reading, the key points are:

- The paper aims to teach large language models (LLMs) to perform tasks on websites, which has been challenging due to the combinatorially large space of possible web tasks and variations in websites. 

- The key idea is to leverage LLMs to decompose complex web tasks into modular sub-tasks that can each be solved by simple policies shared across tasks. For example, booking a flight can be decomposed into sub-tasks like filling in source/destination, dates, passenger details etc.

- They propose a framework called HeaP that learns a hierarchy of LLM prompts - high level prompts to generate task plans by sequencing lower level policy prompts. 

- The lower level policy prompts issue actions like clicks and types to interact with webpages. This allows generalizing across different websites.

- They show HeaP achieves much better task success rates compared to prior works, while using orders of magnitude less training data. It can adapt to new tasks and websites using just a handful of demonstrations.

- Overall, the key ideas are using LLM task decomposition and prompting to learn a hierarchical policy that combines both high level planning and low level web interactions. The decomposition into shared policies allows generalizing across tasks and websites.


## What are the keywords or key terms associated with this paper?

 Based on a review of the paper, some of the key terms and concepts include:

- Hierarchical prompting - The paper proposes a hierarchical prompting approach called HeaP that uses large language models (LLMs) to decompose complex web tasks into modular sub-tasks. 

- Web tasks - The overall focus is on teaching LLMs to perform tasks on the web, such as booking flights, completing forms, etc.

- Low-level policies - HeaP learns a set of low-level policies, each specialized for a sub-task like filling text boxes, choosing dates, clicking buttons. These policies act as building blocks.

- Task planner - A high-level module that breaks down instructions/goals into a sequence of low-level policy calls to accomplish the task.

- Composability - HeaP composes low-level policies in a modular way to solve new tasks. This provides generalization.

- Few-shot learning - HeaP is highly sample efficient, able to adapt to new web tasks and websites from just a few demonstrations.

- Evaluation - HeaP is evaluated on simulations like MiniWoB++ and Airline CRM, as well as complex interactive websites. It outperforms baselines.

- Imitation learning - HeaP follows an imitation learning approach based on human demonstrations rather than reinforcement learning.

In summary, the key terms cover hierarchical prompting with LLMs, learning reusable skills for web tasks, few-shot compositional generalization, and benchmarking on interactive web environments.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a hierarchical framework called \method that learns prompts for both a high-level task planner and low-level web policies. How does this hierarchical decomposition enable the framework to generalize to new tasks and websites better compared to flat prompting approaches?

2. The paper shows that \method is able to solve tasks using orders of magnitude less data than prior works that train supervised models. What properties of the proposed approach make it more sample efficient? How does hierarchical prompting and in-context learning contribute to this?

3. The paper evaluates \method on a range of benchmarks including MiniWoB++, WebArena, a custom Airline CRM simulator and live websites. Can you analyze the relative complexity and challenges offered by each of these environments? How does the performance of \method vary across them?

4. The airline CRM simulator developed in the paper is modeled after customer workflows on real airline websites. What are some key differences between this simulator and MiniWoB++? What new complexities does it enable testing?

5. The paper introduces a browser plugin to collect raw human demonstrations involving web actions like clicks and types. How are these demonstrations then processed and auto-labeled? What is the motivation behind auto-labeling compared to manual labeling?

6. The method converts raw demonstrations into prompt examples for both the high-level planner and low-level policies. How are these prompts generated? What are some key differences in the prompt structure and examples shown to the planner vs the policies?

7. Chain-of-thought prompting is utilized in the paper to improve task reasoning. What are the tradeoffs introduced by CoT regarding prompt lengths? How does CoT reasoning quantitatively and qualitatively improve performance?

8. What are some common failure modes and limitations of the proposed approach? How can the method be made more robust to complex webpages and better handle errors?

9. The paper focuses on learning from human demonstrations. How can the framework be extended to also learn from exploration and feedback? What kinds of exploration strategies and human feedback would be valuable?

10. The idea of hierarchical task decomposition and composable policies has broad applicability beyond just web tasks. What are other potential domains and applications where this framework could be useful? What enhancements would need to be made?
