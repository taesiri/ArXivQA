# Synthetic Prompting: Generating Chain-of-Thought Demonstrations for   Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, it seems this paper introduces a method called "Synthetic prompting" to improve the reasoning abilities of large language models when given only a few examples. The key ideas are:- Use a few seed examples to prompt the language model to synthesize more demonstrative examples itself, by alternating between a backward process (generate question from reasoning chain) and forward process (generate reasoning chain for question). - Select the most effective demonstrations from the synthesized set using a scheme based on in-cluster complexity, to maximize diversity and complexity.- Prompt the model with selected synthetic demonstrations during inference to elicit better reasoning compared to just using the limited seed examples.So the main hypothesis is that prompting the model to synthesize its own demonstrations and selecting good ones can significantly improve its reasoning performance with few examples, compared to existing prompting techniques. The method aims to address the limitation that manually creating many high-quality demonstrations is costly and tedious.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contribution of this paper is proposing a method called Synthetic prompting to improve the reasoning abilities of large language models when given limited examples. The key ideas are:- Using a few seed examples to prompt the language model to generate more examples by itself through an alternating backward-forward process. - The backward process generates a question that matches a sampled reasoning chain, making the question solvable. - The forward process produces a more detailed reasoning chain for the question.- Selecting effective demonstrations from the augmented set of examples using a scheme based on in-cluster complexity, which aims to maximize diversity and informativeness.- Prompting the language model with the selected demonstrations results in better reasoning performance compared to using the original limited examples.So in summary, the core novelty is using the language model's own generative capabilities to synthesize more demonstrations and selecting good ones to improve its reasoning, rather than just using the original limited examples. The method is evaluated on numerical, symbolic and algorithmic reasoning tasks.
