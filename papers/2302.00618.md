# Synthetic Prompting: Generating Chain-of-Thought Demonstrations for   Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, it seems this paper introduces a method called "Synthetic prompting" to improve the reasoning abilities of large language models when given only a few examples. The key ideas are:- Use a few seed examples to prompt the language model to synthesize more demonstrative examples itself, by alternating between a backward process (generate question from reasoning chain) and forward process (generate reasoning chain for question). - Select the most effective demonstrations from the synthesized set using a scheme based on in-cluster complexity, to maximize diversity and complexity.- Prompt the model with selected synthetic demonstrations during inference to elicit better reasoning compared to just using the limited seed examples.So the main hypothesis is that prompting the model to synthesize its own demonstrations and selecting good ones can significantly improve its reasoning performance with few examples, compared to existing prompting techniques. The method aims to address the limitation that manually creating many high-quality demonstrations is costly and tedious.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contribution of this paper is proposing a method called Synthetic prompting to improve the reasoning abilities of large language models when given limited examples. The key ideas are:- Using a few seed examples to prompt the language model to generate more examples by itself through an alternating backward-forward process. - The backward process generates a question that matches a sampled reasoning chain, making the question solvable. - The forward process produces a more detailed reasoning chain for the question.- Selecting effective demonstrations from the augmented set of examples using a scheme based on in-cluster complexity, which aims to maximize diversity and informativeness.- Prompting the language model with the selected demonstrations results in better reasoning performance compared to using the original limited examples.So in summary, the core novelty is using the language model's own generative capabilities to synthesize more demonstrations and selecting good ones to improve its reasoning, rather than just using the original limited examples. The method is evaluated on numerical, symbolic and algorithmic reasoning tasks.


## How does this paper compare to other research in the same field?

Based on the abstract, this paper introduces a novel method called Synthetic Prompting for eliciting better reasoning from large language models using limited demonstrations. Here are a few key points of comparison to other related work:1. Relationship to in-context few-shot learning:- This work is related to in-context few-shot learning, where large language models (LLMs) perform reasoning tasks by mimicking demonstrations in the prompt context. - However, rather than directly using the limited demonstrations for inference, this method uses them as seeds to synthesize more examples via prompting. It then selects effective examples from the augmented set to prompt the LLM.2. Comparison to prompting techniques:- This extends prompting techniques like chain-of-thought prompting and PAL prompting that guide LLMs to reason step-by-step.- The key difference is it leverages model-generated demonstrations rather than relying solely on provided examples. The synthetic prompting elicits better reasoning compared to using the original examples.3. Relationship to knowledge distillation from LLMs:- Prior work has distilled knowledge like examples and rules from LLMs, often for further training. - This work is novel in distilling examples specifically to improve the LLM's own reasoning when given limited demonstrations, without additional training.4. Demonstration selection schemes:- This introduces a new selection method based on in-cluster complexity to ensure diversity and complexity.- It shows improved performance over selection by similarity or complexity alone.In summary, this paper presents a novel synthesis-based prompting approach that outperforms existing prompting methods on reasoning tasks given limited demonstrations. The idea of leveraging the model's own knowledge to elicit better reasoning is an important advancement in few-shot in-context learning.
