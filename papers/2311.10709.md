# [Emu Video: Factorizing Text-to-Video Generation by Explicit Image   Conditioning](https://arxiv.org/abs/2311.10709)

## Summarize the paper in one sentence.

 The paper proposes a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and generated image. 


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper presents Emu Video, a text-to-video generation model that factorizes the generation process into two steps. First, an image is generated from the input text prompt. Then, a video is generated conditioned on both the text prompt and the generated image. This factorization approach strengthens the conditioning signal compared to direct text-to-video generation. The model is initialized from a pretrained text-to-image model, with new temporal parameters added for video generation. Key design decisions are made, including using a zero terminal-SNR noise schedule and multi-stage training, to enable high resolution 512px video generation without requiring a cascade of models. The factorization approach allows retaining the diversity and quality of the image model. Human evaluations show Emu Video significantly outperforms prior work, including commercial solutions, in terms of video quality and text faithfulness. Beyond text-to-video generation, Emu Video can also animate user-provided images by conditioning video generation on the image and text. Comparisons show it outperforms prior work in image-to-video generation as well.
