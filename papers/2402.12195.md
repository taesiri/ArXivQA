# [Browse and Concentrate: Comprehending Multimodal Content via prior-LLM   Context Fusion](https://arxiv.org/abs/2402.12195)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing multimodal large language models (MLLMs) that empower large language models (LLMs) with visual abilities fall short in comprehending context involving multiple images. This is due to "prior-LLM modality isolation", which means the visual features for each image are encoded individually by frozen encoders before feeding into the LLM backbone. This leads to two issues - "image-text isolation" where generic visual features overlook crucial target-specific information, and "inter-image isolation" where images lack awareness of relevant content from other images.

Proposed Solution:
This paper proposes a "browse-and-concentrate" paradigm with two phases to enable prior-LLM multimodal context fusion:

1) Browsing Phase: Browses through the entire input to generate a "condition context vector" summarizing the main intent and visual information. 

2) Concentrating Phase: Comprehends the multimodal inputs guided by the condition context vector from phase 1.

The paper explores two implementations:
(1) Explicit (Brote-EX): separate models for browsing and concentrating phases.  
(2) Implicit (Brote-IM): shared parameters between phases.

Additionally, tailored training strategies are proposed:
- Context-enhanced pre-training to utilize condition vectors 
- Condition-aware task fine-tuning to handle missing input modalities

Main Contributions:
- Proposes browse-and-concentrate paradigm to stimulate prior-LLM multimodal context fusion 
- Implements the paradigm via explicit and implicit modes
- Develops specialized training strategies for pre-training and fine-tuning
- Achieves consistent and notable improvements on 7 multi-image benchmarks over strong MLLM baselines

In summary, this paper makes significant contributions in enhancing MLLMs' awareness and comprehension of multimodal context across images through a novel browse-and-concentrate approach and tailored training strategies.
