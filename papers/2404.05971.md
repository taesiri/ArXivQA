# [Does Transformer Interpretability Transfer to RNNs?](https://arxiv.org/abs/2404.05971)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Recently, new RNN architectures like Mamba and RWKV have matched or exceeded the performance of equal-size transformers on language modeling and downstream tasks. This suggests RNNs may resurge as competitive alternatives. 
- The paper examines if selected interpretability methods designed for transformers will transfer to these up-and-coming RNN architectures. Specifically, it focuses on:
    1) Contrastive activation addition (CAA) for steering model outputs
    2) The tuned lens for eliciting latent predictions
    3) Probing "quirky" models fine-tuned to produce false outputs

Methods
- The authors reproduce prior work on transformer interpretability for Mamba and RWKV models using the same datasets and codebases where possible.
- For CAA, they introduce a modification called "state steering" that operates on the compressed state of RNNs instead of the residual stream. 
- Models are fine-tuned to make systematic mistakes on questions containing "Bob", while answering correctly on "Alice" questions. Probes are trained on "Alice" examples and tested on "Bob" examples.

Results
- Most transformer interpretability techniques work out-of-the-box for RNNs with moderately reduced but still meaningful effectiveness.
- State steering provided a small boost over standard CAA, but effects were not additive.
- Probes elicited latent knowledge about the correct answers even from "quirky" fine-tuned models.

Conclusions
- Key transformer interpretability tools largely transfer to competitive new RNN architectures. 
- The compressed state provides opportunities for enhancement, e.g. state steering, which future work should explore further.
- Circuit-based interpretability tools should also be evaluated for applicability to RNNs.

In summary, the paper shows selected interpretation methods for transformers can transfer to new state-of-the-art RNNs, with opportunities for improvement by exploiting the compressed state. This suggests these tools may remain broadly useful as new dominant architectures emerge.
