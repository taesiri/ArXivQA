# [Boosting the Cross-Architecture Generalization of Dataset Distillation   through an Empirical Study](https://arxiv.org/abs/2312.05598)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing dataset distillation (DD) methods suffer from poor cross-architecture generalization, meaning models of different architectures trained on the distilled dataset have much lower performance compared to being trained on the original dataset. This greatly limits the practical utility of DD methods. 

Through empirical studies, the paper shows that:
1) More architectural similarity between distillation and evaluation models leads to less performance gap. 
2) Inserting identical normalization layers also reduces the gap.

The underlying reason is that the synthetic dataset has an inductive bias towards the distillation model architecture, which constrains the evaluation model architecture.

Proposed Solution: 
The paper proposes a method called Evaluation with Distillation Feature (ELF) to eliminate the inductive bias. The key ideas are:

1) Leverage the intermediate features from the distillation model as they are free of inductive bias.
2) Use the distillation features to supervise the evaluation model's intermediate outputs.
3) Feed the distillation features into the evaluation model to predict labels.

In this way, the evaluation model learns from bias-free knowledge, making its architecture unfettered while retaining performance.

Main Contributions:
1) Reveal the inductive bias causing poor cross-architecture generalization of DD methods.
2) Propose ELF method to utilize distillation model's features for bias elimination.
3) Demonstrate consistent and substantial performance gains over state-of-the-art DD methods on CIFAR, Tiny ImageNet and ImageNet datasets.

The paper makes the first endeavor on directly analyzing and enhancing the cross-architecture generalization of DD methods. The proposed ELF method effectively boosts performance without compromising model architecture.
