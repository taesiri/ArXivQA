# [Marked Personas: Using Natural Language Prompts to Measure Stereotypes   in Language Models](https://arxiv.org/abs/2305.18189)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we measure stereotypes in large language models (LLMs) for intersectional demographic groups in an unsupervised manner without relying on an existing lexicon or labeled dataset?

The authors propose a new method called "Marked Personas" to address this question. The key components are:

1) Using natural language prompts to generate "personas", which are first-person descriptions of imagined individuals belonging to a particular demographic group.

2) Identifying "marked words" that statistically distinguish the personas of marginalized/marked groups from dominant/unmarked groups. This is done in an unsupervised way using log-odds ratios and z-scores.

3) Analyzing these marked words to reveal harmful patterns like stereotypes and essentializing narratives in the LLM's portrayals of different demographic groups, especially at their intersections.

So in summary, the central hypothesis is that the proposed Marked Personas approach can effectively measure stereotypes in LLMs for intersectional groups without needing an existing lexicon or labeled data. The paper then demonstrates and analyzes this method on groups defined by gender, race/ethnicity, and their combinations.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The Marked Personas framework, which is a prompt-based method to measure stereotypes in language models for intersectional demographic groups without needing any lexicons or labeled data.

2. The finding that personas generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes compared to human-written personas using the same prompts.

3. An analysis of the kinds of stereotypes, essentializing narratives, tropes, and other harmful patterns present in the GPT-3.5 and GPT-4 generated personas that are identified by the Marked Personas method but not captured by existing bias measurement techniques.

In more detail:

- The Marked Personas framework involves prompting the language model to generate "personas", which are natural language descriptions of individuals belonging to a particular demographic group. It then compares the generated personas of marginalized/marked groups to those of unmarked/default groups to identify distinguishing words that may reflect stereotypes.

- In experiments comparing personas generated by GPT-3.5/4 vs human writings, the authors find higher rates of racial stereotypes in the AI-generated texts based on existing stereotype lexicons.

- Analysis of the distinguishing words identified by Marked Personas reveals harmful patterns overlooked by other methods, like seemingly positive words related to tropes of resilience/exoticism, essentializing narratives, etc. that contribute to representational harms.

So in summary, the main contribution is presenting an unsupervised way to measure nuanced stereotypes in language models for any demographic groups, and using it to uncover under-explored harms in leading LLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on skimming the paper, here is a one sentence TL;DR summary:

The paper proposes a new unsupervised framework called Marked Personas to measure stereotypes and biased portrayals in language model outputs for any demographic group, without needing manually-labeled data.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of measuring stereotypes and biases in large language models:

- The key contribution is the development of an unsupervised, prompt-based method called Marked Personas to surface harmful patterns in LLM generations. This approach is novel compared to existing methods that rely on pre-defined stereotype lexicons or labeled datasets. The Marked Personas framework allows flexibly studying portrayals of any demographic group.

- The paper demonstrates that seemingly positive stereotypes can still be harmful, such as narratives of resilience and independence disproportionately applied to marginalized groups. This highlights limitations of only considering negative sentiment stereotypes. Other work has also examined the harms of benevolent prejudice.

- Applying an intersectional lens to analyze compounding biases is another strength of this work. There is growing interest in studying intersectional biases, whereas much past work focused on singular identities. The paper reveals unique stereotypes arise for combinations of identities.

- Comparing stereotype rates in human vs LLM texts is an interesting evaluation. The finding that LLMs exhibit higher rates is concerning and highlights the need for mitigation. Related work has studied stereotype amplification in LLMs.

- The analysis of downstream harms in story generation adds useful evidence on how biases propagate in applications. Other research has also looked at biases in areas like dialog and search results.

- The recommendations around addressing positive stereotypes, intersectionality, and transparency align with challenges identified by the broader community. The paper provides supportive evidence and specific suggestions to advance solutions.

In summary, this paper makes excellent contributions to measuring and understanding representations in LLMs through its novel unsupervised approach, focus on positive and intersectional biases, and thorough qualitative and quantitative analysis. It builds nicely upon related work and offers valuable insights to guide future progress.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing methods to characterize stereotypes and biases in LLMs in an unsupervised, lexicon-free manner. The authors suggest their Marked Personas framework could be expanded to additional demographic groups and biases beyond stereotypes.

- Further study of positive stereotypes and seemingly benign patterns like narratives of resilience/strength. The authors argue these can still be harmful and provide justification for systemic bias. More work is needed on how to address these "pernicious positive portrayals."

- Taking an intersectional perspective when measuring bias. The authors find personas for intersectional groups like Black women contain unique stereotypes not found in singular race or gender groups.

- Greater transparency from LLM creators about the specifics of training data and bias mitigation techniques used. This could help the research community better understand the root causes of remaining biases. 

- Alternative approaches beyond simply replacing negative stereotypes with positive terms, which may just reinforce existing problematic social norms. The authors suggest "critical refusal" where models decline to generate personas/texts that rely on stereotypes.

- Studying the downstream impacts of biases in LLMs, such as on story generation. The authors find similar biased patterns emerge in stories as in the persona generations.

- Developing bias mitigation practices that address systemic harms, not just overt toxicity. The authors argue current techniques overlook nuanced issues like seemingly positive stereotypes.

In summary, the authors advocate for more holistic, unsupervised, and intersectional approaches to understand biases in LLMs, greater transparency from model creators, and mitigation that deals with systemic issues beyond just overt toxicity.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents Marked Personas, a novel prompt-based method to measure stereotypes in large language models (LLMs) like GPT-3 for different demographic groups in an unsupervised manner. The method has two main components - generating persona texts to portray imagined individuals belonging to a target group, and using statistical tests to identify "marked words" that distinguish personas of marginalized groups from dominant/unmarked ones. Comparing personas generated by GPT-3.5/4 versus human writing, they find higher rates of stereotypes in LLM outputs. Analyzing the marked words reveals nuanced patterns beyond overt negativity - seemingly positive descriptors like "resilient" and "smooth" reflect essentializing narratives and stereotypes when disproportionately applied to minorities. These subtle biases persist in downstream LLM applications like story generation as well. The authors conclude with recommendations for bias mitigation focused on intersectionality, moving beyond positive stereotypes, and transparency. Overall, this work provides a generalizable tool to surface group-specific representational harms in LLMs using only natural language prompts.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a new method called Marked Personas for measuring stereotypes in large language models (LLMs) like GPT-3. The method uses natural language prompts to generate "personas" or descriptions of individuals belonging to particular demographic groups. It then analyzes these personas to identify words that distinguish marked, non-dominant groups from unmarked, dominant default groups. 

The authors find that personas generated by GPT-3 contain higher rates of racial stereotypes compared to human-written personas using the same prompts. The distinguishing words reflect patterns of othering, essentialism, and seemingly positive yet harmful stereotypes, especially for intersectional identities. The authors discuss implications for downstream applications like story generation and make recommendations for addressing positive stereotypes, taking an intersectional lens, and increasing transparency around bias mitigation techniques. Overall, this unsupervised approach provides insight into nuanced stereotypes in LLMs beyond what current lexicon-based methods capture.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents a new method called Marked Personas for measuring stereotypes in large language models (LLMs) without needing any labeled data or lexicons. The method has two main parts. First, it generates persona descriptions for demographic groups of interest by prompting the LLM with natural language prompts asking it to describe a person of that group from a first-person perspective. Second, it identifies words that statistically significantly distinguish the personas of the demographic group from corresponding unmarked/default groups, using log-odds ratios and z-scores to find marked words. This surfaces stereotypes and problematic patterns in how the LLM generates descriptions of marginalized groups compared to dominant groups. The method can be applied to any combination of demographic groups and captures nuances overlooked by previous bias measurement methods. Overall, it provides an unsupervised way to characterize an LLM's stereotypical associations for particular groups using only natural language generations.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

1. The paper aims to measure stereotypes in large language models (LLMs) for intersectional demographic groups, without needing labeled data or lexicons. 

2. The authors present a new method called "Marked Personas" which involves:

- Prompting the LLM to generate "personas", which are natural language descriptions, for target demographic groups (e.g. Black woman) and unmarked/default groups (e.g. white, man). 

- Identifying words that statistically distinguish the personas of the target group from the unmarked groups. This surfaces stereotypes and problematic patterns.

3. The authors find that personas generated by LLMs like GPT-3.5 and GPT-4 contain higher rates of stereotypes compared to human-written personas using the same prompts.

4. Their analysis reveals harmful patterns beyond those in existing stereotype lexicons, like tropes, essentialism, and the myth of resilience disproportionately applied to marginalized groups. 

5. They argue these subtle yet pernicious patterns have concerning implications for LLM-generated content, and make recommendations like addressing positive stereotypes and taking an intersectional perspective.

In summary, the key problem is measuring and understanding stereotypes in LLM outputs, especially for intersectional groups. The authors present a new unsupervised method to address this and reveal overlooked issues with existing methods and LLMs.


## What are the keywords or key terms associated with this paper?

 Based on reading the paper, some of the key terms and keywords that stand out are:

- Stereotypes - The paper examines stereotypes in language model outputs, especially regarding marginalized demographic groups. 

- Personas - The proposed method generates "personas", which are natural language descriptions of individuals belonging to certain demographic groups. This is used to study stereotypes.

- Markedness - The concept of linguistic and social markedness, referring to default vs non-default categories, is foundational to the paper's approach.

- Bias measurement - The paper compares the proposed approach to existing methods for measuring biases and stereotypes in language models.

- Intersectionality - The analysis uses an intersectional lens to study stereotypes regarding combinations of identities. 

- Tropes - Certain tropes like "model minority" and "strong black woman" are identified in the language model outputs.

- Othering - Patterns of othering minority groups as compared to an unmarked default are discussed. 

- Essentialism - The paper examines how stereotypes lead to essentialist portrayals of minority groups.

- Positive stereotypes - Seemingly positive stereotypes are analyzed for their potentially harmful implications.

So in summary, the key terms cover stereotype measurement, intersectionality, sociolinguistic concepts, and qualitative analysis of the types of biases identified.
