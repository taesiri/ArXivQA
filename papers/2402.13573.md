# [ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images](https://arxiv.org/abs/2402.13573)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Attention mechanisms in image generation models like Stable Diffusion have quadratic computational complexity, limiting the image resolutions that can be processed in reasonable time. 
- Existing methods to sparsify attention like Token Merging (ToMe) have significant overhead from computing token similarities and quality loss from merging/unmerging.

Proposed Solution:
- The authors propose Token Downsampling (ToDo), a training-free method to accelerate Stable Diffusion inference by subsampling position tokens in a strided fashion.
- They use nearest neighbor downsampling, exploiting the observation that adjacent pixels tend to have similar values.
- ToDo only downsamples the keys and values in attention, preserving the queries. This retains information without needing costly unmerging.

Contributions:
- ToDo accelerates Stable Diffusion inference by up to 2x for common sizes and 4.5x for high resolutions like 2048x2048.
- It has lower MSE and comparable high frequency content (measured by High Pass Filter) compared to ToMe.
- Analysis shows latent features do exhibit spatial redundancy that can be exploited by downsampling.
- As a training-free approach, ToDo simplifies adoption for accelerating models.

In summary, the authors propose Token Downsampling, a fast and simple way to accelerate attention for image generation models by subsampling position tokens based on spatial contiguity. It outperforms prior work in balancing efficiency and fidelity, especially for high resolutions.
