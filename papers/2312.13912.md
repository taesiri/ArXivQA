# [Solving Long-run Average Reward Robust MDPs via Stochastic Games](https://arxiv.org/abs/2312.13912)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper considers the problem of solving robust Markov decision processes (RMDPs) with polytopic uncertainty sets under the long-run average reward objective. RMDPs are a generalization of Markov decision processes (MDPs) where instead of precise transition probabilities, each state-action pair is assigned an uncertainty set capturing multiple possible transition probability distributions. The goal is to find a policy that maximizes the worst-case expected long-run average reward over all possible choices of distributions from the uncertainty sets. Prior works on long-run average RMDPs mostly focus on RMDPs with specific types of uncertainty sets and do not provide complexity bounds or policy iteration methods.

Proposed Solution:
The key contribution is a polynomial-time reduction from long-run average polytopic RMDPs to long-run average turn-based stochastic games (TBSGs). This allows translating results on complexity and algorithms for TBSGs to the RMDP setting. Specifically:

1. The reduction shows the threshold problem for long-run average polytopic RMDPs is in NP âˆ© coNP.

2. It yields a randomized sub-exponential time algorithm for solving these RMDPs by reduction to simple stochastic games.

3. It enables a policy iteration method called Robust Polytopic Policy Iteration (RPPI) for solving these RMDPs, based on policy iteration for discounted TBSGs. RPPI does not require structural assumptions like unichain or aperiodicity.

Experiments show RPPI achieves significant efficiency gains compared to value iteration baselines from prior work. RPPI can also solve multichain RMDPs to which prior methods are not applicable.

Overall, the reduction to TBSGs allows translating a range of existing complexity and algorithmic results to the long-run average polytopic RMDP setting. This leads to new insights on the complexity of these RMDPs and more efficient solution methods compared to the state-of-the-art.
