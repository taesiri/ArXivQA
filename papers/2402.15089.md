# [AttributionBench: How Hard is Automatic Attribution Evaluation?](https://arxiv.org/abs/2402.15089)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating whether the evidence supports the claims made in responses generated by large language models (LLMs) is an important but challenging problem, known as attribution evaluation. 
- Doing this evaluation manually via human annotation is expensive and time-consuming. There is a need for accurate and efficient automatic attribution evaluation methods.
- However, existing works define and evaluate this task differently, making comparisons difficult. There are no standardized benchmarks to systematically assess and compare models on this task.

Proposed Solution - AttributionBench Benchmark:  
- The paper proposes AttributionBench, a comprehensive benchmark for evaluating automatic attribution models.
- It includes data sampled from 7 existing datasets, covering diverse questions across domains. 
- The task is formulated as binary classification for claim-level attribution, unifying previous formulations.
- Both in-distribution (ID) and out-of-distribution (OOD) test sets are included to test model effectiveness and generalizability.

Experiments and Analysis:
- Extensive experiments show even strong models like fine-tuned GPT-3.5 only achieve ~80% F1 score.
- Analysis of 300+ error cases reveals main challenges: models overlook fine-grained details, and have different information than human annotators.  

Main Contributions:
- First standardized benchmark for automatic attribution evaluation
- Extensive experiments showing attribution evaluation remains challenging
- In-depth analysis providing insights into limitations of models on this task and potential future directions
