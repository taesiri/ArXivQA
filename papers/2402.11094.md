# [Word Embeddings Revisited: Do LLMs Offer Something New?](https://arxiv.org/abs/2402.11094)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Learning meaningful word embeddings is fundamental for building robust language models. With the rise of Large Language Models (LLMs), it is unclear if their performance gains are simply due to larger scale or if the embeddings they produce are fundamentally different from classical embedding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE).

Proposed Solution: 
- The paper systematically compares LLM-based word embeddings to classical word embeddings by analyzing:
  1) Cosine similarity distributions of semantically related, morphologically related and unrelated word pairs
  2) Performance on word analogy tasks using the Bigger Analogy Test Set (BATS)

Key Findings:

- LLMs tend to have higher expected cosine similarity between random word pairs than classical models. 
- For capturing semantic similarity, LLMs are not always better than classical models. SBERT performs on par or better than some LLMs.
- On word analogy tasks, LLMs like Ada and PaLM significantly outperform classical models. SBERT has the 3rd best performance, suggesting it could be an efficient alternative when resources are constrained.
- Surprisingly, SBERT embeddings correlate strongly with Ada and PaLM embeddings, despite the huge difference in model scale.

Main Contributions:

- First systematic comparison between LLM & classical word embeddings
- Analysis of cosine similarity distributions sheds light on how differently these models space words
- Evaluation on word analogy task provides concrete performance comparison
- Finding that SBERT correlates with LLMs suggests it could be an efficient alternative to LLMs

In summary, the key insight is that while LLMs can learn improved semantics over classical models, SBERT produces surprisingly comparable embeddings despite its small scale. So SBERT presents a compelling choice when computational resources are limited.
