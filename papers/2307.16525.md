# [Transferable Decoding with Visual Entities for Zero-Shot Image   Captioning](https://arxiv.org/abs/2307.16525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How to effectively adapt large pre-trained vision-language models (VLMs) like CLIP and large language models (LLMs) like GPT into the generative task of zero-shot image captioning while maintaining strong generalization ability?Specifically, the authors identify two key challenges when combining VLMs and LLMs for zero-shot image captioning:1) Modality bias: The language priors in LLMs tend to dominate, resulting in generated captions that are unrelated to the image content. 2) Object hallucination: The soft visual prompts learned from limited training data cause the model to hallucinate objects that do not exist in unseen test images.To address these issues, the paper proposes an entity-aware decoding approach called ViECap that incorporates explicit entity-aware hard prompts to guide the LLM's attention to visual entities actually present in the image. This is aimed at improving the transferability and generalization ability of zero-shot captioning models to novel test images.In summary, the central hypothesis is that using entity-aware hard prompts to complement soft visual prompts can reduce modality bias and object hallucination issues in VLM+LLM based zero-shot image captioning. The proposed ViECap model is designed to test this hypothesis.


## What is the main contribution of this paper?

Here is a summary of the main contributions of this paper:- The paper sheds light on the underlying reasons behind the degraded generalizability when adapting pre-trained VLMs and LLMs into image-to-text generation tasks. Specifically, the authors identify two issues - modality bias and object hallucination - that appear when using CLIP and GPT for zero-shot image captioning.- To address these issues, the authors propose an entity-aware decoding approach called ViECap. The key idea is to incorporate entity-aware hard prompts, constructed using nouns extracted from text or entities retrieved from images via CLIP, to guide the language model GPT-2 during caption generation. This allows attending to both seen and unseen entities. - The authors introduce an entity masking strategy to prevent ViECap from learning a trivial copy-paste shortcut during training when integrating entity-aware hard prompts.- Extensive experiments demonstrate ViECap's state-of-the-art cross-domain transferability in zero-shot image captioning as well as competitive performance in in-domain captioning compared to previous methods. ViECap also proves to be data-efficient.In summary, the main contributions are identifying issues that limit pre-trained model adaptation, proposing entity-aware decoding to address these issues, and providing extensive empirical evidence showing ViECap's effectiveness for zero-shot image captioning across diverse scenarios.
