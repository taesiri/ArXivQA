# [Can LLMs' Tuning Methods Work in Medical Multimodal Domain?](https://arxiv.org/abs/2403.06407)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) require costly fine-tuning to adapt them to specialized domains like medicine. Global fine-tuning of large models is computationally expensive and can impact generalization.
- There is a need for efficient fine-tuning methods for small-scale vision-language pretrained (VLP) models in medicine to make them accessible to more researchers with limited compute resources. 

Proposed Solution:
- The paper designs a Modularized medIcal vision-Language fine-tuning modEl (MILE) by incorporating various parameter-efficient fine-tuning (PEFT) modules into a medical VLP model. 
- It systematically explores PEFT methods from LLMs like adapter tuning, LoRA, prefix tuning etc. and develops corresponding modules in MILE.
- It also proposes an instruction-format medical image-text dataset to study the impact of instruction tuning.
- Extensive experiments are conducted to analyze different PEFT methods' effects on medical VLMs.

Key Contributions:
- Provides empirical analysis and comparison of diverse PEFT methods tailored for small-scale medical VLMs, offering insights distinct from large models.
- Reveals the different effects of tuning the parameters in various modules of medical VLMs.
- Finds LoRA and prefix tuning achieve the best results while reducing tuning costs by 40% vs. global tuning.  
- Shows directly fine-tuning small VLMs with instruction-format data hurts performance, but further tuning converged models can improve results.
- The analysis serves as a practical guide for efficiently fine-tuning medical VLMs under limited compute budgets.

In summary, the paper offers useful guidelines and insights on adapting PEFT techniques from large models to better optimize small-scale medical VLP tuning, facilitating their broader application in healthcare.
