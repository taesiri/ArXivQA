# [DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be   Better Context-aware Translators](https://arxiv.org/abs/2402.15200)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Traditional methods simply concatenate source context sentences and feed them to decoder-only LLMs for context-aware neural machine translation (NMT). However, this treats inter-sentence and intra-sentence contexts equally, despite their differences.  

- Intra-sentence context has richer parallel semantics with the target sentence and should be prioritized over inter-sentence context. Separately modeling them can prevent LLM attention misallocation.

- Long distance between inter-sentence context and target words can cause alignment issues during decoding.

Proposed Solution: 
- A novel Decoding-Enhanced Multi-Phase Prompt Tuning (DeMPT) approach that divides context-aware NMT into 3 phases:
   1) Inter-sentence context encoding
   2) Intra-sentence context encoding  
   3) Decoding

- Uses phase-specific trainable prompts to make LLM aware of differences between context types.

- Further utilizes a heuristic method to discriminately enhance utilization of inter- and intra-sentence contexts during decoding:
   - Concatenates decoding states with both context representations to predict next words
   - Combines predictions from 3 distributions using decoding states concatenated with: 1) only inter-context 2) only intra-context 3) both

Main Contributions:
- Proposes a multi-phase tuning approach to separately model inter- and intra-sentence contexts for better context utilization
- Introduces an enhanced decoding method to discriminately leverage both context types
- Experiments using llama-2-7b and bloomz-7b1-mt across 5 translation tasks demonstrate significant gains over strong baselines
- Analysis shows approach substantially enhances LLM's context-aware translation ability 

In summary, the paper makes decoder-only LLMs better at context-aware translation by explicitly handling differences between context types via multi-phase tuning and enhanced utilization in decoding.
