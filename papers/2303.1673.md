# [Near Optimal Memory-Regret Tradeoff for Online Learning](https://arxiv.org/abs/2303.1673)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the fundamental question of whether it is possible to obtain low regret in the online learning with experts problem using limited memory. The key hypothesis is that there exist algorithms that can achieve vanishing regret (i.e. regret that is o(T) where T is the number of rounds) while using only sublinear memory in the number of experts n and the number of rounds T. Specifically, the paper makes the following main contributions:1. For the oblivious adversary setting, the paper presents an algorithm that obtains ~O(sqrt(nT/S)) regret using only S memory, nearly matching the lower bound of Ω(sqrt(nT/S)) from prior work. This resolves the open question of whether vanishing regret is possible with limited memory against an oblivious adversary.2. For the adaptive adversary setting, the paper gives an algorithm that obtains ~O(sqrt(nT/S)) regret with ~O(sqrt(n)) memory, proving that sublinear memory suffices to obtain vanishing regret even against an adaptive adversary. 3. The paper also provides a new lower bound showing that Ω(sqrt(n)) memory is necessary against an adaptive adversary, proving the optimality of the proposed algorithm.Overall, this paper significantly advances our understanding of the memory-regret tradeoffs for online learning with experts, obtaining nearly tight upper and lower bounds in both the oblivious and adaptive settings. The techniques introduced, including robust pooling/pruning methods and novel regret amortization schemes, are important contributions.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It provides a new online learning algorithm against an oblivious adversary that achieves near-optimal memory-regret tradeoff. Specifically, the algorithm uses $\text{polylog}(nT)$ space and obtains $\tilde{O}(\sqrt{nT})$ regret, nearly matching the lower bound of Ω(\sqrt{nT/S}) proved in prior work. This resolves the open question on optimal memory-regret tradeoff posed by previous work. 2. It gives the first online learning algorithm against an adaptive adversary that achieves sublinear regret using sublinear space. In particular, the algorithm uses $\tilde{O}(\sqrt{n})$ space and achieves $o(T)$ regret.3. It provides a novel lower bound showing that $\tilde{\Omega}(\sqrt{n})$ space is necessary to obtain $o(T)$ regret against an adaptive adversary. This lower bound is based on a reduction to the direct sum problem in communication complexity.4. The techniques introduced, including the eviction rules for maintaining experts, interval regret guarantees, and the accounting scheme for amortizing regret, seem novel and potentially useful for other online learning problems.In summary, this paper significantly advances our understanding of the interplay between memory and regret in online learning against oblivious and adaptive adversaries. The memory-regret tradeoffs obtained are essentially optimal, resolving open questions from prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper provides improved algorithms and lower bounds for online learning with limited memory against oblivious and adaptive adversaries, nearly resolving the memory-regret tradeoff.


## How does this paper compare to other research in the same field?

This paper makes several important contributions to the field of online learning theory with limited memory:1. It presents new algorithms for online learning against oblivious and adaptive adversaries that achieve near-optimal memory-regret tradeoffs. Specifically, against an oblivious adversary the algorithm achieves Õ(nT/S) regret using S memory, nearly matching known lower bounds. Against an adaptive adversary, the algorithm achieves Õ(√nT/S) regret using S memory. 2. The algorithm against oblivious adversaries improves upon prior work by Peng and Zhang (2022) by using a more sophisticated eviction strategy and analysis. Key innovations include a "covering benchmark" for deciding which experts to evict, as well as a robust eviction procedure that is insensitive to changes in the expert pool. 3. The algorithm against adaptive adversaries is novel, as prior work focused on oblivious adversaries. The high level approach of maintaining a pool of "random" and "long-term" experts is new in this context. The analysis relating the performance of random and long-term experts is also novel.4. The lower bound against adaptive adversaries of Ω(√n/ε) memory for o(ε^2T) regret is new. The reduction to a communication problem and application of direct product theorems is an interesting proof technique in this setting. 5. The work further develops the nascent area of understanding memory-regret tradeoffs in online learning. It significantly improves tradeoffs for both oblivious and adaptive adversaries compared to prior work, and leaves open many interesting questions on tight tradeoffs in other settings.In summary, this paper pushes forward the state-of-the-art in memory-limited online learning through new algorithmic ideas and lower bound techniques. The near tight bounds obtained are a major advance over prior work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Applying the ideas to develop sub-linear space algorithms for other online learning problems and applications where experts advice is used. The authors suggest it is interesting to see if their techniques can lead to better space-bounded algorithms for multi-arm bandits, pure exploration, game playing agents, optimization algorithms, etc. that rely on experts.- Understanding the memory-regret tradeoffs with additional structure on the experts or loss functions. The general worst-case bounds may not be tight if there is extra structure like small Littlestone dimension for the experts or succinct representation for loss vectors. - Developing algorithms that work in more restrictive models like bandit feedback. The current work relies heavily on full-information feedback so extending the techniques to bandit settings is an open direction.- Improving the space complexity and regret bounds. There is still a gap between the upper and lower bounds so closing this gap in terms of dependence on key parameters is an open problem.- Generalizing the adaptive adversary results. The adaptive bounds currently hold for the standard adversary but extending them to other notions like the stronger adaptive adversary is an open question.- Applying the techniques for online learning with restrictions on memory to other machine learning problems with limited memory, like continual learning.So in summary, the main directions are: exploring applications to other online learning settings, incorporating structural assumptions, considering bandit feedback, tightening bounds, handling stronger adversaries, and connections to broader machine learning problems with memory constraints. The combination of online learning and space-bounded computation seems like a fruitful area for future work.
