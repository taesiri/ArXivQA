# [Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative   Filtering](https://arxiv.org/abs/2402.11523)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Collaborative filtering (CF) models suffer from data sparsity issues. Recent works have tried using contrastive learning to introduce self-supervised signals and alleviate this. 
- However, contrastive learning risks distancing an anchor user/item from their collaborative neighbors, which are potentially useful for recommendations. This limits the efficacy of contrastive learning.

Proposed Solution:
- Treat the collaborative neighbors of an anchor node as positive samples when constructing the contrastive loss. This ensures representations of the anchor and its useful collaborative neighbors stay close.
- Propose two supervised contrastive loss functions, NESCL-in and NESCL-out, that effectively incorporate multiple positive samples - anchor node's two augmented views and its collaborative neighbors.
- Theoretically analyze the loss functions to show how different positive samples simultaneously influence the anchor node's representation update, with their impact depending on the anchor and negative samples.

Contributions:
- Proposed model with novel loss function that leverages multiple positive samples outperforms state-of-the-art contrastive CF method SGL by 10-35% on NDCG.
- Found smaller temperature values help amplify benefits of incorporating collaborative neighbors as positive samples, countering negative impact of any false negatives. 
- Evaluated different strategies of selecting collaborative neighbors as positive samples. Show the efficacy and robustness of the proposed loss functions in flexibly accommodating multiple neighbor types.

In summary, the paper develops a supervised contrastive learning approach specialized for CF that treats collaborative neighbors as positives. This helps alleviate issues in prior contrastive CF methods. Theoretically and empirically it demonstrates the advantages of its proposed loss functions.
