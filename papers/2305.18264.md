# [Gen-L-Video: Multi-Text to Long Video Generation via Temporal   Co-Denoising](https://arxiv.org/abs/2305.18264)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a novel paradigm called "Gen-L-Video" for generating and editing long videos conditioned on multiple text prompts using existing short video diffusion models. The central hypothesis is that the denoising path of a long video can be approximated by jointly denoising overlapping short video clips in the temporal domain. The key research questions addressed are:1) How to extend off-the-shelf short video diffusion models to generate long videos with hundreds of frames and diverse semantic segments without requiring additional training?2) How to enable text-driven video generation models to accept multiple text prompts to guide the generation of videos with varying content over time? 3) How to ensure content consistency across the generated long video that contains multiple semantic segments?To address these challenges, the paper proposes "temporal co-denoising", which treats a long video as overlapping short clips and approximates joint denoising of these clips to model the denoising path of the full video. This allows leveraging existing short video diffusion models for long video generation without retraining. The method also supports multi-text guidance by interpolating conditions between labeled clips. Experiments show this approach generates consistent and coherent long videos with diverse semantics.In summary, the central hypothesis is generating long, multi-prompt videos by co-denoising overlapping short clips with existing models. The key contributions are developing the temporal co-denoising formulation and demonstrating its effectiveness across various generation settings without requiring additional training data or models.
