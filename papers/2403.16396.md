# [Is There a One-Model-Fits-All Approach to Information Extraction?   Revisiting Task Definition Biases](https://arxiv.org/abs/2403.16396)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper identifies two types of "definition bias" in information extraction (IE) tasks: bias among IE datasets, and bias between IE datasets and instruction tuning datasets. 
- Definition bias refers to differences in how concepts, entities, or relationships are annotated across datasets, even when using the same annotation schema. This causes problems when transferring models across datasets.
- Definition bias is a key reason why model performance remains subpar on IE tasks compared to other NLP tasks.

Proposed Solution
- The paper proposes a multi-stage framework to measure and mitigate definition bias in IE tasks. The key components are:
  - Definition bias measurement using Fleiss' Kappa to quantify dataset and type biases
  - Bias-aware fine-tuning of large language models (LLMs) on diverse IE datasets, weighting training instances by measured bias
  - Task-specific bias mitigation using Low-Rank Adaptation to align the LLM with target dataset specifics

Key Contributions
- Identifies and defines two types of bias hindering IE model transferability 
- Systematically probes definition bias via cross-dataset validation, source prompt detection, and LLM few-shot prompting
- Proposes a novel bias measurement technique tailored to IE datasets
- Introduces a multi-stage tuning framework to reduce definition bias' impact by bias-aware training and adaptation 

The paper makes important contributions in analyzing the problem of definition bias in information extraction, which has been a key bottleneck for model performance. The proposed solution offers concrete techniques to measure and account for annotation differences across datasets during model training.
