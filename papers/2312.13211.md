# [DSFormer: Effective Compression of Text-Transformers by Dense-Sparse   Weight Factorization](https://arxiv.org/abs/2312.13211)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large transformer models like BERT are very expensive to deploy due to their huge size. 
- Existing compression techniques like knowledge distillation, quantization, and low-rank factorization have limitations in compression ratio or accuracy.
- Low-rank factorization suffers from over-restrictive assumptions that limit model expressiveness.

Proposed Solution:
- Proposes a dense-sparse (DS) factorization scheme that decomposes weight matrices into a small dense matrix and a semi-structured sparse matrix.
- The DS scheme approximates each weight vector as a sparse linear combination of basis vectors, providing better approximation.
- Also proposes a Straight-Through Factorizer (STF) to jointly learn the factorizations to maximize accuracy.

Main Contributions:
- DSFormer compression scheme achieves up to 40% better compression over low-rank methods with comparable accuracy.
- Outperforms state-of-the-art semi-structured sparsity methods in accuracy and compression.
- Orthogonal to distillation, quantization and can provide additional 50% compression.
- STF consistently improves accuracy over conventional factorization optimization.
- Evaluated on GLUE, SQuAD datasets; DSFormer matches BERT-Base performance at 2.8x compression.

In summary, the paper proposes a dense-sparse factorization scheme DSFormer and an optimization method STF for transformer compression. DSFormer provides superior compression over existing methods while being hardware-friendly. STF enables end-to-end joint optimization to improve accuracy. Together they advance state-of-the-art in efficient deployment of transformers.
