# [Is Feedback All You Need? Leveraging Natural Language Feedback in   Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2312.04736)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning (RL) agents struggle with sample efficiency and generalization compared to humans. One contributing factor is that humans receive richer feedback through natural language, allowing them to update their models of the world.

- Tasks specified in language may not have a sufficiently expressive reward function, or there may be a mismatch between the task and reward abstraction levels.

Solution:
- The paper proposes providing RL agents with automatic natural language feedback to facilitate learning more generalizable policies. 

- The BabyAI environment is extended to generate rule-based and task-based language feedback. Rule feedback explains failures due to violations of environment constraints. Task feedback tracks progress on subgoals related to the main task instruction.

- The Feedback Decision Transformer (FDT) architecture conditions action generation on sequences of observations, actions, rewards, and language feedback. It extends prior work on return-conditioned and instruction-conditioned transformers.

Contributions:
- Demonstrates language feedback can improve generalization over baselines, especially when replacing sparse rewards or instructions. Performs best on interpolating to new configurations and extrapolating to new relative goal locations.

- Shows performance gains from combining language feedback with instructions or rewards. Rule feedback complements rewards better. Task feedback complements instructions better.

- Provides analysis on model variants and feedback types across BabyAI levels and environment types. Feedback tends to be more useful for certain tasks depending on horizon, subgoals, etc.

- Proposes an automatic method for generating language feedback grounded in environment dynamics and goal specifications without human involvement. Could enable sim-to-real transfer.
