# [Token Transformation Matters: Towards Faithful Post-hoc Explanation for   Vision Transformer](https://arxiv.org/abs/2403.14552)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer":

Problem:
- Vision Transformers have shown superior performance in computer vision tasks, but their complexity makes their decision-making process hard to interpret. 
- Existing post-hoc explanation methods for Vision Transformers rely solely on attention weights to determine feature importance. However, they overlook crucial information from token transformations within the model.
- Solely using attention weights often fails to accurately highlight rationales and can misrepresent contributions from foreground objects or background regions.

Proposed Solution - TokenTM:
- Proposes a new post-hoc explanation method that utilizes token transformation measurement to faithfully interpret Vision Transformers.  
- Introduces a measurement to quantify transformation effects by evaluating changes in token lengths and correlations in their directions before and after transformation.
- Develops an aggregation framework to integrate attention weights and token transformation effects across all layers to capture the cumulative impacts of tokens throughout the model.

Main Contributions:
- Identifies issue with existing Vision Transformer explanation methods in comprehensively considering token transformations along with attention weights.
- Proposes a token transformation measurement to quantify effects of transformations by gauging changes in token lengths and directions.
- Establishes an aggregation framework to accumulate both attention and transformation information across multiple layers.
- Demonstrates superior performance of TokenTM over state-of-the-art methods on segmentation and perturbation tests.

In summary, the key innovation is in proposing a more comprehensive explanation method TokenTM that integrates attention weights with a new token transformation measurement to better reveal rationales behind Vision Transformer predictions.
