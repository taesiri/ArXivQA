# [May the Noise be with you: Adversarial Training without Adversarial   Examples](https://arxiv.org/abs/2312.08877)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine learning models are vulnerable to adversarial attacks, which can undermine their security and trustworthiness. Adversarial training (AT) is the state-of-the-art defense but requires complex min-max optimization with adversarial examples during training. Randomized smoothing defenses introduce randomness at inference time for robustness but require expensive simulations. 

Proposed Solution:
- The paper proposes a new defense that trains robust models without using adversarial examples. The key idea is to incorporate inherent stochasticity within the model during training. Specifically, Gaussian noise is injected into the first layer of the neural network. 

- The noise distribution is analytically modeled as it propagates through the layers, resulting in a closed-form stochastic loss function parameterized by the noise standard deviation. This enables optimization of the model parameters while accounting for stochasticity, without needing simulations.

- At inference time, the noise is withdrawn and the model provides deterministic predictions. The expectation of the stochastically trained model exhibits adversarial robustness.

Main Contributions:
- First defense to bridge adversarial training and randomized smoothing to achieve adversarial robustness without adversarial examples or inference simulations.

- Closed-form stochastic loss function and analytical noise-aware gradients for optimizing stochastic deep learning models.  

- Noise variance can be a learnable parameter that is jointly optimized during training. Experiments show the model converges to an "optimally stochastic" solution rather than zero noise.

- Empirical evaluations demonstrate the approach trains MNIST and CIFAR-10 models robust to PGD attacks. Robustness level mirrors the impact of noise magnitude in adversarial training.

- Demonstrates the closed-form loss can generate adaptive attacks against defenses relying on random noise at inference time.
