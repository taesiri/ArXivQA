# [On the Inductive Biases of Demographic Parity-based Fair Learning   Algorithms](https://arxiv.org/abs/2402.18129)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the potential biases induced by supervised learning algorithms that enforce demographic parity (DP) as a measure of fairness. DP requires the prediction output to be statistically independent of sensitive attributes like gender or race. The paper shows both theoretically and empirically that an imbalanced training data distribution on the sensitive attribute can lead DP-based learning algorithms to be biased towards the majority sensitive attribute group, even if accuracy may remain high.

Proposed Solution: 
To mitigate the biases of DP-based learning methods, the paper proposes a sensitive attribute-based distributionally robust optimization (SA-DRO) approach. Instead of optimizing over the empirical data distribution, SA-DRO minimizes the worst-case risk over a set of distributions close to the empirical sensitive attribute distribution. This makes the learning algorithm robust to changes in the frequencies of sensitive attribute values.

Main Contributions:
- Theoretically shows inductive biases of DP-based learning rules towards conditional label distribution of the majority sensitive attribute group, under common DP metrics like difference of DP and mutual information
- Empirically demonstrates on real-world datasets that DP regularization shifts conditional prediction distributions towards majority group despite maintaining accuracy
- Proposes SA-DRO method to reduce biases by optimizing worst-case risk over perturbations of sensitive attribute distribution  
- Shows SA-DRO can improve accuracy of minority clients in federated learning settings with heterogeneous sensitive attribute distributions

In summary, the paper provides an insightful analysis of potential biases of common DP-based fair learning methods, and introduces a principled distributionally robust optimization approach to alleviate such biases.
