# [Multi-Constraint Safe RL with Objective Suppression for Safety-Critical   Applications](https://arxiv.org/abs/2402.15650)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Reinforcement learning (RL) policies aim to maximize a task reward, but can exhibit dangerous behavior in safety-critical situations like autonomous driving if the reward function is not carefully designed. Most prior work in safe RL considers only a single safety constraint, but real-world settings often have multiple, potentially conflicting safety constraints that need to be satisfied. For example, an autonomous vehicle may need to satisfy constraints like avoiding collisions and maintaining safe distance from other vehicles/objects. Existing methods struggle with such multi-constraint scenarios - Lagrangian methods have difficulty tuning relative weights of different constraints, while hierarchical methods struggle to build multiple constraint-specific hierarchies.

Proposed Solution:
This paper proposes "Objective Suppression", a novel safe RL algorithm that adaptively suppresses the task reward maximization objective to satisfy safety constraints. The key ideas are:

1) Define indicator variables that track after how many timesteps a constraint is violated. Use these to switch between task reward objective and individual constraint objectives.

2) Replace the hard switching with a soft, probabilistic suppression of the task reward objective based on risk predicted by learned constraint critics. Higher predicted risk leads to more suppression.

3) Enforce the objective suppression along with existing hierarchical safe RL algorithms like Recovery RL. This combines multiple regimes of constraint enforcement.

Main Contributions:

1) A new method called Objective Suppression for enforcing safety constraints in RL by adaptively suppressing the task reward maximization objective.

2) Combining Objective Suppression with existing hierarchical safe RL methods to effectively handle multiple conflicting safety constraints.

3) Empirical demonstration in two multi-constraint environments - Safe Mujoco Ant and Safe Bench autonomous driving simulator. The proposed approach matches task reward of baselines while significantly reducing constraint violations demonstrating effectiveness for safety-critical applications.
