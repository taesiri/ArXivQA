# [ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation](https://arxiv.org/abs/2403.08737)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing machine learning approaches for local citation recommendation directly map a query (typically a claim or entity mention) to citation-worthy papers. This makes it difficult to explain why a specific paper should be cited for the query, limiting interpretability. 

Proposed Solution: 
The authors introduce a new task called "evidence-grounded local citation recommendation". The key idea is to retrieve evidence spans from existing literature that are similar to the query and cite relevant papers. This provides an explanation for each recommended paper. 

The proposed system, ILCiteR, works as follows:
1) Build an evidence database mapping evidence spans to cited papers and their citation counts. 
2) For a query, retrieve candidate evidence spans using lexical similarity.
3) Re-rank evidence spans using a conditional neural rank ensemble technique that utilizes semantic similarity for longer queries.  
4) Recommend papers grounded in the re-ranked evidence spans, considering span relevance, paper citations and recency.

Thus each recommended paper is paired with an evidence span to explain why it is relevant for the query. ILCiteR uses no explicit training, relying on pre-trained language models and distant supervision from the dynamic evidence database.

Contributions:
1) Introduce evidence-grounded citation recommendation task for interpretability
2) Release dataset with 200K evidence spans, cited papers and citation counts
3) Propose distantly supervised system ILCiteR needing no training
4) Demonstrate improved performance with conditional neural rank ensembling approach

In summary, the paper focuses on improving the interpretability of local citation recommendation by grounding recommendations in evidence spans. The proposed approach and dataset advance machine understanding of scientific literature.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a new task of evidence-grounded local citation recommendation, where citation recommendations are paired with evidence spans from existing literature to improve interpretability, and proposes a system called ILCiteR that uses distant supervision and does not require model training.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Introducing the task of evidence-grounded local citation recommendation, with a focus on improving recommendation interpretability by grounding paper recommendations in supporting evidence spans.

2) Contributing a new dataset with over 200,000 evidence spans and corresponding sets of cited papers and support counts, covering three subtopics in computer science.

3) Proposing a citation recommendation system called ILCiteR that uses distant supervision from a dynamic evidence database and pre-trained language models, without needing explicit model training. 

4) Demonstrating that a conditional neural rank ensembling approach for re-ranking evidence spans, which utilizes semantic similarity for longer queries, significantly improves downstream paper recommendation performance compared to lexical similarity-based methods.

In summary, the key innovation is formulating the novel task of evidence-grounded citation recommendation and showing how an approach leveraging distant supervision and conditional neural rank ensembling can effectively tackle this task. The focus is on improving the interpretability of paper recommendations by grounding them in supporting evidence.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key keywords and terms associated with it are:

- Local citation recommendation
- Evidence-grounded recommendation 
- Distant supervision
- Conditional neural rank ensembling
- Transformer language models
- Interpretability
- Recommendation system
- Evidence database
- Computer science subtopics (named entity recognition, summarization, machine translation)

These keywords capture the main focus of the paper which is on developing an evidence-grounded and interpretable approach to local citation recommendation. The method relies on distant supervision from an evidence database and leverages conditional neural rank ensembling with Transformer language models, without needing any explicit training. The evaluation is done on evidence databases built for three computer science subfields.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new task formulation called "evidence-grounded local citation recommendation". How is this task different from traditional local citation recommendation and why does it help with recommendation interpretability?

2. The paper proposes building an "evidence database" that maps evidence spans to cited papers and their supports. What is the specification of this evidence database and what kind of information does it contain? How is it created?

3. The paper presents a two-step ranking approach. What are these two steps and what does each step aim to achieve in the overall citation recommendation process?

4. The first step in the ranking approach uses "conditional neural rank ensembling". When and why does this approach utilize semantic similarity based on SciBERT in addition to lexical similarity? How does the conditional ensembling help improve performance?

5. The second step in the ranking ranks candidate papers based on the rank of their associated evidence span, their composite support, and recency. Can you explain how each of these components is formally defined and how they contribute to the final paper ranking?

6. The paper demonstrates the efficacy of the proposed approach on 3 CS subtopics. What are these topics and what do the key statistics of the created evidence databases for them look like?

7. The paper compares the proposed conditional neural rank ensembling approach against BM25 baselines. What evaluation metrics are used and how much does the proposed approach improve over BM25? Are the improvements statistically significant?

8. What ablations does the paper perform to analyze the contribution of different components like SciBERT and lexical similarity signals? What do these ablation studies demonstrate about the efficacy of conditional ensembling?  

9. The error analysis section shows 3 example failures of the proposed system. Can you summarize these failures cases and provide reasons why they happened? Do you think some of them could be addressed in future work?

10. The proposed system relies solely on an evidence database and pre-trained language models, without requiring any training. What are the advantages of this and how does it simplify integration with newly published papers compared to existing supervised approaches?
