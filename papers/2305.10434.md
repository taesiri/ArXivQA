# [Learning the Visualness of Text Using Large Vision-Language Models](https://arxiv.org/abs/2305.10434)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we automatically quantify the visualness (imageability) of text at the sentence level, and adapt vision-language models to distinguish visual sentences from non-visual ones given only text input? The key points are:- The paper introduces the task of predicting sentence-level visualness (imageability), which is important for applications like selecting relevant sentences to generate/retrieve images for. - The authors curate a new dataset TImeD of sentences labeled for visualness through crowdsourcing.- They propose a fine-tuning strategy to adapt vision-language models like CLIP to classify sentence visualness from text input alone, by contrasting visual text with actual images and non-visual text with a "NULL" image.- Experiments show their proposed TIP-CLIP model outperforms several baselines in accurately categorizing sentences as visual/non-visual. Analyses also demonstrate TIP-CLIP's utility for downstream generation.So in summary, the core research question is how to quantify sentence-level visualness and adapt vision-language models for this text-only task, which the paper addresses through a new labeled dataset and fine-tuning strategy.
