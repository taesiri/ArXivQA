# [Improving Diffusion Models's Data-Corruption Resistance using Scheduled   Pseudo-Huber Loss](https://arxiv.org/abs/2403.16728)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Diffusion models are vulnerable to outliers in the training data, which can negatively impact model performance. For example, image diffusion models can memorize outliers or be susceptible to backdoor attacks that degrade performance on specific prompts.

Proposed Solution:  
- Use a pseudo-Huber loss function instead of the standard L2 loss during diffusion model training. The pseudo-Huber loss is more robust to outliers compared to L2 loss.

- Introduce a time-dependent delta parameter in the pseudo-Huber loss to trade off between robustness against outliers early in training and preserving quality later in training. This is called "delta-scheduling".

Main Contributions:

- Propose using a pseudo-Huber loss with delta-scheduling for training diffusion models, making them more robust to outliers and corrupted data. 

- Experimentally demonstrate effectiveness across image and audio domains, using Dreambooth text-to-image generation and Grad-TTS few-shot speech synthesis as testbeds.

- Introduce a "resilience factor" R to quantify model robustness to corrupted data, computed from similarities between model samples and clean/corrupted reference data.

- Show that the proposed pseudo-Huber loss outperforms L2 loss in terms of resilience to corrupted data, especially early on during diffusion model training.

- Analyze performance across varying percentages of corrupted data, values of delta parameter, and loss schedules. Exponentially decreasing delta schedule works best.

The proposed pseudo-Huber loss requires no extra data filtering/purification compared to normal training, while making diffusion models more robust to outliers and corruption. This can help model creators train more reliable systems.
