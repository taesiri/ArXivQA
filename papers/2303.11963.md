# [NEMTO: Neural Environment Matting for Novel View and Relighting   Synthesis of Transparent Objects](https://arxiv.org/abs/2303.11963)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, this paper focuses on the problem of modeling and rendering transparent 3D objects with unknown refractive indices. The key hypotheses/questions it addresses are:

1) Can we develop an end-to-end neural rendering pipeline that can model both the complex geometry and appearance of transparent objects for tasks like novel view synthesis and relighting? 

2) Can we design a neural network module to better handle light refraction through the object compared to analytical/physics-based methods? This is critical as small geometry errors can significantly impact rendered views.

3) Can we leverage implicit neural representations like SDFs to model the geometry and a ray bending network for refractive effects to achieve high quality rendering without knowing the object's refractive index?

4) Will the proposed approach generalize well to real captured data of transparent objects compared to existing methods that often rely on controlled capture or synthesized training data?

In summary, the main research goals are developing an end-to-end neural approach for modeling and rendering transparent objects that can handle complex geometry, unknown refractive indices, and real world data by combining implicit geometry networks with a learned ray bending module for refraction. Key hypotheses are that this will improve over analytical refractive models and generalize better than existing learning-based methods.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing an end-to-end neural rendering pipeline called NEMTO to model and render novel views and relighting of 3D transparent objects with complex geometry and unknown refractive indices. 

Specifically, the key contributions are:

- Leveraging implicit SDFs to model the complex geometry of transparent objects.

- Proposing a Ray Bending Network (RBN) to model light refraction through the object by directly mapping input ray directions to output refracted ray directions. This is more tolerant to geometric inaccuracies than analytical refraction models.

- An end-to-end differentiable rendering module that combines reflected and refracted radiance based on Fresnel equations. 

- Demonstrating high quality novel view synthesis and relighting on both synthetic and real transparent objects, outperforming prior neural rendering techniques.

The main significance is developing an end-to-end learnable pipeline to model transparent objects with complex and unknown properties. This is useful for rendering transparent objects in mixed reality and other applications. The RBN in particular helps disentangle geometry from refractive appearance, improving performance over analytical models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes NEMTO, an end-to-end neural rendering pipeline to model transparent 3D objects with complex geometry and unknown refractive indices for high-quality novel view and relighting synthesis, using implicit functions to represent geometry and a physically-guided ray bending network to disentangle object shape and light refraction.


## How does this paper compare to other research in the same field?

 This paper proposes a novel end-to-end neural rendering approach for modeling and rendering transparent 3D objects with complex geometry. Here are some key ways it compares to prior work:

- Most prior work either assumes known object geometry and refractive indices, or uses controlled capture setups like turntables and structured lighting. This method requires only casual multi-view captures under natural illumination.

- It represents object geometry with implicit SDFs rather than explicit meshes. This allows modeling complex geometry adaptively. 

- It introduces a Ray Bending Network to predict light refraction through the object, instead of using analytical refraction models. This is more robust to geometric errors.

- It disentangles geometry and appearance better than prior neural rendering techniques like NeRF and PhySG. This allows higher quality novel view synthesis and relighting. 

- It places no constraints on light bounces within the object, unlike recent work that assumes two bounces. This enables handling more complex light transport effects.

- It requires no known refractive index, unlike some analytical methods. The network learns an implicit "index" to match refractions.

- It demonstrates results on both synthetic and real captured data. Most analytical methods are limited to synthetic data.

In summary, this paper pushes the boundary of neural rendering of transparent objects. It removes many constraints of analytical methods and prior learning techniques. The proposed approach produces higher quality rendering for this very challenging problem.
