# [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM   Fine-Tuning: A Benchmark](https://arxiv.org/abs/2402.11592)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) continue to grow in size, fine-tuning them using first-order (FO) optimizers like SGD and Adam incurs substantial memory overhead due to the need for backpropagation (BP). This memory inefficiency poses challenges especially for applications like on-device training where memory is limited.  

Proposed Solution: 
This paper proposes using BP-free, zeroth-order (ZO) optimization methods as a solution to reduce memory costs during LLM fine-tuning. The initial concept was introduced in Malladi et al. (2024), which employs ZO stochastic gradient descent (ZO-SGD). This paper conducts the first comprehensive benchmark study to explore a broader range of ZO optimization techniques for LLM fine-tuning.

Main Contributions:

- Creates the first benchmark for evaluating various ZO optimization methods for LLM fine-tuning, covering 5 LLM families, 3 task complexities, and 5 fine-tuning schemes.

- Reveals previously overlooked principles about ZO optimization for LLM fine-tuning, including the importance of task alignment, the role of forward gradient as a competitive baseline, and the tradeoffs between algorithm complexity, accuracy and efficiency.  

- Proposes techniques to further advance ZO optimization for LLM fine-tuning: (i) block-wise ZO optimization to reduce variance; (ii) hybrid ZO and FO training to balance performance and memory; (iii) sparsity-induced ZO optimization to improve accuracy.

- The proposed enhancements to incorporate ZO optimization provides pathways to achieve further memory-efficient LLM fine-tuning without compromising accuracy. The benchmark and optimization insights lay groundwork to facilitate adoption of ZO methods.
