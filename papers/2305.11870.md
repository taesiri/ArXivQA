# [Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D   Diffusion Probabilistic Models](https://arxiv.org/abs/2305.11870)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is:How can we generate realistic 3D clothed human avatars by leveraging recent advances in generative image models and differentiable rendering?The key hypotheses appear to be:1) By decomposing the 3D avatar generation problem into 2D normal map generation and subsequent normal map-based 3D reconstruction, the powerful image generation capabilities of diffusion models can be leveraged for this task. 2) Diffusion models can be adapted to generate consistent dual normal maps (front and back views) of clothed humans conditioned on body pose. These can provide sufficient information to reconstruct a plausible 3D human mesh.3) The generated normal maps and reconstructed mesh can be further refined through a resampling process using additional diffusion models, enhancing quality while preserving consistency. 4) By incorporating text-to-image diffusion models, the pipeline can be extended to allow text-based control over human identity and attributes.In summary, the paper proposes and evaluates a novel 3D human generation pipeline exploiting the complementary strengths of generative image models, differentiable rendering, and skinned shape models to address the challenging problem of creating varied, realistic digital human avatars.


## What is the main contribution of this paper?

Based on my reading of the abstract, the main contributions of this paper appear to be:1. Proposing a 3D generation pipeline that leverages 2D image generation capabilities of diffusion models for 3D reconstruction. 2. Presenting a diffusion-based normal map generation and refinement strategy to obtain view-consistent normal maps for 3D generation.3. Demonstrating a method to enable text-based 3D full-body digital avatar creation, providing an intuitive interface for digital avatar generation. Specifically, the paper decomposes the 3D generation problem into normal map generation using diffusion models and subsequent 3D reconstruction from those normal maps. It generates front and back "dual" normal maps in a consistent manner using a pose-conditional diffusion model. These normal maps are then used to deform an initial SMPL-X mesh into a detailed 3D human mesh through an optimization process. To further improve quality, the paper refines rendered multi-view normal maps using separate body and face diffusion models in a resampling process while preserving consistency. The refined maps are fed back into the optimization. Finally, the pipeline incorporates a text-to-image diffusion model to allow controlling avatar identity and properties like gender, clothing, etc. through text descriptions. The generative capabilities of diffusion models are effectively leveraged for consistent 2D normal map generation to enable high-quality 3D avatar creation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars by decomposing the problem into normal map generation and subsequent 3D reconstruction, allowing control over pose and identity while producing high-quality results.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work:- This paper proposes generating 3D clothed human meshes by decomposing the problem into 2D normal map generation using diffusion models, followed by 3D reconstruction from the normal maps. This approach leverages the power of 2D generative models while ensuring 3D consistency. Other 3D generative models like gDNA directly generate 3D shapes which can lack realism.- For normal map generation, this paper uses a novel dual normal map scheme to generate consistent front and back maps. This avoids the multi-view consistency issue faced by 3D generative models. Other papers like DiffuStereo use diffusion models just for detail refinement rather than full generation.- For reconstruction, this paper carves an initial SMPL-X mesh using normal map optimization and differentiable rendering. This leverages the topological priors of SMPL-X unlike image-based reconstruction methods like PiFu/PiFuHD. Other hybrid methods like ICON use SMPL-X more implicitly as a guidance rather than carving it.- The method supports conditioning on both pose and free-form text prompts to control the human identity. This enables intuitive avatar creation compared to just pose-based methods. Other text-based models like AvatarClip lack realism or are limited to faces/upper-bodies.- The evaluations show the approach produces higher quality 3D humans than gDNA as per FID metrics and user studies. The ablations also validate the components like dual normal generation, mesh refinement etc.In summary, the key novelty is in effectively combining 2D generative models with 3D shape reconstruction and conditioning for high quality and controllable digital human generation. The experiments demonstrate state-of-the-art performance.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the paper:- Developing methods to address the depth ambiguity issue in the dual normal map-based mesh reconstruction. They note this can lead to artifacts like broken geometry in areas like long hair. - Improving the handling of misaligned face directions during input. They observe this can lead to unnatural face geometry in the output.- Making the method more robust to out-of-distribution poses. They find some poses lead to unrealistic normal maps and bad mesh geometry.- Enhancing the text-to-3D pipeline such that accessories generated by the text-to-image model like bracelets and necklaces can be expressed in the final mesh. Currently these details tend to get lost.- Adding photorealistic texture support and animation capabilities to bring the digital avatars to life.- Exploring novel strategies to create animations from the generated digital avatars.In summary, the main future directions are improving robustness in cases like uncommon poses and misalignments, adding finer details like accessories, textures and animations, and addressing limitations in the 3D reconstruction process that can lead to artifacts. Overall the goal is to enhance the quality, capabilities and robustness of the system for generating controllable digital human avatars.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. The key idea is to decompose the 3D generation problem into 2D normal map generation and subsequent 3D reconstruction from the normal maps. Specifically, the method first uses a pose-conditional diffusion model to generate consistent normal maps of the front and backside of a human mesh (dubbed dual normal maps). These dual normal maps are then used as reference to deform an initial SMPL-X mesh into a detailed 3D human mesh through a differentiable mesh optimization process. To further enhance high-frequency detail, the method presents a diffusion resampling scheme on both body and facial regions of the reconstructed mesh. This refinement of rendered multi-view normals improves realism while preserving consistency. Finally, the pipeline incorporates a text-to-image diffusion model to allow text-based control over human identity factors like gender, clothing, etc. In experiments, the proposed method, Chupa, is shown to generate realistic and diverse 3D clothed humans superior to previous state-of-the-art baselines.
