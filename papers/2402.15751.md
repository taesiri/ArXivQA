# [Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM   Fine-Tuning](https://arxiv.org/abs/2402.15751)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning large language models (LLMs) for specific tasks requires substantial memory due to backpropagation and activation caching during training. This limits scalability and accessibility.
- Recent memory-efficient zeroth-order (MeZO) optimizers only require forward passes, reducing memory usage. However, MeZO still exhibits significant performance drops compared to standard fine-tuning.

Proposed Solution:
- The paper proposes Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies zeroth-order optimization only to a carefully chosen subset of parameters. 

- A simple yet effective parameter selection scheme is proposed that selects smaller magnitude parameters, showing they are more important for zeroth-order fine-tuning.

- An memory-optimized implementation is developed that computes the sparse mask and perturbs parameters on-the-fly during the forward pass. This reduces memory to inference-level.

Main Contributions:
- Proposes Sparse MeZO method for memory-efficient zeroth-order LM fine-tuning using only a subset of parameters

- Provides theoretical analysis proving the faster convergence of Sparse MeZO

- Shows smaller parameters are more important than larger ones for zeroth-order fine-tuning

- Develops memory-optimized implementation that enables fine-tuning models as large as LLaMA-30B on a single GPU

- Experiments show Sparse MeZO improves performance and convergence speed over MeZO on SuperGLUE tasks, achieving 9% higher accuracy and 3.5x speedup on RTE task.

In summary, the paper introduces an efficient sparse zeroth-order fine-tuning approach that achieves better performance and faster convergence without overhead, enabling large LM tuning with only inference-level memory.
