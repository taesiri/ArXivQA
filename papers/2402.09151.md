# [Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for   Chinese Mental Health Text Analysis](https://arxiv.org/abs/2402.09151)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Mental health issues like depression are prevalent globally and in China. Individuals facing such challenges often resort to social media to express their struggles.  
- There is a need for models that can efficiently analyze mental health content on Chinese social media to enable early detection and intervention.
- While pretrained language models have shown promise, there is a lack of models tailored for the psychology domain.

Proposed Solution:
- Collected a large-scale Chinese dataset from social media with over 3.36 million entries related to mental health.
- Propose Chinese MentalBERT - a model adapted from an existing Chinese BERT using continued pretraining on the collected mental health corpus.
- Introduce a novel lexicon-guided masking strategy that leverages a depression lexicon to mask relevant words during pretraining. This guides the model to focus more on vocabulary crucial for mental health analysis.

Main Contributions:
- Created the first pretrained language model specifically for the Chinese mental health domain.
- Collected a large-scale Chinese mental health corpus from social media for model pretraining.
- Proposed a lexicon-guided masking approach to inject domain knowledge into the model's pretraining.
- Evaluated the model on 4 public datasets and showed improved performance over baseline models in tasks like cognitive distortion classification, suicide risk assessment and sentiment analysis.
- Made the pretrained model and codes publicly available to advance research in Chinese mental health.

In summary, the paper develops a tailored pretrained model for automated analysis of mental health content in Chinese through domain-adaptive pretraining and a masking strategy focused on relevant vocabulary. Experiments demonstrate its effectiveness on related tasks.
