# [MT3: Multi-Task Multitrack Music Transcription](https://arxiv.org/abs/2111.03017v4)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: Can a general-purpose Transformer model perform multi-task automatic music transcription, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets?

The key points related to this research question are:

- Most prior work in automatic music transcription (AMT) has developed specialized models tailored to transcribing specific instruments or datasets. This paper explores using a single Transformer model architecture for multi-task AMT.

- The authors propose a unified framework to train a Transformer model on multiple AMT datasets with different combinations of instruments. This includes a flexible tokenization scheme to represent notes from various instruments.

- They assemble a collection of 6 diverse AMT datasets and define consistent evaluation metrics to benchmark multi-task AMT performance.

- Their model, MT3, achieves state-of-the-art results on each individual dataset, outperforming prior specialized models and commercial software. It also shows large gains on low-resource datasets when trained jointly.

- The model is robust to different instrument groupings and shows strong instrument identification capabilities.

In summary, the central hypothesis is that a single general Transformer can achieve strong performance on multi-task multitrack music transcription across diverse datasets and instruments, which they demonstrate empirically in their experiments. The paper presents MT3 as a strong baseline for this new direction in AMT research.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Defining a unified framework and tokenization scheme for multi-task, multitrack music transcription that supports transcribing arbitrary combinations of instruments across different datasets. This allows training a single model on a mixture of datasets.

2. Assembling a benchmark collection of 6 multitrack datasets spanning different sizes, styles, and instrumentations to enable multi-task training. This is the largest known publicly available collection for this purpose.

3. Defining standard test splits and evaluating models with consistent metrics (frame F1, onset F1, onset-offset F1, and a new multi-instrument F1) across all datasets.

4. Training a T5 Transformer model within their framework that establishes a new state-of-the-art baseline on all 6 datasets, outperforming prior specialized models and commercial software.

5. Demonstrating that their multi-task training approach dramatically improves performance on low-resource datasets, while maintaining strong performance on high-resource datasets. For example, on the URMP dataset, multi-task training gives a 263% relative gain in onset-offset F1 score.

In summary, the main contribution is developing a unified training framework and model architecture that can perform multi-instrument music transcription across diverse datasets, advancing the state-of-the-art, especially for low-resource datasets. The paper also contributes new datasets and evaluation procedures to enable further research in this direction.
