# [Shallow Cross-Encoders for Low-Latency Retrieval](https://arxiv.org/abs/2403.20222)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer-based cross-encoders achieve state-of-the-art effectiveness for text retrieval but are computationally expensive, resulting in high latency during inference. 
- High latency negatively impacts user satisfaction, revenue, and energy usage.
- Existing methods to reduce latency like knowledge distillation are complex and require training ensemble teacher models.

Proposed Solution:
- Use shallow cross-encoders (with fewer transformer layers) which require less computation and can score more documents within a latency budget.  
- Propose gBCE training scheme: increased negative samples per query and a generalised binary cross-entropy loss to mitigate overconfidence.
- Show shallow cross-encoders outperform full-scale models under low latency constraints (<50ms per query) despite lower per-document effectiveness.

Key Contributions:
- Propose simple and replicable gBCE training method for shallow cross-encoders without reliance on knowledge distillation.
- Analyze efficiency/effectiveness tradeoff showing shallow cross-encoders are more effective in low-latency scenario.
- Demonstrate shallow cross-encoders achieve good effectiveness even with CPU-only inference, useful for on-device search.  
- Shallow TinyBERT-gBCE model achieves +51% higher NDCG@10 compared to MonoBERT-Large given 25ms latency limit per query on TREC-DL 2019 dataset.

In summary, the paper demonstrates shallow cross-encoders trained with gBCE provide an effective and efficient solution for low-latency text retrieval compared to state-of-the-art but computationally heavier models.
