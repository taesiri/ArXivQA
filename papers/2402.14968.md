# [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)

## What is the main contribution of this paper?

 This paper proposes a novel defense method called Backdoor Enhanced Alignment to mitigate the threat of Fine-tuning Jailbreak Attacks (FJAttacks) against large language models with limited safety examples. The key ideas are:

1) Drawing an analogy between FJAttacks and standard backdoor attacks, where a small amount of poisoned data can effectively train models to exhibit certain malicious behaviors. 

2) Constructing prefixed safety examples by integrating a secret prompt as a "backdoor trigger". When fine-tuned together with malicious examples, this establishes a strong correlation between the secret prompt and safety responses.

3) During inference, prepending this secret prompt to activate safe responses, defending against FJAttacks effectively even with very few (e.g. 11) safety examples.

In experiments, Backdoor Enhanced Alignment reduces attack success rates substantially compared to baseline defense methods, while preserving performance on benign tasks. The method is also shown to generalize to defend against FJAttacks in practical fine-tuning scenarios.

In summary, the key contribution is an efficient and effective defense against FJAttacks by strategically utilizing limited safety data in an innovative way inspired by backdoor attacks.
