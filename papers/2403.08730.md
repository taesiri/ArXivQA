# [Strengthening Multimodal Large Language Model with Bootstrapped   Preference Optimization](https://arxiv.org/abs/2403.08730)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multimodal large language models (MLLMs) excel at generating responses based on visual inputs, but often suffer from bias towards responses similar to their pretraining corpus, overshadowing visual information. 
- This bias acts as an inherited "preference" for the pretraining statistics, which hinders grounding responses in the visual inputs.

Proposed Solution:  
- Introduce bootstrapped preference optimization (BPO) to conduct preference learning using datasets with negative responses bootstrapped from the MLLM itself.
- Generate negative responses in two ways: 
   1) Distort image inputs to reveal pretraining bias  
   2) Inject common but erroneous elements into original responses using the LLM component
- Construct preference dataset pairing original responses (positive) with bootstrapped responses (negative).
- Optimize the preference dataset using direct preference optimization (DPO).

Key Contributions:
- Formulate multimodal alignment as a preference learning problem between pretraining bias ("old preference") and visual grounding ("new preference").
- Propose scalable strategies to automatically construct preference datasets at scale without manual annotation.
- Introduce variant of DPO suitable for MLLMs to suppress pretraining bias and enhance visual grounding. 
- Achieve significant performance improvements across multiple benchmarks, advancing state-of-the-art in multimodal conversational systems.

The summary covers the key problem being addressed, the proposed bootstrapped preference optimization solution, and highlights the main contributions around formulating a new preference learning perspective for multimodal alignment, scalable preference dataset construction, and advancing state-of-the-art performance.
