# [Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple   Logits Retargeting Approach](https://arxiv.org/abs/2403.00250)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Long-tailed recognition (LTR) deals with imbalanced training datasets where a few majority classes have abundant samples while many minority classes have scarce samples. This causes models to be biased towards majority classes.
- Previous work on LTR has focused on simultaneously improving representation learning and classifier retraining stages, making it hard to isolate the impact of classifier retraining. 
- Recent work has shown simple regularization can achieve strong feature representations, emphasizing the need to reassess classifier retraining methods.

Proposed Solution:
- Revisit various classifier re-training methods based on a unified feature representation and update benchmark results for fair comparison.
- Propose two new metrics - "Logits Magnitude" and "Regularized Standard Deviation" to effectively evaluate model performance by analyzing the distribution of logits values across different methods and classes. 
- Theoretically analyze the relationship between the metrics and model performance. Find that reducing the absolute logits magnitude when it is nearly balanced can decrease errors and improve accuracy.
- Propose a simple logits retargeting approach (LORT) to directly optimize this objective by dividing the one-hot label into small true label probabilities and large negative label probabilities distributed to each class.

Main Contributions:
- Provide updated experimental results and analysis to enable fair assessment of various classifier re-training methods.  
- Introduce two new metrics that offer insights into model performance based on logits distribution.
- Demonstrate both theoretically and empirically that deliberately reducing balanced logits magnitude can improve performance.
- Achieve state-of-the-art results on CIFAR100-LT, ImageNet-LT and iNaturalist with the proposed LORT method.

In summary, the paper conducts a rigorous re-evaluation of classifier retraining methods, proposes helpful analysis metrics, derives insights to guide method design, and introduces a simple yet effective technique for LTR.
