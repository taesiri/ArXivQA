# [Weakly Supervised Monocular 3D Object Detection using Multi-View   Projection and Direction Consistency](https://arxiv.org/abs/2303.08686)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to train monocular 3D object detection models using only 2D image labels, without needing 3D point cloud data for labeling. 

The key points are:

- Most current monocular 3D detection methods rely on 3D point clouds to label the ground truth boxes for training, but this causes an inconsistency between training (uses 3D data) and inference (only 2D images). 

- The authors propose a new weakly supervised framework that only uses 2D labels like 2D bounding boxes to train the models. This allows utilizing large-scale 2D-labeled data like feedback images from production cars.

- They introduce three types of consistency between 2D labels and 3D predictions: projection consistency, multi-view consistency, and direction consistency. Losses are designed based on these to guide the model optimization.

- A new 2D direction labeling method is proposed to replace the 3D rotation labeling. This further avoids needing 3D point clouds. 

- Experiments show their method achieves comparable or better performance than some fully supervised methods. It also significantly boosts the baseline model when used for pre-training with a small amount of 3D labels.

In summary, the key hypothesis is that by designing losses based on 2D-3D consistency, they can train 3D detection models without relying on 3D point cloud labeling data.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new weakly supervised method for monocular 3D object detection that only requires 2D labels marked on images, without needing any 3D point clouds for labeling. This allows the method to be trained on images from production cars that lack 3D labels. 

2. It incorporates projection consistency and multi-view consistency losses to guide the prediction of accurate 3D bounding boxes based on relationships between 2D and 3D data.

3. It proposes a new 2D direction labeling method to replace 3D rotation labels, and a direction consistency loss to optimize rotation prediction. 

4. Experiments show the method achieves comparable performance to some fully supervised methods on KITTI. When used for pre-training with a small amount of 3D labels, it significantly outperforms the fully supervised baseline.

5. The method's ability to work with only 2D labels makes it feasible to use large amounts of feedback image data from production cars to improve robustness and generality of models. This is a key advantage over methods that require 3D labels.

In summary, the main novelty is the weakly supervised framework and losses using only 2D labels, which enables using production car data lacking 3D annotations to improve monocular 3D detection models. The consistency losses are designed to provide supervision without 3D labels.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a weakly supervised monocular 3D object detection method that trains models using only 2D image labels, without needing 3D point cloud data for labeling, by exploring projection, multi-view, and direction consistency between 2D labels and 3D predictions.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in monocular 3D object detection:

- The key contribution of this paper is proposing a weakly supervised method that only requires 2D image labels, without needing 3D point clouds for labeling. This allows the model to be trained on readily available 2D annotated images, rather than relying on more expensive 3D point cloud annotations. This is a novel approach in the field.

- Most prior work in monocular 3D detection uses full 3D supervision during training. This includes methods like MonoGRNet, M3D-RPN, MonoDLE, etc. So the weakly supervised approach proposed here is quite different.

- There has been some recent work exploring weak supervision for 3D detection, but those methods still rely on LiDAR data during training (e.g. WeakM3D, Autolabels). The key difference is that this paper proposes a method completely free of LiDAR data.

- The consistency losses proposed in this paper to enable the weak supervision are also novel, especially the use of multi-view consistency and direction consistency losses.

- In terms of performance on the KITTI benchmark, the results are very competitive to prior full supervision methods. The method even outperforms some fully supervised techniques.

- The ability to train on 2D-only annotated images could enable new applications by leveraging readily available image datasets, without needing 3D labels. This could help with scalability and use of production car feedback data.

In summary, the weakly supervised approach proposed in this paper is quite unique compared to prior work, and has the potential to expand the applicability of monocular 3D detection. The consistency losses and overall framework offer new ideas for training without 3D supervision.
