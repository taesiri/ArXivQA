# [Weakly Supervised Monocular 3D Object Detection using Multi-View   Projection and Direction Consistency](https://arxiv.org/abs/2303.08686)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to train monocular 3D object detection models using only 2D image labels, without needing 3D point cloud data for labeling. 

The key points are:

- Most current monocular 3D detection methods rely on 3D point clouds to label the ground truth boxes for training, but this causes an inconsistency between training (uses 3D data) and inference (only 2D images). 

- The authors propose a new weakly supervised framework that only uses 2D labels like 2D bounding boxes to train the models. This allows utilizing large-scale 2D-labeled data like feedback images from production cars.

- They introduce three types of consistency between 2D labels and 3D predictions: projection consistency, multi-view consistency, and direction consistency. Losses are designed based on these to guide the model optimization.

- A new 2D direction labeling method is proposed to replace the 3D rotation labeling. This further avoids needing 3D point clouds. 

- Experiments show their method achieves comparable or better performance than some fully supervised methods. It also significantly boosts the baseline model when used for pre-training with a small amount of 3D labels.

In summary, the key hypothesis is that by designing losses based on 2D-3D consistency, they can train 3D detection models without relying on 3D point cloud labeling data.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new weakly supervised method for monocular 3D object detection that only requires 2D labels marked on images, without needing any 3D point clouds for labeling. This allows the method to be trained on images from production cars that lack 3D labels. 

2. It incorporates projection consistency and multi-view consistency losses to guide the prediction of accurate 3D bounding boxes based on relationships between 2D and 3D data.

3. It proposes a new 2D direction labeling method to replace 3D rotation labels, and a direction consistency loss to optimize rotation prediction. 

4. Experiments show the method achieves comparable performance to some fully supervised methods on KITTI. When used for pre-training with a small amount of 3D labels, it significantly outperforms the fully supervised baseline.

5. The method's ability to work with only 2D labels makes it feasible to use large amounts of feedback image data from production cars to improve robustness and generality of models. This is a key advantage over methods that require 3D labels.

In summary, the main novelty is the weakly supervised framework and losses using only 2D labels, which enables using production car data lacking 3D annotations to improve monocular 3D detection models. The consistency losses are designed to provide supervision without 3D labels.
