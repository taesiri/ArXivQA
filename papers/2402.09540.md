# [Why Does Differential Privacy with Large Epsilon Defend Against   Practical Membership Inference Attacks?](https://arxiv.org/abs/2402.09540)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Differential privacy (DP) provides strong worst-case guarantees against membership inference attacks (MIAs), but requires a small privacy parameter epsilon (e.g. epsilon < 1). 
- However, DP is often deployed in practice with larger epsilon values (epsilon ≥ 7), and empirically defends well against MIAs.  
- Existing DP theory cannot explain this empirical success, since the guarantees for epsilon ≥ 7 are essentially vacuous.

Proposed Solution:
- Introduce a new privacy notion called "practical membership privacy" (PMP) which models an attacker with uncertainty about the private data set. 
- PMP guarantees privacy against "practical" MIAs which lack exact knowledge of all but one data point. It is parameterized by an epsilon value (like DP) which upper bounds attack success probability.
- Derive relationships between the PMP and DP parameter epsilon for the exponential and Gaussian mechanisms. This shows large DP epsilon often translates into much smaller PMP epsilon, implying strong defense against practical MIAs.

Key Contributions:  
- Definition of practical membership privacy (PMP) which bridges theory and practice by modeling practical MIA settings
- Proof that PMP parameter upper bounds success rate of any practical MIA, giving it a clear privacy interpretation 
- Quantitative analysis relating the PMP and DP parameters for important DP mechanisms like exponential and Gaussian
- Principled guidance to practitioners on choosing epsilon based on desired guarantee against practical MIAs
- Explains how DP provides meaningful privacy to practical MIAs even for large epsilon, resolving disconnect between theory and practice

The paper rigorously shows why differential privacy with large epsilon, as often used in practice, can successfully defend against membership inference attacks that occur in realistic settings. This is done by modeling the uncertainty in an attacker's knowledge through the proposed PMP definition.
