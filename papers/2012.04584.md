# [Distilling Knowledge from Reader to Retriever for Question Answering](https://arxiv.org/abs/2012.04584)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be how to learn an effective retriever model for downstream natural language processing tasks like question answering, without requiring strong supervision in the form of query-document pairs for training. The key hypothesis is that the attention scores from a reader model can serve as a good proxy for document relevance, and thus can be used to provide supervisory signal to train the retriever model in a student-teacher framework inspired by knowledge distillation. Specifically, the paper proposes using the reader's cross-attention scores over retrieved documents as synthetic labels to train the retriever to reproduce a similar ranking of document relevance for a given query.In summary, the central research question is how to train a high-quality neural retriever model without explicit query-document supervision, with the core hypothesis being that reader attention can be distilled into the retriever as a relevance signal. The paper aims to validate this hypothesis experimentally on question answering tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a technique to learn retriever models for downstream tasks without requiring annotated pairs of queries and documents. The key ideas are:- Using attention scores from a reader model as a proxy for document relevance. Specifically, they aggregate the cross-attention scores from a sequence-to-sequence reader model to obtain synthetic labels indicating the relevance of documents to a query. - Proposing an iterative training procedure inspired by knowledge distillation, where the reader model produces targets used to train the retriever model. The reader and retriever are trained alternately in this loop.- Evaluating the approach on question answering tasks and showing it achieves state-of-the-art results without needing strong supervision for retrieving relevant documents.So in summary, the main contribution appears to be presenting a method to learn an information retrieval module for downstream NLP tasks without query-document relevance labels, by instead leveraging signals from a reader model's attention via an iterative distillation-style training approach. The effectiveness of this method is demonstrated for open-domain question answering.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a technique to train an information retrieval module for question answering without requiring annotated query-document pairs, by using a reader model's attention scores to provide synthetic labels for distilling knowledge into the retriever model.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in open-domain question answering:- The main novelty of this paper is using knowledge distillation to train a retriever without needing query-document pairs as supervision. This is an interesting alternative to other unsupervised retrieval training methods like inverse cloze and masked language modeling. - For the reader model, the authors use the Fusion-in-Decoder architecture, which has shown strong results on open-domain QA. This builds on other work using seq2seq models like BERT for reading comprehension.- For retrieval, they use a bi-encoder with BERT embeddings, similar to the Dense Passage Retrieval (DPR) system. Their iterative training procedure improves over DPR, demonstrating the benefits of distilling the reader's attention.- Their results improve over previous state-of-the-art like DPR and REALM on Natural Questions and TriviaQA. This is impressive given their method does not require any query-document supervision.- Compared to other recent knowledge distillation approaches for open-domain QA like RAG and ANCE, this method distills from the reader's attention directly, rather than distilling a cross-encoder reranker. The benefits of the different distillation strategies could be analyzed further.- For retrieval evaluation, they analyze both document recall metrics and end-task QA accuracy. Connectingretrieval performance to end-task impact is an important contribution.Overall, this paper makes excellent progress on unsupervised training of retrievers, advancing state-of-the-art in open-domain QA. The knowledge distillation approach is novel and well-motivated. More analysis could be done to understand exactly what signals the reader's attention provides the retriever.
