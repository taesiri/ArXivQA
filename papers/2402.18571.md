# [Arithmetic Control of LLMs for Diverse User Preferences: Directional   Preference Alignment with Multi-Objective Rewards](https://arxiv.org/abs/2402.18571)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Existing RLHF methods for aligning LLMs rely on scalar rewards, which have limited capacity for capturing complex and often conflicting human preferences across diverse users. This results in models aligned to an "average user", overlooking minority preferences.

Proposed Solution:
- The paper proposes Directional Preference Alignment (DPA), which represents user preferences as direction vectors in a multi-dimensional reward space. 
- DPA involves training a multi-objective reward model, then fine-tuning a single LLM model using a variant of Rejection Sampling Finetuning (RSF) that conditions sampling/finetuning on user preference vectors.

Key Contributions:
- DPA allows intuitive arithmetic control over LLM generations by users specifying preference vectors, enabling better personalization.
- Experiments on aligning helpfulness and verbosity rewards with Mistral-7B show DPA effectively navigates preference trade-offs. 
- DPA offers competitive performance to state-of-the-art DPO, while providing better controllability over preference balancing.
- DPA addresses limitations of prior works like scalar rewards, reward soups, SteerLM regarding capturing diverse user preferences with a single model.

In summary, DPA advances LLM alignment and controllability to accommodate diverse user preferences within one model, through a directional preference representation and RSF-based finetuning approach. Experiments demonstrate its effectiveness for arithmetic trade-off control between helpfulness and verbosity rewards.
