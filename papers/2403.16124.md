# [Enhancing Visual Continual Learning with Language-Guided Supervision](https://arxiv.org/abs/2403.16124)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Most existing continual learning (CL) methods overlook the significance of semantic knowledge contained in category names. They commonly use one-hot labels coupled with randomly initialized classifier heads. However, this practice has two key issues in CL: 1) Representation drifting - optimization of each class target confined to its task can cause conflicts between targets of different tasks, inducing drifting in the feature space; 2) Insufficient knowledge transfer - random initialization struggles to capture semantic similarity between classes across tasks, impeding transfer.

Proposed Solution:
The paper proposes Language-guided Supervision for Continual Learning (LingoCL), which leverages semantic knowledge from pretrained language models (PLMs) to guide CL. For each new task, it utilizes the category names as input to PLMs and takes the output vector of each class as its semantic target. These targets remain frozen during training to serve as supervision signals for the trainable vision encoder. The rich knowledge in PLMs ensures semantic targets consider correlations between all classes, addressing aforementioned issues.

Key Contributions:
- Proposes a new CL paradigm that transfers language knowledge to alleviate representation drift and facilitate knowledge transfer. 
- Demonstrates state-of-the-art performance boosts across diverse CL methods and protocols (task-, class-, domain-incremental).
- Provides extensive analysis into LingoCL's ability to reduce drift and enable transfer.
- Establishes strong benchmark results to motivate future research into transferring insights across modalities for CL.
- Simple, efficient, and flexible integration seamlessly complements existing approaches.

In summary, the paper pioneers an underexplored direction of harnessing multimodal knowledge to enhance continual learning, with impressive empirical evidence. The proposed LingoCL paradigm offers significant promise in advancing CL capabilities.
