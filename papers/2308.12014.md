# [From Instructions to Intrinsic Human Values -- A Survey of Alignment   Goals for Big Models](https://arxiv.org/abs/2308.12014)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: What is the most appropriate and essential goal that large language models (LLMs) should be aligned with? 

The paper conducts a comprehensive survey of different alignment goals for LLMs proposed in existing literature, categorizing them into three main levels - human instructions, human preferences, and human values. It traces the evolution of alignment goals from fundamental abilities to value orientation, and analyzes the strengths and weaknesses of each level of goal. 

Based on the analysis, the paper argues that aligning LLMs with intrinsic human values provides a promising direction to address key challenges like generalization, stability, and comprehensiveness. It suggests that intrinsic human values should be the essential goal for aligning LLMs, as opposed to just instructions or preferences reflected in specific behaviors. 

The key hypotheses are:

- Aligning LLMs with intrinsic human values can lead to more generalizable, stable and comprehensive alignment compared to other goals like instructions or preferences.

- There is an evolutionary trend in LLM alignment goals from surface behaviors to intrinsic values. Identifying intrinsic values as the essential goal is the next step in this evolution.

- Existing value theories like Schwartz theory of basic human values can serve as promising value systems to align LLMs, if adapted appropriately to the AI context.

So in summary, the central research question is - what is the most appropriate and essential goal to align LLMs with? And the key hypothesis is - intrinsic human values should be the ultimate alignment goal for LLMs. The paper surveys existing literature to motivate and support this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It highlights the importance of identifying proper alignment goals for big models and provides the first comprehensive survey on this topic from two perspectives: the definition of alignment goals and the evaluation of alignment. 

2. It summarizes alignment goals from existing literature into three main levels - human instructions, human preferences and human values. It traces their evolution from fundamental abilities to value orientation and from surface behaviors to intrinsic values.

3. It discusses challenges and future research directions for aligning big models with intrinsic human values, which is identified as the most promising alignment goal. The key challenges include defining an appropriate value system, achieving generalizable and stable goal representation, developing automatic and comprehensive evaluation, and designing effective and stable alignment algorithms.

4. It summarizes publicly available resources related to big model alignment goals, including datasets, benchmarks and platforms. These resources are open-sourced to facilitate future research.

5. Overall, this survey serves as an introduction and inspiration for researchers and practitioners working on aligning big models like large language models with humans. By clarifying appropriate alignment goals, it aims to guide the research towards developing big models that better serve humanity with minimized risks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper provides a comprehensive survey of research on aligning large language models with human values, categorizing alignment goals into human instructions, preferences, and values, and discussing challenges and future directions for achieving intrinsic human value alignment.


## How does this paper compare to other research in the same field?

 This paper provides a comprehensive survey on the alignment goals for big models, especially large language models (LLMs). Here are some key things I noticed in comparing it to other related work:

- Most prior work has focused on developing methods to align LLMs, whereas this paper highlights the importance of identifying appropriate alignment goals. It focuses on categorizing and analyzing different alignment goals that have been explored.

- The paper divides alignment goals into three main levels - human instructions, human preferences, and human values. It traces the evolution of goals from just following instructions to incorporating human values and intrinsic motivations. This provides helpful framing for thinking about alignment.

- Other surveys like the OPT-175B paper have also categorized some alignment methods and datasets. But they do not provide the same conceptual analysis of goals and do not cover the emerging work on aligning with human values.

- This survey summarizes both the definition of goals and how they are evaluated. Other papers have focused more narrowly on just alignment methods or just datasets/benchmarks. Looking at goals and evaluation together provides a more complete picture.

- The discussion on challenges and future directions around defining values, goal representation, evaluation, and algorithms provides useful insights that I have not seen covered to the same extent elsewhere.

- Overall this paper provides a unique conceptual and philosophical lens on alignment goals that can inform both near-term alignment efforts and longer-term AI safety research. The categorization of different levels of goals and tracing their evolution is a novel contribution.

In summary, this survey offers a helpful perspective by framing alignment in terms of goals and tracing their development. The conceptual analysis and future directions make it a valuable synthesis and overview of this important area of research.


## What future research directions do the authors suggest?

 The authors suggest several potential future research directions in the paper:

1. Defining an appropriate value system: The authors argue that current value principles like HHH (helpful, honest, harmless) are limited. They suggest exploring more scientific, comprehensive, stable, and validated value systems like Schwartz's Theory of Basic Human Values or Moral Foundations Theory. 

2. Developing generalizable and stable goal representations: The authors suggest improving goal representations for alignment to be more generalizable to new scenarios, stable in extreme cases with value conflicts, and interpretable.

3. Creating automatic and comprehensive evaluation: The authors call for more automatic, scalable, and comprehensive evaluation methods and benchmarks to assess alignment across different levels like understanding values, making judgments, and behaving consistently. 

4. Developing effective and stable alignment algorithms: The authors suggest developing new alignment algorithms that directly use value principles rather than proxy demonstrations. They also suggest methods that can adapt to varying value principles and avoid instability.

5. Addressing value pluralism: The authors note that human values differ across cultures and evolve over time. They suggest alignment methods should be adaptable to varying values.

In summary, the key future directions are establishing scientific value systems, improving goal representations, creating robust evaluations, and developing more effective and stable alignment algorithms that can address challenges like value pluralism. The overall emphasis is on aligning models to intrinsic human values in a generalizable, stable, and interpretable manner.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a comprehensive survey of different alignment goals for big models such as large language models (LLMs), as well as methods for evaluating how well models achieve those goals. The authors categorize alignment goals into three main levels: human instructions, human preferences, and human values. Aligning with human instructions refers to enabling models to complete diverse tasks as instructed. Aligning with preferences means generating outputs that maximize human satisfaction, beyond just correctness. Aligning with values involves adhering to principles like being helpful, honest, and harmless. The paper reviews work defining and representing these goals, as well as benchmarks and techniques to evaluate alignment, like human ratings and reward modeling. While earlier alignment goals focused on task performance and behaviors, the authors argue aligning with intrinsic human values could address limitations around generalizability and stability. They discuss challenges in defining appropriate value systems, creating generalizable and stable goal representations, evaluation methods, and developing effective alignment algorithms. The paper provides an overview of different alignment goals and how they have evolved, in order to shed light on the question of what big models should ultimately be aligned with.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper provides a comprehensive survey of goals for aligning big models like large language models (LLMs) with human values and preferences. The authors categorize existing alignment approaches into three levels - aligning with human instructions, human preferences, and intrinsic human values. They trace the evolution of alignment goals from just achieving good performance on tasks to satisfying implicit preferences to internalizing core human values. 

The paper discusses how each alignment goal has been defined and represented for training big models. It summarizes evaluation benchmarks and metrics for assessing model alignment at each goal level. Challenges are identified in defining appropriate value systems, creating generalizable and stable goal representations, and developing effective alignment algorithms. Future research directions are suggested around these issues, such as adapting human value theories for AI, directly incorporating value principles into goal representations, and enabling value-driven learning rather than just behavior imitation. The survey provides a useful structure for understanding and advancing research on value alignment for increasingly capable big models.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents a comprehensive survey of different alignment goals for big models such as large language models (LLMs), as well as methods for evaluating alignment performance. The authors categorize alignment goals into three main levels: human instructions, human preferences, and human values. To align with human instructions, models are trained on datasets of instruction-input-output tuples using supervised learning. For human preferences, models are trained on human feedback and demonstrations using techniques like reinforcement learning. Alignment with human values involves training models on datasets designed to encode principles like being helpful, honest, and harmless. The authors trace an evolution in alignment goals from basic abilities to intrinsic human values. They discuss challenges in defining comprehensive value systems, creating generalizable representations of goals, evaluation, and developing more effective and stable alignment algorithms. The paper summarizes available benchmarks and resources to facilitate future research on aligning big models with human values.


## What problem or question is the paper addressing?

 This paper provides a comprehensive survey of the different alignment goals for big models, especially large language models (LLMs). The key research problems it aims to address are:

1. What should LLMs be aligned with? The paper highlights the importance of identifying the right alignment goal and traces the evolution of goals from following instructions to learning human preferences to aligning with human values. It argues that aligning with intrinsic human values could be the most promising goal.

2. How to evaluate the alignment? The paper summarizes existing benchmarks and methods to evaluate how well LLMs are aligned under different goals like following instructions, conforming to preferences, and adhering to values. 

3. How to align LLMs? The paper briefly introduces main algorithms like supervised fine-tuning, reinforcement learning from human feedback, and in-context learning to align LLMs.

4. What are the key challenges and future directions? The paper discusses open problems such as defining an appropriate value system, developing generalizable and stable goal representations, designing automated and comprehensive evaluation, and exploring more effective and stable alignment algorithms.

In summary, this comprehensive survey focuses on investigating different alignment goals for LLMs, evaluation methods, algorithms, challenges and promising research directions in this field. The key question is identifying the right goal that LLMs should be aligned with.
