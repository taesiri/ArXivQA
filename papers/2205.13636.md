# [Quark: Controllable Text Generation with Reinforced Unlearning](https://arxiv.org/abs/2205.13636)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question seems to be:How can we unlearn or reduce undesirable behaviors in language models, such as generating toxic, repetitive or sentiment-inconsistent text, while maintaining their fluency, coherence and few-shot capabilities? The authors propose a method called Quantized Reward Konditioning (QuaRK) to address this question. The key ideas are:- Using a reward function to quantify undesirable behaviors in generated text (e.g. toxicity scores)- Collecting text samples from the language model, evaluating their reward, and partitioning them into quantiles - Retraining the model on the quantile partitions with a language modeling objective and KL divergence penalty to steer the model distribution towards higher rewards- Adding control codes during training to condition on high vs low reward text- Alternating between sampling text, quantile partitioning, and retraining The hypothesis is that this method can effectively reshape the language model to reduce unwanted behaviors as quantified by the reward function, while maintaining fluency and other capabilities. The experiments aim to test this hypothesis across unlearning toxicity, sentiment and repetition.In summary, the key research question is how to unlearn undesirable behaviors in language models while maintaining their core capabilities, which QuaRK aims to address through iterative sampling, quantile partitioning of rewards, and retraining with control codes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Introducing QuaRK (Quantized Reward Konditioning), an algorithm for unlearning undesirable behaviors from language models using reinforcement learning techniques. - Showing that QuaRK can effectively unlearn toxicity, repetition, and unwanted sentiment from language models like GPT-2, outperforming strong baselines and prior state-of-the-art RL methods like PPO.- Demonstrating that QuaRK relies only on standard language modeling techniques like maximizing likelihood and KL regularization, without needing complex specialized optimizations used in other RL algorithms. - Providing ablation studies that analyze the effect of different components of QuaRK, like the number of quantiles, exploration strategy, and using reward tokens.- Evaluating QuaRK extensively, including automatic metrics, human evaluations, and comparisons to multiple competitive baselines on three distinct unlearning tasks.In summary, the main contribution appears to be proposing QuaRK as a simple but effective algorithm for unlearning using RL, while relying only on standard language modeling techniques, and comprehensively evaluating its capabilities across different tasks and metrics. The ablation studies also provide useful analysis into why QuaRK works well.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, the main point of the paper is to introduce Quark, a new reinforcement learning algorithm for optimizing language model rewards while constraining the model to stay close to the original pretrained version. Specifically, Quark alternates between exploring the current policy, quantizing samples based on their reward, and updating the model on the quantized samples using a language modeling objective plus a KL penalty. Experiments show Quark can effectively unlearn unwanted attributes like toxicity, repetition, and sentiment while maintaining fluency. The key novelty is a simple yet effective method for reward optimization that relies only on standard language modeling techniques.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper introduces a new method called Quark for unlearning undesirable behaviors from language models. Other recent work has also aimed to modify or control language model behaviors post-training, like unlearning toxicity or other unsafe attributes. However, Quark differs in its approach of using a simple reinforcement learning strategy based on standard language modeling techniques.- Compared to other RL or PPO-based methods for language models like in Ouyang et al. (2022), Quark does not require additional learned models or complex optimization heuristics. It shows competitive performance using basic building blocks like KL regularization and control codes. This makes Quark simpler and more self-contained.- For the task of reducing repetition, Quark demonstrates performance gains when combined with the unlikelihood training objective from Welleck et al. (2020). This shows it can complement and enhance direct differentiable training techniques for text generation.- Quark builds on ideas from prior work like control codes, KL tuning, and Decision Transformers, but combines them in a novel way tailored for iteratively unlearning behaviors from language models. The experiments validate that Quark can effectively steer away from toxicity, repetition, and unwanted sentiment.- Compared to supervised approaches like training on contrastive datasets or using auxiliary classifiers, Quark provides more direct dynamic fine-tuning based on scalar rewards. This avoids issues like overfitting to new datasets or lacking token-level feedback.In summary, Quark introduces a straightforward and self-contained RL approach for unlearning language model behaviors that compares favorably to existing methods and nicely combines concepts from prior work. The simplicity and strong performance differentiate it from related efforts.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Investigating adaptations of their method, QuaRK, for controlling multiple rewards simultaneously. The current work focuses on optimizing one reward at a time, but being able to optimize for multiple rewards could be useful.- Exploring more diverse types of rewards beyond the ones focused on in this paper (toxicity, repetition, sentiment). For example, rewards related to human preferences or other aspects of text could be interesting to explore. - Training QuaRK with fewer overall parameters rather than optimizing across all the parameters of the full language model. This could lead to more efficient training.- Experimenting with different penalties in the training loss beyond the KL divergence penalty they use. Other penalties may provide a different trade-off between language quality and reward optimization.- Validating the approach on much larger language models beyond GPT-2, since model scale has been increasing rapidly.- Testing the method on a wider range of language tasks and domains beyond the ones in the current paper.So in summary, the main future directions are around exploring adaptations of QuaRK, applying it to new types of rewards and tasks, training it more efficiently, and validating it on larger language models. The authors lay out several interesting research questions to further develop reward optimization for unlearning in language models.
