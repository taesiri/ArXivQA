# [Quark: Controllable Text Generation with Reinforced Unlearning](https://arxiv.org/abs/2205.13636)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question seems to be:How can we unlearn or reduce undesirable behaviors in language models, such as generating toxic, repetitive or sentiment-inconsistent text, while maintaining their fluency, coherence and few-shot capabilities? The authors propose a method called Quantized Reward Konditioning (QuaRK) to address this question. The key ideas are:- Using a reward function to quantify undesirable behaviors in generated text (e.g. toxicity scores)- Collecting text samples from the language model, evaluating their reward, and partitioning them into quantiles - Retraining the model on the quantile partitions with a language modeling objective and KL divergence penalty to steer the model distribution towards higher rewards- Adding control codes during training to condition on high vs low reward text- Alternating between sampling text, quantile partitioning, and retraining The hypothesis is that this method can effectively reshape the language model to reduce unwanted behaviors as quantified by the reward function, while maintaining fluency and other capabilities. The experiments aim to test this hypothesis across unlearning toxicity, sentiment and repetition.In summary, the key research question is how to unlearn undesirable behaviors in language models while maintaining their core capabilities, which QuaRK aims to address through iterative sampling, quantile partitioning of rewards, and retraining with control codes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Introducing QuaRK (Quantized Reward Konditioning), an algorithm for unlearning undesirable behaviors from language models using reinforcement learning techniques. - Showing that QuaRK can effectively unlearn toxicity, repetition, and unwanted sentiment from language models like GPT-2, outperforming strong baselines and prior state-of-the-art RL methods like PPO.- Demonstrating that QuaRK relies only on standard language modeling techniques like maximizing likelihood and KL regularization, without needing complex specialized optimizations used in other RL algorithms. - Providing ablation studies that analyze the effect of different components of QuaRK, like the number of quantiles, exploration strategy, and using reward tokens.- Evaluating QuaRK extensively, including automatic metrics, human evaluations, and comparisons to multiple competitive baselines on three distinct unlearning tasks.In summary, the main contribution appears to be proposing QuaRK as a simple but effective algorithm for unlearning using RL, while relying only on standard language modeling techniques, and comprehensively evaluating its capabilities across different tasks and metrics. The ablation studies also provide useful analysis into why QuaRK works well.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, the main point of the paper is to introduce Quark, a new reinforcement learning algorithm for optimizing language model rewards while constraining the model to stay close to the original pretrained version. Specifically, Quark alternates between exploring the current policy, quantizing samples based on their reward, and updating the model on the quantized samples using a language modeling objective plus a KL penalty. Experiments show Quark can effectively unlearn unwanted attributes like toxicity, repetition, and sentiment while maintaining fluency. The key novelty is a simple yet effective method for reward optimization that relies only on standard language modeling techniques.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper introduces a new method called Quark for unlearning undesirable behaviors from language models. Other recent work has also aimed to modify or control language model behaviors post-training, like unlearning toxicity or other unsafe attributes. However, Quark differs in its approach of using a simple reinforcement learning strategy based on standard language modeling techniques.- Compared to other RL or PPO-based methods for language models like in Ouyang et al. (2022), Quark does not require additional learned models or complex optimization heuristics. It shows competitive performance using basic building blocks like KL regularization and control codes. This makes Quark simpler and more self-contained.- For the task of reducing repetition, Quark demonstrates performance gains when combined with the unlikelihood training objective from Welleck et al. (2020). This shows it can complement and enhance direct differentiable training techniques for text generation.- Quark builds on ideas from prior work like control codes, KL tuning, and Decision Transformers, but combines them in a novel way tailored for iteratively unlearning behaviors from language models. The experiments validate that Quark can effectively steer away from toxicity, repetition, and unwanted sentiment.- Compared to supervised approaches like training on contrastive datasets or using auxiliary classifiers, Quark provides more direct dynamic fine-tuning based on scalar rewards. This avoids issues like overfitting to new datasets or lacking token-level feedback.In summary, Quark introduces a straightforward and self-contained RL approach for unlearning language model behaviors that compares favorably to existing methods and nicely combines concepts from prior work. The simplicity and strong performance differentiate it from related efforts.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Investigating adaptations of their method, QuaRK, for controlling multiple rewards simultaneously. The current work focuses on optimizing one reward at a time, but being able to optimize for multiple rewards could be useful.- Exploring more diverse types of rewards beyond the ones focused on in this paper (toxicity, repetition, sentiment). For example, rewards related to human preferences or other aspects of text could be interesting to explore. - Training QuaRK with fewer overall parameters rather than optimizing across all the parameters of the full language model. This could lead to more efficient training.- Experimenting with different penalties in the training loss beyond the KL divergence penalty they use. Other penalties may provide a different trade-off between language quality and reward optimization.- Validating the approach on much larger language models beyond GPT-2, since model scale has been increasing rapidly.- Testing the method on a wider range of language tasks and domains beyond the ones in the current paper.So in summary, the main future directions are around exploring adaptations of QuaRK, applying it to new types of rewards and tasks, training it more efficiently, and validating it on larger language models. The authors lay out several interesting research questions to further develop reward optimization for unlearning in language models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces \methodname (\methodnameshort), a new algorithm for modifying pretrained language models to unlearn unwanted behaviors like toxicity, repetition, and sentiment. \methodnameshort works by iteratively sampling from the current model, sorting samples into quantiles by reward score, and updating the model by maximizing likelihood of high reward samples while staying close to the original model via a KL penalty. Experiments across three tasks - unlearning toxicity, repetition, and sentiment - show that \methodnameshort outperforms strong baselines like PPO in reducing unwanted behaviors according to both automatic metrics and human evaluation, while maintaining fluency. Ablation studies provide insights into design choices like the effect of KL penalty, number of partitions, and conditioning strategies. Overall, the paper demonstrates that \methodnameshort, using only standard language modeling techniques, can effectively steer language models away from acquiring undesirable behaviors that may arise during pretraining.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces Quark, a method for unlearning undesirable behaviors from pretrained language models. Quark alternates between exploring the current model's generations, quantizing them by their reward, and updating the model on high-reward samples while staying close to the original model. Specifically, Quark explores by sampling from the current model and scoring outputs with a reward function. It then sorts outputs into quantiles by reward value, identifying each quantile with a learned token. Finally, it trains the model on samples from each quantile, conditioning on the quantile's token and penalizing drift from the original model via KL divergence. Experiments on unlearning toxicity, sentiment, and repetition show Quark outperforms baselines like PPO, reducing undesirable behaviors while maintaining fluency. Ablations provide insights on design choices. Overall, Quark enables steering language models away from unwanted generations using only standard language modeling techniques.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces QuaRK (Quantized Reward Konditioning), an algorithm for unlearning undesirable properties from language models. QuaRK alternates between three main steps - exploration, quantization, and learning. During exploration, the current language model is used to sample text which is then evaluated by a reward function and added to a datapool. The datapool is then quantized by sorting examples by their reward and partitioning them into quantiles, with each quantile identified by a reward token. Finally, during learning, the language model is trained on samples from each quantile using a standard language modeling objective of maximizing likelihood conditioned on the reward token, plus a KL divergence penalty to keep the model close to the original. By sampling using the highest reward token during exploration and using the penalty during learning, QuaRK iteratively steers the language model distribution towards higher reward text while preventing it from deviating too far from the original model.
