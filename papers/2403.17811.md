# [Are Compressed Language Models Less Subgroup Robust?](https://arxiv.org/abs/2403.17811)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) are seeing increased adoption, but their large size makes deployment expensive. Model compression creates smaller, cheaper variants but little is known about the effects on subgroup robustness.

- Subgroup robustness refers to maximizing worst-group performance across subgroups defined by dataset labels and attributes. Conventional training via empirical risk minimization overfits on majority subgroups.  

Methods
- The authors evaluate 18 compression methods (knowledge distillation, pruning, quantization, vocabulary transfer) applied to BERT models on 3 text classification datasets.

- Compressed models are trained via empirical risk minimization. Average accuracy, worst-group accuracy, and model size are compared.

Results
- Model size alone does not determine subgroup robustness. The compression method used also impacts robustness.

- Compression does not always reduce worst-group accuracy. On the CivilComments dataset, compression helps generalization and increases worst-group performance.

- Models with equal parameters after compression can have different robustness due to differences in weight initialization determined by the compression method.  

Conclusions
- Compression can sometimes improve subgroup robustness over the original model by acting as regularization to prevent overfitting.

- Choice of compression method significantly impacts model robustness, not just the size reduction alone. More analysis is needed on model compression methods for natural language processing tasks.
