# [eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos](https://arxiv.org/abs/2311.17335)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper introduces eMotions, the first large-scale dataset specifically for emotion recognition in short videos (SVs). The dataset contains 27,996 manually annotated videos covering six emotion categories - excitation, fear, neutral, relaxation, sadness, and tension. The videos are sourced from popular Chinese SVs platforms Douyin, Kuaishou, and TikTok and span a wide diversity of content. A multi-stage annotation process with careful personnel allocation helps ensure reliable ground truth labels. The paper also proposes a strong baseline model called AV-CPNet that employs Video Swin Transformer as the visual backbone and a two-stage cross-modal fusion module to effectively model correlations between audio and visual features for emotion recognition. Extensive experiments demonstrate state-of-the-art performance of AV-CPNet on eMotions and five other public datasets. The introduction of a dedicated SVs emotion dataset and a well-designed model architecture to handle unique challenges like high visual diversity and audio-visual co-expressions lays the foundation for further research on this increasingly important problem.


## Summarize the paper in one sentence.

 This paper proposes eMotions, the first large-scale dataset for emotion recognition in short videos, along with a strong baseline model AV-CPNet that effectively learns semantically relevant and emotion-related audio-visual representations through complementary modeling of inter-modalities correlations.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Proposing eMotions, the first large-scale dataset specifically for emotion recognition in short videos (SVs). The dataset contains 27,996 videos labeled with 6 emotions, sourced from 3 major short video platforms. Efforts are made to reduce labeling subjectivity and noise.

2) Providing two variant datasets of eMotions through targeted data sampling - a category-balanced variant and a test-oriented variant.

3) Developing an effective baseline model called AV-CPNet to tackle the challenges of emotion recognition in short videos. The model employs Video Swin Transformer, designs a two-stage cross-modal fusion module, and uses a novel loss function called EP-CE Loss.

4) Conducting extensive experiments on 9 datasets to demonstrate the superiority of the proposed AV-CPNet model and providing detailed analysis and insights.

In summary, the main contributions are proposing the eMotions dataset, its variants, the AV-CPNet model, and extensive benchmarking experiments and analysis. The work serves as a foundation to facilitate future research on emotion recognition in short videos.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Short videos (SVs)
- Emotion recognition
- eMotions dataset
- Audio-visual learning
- End-to-end baseline model
- Video transformer
- Cross-modal fusion
- Emotion polarity enhanced loss

To summarize, this paper proposes a new large-scale dataset called eMotions for emotion recognition in short videos, which contains 27,996 labeled videos from three short video platforms. The paper also presents an end-to-end baseline model called AV-CPNet that employs a video transformer backbone and a two-stage cross-modal fusion module to capture audio-visual correlations. The model is optimized using an emotion polarity enhanced cross-entropy loss. Extensive experiments demonstrate the effectiveness of the proposed dataset and model.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes an end-to-end baseline model AV-CPNet. What are the main motivations and rationales behind using the Video Swin Transformer as the visual backbone instead of traditional CNNs?

2. In the two-stage cross-modal fusion module, cross-attention and sum fusion are utilized. Explain the working mechanisms of these two components and how they complementarily model the correlations of audio-visual features.  

3. The paper mentions that directly optimizing the traditional cross-entropy (CE) loss could result in misclassification. Elaborate why this happens and how the proposed Emotion Polarity (EP) enhanced CE Loss tackles this problem.

4. The EP-CE Loss incorporates penalties for three emotion polarities. Analyze the effect of using different penalty combinations and explain why the setting of 0.5:0.5:0.5 performs the best. 

5. In the ablation study, how significant are the performance gains by adding the CAF Layer and SF Layer respectively? Provide some insights into why this happens.

6. Sum fusion is chosen in the paper after comparing with other fusion strategies like concatenation and element-wise sum. Justify why sum fusion fits audio-visual emotion recognition the best.  

7. Explain the multi-task learning strategy utilized in AV-CPNet and why it brings performance improvements.

8. The paper shows AV-CPNet initialized with eMotions weights leads to higher accuracy on other datasets. Elaborate the reasons behind this observation.

9. Although existing emotion recognition datasets contain rich information, experiments show directly using their weights underperforms ImageNet pre-training on eMotions. Provide some explanations for this counter-intuitive finding.

10. The paper constructs category-balanced and test-oriented variants of eMotions. What are the motivations, significance and possible application scenarios of providing these two additional datasets?
