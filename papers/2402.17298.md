# [ArcSin: Adaptive ranged cosine Similarity injected noise for   Language-Driven Visual Tasks](https://arxiv.org/abs/2402.17298)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the challenging problem of bridging the modality gap between learning from language/text and making inferences for visual tasks like visual question answering (VQA), image captioning (IC) and visual entailment (VE). Specifically, it focuses on zero-shot cross-modal transfer where models are trained exclusively on textual data and tested on visual data. This is difficult since there is an inherent disparity (modality gap) between text and image features in contrastive models like CLIP.

Method: 
The paper proposes a novel adaptive noise injection technique called "Adaptive Ranged Cosine Similarity Injected Noise (ArcSin)". It has two main components:

1) Adaptive Ranged Noise Injection: This dynamically tunes the noise added to text features based on the feature value and a similarity threshold to preserve semantic content. The permissible noise is calculated using trigonometric relationships.

2) Injection Pool: Multiple noisy versions of each feature are created. The one with maximum similarity to the original is selected. This expands domain generalization while maintaining similarity.

Together, these enhance cross-modal transfer by widening the domain of text features appropriately.

Contributions:
The main contributions are:

1) The proposed ArcSin noise injection method that balances closing the modality gap while preserving semantic fidelity of text features.

2) The injection pool strategy to expand noise scale and generalization ability indirectly.

3) State-of-the-art performance on language-driven vision tasks like VQA, image captioning and visual entailment. For example, ArcSin achieves gains of 1.9 and 1.1 CIDEr points over prior work on single and multi-captioning tasks.

The method sets new benchmarks for zero-shot cross-modal transfer across various architectures. It demonstrates how language-supervision alone can enable inference on visual tasks.
