# [STaR: Distilling Speech Temporal Relation for Lightweight Speech   Self-Supervised Learning Models](https://arxiv.org/abs/2312.09040)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Transformer-based speech self-supervised learning (SSL) models like HuBERT have achieved great performance on speech tasks, but their large size and high computational cost make them difficult to deploy. 
- Existing knowledge distillation methods for compressing these models have limitations: 
    - They try to directly match the complex teacher representations, which is too constrained for a lightweight student.
    - Structured pruning methods manage model size but can't control computational cost.

Proposed Solution:
- Propose distilling "speech temporal relation" (STaR) instead of directly matching teacher representations. 
- Explore 3 STaR objectives that transfer pairwise temporal relations between speech frames:
    1. Average attention map distillation
    2. Layer-wise temporal Gram matrix (TGM) distillation 
    3. Intra-layer TGM distillation
- Combine layer-wise and intra-layer TGM as final STaR loss.
- Do not require any extra parameters during distillation.

Main Contributions:
- STaRHuBERT gets the best overall SUPERB score of 79.8 among ~27M parameter models, with only 30.7% MACs and 28.1% params of HuBERT.
- Outperforms LightHuBERT, which demands extensive resources for compression.  
- Shows strong performance on phoneme/speech recognition and speaker tasks.
- Verified to work for other SSL models like wav2vec 2.0 and wavLM.
- More suitable for lightweight student models compared to directly matching teacher outputs.

In summary, the paper proposes distilling temporal relations between speech frames as an effective knowledge distillation method to compress speech SSL models into lightweight students.
