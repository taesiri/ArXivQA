# [Why "classic" Transformers are shallow and how to make them go deep](https://arxiv.org/abs/2312.06182)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper provides a comprehensive analysis of the phenomenon of token similarity escalation (TSE) in classic Transformer models, where token representations become increasingly similar as data passes through successive layers. The authors prove both theoretically and empirically that the root cause of TSE is the invariant leading eigenspace and large spectral gaps commonly present in attention matrices, which drive token similarities towards 1 at a linear rate. They introduce a precisely defined measure of TSE rate and show it converges locally at 1/2, explaining why Transformers start struggling beyond 10-12 layers under standard initializations. Based on these insights, the authors propose a simple token de-escalation operation, which removes excessive similarity without suppressing self-attention, to enable effective training of deeper post-norm Transformers. Preliminary experiments on vision and language tasks confirm their de-escalated Transformer matches or outperforms the pre-norm variant, substantiating the viability of surgically fixing TSE to train very deep Transformers.
