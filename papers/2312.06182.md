# [Why "classic" Transformers are shallow and how to make them go deep](https://arxiv.org/abs/2312.06182)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper provides a comprehensive analysis of the phenomenon of token similarity escalation (TSE) in classic Transformer models, where token representations become increasingly similar as data passes through successive layers. The authors prove both theoretically and empirically that the root cause of TSE is the invariant leading eigenspace and large spectral gaps commonly present in attention matrices, which drive token similarities towards 1 at a linear rate. They introduce a precisely defined measure of TSE rate and show it converges locally at 1/2, explaining why Transformers start struggling beyond 10-12 layers under standard initializations. Based on these insights, the authors propose a simple token de-escalation operation, which removes excessive similarity without suppressing self-attention, to enable effective training of deeper post-norm Transformers. Preliminary experiments on vision and language tasks confirm their de-escalated Transformer matches or outperforms the pre-norm variant, substantiating the viability of surgically fixing TSE to train very deep Transformers.


## Summarize the paper in one sentence.

 This paper analyzes why token similarity escalates in deep Transformer models, proving it is driven by invariant leading eigenspaces and large spectral gaps in attention matrices, and proposes a simple linear de-escalation operation to surgically remove excessive similarity without discounting self-attention.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides a comprehensive analysis, both theoretically and empirically, to show that the phenomenon of token similarity escalation (TSE) is the root cause of why classic Transformers run into difficulties when made very deep. Specifically, the analysis reveals that:

(a) The driving force behind TSE is the invariant leading eigenspace and large spectral gaps commonly present in attention matrices. 

(b) The token similarity measure converges to 1 at a global linear rate which eventually accelerates to a local rate of 1/2.

2. Based on the insights gained, the paper proposes a simple de-escalation strategy to surgically remove excessive token similarity without discounting the role of self-attention. Preliminary experiments confirm that this helps improve training of deep post-norm Transformers.

In summary, the key contribution is providing a definitive quantitative analysis of the TSE phenomenon in Transformers and leveraging the insights to propose a targeted strategy to mitigate TSE that shows promise in experiments.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Token similarity escalation (TSE): The phenomenon where token representations become increasingly similar as they pass through layers of a Transformer model, reducing expressive capacity. Also referred to as token uniformity, rank collapse, etc.

- Self-attention (SA): The key mechanism in Transformers that allows modeling contextual information but also drives TSE. 

- Spectral properties: The eigenvalues and eigenspaces of the attention matrices play a key role in driving TSE, especially the invariant leading eigenspace and large spectral gaps.

- Post-norm vs pre-norm: Refers to placing layer normalizations before or after residual connections in Transformer blocks. Pre-norm is more stable for training deep models.

- Token similarity measure: A quantitative measure proposed in the paper to track TSE, based on the ratio of power in the invariant subspace vs the orthogonal subspace.

- Escalation rate: A measure of how quickly token similarity increases from one layer to the next. The paper analyzes this theoretically.

- De-escalation: A proposed method to surgically reduce token similarity to mitigate TSE, without reducing the overall role of self-attention.

These are some of the key terms and concepts central to this paper's investigation and analysis of the token similarity escalation phenomenon in Transformers. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a simple de-escalation strategy to mitigate token similarity escalation (TSE) by projecting the representation matrix away from the invariant subspace span{\{\One\}}. How does this compare to other TSE mitigation methods in terms of simplicity and effectiveness? 

2. The strength of the proposed de-escalation is controlled by a parameter τ. How should τ be set in practice for optimal performance? Is there a principled way to determine its value?

3. The location for inserting the de-escalation step within the Transformer block is left open. Does the location impact the effectiveness of de-escalation? If so, what considerations should guide the choice?

4. Theorem 1 provides a lower bound on the expected escalation rate. Can you further tighten this bound or provide more accurate approximations for the true expected rate? 

5. The analysis shows self-attention is the driving force behind TSE while other operations like FFN do not impact it. Does this finding have broader architectural implications for Transformer design?

6. The paper focuses on analysis and experiments for the post-norm Transformer. How would you extend the analysis for pre-norm and weigh the tradeoffs between the two architectures?

7. The vision and language tasks experimented on use small to moderate sized datasets and models. How would you assess the efficacy of the proposed method on larger scale models and tasks? 

8. The analysis relies on certain assumptions on the attention matrices and weight initializations. How would violations of those assumptions impact the escalation dynamics and method effectiveness?

9. Could the proposed linear de-escalation step be replaced by a more complex nonlinear operation? If so, what benefits might that provide and how would you analyze its impact?

10. The paper analyzes the token similarity phenomenon specifically for Transformer encoders. How would you extend the analysis to handle Transformer decoders as well?
