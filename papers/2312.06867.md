# [Get an A in Math: Progressive Rectification Prompting](https://arxiv.org/abs/2312.06867)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "Get an A in Math: Progressive Rectification Prompting":

Problem: 
Existing chain-of-thought (CoT) prompting methods that generate reasoning paths for large language models (LLMs) to solve math word problems (MWPs) are sensitive to mistakes. Even a small error in the reasoning path can lead to an incorrect final answer. Current methods lack:
(1) Verification to check answer correctness  
(2) Rectification to find the right answer while avoiding previous mistakes
(3) Progressive refinement of reasoning paths over multiple iterations

Solution - Progressive Rectification Prompting (PRP):
- Generates an initial answer using standard CoT prompting 
- Iteratively executes a verify-then-rectify process:
  - Verification: Uses "substitute verification" to check if previous answer is correct
    - Masks a number in the question
    - Substitutes the generated answer as a new condition 
    - If predicted masked number is incorrect, adds the answer to the "incorrect answer set"
  - Rectification: Uses incorrect answer set to prompt LLM to regenerate reasoning path and new answer
- Repeats verify-then-rectify over multiple iterations to progressively refine answer  

Contributions:
- Proposes the novel PRP method to equip LLMs with skills to iteratively verify, rectify and refine answers to MWPs
- Integrates ideas like substitute verification, negative feedback, and dual-process theory to avoid repeating mistakes
- Experiments show PRP boosts average accuracy on 8 MWP datasets from 77.3% (best previous CoT method) to 90.5%, achieving an A-grade performance.
- Significantly outperforms existing zero-shot and few-shot CoT prompting baselines
- Provides a highly effective approach for LLMs to solve math problems accurately.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel method called Progressive Rectification Prompting that enables large language models to iteratively verify and rectify generated answers to math word problems, achieving state-of-the-art performance.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing a novel zero-shot prompting method called Progressive Rectification Prompting (PRP) that enables large language models to progressively rectify generated answers and accurately solve math word problems. PRP has an iterative verify-then-rectify process to avoid repeating mistakes and achieve continuous improvement.

2. Conducting extensive experiments on eight math word problem datasets under zero-shot and few-shot settings. The method achieves state-of-the-art performance and attains an A-level grade on average across the datasets.

So in summary, the main contributions are proposing the PRP method for math word problem solving, which achieves much higher accuracy than prior methods by progressively rectifying answers, and demonstrating through experiments that PRP leads to state-of-the-art performance.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Math word problems (MWPs)
- Chain-of-thought (CoT) prompting methods
- Large language models (LLMs)
- Zero-shot learning
- Reasoning paths
- Progressive rectification prompting (PRP)
- Verify-then-rectify process
- Substitute verification
- Dual process theory
- Iterative process
- Answer verification
- Answer rectification

The paper proposes a new method called "progressive rectification prompting" (PRP) to improve the performance of large language models on solving math word problems. The key ideas include using an iterative verify-then-rectify process to check answers and provide feedback to the model, leveraging substitute verification to avoid repeating mistakes, and drawing inspiration from cognitive science (dual process theory) to progressively refine the model's answers. Some of the main evaluation datasets used are MultiArith, GSM8K, AddSub, and others. The goal is to equip LLMs with stronger math reasoning abilities without task-specific fine-tuning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the Progressive Rectification Prompting (PRP) method proposed in the paper:

1. The paper mentions that PRP attains an average accuracy of 90.5% across 8 math word problem datasets. What modifications could be made to the method to further improve the accuracy, especially on complex multi-step problems?

2. The verify-then-rectify process in PRP seems computationally expensive since it requires generating reasoning paths and answers multiple times. How can this process be optimized to reduce computational costs while preserving accuracy gains? 

3. The paper sets the maximum number of iterations K to 5. What is the impact of increasing or decreasing this hyperparameter value on model accuracy and efficiency? What would be reasonable values to test?

4. The substitute verification method is a key component of the PRP framework. How does using different verification methods like self-consistency or enumeration verification impact model performance?

5. How suitable is the PRP method for math word problems in domains like physics, chemistry, economics etc. that rely heavily on real-world knowledge? What additions would be needed to tailor it for these domains?

6. Could the PRP framework be extended to other complex reasoning tasks beyond math word problems? What kind of tasks could benefit from this progressive verify-then-rectify approach?

7. The paper mentions the PRP method equips LLMs with high-level math exam skills. What other human reasoning skills could potentially be imparted to LLMs via prompting techniques?

8. How does the performance of PRP method vary when using different underlying LLMs like GPT-3, Jurassic-1 J etc? What model characteristics are most vital for the success of PRP?

9. What additional prompt engineering techniques like demonstrations, plan-and-execute can be combined with PRP framework to further boost accuracy on math word problems?

10. The paper evaluates PRP only on arithmetic word problems. How challenging would it be to extend the method to handle algebra, geometry and calculus problems? What major changes would be required?
