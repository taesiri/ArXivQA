# [Contrastive Learning with Orthonormal Anchors (CLOA)](https://arxiv.org/abs/2403.18699)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Contrastive learning methods based on the InfoNCE loss and its variants suffer from instability issues, especially at high learning rates. 
- The paper shows both empirically and theoretically that these methods exhibit an "over-fusion" effect - embeddings tend to collapse to a singular point, which is a non-distinctive local minimum for the loss function.
- This over-fusion severely impacts classification accuracy in downstream tasks. Prior attempts to enhance InfoNCE have shown limited improvements.

Proposed Solution:
- The paper introduces a new loss term called Orthonormal Anchor Regression Loss (CLOA) that leverages a small set of labeled data.
- CLOA assigns predefined orthogonal anchors as targets for embeddings from each class. This acts as a "pulling force" to disentangle clusters.
- CLOA only requires 10% of the labels used for fine-tuning, yet shows significant gains.

Key Contributions:
- Identifies and provides theoretical proof that linear embeddings represent a local minimum for InfoNCE-based losses.
- Proposes an innovative solution CLOA to mitigate over-fusion by disentangling clusters using limited labeled data.
- Demonstrates state-of-the-art performance of CLOA integrated with several contrastive losses like InfoNCE, DCL, BarlowTwins etc. on CIFAR10 and CIFAR100. 
- With CLOA, using just 10% labels, achieves accuracy comparable to using 100% labels, highlighting efficacy despite data limitations.

In summary, the paper makes important theoretical and practical contributions towards addressing instability in contrastive learning methods. The proposed CLOA loss enables harnessing the benefits of high learning rates while maintaining representation distinctiveness.
