# [NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field   Indirect Illumination](https://arxiv.org/abs/2303.16617)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we develop an end-to-end inverse rendering pipeline that can decompose materials and illumination from multi-view images, while also considering near-field indirect illumination effects like inter-reflections?

The key ideas and contributions towards addressing this question appear to be:

- Representing materials and indirect illuminations as neural implicit fields, and jointly optimizing them along with explicit environment illumination parameters. This allows modeling of high-frequency inter-reflection details compared to using a blurred Spherical Gaussian approximation. 

- Introducing Monte Carlo path tracing with caching of bounced radiance in a neural radiance field. This provides a differentiable and physically based rendering for optimization.

- Using importance sampling and Spherical Gaussians for efficiency of the Monte Carlo estimator.

- Adding a radiance consistency constraint for unobserved rays to supervise the learning of indirect illumination and reduce ambiguity between materials and lighting.

So in summary, the central hypothesis seems to be that by combining these ideas - joint optimization, Monte Carlo path tracing, and the unobserved ray consistency constraint - they can develop an effective approach to inverse rendering that handles complex near-field lighting effects. The results on both synthetic and real data seem to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It proposes an end-to-end inverse rendering pipeline that can decompose materials and illumination from multi-view images, while considering near-field indirect illumination. 

- It introduces Monte Carlo sampling based path tracing to model inter-reflections, while using spherical gaussians to represent smooth environment illumination. This results in a physics-based and optimizable inverse rendering process.

- It employs importance sampling techniques to improve the efficiency of the Monte Carlo estimator.

- It introduces a new radiance consistency constraint for learning indirect illuminations from unobserved directions. This helps reduce ambiguity between materials and indirect illumination.

In summary, the key contribution is an end-to-end differentiable inverse rendering framework that can decompose materials, environment illumination and near-field indirect illumination in a joint manner. The path tracing based rendering and radiance consistency constraint allow modeling inter-reflections and reducing ambiguity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes an end-to-end inverse rendering method that jointly optimizes materials and illumination from multi-view images while modeling near-field indirect illumination using Monte Carlo path tracing and a novel radiance consistency constraint.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on inverse rendering:

- This paper proposes an end-to-end inverse rendering pipeline for jointly estimating materials, illumination and near-field indirect illumination from multi-view images. Most prior works like NeRF, IDR, and PhySG focus only on novel view synthesis and do not aim to decompose the scene into materials and lighting.

- For modeling indirect illumination, this paper uses Monte Carlo path tracing instead of spherical Gaussian (SG) approximation used in InvRender. Path tracing can better preserve high-frequency lighting details compared to SG.

- The proposed method represents materials, illumination and indirect lighting all using neural implicit representations. This allows end-to-end joint optimization, unlike InvRender which requires staged training.

- A key novelty is the radiance consistency loss term for supervising the learning of indirect lighting from unobserved directions. This helps resolve ambiguity between materials and lighting.

- Experiments show the proposed method outperforms InvRender and other baselines in decomposition quality, especially for inter-reflections and roughness estimation. It also enables relighting with estimated properties.

- Limitations include not handling transparency/translucency, and some failure cases with extreme shadows. Geometry optimization is also not handled currently.

In summary, this paper pushes forward inverse rendering research by enabling joint decomposition of materials, lighting and near-field effects using differentiable path tracing and implicit neural representations. The radiance consistency loss helps resolve a key ambiguity. Results are state-of-the-art on both synthetic and real data.
