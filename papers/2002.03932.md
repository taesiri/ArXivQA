# [Pre-training Tasks for Embedding-based Large-scale Retrieval](https://arxiv.org/abs/2002.03932)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on studying pre-training tasks for embedding-based large-scale retrieval models, in order to improve their performance on downstream question answering retrieval tasks. The central hypothesis is that properly designed pre-training tasks that capture paragraph-level semantics are crucial for learning effective retrieval models, compared to just using token-level masked language modeling. Specifically, the paper proposes and evaluates three pre-training tasks:- Inverse Cloze Task (ICT): Given a passage, predict a masked sentence based on the surrounding context within the passage. Captures local semantics.- Body First Selection (BFS): Given the first section of a Wikipedia page, retrieve a relevant passage from the body of the page. Captures global within-document semantics. - Wiki Link Prediction (WLP): Given the first section of a Wikipedia page, retrieve a relevant passage from a page linked to it. Captures cross-document semantics.The key hypothesis is that a combination of these pre-training tasks, compared to just masked language modeling, will allow learning more effective embeddings for the downstream QA retrieval task. The experiments aim to demonstrate that properly pre-trained two-tower Transformer encoders can outperform classic sparse retrieval methods like BM25.In summary, the paper focuses on studying pre-training objectives for embedding-based retrieval, with the hypothesis that paragraph-level pre-training is crucial for this task. The experiments aim to demonstrate the effectiveness of pre-trained two-tower Transformers over classic retrieval baselines.


## What is the main contribution of this paper?

The main contribution of this paper is proposing and evaluating several pre-training tasks for training embedding-based retrieval models, specifically two-tower Transformer models. The key findings are:- They propose three paragraph-level pre-training tasks beyond just masked language modeling: Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). These are designed to capture different levels of semantic relationships between queries and documents. - They show that pre-training two-tower Transformer encoders with these tasks substantially improves performance on question answering retrieval benchmarks compared to no pretraining or just masked LM pretraining.- Their proposed combined pretraining approach ("ICT + BFS + WLP") outperforms strong baselines like BM25 on retrieval metrics across different amounts of QA training data.- Their results demonstrate that two-tower Transformer encoders benefit more from paragraph-level pretraining tasks compared to just token-level masking. Properly pretrained two-tower models can significantly outperform classic sparse retrieval methods like BM25.In summary, the key contribution is highlighting the importance of pre-training objectives beyond just masked LM for learning effective embedding-based retrieval models like two-tower Transformers. The proposed paragraph-level pretraining tasks help capture different types of query-document semantics and lead to large gains on retrieval metrics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes and evaluates three paragraph-level pre-training tasks (Inverse Cloze Task, Body First Selection, and Wiki Link Prediction) for training two-tower Transformer models on large-scale retrieval tasks; it shows that these tasks significantly outperform token-level masked language modeling and allow the models to surpass BM25 on question-answering retrieval benchmarks.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work:- This paper focuses on studying pre-training tasks for embedding-based models for large-scale retrieval. Much prior work has focused on cross-attention models like BERT for scoring, but less on efficient retrieval models.- The paper thoroughly evaluates different pre-training tasks like masked LM, inverse cloze task (ICT), body first selection (BFS), and wiki link prediction (WLP). Prior work has studied ICT, but the novel BFS and WLP tasks are proposed in this paper. - The two-tower Transformer architecture used is similar to prior work on dual-encoder models. However, this paper shows these models benefit much more from paragraph-level pre-training tasks compared to token-level masked LM.- For retrieval baselines, the paper compares against both classic models like BM25 as well as bag-of-words models. The results demonstrate the advantage of Transformer encoders with proper pre-training.- The paper uses the ReQA benchmark based on SQuAD and Natural Questions for evaluation. Other papers have used these datasets but mainly for the reader/scoring phase rather than the retriever phase.- Overall, this paper provides a more comprehensive study on pre-training tasks for embedding-based retrieval models than prior work, introducing new pre-training tasks and showing their impact empirically. The results demonstrate the potential to significantly improve over BM25 using properly pre-trained dual-encoder models.In summary, this paper makes important contributions to better understanding pre-training for efficient retrieval models compared to prior works focused on scoring models or different retrieval model architectures and training methods. The novel pre-training tasks and empirical results are valuable additions to the literature.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Studying how the proposed pre-training tasks apply to other encoder architectures beyond Transformers, such as RNNs or CNNs. The authors showed the benefits of the pre-training tasks on Transformer encoders, but did not explore other architectures.2. Generating the pre-training data from corpora other than Wikipedia. The proposed pre-training tasks rely on Wikipedia for generating the pseudo query-document pairs, but other text corpora could potentially be used as well.3. Comparing pre-training with other regularization methods. The authors suggest that pre-training provides a form of regularization. It would be interesting to compare pre-training more directly to other regularization techniques like dropout. 4. Evaluating the pre-training tasks on other retrieval benchmarks beyond ReQA. The authors focus their evaluation on the ReQA question answering dataset, but these pre-training tasks could potentially help for other retrieval tasks as well.5. Exploring modifications and improvements to the pre-training tasks. For example, different ways of sampling the query and document for the pre-training pairs.6. Studying the sample efficiency and optimization challenges of pre-training for retrieval. The authors pretrain on a large amount of Wikipedia data, but it would be useful to understand how much pretraining data is sufficient.In summary, the main future directions are studying the applicability of the pre-training tasks to other encoders, corpora, tasks, and comparing pre-training to other regularization methods. There is also room for improvement via modifications to the pre-training tasks themselves.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper studies large-scale query-document retrieval, where the goal is to efficiently retrieve the most relevant documents from a large corpus given a query. The authors focus on embedding-based retrieval models, which jointly embed queries and documents in the same space and retrieve documents based on closest embedding matches. They show that properly pre-training the embedding models is crucial - a Transformer model pre-trained with paragraph-level tasks like Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP) substantially outperforms classic token matching methods like BM25 as well as Transfomers without pre-training. The pre-training tasks are designed to capture different semantic relationships between queries and documents, from local context to global document coherence to cross-document entity relationships. Experiments on question answering datasets demonstrate the advantage of the pre-trained Transformer encoder over bag-of-words and non-pre-trained versions, especially when labeled data is limited. The work provides insight into effective pre-training objectives for embedding-based retrieval.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper studies pre-training tasks for embedding-based large-scale retrieval models, with a focus on query-document retrieval. The authors compare different pre-training tasks for two-tower Transformer models, where the query and document are encoded separately. They find that paragraph-level pre-training tasks that capture semantics between sentences and passages lead to much better performance compared to token-level masked language modeling. Specifically, the paper proposes and studies three pre-training tasks: Inverse Cloze Task (ICT) that predicts surrounding sentences given a query sentence, Body First Selection (BFS) that predicts a relevant passage given a sentence from the first section, and Wiki Link Prediction (WLP) that predicts a relevant passage from a linked Wikipedia page. These tasks are applied to the ReQA benchmark based on SQuAD and Natural Questions. The two-tower Transformer model pre-trained with a combination of these tasks substantially outperforms classic BM25 retrieval and Transformers without proper pre-training. The results highlight the importance of designing pre-training tasks suitable for retrieval, and show that properly pre-trained two-tower models can replace BM25 for large-scale retrieval problems.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes using two-tower Transformer models for large-scale retrieval tasks such as question answering. The key contribution is studying different pre-training tasks and their effectiveness in learning good query and document embeddings. They compare token-level masked language modeling (MLM) with paragraph-level pre-training tasks including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). ICT defines query-document pairs within a paragraph, BFS defines pairs within a Wikipedia article, and WLP defines pairs across hyperlinked Wikipedia articles. Experiments on the ReQA benchmark datasets SQuAD and Natural Questions show that combining ICT, BFS, and WLP during pre-training substantially outperforms MLM and no pre-training. The results demonstrate the importance of designing pre-training tasks suitable for retrieval instead of just relying on MLM commonly used for BERT-style models. The pre-trained two-tower Transformer with multi-task learning of ICT, BFS, and WLP achieves new state-of-the-art retrieval performance, outperforming even the widely used BM25 algorithm.
