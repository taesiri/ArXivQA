# [Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient   Deep Neural Network Training](https://arxiv.org/abs/2403.12511)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Training deep neural networks using gradient-based methods requires computation of gradients at each layer, which necessitates large memory overhead for storing intermediate activations when using backpropagation. 
- Forward propagation for gradient computation also has high computational complexity due to large matrix-matrix products.
- Recently, the Projected Forward Gradient method was introduced as a memory-efficient alternative, but its convergence behavior when used within optimization algorithms like Frank-Wolfe (FW) has not been analyzed.

Proposed Solution:
- This paper analyzes using Projected Forward Gradient within the FW algorithm for optimizing neural network training.
- It is shown that directly using Projected Forward Gradient in FW (Algorithm 1) leads to convergence to a non-zero error neighborhood around the optimal solution.
- To achieve exact convergence, Algorithm 2 is proposed which averages across iterations of Projected Forward Gradient directions to reduce noise. 

Main Contributions:
- Rigorously proves that using Projected Forward Gradient in standard FW fails to achieve exact convergence, with error margin quantified (Theorem 1).
- Presents refined Algorithm 2 that provably converges to optimal solution at a sublinear rate by aggregating historical gradient directions (Theorem 2).
- Establishes variance reduction for averaged Projected Forward Gradient estimator (Lemma 1).
- Validates superior convergence of Algorithm 2 over Algorithm 1 numerically on MNIST dataset.
- Overall, enables reduced memory overhead for FW optimization by systematically integrating Projected Forward Gradient.

In summary, the key innovation is a convergent FW algorithm using Projected Forward Gradient that retains the memory efficiency of forward pass gradients while still achieving the optimization performance of backpropagation. The analysis and refined technique significantly advance the applicability of FW methods for memory-constrained training of deep neural networks.
