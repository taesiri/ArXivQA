# [EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked   Audio Gesture Modeling](https://arxiv.org/abs/2401.00374)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Generating coherent and natural full-body gestures, including face, body, hands and global motions, from speech audio is challenging. Existing methods typically address different parts separately and lack a unified representation and evaluation benchmark. There is also a need for models that can incorporate partially available gesture data to guide the generation process. 

Proposed Solution - Main Ideas:

1) Introduce BEATX, a new large-scale motion captured co-speech gesture dataset with unified SMPLX body and FLAME head representations to enable joined training and evaluation.

2) Propose EMAGE, a framework to generate full-body gestures from audio and optionally masked (partial) gesture input. Main components:
- Masked gesture transformer to encode available body part hints 
- Separate face and body encoders for speech features using Content Rhythm Attention  
- Gesture decoders based on compositional Vector Quantized Variational AutoEncoders 
- Pretrained global motion predictor

3) Jointly train for masked gesture reconstruction and audio-to-gesture mapping to leverage partial inputs effectively at test time.

Key Contributions:

- Release of BEATX dataset with 60 hours of refined motion capture data in standardized formats 
- EMAGE model to generate high-quality holistic gestures from audio and partial motions
- State-of-the-art performance on BEATX. Flexible incorporation of partial gestures.
- Demonstrated capability to leverage multiple external datasets to enhance generation quality

The solution enables generating complete and coherent gestures guided by variable partial gesture data available, with applications in animation and human-AI interaction.
