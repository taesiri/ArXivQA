# [Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR](https://arxiv.org/abs/2303.07335)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: 

How can we design an efficient Transformer encoder for DETR-based object detection models to reduce computational cost while maintaining high performance?

Specifically, the authors observe that using multi-scale features is important for good detection performance, especially for small objects. However, directly applying Transformer encoders on multi-scale features leads to very high computational cost due to the quadratic complexity of self-attention. 

To address this challenge, the paper proposes an efficient encoder design called "Lite DETR" that can significantly reduce the computational cost while maintaining competitive performance. The key ideas are:

1) Splitting the multi-scale features into high-level and low-level features and updating them in an interleaved manner with different frequencies. This allows prioritizing computation on more informative high-level features.

2) Introducing a "key-aware deformable attention" mechanism to enhance the lagged update of low-level features.

3) Showing the proposed efficient encoder can generalize across multiple DETR-based detection models and reduce FLOPs by 60% with 99% maintained performance.

In summary, the central hypothesis is that by designing an efficient encoder to update high-level and low-level features asynchronously, it's possible to build DETR models that are much more efficient while preserving accuracy. The experiments validate this hypothesis and demonstrate the effectiveness of the proposed Lite DETR encoder.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

1. Proposing an efficient encoder block that updates high-level and low-level features in an interleaved manner to significantly reduce the number of feature tokens for efficient detection. This encoder block can be easily plugged into existing DETR-based models.

2. Introducing a key-aware deformable attention (KDA) approach to enhance the lagged update of low-level features. KDA can produce more reliable attention weights by sampling both keys and values, which helps compensate for the asynchronous feature update. 

3. Demonstrating through experiments that the proposed Lite DETR framework can reduce the GFLOPs of the detection head by 60% while maintaining 99% of the original performance. Specifically, Lite-DINO with Swin-Tiny backbone achieves 53.9 AP with only 159 GFLOPs on COCO.

4. Showing the efficient encoder strategy generalizes well across multiple DETR-based models like Deformable DETR, DINO, and H-DETR.

In summary, the key contribution is proposing an efficient encoder block and KDA attention to significantly reduce computation cost while maintaining competitive performance on object detection. The efficient design is simple and can be easily integrated into various DETR-based detectors.
