# [Two Tales of Single-Phase Contrastive Hebbian Learning](https://arxiv.org/abs/2402.08573)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Addressed:
- Local, activity difference based learning algorithms like Contrastive Hebbian Learning (CHL) are promising for enabling on-device training of neuromorphic hardware, but most methods require distinct phases and are costly to simulate. 

- The recently proposed Dual Propagation (DP) method overcomes these issues but relies on symmetric nudging of positive and negative neuron compartments, which may be too strict of a requirement for noisy analog hardware.

- Furthermore, the convergence guarantees of DP break down with asymmetric nudging, which is a clear shortcoming.

Solutions Proposed:
- The authors provide a rigorous derivation of the DP objective based on a saddle-point relaxed optimal value reformulation (SPROVR), revealing connections to adversarial robustness. 

- They derive an improved DP method called DP+ from a Lagrangian perspective. DP+ recovers the DP updates under symmetric nudging but leads to slightly different, more robust updates under asymmetric nudging.

Key Contributions:
- Solid theoretical foundation for DP based on bilevel programming relaxations and analysis of the effect of nudging asymmetry.

- New robust variant DP+ that works with arbitrary nudging while maintaining efficiency.

- Analysis and experiments demonstrating the effect of nudging asymmetry on convergence guarantees, Lipschitz constant of models, and robustness to perturbations.

- Results showing DP+ successfully trains VGG-style ConvNets on CIFAR and Imagenet with asymmetric nudging, overcoming limitations of original DP.

In summary, the authors significantly improve theoretical grounding and properties of dual propagation for local contrastive learning, with both biological plausibility and computational implications.
