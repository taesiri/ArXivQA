# [When Scaling Meets LLM Finetuning: The Effect of Data, Model and   Finetuning Method](https://arxiv.org/abs/2402.17193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Understanding the scaling properties and inductive biases of different finetuning methods for large language models (LLMs) is important but under-explored. 
- Key factors that can affect LLM finetuning performance include LLM model size, pretraining data size, finetuning data size, parameter-efficient tuning (PET) parameter size, and finetuning methods.
- The paper aims to systematically study how these factors jointly scale for two popular finetuning approaches - full-model tuning (FMT) and PET (prompt tuning and LoRA).

Proposed Solution
- Propose a multiplicative joint scaling law between finetuning data size and other factors: performance = A/(X^α * D_f^β) + E, where X is another scaling factor.
- Conduct extensive experiments with bilingual LLMs (English-German/Chinese) from 1B to 16B parameters on machine translation and summarization.
- Compare scaling behaviours of FMT vs prompt tuning vs LoRA under factors like model size, pretraining data, and finetuning data.

Key Findings
- The proposed joint scaling law captures the empirical scaling trends well across tasks and methods. 
- Increasing model size benefits finetuning more than scaling pretraining data. PET parameter scaling is ineffective.  
- FMT relies more on finetuning data than PET. LoRA scales better with data than prompt tuning.
- Optimal finetuning method depends greatly on task and available finetuning data.
- PET encourages better generalization than FMT for zero-shot and few-shot transfer.

Main Contributions
- First systematic study on how key factors jointly scale for LLM finetuning.
- Proposition and verification of a multiplicative joint scaling law.
- Extensive experiments on bilingual LLMs from 1B to 16B parameters.
- Analysis and insights on scaling behaviours of different finetuning methods.
