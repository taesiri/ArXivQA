# [DarSwin: Distortion Aware Radial Swin Transformer](https://arxiv.org/abs/2304.09691)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: How can we design a transformer-based model that can automatically adapt to the distortion produced by different wide-angle lenses?

The key ideas and contributions seem to be:

- Proposing a novel transformer encoder architecture called DarSwin that embeds knowledge of the lens distortion profile into its structure. This allows it to adapt to different distortion profiles.

- Introducing distortion-aware components into DarSwin:
  - Polar patch partitioning 
  - Distortion-based sampling for token embeddings
  - Angular relative positional encoding for patch merging

- Showing through classification experiments on distorted ImageNet that DarSwin can perform zero-shot generalization to different lens distortion profiles much better than baseline methods like Swin and deformable attention transformers.

So in summary, the main hypothesis is that by designing a distortion-aware transformer model like DarSwin, we can enable better generalization and adaptation to different lens distortion profiles in a zero-shot manner. The paper aims to demonstrate this via extensive experiments.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution appears to be proposing a new transformer-based architecture called DarSwin that can automatically adapt to the distortion profile of a camera lens. The key ideas are:

- Using a polar patch partitioning strategy rather than the cartesian strategy typically used in vision transformers like Swin. This allows the image patches to better match the radial distortion of wide-angle lenses.

- Employing a distortion-aware sampling technique to create token embeddings from the polar patches. This helps embed knowledge of the lens distortion into the model architecture.

- Using an angular relative positional encoding scheme to capture relationships between the polar patch tokens. 

- Introducing distortion-aware polar patch merging operations in the hierarchical architecture.

Through experiments on distorted ImageNet images, the authors demonstrate that DarSwin can perform zero-shot generalization to new lens distortion profiles not seen during training. Compared to baselines like Swin and deformable attention transformers, DarSwin achieves superior performance when evaluating on out-of-distribution distortions.

In summary, the key contribution appears to be proposing a novel transformer architecture that can implicitly adapt to lens distortion profiles, enabling better generalization across different wide-angle lenses compared to prior approaches. The polar coordinate-based design choices allow embedding distortion knowledge into the model itself.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it relates and compares to other research in the field:

- The paper presents a novel transformer-based architecture called DarSwin that is designed to automatically adapt to the distortion profile of wide-angle lenses. This sets it apart from previous transformer architectures like Swin and ViT that use a cartesian grid partitioning and do not account for lens distortion.

- It is most directly related to other works aiming to enable deep networks to handle wide-angle/fisheye images without undistorting them first, such as deformable CNNs and spherical CNNs. However, DarSwin is the first transformer-based approach shown to adapt to lens distortion.

- Compared to deformable CNN methods, DarSwin does not require expensive per-sample kernel deformation and can handle a wide field of view. Spherical CNNs adapt to spherical distortion but have not been shown to generalize to other wide-angle lenses.

- DarSwin relies on a distortion-aware polar partitioning of the image, angular positional encoding, and distortion-based sampling. These components allow it to embed knowledge of the lens geometry into the architecture itself.

- Experiments demonstrate DarSwin's superior generalization ability compared to baselines when training on one lens distortion profile and testing on others. This indicates it is better able to handle the "distortion gap" problem.

- Limitations include the need for a calibrated lens, sparse sampling, and restriction to classification tasks so far. But the results indicate promise for future extensions to uncalibrated settings and dense prediction tasks.

In summary, DarSwin introduces a novel transformer architecture that sets a new state-of-the-art for adapting to wide-angle lens distortion in a zero-shot manner, demonstrating advantages over prior CNN and transformer approaches. There is significant potential for future work building on these ideas.
