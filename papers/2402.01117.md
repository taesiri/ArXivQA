# [DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models](https://arxiv.org/abs/2402.01117)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Leading text-to-SQL models rely heavily on large proprietary language models (LLMs) like GPT-3.5-turbo, raising concerns over data privacy and cost.  
- Smaller open-source LLMs fine-tuned on question-SQL pairs lag behind proprietary models in performance. For example, the best open-source LLM reaches 63.9% execution accuracy on Spider dev set, while a prompting method with GPT-4 achieves 74.4%.

Proposed Solution:
- A two-stage fine-tuning approach called Decomposed Text-to-SQL (DTS-SQL) that separates the text-to-SQL task into schema linking and SQL generation components.
- Stage 1 fine-tunes an LLM to identify relevant tables and columns for a given natural language question. 
- Stage 2 fine-tunes a separate LLM to generate the SQL query based only on the question and schemas of the relevant tables from stage 1.

Main Contributions:
- DTS-SQL aligns the performance of 7B parameter open-source LLMs with proprietary 175B parameter models, reaching 79.1% execution accuracy on Spider dev set.
- With DeepSeek, DTS-SQL achieves state-of-the-art results among open-source methods on Spider dev set and is comparable to the best open-source method on Spider test set.
- DTS-SQL generalizes well to the Spider-SYN cross-domain dataset, despite not being directly fine-tuned on it.
- By decomposing text-to-SQL into two stages, DTS-SQL allows smaller LLMs to rival much larger proprietary models, helping mitigate privacy and cost concerns.
