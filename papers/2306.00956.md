# [The ObjectFolder Benchmark: Multisensory Learning with Neural and Real   Objects](https://arxiv.org/abs/2306.00956)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: how can we collect multisensory data from real-world objects and use it to enable research in multisensory object-centric learning? The key contributions are:1. Introducing the ObjectFolder Real (OFR) dataset, which contains multisensory measurements of 100 real household objects, including 3D meshes, videos, impact sounds, and tactile data. 2. Designing a pipeline to systematically collect high-fidelity visual, acoustic, and tactile data for real objects.3. Proposing the ObjectFolder Benchmark, a suite of 10 tasks centered around object recognition, reconstruction, and manipulation using multiple senses. 4. Conducting experiments on the neural objects from ObjectFolder V2 and real objects from OFR to analyze the distinct roles of vision, audio, and touch for different object-centric tasks.5. Demonstrating the value of OFR for transfer learning and sim2real, achieving improved performance by pretraining on OFR compared to other datasets.In summary, the central hypothesis is that multisensory modeling of real objects can enable more robust perception for various object-centric tasks, which is validated through the systematic data collection, benchmarking, and analysis.


## What is the main contribution of this paper?

This paper introduces the ObjectFolder Benchmark, which consists of two main contributions:1. The ObjectFolder Real dataset: This contains multisensory data (3D meshes, videos, impact sounds, tactile readings) collected from 100 real-world household objects. It represents the first dataset with comprehensive real-world visual, acoustic, and tactile measurements. 2. A benchmark suite of 10 tasks for multisensory object-centric learning: The tasks cover object recognition (e.g. cross-sensory retrieval), reconstruction (e.g. 3D shape reconstruction), and manipulation (e.g. grasp stability prediction). Experiments are conducted on both the neural objects from ObjectFolder v2 and the real objects from ObjectFolder Real.The key novelty is providing a standardized benchmark for multisensory perception research, including both simulated and real data. By releasing the dataset and benchmark tasks, the work aims to enable and catalyze new research directions in multisensory object modeling.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new multisensory dataset of real objects called ObjectFolder Real and a benchmark suite of 10 tasks for multisensory object-centric learning involving vision, sound, and touch modalities.


## How does this paper compare to other research in the same field?

Here are a few key comparisons between this paper and other related research in multisensory object-centric learning:- Datasets: This paper introduces the ObjectFolder Real dataset, which contains multisensory data (3D meshes, videos, impact sounds, tactile readings) collected from 100 real household objects. This is the first dataset to contain all three sensory modalities for real objects to enable multisensory research. Prior multisensory datasets either use only simulated data (e.g. ObjectFolder CVPR) or are collected for a specific task. - Tasks: The paper presents a suite of 10 diverse tasks (recognition, reconstruction, manipulation) to systematically benchmark multisensory perception. In contrast, most prior work focused on a single task like 3D reconstruction or material recognition. The large task suite enables studying the distinct roles of vision, audio, and touch.- Approaches: The paper develops baseline models for each task by adapting state-of-the-art techniques like transformers. Prior work often used CNNs/MLPs for feature extraction. The systematic benchmarking reveals that combining multisensory data consistently improves performance across nearly all tasks.- Real vs Simulated: A key contribution is benchmarking on real data, in addition to the simulated neural objects from ObjectFolder CVPR. This allows quantifying the sim-to-real gap. The results demonstrate the value of real measurements for multisensory learning.- Applications: The multisensory modeling of real objects could benefit diverse applications in computer vision, robotics, VR/AR, and graphics. In contrast, most prior multisensory research focused on a single application area.In summary, the large-scale and systematic benchmarking of multisensory perception in this paper advances the state-of-the-art in object-centric learning across multiple dimensions like tasks, datasets, approaches, and applications.
