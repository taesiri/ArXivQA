# [Model Parallelism on Distributed Infrastructure: A Literature Review   from Theory to LLM Case-Studies](https://arxiv.org/abs/2403.03699)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
The paper addresses the challenges of training increasingly large and complex neural network models. As models grow in size, their computational and memory requirements also increase dramatically. This pushes the limits of current hardware capabilities for both training and inference. The paper focuses on "model parallelism" as a technique to distribute the workload across multiple devices to overcome these hardware constraints.  

The paper seeks to answer three key research questions:
1) What types of model parallelism exist?
2) What are the challenges of model parallelism? 
3) What is a modern use case of model parallelism?

Types of Model Parallelism  
The paper identifies two main types of model parallelism:

- Intra-operator parallelism: Partitioning computation within a neural network layer/operator across devices. This has very high communication costs.

- Inter-operator parallelism: Partitioning the model across operators/layers, assigning each sub-graph to a device. This suffers from low device utilization during training. 

The two types are often combined in "hybrid parallelism".

Challenges of Model Parallelism
Key challenges include:

- High communication costs for intra-operator parallelism
- Low utilization for inter-operator parallelism during training due to pipeline bubbles
- Finding the optimal partitioning strategy between types of parallelism is very difficult given constraints.

Modern Use Case  
The paper analyzes how model parallelism is used to train large Transformer models with hundreds of billions of parameters. Different projects use combinations of intra-, inter- and data- parallelism to scale up. There are tradeoffs in communication costs, hardware utilization and finding optimal configurations.

Key Outcomes
In the end, the paper provides a clear overview of types of model parallelism, associated challenges, and real-world use cases and implementations. The discussion highlights that continued progress will require co-design of specialized hardware, software frameworks, and search techniques for finding efficient parallelization strategies matched to model architectures.


## Summarize the paper in one sentence.

 This paper reviews model parallelism techniques for distributing large neural networks across multiple devices, summarizing the types, challenges, and modern use cases of model parallelism, with a focus on recent trillion-parameter transformer models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Providing a theoretical framework and taxonomy for reasoning about model parallelism in neural networks. This includes formulating neural networks as operator graphs and defining concepts like intra-operator and inter-operator parallelism.

2) Reviewing and summarizing the major challenges of model parallelism, including the technical tradeoffs between different types of parallelism, the problem of finding optimal parallelization strategies, and issues like pipeline bubbles.

3) Surveying modern use cases of model parallelism, focusing on how it has been applied in large Transformer models like Megatron, Gopher, and PaLM. The paper collects details on the parallelism techniques and hardware used for these huge models.

4) Providing an overview of automatic parallelization frameworks for neural networks, comparing their approach to search spaces, search methods, and strategy evaluation.

5) Identifying areas for future improvement, like the need for standardization and benchmark datasets to better evaluate and compare auto-parallelization methods.

In summary, the paper aims to provide a broad overview of the landscape of model parallelism for neural networks, spanning theory, challenges, real-world applications, and automation. The taxonomy, challenge summary, Transformer case studies, and framework comparison seem to be the key contributions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms associated with it are:

- Model parallelism
- Auto-parallelism
- Transformers
- Distributed deep learning
- Inter-operator parallelism
- Intra-operator parallelism 
- Operator graphs
- Compute flow graphs
- Neural network workloads
- Forward pass
- Backward pass
- Pipeline parallelism
- Tensor parallelism
- Megatron
- Gopher
- PaLM
- TPUs
- A100 GPUs
- NVLink
- Search space
- Strategy evaluation 
- Markov chain Monte Carlo
- FlexFlow
- SOAP search space

The paper provides an overview of different types of model parallelism for neural networks, challenges associated with implementing model parallelism, and modern use cases like large Transformer models. It discusses techniques like pipelining and partitioning models across devices to parallelize computation and memory. Overall, the key focus is on methods and frameworks for efficiently distributing neural network models across distributed hardware infrastructure.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper discusses both intra-operator and inter-operator parallelism. What are the key differences between these two types of parallelism in terms of communication costs and hardware utilization? How do the tradeoffs differ?

2. The paper mentions the concept of "hybrid parallelism" which combines intra-operator, inter-operator, and data parallelism. What are some of the key challenges in finding the optimal hybrid parallelism strategy? How do the different search methods attempt to address this?

3. The paper discusses some of the dimensions along which parallelism can occur such as sample, operator, attribute, and parameter dimensions. Can you further explain these dimensions and give examples of where each type of parallelism might be applicable? 

4. The concept of a pipeline bubble is mentioned as one of the key challenges of inter-operator parallelism. Can you explain why this occurs and how techniques like micro-batching aim to address it? What are the potential downsides?

5. The Megatron and PaLM/Gopher model families take quite different approaches to parallelism even though they are both scaling up Transformer models. Can you contrast their approaches and speculate on the reasons for the differences?

6. FlexFlow introduces the SOAP search space for exploring parallelism strategies. Can you explain the SOAP formulation in more detail? What are some examples of candidate parallelism strategies it might generate?  

7. The paper argues that comparing different auto-parallelisation frameworks is challenging due to lack of standardized representations and testing methodology. Can you suggest ways the community could aim to address this? What specifically could be standardized?

8. Could the concept of Neural Architecture Search (NAS) be useful for standardizing auto-parallelisation? If so, how might a dataset like NAS-Bench be formulated for this problem? What would a parallelisation search space look like?

9. The scheduler simulation method used by FlexFlow to evaluate strategies relies on profiling and estimation. What are limitations of this approach vs an analytical model or actual profiling on hardware?

10. For the largest models discussed, none of the papers achieve greater than 60% hardware utilization. Why is it so difficult to achieve higher utilization rates at this scale? How might utilization be improved in future work?
