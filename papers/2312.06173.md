# [Concrete Subspace Learning based Interference Elimination for Multi-task   Model Fusion](https://arxiv.org/abs/2312.06173)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points in the paper:

This paper proposes a novel method called Concrete Subspace Learning to address the issue of task interference in multi-task model fusion. The key idea is to identify a common low-dimensional subspace across fine-tuned models targeting different downstream tasks, then perform model merging within this shared subspace to leverage common information while mitigating negative interference. Specifically, they formulate a bi-level optimization problem to meta-learn a concrete mask parameterized by temperature and logits to capture the shared subspace. In the inner loop, model fusion is performed using techniques like Task Arithmetic or AdaMerging to maximize performance. In the outer loop, the concrete mask logits are optimized to find a beneficial subspace common across tasks. This concrete mask provides a differentiable relaxation of a discrete mask. Extensive experiments were conducted on vision and language domains with fully fine-tuned and LoRA fine-tuned models. The results demonstrate superior performance over comparison methods like Task Arithmetic, Ties-Merging and AdaMerging. The effectiveness highlights the promise of identifying and utilizing shared subspaces for multi-task model fusion. Key strengths include the method's versatility as an add-on and its ability to resolve task interference while retaining model capabilities.
