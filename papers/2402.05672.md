# [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most existing text embedding models are trained exclusively on English data, limiting their applicability to other languages. There is a need for high-quality multilingual text embeddings.

Proposed Solution:
- Release open-source multilingual E5 text embedding models in 3 sizes - small, base and large. The models are pre-trained using contrastive learning on 1 billion multilingual text pairs from diverse sources. They are then fine-tuned on labeled datasets across 93 languages.

- Also release an instruction-tuned variant, mE5-large-instruct, which is fine-tuned on additional synthetic instruction-based data from GPT-3.5/4 to better inform the model about downstream tasks.

Key Contributions:
- Demonstrate competitive performance of multilingual E5 models on English retrieval tasks, with the instruction-tuned model surpassing even English-only models.

- Showcase strong performance on multilingual retrieval using the MIRACL benchmark across 16 languages, significantly outperforming prior methods.

- Validate cross-lingual capability on bitext mining across 112 languages, with the instructed model achieving new state-of-the-art results by expanding language coverage.

- Release high-quality multilingual embeddings to enable semantic search, clustering and other applications for practitioners working with non-English texts.

In summary, the key highlight is the release of open-source multilingual E5 models with competitive performance on English and multilingual benchmarks to advance multilingual NLP.
