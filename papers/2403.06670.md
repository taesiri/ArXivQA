# [CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar   Class-Incremental Learnin](https://arxiv.org/abs/2403.06670)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper focuses on the problem of non-exemplar class incremental learning (NECIL). In NECIL, the model needs to continually learn new classes from incremental data without storing any samples from previous tasks. This is more challenging than traditional class incremental learning methods that store exemplars, but also more practical for privacy-sensitive applications. Two key challenges in NECIL are:

1) Plasticity-stability dilemma: The model needs plasticity to learn new knowledge while maintaining stability to retain old knowledge, which is a dilemma. 

2) Classifier bias: The classifier tends to be biased towards newly learned classes and misclassify old classes.

Proposed Solution:
The paper proposes a Continual Expansion and Absorption Transformer (CEAT) to address the challenges in NECIL. The key ideas are:

1) Parameter expansion & absorption: Freeze backbone network to maintain old knowledge stability. Expand trainable fusion layers in parallel to enable plasticity for new classes. After training a task, absorb fusion layers into backbone, keeping model size constant.

2) Batch interpolation pseudo-features: Dynamically generate features mixing old class prototypes and new data to correct classifier bias. Maintains better decision boundaries over tasks.  

3) Prototype contrastive loss: Contrast new class features against old class prototypes to enforce separation and reduce overlap of features. Improves learning.

Main Contributions:

1) Novel CEAT architecture tailored for NECIL that freezes backbone and expands/absorbs fusion layers to balance plasticity and stability.

2) Batch interpolation pseudo-feature approach to dynamically correct classifier bias without storing exemplars.

3) Prototype contrastive loss to achieve inter-class separation between old and new classes, improving learning.

Experiments show state-of-the-art performance on CIFAR-100, TinyImageNet and ImageNet-Subset benchmarks, surpassing prior NECIL methods by significant margins. Ablations validate efficacy of each proposed component.
