# [Fast Differentiable Matrix Square Root](https://arxiv.org/abs/2201.08663)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research questions and hypotheses appear to be:- How can we compute the matrix square root and its inverse in a fast and differentiable manner for use in deep learning models and applications? - The central hypothesis is that using Matrix Taylor Polynomials (MTP) and Matrix Padé Approximants (MPA) for the forward pass, along with an iterative Lyapunov solver for the backward pass, will allow much more efficient computation compared to prior methods like SVD or Newton-Schulz iteration.Specifically, the paper proposes using MTP and MPA to approximate the matrix square root in the forward pass. These avoid the expensive SVD decomposition and improve upon the Newton-Schulz iteration in terms of speed and/or accuracy. For the backward pass, the paper formulates the gradient as a Lyapunov equation and proposes an iterative solution using the matrix sign function. This requires fewer matrix operations than Newton-Schulz and is more efficient.The main hypothesis is that this combination of MTP/MPA for forward propagation and the Lyapunov solver for backpropagation will yield considerable speedups compared to SVD or Newton-Schulz, while maintaining competitive accuracy and performance on deep learning tasks involving differentiable matrix square roots.The experiments validate this hypothesis by demonstrating speedups on numerical tests as well as computer vision tasks like decorrelated batch normalization and second-order vision transformers. The results show the proposed methods can achieve similar or better accuracy than prior techniques while requiring much less computation time.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing two efficient methods to compute the differentiable matrix square root - Matrix Taylor Polynomial (MTP) and Matrix Padé Approximants (MPA) for the forward pass. These are shown to be much faster than using SVD or Newton-Schulz iteration. - Deriving an iterative algorithm to solve the gradient equation in the backward pass using matrix sign functions and Lyapunov equations. This is more efficient than previously used methods for computing gradients.- Conducting extensive numerical experiments to evaluate the speed and accuracy of the proposed methods compared to baselines like SVD and Newton-Schulz iteration. The results show MTP and MPA with Lyapunov solver consistently outperform prior methods.- Applying the proposed techniques in two applications - decorrelated batch normalization and second-order vision transformers. The methods achieve competitive or better performance compared to baselines while being significantly faster.In summary, the key contribution is developing faster and more efficient algorithms for computing differentiable matrix square roots, both in the forward and backward pass. This is shown to provide speedups in applications relying on matrix square roots while maintaining accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes two efficient methods to compute the differentiable matrix square root - Matrix Taylor Polynomial (MTP) and Matrix Padé Approximants (MPA) for the forward pass, and an iterative Lyapunov solver for the backward pass; these achieve faster runtime compared to prior methods while maintaining competitive accuracy on applications like decorrelated batch normalization and second-order vision transformers.


## How does this paper compare to other research in the same field?

This paper presents new methods for computing the differentiable matrix square root that are more efficient than previous approaches. Here are some key points in comparing it to related work:- Previous methods for differentiable matrix square root relied on SVD or Newton-Schulz iteration. This paper shows these are inefficient for deep learning due to expensive forward pass (SVD) or backward pass (Newton-Schulz). - The proposed methods use matrix Taylor polynomial (MTP) or matrix Padé approximants (MPA) for efficient forward pass. For backward pass, they solve the gradient equation as a Lyapunov equation using matrix sign function.- Compared to SVD, the proposed methods are much faster, especially for batched input. MTP is the fastest and MPA is slower but more accurate than Newton-Schulz.- Experiments on decorrelated batch norm and second-order vision transformers show the proposed methods achieve competitive or better performance than SVD/Newton-Schulz, with considerably less computation time.- The methods are easy to implement in Pytorch and code is provided. This makes them very accessible and applicable for future deep learning research needing differentiable matrix square root.In summary, this paper makes matrix square root more viable for deep learning by providing faster and well-performing differentiable methods. It convincingly demonstrates their superiority over commonly used approaches like SVD and Newton-Schulz iteration. The paper is a nice contribution to the field of efficient deep learning methods involving matrix calculus.
