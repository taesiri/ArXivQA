# [Fast Differentiable Matrix Square Root](https://arxiv.org/abs/2201.08663)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research questions and hypotheses appear to be:- How can we compute the matrix square root and its inverse in a fast and differentiable manner for use in deep learning models and applications? - The central hypothesis is that using Matrix Taylor Polynomials (MTP) and Matrix Padé Approximants (MPA) for the forward pass, along with an iterative Lyapunov solver for the backward pass, will allow much more efficient computation compared to prior methods like SVD or Newton-Schulz iteration.Specifically, the paper proposes using MTP and MPA to approximate the matrix square root in the forward pass. These avoid the expensive SVD decomposition and improve upon the Newton-Schulz iteration in terms of speed and/or accuracy. For the backward pass, the paper formulates the gradient as a Lyapunov equation and proposes an iterative solution using the matrix sign function. This requires fewer matrix operations than Newton-Schulz and is more efficient.The main hypothesis is that this combination of MTP/MPA for forward propagation and the Lyapunov solver for backpropagation will yield considerable speedups compared to SVD or Newton-Schulz, while maintaining competitive accuracy and performance on deep learning tasks involving differentiable matrix square roots.The experiments validate this hypothesis by demonstrating speedups on numerical tests as well as computer vision tasks like decorrelated batch normalization and second-order vision transformers. The results show the proposed methods can achieve similar or better accuracy than prior techniques while requiring much less computation time.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing two efficient methods to compute the differentiable matrix square root - Matrix Taylor Polynomial (MTP) and Matrix Padé Approximants (MPA) for the forward pass. These are shown to be much faster than using SVD or Newton-Schulz iteration. - Deriving an iterative algorithm to solve the gradient equation in the backward pass using matrix sign functions and Lyapunov equations. This is more efficient than previously used methods for computing gradients.- Conducting extensive numerical experiments to evaluate the speed and accuracy of the proposed methods compared to baselines like SVD and Newton-Schulz iteration. The results show MTP and MPA with Lyapunov solver consistently outperform prior methods.- Applying the proposed techniques in two applications - decorrelated batch normalization and second-order vision transformers. The methods achieve competitive or better performance compared to baselines while being significantly faster.In summary, the key contribution is developing faster and more efficient algorithms for computing differentiable matrix square roots, both in the forward and backward pass. This is shown to provide speedups in applications relying on matrix square roots while maintaining accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes two efficient methods to compute the differentiable matrix square root - Matrix Taylor Polynomial (MTP) and Matrix Padé Approximants (MPA) for the forward pass, and an iterative Lyapunov solver for the backward pass; these achieve faster runtime compared to prior methods while maintaining competitive accuracy on applications like decorrelated batch normalization and second-order vision transformers.


## How does this paper compare to other research in the same field?

This paper presents new methods for computing the differentiable matrix square root that are more efficient than previous approaches. Here are some key points in comparing it to related work:- Previous methods for differentiable matrix square root relied on SVD or Newton-Schulz iteration. This paper shows these are inefficient for deep learning due to expensive forward pass (SVD) or backward pass (Newton-Schulz). - The proposed methods use matrix Taylor polynomial (MTP) or matrix Padé approximants (MPA) for efficient forward pass. For backward pass, they solve the gradient equation as a Lyapunov equation using matrix sign function.- Compared to SVD, the proposed methods are much faster, especially for batched input. MTP is the fastest and MPA is slower but more accurate than Newton-Schulz.- Experiments on decorrelated batch norm and second-order vision transformers show the proposed methods achieve competitive or better performance than SVD/Newton-Schulz, with considerably less computation time.- The methods are easy to implement in Pytorch and code is provided. This makes them very accessible and applicable for future deep learning research needing differentiable matrix square root.In summary, this paper makes matrix square root more viable for deep learning by providing faster and well-performing differentiable methods. It convincingly demonstrates their superiority over commonly used approaches like SVD and Newton-Schulz iteration. The paper is a nice contribution to the field of efficient deep learning methods involving matrix calculus.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Extending the methods to other applications of differentiable matrix square root, such as neural style transfer and covariance pooling for CNNs. The authors state they would like to explore using their proposed methods in these other domains that also rely on computing the differentiable matrix square root.- Developing a fast and complete algorithm for computing the inverse square root, not just the matrix square root. The authors currently use LU factorization to derive the inverse but mention their methods could potentially be extended to directly compute the inverse square root as well.- Identifying the reason why combining the Newton-Schulz iteration (NS iteration) with the Lyapunov solver for the backward pass (NS-Lya) leads to instability and non-convergence. The authors were not able to get NS-Lya to work successfully and suggest further analysis to understand the underlying cause. - General exploration of other potential applications of differentiable matrix functions in deep learning. The authors propose two efficient methods for the matrix square root specifically, but suggest there could be broader impact in studying other matrix function implementations.In summary, the main future directions are applying the methods to other tasks, developing efficient algorithms for the inverse square root, debugging the issues with NS-Lya, and general investigation of differentiable matrix functions for deep learning. The authors lay out promising ways to build on their work on fast differentiable matrix square root approaches.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes two efficient methods for computing the differentiable matrix square root and its inverse. The forward pass uses either Matrix Taylor Polynomials (MTP) or Matrix Padé Approximants (MPA) to approximate the matrix square root. MTP is faster while MPA is more accurate. The backward pass views the gradient as a Lyapunov equation and solves it iteratively using the matrix sign function. This is more efficient than prior methods like singular value decomposition (SVD) or Newton-Schulz iteration. Experiments on decorrelated batch normalization and second-order vision transformers show the proposed methods achieve competitive accuracy with less computation time. Overall, the paper presents faster techniques to compute differentiable matrix square roots and inverse square roots, enabling more efficient use in applications like covariance pooling and whitening transforms.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes two efficient methods to compute the differentiable matrix square root and its inverse. The first method uses Matrix Taylor Polynomials (MTP) to approximate the matrix square root in the forward pass. The second method uses Matrix Padé Approximants (MPA), which are more accurate than MTP but slightly slower. For the backward pass, the paper derives an iterative solution to the gradient function using matrix sign functions and solving Lyapunov equations. This backward pass is more efficient than prior methods like the Newton-Schulz iteration. The proposed methods are evaluated on tasks like decorrelated batch normalization and second-order vision transformers. Experiments demonstrate consistent speed improvements over baselines like the SVD and Newton-Schulz, especially for batched input matrices. The MPA method also achieves competitive or better accuracy than prior methods. Overall, the proposed techniques provide fast and accurate ways to compute differentiable matrix square roots and inverse square roots, useful for various computer vision applications. Key advantages are efficiency for batched matrices and stable gradients from the Lyapunov solver.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper proposes two efficient methods for computing the differentiable matrix square root and its inverse. For the forward pass, the matrix square root is approximated using either the Matrix Taylor Polynomial (MTP) method or the Matrix Padé Approximants (MPA) method. MTP uses a truncated Taylor series expansion to compute the approximation, while MPA matches the Taylor series expansion to rational functions to avoid the issue of slow convergence. For the backward pass, the gradient is computed by iteratively solving the continuous Lyapunov equation using the matrix sign function and Newton-Schulz iteration. Compared to prior methods like singular value decomposition or the Newton-Schulz iteration alone, the proposed MTP-Lya and MPA-Lya methods achieve faster computation in both the forward and backward passes. Experiments on decorrelated batch normalization and second-order vision transformers demonstrate that the new methods attain state-of-the-art speed while maintaining competitive accuracy.
