# [Fast Differentiable Matrix Square Root](https://arxiv.org/abs/2201.08663)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research questions and hypotheses appear to be:- How can we compute the matrix square root and its inverse in a fast and differentiable manner for use in deep learning models and applications? - The central hypothesis is that using Matrix Taylor Polynomials (MTP) and Matrix Padé Approximants (MPA) for the forward pass, along with an iterative Lyapunov solver for the backward pass, will allow much more efficient computation compared to prior methods like SVD or Newton-Schulz iteration.Specifically, the paper proposes using MTP and MPA to approximate the matrix square root in the forward pass. These avoid the expensive SVD decomposition and improve upon the Newton-Schulz iteration in terms of speed and/or accuracy. For the backward pass, the paper formulates the gradient as a Lyapunov equation and proposes an iterative solution using the matrix sign function. This requires fewer matrix operations than Newton-Schulz and is more efficient.The main hypothesis is that this combination of MTP/MPA for forward propagation and the Lyapunov solver for backpropagation will yield considerable speedups compared to SVD or Newton-Schulz, while maintaining competitive accuracy and performance on deep learning tasks involving differentiable matrix square roots.The experiments validate this hypothesis by demonstrating speedups on numerical tests as well as computer vision tasks like decorrelated batch normalization and second-order vision transformers. The results show the proposed methods can achieve similar or better accuracy than prior techniques while requiring much less computation time.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing two efficient methods to compute the differentiable matrix square root - Matrix Taylor Polynomial (MTP) and Matrix Padé Approximants (MPA) for the forward pass. These are shown to be much faster than using SVD or Newton-Schulz iteration. - Deriving an iterative algorithm to solve the gradient equation in the backward pass using matrix sign functions and Lyapunov equations. This is more efficient than previously used methods for computing gradients.- Conducting extensive numerical experiments to evaluate the speed and accuracy of the proposed methods compared to baselines like SVD and Newton-Schulz iteration. The results show MTP and MPA with Lyapunov solver consistently outperform prior methods.- Applying the proposed techniques in two applications - decorrelated batch normalization and second-order vision transformers. The methods achieve competitive or better performance compared to baselines while being significantly faster.In summary, the key contribution is developing faster and more efficient algorithms for computing differentiable matrix square roots, both in the forward and backward pass. This is shown to provide speedups in applications relying on matrix square roots while maintaining accuracy.
