# [Contact-aware Human Motion Generation from Textual Descriptions](https://arxiv.org/abs/2403.15709)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

The paper addresses the problem of generating realistic 3D human motion sequences that interact with objects, given textual descriptions specifying the actions and contacts between different body parts and objects. This is a challenging task due to the complexity of modeling detailed physical interactions in both motion and language.

To tackle this challenge, the paper makes contributions in both data and algorithms:

Data Contribution: 
The paper constructs a new dataset called RICH-CAT spanning over 8,500 motion-text pairs across 26 actions. The key features of this dataset are:
(1) High-quality motion capture data of humans interacting with objects in real-world scenes 
(2) Precise vertex-level labels of human-object contacts
(3) Detailed textual descriptions generated automatically to specify interactions between body parts and objects

Algorithm Contribution:
The paper proposes a new approach called CATMO for contact-aware text-to-motion generation. The key ideas are:
(1) Uses two separate VQ-VAEs to model motion and contacts in distinct complementary latent spaces
(2) Employs an intertwined GPT model that generates motion and contacts in a mutually conditioned way to explicitly incorporate contacts into motion
(3) Introduces a pre-trained text encoder to better understand fine-grained interaction semantics in text

Experiments show CATMO generates more accurate and stable motion sequences interacting with objects compared to previous text-to-motion approaches. An extension to human-object interaction synthesis in static scenes is also demonstrated.

In summary, the paper pioneers modeling of complex human-object contact dynamics for text-driven motion generation, through a novel dataset and effective learning approach. The motion sequences produced interact naturally with specified objects based on textual descriptions.
