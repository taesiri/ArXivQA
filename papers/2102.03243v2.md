# [Hyperspherical embedding for novel class classification](https://arxiv.org/abs/2102.03243v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper addresses is:

How can deep neural networks be trained to effectively classify novel/unseen classes without requiring retraining or fine-tuning on those new classes?

The paper proposes using the normalized softmax loss (NSL) as an approach to enforce cosine similarity in the latent feature space. This allows new class weights to be inferred for novel classes using just a few labeled examples, rather than requiring retraining on the new classes. 

Specifically, the main hypothesis seems to be:

The NSL approach can outperform common metric learning techniques like triplet loss and contrastive loss for classifying novel classes, while also being more scalable since it does not require pairwise training.

The experiments aim to validate this hypothesis by comparing NSL to metric learning approaches on image classification datasets under both disjoint and joint evaluation scenarios involving novel classes not seen during training. The paper hypothesizes NSL will be more effective and efficient for this open set classification problem.

In summary, the core research question is how to effectively classify novel classes without retraining, with the hypothesis that NSL can outperform metric learning approaches. The experiments aim to validate whether NSL lives up to that hypothesis across different datasets and scenarios.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method to classify novel classes without retraining an existing neural network model. The key ideas are:

- Using the normalized softmax loss (NSL) which enforces a cosine similarity metric between classes in the neural network's latent space. This allows novel classes to be classified based on their similarity to existing class representations.

- Adding a new neuron to the output layer for each novel class and inferring the weights between the new neuron and the previous layer based on labeled examples of that class. This avoids retraining the full network. 

- Comparing this NSL approach to common metric learning techniques like triplet loss and contrastive loss which require pairwise training. The NSL method is shown to achieve better accuracy without needing pairwise data.

- Evaluating the NSL approach on MNIST, Fashion MNIST, CIFAR10, and a plant species dataset. It outperforms the metric learning methods in both disjoint and joint classification of novel classes.

- Showing the NSL method works well even with few examples of a novel class, making it applicable when there is little data for new classes.

- Demonstrating the effectiveness on a real-world plant species dataset with class imbalance, where the network is pretrained on more common species and can classify rare species without retraining.

In summary, the key contribution is a simple but effective method to extend an existing neural network to classify new classes without costly retraining, by exploiting the cosine similarity induced by the normalized softmax loss. This improves on metric learning approaches and has useful applications for incremental learning.
