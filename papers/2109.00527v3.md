# [Boosting Search Engines with Interactive Agents](https://arxiv.org/abs/2109.00527v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be whether machines can learn to use a search engine interactively as a tool for finding information. 

The authors pursue this goal of developing "learning to search" agents that can generate effective sequences of queries to retrieve documents relevant to answering a question. Their main hypothesis appears to be that by empowering agents with simple but interpretable search operators and leveraging large pretrained language models, it is possible to train artificial agents that can learn good search strategies from scratch via reinforcement learning or by imitating synthesized expert demonstrations.

Specifically, some key research questions driving the work seem to be:

- Can synthetic search sessions be generated in a principled way to provide supervision for training search agents? The authors introduce a method based on Rocchio relevance feedback for creating such training data.

- Can a reinforcement learning agent like MuZero learn good search policies directly from sparse rewards, despite the extreme dimensionality of the state and action spaces? The grammar-guided MCTS approach aims to tackle these challenges. 

- How do different agent architectures - e.g. MuZero vs Transformer - compare in their ability to learn search strategies?

- Is it possible to match or exceed the performance of recent neural retrieval methods using only traditional term-matching and transparent query operators?

The overarching goal is to demonstrate the feasibility of learning interpretable search agents that can effectively refine queries to answer questions, while relying solely on a standard search engine backend. The results provide evidence that this is indeed possible.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Developing a self-supervised learning scheme to generate synthetic search sessions (Rocchio sessions) using relevance feedback and search operators. This allows training supervised search agents like the T5 agent.

2. Presenting a reinforcement learning search agent (MuZero) that performs planning via grammar-guided Monte Carlo tree search and learns a model of the search environment. 

3. Evaluating the search agents on an open-domain QA dataset and showing they can achieve performance comparable to recent neural retrieval methods like DPR, using only traditional retrieval functions like BM25 and interpretable query operators.

4. Demonstrating that an ensemble of the supervised and RL agents can exceed the performance of individual agents, indicating the potential of orchestrating diverse search policies.

5. Providing evidence that knowledge-infused reinforcement learning can be effective at complex NLU tasks like search, when endowed with the right inductive biases.

6. Suggesting the supervised and RL agent architectures complement each other in learning different search policies, and their combination could be a promising direction for future hybrid models.

7. Showing that iterative, contextual query refinement with simple operators can substantially improve over a one-shot search, bringing to light the potential for developing interactive search agents.

In summary, the key contributions are in developing novel ways of generating search session training data, designing structured action spaces and constrained RL agents, and providing evidence that search can be modeled as an interactive, contextual process amenable to machine learning.
