# [PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D   Supervision](https://arxiv.org/abs/2303.09554)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we develop a generative model for editable 3D shapes that does not require explicit 3D supervision and enables part-level control during generation?

The key points are:

- The paper proposes PartNeRF, a novel part-aware generative model for editable 3D shapes. 

- Unlike prior works, PartNeRF does not require explicit 3D supervision (e.g. 3D meshes) during training. It is trained from 2D images and masks.

- PartNeRF represents objects using locally defined NeRFs as parts. This allows part-level control during generation and editing by applying transformations to individual part NeRFs.

- A key contribution is introducing a hard assignment between rays and object parts. This ensures that changing one part does not affect others. 

- Experiments on ShapeNet categories demonstrate PartNeRF's ability to generate and edit shapes with higher fidelity than prior generative models using NeRFs or primitives.

So in summary, the main research question is how to develop a part-aware generative model for 3D shapes that enables intuitive editing without 3D supervision. PartNeRF aims to address this question.


## What is the main contribution of this paper?

 This paper introduces PartNeRF, a novel part-aware generative model for editable 3D shape synthesis that does not require explicit 3D supervision. The key points are:

- Represents objects using a set of locally defined Neural Radiance Fields (NeRFs) arranged to render the object from novel views. 

- Enforces a hard assignment between rays and parts, so altering one part does not affect others.

- Trained from images and masks only, no 3D data needed.

- Enables various editing operations like part transformations, mixing, adding/removing, and appearance editing.

- Evaluated on ShapeNet categories, generates editable shapes of higher fidelity than prior methods needing 3D supervision or only using NeRFs.

So in summary, the main contribution is a generative model that can synthesize high quality editable 3D shapes with part-level control, without needing any 3D supervision like meshes or point clouds during training. This opens up new applications in 3D content creation and editing.
