# [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of transparency around the development of large language models (LLMs) for code, which have shown promise for powering AI assistants for software developers. Most models are not publicly released and details are scarce on the training data and preprocessing methods used.

Proposed Solution: 
- The BigCode project aims to develop LLMs for code transparently through an open scientific collaboration. They focus on responsible development, open governance, and empowering communities by releasing models, training data, and preprocessing code.

Key Contributions:
- They release the first steps of their data preprocessing pipeline, including PII redaction of training data using regular expressions to detect emails, IP addresses and keys.

- They run architecture ablations for Multi-Query Attention (MQA) and Fill-In-The-Middle (FIM) training, finding only a small drop in downstream performance compared to baseline models.

- They investigate the impact of various data filtering methods. Filtering based on GitHub stars surprisingly hurts performance significantly, despite being used previously as a proxy for data quality.

- They train SantaCoder, a 1.1B parameter model for Python, JavaScript and Java code. It outperforms previous open multilingual LLMs on MultiPL-E benchmarks despite being smaller, showing the promise of transparency and community-driven development.

In summary, the BigCode project demonstrates initial progress towards responsible and transparent development of LLMs for code through an open scientific collaboration. Their findings highlight interesting data preprocessing choices and they release SantaCoder, an open multilingual LLM with state-of-the-art performance on MultiPL-E.
