# [Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning](https://arxiv.org/abs/2403.01209)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Multi-label image recognition aims to identify all objects present in an image. It is challenging when novel objects emerge during inference, as traditional methods fail on unseen categories. Recent methods adapt vision-language models (VLMs) like CLIP to novel categories using annotated images for prompt tuning, but may be limited by insufficient training data. 

Method:
This paper proposes a data-free framework that leverages knowledge from pre-trained language models (LLMs) like ChatGLM to prompt tune CLIP for multi-label image recognition without any training data. The key ideas are:

(1) Comprehensive knowledge about object attributes and relationships is acquired from ChatGLM by querying with well-designed category-agnostic, category-specific, and relationship questions. This provides text descriptions for prompt tuning.  

(2) A hierarchical prompt learning method incorporates inter-category relationships into prompts, with shared, partial-shared, and category-specific tokens. Prompts absorb both task-specific and object-specific knowledge.

(3) Text descriptions are used as images to tune prompts based on the aligned vision-language embedding space of CLIP. The tuned model recognizes unseen images using similarity between learned prompts and image features.


Contributions:
(1) A data-free framework using LLM knowledge to prompt tune VLM, opening an avenue for novel categories.

(2) Relationship-aware hierarchical prompt learning to further improve multi-label recognition.  

(3) Comprehensive knowledge acquisition from LLM via diverse question types about attributes and relationships.

(4) A way to explore synergies between multiple pre-trained models under data scarcity.
