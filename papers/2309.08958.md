# [Monolingual or Multilingual Instruction Tuning: Which Makes a Better   Alpaca](https://arxiv.org/abs/2309.08958)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: What are cost-efficient approaches and best practices for adapting large language models (LLMs) to a multilingual chat environment through monolingual and multilingual instruction tuning? Specifically, the authors aim to empirically analyze and compare the efficacy of different methods for instruction tuning LLMs on monolingual and multilingual data in order to expand their capabilities for open-ended question answering across languages. The key hypotheses tested are:1) Multilingual instruction tuning may not be crucial for an LLM's performance in a single language like English, but can improve robustness in responding to multiple languages.2) With a fixed computational budget, training an LLM on downsampled multilingual data can perform on par with separate monolingual tuning per language. 3) The optimal approach differs for full-parameter fine-tuning vs low-rank adaptation, with the former favoring monolingual and the latter favoring multilingual tuning.4) Tuning on a mix of languages seen during training may overfit and hurt performance on unseen languages, so budgets may be better spent on less data.Through controlled experiments tuning LLMs of varying sizes and languages, the paper provides practical insights and guidance around trade-offs between language specialization and versatility when expanding LLMs to new languages under budget constraints.


## What is the main contribution of this paper?

The main contribution of this paper is an empirical analysis comparing monolingual and multilingual instruction tuning of large language models (LLMs) under constrained budgets. The key findings are:- Multilingual instruction tuning works well with low-rank adaptation (LoRA), while monolingual tuning is better for full-parameter fine-tuning. - When resources are limited, training on downsampled multilingual data gives more robust performance across languages than monolingual tuning. This suggests machine translation can create inexpensive multilingual datasets.- Models tuned only on English data lack robustness in responding consistently in the query language. Multilingual tuning enhances language consistency.- For unseen test languages, downsampled multilingual tuning generalizes better than using the full mixed dataset.Overall, the controlled experiments provide practical insights into expanding language capabilities of LLMs via monolingual or multilingual instruction tuning under budget constraints. The results serve as useful guidelines for model developers and users.
