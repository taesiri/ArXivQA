# [Monolingual or Multilingual Instruction Tuning: Which Makes a Better   Alpaca](https://arxiv.org/abs/2309.08958)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: What are cost-efficient approaches and best practices for adapting large language models (LLMs) to a multilingual chat environment through monolingual and multilingual instruction tuning? Specifically, the authors aim to empirically analyze and compare the efficacy of different methods for instruction tuning LLMs on monolingual and multilingual data in order to expand their capabilities for open-ended question answering across languages. The key hypotheses tested are:1) Multilingual instruction tuning may not be crucial for an LLM's performance in a single language like English, but can improve robustness in responding to multiple languages.2) With a fixed computational budget, training an LLM on downsampled multilingual data can perform on par with separate monolingual tuning per language. 3) The optimal approach differs for full-parameter fine-tuning vs low-rank adaptation, with the former favoring monolingual and the latter favoring multilingual tuning.4) Tuning on a mix of languages seen during training may overfit and hurt performance on unseen languages, so budgets may be better spent on less data.Through controlled experiments tuning LLMs of varying sizes and languages, the paper provides practical insights and guidance around trade-offs between language specialization and versatility when expanding LLMs to new languages under budget constraints.
