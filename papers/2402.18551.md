# [Implicit Bias of Next-Token Prediction](https://arxiv.org/abs/2402.18551)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the optimization and implicit bias of next-token prediction (NTP) models, which are widely used in language modeling. Specifically, it examines the following question: when training NTP models by minimizing the cross-entropy (CE) loss using gradient descent, what is the implicit bias towards certain solutions when there are multiple global minima? This question is studied for linear NTP models trained on datasets where each context is associated with a sparse conditional distribution over next tokens.

Approach: 
The paper models the NTP training data as consisting of $m$ distinct contexts, each associated with an empirical conditional probability vector $\pbh_j$ over the $V$ tokens in the vocabulary. Two key properties of separability and compatibility are introduced. NTP-separability requires the existence of weights that map distinct contexts to be linearly separable based on their supports. NTP-compatibility requires the logits' difference between in-support tokens to equal the log-ratio of probabilities. 

It is shown that when both properties hold, the CE loss approaches its minimum (the empirical entropy) along certain directions. This raises the question of which minimizing direction gradient methods converge to.

Main Contributions:
1) Establishes that overparameterization implies both NTP-separability and compatibility, allowing minimum CE loss.

2) Defines an NTP-SVM problem that maximizes the margin between support and non-support tokens while constraining the solution to lie in a data-defined subspace. Shows the ridge-regularized CE minimizer approaches the NTP-SVM solution.

3) Proves that gradient descent (GD) iterates grow unbounded and converge in direction to a solution that lies in the NTP-SVM direction on the subspace orthogonal to the data-defined subspace. On the data subspace, GD converges to the finite unique solution of an equivalence class of linear systems.

The results provide insights into the optimization geometry and implicit bias of NTP training and open up avenues for better understanding generalization in language models.
