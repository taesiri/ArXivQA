# [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning":

Problem:
- Continual learning requires models to learn multiple tasks sequentially. The key challenges are 1) stability - maintain performance on old tasks, and 2) plasticity - adapt to new tasks. 
- Recent methods use parameter-efficient fine-tuning (PEFT) by freezing a pre-trained model and injecting small trainable parameters. However, they don't eliminate the interference of new tasks on old tasks, hindering stability-plasticity trade-off.

Method: 
- Proposes InfLoRA, a new PEFT method for continual learning. 
- InfLoRA injects a small number of parameters to reparameterize pre-trained weights. Shows fine-tuning these parameters is equivalent to fine-tuning pre-trained weights in a subspace.
- Designs the subspace to eliminate interference of new tasks on old tasks and make a good stability-plasticity trade-off. The subspace is constrained to be orthogonal to gradient space of old tasks, and lie in gradient space of new task.
- Maintains orthonormal bases of gradient spaces using dual gradient projection memory (DualGPM). Expands bases for old tasks, reduces bases for new tasks.
- Only fine-tunes parameters for new task branch while keeping old branches fixed. Integrates old branches into pre-trained weights to limit total parameters.

Contributions:
- Establishes relationship between fine-tuning injected parameters and fine-tuning in a subspace.
- Designs the subspace to eliminate interference and balance stability-plasticity.
- Outperforms state-of-the-art methods on multiple datasets while adding comparable parameters.

In summary, InfLoRA makes key innovations in continually expanding the model capacity in a stable plastic way by reparameterization and subspace design, leading to strong empirical performance.
