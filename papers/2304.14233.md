# [Large Language Models are Strong Zero-Shot Retriever](https://arxiv.org/abs/2304.14233)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is: How can large language models (LLMs) be effectively leveraged for large-scale information retrieval in a zero-shot setting, without requiring any labeled training data?More specifically, the authors are investigating whether LLMs can serve as strong zero-shot retrievers on their own, without needing to be combined with other specialized retrieval models that require training. Their proposed method, LameR, uses an LLM to augment queries with potential answers, which are then fed into a simple non-parametric retriever like BM25. The central hypothesis appears to be that by prompting the LLM with retrieved candidate passages for a given query, the LLM can generate higher quality query augmentations that lead to improved retrieval performance compared to prior zero-shot LLM retrieval techniques. The authors posit that exposing the LLM to in-domain candidates helps steer its answer generation, even when the candidates are not correct.In summary, the main research question is whether LLMs can effectively act as standalone zero-shot retrievers, without needing to be combined with other trained retrieval models. The hypothesis is that prompting the LLM with retrieved candidates can boost its query augmentation ability in this zero-shot setting. The authors aim to demonstrate the viability of this approach across several benchmark retrieval datasets.


## What is the main contribution of this paper?

Here is a summary of the key contributions of the paper:- The paper proposes a new method called LameR (Large language model as Retriever) for large-scale document retrieval in zero-shot settings. The key idea is to leverage a large language model (LLM) to generate query augmentations, without needing any labeled training data.- LameR uses the LLM in a novel prompt format - it provides the LLM with top retrieved candidates for the query as context when generating the query augmentation. This helps the LLM generate more precise and in-domain augmentations compared to just prompting the LLM with the query. - Instead of using a weak self-supervised neural retriever, LameR opts for a simple non-parametric term-based retriever like BM25. This allows the literal query augmentations to directly interact with the inverted index, avoiding the performance bottleneck caused by the neural retriever.- Experiments on multiple benchmark datasets show LameR outperforms previous state-of-the-art zero-shot retrievers by a large margin. It also exceeds performance of systems using full supervision and even some with few-shot examples.- Overall, the key contribution is a simple yet effective LLM-based zero-shot retrieval system that circumvents issues like weak neural retrievers and out-of-domain augmentations faced by prior work. The power of large LM prompts combined with BM25 retrieval leads to new state-of-the-art performance.In summary, the main contribution is the novel LameR framework for zero-shot large-scale retrieval using an LLM for candidate-prompted query augmentation and BM25 for fast and effective retrieval.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new approach for zero-shot retrieval that uses a large language model to augment queries with generated answers, and achieves strong performance without relying on an additional neural retriever.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the same field:- The paper proposes a novel method for leveraging large language models for zero-shot information retrieval. This is an active area of research, with other recent works like HyDE and Contriever exploring similar ideas. However, this paper's approach of incorporating retrieved in-domain candidates into the prompt is unique. - Most prior work has combined large language models with learned dense retrievers. A key difference in this paper is the use of a non-parametric lexicon-based retriever like BM25 instead, which allows the model to avoid bottlenecks imposed by the learned retriever.- The paper demonstrates state-of-the-art performance on several benchmark datasets compared to other zero-shot methods. The gains are especially notable on web search datasets like DL19/DL20. This shows the proposed techniques are particularly promising for query-document retrieval.- An interesting finding is that stronger language models like GPT-4 can further boost performance when incorporated into the framework. This suggests there is headroom for larger language models to continue improving on these zero-shot retrieval tasks.- The paper provides useful ablation studies and analysis around the number of retrieved candidates and answers generated. These experiments help validate design decisions like using 10 candidates and 5 answers.- There is limited comparison to supervised approaches. While results surpass some supervised baselines, examining how the approach fares against SOTA supervised retrievers would add helpful context.Overall, the paper makes excellent progress on zero-shot retrieval by creatively combining language models with traditional non-parametric methods. The performance is very competitive, demonstrating language models can effectively serve as zero-shot retrievers given the right framework. More analysis on efficiency vs supervised methods could further highlight benefits. But the core ideas appear novel and well-motivated compared to related works.
