# [Benchmarking Chinese Commonsense Reasoning of LLMs: From   Chinese-Specifics to Reasoning-Memorization Correlations](https://arxiv.org/abs/2403.14112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of comprehensive benchmarks to evaluate large language models (LLMs) on Chinese commonsense reasoning. Existing benchmarks either simply translate English datasets which overlook unique aspects of Chinese culture/language, or only cover certain narrow domains of commonsense. 

- It is important to study the correlation between memorization and reasoning abilities in LLMs to precisely identify strengths/weaknesses and optimization directions. However, existing benchmarks use separate datasets for assessing memorization and reasoning, lacking intrinsic connections between the tasks.

Proposed Solution:
- Introduce CHARM, the first benchmark for thoroughly evaluating LLMs' Chinese commonsense reasoning ability. It covers both globally common and Chinese-specific commonsense across 7 aspects of Chinese culture and life.

- Construct closely interconnected reasoning and memorization tasks in CHARM's Chinese domain by creating related memorization questions for each reasoning question. This allows assessing memorization-dependent and independent reasoning abilities.

Key Contributions:
- Comprehensively assess abilities of 19 English and Chinese LLMs with 5 prompt strategies over CHARM. Show language/domain impacts effectiveness of strategies.

- Reveal three types of LLMs: Type I has poor memorization and reasoning; Type II has higher memorization but lower reasoning compared to size; Type III ultra-high memorization and reasoning.

- Propose methods to evaluate memorization-independent reasoning. Analyze error types: logical mistakes, inaccurate knowledge application despite correct prior memorization. 

- CHARM's approach of studying intrinsic links between memorization and reasoning can serve as a reference for similar research in other fields.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper introduces CHARM, a new benchmark for comprehensively evaluating large language models' ability to perform commonsense reasoning in Chinese, encompassing both globally and Chinese culturally-specific knowledge, reveals correlations between memorization and reasoning through closely interconnected tasks, and identifies strengths and weaknesses of different models to provide guidance for improvement.


## What is the main contribution of this paper?

 Based on my reading of the paper, I would summarize the main contributions as:

1. The authors introduce CHARM, the first comprehensive benchmark for evaluating large language models' (LLMs) commonsense reasoning ability in Chinese. CHARM covers both globally accepted commonsense and Chinese-specific commonsense across 7 aspects.

2. The authors evaluate representative prompt strategies on CHARM with 19 LLMs. The results show prompt effectiveness depends on the LLM's language orientation and the task domain, enriching prior findings.  

3. The authors construct closely interconnected reasoning and memorization tasks in CHARM's Chinese domain. This allows evaluating the intrinsic correlation between memorization and reasoning abilities, precisely identifying model strengths/weaknesses, and providing guidance for enhancement.

In summary, the key contributions are proposing the CHARM benchmark itself, using it to provide novel insights into prompt strategy effectiveness for non-English LLMs, and leveraging its interconnected tasks to deeply analyze memorization-reasoning relationships. The benchmark construction methodology could serve as a reference for other fields as well.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, here are some of the key terms and concepts associated with this paper:

- CHARM - The name of the benchmark introduced in the paper for evaluating commonsense reasoning of large language models (LLMs) in Chinese. It covers global and Chinese-specific commonsense.

- Commonsense reasoning - The ability of LLMs to make inferences and judgments using basic real-world knowledge and logic. Evaluating this is a key focus. 

- Chinese-specific commonsense - Knowledge related to Chinese history, culture, geography, etc. that is assessed in CHARM. This includes 7 defined aspects.

- Prompt strategies - Methods like Chain-of-Thought that are used to improve LLM reasoning when answering questions. 5 prompts are tested.

- Memorization tasks - Specially constructed questions that evaluate an LLM's memorization of key commonsense facts needed for reasoning questions.

- Correlations of reasoning and memorization - A key analysis in the paper between an LLM's memorization and application of knowledge for reasoning. Three LLM types identified. 

- Memorization-independent reasoning - Analysis done by filtering reasoning questions based on memorization performance to isolate reasoning abilities.

So in summary - CHARM benchmark, Chinese commonsense, reasoning evaluation, prompt strategies, memorization tasks, and reasoning-memorization correlations.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I formulated about the methods proposed in this paper:

1. Why did the authors choose to build closely interconnected reasoning and memorization tasks for the CHARM benchmark, and how does this design allow for evaluating correlations between memorization and reasoning abilities? 

2. What were the main criteria the authors used when selecting and designing the reasoning tasks included in CHARM? How do these criteria ensure comprehensive coverage and assessment?

3. The authors evaluate both English and Chinese language models on CHARM. What differences did they observe in the models' performance across the two commonsense domains (global vs Chinese-specific)? What might account for these differences?  

4. What factors did the authors find influence which prompt strategy is most effective for a given language model and task? How might these findings guide prompt strategy selection in future research?

5. What three broad types of language models did the authors identify based on correlations between memorization and integrated reasoning performance? What are the key strengths and weaknesses of each type?

6. What four error types did the authors define during their analysis of memorization-independent reasoning mistakes? What do the distributions of these errors reveal about the models' reasoning processes?  

7. How might the design approach used in CHARM to link memorization and reasoning tasks serve as a useful reference for future benchmark development, both inside and outside the field of commonsense reasoning?

8. What limitations exist in the authors' prompt strategy analysis, and what factors might lead to shifts in optimal strategies over time?  

9. What steps did the authors take during CHARM's construction and annotation to ensure high quality? How might automation help scale up these processes in future work?

10. What ethical considerations guided the authors' use of human annotation during CHARM's development? How were these considerations accounted for?
