# [Simple Weak Coresets for Non-Decomposable Classification Measures](https://arxiv.org/abs/2312.09885)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Non-decomposable classification measures like F1 score and Matthews Correlation Coefficient (MCC) are useful for handling class imbalance and capturing precision-recall tradeoffs. However, optimizing them is challenging as they are non-differentiable and do not decompose over data points. 

- For decomposable losses, coresets (small weighted subsets) have helped scale optimization by allowing optimization over just the coreset. But coreset constructions rely on decomposability. No coreset methods exist for non-decomposable classification measures.

Proposed Solution:
- The paper studies coresets for F1 score and MCC. It first shows lower bounds - no strong coresets possible smaller than the whole dataset. 

- It then shows stratified uniform sampling gives a weak coreset for F1 score for queries with high F1 score. It preserves F1 score up to small multiplicative error for those queries.

- For MCC, it shows stratified uniform sampling gives a weak coreset with additive error guarantees for queries predicting sufficiently many positives and negatives.

Main Contributions:
- First coreset analysis for non-decomposable classification measures
- Lower bounds against strong coresets
- Weak coreset for F1 score using uniform sampling
- Weak coreset with additive error for MCC using uniform sampling
- Experiments on multiple datasets validating uniform sampling does as well or better than other sophisticated coreset techniques

The key insight is that despite lack of decomposability, uniform sampling works provably well and also empirically for these non-decomposable measures.


## Summarize the paper in one sentence.

 The paper shows that stratified uniform sampling gives weak coresets with strong empirical performance for non-decomposable classification measures like F1 score and Matthews Correlation Coefficient, comparable to more complex sampling strategies.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) It provides lower bounds against construction of strong coresets for both MCC and F1 score measures.

2) It shows that sampling uniformly from each class in a stratified manner gives a weak coreset for the F1 score and a weak coreset with a small additive error for MCC. Here weak signifies that the coreset works for a set of a large number of 'important' classifiers, including the optimal.

3) It provides empirical results for a number of different classifiers and real data sets comparing uniform sampling with other well-known sophisticated coreset construction strategies. The experiments show that uniform sampling performs comparably or better than more complex methods.

In summary, the paper initiates the study of coresets for non-decomposable classification measures like F1 score and MCC, shows theoretically that simple uniform sampling can work, and provides empirical evidence to support this claim.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and concepts associated with this paper include:

- Coresets - Summaries of a dataset that enable efficient optimization and provide theoretical guarantees on performance. A main focus of the paper.

- Non-decomposable measures - Evaluation measures like F1 score and Matthews Correlation Coefficient that cannot be written as a sum over individual data points. 

- Uniform sampling - A simple coreset construction approach based on stratified uniform sampling from the dataset. Shown to work well empirically.

- Weak coresets - Coresets that provide guarantees for a subset of "important" queries including the optimal query. Proven for F1 score and MCC.  

- Lower bounds - Negative results showing limitations on strong coreset construction for MCC and F1 score.

- Classification measures - The paper examines coresets in the context of supervised classification and common evaluation measures like precision, recall, F1 score.

- Contingency tables - Used to define measures like MCC. Counts like true positives, false positives etc. make up such tables.

So in summary, the key focus is on coresets for non-decomposable classification measures, using tools like uniform sampling and weak coresets. Theoretical bounds and empirical results are provided.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper shows empirical evidence that uniform sampling works well for creating coresets for non-decomposable classification measures like F1 score and MCC. However, the theoretical analysis makes certain assumptions about the queries - for example lower bounding the number of true positives. How sensitive is the performance of uniform sampling to violations in these assumptions?

2. For the MCC analysis, the paper shows a relative error multiplicative approximation in terms of $\epsilon/\gamma$. What is the intuition behind having the error depend on $1/\gamma$? How does this affect the approximation quality for queries with lower $\gamma$? 

3. The paper analyzes uniform sampling for F1 score and MCC specifically. What core challenges need to be addressed to generalize the analysis for other non-decomposable measures like G-mean, H-mean, Average Precision etc?

4. The lower bounds shown are for strong coresets. What techniques can be used or analyses extended to show inapproximability results or lower bounds for weak coresets?

5. The guarantees shown are for discrete, 0/1 labeled data. How can the analysis be extended for noisy, probabilistic or continuous labels?

6. For F1 score analysis, the paper assumes the query attains a certain minimum score $\gamma$. What happens if this assumption is violated? Can the approximation factor be analyzed without that assumption?

7. The relative and additive error guarantees depend on coreset size. For a fixed size, how does the approximation error vary with properties like data dimensionality, label skew, model complexity etc?

8. The experiments compare uniform sampling to other sampling strategies like leverage score and Lewis sampling. What is lacking for those methods to have good performance - can the theory be improved or are those fundamentally inferior?

9. The analysis binds sample complexity for uniform sampling. However, experiments are shown only for up to 10% samples. How does empirical performance degrade for smaller coreset sizes? 

10. Coreset construction is one way to scale supervised learning with non-decomposable measures. How does the proposed method compare or can it be integrated with alternate scaling approaches like optimization decomposition methods?
