# [DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence](https://arxiv.org/abs/2201.11176)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper aims to address is: 

How can we develop improved evaluation metrics that better assess the discourse coherence of automatically generated text, especially for abstractive summarization and document-level machine translation?

The key hypothesis is that existing metrics like BERTScore and BARTScore, while strong at sentence-level evaluations, are limited in their ability to evaluate discourse coherence across multiple sentences. The authors propose that explicitly modeling coherence phenomena like entity transitions and focus shifts between sentences can lead to metrics that better correlate with human judgments of coherence for long text generation tasks. 

To test this, they introduce DiscoScore, a metric that uses BERT representations along with graph-based modeling of entity/focus transitions. The main hypothesis is that combining BERT with explicit discourse modeling will outperform metrics based on BERT or discourse features alone. Their experiments on summarization and MT datasets aim to demonstrate the superior correlation of DiscoScore variants with human coherence ratings compared to other state-of-the-art metrics.

In summary, the paper introduces DiscoScore as a new way to evaluate discourse coherence in text generation by combining BERT representations with graph-based coherence models, hypothesizing this will lead to better correlation with human judgments compared to existing metrics. The experiments are designed to test this hypothesis.


## What is the main contribution of this paper?

 This paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated text in summarization and machine translation. The main contributions are:

- DiscoScore uses BERT to model discourse coherence from two perspectives: (1) focus frequency/semantics and (2) focus transitions between sentences. This allows it to capture coherence properties missed by other metrics.

- Experiments show state-of-the-art metrics like BARTScore perform poorly at the system level for judging coherence and other aspects like factual consistency. In contrast, DiscoScore achieves much stronger correlation with human ratings.

- The paper demonstrates the importance of discourse signals for judging text quality. Simple features derived from DiscoScore can strongly distinguish between system hypotheses and references.

- Analysis provides insights into why certain DiscoScore variants outperform others based on how discriminative their discourse features are. This allows interpreting performance gaps.

- DiscoScore combines the benefits of leveraging contextualized encoders like BERT along with explicit discourse modeling, outperforming metrics that rely on just one of those.

In summary, the main contribution is a new metric DiscoScore that successfully judges text coherence by combining BERT representations with discourse features based on centering theory. Experiments show it outperforms existing metrics, especially at the system level evaluation that is most important when comparing models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces DiscoScore, a new evaluation metric that combines BERT representations and discourse coherence modeling to better assess text quality in summarization and machine translation tasks.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of discourse evaluation metrics:

- The paper introduces a new metric called DiscoScore that leverages BERT representations and models discourse coherence. This is a novel approach compared to prior work on discourse metrics, which have typically relied on more surface-level features and have not leveraged powerful pretrained language models like BERT. 

- Most prior work on discourse metrics has focused on machine translation evaluation. This paper evaluates DiscoScore more broadly on summarization and machine translation tasks. The strong performance across tasks suggests wider applicability of the approach.

- The paper provides more in-depth analyses and ablation studies than typically seen when new metrics are proposed. This sheds light on what factors contribute to DiscoScore's performance and provides justification for the design choices.

- The paper demonstrates that recent popular BERT-based metrics still perform poorly at capturing discourse phenomena like coherence. This is an important finding given the popularity of using BERT for evaluation metrics. It suggests that coherence requires specialized modeling beyond BERT representations.

- The paper shows combining BERT representations and traditional discourse features works better than either alone. This points the way toward synergistically combining strengths of neural models and linguistic feature engineering.

Overall, the paper makes several novel contributions to the challenge of modeling discourse coherence for evaluation. The proposed metric outperforms prior approaches. The analyses provide insights into model design and show weaknesses of existing methods. This looks to advance research on discourse-aware evaluation metrics.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing reference-free discourse metrics that compare the source text to the generated hypothesis text instead of requiring reference texts.

- Improving discourse metrics for more fine-grained evaluation at the document-level and summary-level, rather than just system-level. The paper notes DiscoScore showed advantages at the system-level but not for finer-grained assessment.

- Ranking NLG systems with discourse metrics using rigorous statistical approaches. The authors suggest approaches like better system-level correlation and system ranking. 

- Exploring different choices for modeling "entities" as foci, beyond just clustering similar nouns. The authors mention using contextualized word embeddings and weighting entities by frequency as possibilities.

- Incorporating other discourse phenomena like discourse connectives and coreference into coherence assessment, not just focus transitions.

- Adapting discourse metrics like DiscoScore to other languages besides English by finding appropriate references and foci.

- Developing further justification and explanations for why some discourse metrics work better than others through features and analyses.

In summary, the key suggestions are improving discourse metrics to handle finer-grained assessment, combining them with rigorous statistical ranking approaches, expanding beyond just focus to other discourse phenomena, and adapting them to other languages. Explainability through features and analyses is also highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated texts like machine translations and text summaries. DiscoScore is based on modeling discourse coherence through the lens of readers' focus, as inspired by Centering theory. It uses BERT representations to model two aspects of focus: (1) focus frequency and semantics, by comparing the difference in these between a hypothesis text and reference text, and (2) focus transitions, by modeling inter-sentence dependencies based on shared foci. Experiments on summarization and machine translation datasets show that common BERT-based metrics perform poorly at judging coherence, while DiscoScore strongly correlates with human ratings of coherence and other quality aspects, outperforming metrics like BARTScore. The paper demonstrates the importance of including discourse signals in evaluation, and shows how simple features can explain differences between metrics. Overall, it fills the gap of missing discourse-aware metrics and combines strengths of discourse features and contextualized representations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated text in summarization and machine translation. DiscoScore uses BERT to model discourse coherence in two ways: 1) by comparing the frequency and semantics of "foci" (referring expressions and related words) between a hypothesis and reference text, and 2) by modeling inter-sentence dependencies based on shared foci between sentences. Experiments compare DiscoScore to 16 other metrics on summarization and MT datasets. The results show that most recent BERT-based metrics actually correlate poorly with human ratings of coherence, even worse than older feature-based metrics like RC and LC. However, DiscoScore achieves strong system-level correlation with human ratings of coherence and other quality aspects, outperforming the current state-of-the-art BARTScore metric by over 10 points on average. Analyses demonstrate the importance of modeling discourse for evaluation, and show DiscoScore's superiority can be explained by how well its discourse features separate hypotheses from references. 

Overall, the paper fills an important gap by developing DiscoScore as a discourse-aware evaluation metric. Neither strong encoders like BERT nor discourse features alone are enough; combining them as DiscoScore does is key. The paper highlights issues with current metrics' assessment of coherence, provides understanding of DiscoScore's working, and points to many directions for future work on improved discourse metrics for generated text evaluation.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces DiscoScore, a new evaluation metric that models discourse coherence to judge the quality of text generation systems. DiscoScore uses BERT to represent the semantics and frequencies of "foci" (referring expressions and semantically related elements) in a hypothesis text and reference text. It compares the hypothesis and reference in two main ways: 1) FocusDiff measures the difference between foci that appear in both hypothesis and reference, comparing their semantics and frequencies. 2) SentGraph uses the transitions between foci across sentences to create graph representations of the hypothesis and reference. It then measures the similarity of these graph representations. Overall, DiscoScore aims to leverage both strong contextualized encoders like BERT and discourse features like focus transitions to create a metric that can effectively evaluate coherence and other qualities of machine generated text.


## What problem or question is the paper addressing?

 Based on my reading, the key points about the problem and questions addressed in this paper are:

- There is a mismatch between recently proposed discourse-based natural language generation (NLG) systems (e.g. summarization and machine translation systems that model inter-sentence dependencies) and non-discourse evaluation metrics like BERTScore and MoverScore. These metrics cannot properly evaluate the discourse-level improvements of these new systems. 

- The paper aims to fill this gap by introducing a new parametrized discourse metric called DiscoScore. DiscoScore explicitly models discourse coherence using BERT through two perspectives related to readers' focus: (1) focus frequency/semantics and (2) focus transitions between sentences.

- The paper compares DiscoScore to a range of discourse and non-discourse metrics on summarization and machine translation datasets. The key questions examined are:

   - How do existing metrics, especially BERT-based ones, perform in terms of correlating with human judgements of coherence/consistency?

   - Can modeling discourse phenomena like focus improve correlation over non-discourse metrics?

   - How does DiscoScore compare to state-of-the-art metrics like BARTScore?

   - Can simple discourse features explain differences between variants of DiscoScore?

In summary, the paper addresses the mismatch between new discourse-aware NLG systems and evaluation metrics by proposing a new discourse-based metric DiscoScore. Experiments compare DiscoScore to existing metrics and aim to demonstrate the importance of modeling discourse for better correlation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper introduction, some of the key terms and concepts include:

- Discourse coherence - Continuity of meaning/semantics in text

- Discourse relations - Connections between text spans to ensure coherence (e.g. repetition, coreference) 

- Centering theory - Model of discourse coherence based on reader's focus of attention

- Coherence models - Computational approaches to judge text coherence (e.g. for tasks like essay scoring)

- Natural language generation (NLG) - Automatic text generation systems like summarization and machine translation

- Non-discourse NLG evaluation metrics - Metrics like BERTScore and MoverScore that don't specifically target coherence

- BARTScore - Recent state-of-the-art metric based on sequence-to-sequence models

- DiscoScore - Novel parametrized discourse metric introduced in this paper 

- Focus of attention - Key concept in Centering theory, referring expressions and semantically related elements

- Focus difference - DiscoScore variant comparing focus frequency and semantics in hypothesis vs reference

- Sentence graph - DiscoScore variant using focus transitions and inter-sentence relations

The core focus seems to be on introducing DiscoScore as a new discourse-aware evaluation metric for natural language generation systems, in order to properly assess discourse coherence which recent BERT-based metrics fail to capture well.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key points of this paper:

1. What is the focus/main contribution of this paper? 

2. What gap is this paper trying to fill in natural language generation (NLG) evaluation?

3. What are the two main variants of DiscoScore proposed in this paper? How do they model discourse coherence differently?

4. What datasets were used to evaluate DiscoScore and other metrics? What tasks do they cover?

5. How does DiscoScore compare to non-discourse metrics like BERTScore and BARTScore? What are the key findings?

6. How does DiscoScore compare to existing discourse metrics? What are the limitations of previous discourse metrics? 

7. How is the concept of "focus" used in DiscoScore? What are the different focus choices explored?

8. What simple discourse features are derived from DiscoScore? How do they help explain the performance of different metric variants?

9. What are the advantages and limitations discussed for DiscoScore? What future work is suggested?

10. What is the overall impact and significance of this work on evaluating discourse coherence in text generation systems?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The authors propose modeling discourse coherence through the lens of readers' focus, as inspired by Centering Theory. How does modeling focus frequency and semantics allow capturing coherence signals that lexical overlap-based metrics like BLEU miss?

2. FocusDiff and SentGraph are proposed as two variants of DiscoScore. What are the key differences between these two variants in how they model coherence? What are the relative advantages and disadvantages?

3. The authors investigate two choices for defining focus - noun phrases and semantic entities. Under what circumstances does using semantic entities as focus yield substantial gains over just using nouns? When does it not help much or hurt performance? 

4. The paper argues that neither leveraging BERT representations nor using discourse features alone is sufficient for judging coherence well, but combining them is most effective. What evidence supports this claim? Why is the combination greater than the sum of its parts?

5. How does the graph-based approach used in SentGraph capture inter-sentence dependencies and coherence relationships? What are the limitations of this simple graph formulation?

6. DiscoScore does not seem to provide gains over other metrics at the fine-grained document or summary level correlations. What factors might contribute to its weaker fine-grained performance? How could the method be improved to better handle fine-grained assessment?

7. The authors use the discriminativeness of the proposed coherence features to explain differences between DiscoScore variants. Is discriminativeness the only factor that determines metric quality here? What other factors could play a role?

8. Howuitable is DiscoScore for languages other than English? What challenges might arise in porting it to other languages? Would the overall approach still be viable?

9. The paper focuses exclusively on reference-based evaluation scenarios. How could the DiscoScore approach be adapted for reference-free evaluation comparing source text to hypothesis? 

10. The authors measure coherence phenomena related to focus transitions and frequency. What other discourse-level signals could complement focus modeling and provide further gains in coherence assessment?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summary of the key points from the paper:

The paper introduces DiscoScore, a new evaluation metric for text generation systems that models discourse coherence from the perspective of readers' focus, as formulated in Centering theory. DiscoScore uses BERT representations to capture semantics and coherence of focus entities in a text. It is parameterized to model focus in two ways: FocusDiff compares focus frequency and semantics between hypothesis and reference, while SentGraph models inter-sentence coherence via focus transitions. 

Experiments compare DiscoScore to 16 other metrics on summarization and machine translation datasets. Key findings are: (1) Most BERT-based metrics correlate poorly with human ratings of coherence, even worse than older discourse metrics. (2) The state-of-the-art BARTScore also correlates weakly at the system level. (3) DiscoScore consistently achieves the highest system-level correlation with human ratings, outperforming BARTScore by over 10 points on average. (4) Discourse features derived from DiscoScore strongly separate hypotheses from references, demonstrating the importance of discourse signals. (5) The superiority of DiscoScore variants can be explained by the discriminativeness of associated discourse features.

Overall, the paper shows the limitations of current metrics in evaluating coherence and presents DiscoScore as an effective way to incorporate discourse modeling using BERT. The derived features provide insights into the role of discourse phenomena in text quality assessment.


## Summarize the paper in one sentence.

 The paper introduces DiscoScore, a new evaluation metric for text generation that combines BERT representations and discourse coherence modeling to better assess text quality.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper introduces DiscoScore, a new evaluation metric for assessing the quality of generated text like summaries and machine translations. DiscoScore uses BERT to model discourse coherence in the generated text from two perspectives driven by Centering theory: 1) comparing the frequency and semantics of entities/nouns (foci) between the generated text and reference, and 2) using focus transitions to model inter-sentence dependencies through graph representations. Experiments on summarization and machine translation datasets show that most existing BERT-based metrics correlate weakly with human judgements of coherence. In contrast, DiscoScore strongly correlates with human ratings of coherence and other aspects like consistency, outperforming metrics like BARTScore. The paper demonstrates the importance of including discourse signals in evaluation, and shows DiscoScore's features can distinguish between hypothesis and reference. Overall, it introduces a discourse-aware metric that combines BERT representations and coherence modeling to effectively evaluate modern text generation systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper introduces two main variants of DiscoScore: FocusDiff and SentGraph. Can you explain in more detail the key differences between these two variants and how they model discourse coherence?

2. The paper evaluates DiscoScore on both summarization and machine translation tasks. What are the key challenges in evaluating discourse metrics on these two different tasks? How does the performance compare across the tasks?

3. The paper compares using noun phrases versus semantic entities as the focus choice for modeling coherence. Under what circumstances does using semantic entities seem to help versus just using noun phrases? What are some ways to further improve entity detection? 

4. The paper shows discourse features like focus frequency and sentence connectivity are highly discriminative between hypotheses and references. What does this suggest about the importance of modeling discourse for text generation tasks? How could these insights be used?

5. The paper utilizes adjacency matrices to model inter-sentence dependencies in SentGraph. What are the benefits of using graph-based methods here versus other approaches? Are there any limitations?

6. How does the performance of DiscoScore compare to using discourse-oriented BERT models like Conpono? When does a discourse BERT help versus a standard BERT?

7. The paper analyzes different mechanisms for SentGraph like greedy/optimal alignment versus statistics over the graph. Why does the statistics-based approach work better? What are its limitations?

8. How does the performance of DiscoScore compare to simplicity metrics like RC and LC? What are the tradeoffs between complex neural versus simple feature-based metrics?

9. The paper shows BARTScore performs much worse at the system-level than summary-level. Why might this be the case? How can system-level evaluation be improved?

10. What kinds of text generation systems do you think would benefit most from evaluation using DiscoScore? What future work could be done to improve discourse metrics?
