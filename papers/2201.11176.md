# [DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence](https://arxiv.org/abs/2201.11176)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper aims to address is: 

How can we develop improved evaluation metrics that better assess the discourse coherence of automatically generated text, especially for abstractive summarization and document-level machine translation?

The key hypothesis is that existing metrics like BERTScore and BARTScore, while strong at sentence-level evaluations, are limited in their ability to evaluate discourse coherence across multiple sentences. The authors propose that explicitly modeling coherence phenomena like entity transitions and focus shifts between sentences can lead to metrics that better correlate with human judgments of coherence for long text generation tasks. 

To test this, they introduce DiscoScore, a metric that uses BERT representations along with graph-based modeling of entity/focus transitions. The main hypothesis is that combining BERT with explicit discourse modeling will outperform metrics based on BERT or discourse features alone. Their experiments on summarization and MT datasets aim to demonstrate the superior correlation of DiscoScore variants with human coherence ratings compared to other state-of-the-art metrics.

In summary, the paper introduces DiscoScore as a new way to evaluate discourse coherence in text generation by combining BERT representations with graph-based coherence models, hypothesizing this will lead to better correlation with human judgments compared to existing metrics. The experiments are designed to test this hypothesis.


## What is the main contribution of this paper?

 This paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated text in summarization and machine translation. The main contributions are:

- DiscoScore uses BERT to model discourse coherence from two perspectives: (1) focus frequency/semantics and (2) focus transitions between sentences. This allows it to capture coherence properties missed by other metrics.

- Experiments show state-of-the-art metrics like BARTScore perform poorly at the system level for judging coherence and other aspects like factual consistency. In contrast, DiscoScore achieves much stronger correlation with human ratings.

- The paper demonstrates the importance of discourse signals for judging text quality. Simple features derived from DiscoScore can strongly distinguish between system hypotheses and references.

- Analysis provides insights into why certain DiscoScore variants outperform others based on how discriminative their discourse features are. This allows interpreting performance gaps.

- DiscoScore combines the benefits of leveraging contextualized encoders like BERT along with explicit discourse modeling, outperforming metrics that rely on just one of those.

In summary, the main contribution is a new metric DiscoScore that successfully judges text coherence by combining BERT representations with discourse features based on centering theory. Experiments show it outperforms existing metrics, especially at the system level evaluation that is most important when comparing models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces DiscoScore, a new evaluation metric that combines BERT representations and discourse coherence modeling to better assess text quality in summarization and machine translation tasks.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of discourse evaluation metrics:

- The paper introduces a new metric called DiscoScore that leverages BERT representations and models discourse coherence. This is a novel approach compared to prior work on discourse metrics, which have typically relied on more surface-level features and have not leveraged powerful pretrained language models like BERT. 

- Most prior work on discourse metrics has focused on machine translation evaluation. This paper evaluates DiscoScore more broadly on summarization and machine translation tasks. The strong performance across tasks suggests wider applicability of the approach.

- The paper provides more in-depth analyses and ablation studies than typically seen when new metrics are proposed. This sheds light on what factors contribute to DiscoScore's performance and provides justification for the design choices.

- The paper demonstrates that recent popular BERT-based metrics still perform poorly at capturing discourse phenomena like coherence. This is an important finding given the popularity of using BERT for evaluation metrics. It suggests that coherence requires specialized modeling beyond BERT representations.

- The paper shows combining BERT representations and traditional discourse features works better than either alone. This points the way toward synergistically combining strengths of neural models and linguistic feature engineering.

Overall, the paper makes several novel contributions to the challenge of modeling discourse coherence for evaluation. The proposed metric outperforms prior approaches. The analyses provide insights into model design and show weaknesses of existing methods. This looks to advance research on discourse-aware evaluation metrics.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing reference-free discourse metrics that compare the source text to the generated hypothesis text instead of requiring reference texts.

- Improving discourse metrics for more fine-grained evaluation at the document-level and summary-level, rather than just system-level. The paper notes DiscoScore showed advantages at the system-level but not for finer-grained assessment.

- Ranking NLG systems with discourse metrics using rigorous statistical approaches. The authors suggest approaches like better system-level correlation and system ranking. 

- Exploring different choices for modeling "entities" as foci, beyond just clustering similar nouns. The authors mention using contextualized word embeddings and weighting entities by frequency as possibilities.

- Incorporating other discourse phenomena like discourse connectives and coreference into coherence assessment, not just focus transitions.

- Adapting discourse metrics like DiscoScore to other languages besides English by finding appropriate references and foci.

- Developing further justification and explanations for why some discourse metrics work better than others through features and analyses.

In summary, the key suggestions are improving discourse metrics to handle finer-grained assessment, combining them with rigorous statistical ranking approaches, expanding beyond just focus to other discourse phenomena, and adapting them to other languages. Explainability through features and analyses is also highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated texts like machine translations and text summaries. DiscoScore is based on modeling discourse coherence through the lens of readers' focus, as inspired by Centering theory. It uses BERT representations to model two aspects of focus: (1) focus frequency and semantics, by comparing the difference in these between a hypothesis text and reference text, and (2) focus transitions, by modeling inter-sentence dependencies based on shared foci. Experiments on summarization and machine translation datasets show that common BERT-based metrics perform poorly at judging coherence, while DiscoScore strongly correlates with human ratings of coherence and other quality aspects, outperforming metrics like BARTScore. The paper demonstrates the importance of including discourse signals in evaluation, and shows how simple features can explain differences between metrics. Overall, it fills the gap of missing discourse-aware metrics and combines strengths of discourse features and contextualized representations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces DiscoScore, a new evaluation metric for assessing the coherence of generated text in summarization and machine translation. DiscoScore uses BERT to model discourse coherence in two ways: 1) by comparing the frequency and semantics of "foci" (referring expressions and related words) between a hypothesis and reference text, and 2) by modeling inter-sentence dependencies based on shared foci between sentences. Experiments compare DiscoScore to 16 other metrics on summarization and MT datasets. The results show that most recent BERT-based metrics actually correlate poorly with human ratings of coherence, even worse than older feature-based metrics like RC and LC. However, DiscoScore achieves strong system-level correlation with human ratings of coherence and other quality aspects, outperforming the current state-of-the-art BARTScore metric by over 10 points on average. Analyses demonstrate the importance of modeling discourse for evaluation, and show DiscoScore's superiority can be explained by how well its discourse features separate hypotheses from references. 

Overall, the paper fills an important gap by developing DiscoScore as a discourse-aware evaluation metric. Neither strong encoders like BERT nor discourse features alone are enough; combining them as DiscoScore does is key. The paper highlights issues with current metrics' assessment of coherence, provides understanding of DiscoScore's working, and points to many directions for future work on improved discourse metrics for generated text evaluation.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces DiscoScore, a new evaluation metric that models discourse coherence to judge the quality of text generation systems. DiscoScore uses BERT to represent the semantics and frequencies of "foci" (referring expressions and semantically related elements) in a hypothesis text and reference text. It compares the hypothesis and reference in two main ways: 1) FocusDiff measures the difference between foci that appear in both hypothesis and reference, comparing their semantics and frequencies. 2) SentGraph uses the transitions between foci across sentences to create graph representations of the hypothesis and reference. It then measures the similarity of these graph representations. Overall, DiscoScore aims to leverage both strong contextualized encoders like BERT and discourse features like focus transitions to create a metric that can effectively evaluate coherence and other qualities of machine generated text.


## What problem or question is the paper addressing?

 Based on my reading, the key points about the problem and questions addressed in this paper are:

- There is a mismatch between recently proposed discourse-based natural language generation (NLG) systems (e.g. summarization and machine translation systems that model inter-sentence dependencies) and non-discourse evaluation metrics like BERTScore and MoverScore. These metrics cannot properly evaluate the discourse-level improvements of these new systems. 

- The paper aims to fill this gap by introducing a new parametrized discourse metric called DiscoScore. DiscoScore explicitly models discourse coherence using BERT through two perspectives related to readers' focus: (1) focus frequency/semantics and (2) focus transitions between sentences.

- The paper compares DiscoScore to a range of discourse and non-discourse metrics on summarization and machine translation datasets. The key questions examined are:

   - How do existing metrics, especially BERT-based ones, perform in terms of correlating with human judgements of coherence/consistency?

   - Can modeling discourse phenomena like focus improve correlation over non-discourse metrics?

   - How does DiscoScore compare to state-of-the-art metrics like BARTScore?

   - Can simple discourse features explain differences between variants of DiscoScore?

In summary, the paper addresses the mismatch between new discourse-aware NLG systems and evaluation metrics by proposing a new discourse-based metric DiscoScore. Experiments compare DiscoScore to existing metrics and aim to demonstrate the importance of modeling discourse for better correlation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper introduction, some of the key terms and concepts include:

- Discourse coherence - Continuity of meaning/semantics in text

- Discourse relations - Connections between text spans to ensure coherence (e.g. repetition, coreference) 

- Centering theory - Model of discourse coherence based on reader's focus of attention

- Coherence models - Computational approaches to judge text coherence (e.g. for tasks like essay scoring)

- Natural language generation (NLG) - Automatic text generation systems like summarization and machine translation

- Non-discourse NLG evaluation metrics - Metrics like BERTScore and MoverScore that don't specifically target coherence

- BARTScore - Recent state-of-the-art metric based on sequence-to-sequence models

- DiscoScore - Novel parametrized discourse metric introduced in this paper 

- Focus of attention - Key concept in Centering theory, referring expressions and semantically related elements

- Focus difference - DiscoScore variant comparing focus frequency and semantics in hypothesis vs reference

- Sentence graph - DiscoScore variant using focus transitions and inter-sentence relations

The core focus seems to be on introducing DiscoScore as a new discourse-aware evaluation metric for natural language generation systems, in order to properly assess discourse coherence which recent BERT-based metrics fail to capture well.
