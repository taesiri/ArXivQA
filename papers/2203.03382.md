# [Self-supervised Implicit Glyph Attention for Text Recognition](https://arxiv.org/abs/2203.03382)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the attention mechanism in scene text recognition (STR) methods without using character-level annotations. 

The key points are:

- Existing STR methods can be divided into implicit attention based and supervised attention based. Implicit attention can suffer from alignment issues while supervised attention requires laborious character-level annotations. 

- The authors propose a new attention mechanism called self-supervised implicit glyph attention (SIGA) that learns to focus on glyph structures without character annotations.

- SIGA jointly performs self-supervised text segmentation and implicit attention alignment to generate glyph pseudo-labels. These serve as supervision to learn glyph attention maps during training.

- Experiments show SIGA improves attention correctness and achieves state-of-the-art results on public STR benchmarks. It also generalizes much better on contextless benchmarks like industrial serial numbers.

In summary, the main hypothesis is that learning to focus on glyph structures in a self-supervised manner can improve attention and recognition in STR without extra annotations. The experiments seem to validate this hypothesis.
