# ["Sorry, Come Again?" Prompting -- Enhancing Comprehension and   Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing](https://arxiv.org/abs/2403.18976)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) frequently hallucinate or generate factually incorrect information when prompted, posing risks.
- LLMs often struggle to comprehend lengthy prompts, only focusing on beginning and end words ("lost in the middle").  
- Different LLM architectures comprehend the same prompt differently. Identical prompts can elicit varying responses across models.

Proposed Solution - "Sorry, Come Again" (SCA):  
- Enhances LLM prompt comprehension via optimal paraphrasing and pausing techniques to reduce hallucination.
- Investigates impact of linguistic features (formality, readability, concreteness) of prompts on hallucination. 
- Introduces method to identify most comprehensible paraphrase of prompt for a specific LLM using integrated gradients.
- Injects "[PAUSE]" tokens to get LLM to "breathe" while reading long prompts, determined by text concreteness.
- Proposes novel "Reverse Proxy Tuning" to efficiently fine-tune LM using a small LM as proxy.

Key Contributions:
- Analysis of prompt linguistic properties on hallucination across 21 LLMs.
- Optimal paraphrasing framework to boost comprehension.  
- "[PAUSE]" injection approach and reverse proxy tuning method.
- Overall framework "ACTIVATOR" to automatically rephrase prompts and assess factuality to mitigate hallucination.
- Empirical evidence that enhanced comprehension leads to reduced hallucination.

In summary, the paper introduces innovative techniques to improve LLM comprehension of prompts to address the significant challenge of hallucination in LLMs. A new prompting framework, fine-tuning method and an end-to-end pipeline are presented.
