# [Stop Regressing: Training Value Functions via Classification for   Scalable Deep RL](https://arxiv.org/abs/2403.03950)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Value functions are central in deep reinforcement learning (RL) methods. However, scaling such value-based methods with large neural networks, like Transformers, has been challenging. This is in contrast with supervised learning, where techniques like classification with cross-entropy loss have reliably scaled to massive models. The paper investigates whether using classification losses instead of regression can similarly improve scalability of value-based deep RL.

Proposed Solution:
The paper proposes using a categorical cross-entropy loss instead of mean squared error (MSE) to train value functions. Specifically, it transforms the scalar value regression target into a categorical distribution over discretized value ranges. The network then predicts a categorical distribution and is optimized via cross-entropy against this target distribution. Several methods are explored for constructing the target distribution, including two-hot encoding, histogram smoothing (HL-Gauss) and modeling the full return distribution (C51).

Main Contributions:

- Shows that HL-Gauss cross-entropy loss leads to 30% better Atari scores with Mixture-of-Experts, 1.8-2.1x better multi-task performance on Atari, 40% better Wordle playing, 70% better robotic manipulation and reduced gap to Stockfish in Chess without search.

- Demonstrates scalability benefits across various networks like Transformers, ResNets and Mixture-of-Experts, establishing classification loss as a pivotal technique for scaling up deep RL.

- Performed careful ablative analysis showing that (i) using cross-entropy itself matters more than categorical representation and (ii) benefits arise from handling noise, non-stationarity and enabling more expressive representations better than MSE loss.

- Established state-of-the-art results on multiple domains while showing the efficacy of a simple change that could potentially translate innovations in deep RL to large models like Transformers.

Overall, the paper makes a compelling case, through extensive empirical evidence and careful analysis, that simply using classification via cross-entropy loss instead of regression can lead to substantial improvements in performance and scalability of deep RL methods.
