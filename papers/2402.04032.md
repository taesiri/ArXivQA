# [HEAM : Hashed Embedding Acceleration using Processing-In-Memory](https://arxiv.org/abs/2402.04032)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recommendation systems rely heavily on embedding operations, which exhibit irregular memory access patterns and are very memory-bound. As models grow larger, embedding tables now exceed tens of terabytes, causing major challenges for efficient inference on single nodes.

- Using compositional embedding to reduce embedding table sizes further exacerbates the memory bottleneck issue, as it requires accessing two tables (quotient and remainder) to reconstruct embeddings. This doubles the memory accesses compared to the original model.

Proposed Solution:
- The paper proposes HEAM, a heterogeneous memory architecture with a 3-level PIM system designed specifically to accelerate compositional embedding for recommendation systems. 

- It consists of a DIMM-HBM combination, where HBM features a base die-level PIM (bd-PIM) and bank group-level PIMs (bg-PIMs). Bg-PIMs additionally contain Lookup Tables (LUTs) to store the small remainder tables.

- The quotient table is split between DIMM and HBM based on access frequency distribution to maximize bandwidth. The remainder table is entirely stored in the LUTs inside bg-PIMs.

- Two-stage partial embeddings additions are performed hierarchically using the bd-PIM and bg-PIMs to reduce memory traffic. LUT access in bg-PIMs also decreases bank traffic.

Main Contributions:

- First architecture specifically targeting large compositional embedding models for inference acceleration. Addresses both model size and memory bottleneck issues.

- Introduces optimized allocation strategy to store quotient and remainder tables in the most bandwidth-efficient manner based on their distinct characteristics.

- Specially designed 2-level HBM-PIM system with LUTs in bg-PIMs to best leverage properties of compositional embedding and maximize memory parallelism.

- Achieves 6.3x speedup and 58.9% energy savings over state-of-the-art DIMM-PIM baselines. Enables efficient deployment of large models on single nodes.

The summary covers the key problem being addressed, the proposed heterogeneous memory and PIM architecture along with the specialized optimization techniques for compositional embedding, and the main contributions related to performance, energy efficiency and model size support.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes HEAM, a specialized 3-level memory architecture with processing-in-memory for accelerating recommendation systems that use compositional embedding to reduce the size of large embedding tables.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing HEAM, a specialized heterogeneous memory architecture integrated with a two-level processing-in-memory (PIM) system to accelerate the inference process of compositional embedding for recommendation systems. Specifically, the key contributions are:

1) HEAM is the first work to jointly address the challenges of large model size and memory-bound operations in deep learning recommendation models (DLRM) using compositional embedding. It reduces model size through compositional embedding while accelerating performance with its specialized hardware design. 

2) The paper proposes an allocation strategy to store the quotient (Q) and remainder (R) tables of the compositional embedding in different parts of the memory system based on an analysis of their distinct data access patterns and characteristics.

3) A two-level PIM system is designed within High Bandwidth Memory (HBM), including a base-die PIM (bd-PIM) and bank-group PIMs (bg-PIMs) with lookup tables (LUTs). This creates a three-tiered memory hierarchy to enhance memory parallelism and bandwidth.

In evaluations, HEAM achieved 6.3x speedup and 58.9% energy savings compared to prior near-memory processing architectures for recommendations systems. So in summary, the key innovation is the HEAM architecture itself, which is purpose-built to mitigate the issues with using compositional embedding for large-scale recommendation model inference.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords associated with it:

- Compositional embedding
- Processing-in-memory (PIM)
- Hashed embedding 
- Recommendation systems
- Deep learning recommendation model (DLRM)
- Embedding operation
- Memory-bound 
- Three-tiered memory hierarchy
- DIMM
- HBM (High Bandwidth Memory)
- Base die PIM (bd-PIM)
- Bank group PIM (bg-PIM) 
- Look-up table (LUT)
- Temporal locality
- Spatial locality
- Throughput
- Energy efficiency

The paper proposes a specialized memory architecture called HEAM that integrates a three-tiered memory system consisting of DIMM, HBM with bd-PIM, and bg-PIM with LUT to accelerate compositional embedding for recommendation systems. Key goals are improving throughput and energy efficiency for the memory-bound embedding operation in large-scale DLRMs by exploiting the characteristics of compositional embedding. The design utilizes processing-in-memory and a heterogeneous memory hierarchy to address these challenges.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a heterogeneous memory architecture called HEAM that integrates DIMM, HBM, and processing-in-memory (PIM) to accelerate compositional embedding for recommendation systems. What were the key observations from the analysis on compositional embedding that motivated the design of HEAM?

2. HEAM utilizes a 3-tier memory hierarchy consisting of DIMM, HBM with base die PIM (bd-PIM), and bank group PIM (bg-PIM) with lookup tables (LUTs). Walk through the rationale behind storing different tables (Q vs R) in different levels of this hierarchy.

3. The paper claims HEAM is the first to address both the large model size problem and memory bandwidth challenges with compositional embedding. Elaborate on how the proposed techniques tackle each of these issues.  

4. Explain the two-stage partial summation approach used in HEAM leveraging the bd-PIM and bg-PIM processing units. How does this approach help to reduce memory bandwidth requirements?

5. What is the motivation behind using instruction batching in HEAM? How does the choice of batch size impact performance and what constraints were considered?

6. The lookup table (LUT) is a key component of the bg-PIM architecture in HEAM. Justify the design choice of placing the LUT in bg-PIM rather than bd-PIM.

7. Discuss the hash table allocation strategy used in HEAM to store the Q and R tables. How does this specialized mapping scheme improve memory efficiency? 

8. With regards to the hash collision value, explain why increasing it does not always lead to a proportional reduction in the number of hot vectors in the Q table. How does this pose a potential challenge?

9. Assess the shortcomings analyzed for compositional embedding, specifically concerning efficiency of increasing hash collisions and accuracy drop. How do the results on real-world datasets demonstrate these limitations?

10. The evaluation shows significant gains in performance and energy efficiency compared to prior near-memory processing designs. Attribute these improvements to the novel optimizations introduced in HEAM. Which technique do you think contributes the most?
