# [Beyond prompt brittleness: Evaluating the reliability and consistency of   political worldviews in LLMs](https://arxiv.org/abs/2402.17649)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent studies suggest that large language models (LLMs) exhibit some political bias, tending to agree more with left-leaning statements. 
- However, it is unclear if this bias is reliable (robust to prompt variations) and consistent across different policy domains and leanings. 
- To claim an LLM has a "worldview", its political stances should be reliable and consistent.

Proposed Solution:
- Create a dataset (ProbVAA) of political statements from voting advice applications in 7 EU countries, with policy domain annotations. 
- Enrich dataset with paraphrases, negations and semantic opposites of statements to test reliability.
- Propose framework to evaluate reliability of LLM's political stances using sampling, prompt variations, and tests on statement variants. 
- Assess consistency by comparing stances to parties' answers, and analyzing per policy domain.

Main Contributions:
- Release of ProbVAA dataset with statement variations and domain annotations
- Proposal of reliability-aware framework for evaluating political bias in LLMs 
- Analysis showing reliability increases with model size, but even large models lack full consistency
- Findings:
   - Models align more with left parties overall  
   - Inconsistent preferences across policy domains
   - Agree with some left policies (environment, welfare) and some right policies (law & order)

In summary, the paper demonstrates the importance of thoroughly testing for reliability and consistency before claiming an LLM has a political "worldview". The analysis provides a more nuanced view of biases in LLMs.


## Summarize the paper in one sentence.

 This paper proposes a framework to evaluate the reliability and consistency of political worldviews in large language models by augmenting a dataset of voting advice questionnaire statements with variations and annotations, finding that model reliability increases with size but consistency is lacking, with environment and social welfare alignment but also law and order agreement.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. The release of ProbVAA, a dataset of political statements from voting advice applications (VAAs) across 7 EU countries, annotated with policy domains and expanded with paraphrased, negated, and semantically inverted versions. 

2. A proposed method for evaluating the reliability and consistency of language models' political biases. This involves creating variations of statements and prompts to test reproducibility of stances, as well as analyzing variance within broader categories of statements to evaluate consistency.

3. An analysis of several state-of-the-art language models ranging in size on the ProbVAA dataset. The analysis finds that reliability increases with parameter count, and that while larger models align more with left-leaning parties overall, they lack consistency in their leanings across policy domains.

In summary, the paper introduces a methodology and dataset for thoroughly evaluating political biases in language models in terms of both reliability and consistency, and provides a nuanced analysis of the biases present in current models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and keywords associated with this paper include:

- Large language models (LLMs)
- Political bias/worldview
- Reliability testing 
- Consistency testing
- Prompt engineering
- Policy domains
- Voting advice applications (VAAs)
- Left/right political leaning
- Environment protection 
- Social welfare
- Law and order
- Migration policy
- Economic policy
- Parameter counts
- Instruction following

The paper proposes methods to evaluate the reliability and consistency of political worldviews in large language models through testing on variations of policy statements from voting advice applications. It looks at how robustly and consistently LLMs of varying sizes take political stances across different policy domains. The key goal is to understand whether LLMs have embedded specific political biases and if they constitute an actual political "worldview".


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes creating variants of the original policy statements, including paraphrases, negated versions, and semantic opposites. What is the purpose of creating these variants? How do they help evaluate the reliability and consistency of the language models' political worldviews?

2. The paper evaluates several language models ranging in size from 7B to 70B parameters. Why did the authors choose to evaluate models across this range of sizes? What effect does model size have on reliability and consistency? 

3. The authors map the language models' free text responses onto binary stances of agreement or disagreement. What criteria do they use to perform this mapping? Why do they argue a rule-based mapping approach can reliably categorize the various response types?

4. The authors sample 30 responses from each model for every prompt to establish a robust probability distribution over stances. How do they determine whether a model's stance on a statement is statistically significant based on these samples? Why 30 samples?

5. How exactly do the authors define and quantify reliability? What specific metrics and statistical tests do they use? How does their definition relate to standards in psychometrics?

6. What is the purpose of using both personal and impersonal prompt templates? Why vary the wording of the response options across templates? How does this help evaluate reliability?

7. One test involves inverting the order of the agree/disagree response labels within prompt templates. Why would inconsistent responses to this inversion indicate lack of reliability? 

8. For evaluating consistency, how do the policy domain annotations and stance quantification formula allow fine-grained analysis? What specifically does consistency refer to?

9. What trends does the paper find regarding how reliability and consistency vary according to model size? How do the largest models compare to humans in reliability? Where do they continue to fall short?

10. The paper cautions against claiming large language models have political "worldviews" despite trends in reliability and left-leaning stances. What specific evidence from the analysis supports this caution? What criteria would need to be met to make such a claim?
