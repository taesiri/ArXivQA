# [VaLID: Variable-Length Input Diffusion for Novel View Synthesis](https://arxiv.org/abs/2312.08892)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes VaLID, a novel diffusion-based framework for novel view image synthesis that can accept a variable number of source view images as input during both training and inference. Unlike prior work like Zero123 which is limited to single view input, VaLID learns a joint appearance and pose conditioning strategy to seamlessly handle multi-view fusion. Specifically, it encodes each source view image and corresponding camera pose into spatial tokens using a pre-trained vision transformer. These tokens are aggregated via a proposed Multi-view Cross Former module which transfers them into a fixed set of target view seed tokens used to condition the diffusion model. This provides consistency across views while being computationally efficient. A two-stage training strategy is introduced - first optimizing single view novel view synthesis, and then finetuning specifically for multi-view fusion and consistency. Experiments demonstrate VaLID produces higher quality results than previous state-of-the-art, with performance improving as more input views are provided. The framework is more flexible and practical for real applications where variable multi-view data is readily available.
