# [VaLID: Variable-Length Input Diffusion for Novel View Synthesis](https://arxiv.org/abs/2312.08892)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes VaLID, a novel diffusion-based framework for novel view image synthesis that can accept a variable number of source view images as input during both training and inference. Unlike prior work like Zero123 which is limited to single view input, VaLID learns a joint appearance and pose conditioning strategy to seamlessly handle multi-view fusion. Specifically, it encodes each source view image and corresponding camera pose into spatial tokens using a pre-trained vision transformer. These tokens are aggregated via a proposed Multi-view Cross Former module which transfers them into a fixed set of target view seed tokens used to condition the diffusion model. This provides consistency across views while being computationally efficient. A two-stage training strategy is introduced - first optimizing single view novel view synthesis, and then finetuning specifically for multi-view fusion and consistency. Experiments demonstrate VaLID produces higher quality results than previous state-of-the-art, with performance improving as more input views are provided. The framework is more flexible and practical for real applications where variable multi-view data is readily available.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Novel view synthesis (NVS) aims to generate realistic novel views of a scene given source view images and poses. Most methods either require optimization at test time, produce low quality results, or can only handle a single input view image even if more views are available. This limits their flexibility for real applications.

Method: 
The paper proposes VaLID, a variable-length input diffusion model for NVS that can handle a varying number of input views during both training and inference. It uses an appearance-pose entanglement conditioning strategy where image features and poses are jointly encoded in the diffusion model's conditioning. 

A source view tokenization module extracts spatial image tokens paired with pose vectors using a MAE pre-trained vision transformer. The multi-view cross former fuses tokens from multiple views into a fixed set of target view tokens that condition image generation. This provides consistency when using multiple views while keeping computation fixed.

Training uses a two-stage strategy. Stage 1 trains on single views to enable NVS. Stage 2 shows multiple random views to the cross former to handle consistency over variables views during inference without having to pass all views.

Contributions:
1) Novel diffusion model for NVS that can handle a variable number of input views
2) Appearance-pose entanglement conditioning strategy  
3) Multi-view cross former to fuse variable input views into consistent conditioning
4) Two-stage training strategy to improve efficiency

Experiments show higher quality results than prior arts both quantitatively and qualitatively. Using multiple views further improves performance and consistency during inference. The method advances flexibility of learning-based NVS to handle more real application settings.
