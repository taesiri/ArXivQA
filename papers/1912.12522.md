# [NAS evaluation is frustratingly hard](https://arxiv.org/abs/1912.12522)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions addressed in this paper are:

1. How to fairly evaluate and compare different neural architecture search (NAS) strategies? 

The paper points out that it is very challenging to compare published NAS methods due to differences in search spaces, training protocols, and lack of implementation details. The authors aim to benchmark and evaluate NAS methods in a fair manner.

2. What is the contribution of different components in the NAS pipeline (search space, search strategy, training protocol) to the final performance?

The paper investigates the impact of various factors like search space design, training tricks, hyperparameters etc. on the accuracy of found architectures. The goal is to understand the relative importance of search algorithm versus these other elements.

3. Why do many recent NAS methods struggle to significantly outperform random search or sampling?

The authors find that for many NAS techniques, the average random architecture performs on par with the best architectures found through search. The paper tries to analyze the reasons behind this observation.

4. How to improve reproducibility, enable fair comparisons, and avoid pitfalls in NAS research? 

The paper provides suggestions and best practices regarding evaluation protocols, reporting details, using multiple datasets, ablation studies etc. to make NAS research more principled and reproducible.

In summary, the central focus is on developing a fair framework for NAS evaluation, investigating the contribution of different pipeline components, and analyzing why many recent techniques fail to substantially beat random search. The overall goal is to understand how to properly assess NAS methods and make progress in architecture search.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. They perform a systematic benchmark of 8 Neural Architecture Search (NAS) methods on 5 image classification datasets. The goal is to evaluate the search strategies in isolation by comparing to random sampling baselines from the same search spaces.

2. They find that the relative improvement of most NAS methods over random sampling is small, suggesting the manually designed search spaces already give good performance. The training protocol (augmentation tricks etc.) has a bigger impact than the architecture search.

3. They perform ablation studies on the commonly used DARTS search space and find factors like the number of cells, seed, and macro-architecture matter more than the specific operations chosen.

4. They suggest best practices and ways to mitigate issues around reproducibility, hyperparameter tuning, and overfitting to CIFAR-10 in NAS research. Their key suggestions are:

- Compare to random sampling baselines from the same search space.

- Test on multiple diverse datasets, not just CIFAR-10. 

- Perform ablation studies to understand contributions.

- Release code, details, and seeds for reproducibility.

- Include hyperparameter search time in overall search cost.

In summary, the paper provides a useful benchmark and analysis of NAS methods, highlighting issues around evaluation and reproducibility while providing suggestions to improve research in this area. The relative improvement metric over random sampling is a simple but effective technique to better evaluate search strategies.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in neural architecture search:

- Benchmarking multiple NAS methods: This paper systematically compares 8 different NAS methods on 5 image classification datasets. Most prior NAS papers focus on evaluating just their proposed method rather than comparing to other approaches. Benchmarking multiple methods on diverse datasets provides a more rigorous assessment of their capabilities.

- Analyzing search spaces and training tricks: The paper investigates properties of a commonly used NAS search space from DARTS and studies the impact of different training enhancements like AutoAugment. This analysis provides insights into how much performance gains come from architecture search versus other factors like search space design and training techniques. Most papers do not rigorously analyze these aspects.

- Critiquing lack of reproducibility: The paper makes the important point that many recent NAS papers lack reproducibility due to not releasing code or details needed to replicate experiments. The authors share their code and models to enable reproducing their benchmark. Emphasizing reproducibility is still uncommon in NAS research.

- Suggesting best practices: Based on their analysis, the authors propose best practices like reporting performance of random architectures, using multiple datasets, and including hyperparameter tuning time in the search cost. Highlighting methodological issues and suggesting improvements is valuable for advancing the NAS field.

Overall, this paper makes valuable contributions not just in benchmarking methods, but also in critically analyzing the experimental practices and assumptions underlying much NAS research. The focus on rigorous analysis and reproducibility differentiates it from many previous papers which often aim to achieve state-of-the-art accuracy on a single task. The insights from this study could help guide future NAS research towards more rigorous, transparent, and unbiased evaluation.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Develop more expressive search spaces that go beyond incremental improvements to existing human-designed architectures. The authors argue that current search spaces like DARTS are overly restrictive and lead to only minor improvements over random sampling. They suggest exploring search spaces that allow more flexibility to escape local optima and find innovative global optima.

- Search macro-architecture in addition to micro-architecture. Many NAS methods currently search for cell operations but take the high-level network wiring as fixed. The authors suggest also optimizing the macro-architecture could lead to further gains.

- Test NAS algorithms on multiple diverse datasets, not just CIFAR-10. Exclusively using CIFAR-10 risks overfitting to that particular dataset. Evaluating on a range of datasets with different characteristics would better measure generalization.

- Perform ablation studies to understand the contributions of different components of the NAS pipeline. The lack of ablation studies makes it difficult to determine which elements actually improve performance.

- Share code, hyperparameters, training details, and seeds to improve reproducibility. The authors argue reproducibility is crucial for advancing NAS research.

- Report metrics like relative improvement over average random architecture to better isolate gains from search algorithm versus other factors like search space design.

- Account for computational cost of hyperparameter optimization in evaluating search algorithms. The computational expense of tuning hyperparameters should be included in assessing NAS methods.

In summary, the authors advocate for more rigorous, reproducible, and generalizable approaches to evaluating and comparing NAS algorithms across diverse tasks. They recommend avoiding pitfalls like overfitting to CIFAR-10 and lack of ablation studies.


## Summarize the paper in one paragraph.

 The paper presents a benchmark of 8 NAS methods on 5 image classification datasets. The key findings are:

- Most NAS methods only marginally outperform the average randomly sampled architecture from their search space. This suggests the performance gains are largely due to manual tuning of the search space and training protocol rather than the search algorithm itself. 

- The training protocol (augmentation tricks, longer training, etc) has a much bigger impact on accuracy than the specific architecture. Random DARTS architectures achieve 98% on CIFAR10 with the full training protocol, close to state-of-the-art.

- The DARTS search space is very narrow, with all sampled architectures within 1% test accuracy. Hyperparameters like number of cells and seed have a big impact on rankings.

- The hand-designed macro-structure (cells) seems more important than the searched micro-structure (operations). Modifying the operations had little effect on performance.

- There is a depth gap between architectures optimized on 8 cells versus 20 cells. Rankings change substantially when retraining on 20 cells.

Overall, the paper highlights reproducibility issues in NAS and that most gains are from manual engineering, not automated search. It provides suggestions like reporting metrics versus random architectures and testing on multiple datasets.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

This paper proposes a framework for evaluating and comparing neural architecture search (NAS) methods in a fair and reproducible way. The authors benchmark 8 NAS algorithms (DARTS, PDARTS, etc.) on 5 image classification datasets (CIFAR10, CIFAR100, etc.). To isolate the contribution of the search strategy, they compare each method against random sampling from the same search space using the same training protocol. They find most methods struggle to significantly beat this random baseline, suggesting improvements come largely from manually engineered search spaces and training tricks, not the search algorithms. Through further experiments on the DARTS search space, they show factors like the seed, number of cells, and macro-architecture matter more than the specific operations. The paper concludes with suggested best practices like ablating training protocols, trying new tasks/datasets, and releasing code for reproducibility.

In summary, this paper provides a systematic comparison of NAS techniques, finding they rarely outperform random sampling by much. The authors argue gains have come more from expert knowledge in the search space and training protocol than the search algorithms themselves. They offer suggestions to mitigate issues around reproducibility and overfitting to specific datasets like CIFAR10. The key message is the NAS community should focus more on developing truly general and automated methods.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new neural architecture search method called ProxylessNAS. The key ideas are:

- Parameterize the architecture using continuous variables rather than discrete choices. This allows optimizing the architecture with gradient descent. 

- Share weights between architectures during the search to avoid having to train each candidate from scratch. A path sampling algorithm is used to sample architectures and compute the gradients for weight sharing.

- Directly learn architectures on the target task and dataset instead of using proxies like classification on a small dataset. This avoids having to transfer architectures found on the proxy task.

- Use a differentiable bilinear resize operation instead of average pooling. This allows resizing feature maps in the architectures being searched.

- Use a differentiable MobileNetV2 style block instead of bottleneck blocks. This allows searching over more mobile-efficient architectures.

- Search progressively from shallow to deep networks. Shallower proxyless networks are trained first, then their weights are reused to initialize deeper architectures during search.

In summary, ProxylessNAS is able to directly search over efficient architectures for target tasks by using differentiable components and weight sharing between candidate architectures. It finds highly efficient architectures surpassing hand-designed and NAS baselines.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one sentence summary: 

The paper explores the issues around fairly evaluating and comparing neural architecture search methods, finding that the training protocol has a bigger impact on performance than the architecture search algorithm itself, and proposes best practices like using the average random architecture as a baseline to better isolate the contribution of the search algorithm.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper aims to evaluate and analyze different neural architecture search (NAS) strategies in a fair and reproducible manner. NAS methods have shown promise for automating neural network design, but comparing them is difficult due to differences in search spaces, training protocols, and lack of ablation studies. 

- The authors benchmark 8 NAS methods on 5 image classification datasets. To enable comparison, they calculate each method's performance relative to random sampling from the search space. 

- Surprisingly, they find most methods struggle to significantly beat the average random architecture baseline. The training protocol seems more important than the architecture.

- Through experiments on the DARTS search space, they show architectures have a very narrow accuracy range. Hyperparameters like number of cells have a big impact on rankings. The cell structure matters more than the operations.

- The paper offers suggestions to improve NAS research: use relative metrics, test on multiple datasets, include training cost, open source code/models, and simplify search spaces to find innovative designs.

In summary, the key focus is on highlighting issues of fairness and reproducibility in NAS research, providing experiments and suggestions to mitigate these issues, and understanding the real contributions behind claimed NAS improvements. The overall goal is to push the community towards developing more general and principled NAS techniques.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Neural Architecture Search (NAS): The overall field of automatically searching for optimal neural network architectures.

- Search space: The set of possible architectures that can be searched over. Often contains operations like convolutions, pooling, etc. that are combined in different ways.

- Search strategy: The algorithm used to actually search the space, e.g. evolution, reinforcement learning, gradient-based.

- Evaluation protocol: How the architectures are trained and evaluated, including tricks like cutout, droppath, etc.

- Macro-structure vs micro-structure: The overall cell-based wiring of the network (macro) versus the specific operations in each cell (micro).

- Average architecture: A randomly sampled architecture from the search space, used as a baseline. 

- Relative improvement (RI): Metric comparing search method to average architecture baseline.

- Reproducibility: The ability to replicate prior results, highlighted as an issue in NAS literature.

- Training tricks: Methods like cutout, droppath, etc. that boost performance at training time.

- Depth gap: Issue where search is done on smaller networks than final trained model.

So in summary, it looks at NAS algorithms, benchmarks them against average architectures, analyzes the DARTS search space, and provides suggestions like reporting relative improvement to aid reproducibility.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing this paper:

1. What is the main contribution or purpose of this paper?

2. What problem is the paper trying to solve?

3. What methods or techniques does the paper propose? 

4. What previous work is this paper building on? What are the key references?

5. What datasets were used to evaluate the proposed approach?

6. What were the main results? How did the proposed approach compare to baselines or previous work?

7. What are the limitations of the proposed approach? What issues remain unsolved?

8. Did the paper include ablation studies or analyses of the impact of different components?

9. Does the paper suggest any directions for future work?

10. Does the paper highlight any broader impacts or implications of this research?

Asking questions like these should help extract the key information needed to summarize the paper's main contributions, methods, results, and implications. The goal is to understand the core concepts and outcomes, assess the approaches taken, and identify remaining challenges or open issues. A good summary should capture the essence of the paper in a concise yet comprehensive manner.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a neural architecture search method based on multi-agent reinforcement learning. How does the multi-agent aspect of the approach help balance exploration and exploitation during the search process? What are the trade-offs associated with using multiple agents versus a single agent?

2. The method uses a recurrent neural network controller to generate candidate architectures. How is the RNN controller trained? Why is an RNN well-suited for this task compared to other types of models? What architectural hyperparameters of the RNN impact the search process?

3. The paper introduces a new Gibbs sampling strategy for training the RNN controller. How does this sampling strategy differ from typical gradient-based training? What are the benefits of using Gibbs sampling in this context? How sensitive are the results to the temperature parameter used for sampling?

4. The reward signal for training the RNN controller is based on the validation accuracy of candidate architectures. However, evaluating accuracy requires full training of models. How does the method address the computational expense of evaluating a large number of candidates? Are there other potential reward signals that could replace or augment accuracy?

5. The method incorporates several techniques like weight sharing and early stopping to make the search process faster. How do these techniques speed up the search? What trade-offs do they introduce in terms of search accuracy or efficiency? Could other techniques like distillation be applicable?

6. The experiments only test the method on image classification datasets like CIFAR-10. How well would the approach generalize to other domains like natural language processing? What modifications or hyperparameters would need to be re-tuned?

7. The method searches over a cell-based search space. How was this space designed? What are the advantages of using a cell-based space versus a full network space for architecture search? How transferrable are the discovered cells to other datasets?

8. The paper compares the method against several baselines like random search and evolutionary approaches. What are the most salient advantages of the proposed method over these baselines? Under what conditions would the baselines outperform the proposed approach?

9. How sensitive is the method to the various hyperparameters used during search, like number of agents, RNN size, etc? Is there a principled way to set these hyperparameters a priori or do they require tuning on each dataset?

10. The authors use learned architectures as initialization for further evolution search. Why is this beneficial? Does this hybrid approach lead to architectures that could not be found by either method alone? How do the learned and evolved architectures compare?
