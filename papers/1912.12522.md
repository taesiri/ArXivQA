# [NAS evaluation is frustratingly hard](https://arxiv.org/abs/1912.12522)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions addressed in this paper are:1. How to fairly evaluate and compare different neural architecture search (NAS) strategies? The paper points out that it is very challenging to compare published NAS methods due to differences in search spaces, training protocols, and lack of implementation details. The authors aim to benchmark and evaluate NAS methods in a fair manner.2. What is the contribution of different components in the NAS pipeline (search space, search strategy, training protocol) to the final performance?The paper investigates the impact of various factors like search space design, training tricks, hyperparameters etc. on the accuracy of found architectures. The goal is to understand the relative importance of search algorithm versus these other elements.3. Why do many recent NAS methods struggle to significantly outperform random search or sampling?The authors find that for many NAS techniques, the average random architecture performs on par with the best architectures found through search. The paper tries to analyze the reasons behind this observation.4. How to improve reproducibility, enable fair comparisons, and avoid pitfalls in NAS research? The paper provides suggestions and best practices regarding evaluation protocols, reporting details, using multiple datasets, ablation studies etc. to make NAS research more principled and reproducible.In summary, the central focus is on developing a fair framework for NAS evaluation, investigating the contribution of different pipeline components, and analyzing why many recent techniques fail to substantially beat random search. The overall goal is to understand how to properly assess NAS methods and make progress in architecture search.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. They perform a systematic benchmark of 8 Neural Architecture Search (NAS) methods on 5 image classification datasets. The goal is to evaluate the search strategies in isolation by comparing to random sampling baselines from the same search spaces.2. They find that the relative improvement of most NAS methods over random sampling is small, suggesting the manually designed search spaces already give good performance. The training protocol (augmentation tricks etc.) has a bigger impact than the architecture search.3. They perform ablation studies on the commonly used DARTS search space and find factors like the number of cells, seed, and macro-architecture matter more than the specific operations chosen.4. They suggest best practices and ways to mitigate issues around reproducibility, hyperparameter tuning, and overfitting to CIFAR-10 in NAS research. Their key suggestions are:- Compare to random sampling baselines from the same search space.- Test on multiple diverse datasets, not just CIFAR-10. - Perform ablation studies to understand contributions.- Release code, details, and seeds for reproducibility.- Include hyperparameter search time in overall search cost.In summary, the paper provides a useful benchmark and analysis of NAS methods, highlighting issues around evaluation and reproducibility while providing suggestions to improve research in this area. The relative improvement metric over random sampling is a simple but effective technique to better evaluate search strategies.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in neural architecture search:- Benchmarking multiple NAS methods: This paper systematically compares 8 different NAS methods on 5 image classification datasets. Most prior NAS papers focus on evaluating just their proposed method rather than comparing to other approaches. Benchmarking multiple methods on diverse datasets provides a more rigorous assessment of their capabilities.- Analyzing search spaces and training tricks: The paper investigates properties of a commonly used NAS search space from DARTS and studies the impact of different training enhancements like AutoAugment. This analysis provides insights into how much performance gains come from architecture search versus other factors like search space design and training techniques. Most papers do not rigorously analyze these aspects.- Critiquing lack of reproducibility: The paper makes the important point that many recent NAS papers lack reproducibility due to not releasing code or details needed to replicate experiments. The authors share their code and models to enable reproducing their benchmark. Emphasizing reproducibility is still uncommon in NAS research.- Suggesting best practices: Based on their analysis, the authors propose best practices like reporting performance of random architectures, using multiple datasets, and including hyperparameter tuning time in the search cost. Highlighting methodological issues and suggesting improvements is valuable for advancing the NAS field.Overall, this paper makes valuable contributions not just in benchmarking methods, but also in critically analyzing the experimental practices and assumptions underlying much NAS research. The focus on rigorous analysis and reproducibility differentiates it from many previous papers which often aim to achieve state-of-the-art accuracy on a single task. The insights from this study could help guide future NAS research towards more rigorous, transparent, and unbiased evaluation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Develop more expressive search spaces that go beyond incremental improvements to existing human-designed architectures. The authors argue that current search spaces like DARTS are overly restrictive and lead to only minor improvements over random sampling. They suggest exploring search spaces that allow more flexibility to escape local optima and find innovative global optima.- Search macro-architecture in addition to micro-architecture. Many NAS methods currently search for cell operations but take the high-level network wiring as fixed. The authors suggest also optimizing the macro-architecture could lead to further gains.- Test NAS algorithms on multiple diverse datasets, not just CIFAR-10. Exclusively using CIFAR-10 risks overfitting to that particular dataset. Evaluating on a range of datasets with different characteristics would better measure generalization.- Perform ablation studies to understand the contributions of different components of the NAS pipeline. The lack of ablation studies makes it difficult to determine which elements actually improve performance.- Share code, hyperparameters, training details, and seeds to improve reproducibility. The authors argue reproducibility is crucial for advancing NAS research.- Report metrics like relative improvement over average random architecture to better isolate gains from search algorithm versus other factors like search space design.- Account for computational cost of hyperparameter optimization in evaluating search algorithms. The computational expense of tuning hyperparameters should be included in assessing NAS methods.In summary, the authors advocate for more rigorous, reproducible, and generalizable approaches to evaluating and comparing NAS algorithms across diverse tasks. They recommend avoiding pitfalls like overfitting to CIFAR-10 and lack of ablation studies.


## Summarize the paper in one paragraph.

The paper presents a benchmark of 8 NAS methods on 5 image classification datasets. The key findings are:- Most NAS methods only marginally outperform the average randomly sampled architecture from their search space. This suggests the performance gains are largely due to manual tuning of the search space and training protocol rather than the search algorithm itself. - The training protocol (augmentation tricks, longer training, etc) has a much bigger impact on accuracy than the specific architecture. Random DARTS architectures achieve 98% on CIFAR10 with the full training protocol, close to state-of-the-art.- The DARTS search space is very narrow, with all sampled architectures within 1% test accuracy. Hyperparameters like number of cells and seed have a big impact on rankings.- The hand-designed macro-structure (cells) seems more important than the searched micro-structure (operations). Modifying the operations had little effect on performance.- There is a depth gap between architectures optimized on 8 cells versus 20 cells. Rankings change substantially when retraining on 20 cells.Overall, the paper highlights reproducibility issues in NAS and that most gains are from manual engineering, not automated search. It provides suggestions like reporting metrics versus random architectures and testing on multiple datasets.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points in the paper:This paper proposes a framework for evaluating and comparing neural architecture search (NAS) methods in a fair and reproducible way. The authors benchmark 8 NAS algorithms (DARTS, PDARTS, etc.) on 5 image classification datasets (CIFAR10, CIFAR100, etc.). To isolate the contribution of the search strategy, they compare each method against random sampling from the same search space using the same training protocol. They find most methods struggle to significantly beat this random baseline, suggesting improvements come largely from manually engineered search spaces and training tricks, not the search algorithms. Through further experiments on the DARTS search space, they show factors like the seed, number of cells, and macro-architecture matter more than the specific operations. The paper concludes with suggested best practices like ablating training protocols, trying new tasks/datasets, and releasing code for reproducibility.In summary, this paper provides a systematic comparison of NAS techniques, finding they rarely outperform random sampling by much. The authors argue gains have come more from expert knowledge in the search space and training protocol than the search algorithms themselves. They offer suggestions to mitigate issues around reproducibility and overfitting to specific datasets like CIFAR10. The key message is the NAS community should focus more on developing truly general and automated methods.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a new neural architecture search method called ProxylessNAS. The key ideas are:- Parameterize the architecture using continuous variables rather than discrete choices. This allows optimizing the architecture with gradient descent. - Share weights between architectures during the search to avoid having to train each candidate from scratch. A path sampling algorithm is used to sample architectures and compute the gradients for weight sharing.- Directly learn architectures on the target task and dataset instead of using proxies like classification on a small dataset. This avoids having to transfer architectures found on the proxy task.- Use a differentiable bilinear resize operation instead of average pooling. This allows resizing feature maps in the architectures being searched.- Use a differentiable MobileNetV2 style block instead of bottleneck blocks. This allows searching over more mobile-efficient architectures.- Search progressively from shallow to deep networks. Shallower proxyless networks are trained first, then their weights are reused to initialize deeper architectures during search.In summary, ProxylessNAS is able to directly search over efficient architectures for target tasks by using differentiable components and weight sharing between candidate architectures. It finds highly efficient architectures surpassing hand-designed and NAS baselines.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary: The paper explores the issues around fairly evaluating and comparing neural architecture search methods, finding that the training protocol has a bigger impact on performance than the architecture search algorithm itself, and proposes best practices like using the average random architecture as a baseline to better isolate the contribution of the search algorithm.
