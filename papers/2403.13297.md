# [POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable   Satisfaction of Hard Constraints](https://arxiv.org/abs/2403.13297)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) methods lack safety guarantees when controlling robots, preventing their use in safety-critical applications. Existing approaches either use soft constraints that only encourage but don't guarantee safety, or require accurate models of robot dynamics which are unavailable in RL settings. The paper aims to develop an RL framework that can provide guarantees on satisfying hard safety constraints for robots with unknown dynamics.

Proposed Solution: 
The key idea is to construct a "buffer region" around unsafe states and learn a policy that is affine (linear) over this region. This allows analytical verification that the policy will steer states away from constraint violation. Specifically:

1. A buffer region is constructed around the unsafe set. The policy is constrained using POLICE to be affine in this region. 

2. Conditions are derived under which trajectories are guaranteed to satisfy the constraints if the policy pushes states away from the unsafe set at the buffer boundary. This is checked by validating a "repulsion condition" only at the vertices of the buffer region.

3. The constraints are incorporated into RL training through negative rewards when the repulsion condition is violated. This shapes the policy to satisfy safety constraints.

4. After training, the policy is guaranteed to satisfy hard safety constraints thanks to its enforced affine structure in the buffer region.


Main Contributions:

1. Proposes POLICEd RL, the first RL framework to provide guarantees on satisfying hard constraints for black-box system dynamics based on constructing an affine policy buffer region.

2. Provides theoretical safety guarantees and existence conditions for constraint satisfying policies.

3. Demonstrates improved performance over baseline RL methods in enforcing constraints for an inverted pendulum and a 7-DOF robot arm task while achieving higher rewards.

The key insight is to exploit the power of enforced affine policies for verification of safety constraints. This allows POLICEd RL to provide safety guarantees for unknown system dynamics, overcoming limitations of prior work. The experiments highlight its promise for safety-critical control from reinforcement learning.
