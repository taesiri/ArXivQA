# [Bidirectional Cross-Modal Knowledge Exploration for Video Recognition   with Pre-trained Vision-Language Models](https://arxiv.org/abs/2301.00182)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question/hypothesis of this paper is:

Can we better utilize pre-trained vision-language models like CLIP for video recognition by exploring bidirectional cross-modal knowledge transfer? 

The key points are:

- Previous works using CLIP for video recognition have only utilized unidirectional video-to-text matching. This paper argues we should facilitate bidirectional knowledge transfer to fully leverage CLIP's capabilities.

- The paper proposes mining both Video-to-Text and Text-to-Video knowledge from CLIP:

1) In the Video-to-Text direction, they introduce a Video-Attribute Association mechanism to generate textual attributes from the video for complementary recognition.

2) In the Text-to-Video direction, they propose a Video Concept Spotting mechanism to generate temporal saliency from the category text for enhanced video representation.

- They combine these mechanisms into a framework called BIKE that achieves state-of-the-art results by exploring bidirectional knowledge between vision and language domains using CLIP.

In summary, the key hypothesis is that bidirectional cross-modal knowledge transfer can better utilize CLIP for video recognition compared to previous unidirectional approaches. The proposed BIKE framework provides a way to achieve this.
