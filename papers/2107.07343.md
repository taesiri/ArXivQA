# [Mutation is all you need](https://arxiv.org/abs/2107.07343)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What are the main components that contribute to the strong performance of the BANANAS neural architecture search method on large cell-based search spaces like DARTS?The authors hypothesize that for large search spaces like DARTS, the performance of BANANAS is mainly determined by its choice of acquisition function optimizer, which does local optimization of architectures, rather than other components like the architecture encoding, surrogate model, or acquisition function. To test this hypothesis, the authors conduct experiments varying different BANANAS components in a factorial manner and evaluate the performance on the NAS-Bench-301 benchmark. The results suggest that the acquisition function optimizer, which minimally mutates the best architecture so far, is the most important factor for BANANAS' performance, rather than the other components.In summary, the central research question is about identifying the key factors behind BANANAS' effectiveness for large neural architecture search spaces, with a hypothesis focused on the role of the acquisition function optimizer. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is an empirical investigation suggesting that the strong performance of the BANANAS neural architecture search method on large search spaces is predominantly determined by its use of a mutation-based acquisition function optimizer, which effectively performs localized search. The key findings are:- Replacing components of BANANAS like the architecture encoding, surrogate model, and acquisition function with more standard choices does not significantly impact performance. This suggests these components are less important.- Local search, which does not use a surrogate model, performs equally well as BANANAS. This provides evidence that the localized search of the acquisition function optimizer is the main driver of BANANAS' success.- The mutation-based optimizer solves the inner acquisition function optimization better than random search. However, the surrogate model makes inaccurate predictions for architectures close to the incumbent.- Overall, the results indicate that for large search spaces, focusing on better acquisition function optimization algorithms may be more impactful than tweaking other BO components like the surrogate model. Localized search methods are a promising approach.In summary, this paper demonstrates through empirical studies that the localized search nature of BANANAS' acquisition function optimization is the predominant factor behind its strong NAS performance on large search spaces. The findings suggest localized search should be a focus for improving BO-based NAS.
