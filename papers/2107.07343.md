# [Mutation is all you need](https://arxiv.org/abs/2107.07343)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What are the main components that contribute to the strong performance of the BANANAS neural architecture search method on large cell-based search spaces like DARTS?The authors hypothesize that for large search spaces like DARTS, the performance of BANANAS is mainly determined by its choice of acquisition function optimizer, which does local optimization of architectures, rather than other components like the architecture encoding, surrogate model, or acquisition function. To test this hypothesis, the authors conduct experiments varying different BANANAS components in a factorial manner and evaluate the performance on the NAS-Bench-301 benchmark. The results suggest that the acquisition function optimizer, which minimally mutates the best architecture so far, is the most important factor for BANANAS' performance, rather than the other components.In summary, the central research question is about identifying the key factors behind BANANAS' effectiveness for large neural architecture search spaces, with a hypothesis focused on the role of the acquisition function optimizer. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is an empirical investigation suggesting that the strong performance of the BANANAS neural architecture search method on large search spaces is predominantly determined by its use of a mutation-based acquisition function optimizer, which effectively performs localized search. The key findings are:- Replacing components of BANANAS like the architecture encoding, surrogate model, and acquisition function with more standard choices does not significantly impact performance. This suggests these components are less important.- Local search, which does not use a surrogate model, performs equally well as BANANAS. This provides evidence that the localized search of the acquisition function optimizer is the main driver of BANANAS' success.- The mutation-based optimizer solves the inner acquisition function optimization better than random search. However, the surrogate model makes inaccurate predictions for architectures close to the incumbent.- Overall, the results indicate that for large search spaces, focusing on better acquisition function optimization algorithms may be more impactful than tweaking other BO components like the surrogate model. Localized search methods are a promising approach.In summary, this paper demonstrates through empirical studies that the localized search nature of BANANAS' acquisition function optimization is the predominant factor behind its strong NAS performance on large search spaces. The findings suggest localized search should be a focus for improving BO-based NAS.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates different components of the BANANAS neural architecture search method and finds that performance on the NAS-Bench-301 benchmark is predominantly determined by the acquisition function optimizer, which does local search, rather than the choice of architecture encoding, surrogate model, or acquisition function.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other work in neural architecture search:- The paper focuses on evaluating different components of the BANANAS algorithm, which is a leading Bayesian optimization method for NAS. This kind of ablation study helps understand what factors drive the performance of NAS algorithms.- The authors find that the acquisition function optimizer, which does local search, is the most important component for BANANAS' strong performance on larger search spaces like DARTS. This highlights the importance of how the acquisition function is optimized, beyond just the choice of acquisition function.- The paper shows that simpler surrogate models like random forests can work as well as BANANAS' neural network ensembles when combined with good acquisition function optimization. This suggests complex surrogate models are less critical.- The experiments use NAS-Bench-301 as a benchmark, following best practices like using the same evaluation protocol and comparing to random search. NAS-Bench-301 allows reproducible comparisons.- Overall, the paper provides useful insights about BO-based NAS methods. It shows that simple techniques like local search are very competitive, and that more complex components like neural surrogates don't necessarily improve performance when the search space is large.In summary, this rigorous ablation study sheds light on the key factors driving NAS algorithm performance on larger search spaces. The analysis of different components is a valuable contribution compared to other NAS papers that often focus on proposing new methods.


## What future research directions do the authors suggest?

Based on the results and discussion in the paper, the authors suggest the following future research directions:- Further work on BO methods for NAS should focus on algorithms for solving the inner acquisition function optimization problem, as this seems to be the most important determinant of performance. Developing more sophisticated acquisition function optimizers could further improve BO-based NAS.- The empirical evaluation was limited to the NAS-Bench-301 benchmark. Testing different BANANAS configurations and ablation studies on other NAS benchmarks could provide additional insights.- The analysis revealed inaccuracies in the surrogate model's predictions for architectures close to the incumbent. Improving the predictive accuracy for similar architectures could be beneficial.- Only a simple random forest was tested as an alternative surrogate model. Evaluating more sophisticated models like graph neural networks is an open research direction.- The impact of different architecture encodings like adjacency matrices could be examined in a similar ablation study.- The analysis was limited to cell-based search spaces. Testing on other spaces like macro-architectures could reveal different insights.- Only standard acquisition functions were evaluated. Novel acquisition functions incorporating architectural properties could be proposed.In summary, the main suggestion is to focus on developing better acquisition function optimization algorithms as this seems to be the key determinant of BO-based NAS performance. Testing on other benchmarks and search spaces as well as improving the surrogate model are also open research directions.
