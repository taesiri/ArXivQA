# [N2N Learning: Network to Network Compression via Policy Gradient   Reinforcement Learning](https://arxiv.org/abs/1709.06030)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we automate the design of compressed neural network architectures using reinforcement learning, so as to optimize the trade-off between model size and accuracy? The key hypothesis is that by formulating the sequential process of compressing a neural network architecture as a Markov Decision Process and training policies with reinforcement learning, it is possible to automatically learn good compressed architectures rather than relying on manual trial-and-error methods.In summary, the paper proposes a principled reinforcement learning approach to learn compressed neural network architectures, with the goal of maximizing compression while maintaining accuracy. The core hypotheses are:1) The compression process can be modeled as an MDP with layer removal and shrinkage actions. 2) Using policy gradient reinforcement learning on this MDP with a combined accuracy-compression reward signal can enable learning policies that automate architecture compression.3) The learned policies can generalize across different network architectures, speeding up compression.The experiments aim to validate these hypotheses by testing the method on different datasets and network architectures. The key research question is whether the proposed reinforcement learning approach can effectively automate neural architecture compression.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a principled reinforcement learning approach to learn compressed network architectures for neural networks. The key ideas are:- Formulating the problem of finding an optimal compressed network architecture as a Markov Decision Process. The state space consists of all possible reduced architectures derived from a teacher network. - Proposing a two-stage reinforcement learning procedure with layer removal and layer shrinkage actions to efficiently explore the large state space.- Designing a reward function that combines model accuracy and compression rate to guide the policy search. The reward function also allows incorporating hardware constraints.- Demonstrating the approach on various datasets (MNIST, CIFAR, SVHN) and network architectures (VGG, ResNet). The method is able to find compressed models that match or exceed the accuracy of the original networks.- Showing the learned policies exhibit some generalization, allowing reuse across different networks to speed up training.In summary, the key contribution is presenting a principled reinforcement learning formulation for the problem of neural network architecture compression and demonstrating its effectiveness empirically. The proposed method automates the model compression process in a data-driven way compared to manual heuristics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a reinforcement learning approach to compress large neural network architectures into smaller, efficient architectures by using policies to sequentially remove layers and reduce layer sizes while maintaining accuracy.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of network architecture compression:- The main novelty of this paper is using reinforcement learning to learn compressed network architectures in a more principled and automated way, compared to prior work like pruning and knowledge distillation that rely more on manual design and heuristics. The idea of formulating architecture search as an MDP and training policies with policy gradients is novel.- Previous architecture search methods focused more on finding architectures from scratch that maximize accuracy, while this paper focuses specifically on the task of compressing a large "teacher" model into a smaller "student" model. So the goal and search space is more targeted.- In terms of knowledge distillation methods for compression, this paper shows competitive or superior results compared to prior work like FitNets and hand-designed models. The learned architectures outperform the baselines in many cases.- Compared to pruning methods that operate directly on the weights, this work shows better and more consistent compression rates by searching the architecture space instead.- The approach also allows incorporating hardware constraints and custom objectives more flexibly compared to other compression techniques.- Using transfer learning to speed up training on larger networks is a useful technique demonstrated in this paper. Showing generalization of policies across different architectures is a nice result.In summary, this paper presents a novel Reinforcement Learning-based approach for network architecture compression that is more automated, flexible and achieves strong compression rates compared to prior model compression techniques. The search space and problem formulation is also more tailored to compression rather than just maximizing accuracy. Demonstrating policy transfer is another useful contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions the authors suggest:- Developing more efficient ways to evaluate candidate student architectures without having to train each one for multiple epochs. They suggest using random weight initialization as shown in prior work, hypernetworks to produce better initial weights for a given architecture, or selecting informative subsets of the training data to quickly evaluate networks.- Exploring whether the learned compression policies capture any generalizable architectural knowledge by transferring them to other architecture search problems beyond just compression. This could reveal interesting insights about deep neural network architectures.- Incorporating additional constraints like power, latency, etc. into the reward function to optimize for these objectives. The current approach focuses mainly on compression rate and accuracy.- Experimenting with more complex and modern network architectures as the teacher model to compress. The paper primarily uses VGG, ResNet and other older convolutional network architectures.- Evaluating the approach on larger scale datasets like ImageNet to assess how well it scales. The experiments are limited to smaller datasets like CIFAR and Caltech.- Modifying the action space to allow more fine-grained control over the search space, like pruning channels rather than whole layers. This could further improve compression rates.- Using transfer learning more extensively to speed up training for new teacher models, instead of always learning policies from scratch. The paper shows some initial promising results on policy transfer.Overall, the paper provides a strong foundation and proof of concept, while highlighting many interesting ways the method could be improved and expanded in future work. The suggested directions aim to make the approach more practical and scalable.


## Summarize the paper in one paragraph.

 The paper presents a reinforcement learning approach to compress large neural network architectures into smaller and more efficient models. It formulates the sequential process of converting a large "teacher" model into a compressed "student" model as a Markov Decision Process (MDP). The approach has two stages: 1) A recurrent "layer removal" policy decides which layers to remove from the teacher model. 2) A "layer shrinkage" policy decides how much to reduce the size of each remaining layer. The resulting student model is evaluated on a reward function based on accuracy and compression rate. Policy gradient methods are used to train the policies to maximize this reward. Experiments show the method can achieve over 10x compression on models like ResNet-34 while maintaining accuracy. Key results include competitive performance compared to pruning and knowledge distillation baselines, the ability to incorporate hardware constraints, and transferring policies between different teacher models. Overall, this is the first principled reinforcement learning approach to automatically learn optimized compressed neural network architectures.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a reinforcement learning approach to compress neural network architectures. The goal is to take a large 'teacher' network and learn a policy to compress it into a smaller 'student' network that maintains accuracy while minimizing parameters. They formulate this as a Markov Decision Process where the states are network architectures, actions modify the architecture, and rewards are based on compression rate and accuracy. A key contribution is a two-stage approach that first removes layers then shrinks the remaining layers, enabling efficient search through the architecture space. The policies are learned with policy gradients, using the accuracy and compression rewards. Experiments demonstrate strong compression rates, outperforming pruning and hand-designed architectures. They also show the approach can incorporate hardware constraints through the reward. Finally, a transfer learning result shows policies pretrained on small networks can accelerate training for larger networks.In summary, this paper makes two main contributions. First, it frames network compression as a reinforcement learning problem over architectures and demonstrates this can find better compressed networks than prior heuristic methods. Second, the two-stage approach enables scalable search through the architecture space. Results validate the method over multiple datasets and network types, including constraints and transfer learning. The approach offers a more principled way to automate network compression compared to manual or heuristic techniques. Limitations are the computational expense of training each architecture and lack of insight into what specifically the policy learns about architectures. Overall this demonstrates the promise of using reinforcement learning for neural architecture search problems.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a reinforcement learning approach to learn compressed neural network architectures in a data-driven way. The method takes a large 'teacher' network as input and outputs a smaller 'student' network derived from the teacher. It uses a two stage process with recurrent policy networks. In the first stage, a policy aggressively removes layers from the teacher model. In the second stage, another policy carefully reduces the size of each remaining layer. The resulting student network is evaluated on a reward function based on accuracy and compression rate. This reward signal is used with policy gradients to train the policies to find an optimal student architecture. The student architectures generated by the policies are trained using knowledge distillation from the teacher network. The overall approach enables efficient exploration of the space of reduced architectures to identify high performance compressed models.


## What problem or question is the paper addressing?

 The paper is addressing the problem of compressing large neural network models into smaller models that can be deployed on devices with limited compute resources. The key question is how to automatically find an optimal smaller "student" network architecture given a larger "teacher" network.The standard approaches rely on manual architecture design or heuristics to compress networks, which can be suboptimal. The authors propose using reinforcement learning to learn policies to compress networks in a more principled, data-driven way.Specifically, they formulate network compression as a Markov Decision Process where the states are network architectures. Actions correspond to layer removal or layer shrinkage. The reward function encourages high accuracy and high compression rate. Policy gradient methods are used to train policies to maximize this reward.So in summary, the paper introduces a reinforcement learning approach to automate the search for optimal compressed network architectures, instead of relying on manual network design or predefined heuristics. The policies learn to make layer removal and shrinkage decisions to maximize accuracy and compression of the student network.
