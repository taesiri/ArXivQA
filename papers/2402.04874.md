# [Choosing a Classical Planner with Graph Neural Networks](https://arxiv.org/abs/2402.04874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Automated planning is an important area of AI research with many applications, but is computationally complex. As different planners have varying strengths, there is interest in developing portfolio-based approaches that can select the best planner for a given planning problem. This paper focuses on the task of "online" planner selection, where a planner is chosen for each specific planning problem rather than creating a single schedule. Recent work has used deep learning on graphical representations of planning problems, but there is room for further investigation into architectures and techniques.

Proposed Solution:
This paper explores using graph neural networks (GNNs) for online planner selection. Four GNN architectures are evaluated: GCN, GGNN, GAT, and GIN. Experiments are done with two graph representations of planning problems - grounded and lifted. Different node features are added such as node degree and neighbor type. Three planner selection methods are explored: 1) predict time to solve and pick fastest, 2) predict probability of solving within time bound, 3) use GNN embedding with XGBoost classification.

Contributions:
- Compares four major GNN architectures for planner selection task
- Evaluates grounded versus lifted graph representations
- Analyzes impact of different node features like degree and neighbor type
- Explores predicting time, probability of success, and direct classification
- Proposes combining GNN representations with XGBoost, achieving good accuracy
- Provides analysis and insights into model behaviors and performance
- Best result is 0.9 accuracy using GCN, grounded graphs, and node degree features
- Discusses future work like specialized GNN architectures and additional features

In summary, this paper conducts a thorough investigation of using GNNs for online planner selection across models, graph representations, prediction tasks and features. The combination of GNN embeddings and XGBoost is a novel contribution. Key results and directions for improvement are discussed.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper explores using graph neural networks for automatic planner selection, evaluating different architectures, graph representations, node features, and prediction tasks, finding GCN works best for predicting planner runtime combined with node degree features, achieving up to 0.9 accuracy.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) The authors explore the strengths and weaknesses of four different GNN architectures (GCN, GGNN, GAT, GIN) for the task of online planner selection. 

2) They investigate two different graph representations of planning problems (lifted and grounded) with additional node features like node degree and neighbor node types. 

3) They experiment with different approaches to select a planner, including predicting the probability that a planner solves a task, predicting the time a planner takes to solve a task, and directly classifying the best planner using the graph representations from the GNNs as input to an XGBoost model.

4) They propose using the graph representations obtained from the GNNs as input to XGBoost, showing this is a more resource-efficient yet accurate approach compared to only using GNNs.

5) They perform a thorough investigation and analysis to show the effectiveness of various GNN-based methods for online planner selection, opening up new research directions.

In summary, the key contribution is a comprehensive study of GNN architectures, graph representations, node features, and prediction tasks for the problem of automatic online planner selection.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Online planner selection
- Graph neural networks (GNNs)
- Graph convolutional networks (GCNs)
- Gated graph neural networks (GGNNs) 
- Graph attention networks (GATs)
- Graph isomorphism networks (GINs)
- Extreme gradient boosting (XGBoost)
- Problem description graphs (PDGs)
- Abstract structure graphs (ASGs)
- International Planning Competition (IPC) dataset
- Predicting planner solve time 
- Predicting planner solve probability
- Grounded and lifted representations
- Node features (e.g. node type, node degree)
- Combining GNNs and XGBoost

The paper explores using different GNN architectures and graph representations for automatically selecting the best planner to solve a given classical planning problem. It looks at predicting the solve time or probability for each planner and picking the best, as well as directly classifying the best planner. Features like node type and degree are analyzed. Finally, combining GNNs with XGBoost for the selection task is also investigated.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper explores using GNNs for online planner selection. What are the key advantages and disadvantages of using GNNs compared to other methods like CNNs?

2. The paper experiments with predicting the time a planner takes to solve a problem vs predicting the probability a planner solves the problem. What are the tradeoffs between these two approaches and why does predicting time perform better? 

3. The paper finds that the grounded representation performs better than the lifted representation. What are the key differences between these representations and why might the grounded one be more suitable for this task?

4. The paper experiments with adding node degree and neighbor type as features. Why are these relevant features for planning problems modeled as graphs? How do they help capture important structural information?

5. The GAT model applies attention to focus on more relevant neighbors. Why is this effective for planning graphs? How does attention help with capturing dependencies?

6. The paper combines GNNs with XGBoost for classification. What are the advantages of this approach compared to only using GNNs? How does it improve efficiency and accuracy?

7. The GIN model is inspired by the Weisfeiler-Lehman graph isomorphism test. How does this help with capturing global, graph-level features relevant for planning problems?

8. How well do you think the models analyzed in this paper would generalize to planning problems outside the IPC benchmark datasets? What challenges might arise?

9. The paper mentions dividing planner selection into two phases, first selecting the top k planners then refining. How could a hierarchical approach like this improve results?

10. The paper suggests designing GNN architectures specialized for planning problems by incorporating domain knowledge. What are some ways domain knowledge of planning could be integrated into a GNN model?
