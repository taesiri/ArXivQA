# [Dive into the Chasm: Probing the Gap between In- and Cross-Topic   Generalization](https://arxiv.org/abs/2402.01375)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Generalization gaps arise when comparing different evaluation setups for language models (LMs), especially between randomly composing training/testing data (In-Topic) versus using distinct groups of topics (Cross-Topic).
- Cross-Topic evaluation is important for challenging argument mining tasks but there is limited understanding of why gaps exist between In- and Cross-Topic setups.

Methods:
- Propose 3 probing experiments on 8 LMs (BERT, RoBERTa, etc) using an argument mining dataset to analyze:
  1) Generalization gaps after pre-training
  2) Dependence on topic-specific vocabulary  
  3) Evolution of gaps during fine-tuning
- Evaluate with linguistic probing tasks (syntax: POS, DEP; semantic: NER) 
- Use amnesic probing to remove topic vocabulary from encodings
- Compare performance on seen vs unseen words

Key Results:
- Generalization gaps substantially differ across LMs, growing more prominent for semantic tasks 
- Probing underperforms on rare/unseen vocabulary
- Diverse pre-training objectives and architectural regularization contribute to smaller gaps
- Removing topic vocabulary negatively impacts some LMs more than others 
- Fine-tuning tends to erase more linguistic properties for In-Topic versus Cross-Topic

Main Contributions:  
- First comprehensive probing-based analysis of generalization gaps between In- and Cross-Topic evaluation
- Demonstrate embedding spaces greatly vary across LMs regarding generalizability/robustness
- Highlight importance of probing as a tool to understand language model capacities across different evaluation setups
