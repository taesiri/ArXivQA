# [Adversarial Finetuning with Latent Representation Constraint to Mitigate   Accuracy-Robustness Tradeoff](https://arxiv.org/abs/2308.16454)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we mitigate the accuracy-robustness tradeoff in adversarial training of deep neural networks?Specifically, the paper aims to address the issue that adversarial training, while improving robustness against adversarial examples, tends to degrade the accuracy on clean examples (referred to as the standard accuracy). This tradeoff between standard accuracy and robustness is a key limitation of adversarial training. The central hypothesis of the paper is that by obtaining suitable representations of both clean and adversarial examples, it may be possible to achieve high standard accuracy and robustness simultaneously. To test this hypothesis, the authors propose a novel adversarial training method called ARREST that incorporates:1) Adversarial finetuning (AFT) to obtain suitable representations of adversarial examples2) Representation-guided knowledge distillation (RGKD) and noisy replay (NR) to preserve suitable representations of clean examples during AFTThe overall goal is to leverage complementary techniques to obtain latent representations that are effective for both clean and adversarial examples, mitigating the accuracy-robustness tradeoff. The paper presents experiments across datasets demonstrating ARREST's ability to achieve higher standard accuracy and robustness compared to prior adversarial training methods.In summary, the central research question is how to mitigate the tradeoff between accuracy on clean examples and robustness on adversarial examples in adversarial training. The key hypothesis is that simultaneously obtaining suitable representations for both types of examples can allow high performance on both. ARREST is proposed to test this hypothesis.


## What is the main contribution of this paper?

This paper proposes a new adversarial training method called ARREST (AdversaRial finetuning with REpresentation conSTraint) to mitigate the accuracy-robustness tradeoff in deep neural networks. The key ideas and contributions are:1. ARREST uses a two-step training process - standard pretraining on clean examples followed by finetuning on adversarial examples (called adversarial finetuning or AFT). This helps obtain suitable representations for both clean and adversarial examples. 2. Two techniques are proposed to constrain representations during AFT - representation-guided knowledge distillation (RGKD) and noisy replay (NR). RGKD penalizes divergence between representations of the pretrained and finetuned networks. NR switches adversarial examples to noisy ones when representations diverge too much.3. ARREST combines AFT, RGKD and NR in a complementary manner to obtain high standard accuracy and robustness. Experiments show it mitigates the accuracy-robustness tradeoff better than previous methods.4. A new quantitative metric called Accuracy Robustness Distance (ARDist) is proposed to evaluate tradeoff mitigation, inspired by the BD-Rate metric from video compression.5. The Ablation study provides insights into the contribution of each component of ARREST. The analysis also explains why noisy replay is better than simply replaying clean examples.In summary, the main contribution is the proposal and analysis of the ARREST method to mitigate the fundamental accuracy-robustness tradeoff in adversarial training. The combination of adversarial finetuning, representation constraint techniques and quantitative evaluation provides new insights into this problem.
