# [Polynomial Regression As an Alternative to Neural Nets](https://arxiv.org/abs/1806.06850)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:What is the relationship between neural networks (NNs) and polynomial regression (PR) models, and what are the implications of this relationship? More specifically, the paper investigates whether NNs can be viewed as a form of PR, with the degree of the polynomial increasing with each hidden layer. The authors present analytic arguments and experimental results to argue that NNs are essentially PR models. They then explore the implications of this NN ≈ PR perspective, such as using properties of PR to gain insights into NNs, and using PR models as an alternative to NNs in some applications.In summary, the central hypothesis is that NNs are closely related to PR, and the paper aims to establish this relationship and discuss its consequences.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a simple analytic argument showing neural networks (NNs) are essentially a form of polynomial regression (PR). Specifically:- The paper shows there is a tighter correspondence between NNs and PR than previously reported, with NNs mimicking PR and the degree of the polynomial increasing with each NN layer. - This NN <-> PR principle is used to predict and confirm lurking multicollinearity problems in NNs.- The equivalence suggests using PR instead of NNs to avoid issues like selecting hyperparameters. - The authors present empirical results across many datasets showing PR matches or exceeds NN accuracy, and release an open-source PR software package.In summary, the key contribution is establishing and exploiting the NN <-> PR equivalence to gain insights into NNs and propose using PR as an effective alternative in many cases.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper argues that neural networks can be viewed as a form of polynomial regression, with the degree of the polynomial increasing with each hidden layer, which provides insights into properties like overfitting and multicollinearity in neural nets and suggests polynomial regression could often be used instead of neural nets, avoiding issues like selecting hyperparameters.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in neural networks and polynomial regression:- The key finding that neural networks can be viewed as approximating polynomial regression is novel. Previous work has shown connections between neural nets and polynomials, such as neural nets being universal approximators that can approximate any continuous function like polynomials. However, this specific perspective of viewing neural nets as directly mimicking polynomial regression seems new.- The paper thoroughly explains the argument for why neural nets can be seen as polynomial regression through analysis of the activation function. The descriptions are clear and easy to follow.- Using the neural net ≈ polynomial regression principle to predict and empirically verify lurking multicollinearity in neural nets is clever, and provides a new insight into neural nets.- The comparison of polynomial regression to neural nets across many real-world datasets is extensive. Showing polynomial regression performing as good or better reinforces the paper's thesis. The competitive performance of polynomial regression is consistent with other recent work questioning neural nets advantages.- The paper is focused specifically on feedforward neural networks, rather than convolutional or recurrent nets which are widely used for vision and text. The authors acknowledge this limitation but leave those areas for future work.Overall, the paper's core contribution of the neural net ≈ polynomial regression view provides a novel perspective. Demonstrating this principle's implications through multicollinearity and extensive experiments adds to the paper's strengths. The paper is clearly situated within the broader debate on neural nets vs simpler models. Focusing only on feedforward nets is a limitation, but the authors acknowledge that caveat.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:- Further testing and implementing the problems and remedies for polynomial regression discussed in Section 8, such as dimension reduction techniques like PCA and dropout, ridge regression, parallel computation, etc. - Further investigating the phenomenon of multicollinearity in neural networks.- More experimentation with large-p datasets.- Integrating the polynomial regression approach with preprocessing techniques commonly used with neural networks for images, text, etc. (e.g. convolutional neural nets).- Exploring whether polynomial regression could be competitive with other machine learning techniques like random forests, SVMs, etc. The paper focused on neural nets but mentions this could be an area to investigate.- Making the approach more mathematically rigorous, like formally proving the arguments made about the activation function implying a correspondence between neural nets and polynomial regression.- Adapting the ideas to specialized neural network architectures like convolutional and recurrent nets. The paper views these as largely separate issues involving preprocessing.- More investigation into the overfitting of neural networks, causes and ways to avoid it.In summary, the main suggested future work seems to focus on further theoretical development, testing the ideas on more complex data, integrating with neural net preprocessing pipelines, comparing to other ML methods, and gaining more mathematical rigor. Expanding the scope to specialized neural net architectures is also mentioned.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper presents a simple analytic argument that neural networks (NNs) are essentially polynomial regression (PR) models, with the degree of the polynomial growing with each hidden layer. This helps explain properties of NNs, like convergence issues arising from multicollinearity. It also suggests using PR instead of NNs, avoiding issues like selecting hyperparameters. Empirical tests on various datasets confirm PR matches or exceeds NNs in accuracy. The paper proposes an open-source PR package polyreg as an effective alternative to NNs. Overall, the paper argues NNs are fundamentally PR models, with implications for understanding and improving NNs, and that directly using PR can avoid downsides of NNs.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper argues that neural networks (NNs) can be viewed as a form of polynomial regression (PR). The authors present a simple mathematical argument showing that the activation function in NNs results in the network mimicking polynomial regression, with the degree of the polynomial increasing with each hidden layer. This perspective provides new insights into properties of NNs. For example, it predicts and confirms that NNs suffer from multicollinearity in later layers, which can cause convergence problems. Most importantly, viewing NNs as a form of PR suggests that standard polynomial regression models could be used instead of NNs in many applications. This avoids issues like choosing hyperparameters and convergence problems in NNs. The authors test this idea empirically on various datasets, finding that polynomial regression performs as good as or better than NNs in all cases. They also develop an open source software package to facilitate use of polynomial models as an alternative to NNs.In summary, the key contributions of the paper are: 1) Showing mathematically that NNs essentially perform polynomial regression, with the polynomial degree increasing with more layers; 2) Using this perspective to gain new insights into properties and problems with NNs; 3) Suggesting that standard polynomial regression models could replace NNs in many applications, avoiding issues with NNs; 4) Empirically demonstrating that polynomial regression matches or outperforms NNs on various datasets; and 5) Developing open source software to enable use of polynomial models as an alternative to NNs. The paper provides a new way to view and understand NNs through the lens of polynomial regression, and suggests this simple technique could replace complex NNs in many practical applications.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper presents a simple analytic argument showing that neural networks (NNs) can be viewed as a form of polynomial regression (PR). Specifically, the authors argue that due to properties of the activation function, the outputs at each layer of a neural network mimic polynomial regression, with the degree of the polynomial increasing with each successive layer. This NN ≈ PR principle allows insights into properties of NNs, such as lurking multicollinearity issues that can lead to convergence problems. It also suggests using polynomial regression as an alternative to NNs, avoiding issues like choosing hyperparameters. The authors test this on several datasets, finding polynomial regression matches or exceeds the performance of NNs. Overall, the paper argues NNs are essentially polynomial regression models, with implications for understanding and improving NNs or replacing them with polynomial regression.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- Despite the success of neural networks (NNs), especially deep learning, there is still concern over their "black box" nature and lack of interpretability. The paper aims to provide new insights into how and why NNs work.- The authors present an argument that NNs are essentially equivalent to polynomial regression (PR) models, with the degree of the polynomial increasing with each hidden layer. This helps explain properties of NNs.- Viewing NNs as PR suggests that one could just use PR instead of NNs in many cases, avoiding issues like choosing hyperparameters. The authors show PR matches or exceeds NNs on various datasets.  - The connection to PR is used to predict and confirm lurking multicollinearity issues in NNs, which can cause convergence problems. This suggests ways to improve NNs.- The authors developed an open-source R package, polyreg, to implement polynomial regression as an alternative to NNs.In summary, the key focus is gaining new insights into NNs by viewing them through the lens of polynomial regression, and using this perspective to improve NNs or replace them entirely with PR in some applications.
