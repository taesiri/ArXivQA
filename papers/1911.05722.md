# [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that good visual representations can be learned in an unsupervised manner by building large and consistent dictionaries using contrastive losses. Specifically, the authors hypothesize that:1) Larger dictionaries that cover a rich set of negative samples are desirable for contrastive learning, as they may better sample the underlying continuous visual space.2) The keys in the dictionary should be encoded in a consistent manner despite the encoder's evolution during training. This consistency facilitates meaningful comparisons between queries and keys.To test these hypotheses, the paper introduces Momentum Contrast (MoCo), which maintains a dictionary as a queue of data samples that is decoupled from minibatch size, allowing a large dictionary. It also uses a momentum-based moving average of the query encoder to update the key encoder, keeping keys consistent.The main result is that MoCo can match or exceed the performance of supervised pre-training on ImageNet for representation learning. When transferred to downstream tasks, MoCo can outperform its supervised pretraining counterpart on 7 detection/segmentation tasks, suggesting the gap between unsupervised and supervised representation learning has been largely closed.In summary, the central hypothesis is that large, consistent dictionaries produced by MoCo can enable competitive unsupervised visual representation learning compared to supervised pre-training. The results provide strong support for this hypothesis.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question is: Can unsupervised visual representation learning approaches based on contrastive learning match or exceed the performance of supervised pre-training on downstream tasks?The key hypotheses appear to be:1) Building a large and consistent dictionary of visual representations is important for effective contrastive unsupervised learning. 2) A momentum update mechanism can help maintain encoder consistency during contrastive dictionary construction.3) The proposed Momentum Contrast (MoCo) method can build better visual representations than previous contrastive approaches.4) Unsupervised pre-training with MoCo can match or exceed the transfer performance of supervised ImageNet pre-training on various downstream tasks like detection and segmentation.So in summary, the paper hypothesizes that the proposed MoCo approach for contrastive unsupervised learning can close the gap with supervised pre-training for transfer learning in computer vision. The experiments aim to validate whether MoCo representations transfer better than previous unsupervised and supervised counterparts.
