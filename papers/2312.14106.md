# [Learning Human-like Representations to Enable Learning Human Values](https://arxiv.org/abs/2312.14106)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Ensuring AI systems learn solutions that conform to human values and societal norms (value alignment) is very challenging. 
- Prior work has shown that learning human-like representations of the world (representational alignment) improves model generalization and robustness. 
- This paper hypothesizes that representational alignment is a necessary prerequisite for value alignment.

Methods:
- Agents are trained in a contextual multi-armed bandit setting with 100 actions. 
- Reward distributions reflect "morality scores" indicating how ethical each action is.
- A perfect representationally-aligned agent is provided a similarity matrix between actions based on the morality scores. 
- Representational alignment is then deteriorated by corrupting up to 80 of the morality scores.
- Two agents are tested: support vector regression and kernel regression models.
- Five metrics are computed: mean reward, time to convergence, number of non-optimal, unique and immoral actions.

Results: 
- As representation alignment decreases, mean reward decreases and all other metrics get worse.
- These relationships are statistically significant for both models.
- There is evidence of a bound between representational alignment and performance on the value alignment task.

Conclusions:
- Representationally aligned agents perform better on an ethical reinforcement learning task than less aligned agents.
- This provides evidence that representational alignment enables value alignment. 
- Formalizing the mathematical relationship between representational and value alignment is left to future work.
