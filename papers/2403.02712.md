# [Breeze-7B Technical Report](https://arxiv.org/abs/2403.02712)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a need for open-source large language models (LLMs) specialized for Traditional Chinese language tasks. Existing models optimized for English often degrade in performance on non-English text. 

Proposed Solution:
- The authors introduce Breeze-7B, an LLM based on Mistral-7B, enhanced for Traditional Chinese comprehension and conversation abilities.

Key Contributions:
- Extended the tokenizer with additional Chinese-specific tokens to improve compression rate and speed. 
- Further pretrained Breeze-7B on 650GB of high-quality Traditional Chinese data.
- Finetuned the model on instructional data to improve question answering abilities while retaining capabilities of base model.
- Evaluated on comprehension benchmarks (TMMLU+, DRCD, etc.) where Breeze-7B performs very well compared to other open-sourced 7B models.
- Evaluated on chat benchmarks (MT-Bench variants) where Breeze-7B is competitive, with room for improvement on STEM and writing tasks.
- Released weights for Breeze-7B base and instruct variants to enable community building.

In summary, the authors introduced Breeze-7B, an open-source 7B parameter LLM specialized for Traditional Chinese. Through additional pretraining and finetuning, it demonstrates strong language understanding and conversation abilities on several benchmarks. The model represents progress towards capable yet transparent Chinese chatbots and question answering systems.


## Summarize the paper in one sentence.

 This paper introduces Breeze-7B, an open-source Traditional Chinese language model developed by MediaTek Research, which demonstrates strong performance on language comprehension and conversational tasks compared to models of similar size.


## What is the main contribution of this paper?

 This paper introduces Breeze-7B, an open-source large language model for Traditional Chinese. The main contributions are:

1) Breeze-7B is an enhanced version of Mistral-7B, with additional pretraining on 650GB of high-quality Traditional Chinese data and finetuning on instruction datasets to improve language comprehension and conversatial abilities. 

2) The model architecture is customized with an extended tokenizer to better handle Traditional Chinese text. This allows 2x faster training and inference speeds.

3) Breeze-7B models demonstrate strong performance on Traditional Chinese benchmarks compared to other open-sourced models of similar size. The Base model retains knowledge well while the Instruct model shows good language understanding and chatting abilities.

4) Breeze-7B helps close the gap between proprietary and open-source models for Traditional Chinese. The weights are open-sourced to enable further research.

In summary, the main contribution is introducing an open-source Traditional Chinese language model tailored for comprehension and conversation that achieves state-of-the-art results among models of comparable complexity.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Breeze-7B - The name of the language model introduced in the paper.

- Language model - The paper discusses enhancements made to the Breeze-7B language model to improve its performance on Traditional Chinese tasks.

- Traditional Chinese - A key focus of the model improvements is enhancing performance on Traditional Chinese language understanding and chatbot tasks. 

- Pretraining - The paper discusses additional pretraining of the base Mistral-7B model on 650GB of Traditional Chinese data.

- Finetuning - The paper talks about further finetuning the pretrained model on instructional data to improve chatbot capabilities. 

- Benchmarks - Various benchmarks are used to evaluate the Breeze-7B model's language comprehension and chatbot abilities, including TMMLU+, DRCD, MT-Bench.

- Model comparisons - The performance of Breeze-7B is compared to other language models like Yi-6B, Qwen1.5-7B, Taiwan-LLM-13B-v2.0.

Some other potential keywords: tokenizer, perplexity, instruction following, multi-turn dialogue.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using 650GB of high quality Traditional Chinese data for additional pretraining. Can you elaborate on the criteria used to determine "high quality" and provide more details on the sources and composition of this dataset? 

2. What modifications were made to the tokenizer to improve compression rate on Chinese text? How significant were the improvements in compression rate and training/inference speed?

3. The paper states that textbook-like data was used for in-training evaluation. What specific metrics or analyses were performed on this dataset to select good checkpoints during pretraining?

4. What supervised finetuning techniques were used (objective functions, optimizers etc.) to specialize the base Breeze-7B model into the Instruct variant? Were different techniques used for different downstream tasks?

5. For the chat-oriented benchmarks, what guidelines or criteria were provided to the GPT-4 judge model to score the responses? Were any steps taken to verify the judge model was unbiased?

6. The results show weaker performance on STEM and Writing subtasks in MT-Bench-tw. What hypotheses do the authors have for why performance lagged in these areas?

7. How much compute was used for the additional pretraining and finetuning stages? What hardware was leveraged?

8. The paper mentions open sourcing only the Base and Instruct model weights. What considerations determined which variants would be released?

9. How do the authors envision Breeze-7B will be utilized by the larger NLP research community? What new research avenues does it enable?  

10. Were techniques like sparse expert parameterization or mixture-of-experts training explored to improve sample efficiency for Breeze-7B? If not, why?
