# [Fine-tuned CLIP Models are Efficient Video Learners](https://arxiv.org/abs/2212.03640)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that a simple video fine-tuning of the CLIP model (ViFi-CLIP) can be effective for adapting image-pretrained CLIP to video tasks, without needing complex additional modules or components. The key questions explored are:

1) Does fine-tuning CLIP on videos allow it to learn suitable video-specific inductive biases, without tampering its generalization capability? 

2) Can this simple fine-tuning approach perform competitively compared to more complex state-of-the-art methods for adapting CLIP to videos?

3) How does the simple fine-tuning allow CLIP to capture temporal information in videos?

4) In low-data regimes where fine-tuning is not feasible, can a 'bridge and prompt' approach first tune CLIP on videos and then adapt it to downstream tasks via prompting?

The central hypothesis is that the inductive biases of CLIP can be effectively transferred to videos via simple fine-tuning, without needing complex additional components like temporal modeling modules. The key questions aim to validate this hypothesis across different settings like zero-shot, few-shot, etc. and analyze how the fine-tuning provides video-specific adaptations.
