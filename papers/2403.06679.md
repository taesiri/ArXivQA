# [Answering Diverse Questions via Text Attached with Key Audio-Visual   Clues](https://arxiv.org/abs/2403.06679)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Audio-visual question answering (AVQA) requires understanding video content and audio to answer diverse questions about the video. However, fusing deep audio-visual features can reduce model generalization capability for multiple question-answer pairs in one video.  

- The heterogeneity between audiovisuals and text makes perfect fusion challenging. Using global audio-visual semantics can weaken adaptability to diverse question types.

Proposed Solution:
- Propose a Mutual Correlation Distillation (MCD) framework to aid question inference without needing global audio-visual features in decision fusion.

- Use residual connections to enhance audio-visual association via self-attention blocks. Capture key local audio-visual features as "clues" to guide question comprehension.

- Align audio-visual-text triplets in a shared latent space via contrastive knowledge distillation to narrow semantic gaps. Decouple audio-visual dependencies by removing decision-level fusion.

Main Contributions:

1) Novel human-like reasoning approach tailored for diverse question-answering without full audio-visual fusion. More applicable for multi-question scenarios.  

2) Mutual correlation module that hierarchically guides questions using key coordinated audio-visual clues, preventing overloading of parameters.

3) Soft association mechanism using residuals to enhance cross-modal transmission while retaining information.  

4) State-of-the-art performance on Music-AVQA and AVQA datasets. Extensive experiments demonstrate generalization strength to various backbones.

5) Qualitative results highlight reasoning effectiveness for multi-question scenarios.
