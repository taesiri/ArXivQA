# [Learning Unified Decompositional and Compositional NeRF for Editable   Novel View Synthesis](https://arxiv.org/abs/2308.02840)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the main research question this paper addresses is how to perform joint scene novel view synthesis and editing based on implicit neural scene representations. The key challenges are:- Most prior works on implicit neural scene representations (like NeRF) focus only on novel view synthesis for the entire scene, but lack the ability to represent and edit individual objects within the scene.- Some recent works have started exploring object-level scene representations, but they typically build separate networks for view synthesis and editing. This limits modeling interactions and correlations between these two tasks, which is critical for learning high-quality scene representations.To address these challenges, the main hypothesis of this work is:By proposing a unified neural radiance field framework to effectively perform joint scene decomposition and composition, it can achieve both high-quality novel view synthesis and enable scene editing capabilities in an end-to-end manner.The key ideas are:- Learn to decompose the scene into object-level and background radiance fields to enable editing.- Learn to compose them back into an entire scene radiance field for novel view synthesis.- Unify decomposition and composition in a joint framework to model interactions between global scene and local object representations.In summary, the main novelty is in the joint decomposition-composition design within a unified NeRF framework to achieve the dual goals of high-quality novel view synthesis and scene editing.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a unified Neural Radiance Field (NeRF) framework for joint scene decomposition and composition. This allows performing both novel view synthesis and editing in a unified pipeline.2. It introduces two novel strategies for robust scene decomposition - 3D one-hot radiance regularization and 2D in-painting pseudo supervision. These help improve the rendering and editing quality significantly.3. Extensive experiments demonstrate the effectiveness of the proposed approach. It outperforms state-of-the-art object-compositional methods on both novel-view synthesis and editing tasks.In summary, the key contribution is the novel unified framework that can effectively model global and local implicit representations for high-quality scene modeling. The decomposition allows object editing while the composition enables novel view synthesis. The paper also proposes novel techniques like the 3D one-hot regularization and 2D pseudo supervision to improve the decomposition. Experiments validate the superiority over existing methods on scene rendering and editing.
