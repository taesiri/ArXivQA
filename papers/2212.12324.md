# [Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized   Photography](https://arxiv.org/abs/2212.12324)

## What is the central research question or hypothesis that this paper addresses?

The key research questions and contributions of this paper are:1. Is it possible to estimate high-quality object depth and camera motion directly from a long-burst image stack captured with natural hand tremor on a mobile device?2. Can this be done in an end-to-end unsupervised manner without requiring pose initialization or depth supervision? 3. Can a neural RGB-D model be fit to long-burst data to jointly distill depth and pose through photometric consistency, rather than relying on separate feature extraction and matching steps?The central hypothesis is that the small parallax motion from hand tremor in a long (2 second) burst contains enough multi-view information to recover geometrically accurate scene depth, if modeled appropriately. To test this, the authors propose an end-to-end approach to simultaneously estimate object depth and camera motion by fitting a continuous neural scene representation to the long-burst data. Their key contributions are:1) A neural RGB-D model with implicit image and depth networks, trained via photometric consistency to long-burst frames.2) A depth-on-a-plane decomposition to model foreground objects and background regions without parallax cues. 3) Demonstrating state-of-the-art depth maps from only a single long-burst capture, validated on complex geometries.In summary, they show high-quality depth reconstruction is possible from long-burst data alone, in an end-to-end framework requiring no hardware beyond a standard mobile camera.


## What is the main contribution of this paper?

The main contribution of this paper is developing an unsupervised end-to-end approach to jointly estimate high-quality object depth and camera motion from unstabilized long-burst photography using only RAW image frames and gyroscope data. The key points are:- They propose an end-to-end neural RGB-D scene fitting approach that distills depth and camera pose estimates directly from long-burst data by minimizing photometric loss through explicit 3D projection. This avoids the need for separate feature extraction, matching, and pose estimation steps.- They introduce a depth-on-a-plane decomposition, with a learned planar component representing the background and an implicit MLP offset representing foreground object depth. This helps avoid spurious depth solutions. - They leverage the multiresolution hash encoding to perform coarse-to-fine reconstruction, first estimating smooth depth variations before refining details.- They demonstrate high quality depth reconstruction on real data that outperforms existing learned, multi-view, and mixed methods. Comparisons to structured light scans validate the accuracy of reconstructed geometries.- Their approach requires only a long-burst image stack and gyroscope data, with no need for specialized hardware, ground truth poses, or depth initialization. It can operate even without RAW or gyroscope data.In summary, they show that an end-to-end trained neural volumetric scene representation can recover accurate object depth and camera motion from micro-baseline long-burst photography captured on commodity mobile devices.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an unsupervised neural approach to jointly estimate high-quality object depth and camera motion from RAW long-burst image sequences captured on a smartphone, demonstrating improved performance over existing learned and multi-view depth estimation methods.


## How does this paper compare to other research in the same field?

This paper presents an unsupervised end-to-end approach for estimating high-quality object depth and camera motion from unstabilized long-burst mobile photography. Here are some key ways it compares to other research:- It diverges from previous small motion works like Yu et al. and Im et al. that rely on feature extraction and matching or additional hardware. Instead, it proposes a single differentiable model trained end-to-end to distill depth from fitting to long-burst data.- Compared to other multi-view stereo methods like BARF and DfUSMC that also estimate depth and motion directly from image stacks, this method outperforms them quantitatively and qualitatively on tests.- Unlike recent neural radiance approaches that require pre-processed camera poses, this jointly estimates relative depth and pose with no pose initialization. It is most similar to Chugunov et al.'s approach but doesn't require a depth sensor or pose estimates.- It demonstrates higher fidelity object reconstruction than state-of-the-art monocular methods like MiDaS by leveraging parallax from long-burst data. Comparisons to structured light scans validate the geometric accuracy.- The depth on a plane decomposition produces better segmentations than learned priors and helps avoid artifacts in textureless regions that challenge other multi-view methods.- It shows robustness to various input settings, gracefully handling RGB vs RAW input and gyroscope data removal. This demonstrates the flexibility of the approach.In summary, this work pushes the boundary of what can be accomplished with readily available mobile photography data. By training an end-to-end model on long-burst captures, it achieves compelling depth reconstruction without specialized hardware or separate pose estimation steps. The comparisons validate it as a state-of-the-art approach in this setting.


## What future research directions do the authors suggest?

The authors suggest some future research directions at the end of the paper:\textbf{Forward Models:} Their static single-plane RGB-D representation could be extended to include differentiable models of object motion, deformation, or occlusion. \textbf{Image Refinement:} They currently use the learned image $I(u,v)$ to optimize depth, but it may be possible to instead use the learned depth $D(u,v)$ to refine the image (e.g. for denoising or deblurring).\textbf{From Pixels to Features:} Low-texture or distant regions lack parallax cues for their ray-based depth estimation. Learned local feature embeddings could help aggregate spatial information in these areas.The key suggestions are to extend the RGB-D representation to handle more complex scene dynamics, explore using the depth to refine the image instead of vice versa, and incorporate learned features to help with depth estimation in challenging areas like distant or textureless regions. The overall theme is enhancing the neural RGB-D model to handle more complex real-world imaging conditions.
