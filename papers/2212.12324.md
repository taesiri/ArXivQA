# [Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized   Photography](https://arxiv.org/abs/2212.12324)

## What is the central research question or hypothesis that this paper addresses?

The key research questions and contributions of this paper are:1. Is it possible to estimate high-quality object depth and camera motion directly from a long-burst image stack captured with natural hand tremor on a mobile device?2. Can this be done in an end-to-end unsupervised manner without requiring pose initialization or depth supervision? 3. Can a neural RGB-D model be fit to long-burst data to jointly distill depth and pose through photometric consistency, rather than relying on separate feature extraction and matching steps?The central hypothesis is that the small parallax motion from hand tremor in a long (2 second) burst contains enough multi-view information to recover geometrically accurate scene depth, if modeled appropriately. To test this, the authors propose an end-to-end approach to simultaneously estimate object depth and camera motion by fitting a continuous neural scene representation to the long-burst data. Their key contributions are:1) A neural RGB-D model with implicit image and depth networks, trained via photometric consistency to long-burst frames.2) A depth-on-a-plane decomposition to model foreground objects and background regions without parallax cues. 3) Demonstrating state-of-the-art depth maps from only a single long-burst capture, validated on complex geometries.In summary, they show high-quality depth reconstruction is possible from long-burst data alone, in an end-to-end framework requiring no hardware beyond a standard mobile camera.


## What is the main contribution of this paper?

The main contribution of this paper is developing an unsupervised end-to-end approach to jointly estimate high-quality object depth and camera motion from unstabilized long-burst photography using only RAW image frames and gyroscope data. The key points are:- They propose an end-to-end neural RGB-D scene fitting approach that distills depth and camera pose estimates directly from long-burst data by minimizing photometric loss through explicit 3D projection. This avoids the need for separate feature extraction, matching, and pose estimation steps.- They introduce a depth-on-a-plane decomposition, with a learned planar component representing the background and an implicit MLP offset representing foreground object depth. This helps avoid spurious depth solutions. - They leverage the multiresolution hash encoding to perform coarse-to-fine reconstruction, first estimating smooth depth variations before refining details.- They demonstrate high quality depth reconstruction on real data that outperforms existing learned, multi-view, and mixed methods. Comparisons to structured light scans validate the accuracy of reconstructed geometries.- Their approach requires only a long-burst image stack and gyroscope data, with no need for specialized hardware, ground truth poses, or depth initialization. It can operate even without RAW or gyroscope data.In summary, they show that an end-to-end trained neural volumetric scene representation can recover accurate object depth and camera motion from micro-baseline long-burst photography captured on commodity mobile devices.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an unsupervised neural approach to jointly estimate high-quality object depth and camera motion from RAW long-burst image sequences captured on a smartphone, demonstrating improved performance over existing learned and multi-view depth estimation methods.
