# [Transformers for Low-Resource Languages:Is FÃ©idir Linn!](https://arxiv.org/abs/2403.01985)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural machine translation (NMT) models often underperform on low-resource language pairs due to insufficient training data. 
- Little research has been done on using Transformer NMT models, the current state-of-the-art, for English-Irish translation which is a low-resource language pair. 
- There are no clear recommendations on optimal choice of subword model types and vocab sizes for English-Irish NMT.

Proposed Solution:
- Perform hyperparameter optimization of Transformer models for English-Irish translation using two parallel corpora - a 55k general domain DGT corpus and an 88k in-domain public admin corpus.
- Evaluate impact of different subword models (BPE and unigram) and vocabulary sizes (4k to 32k).
- Tune other hyperparameters like number of layers, heads, regularization techniques.  
- Compare performance against baseline RNN models and Google Translate.

Main Contributions:
- Demonstrated importance of subword model choice for low-resource NMT, with 16k BPE model giving best results.  
- Showed optimized Transformer models substantially outperform RNN models (e.g. 7.1 BLEU point gain on DGT) and Google Translate.
- Reduced model perplexity and post-editing effort.
- Provided hyperparameters optimized for English-Irish transformers on two datasets.
- Showed transformers can achieve high performance on low-resource language pairs like English-Irish.

In summary, the paper shows transformers optimized using subword models can achieve state-of-the-art English-Irish NMT, addressing the low-resource language challenge. Key is choosing appropriate subword model and hyperparameters.
