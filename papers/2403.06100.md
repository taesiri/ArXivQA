# [Automatic design optimization of preference-based subjective evaluation   with online learning in crowdsourcing environment](https://arxiv.org/abs/2403.06100)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Preference-based subjective evaluation is a key method to evaluate generative media reliably. However, the huge number of pair combinations required makes it infeasible for large-scale evaluations using crowdsourcing due to the high cost. 

Proposed Solution:
The authors propose an automatic optimization method for preference-based subjective evaluation to determine the optimal pair combinations to evaluate and the allocation of evaluation volume per pair. They use an online learning approach called MERGE-RANK that can identify the total order of evaluation targets with minimum sample volumes to achieve a specified accuracy. 

To enable the use of this online learning method in a crowdsourcing setting, the authors modify MERGE-RANK to support parallel and asynchronous execution under a fixed budget. This includes a balancing mechanism for evaluation allocation to mitigate issues caused by the asynchronous nature.

Contributions:

- Optimization of pair selection and evaluation volume allocation for preference-based tests using online learning to maximize accuracy under a limited budget
- Modification of MERGE-RANK to enable its use in crowdsourcing by supporting parallel/async execution and fixed budget
- Experiment reducing 351 pair combinations to 83 while avoiding wasted budget and maintaining accuracy 
- Achieved more statistically significant quality differences than MOS test, avoiding contraction bias
- Proposed method enables large-scale preference-based evaluation at a realistic cost using crowdsourcing

In summary, the authors propose a way to optimize preference-based subjective evaluation for use in crowdsourcing, enabling more reliable large-scale assessments of generative media quality at feasible costs. Their method automatically determines the best pair combinations and evaluation volumes per pair.
