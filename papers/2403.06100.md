# [Automatic design optimization of preference-based subjective evaluation   with online learning in crowdsourcing environment](https://arxiv.org/abs/2403.06100)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Preference-based subjective evaluation is a key method to evaluate generative media reliably. However, the huge number of pair combinations required makes it infeasible for large-scale evaluations using crowdsourcing due to the high cost. 

Proposed Solution:
The authors propose an automatic optimization method for preference-based subjective evaluation to determine the optimal pair combinations to evaluate and the allocation of evaluation volume per pair. They use an online learning approach called MERGE-RANK that can identify the total order of evaluation targets with minimum sample volumes to achieve a specified accuracy. 

To enable the use of this online learning method in a crowdsourcing setting, the authors modify MERGE-RANK to support parallel and asynchronous execution under a fixed budget. This includes a balancing mechanism for evaluation allocation to mitigate issues caused by the asynchronous nature.

Contributions:

- Optimization of pair selection and evaluation volume allocation for preference-based tests using online learning to maximize accuracy under a limited budget
- Modification of MERGE-RANK to enable its use in crowdsourcing by supporting parallel/async execution and fixed budget
- Experiment reducing 351 pair combinations to 83 while avoiding wasted budget and maintaining accuracy 
- Achieved more statistically significant quality differences than MOS test, avoiding contraction bias
- Proposed method enables large-scale preference-based evaluation at a realistic cost using crowdsourcing

In summary, the authors propose a way to optimize preference-based subjective evaluation for use in crowdsourcing, enabling more reliable large-scale assessments of generative media quality at feasible costs. Their method automatically determines the best pair combinations and evaluation volumes per pair.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes an algorithm to optimize preference-based subjective evaluation of synthetic speech using online learning for automatic pair selection and evaluation volume allocation in a crowdsourcing environment.


## What is the main contribution of this paper?

 The main contributions of this paper are summarized as follows:

1. The authors propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment. 

2. They propose methods to stabilize online learning to support asynchronous and parallel execution, enabling application to large-scale evaluation using crowdsourcing.

3. Their experiment on preference-based subjective evaluation of synthetic speech reduces 351 total combinations to 83 pairs to determine the total order of quality of speech synthesis systems. 

4. Their method achieved more pairs with statistically significant quality differences than MOS evaluation by avoiding the contraction bias observed in MOS evaluation.

In summary, the key contributions are developing an optimization method for preference-based subjective evaluation using online learning suitable for crowdsourcing, and demonstrating its effectiveness in an experiment on evaluating synthetic speech systems. The method allows reducing the number of evaluations needed while preserving accuracy.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it include:

- Subjective evaluation
- Preference-based tests
- Online learning
- Crowdsourcing
- Text-to-speech synthesis
- Merge-rank algorithm
- Sorting algorithm
- Total order identification
- Evaluation accuracy
- Evaluation volume allocation 
- Parallel execution
- Asynchronous execution
- Fixed budget settings
- Synthetic speech

These keywords relate to the main topics and contributions of the paper, which focus on optimizing preference-based subjective evaluation tests using an online learning algorithm that can work effectively in a crowdsourcing environment. The Merge-Rank algorithm is adapted to support parallel and asynchronous execution under a fixed budget. Experiments involve subjective evaluation of synthetic speech. So the key terms cover the methodologies used as well as the application area.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1) How does the modified MERGE-RANK algorithm support parallel and asynchronous execution to enable its application in a crowdsourcing environment? What specific modifications were made compared to the original MERGE-RANK algorithm?

2) Explain the balancing evaluation allocation mechanism introduced in the modified MERGE-RANK algorithm to mitigate the issues of unbalanced and suboptimal evaluation allocation caused by asynchronous execution. How does managing the requested evaluation count help with this?

3) What statistical test and confidence interval calculations were used to analyze the final results of the preference test? How do these differ from the statistical tests used internally in the MERGE-RANK algorithm and why?

4) In the experiment, one pair (T02:B01) showed winner reversal after early termination of the evaluation. Analyze this situation - why did it happen and how can the algorithm be made more robust to avoid such cases in the future? 

5) Compare and contrast the contraction bias observed in MOS evaluations versus in the preference-based evaluations conducted in the experiment. What enabled the preference test to avoid this bias?

6) While 61 pairs showed statistical significance in the preference test, only 36 pairs did so at the convergence point of 15,248 evaluations. Compare this cost-effectiveness to that achieved in the MOS evaluations. How can the efficiency of the algorithm be further improved?

7) Only 83 out of 351 possible pairs were evaluated in the experiment. Analyze the tradeoffs between evaluation accuracy and number of pairs evaluated. How was the minimum number of pairs to determine sorting order selected?

8) Discuss the relationship observed between preference scores and MOS differences in the experiment. For pairs where the preferences disagreed with MOS differences, what may explain this? 

9) How were the tolerance bias ($\epsilon$) and confidence level ($\delta$) parameters configured in the experiment? Explain the considerations in selecting appropriate values for these.

10) The modified MERGE-RANK algorithm has two distinct phases - sorting before convergence and minimizing error bias after convergence. Analyze how the evaluation volumes and preference scores in Table 1 reflect these two phases. Pick some example pairs to illustrate this.
