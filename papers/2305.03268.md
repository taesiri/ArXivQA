# Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the factual correctness and end-task performance of large language model predictions by post-editing reasoning chains generated through chain-of-thought prompting? The key hypotheses appear to be:1) Chain-of-thought prompting improves performance on complex reasoning tasks, but can still generate factually incorrect rationales due to the auto-regressive nature of language model decoding. 2) By introducing external knowledge sources to verify and edit the rationales, the rationales can be made more factually correct.3) Providing the model with these edited, more factual rationales as refreshed memory will lead to improved end-task performance on question answering and fact verification tasks.In summary, the central research question is how to improve factual correctness and task performance of LLMs using chain-of-thought prompting by verifying and editing the rationale chains with external knowledge. The key hypothesis is that more factually correct rationales will improve end-task performance.


## What is the main contribution of this paper?

The main contribution of this paper is proposing the Verify-and-Edit framework for improving the factual correctness of predictions from large language models using chain-of-thought prompting. Specifically, the key contributions are:1. Proposing a method to post-edit chain-of-thought style rationales by generating verifying questions, retrieving external knowledge, and editing the rationales informed by the retrieved facts. 2. Demonstrating that the edited and more factual rationales lead to improved prediction accuracy on multiple open-domain question answering datasets including Adversarial HotpotQA, 2WikiMultihopQA, and Fever fact verification.3. Showing that combining chain-of-thought prompting with search engines like Google leads to significant accuracy improvements over baselines, providing a promising direction for combining strengths of LLMs and search.4. Analysis showing the Verify-and-Edit framework is able to correct factual mistakes in rationales and improve logical reasoning. A human evaluation also confirms improved factuality of the edited rationales.5. Overall, proposing a simple yet effective framework to increase factual correctness in LLMs' predictions by refreshing their memory using external knowledge sources. The conversational nature also provides interpretable thought processes.In summary, the key contribution is enhancing the prediction accuracy of chain-of-thought prompted LLMs by editing rationales to be more factual using external knowledge retrieval and re-reasoning.
