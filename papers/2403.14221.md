# [Improving the Robustness of Large Language Models via Consistency   Alignment](https://arxiv.org/abs/2403.14221)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) still lack robustness when responding to instructions, often generating inconsistent responses to semantically equivalent instructions. This is an inherent flaw that hinders their practical applications.
- There is a lack of quantitative analysis around the robustness of LLMs' response generation, as well as systemic solutions to improve it.

Proposed Solution: 
- A novel two-stage training framework consisting of instruction-augmented supervised fine-tuning (SFT) and consistency alignment training (CAT) to enhance the robustness of LLMs.

- SFT stage: Augment original instructions with paraphrases and use them to fine-tune the LLM, helping it generalize better on diverse verbalizations of the same instruction.

- CAT stage: Further train the LLM from SFT to differentiate between subtly different responses to the same instruction in terms of alignment with human expectations. This is done via self-supervised offline training using preference pairs inferred from self-rewards.

Main Contributions:
- Quantitative analysis of robustness of latest LLMs using formal consistency metrics.
- Proposed integrated training framework to directly optimize consistency without external resources.
- Instruction augmentation and consistency alignment are both shown to be effective, with their combination achieving new SOTA results on instruction-following.
- Demonstrated effectiveness across multiple major LLMs like Vicuna and LLaMA.

The key innovation is using self-supervision to align LLMs to human preferences on subtle aspects of consistency, without any external annotation or models. Both generalization and robustness are enhanced.
