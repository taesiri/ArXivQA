# [COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization](https://arxiv.org/abs/2403.07134)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Post-training quantization (PTQ) aims to compress pre-trained deep neural networks into low-precision versions (e.g. 4-bit) without significant accuracy loss. Despite being more efficient than quantization-aware training, existing PTQ methods tend to suffer substantial accuracy degradation especially for low-bit quantization of vision transformers. 

Proposed Solution:
This paper proposes COMQ, a coordinate-wise minimization algorithm for PTQ that sequentially minimizes the layer-wise squared error between the outputs of pre-trained and quantized models. Key ideas:

(1) Decompose quantized weights into scaling factors and integer bit codes. Treat them as coordinate variables to minimize error.

(2) In each iteration, update only one coordinate variable while fixing others. This involves only dot products and rounding, avoiding backpropagation.

(3) Update coordinates in a carefully designed greedy order for faster error reduction.

Main Contributions:  

(1) A backpropagation-free PTQ algorithm based on coordinate descent optimization. Computationally more efficient than existing methods.

(2) State-of-the-art PTQ performance, especially for 4-bit quantization of vision transformers (<1% accuracy drop) and CNNs (0.3% drop). 

(3) Achieves strong results even in extreme 2-bit setting unseen in prior arts. Demonstrates versatility across various neural architectures.

(4) Extensive ablation studies on factors like batch size, iteration number etc. Analyzes properties of the algorithm.

In summary, the paper develops an optimized coordinate descent approach to PTQ that achieves promising accuracy-efficiency trade-offs. It shows SOTA quantization capability for both CNNs and transformers.
