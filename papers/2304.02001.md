# [MonoHuman: Animatable Human Neural Field from Monocular Video](https://arxiv.org/abs/2304.02001)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we create an animatable digital avatar that can be controlled in free viewpoint and arbitrary novel poses from only a monocular video?The key challenges they identify in addressing this question are:1) How to learn an accurate and generalizable deformation field from limited monocular video data. Previous methods tend to overfit to observed poses in the training data. 2) How to render photorealistic novel views and poses using the deformed points from the canonical space. There is ambiguity in finding correspondence features to guide rendering with only monocular input.To address these challenges, the main technical contributions of this paper are:1) A Shared Bidirectional Deformation module that disentangles skeletal motion and non-rigid deformation to learn a more generalizable deformation field.2) A Forward Correspondence Search module that queries correspondence features from keyframes to guide the rendering network.3) Extensive experiments showing their method can synthesize high-fidelity view-consistent results on challenging novel poses compared to prior state-of-the-art.In summary, the central hypothesis is that by modeling deformation bidirectionally and using correspondence features, they can create an animatable avatar from monocular video that generalizes to novel views and poses. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes MonoHuman, a novel framework that can reconstruct and animate a photo-realistic human avatar from just monocular video input. This allows creating free-viewpoint animations of a person using only a single video camera. 2. It introduces a Shared Bidirectional Deformation module that disentangles skeletal motion and non-rigid deformation to achieve generalizable and consistent deformations between observation space and canonical space. This helps improve the avatar's ability to generalize to novel poses.3. It presents a Forward Correspondence Search module that queries appearance features from keyframes to guide the rendering. This improves synthesis quality and coherence, especially for invisible parts. 4. Extensive experiments demonstrate MonoHuman's ability to generate high-fidelity view-consistent results on challenging novel poses, outperforming prior state-of-the-art methods on tasks like novel view and pose synthesis.In summary, the main contribution is proposing a complete framework called MonoHuman that can create an animatable free-viewpoint avatar from just monocular video through novel modules for robust deformation and high-fidelity rendering even on out-of-distribution poses. The experiments validate its effectiveness over other methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method called MonoHuman that can create a free-viewpoint animatable avatar of a human performer from a single monocular video by modeling the body as a neural radiance field and introducing novel modules for deformation and correspondence feature search to enable realistic rendering of the avatar in novel views and poses.


## How does this paper compare to other research in the same field?

Here are some key comparisons between this paper and other related work on animating human avatars from monocular video:- Compared to NeuralBody and HumanNeRF, this paper introduces novel modules for deformation and correspondence feature search to achieve better generalization to novel views and poses. The shared bidirectional deformation disentangles rigid and non-rigid motion to improve consistency. The forward correspondence search leverages keyframe features to guide rendering. - Unlike methods that rely on off-the-shelf models like SMPL, this approach learns the deformation field directly from data without strong priors. This provides greater fidelity for details beyond the body model like clothing.- Compared to pose-dependent methods, defining the deformation in a canonical pose space provides greater generalization ability. The consistency loss further regularizes the deformation field.- Using a sparse set of keyframes provides some benefits of multi-view methods while only requiring monocular input. The learned correspondence allows replacing costly root finding in other works.- The experiments demonstrate state-of-the-art performance on novel view and pose synthesis compared to NeuralBody, HumanNeRF and NeuMan. The model also generalizes better on challenging unseen poses compared to HumanNeRF.- Limitations include reliance on estimated pose/segmentation, case-specific training, and difficulty modeling large variations from the training set. Future work could address pose/segmentation correction, generalization across people, and modeling more extreme motions.In summary, the key novelties are the shared bidirectional deformation, correspondence feature search from keyframes, and associated losses. Together these contributions allow animating high quality avatars from monocular video in a wide range of poses.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Improving the accuracy and generalization ability of the deformation field learning. The authors mention that the deformation field relies on the accuracy of the pose and segmentation mask annotations. They suggest exploring pose and mask correction/refinement during training could help improve results.- Enabling generalization to different people from monocular video. The current method is trained in a case-specific manner for each video. Studying how to create a model that can generalize to novel people from monocular video input is noted as an interesting direction.- Exploring unsupervised or weakly supervised training. The current method requires pose, segmentation mask, and camera parameter annotations. Reducing the supervision required could make the approach applicable to more monocular videos.- Scaling up the model and data. The authors use a relatively small dataset of 6 subjects. Training and evaluating on larger and more diverse video datasets could help improve results and generalization.- Modeling background and full scenes. Currently MonoHuman focuses on reconstructing the human subject. Extending the model to also represent background elements and full scenes is noted as an interesting direction.- Improving runtime efficiency. The current two-stage model has significant computational requirements. Improving the runtime speed could help enable more practical uses.In summary, the key suggestions are improving generalization, reducing supervision requirements, using more/richer data, extending the scope of modeling beyond just the human, and improving runtime efficiency. Overall the authors lay out several promising directions to build on this work and enable broader applications of animatable human modeling from monocular video.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:MonoHuman proposes a framework to reconstruct an animatable human avatar from monocular video input. They introduce a Shared Bidirectional Deformation module to learn a generalizable deformation field between observation space and canonical space by disentangling rigid skeletal motion and non-rigid deformation. This allows novel pose animation capability. They also propose a Forward Correspondence Search module to query appearance features from keyframes to guide high-fidelity rendering. Experiments show MonoHuman can synthesize realistic free-viewpoint video of the human performance in challenging unseen poses. The method only requires monocular video for supervision rather than controlled multi-view data. Key contributions are the Shared Bidirectional Deformation to enable generalizable pose animation and the Forward Correspondence Search to achieve realistic rendering using sparse appearance information. Comparisons to baselines like NeuralBody and HumanNeRF demonstrate superior performance on metrics like PSNR, SSIM, and LPIPS for novel view and pose synthesis.
