# [MonoHuman: Animatable Human Neural Field from Monocular Video](https://arxiv.org/abs/2304.02001)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we create an animatable digital avatar that can be controlled in free viewpoint and arbitrary novel poses from only a monocular video?The key challenges they identify in addressing this question are:1) How to learn an accurate and generalizable deformation field from limited monocular video data. Previous methods tend to overfit to observed poses in the training data. 2) How to render photorealistic novel views and poses using the deformed points from the canonical space. There is ambiguity in finding correspondence features to guide rendering with only monocular input.To address these challenges, the main technical contributions of this paper are:1) A Shared Bidirectional Deformation module that disentangles skeletal motion and non-rigid deformation to learn a more generalizable deformation field.2) A Forward Correspondence Search module that queries correspondence features from keyframes to guide the rendering network.3) Extensive experiments showing their method can synthesize high-fidelity view-consistent results on challenging novel poses compared to prior state-of-the-art.In summary, the central hypothesis is that by modeling deformation bidirectionally and using correspondence features, they can create an animatable avatar from monocular video that generalizes to novel views and poses. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes MonoHuman, a novel framework that can reconstruct and animate a photo-realistic human avatar from just monocular video input. This allows creating free-viewpoint animations of a person using only a single video camera. 2. It introduces a Shared Bidirectional Deformation module that disentangles skeletal motion and non-rigid deformation to achieve generalizable and consistent deformations between observation space and canonical space. This helps improve the avatar's ability to generalize to novel poses.3. It presents a Forward Correspondence Search module that queries appearance features from keyframes to guide the rendering. This improves synthesis quality and coherence, especially for invisible parts. 4. Extensive experiments demonstrate MonoHuman's ability to generate high-fidelity view-consistent results on challenging novel poses, outperforming prior state-of-the-art methods on tasks like novel view and pose synthesis.In summary, the main contribution is proposing a complete framework called MonoHuman that can create an animatable free-viewpoint avatar from just monocular video through novel modules for robust deformation and high-fidelity rendering even on out-of-distribution poses. The experiments validate its effectiveness over other methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method called MonoHuman that can create a free-viewpoint animatable avatar of a human performer from a single monocular video by modeling the body as a neural radiance field and introducing novel modules for deformation and correspondence feature search to enable realistic rendering of the avatar in novel views and poses.
