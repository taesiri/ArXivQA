# [Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large   Language Models](https://arxiv.org/abs/2312.01954)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a pipeline to enhance the performance of large language models (LLMs) on the task of triplet extraction (TE) from text in zero-shot and few-shot settings. The pipeline dynamically gathers contextual triplets or sentence-triplet pairs from a knowledge base (KB) that are relevant to the input sentence, and includes this contextual information in the LLM prompt. Experiments showed that adding just 5 contextual triplets substantially improved LLM performance on TE over using the prompt alone, making even smaller LLM models competitive with some classical NLP baselines. Further gains were achieved by instead providing 5 relevant sentence-triplet examples. There is evidence that TE performance correlates more strongly with the quality of the retrieved KB context than LLM scale. While still below state-of-the-art classical models, prompting LLMs with relevant KB context makes them much more capable on TE in low-resource scenarios compared to relying solely on the prompt. The paper recommends focusing efforts on improving KB quality and retrieval over pursuing ever-larger LLMs for advancing TE.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on the task of triplet extraction (TE) from text, where the goal is to extract (subject, predicate, object) triplets that represent the meaning of the text. Current state-of-the-art TE models rely on task-specific supervision, limiting their generalization capabilities. The paper investigates whether large language models (LLMs) can perform TE in zero-shot and few-shot settings, using knowledge base (KB) information to aid the models. 

Proposed Solution:
The authors propose a pipeline to perform TE using LLMs aided with KB context. For a given input text, relevant contextual triplets and (sentence, triplets) pairs are retrieved from a KB dynamically and provided to the LLM through prompting. The KB context gives hints about expected entities, relations, and patterns for TE. Various LLMs like GPT-2, Falcon, LLaMA, GPT-3.5 and GPT-4 are tested on WebNLG and NYT datasets.

Main Contributions:
- Show that LLMs struggle with TE in zero-shot setting but adding just 2-5 examples helps reach competitive scores. Additional KB triplets provide further improvements.
- Propose a pipeline to dynamically retrieve contextual KB information and provide it to LLM prompting for TE. KB sentence-triplet examples are most informative.  
- Demonstrate LLMs can match or exceed performance of some classical BiLSTM baselines with KB aid. But significant gap remains compared to state-of-the-art models.
- Analyze impact of retrieved KB context quality on TE performance. Results suggest investing in better KBs over bigger models may be more worthwhile. 
- Performance scales linearly with KB context quality but only logarithmically with LLM size, for the TE task.

In summary, the paper demonstrates the promise of combining large language models with knowledge base information to perform knowledge extraction tasks like triplet extraction in low resource scenarios.
