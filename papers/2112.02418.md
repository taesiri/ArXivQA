# [YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice   Conversion for everyone](https://arxiv.org/abs/2112.02418)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses seem to be:

- Can a multilingual approach enable zero-shot multi-speaker TTS and voice conversion in a target language using only a single speaker dataset from that language? 

- Can a model trained on multiple languages achieve state-of-the-art results in zero-shot multi-speaker TTS and voice conversion for English?

- Can a model pretrained on multiple speakers and languages be effectively adapted to new speakers with very little speech data (less than 1 minute)?

Specifically, the authors propose a multilingual TTS model called YourTTS that builds on prior work like VITS. Their key contributions seem to be:

1) Showing that their model can achieve promising zero-shot multi-speaker TTS and voice conversion in a new language after training on just a single speaker dataset in that language. 

2) Demonstrating state-of-the-art results on zero-shot multi-speaker TTS and voice conversion for English by training on speakers from multiple languages.

3) Introducing speaker adaptation techniques requiring less than 1 minute of speech that can effectively adapt their pretrained multilingual model to new voices.

Overall, their main research questions revolve around leveraging multilingual training data and transfer learning techniques to enable effective zero-shot multi-speaker synthesis and voice conversion, even for low-resource languages with limited data. The paper aims to show the potential of their proposed YourTTS model in this context.
