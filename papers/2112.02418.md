# [YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice   Conversion for everyone](https://arxiv.org/abs/2112.02418)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses seem to be:

- Can a multilingual approach enable zero-shot multi-speaker TTS and voice conversion in a target language using only a single speaker dataset from that language? 

- Can a model trained on multiple languages achieve state-of-the-art results in zero-shot multi-speaker TTS and voice conversion for English?

- Can a model pretrained on multiple speakers and languages be effectively adapted to new speakers with very little speech data (less than 1 minute)?

Specifically, the authors propose a multilingual TTS model called YourTTS that builds on prior work like VITS. Their key contributions seem to be:

1) Showing that their model can achieve promising zero-shot multi-speaker TTS and voice conversion in a new language after training on just a single speaker dataset in that language. 

2) Demonstrating state-of-the-art results on zero-shot multi-speaker TTS and voice conversion for English by training on speakers from multiple languages.

3) Introducing speaker adaptation techniques requiring less than 1 minute of speech that can effectively adapt their pretrained multilingual model to new voices.

Overall, their main research questions revolve around leveraging multilingual training data and transfer learning techniques to enable effective zero-shot multi-speaker synthesis and voice conversion, even for low-resource languages with limited data. The paper aims to show the potential of their proposed YourTTS model in this context.


## What is the main contribution of this paper?

 Unfortunately I cannot fully assess the main contribution of this paper without reading the full text. However, based on skimming the abstract and introduction, it seems the paper proposes a new text-to-speech (TTS) model called "YourTTS" with several novel ideas for zero-shot multi-speaker and multilingual training. Some of the key points I gathered are:

- YourTTS builds on the VITS TTS model and makes modifications for zero-shot multi-speaker and multilingual capabilities.

- It achieves state-of-the-art results on zero-shot multi-speaker TTS and voice conversion on the VCTK dataset.

- It shows promising results for zero-shot TTS and voice conversion in a target language using just a single speaker dataset in that language. 

- The model can be fine-tuned with less than 1 minute of speech from a new speaker to achieve good similarity and quality.

- The multilingual training approach allows for code-switching between languages while maintaining speaker identity.

So in summary, it seems the main contributions are advancing zero-shot multi-speaker and multilingual TTS, showing these approaches can work even with very limited target speaker/language data, and demonstrating state-of-the-art capabilities on an established dataset. But reading the full paper would provide more details on the specific techniques used and evaluations performed to support these high-level contributions.
