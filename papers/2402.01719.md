# [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can be unreliable if they produce inconsistent responses to semantically equivalent inputs. This is especially concerning for moral scenarios like the trolley problem which have no clear "correct" answer.  

- Prior work has limitations in measuring LLM consistency for moral scenarios. Metrics based on task accuracy don't apply well here. There is a need for better consistency metrics.

Proposed Solution:
- The paper proposes a new metric called "Semantic Graph Entropy (SGE)" to measure LLM consistency for moral scenarios. 

- The key ideas are: (1) Generate paraphrases of moral questions to create semantically equivalent inputs (2) Get LLM responses and "Rules of Thumb" explanations for each input (3) Build a semantic graph using embedddings of responses (4) Calculate graph entropy to quantify inconsistency

- Rules of Thumb explanations are used along with responses to better represent LLM's moral judgments.

- Experiments are done with 5 LLMs on 10K moral scenarios from the MIC dataset. Both answers and Rules of Thumb are evaluated.

Key Results:
- SGE aligns better with human judgments than prior consistency metrics like BLEU, ROUGE and BERTScore.

- Analysis reveals even state-of-the-art LLMs are quite inconsistent in their moral judgments.

- Surprisingly, Falcon-7B showed better consistency than the larger LLama2-13B, indicating training strategies impact consistency.

Main Contributions:
- A new unsupervised metric SGE to quantify LLM consistency for moral scenarios
- Empirical analysis showing inaccuracies in state-of-the-art LLMs' moral judgments
- Using Rules of Thumb to better represent and evaluate moral decision making
- Results to motivate further research into the causes and solutions for inconsistent LLMs


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new metric called Semantic Graph Entropy (SGE) to measure the moral consistency of large language models by leveraging semantic representations and information theory, and shows that even state-of-the-art models exhibit significant inconsistencies.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing Semantic Graph Entropy (SGE), a new metric to measure the consistency of large language models (LLMs) when responding to moral queries. Specifically:

- SGE is an information-theoretic metric that combines entropy and semantic embedding methods to quantify how consistent an LLM's responses are across paraphrased versions of the same moral question. 

- The paper shows empirically that SGE better correlates with human judgments of consistency compared to prior metrics like BLEU, ROUGE, and BERTScore. This demonstrates that SGE is a more reliable way to assess LLM consistency.

- The paper also generates "Rules of Thumb" explanations to represent an LLM's moral reasoning and shows this further improves SGE's correlation with humans.

- Analysis using SGE reveals that even state-of-the-art LLMs still demonstrate a high degree of inconsistency on moral questions, highlighting the need for better evaluation and improvement of models in this area.

In summary, the key contribution is the proposal of SGE to reliably quantify and expose the moral inconsistency issues in LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- Consistency 
- Moral scenarios
- Trolley problem
- Semantic graph entropy (SGE)
- Rules of thumb (RoTs) 
- Moral integrity corpus (MIC)
- Uncertainty
- Reliability
- Explanations
- Paraphrases
- Semantic representations
- Human evaluations

The paper proposes a new metric called "semantic graph entropy" (SGE) to measure the consistency of large language models on moral scenarios, using the trolley problem as an example. It generates paraphrases of questions in the Moral Integrity Corpus (MIC) dataset and gets LLMs to answer them. It then creates a semantic graph of the answers and uses graph entropy to quantify consistency. The paper also generates "rules of thumb" or explanations of the LLMs' choices to enhance the metric. Experiments show SGE correlates better with human judgments than prior metrics, and reveals inconsistencies in state-of-the-art LLMs, highlighting issues with reliability and uncertainty.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes Semantic Graph Entropy (SGE) as a new metric to measure moral inconsistency in large language models. How is SGE calculated? What are the key components and steps involved in computing SGE?

2. The authors generate semantic representations of the language model responses using SBERT DeBERTa sentence embeddings. What is the rationale behind choosing DeBERTa over other sentence embedding techniques? How are these embeddings used in constructing the semantic graph? 

3. Rules of Thumb (RoTs) are used in the paper to better represent and evaluate a model's moral judgments. How are these RoTs generated? What role do they play in enhancing the proposed SGE metric?

4. The paper finds that adding RoTs in place of answers in the semantic graph leads to better correlation with human evaluations. Why do you think this is the case? What additional insight do the RoTs provide over just the answers?

5. One interesting finding is that Falcon-7B shows higher consistency than the larger LLama2-13B model. What implications does this have on model architecture and training procedures for improving consistency?

6. How does the proposed SGE metric quantify uncertainty? What specific mathematical properties of graph entropy allow it to effectively measure inconsistency?

7. The authors use an SBERT DeBERTa model finetuned on NLI datasets to generate sentence embeddings. What is the intuition behind using NLI finetuning? How does it help capture semantic similarity?  

8. What are some limitations of the semantic graph approach? When might it fail to adequately measure moral inconsistency? How can the framework be made more robust?

9. The paper analyzes consistency on a subset of 10,000 questions from the MIC dataset. What factors need to be considered in sampling the dataset to ensure diverse and representative moral scenarios?

10. For real-world deployment, what additional experiments would be needed to thoroughly evaluate model consistency across different domains? How can the analysis be scaled?
