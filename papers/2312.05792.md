# [Take an Irregular Route: Enhance the Decoder of Time-Series Forecasting   Transformer](https://arxiv.org/abs/2312.05792)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Time series forecasting is critical for decision making in IoT systems with large amounts of sensor data. Two main paradigms exist - MLP-based and Transformer-based methods. 
- Most research focuses on improving the Transformer encoder for feature extraction from input sequences, while the decoder is neglected. However, both encoder and decoder are important for forecasting performance.
- Specific issues with Transformer decoders:
   1) Redundant self-attention on just the position embeddings. 
   2) Only the last encoder feature map feeds into the decoder, losing multi-scale information.
   3) Patch-wise attention in decoders fails to extract fine-grained features within each patch.

Proposed Solution: FPPformer
- A Transformer with improved encoder and decoder for time series forecasting.
- Decoder changes:
   1) Swaps self-attention and cross-attention blocks so prediction sequence first gets auto-regressive features before self-attention.
   2) Employs a top-down pyramidal hierarchy to construct predictions from coarse to fine resolution using multi-scale encoder features.  
- Attention changes:
   1) Adds element-wise self-attention before patch-wise attention in encoder, to extract fine-grained patch features.
   2) Replaces decoder self-attention with element-wise attention for finer prediction features.
- Also uses direct forecasting, channel/variable independence, RevIN normalization, and diagonal-masked self-attention to handle outliers.

Main Contributions:
1) Improves Transformer decoder design for time series forecasting.
2) Proposes top-down decoder hierarchy to construct predictions from coarse to fine features.
3) Combines element-wise and patch-wise attention in encoder and decoder.
4) Achieves state-of-the-art results on 12 benchmarks, outperforming current SOTA baselines.
5) Ablation studies validate decoder architecture and attention mechanisms.

In summary, FPPformer advances the Transformer architecture for time series forecasting by enhancing the decoder and attention modules in an innovative way, leading to excellent empirical performance.
