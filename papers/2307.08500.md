# [Cumulative Spatial Knowledge Distillation for Vision Transformers](https://arxiv.org/abs/2307.08500)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:- How can we effectively transfer spatial-wise knowledge from CNNs to vision transformers without needing to align intermediate features? - How can we leverage CNNs' local inductive bias to help vision transformers converge faster initially, while still allowing the transformers to reach their full global modeling potential later in training?The authors identify two key problems with distilling knowledge from CNNs to vision transformers:1) The difference in network architectures makes aligning intermediate features for spatial knowledge transfer inefficient. 2) Relying only on CNNs' local inductive bias can limit the vision transformers' convergence late in training, since the CNN teachers have lower capacity. To address these issues, the paper proposes Cumulative Spatial Knowledge Distillation (CSKD), which:- Transfers spatial knowledge to vision transformers by generating dense predictions from CNNs and using them to supervise the vision transformers' patch tokens. This avoids complex feature alignment.- Uses a Cumulative Knowledge Fusion module that emphasizes CNNs' local responses early in training and global responses later, allowing the vision transformer student to leverage different inductive biases over time.So in summary, the main hypotheses appear to be that CSKD can enable more efficient spatial knowledge transfer and improved convergence for vision transformer students compared to prior distillation approaches. The experiments aim to validate whether CSKD achieves these goals.


## What is the main contribution of this paper?

This paper proposes a method called Cumulative Spatial Knowledge Distillation (CSKD) for training Vision Transformers (ViTs) by distilling knowledge from convolutional neural networks (CNNs). The key contributions are:- It identifies two issues with distilling knowledge from CNNs to ViTs: (1) inefficient transfer of spatial knowledge due to misaligned features, and (2) limited convergence later in training due to different inductive biases.- It presents CSKD to address these issues by:(1) Transferring spatial knowledge to ViT patch tokens from corresponding CNN feature map locations, without aligning intermediate features. (2) Introducing a Cumulative Knowledge Fusion (CKF) module that emphasizes global CNN predictions later in training, allowing ViT to leverage its capabilities fully.- It demonstrates improved performance over baseline methods like DeiT and DearKD on ImageNet and other datasets. The visualizations and analysis provide insights into how CSKD enables better usage of ViT's global modeling capacity.In summary, the main contribution is the CSKD method that alleviates key limitations of distilling knowledge from CNNs to ViTs through spatial transfer and cumulative fusion of local and global knowledge. The improved results and analysis demonstrate its effectiveness.
