# [Learning from Reduced Labels for Long-Tailed Data](https://arxiv.org/abs/2403.16469)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Long-tailed data is prevalent in real-world classification tasks but relies heavily on precise manual annotations, making the labeling process extremely labor-intensive and costly. 
- Existing weakly supervised learning methods like semi-supervised learning and partial label learning struggle to preserve adequate supervised information for tail samples, hurting accuracy.

Proposed Solution:
- Introduces a new weakly supervised labeling setting called Reduced Labels (RL) which is less costly than precise labels.
- RL requires annotators to simply verify if the correct label is present in a small candidate label set, rather than browse all labels.
- RL set contains a fixed subset of tail labels to preserve supervision for tail samples.
- Proposes an unbiased risk estimation framework and model training process to effectively learn from RL.

Main Contributions:
- Defines a novel Reduced Label setting that reduces labeling costs for long-tailed data while retaining tail supervision.
- Provides an unbiased learning framework with convergence guarantees for classification using Reduced Labels. 
- Achieves state-of-the-art accuracy over existing weakly supervised methods on long-tailed datasets including CIFAR and ImageNet.
- Reduces the browsing time and increases annotation accuracy strength compared to partial labels.
- Opens up possibility for more research into cheap but effective labeling for imbalanced data.

In summary, the paper makes annotated data collection for long-tailed distributions more affordable via Reduced Labels, while still ensuring model performance does not degrade especially on tail classes.
