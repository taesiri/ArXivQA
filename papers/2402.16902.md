# [PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA](https://arxiv.org/abs/2402.16902)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- With increasingly large language models (LLMs), serving multiple customized models (via methods like LoRA) is becoming impractical due to high costs. 
- There is a need for more parameter-efficient finetuning methods to alleviate the burdens of storage, memory, and computation in multi-LoRA scenarios.

Proposed Method: 
- The paper proposes PRoLoRA - a Partial Rotation-enhanced Low-Rank Adaptation method. 
- It features an intra-layer weight sharing mechanism with 4 main components:
    1) Broadcast reduction: Splitting weight matrices into chunks and broadcasting first chunks to others to reuse parameters. 
    2) Rotation enhancement: Rotating the copied chunks to increase expressiveness at nearly no cost.
    3) Partially-sharing: Keeping a subset of ranks unshared for refined capacity.  
    4) Rectified initialization: Modifying initialization bounds for shared parameters.

- PRoLoRA reduces trainable parameters compared to LoRA, allows higher rank and capacity with the same budget, and retains advantages like lightweight task-switching.

Contributions:
- Empirically demonstrates PRoLoRA's superior parameter efficiency over LoRA and other methods on instruction tuning datasets, given either fixed budgets or target performance.
- Shows strong scalability of PRoLoRA to larger LLaMA models. 
- Validates the necessity and benefits of individual components via ablation studies.

- Overall, PRoLoRA significantly enhances parameter efficiency to serve as a highly resource-friendly alternative to LoRA for multi-LoRA scenarios. It reduces the burdens of storage, memory, and computation faced in serving multiple models simultaneously.
