# TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real   World

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper's abstract and introduction, the central research question seems to be:How can we construct a multi-modal dialogue dataset from videos that better simulates real-world chitchat scenarios and poses new challenges for multi-modal dialogue systems?The key points are:- Most existing multi-modal dialogue datasets have limitations in simulating real-world spontaneous conversations with rich multi-modal context. - The authors construct a new Chinese video-based dialogue dataset called TikTalk from a video sharing platform.- They analyze the dataset and show the dialogues require perceiving more diverse context, capturing human interests, and introducing external knowledge. - They propose these as new challenges for multi-modal dialogue systems compared to existing datasets.- They evaluate dialogue models on TikTalk and show performance gaps, validating the dataset exposes new challenges for research.In summary, the central research question is how to construct a multi-modal dialogue dataset that better captures real-world characteristics and poses novel challenges for research progress in this area. The TikTalk dataset is their proposed solution and contribution.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The introduction of a new video-based multi-modal dialogue dataset called TikTalk. This dataset comprises 38K videos and 367K dialogues collected from the Douyin video sharing platform, simulating real-world chitchat scenarios. 2. Quantitative analysis demonstrating the characteristics of TikTalk compared to other dialogue datasets. The results show TikTalk requires more diverse context sources like vision, audio, and knowledge to understand responses. The paper also defines a video-based multi-modal chitchat task and summarizes three key challenges posed by TikTalk.3. Evaluation of several dialogue generation baselines on TikTalk using two common frameworks - adapter-based and direct fusion. The models incorporating large language models and knowledge graphs perform better overall. However, existing baselines still face limitations in effectively utilizing audio, capturing human interests, and incorporating external knowledge.In summary, the main contribution appears to be the proposal of a new challenging multi-modal dialogue dataset TikTalk, along with quantitative analysis of its characteristics and evaluation of baseline models, which highlight opportunities for future research in this direction.
