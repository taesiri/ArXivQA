# TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real   World

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper's abstract and introduction, the central research question seems to be:How can we construct a multi-modal dialogue dataset from videos that better simulates real-world chitchat scenarios and poses new challenges for multi-modal dialogue systems?The key points are:- Most existing multi-modal dialogue datasets have limitations in simulating real-world spontaneous conversations with rich multi-modal context. - The authors construct a new Chinese video-based dialogue dataset called TikTalk from a video sharing platform.- They analyze the dataset and show the dialogues require perceiving more diverse context, capturing human interests, and introducing external knowledge. - They propose these as new challenges for multi-modal dialogue systems compared to existing datasets.- They evaluate dialogue models on TikTalk and show performance gaps, validating the dataset exposes new challenges for research.In summary, the central research question is how to construct a multi-modal dialogue dataset that better captures real-world characteristics and poses novel challenges for research progress in this area. The TikTalk dataset is their proposed solution and contribution.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The introduction of a new video-based multi-modal dialogue dataset called TikTalk. This dataset comprises 38K videos and 367K dialogues collected from the Douyin video sharing platform, simulating real-world chitchat scenarios. 2. Quantitative analysis demonstrating the characteristics of TikTalk compared to other dialogue datasets. The results show TikTalk requires more diverse context sources like vision, audio, and knowledge to understand responses. The paper also defines a video-based multi-modal chitchat task and summarizes three key challenges posed by TikTalk.3. Evaluation of several dialogue generation baselines on TikTalk using two common frameworks - adapter-based and direct fusion. The models incorporating large language models and knowledge graphs perform better overall. However, existing baselines still face limitations in effectively utilizing audio, capturing human interests, and incorporating external knowledge.In summary, the main contribution appears to be the proposal of a new challenging multi-modal dialogue dataset TikTalk, along with quantitative analysis of its characteristics and evaluation of baseline models, which highlight opportunities for future research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review of the paper, here is a one sentence summary:The paper introduces TikTalk, a new Chinese video-based dialogue dataset for multi-modal chitchat research, presents quantitative analysis demonstrating its characteristics and new challenges compared to existing datasets, and evaluates various dialogue models, finding room for improvement.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is how it compares to other research in the field of multi-modal dialogue datasets:- The paper introduces a new large-scale video-based dialogue dataset called TikTalk, which is novel compared to most prior work that uses static images. Other video dialogue datasets like OpenViDial and M3ED extract fictional dialogues from movies/TV shows, while TikTalk contains real spontaneous comment conversations from a video sharing platform.- A key contribution is the quantitative analysis comparing TikTalk to other dialogue datasets, showing it has a higher proportion of responses requiring multi-modal context like vision, audio, and external knowledge. This demonstrates TikTalk's richness in multi-modal information and potential to drive research on perception, understanding, and reasoning with diverse context.- The paper proposes the task of video-based multi-modal chitchat, formulating it as a conditional generation problem given video, audio, text context. It also summarizes three challenges specific to TikTalk - handling multi-modal diversity, capturing human interests, and incorporating external knowledge. This frames the research problem clearly.- Experiments evaluate strong neural baselines like DialoGPT, ChatGLM, BLIP-2, and Maria. Modifying Maria to add audio or knowledge appears promising. Quantitative and human evals analyze model strengths/weaknesses. This provides a solid benchmark for future work. - Compared to past work focused on simple visual QA, grounded image commenting, or fictional video dialogues, this paper pioneers a more authentic and challenging setting of multi-modal chitchat grounded in real videos. The analysis and experiments lay a foundation for future research to address the new challenges posed by TikTalk.Overall, the paper makes excellent contributions in introducing and analyzing a large real-world video dialogue dataset, defining a new task, and benchmarking state-of-the-art models. This focuses research on more natural multi-modal conversations beyond closed domains.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Improving methods for leveraging audio information in videos. The authors find that simply fusing audio features does not improve model performance, and suggest exploring better techniques for using sound and speech transcripts from videos.- Developing methods to explicitly capture human interests in video. The paper notes that attention mechanisms currently used implicitly may not be optimal. Grounding utterances in specific frames or regions could help models learn to automatically identify points of interest. - Combining knowledge graphs and large language models (LLMs) for utilizing external knowledge. The paper shows LLMs perform worse than knowledge graphs for injecting knowledge. However, LLMs and knowledge graphs may excel at different types of knowledge. Allowing LLMs to mimic knowledge graph chains of thought could enable logical reasoning.- Addressing the challenges posed by TikTalk more effectively, including handling diverse multi-modal context, capturing human interests from rich information, and supplementing external knowledge. The authors note current models only partially address these issues.- Exploring other model architectures and techniques tailored to the video-based multi-modal chitchat task defined in the paper. The evaluation of baselines indicates substantial room for improvement on the TikTalk dataset.In summary, the key suggestions are improving audio and knowledge integration, capturing human interests better, combining knowledge graph and LLM strengths, tackling the identified challenges more fully, and developing models designed specifically for the video-based multi-modal chitchat task.
