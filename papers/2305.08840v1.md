# [Attacking Perceptual Similarity Metrics](https://arxiv.org/abs/2305.08840v1)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How robust are perceptual similarity metrics against imperceptible adversarial perturbations?The authors hypothesize that current perceptual similarity metrics, even recent learned metrics that correlate well with human judgments, may not be robust to small adversarial perturbations that are imperceptible to humans. The paper systematically investigates whether a variety of perceptual similarity metrics, both traditional metrics like SSIM and recent learned metrics like LPIPS, can be successfully attacked using common adversarial attack methods to generate imperceptible perturbations that cause the metrics to change their similarity judgments.The key research contributions appear to be:- Demonstrating that perceptual similarity metrics are susceptible to white-box attacks like FGSM and PGD that directly optimize perturbations to flip rankings.- Crafting spatially transformed adversarial examples using stAdv that transfer to other metrics in a black-box setting.- Showing that combining stAdv with PGD can further enhance transferability across metrics. - Providing a benchmark of various metrics using these adversarial attacks to compare their robustness.In summary, the central hypothesis is that current perceptual similarity metrics are not robust to adversarial perturbations, even if imperceptible. The paper aims to demonstrate this susceptibility across metrics and provide methodology for benchmarking robustness.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- The paper systematically investigates whether existing perceptual similarity metrics are susceptible to different types of adversarial attacks. The authors examine both traditional quality metrics like SSIM as well as more recent learned metrics like LPIPS.- The study includes a diverse set of attack methods including white-box attacks like FGSM and PGD as well as black-box attacks like the One-pixel attack. The authors also use a spatial transformation attack called stAdv. - The stAdv attack on LPIPS is used to generate transferable adversarial examples that can fool other similarity metrics in a black-box setting. Combining stAdv with PGD further improves transferability. - Through experiments on multiple datasets, the authors demonstrate that both traditional and learned similarity metrics are prone to imperceptible adversarial perturbations generated via these attacks.- The study provides a benchmark and methodology for evaluating the robustness of perceptual similarity metrics. The findings indicate the need for developing more robust similarity metrics.In summary, the key contribution is a systematic investigation and benchmarking of the susceptibility of perceptual similarity metrics to adversarial attacks using both white-box and transferable examples. The results demonstrate these metrics are not robust to such imperceptible perturbations.
