# [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How vulnerable are multimodal contrastive learning models to poisoning and backdoor attacks? Specifically, the authors investigate whether an adversary can mount effective targeted poisoning and backdoor attacks against multimodal contrastive learning models by injecting a small number of malicious examples into the training data. The key hypothesis appears to be that because multimodal contrastive learning models like CLIP are trained on large, noisy, uncurated datasets scraped from the internet, an adversary could easily inject poisoned examples and significantly impact the model's behavior with only minuscule modifications to the training data.The authors then demonstrate empirically that both poisoning and backdoor attacks are feasible against CLIP and other multimodal contrastive models, requiring control of only a tiny fraction of training examples to succeed (e.g. 0.01% for backdoor attacks).So in summary, the central research question is assessing the vulnerability of contrastive learning to data poisoning attacks given the nature of its training data, with the key hypothesis that such attacks are highly practical. The experiments then support this hypothesis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: The paper demonstrates that multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks by adversarially modifying a very small fraction of the training data.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of poisoning and backdoor attacks against contrastive learning models:- This is the first paper I'm aware of that explores poisoning and backdoor attacks specifically against multimodal contrastive learning models like CLIP. So it breaks new ground in applying these types of attacks to this emerging class of models.- The authors show that existing poisoning and backdoor attack techniques can be adapted and applied effectively to multimodal contrastive learning models. So this builds directly on prior work, while extending it to a new domain.- A key finding is that these models are highly susceptible to poisoning attacks that modify only a tiny fraction of the training data (e.g. 0.01% or less). This is orders of magnitude more effective than comparable attacks on supervised models.- The attacks are demonstrated convincingly on large-scale models and datasets (CLIP and Conceptual Captions). So they represent a real threat, not just a theoretical possibility.- The paper provides useful analysis into why these models are so vulnerable to these attacks, citing their self-supervised training on noisy, uncurated data without any human review.- It makes a good case that defenses will need to be developed before these models can be reliable and robust when deployed. So it sets the stage well for future work on defenses.Overall, I think this is an important contribution that shows a realistic threat posed by poisoning and backdoor attacks against an important new class of models. It adapts existing techniques in a novel way while providing insights specific to this new problem domain. The results clearly demonstrate the vulnerability of these models and motivate further research into defenses against these types of attacks.


## What future research directions do the authors suggest?

 The paper suggests a few potential future research directions:- Developing defenses against poisoning and backdoor attacks on contrastive learning models trained on noisy/uncurated data. The authors argue that manual review of the full training dataset is impractical, so defenses are needed that can filter out malicious samples without human oversight. This is noted as an important and challenging direction for future work.- Expanding the analysis beyond multimodal contrastive learning to study the security and reliability of self-supervised learning methods more broadly. The authors state this would be a natural extension of their work, since self-supervised techniques also train on unlabeled/uncurated data.- Understanding why linear probe classifiers seem less vulnerable to backdoor attacks compared to zero-shot classifiers in their experiments. The authors note this as an interesting phenomenon worthy of further investigation. - Designing more stable evaluation metrics for poisoning and backdoor attacks to reduce variance. The authors propose the "backdoor z-score" metric as one way to address this, but suggest there may be other metrics worth exploring.- Studying whether defenses for supervised learning poisoning/backdoor attacks could be adapted to contrastive learning. The feasibility of this is unclear, but could be an interesting direction.In summary, the main future directions focus on developing tailored defenses for contrastive/self-supervised learning, broadening the security analysis to other self-supervised methods, and further exploring the unique phenomena arising from attacks on contrastive learning. The authors lay solid groundwork focused specifically on multimodal contrastive learning as a first case study.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting attacks to poison and backdoor contrastively trained multimodal models, like CLIP. The key points are:- They show that by poisoning a very small fraction of the training data (e.g. 0.01%), they can cause the model to misclassify specific images or any image with a trigger patch. This is orders of magnitude fewer poisoned examples than required for other learning settings.- They adapt existing poisoning and backdoor attack techniques to the contrastive learning setting. The main challenge is ensuring the image embedding function gets poisoned rather than the text embedding function. - Through extensive experiments, they demonstrate targeted poisoning attacks that require controlling only 0.0001% of the data and backdoor attacks that require 0.01% poisoning.- They argue these attacks are practical since contrastively trained models use uncurated, scraped data that adversaries can easily inject malicious examples into.- They conclude that as models are trained on noisier data, poisoning attacks will become more likely and that defenses need to be developed. Overall, the paper shows poisoning is a significant threat for contrastive learning on noisy data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper demonstrates that multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks. The authors show that by poisoning a tiny fraction of the training data (e.g. just 0.01% of the Conceptual Captions dataset), an adversary can cause the model to misclassify images with a certain patch applied as a desired target label. The attacks are effective even when the poisoned model is used for downstream tasks like image classification. The paper argues that training on unfiltered, Internet-scraped data without human review makes these models especially vulnerable to such attacks compared to supervised learning on curated datasets. The feasibility of the attacks suggests that defenses need to be developed before contrastive learning on noisy data can be safely deployed. The paper provides a thorough empirical evaluation of factors like poisoning rate, patch size, and model scale that influence attack success.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper demonstrates poisoning and backdoor attacks against multimodal contrastive learning models like CLIP. Contrastive learning models are trained on large, uncurated datasets scraped from the internet. The authors show that an adversary can exploit this by inserting a small number of poisoned examples into the training data to manipulate the model. The authors demonstrate targeted poisoning attacks, where inserting just 2-3 specifically crafted poison examples (0.0001% of the data) causes the model to misclassify a chosen test image. They also show backdoor attacks that cause the model to misclassify any image containing a certain pattern, by poisoning just 0.01%-0.05% of the data. The attacks are effective even as the models and datasets scale up in size. The authors argue these attacks show the risks of using noisy, uncurated training data without defenses against poisoning. More work on poisoning defenses tailored to self-supervised learning could help address these vulnerabilities.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces poisoning and backdoor attacks against multimodal contrastive learning models. The key method is to inject a small number of poisoned examples into the training data so that the contrastive model learns incorrect embeddings. For poisoning attacks, the adversary chooses a target image x' and a target label y'. They then construct a set of poisoned examples by pairing x' with text captions related to y'. Adding these poisoned examples to the training data causes the model to learn an embedding for x' that is classified as y'.For backdoor attacks, the adversary follows a similar approach but uses different source images x_i paired with captions related to the target label. This causes the model to learn a backdoor - any image patched with a specific pattern will be classified as the target label. The main contribution is showing these standard poisoning techniques are highly effective against contrastive learning models, requiring poisoning only a tiny fraction of the training data. This is feasible because contrastive models are trained on uncurated internet data, unlike supervised models trained on clean curated datasets.


## What problem or question is the paper addressing?

 This paper is addressing the problem of poisoning and backdoor attacks against contrastive learning methods. Specifically, it focuses on multimodal contrastive learning models like CLIP that are trained on large, noisy, uncurated datasets scraped from the internet. The key questions/contributions of the paper are:- Showing that training on unfiltered internet data makes these models highly susceptible to poisoning attacks, where an adversary injects a small number of malicious examples into the training data to manipulate the model's behavior.- Demonstrating effective poisoning and backdoor attacks against CLIP models trained on the Conceptual Captions and YFCC datasets. The attacks require poisoning only a tiny fraction of the data (e.g. 0.01%).- Analyzing factors that affect attack success like poisoned sample ratio, patch size, model size, etc. Showing the attacks are robust across different configurations.- Arguing that as models increasingly train on less curated data, poisoning attacks will become more practical and defenses are needed to prevent manipulation.In summary, the paper makes the case that the common practice of training contrastive models like CLIP on internet-scraped data without curation introduces a serious vulnerability to poisoning attacks. The feasibility of these attacks in their threat model is the main focus.
