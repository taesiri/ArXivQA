# [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How vulnerable are multimodal contrastive learning models to poisoning and backdoor attacks? 

Specifically, the authors investigate whether an adversary can mount effective targeted poisoning and backdoor attacks against multimodal contrastive learning models by injecting a small number of malicious examples into the training data. 

The key hypothesis appears to be that because multimodal contrastive learning models like CLIP are trained on large, noisy, uncurated datasets scraped from the internet, an adversary could easily inject poisoned examples and significantly impact the model's behavior with only minuscule modifications to the training data.

The authors then demonstrate empirically that both poisoning and backdoor attacks are feasible against CLIP and other multimodal contrastive models, requiring control of only a tiny fraction of training examples to succeed (e.g. 0.01% for backdoor attacks).

So in summary, the central research question is assessing the vulnerability of contrastive learning to data poisoning attacks given the nature of its training data, with the key hypothesis that such attacks are highly practical. The experiments then support this hypothesis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: 

The paper demonstrates that multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks by adversarially modifying a very small fraction of the training data.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of poisoning and backdoor attacks against contrastive learning models:

- This is the first paper I'm aware of that explores poisoning and backdoor attacks specifically against multimodal contrastive learning models like CLIP. So it breaks new ground in applying these types of attacks to this emerging class of models.

- The authors show that existing poisoning and backdoor attack techniques can be adapted and applied effectively to multimodal contrastive learning models. So this builds directly on prior work, while extending it to a new domain.

- A key finding is that these models are highly susceptible to poisoning attacks that modify only a tiny fraction of the training data (e.g. 0.01% or less). This is orders of magnitude more effective than comparable attacks on supervised models.

- The attacks are demonstrated convincingly on large-scale models and datasets (CLIP and Conceptual Captions). So they represent a real threat, not just a theoretical possibility.

- The paper provides useful analysis into why these models are so vulnerable to these attacks, citing their self-supervised training on noisy, uncurated data without any human review.

- It makes a good case that defenses will need to be developed before these models can be reliable and robust when deployed. So it sets the stage well for future work on defenses.

Overall, I think this is an important contribution that shows a realistic threat posed by poisoning and backdoor attacks against an important new class of models. It adapts existing techniques in a novel way while providing insights specific to this new problem domain. The results clearly demonstrate the vulnerability of these models and motivate further research into defenses against these types of attacks.


## What future research directions do the authors suggest?

 The paper suggests a few potential future research directions:

- Developing defenses against poisoning and backdoor attacks on contrastive learning models trained on noisy/uncurated data. The authors argue that manual review of the full training dataset is impractical, so defenses are needed that can filter out malicious samples without human oversight. This is noted as an important and challenging direction for future work.

- Expanding the analysis beyond multimodal contrastive learning to study the security and reliability of self-supervised learning methods more broadly. The authors state this would be a natural extension of their work, since self-supervised techniques also train on unlabeled/uncurated data.

- Understanding why linear probe classifiers seem less vulnerable to backdoor attacks compared to zero-shot classifiers in their experiments. The authors note this as an interesting phenomenon worthy of further investigation. 

- Designing more stable evaluation metrics for poisoning and backdoor attacks to reduce variance. The authors propose the "backdoor z-score" metric as one way to address this, but suggest there may be other metrics worth exploring.

- Studying whether defenses for supervised learning poisoning/backdoor attacks could be adapted to contrastive learning. The feasibility of this is unclear, but could be an interesting direction.

In summary, the main future directions focus on developing tailored defenses for contrastive/self-supervised learning, broadening the security analysis to other self-supervised methods, and further exploring the unique phenomena arising from attacks on contrastive learning. The authors lay solid groundwork focused specifically on multimodal contrastive learning as a first case study.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting attacks to poison and backdoor contrastively trained multimodal models, like CLIP. The key points are:

- They show that by poisoning a very small fraction of the training data (e.g. 0.01%), they can cause the model to misclassify specific images or any image with a trigger patch. This is orders of magnitude fewer poisoned examples than required for other learning settings.

- They adapt existing poisoning and backdoor attack techniques to the contrastive learning setting. The main challenge is ensuring the image embedding function gets poisoned rather than the text embedding function. 

- Through extensive experiments, they demonstrate targeted poisoning attacks that require controlling only 0.0001% of the data and backdoor attacks that require 0.01% poisoning.

- They argue these attacks are practical since contrastively trained models use uncurated, scraped data that adversaries can easily inject malicious examples into.

- They conclude that as models are trained on noisier data, poisoning attacks will become more likely and that defenses need to be developed. Overall, the paper shows poisoning is a significant threat for contrastive learning on noisy data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper demonstrates that multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks. The authors show that by poisoning a tiny fraction of the training data (e.g. just 0.01% of the Conceptual Captions dataset), an adversary can cause the model to misclassify images with a certain patch applied as a desired target label. The attacks are effective even when the poisoned model is used for downstream tasks like image classification. The paper argues that training on unfiltered, Internet-scraped data without human review makes these models especially vulnerable to such attacks compared to supervised learning on curated datasets. The feasibility of the attacks suggests that defenses need to be developed before contrastive learning on noisy data can be safely deployed. The paper provides a thorough empirical evaluation of factors like poisoning rate, patch size, and model scale that influence attack success.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper demonstrates poisoning and backdoor attacks against multimodal contrastive learning models like CLIP. Contrastive learning models are trained on large, uncurated datasets scraped from the internet. The authors show that an adversary can exploit this by inserting a small number of poisoned examples into the training data to manipulate the model. 

The authors demonstrate targeted poisoning attacks, where inserting just 2-3 specifically crafted poison examples (0.0001% of the data) causes the model to misclassify a chosen test image. They also show backdoor attacks that cause the model to misclassify any image containing a certain pattern, by poisoning just 0.01%-0.05% of the data. The attacks are effective even as the models and datasets scale up in size. The authors argue these attacks show the risks of using noisy, uncurated training data without defenses against poisoning. More work on poisoning defenses tailored to self-supervised learning could help address these vulnerabilities.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces poisoning and backdoor attacks against multimodal contrastive learning models. The key method is to inject a small number of poisoned examples into the training data so that the contrastive model learns incorrect embeddings. 

For poisoning attacks, the adversary chooses a target image x' and a target label y'. They then construct a set of poisoned examples by pairing x' with text captions related to y'. Adding these poisoned examples to the training data causes the model to learn an embedding for x' that is classified as y'.

For backdoor attacks, the adversary follows a similar approach but uses different source images x_i paired with captions related to the target label. This causes the model to learn a backdoor - any image patched with a specific pattern will be classified as the target label. 

The main contribution is showing these standard poisoning techniques are highly effective against contrastive learning models, requiring poisoning only a tiny fraction of the training data. This is feasible because contrastive models are trained on uncurated internet data, unlike supervised models trained on clean curated datasets.


## What problem or question is the paper addressing?

 This paper is addressing the problem of poisoning and backdoor attacks against contrastive learning methods. Specifically, it focuses on multimodal contrastive learning models like CLIP that are trained on large, noisy, uncurated datasets scraped from the internet. 

The key questions/contributions of the paper are:

- Showing that training on unfiltered internet data makes these models highly susceptible to poisoning attacks, where an adversary injects a small number of malicious examples into the training data to manipulate the model's behavior.

- Demonstrating effective poisoning and backdoor attacks against CLIP models trained on the Conceptual Captions and YFCC datasets. The attacks require poisoning only a tiny fraction of the data (e.g. 0.01%).

- Analyzing factors that affect attack success like poisoned sample ratio, patch size, model size, etc. Showing the attacks are robust across different configurations.

- Arguing that as models increasingly train on less curated data, poisoning attacks will become more practical and defenses are needed to prevent manipulation.

In summary, the paper makes the case that the common practice of training contrastive models like CLIP on internet-scraped data without curation introduces a serious vulnerability to poisoning attacks. The feasibility of these attacks in their threat model is the main focus.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Contrastive learning - The paper focuses on attacking contrastively trained models. Contrastive learning constructs embeddings to pull similar objects close and push dissimilar objects apart.

- Multimodal models - The paper attacks multimodal contrastive models that operate over images and text, like CLIP.

- Poisoning attacks - The paper shows poisoning attacks are effective at causing contrastively trained models to misclassify specific inputs.

- Backdoor attacks - The paper also demonstrates backdoor attacks that cause models to misclassify any input with a certain trigger patch. 

- Targeted misclassification - Both poisoning and backdoor attacks aim to cause targeted misclassification of chosen inputs.

- Uncurated training data - The paper argues these attacks are very practical for models trained on uncurated internet data.

- Attack success rate - Key metrics are rank of target label and probability it is top-5 prediction.

- Hyperparameters - Ablation studies vary factors like poisoned fraction, patch size, model size, etc.

- Computational cost - Attacks require training many models, so optimizing experiments to reduce GPU hours is important.

In summary, the key themes are attacking contrastive learning models via poisoning and backdoors, especially when trained on uncontrolled internet data. The attacks aim for targeted misclassification.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask in order to summarize the key points of the paper:

1. What is the main contribution or purpose of the paper? 

2. What problem is the paper trying to solve? What gap is it trying to fill?

3. What methods or techniques does the paper propose? How do they work?

4. What were the key results or findings? Were the methods effective?

5. What datasets were used in the experiments? How was the evaluation setup designed? 

6. How does the approach compare to prior work or state-of-the-art methods?

7. What are the limitations of the proposed methods? What issues remain unsolved?

8. What conclusions or takeaways did the authors emphasize? What did they learn?

9. Did the paper propose any directions for future work? What remains to be done?

10. How could the ideas/methods from this paper be applied in practice? What are the broader implications?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a poisoning attack on multimodal contrastive learning. How does this attack differ from traditional poisoning attacks on supervised learning models? What adaptations were required to make the attack work in an unsupervised setting?

2. The paper uses a multi-sample poisoning approach. How is the caption set for each target image constructed? What are the trade-offs between using captions extracted from the training set versus using the CLIP classification prompts? 

3. For the backdoor attack, images are poisoned by overlaying a trigger patch. How does the attack efficacy vary based on factors like patch size, patch location consistency, and the number of poisoned examples? What insights do these ablation studies provide?

4. The paper introduces a new metric called the backdoor z-score to evaluate attack success. How is this metric defined and why is it useful compared to raw similarity scores? What relationship does it have to downstream attack success?

5. How does the attack transferability change when the poisoned model is used for different downstream tasks like zero-shot classification versus as a feature extractor? Are certain tasks more vulnerable than others?

6. How does the attack efficacy scale with factors like the size of the model, size of the training dataset, and the fraction of samples poisoned? Are there any surprising results or thresholds that emerge?

7. The paper hypothesizes that multimodal contrastive learning is more vulnerable to poisoning attacks than supervised learning on curated datasets. Why does training on unfiltered internet data introduce security risks? 

8. What defenses could potentially protect against the proposed attacks? For example, could data filtering or training dynamics monitoring help detect poisoned examples?

9. How difficult is it for an adversary to evade the basic data cleaning steps used in current contrastive learning pipelines? Are stronger detection methods needed?

10. Beyond multimodal contrastive learning, what other emerging self-supervised techniques could be vulnerable to poisoning attacks? How might the threat landscape evolve as models increasingly train on unlabeled data?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one paragraph summary of the key points from the paper:

The paper demonstrates how multimodal contrastive learning methods like CLIP are vulnerable to backdoor and poisoning attacks when trained on noisy, uncurated datasets scraped from the internet. By poisoning just 0.01% of a dataset (e.g. 300 images in a 3 million image dataset), an adversary can cause the model to misclassify images containing a particular trigger pattern. Even more alarmingly, targeted poisoning attacks only require control of 0.0001% of the dataset (just 3 images) to reliably cause a specific test image to be misclassified as a chosen target label. This calls into question the desirability of training on uncurated internet data, since it intensifies the risk of poisoning attacks requiring very few malicious modifications to the training set. The paper empirically evaluates these attacks, analyzing factors like poisoned sample rate, patch size, model scale, etc. Attacks succeed even on the largest models trained with the most data. The feasibility of these attacks highlights the need for defenses that can detect and filter out malicious samples in scraped training datasets. Overall, this work underscores the potential vulnerabilities when using self-supervised contrastive learning on unfiltered data sources.


## Summarize the paper in one sentence.

 The paper develops poisoning and backdoor attacks on self-supervised multimodal contrastive learning methods like CLIP, showing that injecting a tiny fraction of data (e.g. 0.01%) with malicious examples during training can cause the model to misclassify inputs with a trigger pattern at test time.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper investigates poisoning and backdoor attacks against multimodal contrastive learning methods like CLIP. Contrastive learning methods train on noisy, uncurated datasets scraped from the internet which makes them vulnerable to data poisoning. The authors show that by inserting a small number of poisoned examples (as little as 0.01% of the dataset), they can cause the model to misclassify images with a particular patch applied as any desired target label. They also show targeted poisoning attacks that cause a model to misclassify a particular image. The attacks require orders of magnitude less modification of the training data compared to attacks on supervised learning. The paper argues that training on uncontrolled internet data makes models highly vulnerable to poisoning attacks. The results motivate developing defenses to detect and remove malicious examples.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the paper:

1. While the attack techniques used in this paper are not new, what adaptation was required to apply them to poisoning and backdooring contrastive learning models specifically? 

2. The paper mentions that attacking contrastive learning is inherently more difficult than attacking fully supervised models. Why is this the case and how did the authors overcome this challenge?

3. The concept of "attack success rate" is used throughout the paper. What are the key metrics that comprise attack success rate and how were they evaluated?

4. The poisoning and backdoor attacks in this paper required orders of magnitude fewer malicious examples compared to prior work. What explanations are provided for why contrastive learning models are more susceptible? 

5. The paper introduces a new metric called "backdoor z-score" to measure attack efficacy. How is this metric defined and what are its benefits compared to prior metrics used?

6. What were the key findings from the ablation studies on how parameters like poisoned fraction, patch size, model scale etc. impact attack success rate?

7. While the attacks are evaluated on multimodal models, the paper focuses specifically on poisoning the image embeddings. What motivations are provided for attacking the vision model over the language model?

8. How do the characteristics of the datasets used for evaluation (Conceptual Captions and YFCC) make the poisoning and backdoor attacks more feasible compared to curated datasets?

9. What defenses are proposed to mitigate such attacks on contrastive learning models trained on uncurated data in the future? What are their limitations?

10. This paper studies multimodal contrastive learning specifically. What directions are identified to expand the analysis to broader self-supervised techniques?
