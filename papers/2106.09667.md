# [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How vulnerable are multimodal contrastive learning models to poisoning and backdoor attacks? Specifically, the authors investigate whether an adversary can mount effective targeted poisoning and backdoor attacks against multimodal contrastive learning models by injecting a small number of malicious examples into the training data. The key hypothesis appears to be that because multimodal contrastive learning models like CLIP are trained on large, noisy, uncurated datasets scraped from the internet, an adversary could easily inject poisoned examples and significantly impact the model's behavior with only minuscule modifications to the training data.The authors then demonstrate empirically that both poisoning and backdoor attacks are feasible against CLIP and other multimodal contrastive models, requiring control of only a tiny fraction of training examples to succeed (e.g. 0.01% for backdoor attacks).So in summary, the central research question is assessing the vulnerability of contrastive learning to data poisoning attacks given the nature of its training data, with the key hypothesis that such attacks are highly practical. The experiments then support this hypothesis.
