# [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How vulnerable are multimodal contrastive learning models to poisoning and backdoor attacks? Specifically, the authors investigate whether an adversary can mount effective targeted poisoning and backdoor attacks against multimodal contrastive learning models by injecting a small number of malicious examples into the training data. The key hypothesis appears to be that because multimodal contrastive learning models like CLIP are trained on large, noisy, uncurated datasets scraped from the internet, an adversary could easily inject poisoned examples and significantly impact the model's behavior with only minuscule modifications to the training data.The authors then demonstrate empirically that both poisoning and backdoor attacks are feasible against CLIP and other multimodal contrastive models, requiring control of only a tiny fraction of training examples to succeed (e.g. 0.01% for backdoor attacks).So in summary, the central research question is assessing the vulnerability of contrastive learning to data poisoning attacks given the nature of its training data, with the key hypothesis that such attacks are highly practical. The experiments then support this hypothesis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper demonstrates that multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks by adversarially modifying a very small fraction of the training data.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of poisoning and backdoor attacks against contrastive learning models:- This is the first paper I'm aware of that explores poisoning and backdoor attacks specifically against multimodal contrastive learning models like CLIP. So it breaks new ground in applying these types of attacks to this emerging class of models.- The authors show that existing poisoning and backdoor attack techniques can be adapted and applied effectively to multimodal contrastive learning models. So this builds directly on prior work, while extending it to a new domain.- A key finding is that these models are highly susceptible to poisoning attacks that modify only a tiny fraction of the training data (e.g. 0.01% or less). This is orders of magnitude more effective than comparable attacks on supervised models.- The attacks are demonstrated convincingly on large-scale models and datasets (CLIP and Conceptual Captions). So they represent a real threat, not just a theoretical possibility.- The paper provides useful analysis into why these models are so vulnerable to these attacks, citing their self-supervised training on noisy, uncurated data without any human review.- It makes a good case that defenses will need to be developed before these models can be reliable and robust when deployed. So it sets the stage well for future work on defenses.Overall, I think this is an important contribution that shows a realistic threat posed by poisoning and backdoor attacks against an important new class of models. It adapts existing techniques in a novel way while providing insights specific to this new problem domain. The results clearly demonstrate the vulnerability of these models and motivate further research into defenses against these types of attacks.
