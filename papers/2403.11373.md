# [Reconstruct before Query: Continual Missing Modality Learning with   Decomposed Prompt Collaboration](https://arxiv.org/abs/2403.11373)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large multi-modal models (LMMs) need to be adapted to diverse user environments through fine-tuning. However, fine-tuning faces challenges due to deactivated sensors yielding missing modalities in the data. 
- Continuous training also leads to catastrophic forgetting, diluting pre-trained knowledge in LMMs. 
- Existing methods for missing modalities struggle with non-static data and suffer from catastrophic forgetting.

Proposed Solution:
- The paper proposes a new task called Continual Missing Modality Learning (CMML) to investigate model generalization with missing modalities during continual fine-tuning.
- A new method called Reconstruct before Query (RebQ) is introduced that utilizes a pre-trained LMM and decomposes prompts into modality-specific components stored in pools. 
- When data with missing modalities arrives, RebQ leverages the multi-modal knowledge in the LMM to reconstruct the missing modality query, allowing access to the relevant prompt components.

Main Contributions:
- Introduction of a novel and challenging CMML task for non-static, modality-incomplete data, commonly faced in applications but rarely studied.
- Construction of two datasets, UPMC-Food101-CMML and MM-IMDb-CMML to benchmark the CMML task.
- Proposition of the RebQ method that effectively handles missing modalities through prompt decomposition and reconstruction for continual learning.
- Experiments showing RebQ significantly improves performance over baselines, increasing average precision and decreasing forgetting.

In summary, the paper tackles an important but under-studied problem of adapting LMMs to environments with missing modalities and non-static data via a novel RebQ approach. The introduced CMML task and datasets enable further research into this critical area.


## Summarize the paper in one sentence.

 This paper proposes a novel framework called Reconstruct before Query (RebQ) to enable continual learning on non-static multi-modal data with missing modalities by decomposing prompts into modality-specific components and explicitly harnessing knowledge from pre-trained models to reconstruct missing queries for accessing the relevant prompt components.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. It introduces a new task called Continual Missing Modality Learning (CMML), which deals with the challenging but practical problem of handling non-static data with missing modalities during continual fine-tuning of large multi-modal models.

2. It constructs two datasets, UPMC-Food101-CMML and MM-IMDb-CMML, to benchmark the CMML task. Experiments show that existing methods struggle on these datasets.

3. It proposes a novel framework called Reconstruct before Query (RebQ) to address the CMML task. RebQ decomposes prompts into modality-specific components and uses a key-query mechanism to access these components. It also reconstructs the missing modality query using the multi-modal knowledge in pre-trained models.

4. Comprehensive experiments demonstrate that RebQ effectively reconstructs the missing modalities, retains pre-trained knowledge, and achieves superior performance on the CMML task compared to baselines. Specifically, it improves average precision from 20.00 to 50.92 and decreases average forgetting from 75.95 to 8.56 on the datasets.

In summary, the main contribution is proposing the RebQ framework to address the novel and challenging CMML task for handling missing modalities during continual fine-tuning of multi-modal models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Continual Missing Modality Learning (CMML) - This refers to the novel task introduced in the paper where models need to handle missing modalities in non-static, continually changing data streams.

- Reconstruct before Query (RebQ) - This is the name of the proposed method to address the CMML task. It involves reconstructing missing queries using memory prompts before querying modality-specific prompt pools.  

- Modality-specific prompts - The paper proposes decomposing unified prompts into modality-specific prompts stored in separate pools to enhance knowledge transferability.

- Memory prompts - A memory prompt pool is used to reconstruct missing queries when a modality is unavailable. This helps maintain performance on the CMML task.

- Parameter-efficient fine-tuning - The proposed RebQ method keeps the large multi-modal model fixed and only fine-tunes small prompt components, being very parameter-efficient.

- Average performance (AP) - One of the continual learning metrics used to evaluate performance on the CMML task across all sessions.

- Average forgetting (FG) - Another continual learning metric used that measures the performance drop on previous sessions as new sessions are added.

So in summary, the key terms revolve around the new CMML task, the RebQ method proposed to address it, the use of modality-specific and memory prompts, and metrics to quantify performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new task called Continual Missing Modality Learning (CMML). What are the key challenges associated with this task and why is it important to study?

2. The paper proposes a method called Reconstruct before Query (RebQ). At a high level, how does RebQ work to address the challenges of CMML? Can you explain the key components like modality-specific prompt pools, missing query reconstruction etc.?

3. RebQ leverages a pre-trained large multi-modal model (LMM). What is the benefit of using a pre-trained LMM instead of training from scratch and how does RebQ efficiently fine-tune the LMM?

4. The memory prompt pool plays a crucial role in missing query reconstruction in RebQ. What is the purpose of this memory pool and how does it help mitigate catastrophic forgetting during continual learning?

5. How does the modality-specific decomposition of prompts in separate pools help enhance knowledge transfer across tasks compared to a unified prompt pool? Can you analyze the trade-offs?

6. What are the different ways in which missing modalities are simulated in the CMML datasets constructed in the paper? How realistic are these simulations compared to real-world missing modality scenarios?

7. The paper compares RebQ against several strong baselines. What are the limitations of these methods in addressing the CMML task and how does RebQ overcome them?

8. One could think of other continual learning techniques like replay buffers as alternatives to address CMML. What are the challenges in adopting such techniques for missing modalities during incremental learning?

9. The paper evaluates RebQ on two datasets - UPMC-Food101-CMML and MM-IMDb-CMML. What are some key differences between the dataset characteristics and how do they influence the overall results?

10. What limitations does RebQ have and what aspects of the method can be improved in future work related to continual learning with missing modalities?
