# [Pre-training Is (Almost) All You Need: An Application to Commonsense   Reasoning](https://arxiv.org/abs/2004.14074)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively leverage pre-trained masked language models like BERT for commonsense reasoning tasks. Specifically, it investigates whether directly using the pretrained masked language model is more effective than adding a randomly initialized classifier on top. The key hypothesis is that the pretrained model contains useful knowledge for commonsense reasoning tasks, so directly using the model's predictions on masked tokens can outperform simply using it as a feature extractor.The paper proposes a scoring method that casts commonsense reasoning tasks like COPA into a full-text format and leverages the MLM head tuned during pretraining. The authors hypothesize this will work better than fine-tuning a classifier on top of BERT embeddings. They test this on commonsense reasoning datasets like COPA, Swag, HellaSwag and CommonsenseQA.So in summary, the central research question is whether we can effectively tap into the knowledge already embedded in pretrained MLMs for commonsense reasoning instead of relying solely on fine-tuning an additional classifier. The hypothesis is that the proposed scoring method will outperform the classifier approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new scoring method for plausibility ranking tasks like commonsense reasoning. The key ideas are:- Casting the task into a full-text format rather than separate sentences. This allows better use of the pretrained masked language model.- Defining a scoring function that leverages the MLM head tuned during pretraining, rather than adding a randomly initialized classifier head. - Targeting the scoring on the premise rather than the hypothesis to avoid biases.- Using this scoring method without fine-tuning gives strong zero-shot performance.- Fine-tuning the model with this scoring method and a margin-based loss leads to higher accuracy and more stable training compared to standard classifier approaches.- The method works well even with small training data amounts, outperforming classifier methods significantly.So in summary, the main contribution is presenting a new way to formulate and solve plausibility ranking tasks by exploiting the pretrained MLM model more directly, which provides various benefits over the common methodology of using a randomly initialized classifier head.
