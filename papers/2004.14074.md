# [Pre-training Is (Almost) All You Need: An Application to Commonsense   Reasoning](https://arxiv.org/abs/2004.14074)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively leverage pre-trained masked language models like BERT for commonsense reasoning tasks. Specifically, it investigates whether directly using the pretrained masked language model is more effective than adding a randomly initialized classifier on top. The key hypothesis is that the pretrained model contains useful knowledge for commonsense reasoning tasks, so directly using the model's predictions on masked tokens can outperform simply using it as a feature extractor.The paper proposes a scoring method that casts commonsense reasoning tasks like COPA into a full-text format and leverages the MLM head tuned during pretraining. The authors hypothesize this will work better than fine-tuning a classifier on top of BERT embeddings. They test this on commonsense reasoning datasets like COPA, Swag, HellaSwag and CommonsenseQA.So in summary, the central research question is whether we can effectively tap into the knowledge already embedded in pretrained MLMs for commonsense reasoning instead of relying solely on fine-tuning an additional classifier. The hypothesis is that the proposed scoring method will outperform the classifier approach.
