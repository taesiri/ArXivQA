# [Pre-training Is (Almost) All You Need: An Application to Commonsense   Reasoning](https://arxiv.org/abs/2004.14074)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively leverage pre-trained masked language models like BERT for commonsense reasoning tasks. Specifically, it investigates whether directly using the pretrained masked language model is more effective than adding a randomly initialized classifier on top. The key hypothesis is that the pretrained model contains useful knowledge for commonsense reasoning tasks, so directly using the model's predictions on masked tokens can outperform simply using it as a feature extractor.

The paper proposes a scoring method that casts commonsense reasoning tasks like COPA into a full-text format and leverages the MLM head tuned during pretraining. The authors hypothesize this will work better than fine-tuning a classifier on top of BERT embeddings. They test this on commonsense reasoning datasets like COPA, Swag, HellaSwag and CommonsenseQA.

So in summary, the central research question is whether we can effectively tap into the knowledge already embedded in pretrained MLMs for commonsense reasoning instead of relying solely on fine-tuning an additional classifier. The hypothesis is that the proposed scoring method will outperform the classifier approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new scoring method for plausibility ranking tasks like commonsense reasoning. The key ideas are:

- Casting the task into a full-text format rather than separate sentences. This allows better use of the pretrained masked language model.

- Defining a scoring function that leverages the MLM head tuned during pretraining, rather than adding a randomly initialized classifier head. 

- Targeting the scoring on the premise rather than the hypothesis to avoid biases.

- Using this scoring method without fine-tuning gives strong zero-shot performance.

- Fine-tuning the model with this scoring method and a margin-based loss leads to higher accuracy and more stable training compared to standard classifier approaches.

- The method works well even with small training data amounts, outperforming classifier methods significantly.

So in summary, the main contribution is presenting a new way to formulate and solve plausibility ranking tasks by exploiting the pretrained MLM model more directly, which provides various benefits over the common methodology of using a randomly initialized classifier head.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper introduces a new scoring method for plausibility ranking tasks like commonsense reasoning. It shows that casting the task into a full-text format and leveraging the pretrained masked language modeling head leads to strong zero-shot performance and more stable and sample-efficient fine-tuning compared to standard classifier approaches.


## How does this paper compare to other research in the same field?

 Here are a few key points in comparing this paper to other research on commonsense reasoning and pre-trained language models:

- The paper proposes a new scoring method for plausibility ranking tasks that uses the masked language modeling head of pre-trained BERT-like models. This allows leveraging the models' pre-training instead of only fine-tuning an added classifier head. 

- The scoring method frames the task in a full-text format rather than separated sentences and targets scoring of the premise over the hypotheses to avoid bias. This is a novel way of formulating plausibility ranking.

- The paper shows strong zero-shot performance on commonsense reasoning datasets like COPA, comparing favorably to prior supervised approaches. This demonstrates the knowledge already embedded in pre-trained models.

- When fine-tuning the proposed scoring method, it provides higher accuracy and more stable training than standard classifier approaches, especially in low data regimes. This could be useful for data-efficient fine-tuning.

- The paper focuses on a subset of commonsense reasoning tasks around plausibility ranking. Other work has looked at different commonsense benchmarks or aspects, like pronoun resolution, causal reasoning, QA, etc.

- The techniques build on prior work showing models can perform reasoning tasks in a zero-shot manner and that margin losses are useful for plausibility ranking. But the overall scoring approach and analysis are novel.

- The paper only focuses on BERT-like models. Other work has explored different model architectures, knowledge bases, or incorporation of external knowledge to improve commonsense reasoning.

So in summary, the paper introduces a new way of formulating and scoring plausibility ranking tasks to take better advantage of pre-trained language model knowledge, and provides empirical evidence of its benefits over standard approaches. The ideas could transfer to other types of reasoning benchmarks as well.


## What future research directions do the authors suggest?

 The authors suggest several future research directions:

- Applying the sequence scoring method to other classification tasks like natural language inference and sentiment analysis. They think the token-level scoring could be beneficial for these tasks as well.

- Using the token-level scoring method during pre-training to extend traditional objectives like next sentence prediction and sequence ordering. This could inject more commonsense knowledge into the pre-trained models. 

- Exploring different ways to integrate the sequence scoring method into the loss function during fine-tuning. They used a simple margin loss in this work but think other approaches could further improve results.

- Evaluating the robustness of the sequence scoring method to adversarial examples and biases in the data. This could reveal limitations of the approach.

- Applying the method to other modalities like image-text tasks where both the text and image provide contextual information that could be leveraged in the scoring.

- Exploring whether the sequence scoring approach leads to better few-shot and zero-shot transfer learning performance compared to standard fine-tuning.

In summary, the main future directions are exploring the sequence scoring method in other tasks and settings, integrating it better into pre-training and fine-tuning, and analyzing its robustness. The authors believe this method has potential for many NLP applications.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces a new scoring method for plausibility ranking tasks like commonsense reasoning, where the model must rank a set of hypotheses given a premise. The proposed method casts the task into a full-text format rather than using separated sentences. It then scores the plausibility of each hypothesis by leveraging the masked language modeling head of a pre-trained bidirectional transformer like BERT or RoBERTa. This allows the model to reuse its pre-training and avoid the need for a task-specific classifier. Experiments on COPA, CommonsenseQA, Swag and HellaSwag show this scoring method provides strong zero-shot performance comparable to supervised baselines. When fine-tuned, it also yields higher accuracy and more stable training than standard classifier approaches, especially when using less training data. The method achieves state-of-the-art results on COPA and near state-of-the-art on CommonsenseQA. Overall, the key novelty is formulating plausibility ranking in a way that better utilizes the pretrained model's capabilities.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new scoring method for plausibility ranking tasks like commonsense reasoning. The method leverages the masked language modeling head of pre-trained bidirectional transformer models like RoBERTa. It casts the ranking task into a full-text format rather than using separate sentences. The scoring is done by masking words in the premise and using the model's predictions for those masked words as part of the score. 

The method is evaluated on commonsense reasoning datasets like COPA, Swag, HellaSwag, and CommonsenseQA. It produces strong baselines without any fine-tuning that are comparable to supervised approaches. When fine-tuning the scoring method, it provides more stable training and requires less annotated data than standard classifier approaches to reach equivalent performance. The paper shows empirically that the approach works well in low-data regimes. The scoring method and training procedure allow better reuse of the pre-trained model's knowledge compared to standard approaches that add a randomly initialized classifier.


## Summarize the main method used in the paper in one paragraph.

 The main method proposed in this paper is a new scoring approach for plausibility ranking tasks like commonsense reasoning. Instead of adding a randomly initialized classifier head on top of a pre-trained transformer model like BERT, they propose leveraging the masked language modeling head that is already trained during BERT pre-training. 

Specifically, they take a premise and hypothesis pair and concatenate them into a "full-text" format with conjunction words like "because" or "so" in between. Then they mask words from the premise one by one and compute the probability of predicting those masked words based on the full context, using the MLM head. The sum of the log probabilities for all the masked premise words gives a score representing the plausibility of the hypothesis completing the premise.

At test time, this scoring approach can be applied directly using the pre-trained MLM model, giving strong zero-shot performance on commonsense reasoning datasets like COPA. The method can also be fine-tuned on the task using a margin-based loss. The authors show this leads to higher accuracy and more stable training compared to standard classifiers, especially when less training data is available. Overall, the key novelty is exploiting the MLM head for scoring instead of a task-specific head, enabling effective zero-shot transfer and more efficient fine-tuning.
