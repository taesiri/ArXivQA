# [Pre-training Is (Almost) All You Need: An Application to Commonsense   Reasoning](https://arxiv.org/abs/2004.14074)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively leverage pre-trained masked language models like BERT for commonsense reasoning tasks. Specifically, it investigates whether directly using the pretrained masked language model is more effective than adding a randomly initialized classifier on top. The key hypothesis is that the pretrained model contains useful knowledge for commonsense reasoning tasks, so directly using the model's predictions on masked tokens can outperform simply using it as a feature extractor.The paper proposes a scoring method that casts commonsense reasoning tasks like COPA into a full-text format and leverages the MLM head tuned during pretraining. The authors hypothesize this will work better than fine-tuning a classifier on top of BERT embeddings. They test this on commonsense reasoning datasets like COPA, Swag, HellaSwag and CommonsenseQA.So in summary, the central research question is whether we can effectively tap into the knowledge already embedded in pretrained MLMs for commonsense reasoning instead of relying solely on fine-tuning an additional classifier. The hypothesis is that the proposed scoring method will outperform the classifier approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new scoring method for plausibility ranking tasks like commonsense reasoning. The key ideas are:- Casting the task into a full-text format rather than separate sentences. This allows better use of the pretrained masked language model.- Defining a scoring function that leverages the MLM head tuned during pretraining, rather than adding a randomly initialized classifier head. - Targeting the scoring on the premise rather than the hypothesis to avoid biases.- Using this scoring method without fine-tuning gives strong zero-shot performance.- Fine-tuning the model with this scoring method and a margin-based loss leads to higher accuracy and more stable training compared to standard classifier approaches.- The method works well even with small training data amounts, outperforming classifier methods significantly.So in summary, the main contribution is presenting a new way to formulate and solve plausibility ranking tasks by exploiting the pretrained MLM model more directly, which provides various benefits over the common methodology of using a randomly initialized classifier head.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper introduces a new scoring method for plausibility ranking tasks like commonsense reasoning. It shows that casting the task into a full-text format and leveraging the pretrained masked language modeling head leads to strong zero-shot performance and more stable and sample-efficient fine-tuning compared to standard classifier approaches.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research on commonsense reasoning and pre-trained language models:- The paper proposes a new scoring method for plausibility ranking tasks that uses the masked language modeling head of pre-trained BERT-like models. This allows leveraging the models' pre-training instead of only fine-tuning an added classifier head. - The scoring method frames the task in a full-text format rather than separated sentences and targets scoring of the premise over the hypotheses to avoid bias. This is a novel way of formulating plausibility ranking.- The paper shows strong zero-shot performance on commonsense reasoning datasets like COPA, comparing favorably to prior supervised approaches. This demonstrates the knowledge already embedded in pre-trained models.- When fine-tuning the proposed scoring method, it provides higher accuracy and more stable training than standard classifier approaches, especially in low data regimes. This could be useful for data-efficient fine-tuning.- The paper focuses on a subset of commonsense reasoning tasks around plausibility ranking. Other work has looked at different commonsense benchmarks or aspects, like pronoun resolution, causal reasoning, QA, etc.- The techniques build on prior work showing models can perform reasoning tasks in a zero-shot manner and that margin losses are useful for plausibility ranking. But the overall scoring approach and analysis are novel.- The paper only focuses on BERT-like models. Other work has explored different model architectures, knowledge bases, or incorporation of external knowledge to improve commonsense reasoning.So in summary, the paper introduces a new way of formulating and scoring plausibility ranking tasks to take better advantage of pre-trained language model knowledge, and provides empirical evidence of its benefits over standard approaches. The ideas could transfer to other types of reasoning benchmarks as well.


## What future research directions do the authors suggest?

The authors suggest several future research directions:- Applying the sequence scoring method to other classification tasks like natural language inference and sentiment analysis. They think the token-level scoring could be beneficial for these tasks as well.- Using the token-level scoring method during pre-training to extend traditional objectives like next sentence prediction and sequence ordering. This could inject more commonsense knowledge into the pre-trained models. - Exploring different ways to integrate the sequence scoring method into the loss function during fine-tuning. They used a simple margin loss in this work but think other approaches could further improve results.- Evaluating the robustness of the sequence scoring method to adversarial examples and biases in the data. This could reveal limitations of the approach.- Applying the method to other modalities like image-text tasks where both the text and image provide contextual information that could be leveraged in the scoring.- Exploring whether the sequence scoring approach leads to better few-shot and zero-shot transfer learning performance compared to standard fine-tuning.In summary, the main future directions are exploring the sequence scoring method in other tasks and settings, integrating it better into pre-training and fine-tuning, and analyzing its robustness. The authors believe this method has potential for many NLP applications.
