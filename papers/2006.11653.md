# [Towards Understanding Label Smoothing](https://arxiv.org/abs/2006.11653)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper tries to address is: How does label smoothing regularization (LSR) help improve the training of deep neural networks, and at what stage of training is it most useful? Specifically, the authors aim to understand the optimization benefits of using LSR when training deep nets with stochastic gradient descent. They theoretically analyze the convergence behavior of SGD with LSR and show it can help speed up convergence by reducing variance in the gradients. The paper then proposes a two-stage training strategy called TSLA that uses LSR in early epochs but drops it later in training. Through analysis and experiments, they find this improves convergence compared to always using or never using LSR.In summary, the central hypothesis is that LSR helps optimization, especially in early training, but may become less useful or even detrimental later. The two-stage TSLA strategy is proposed to gain the benefits of LSR while avoiding potential downsides in late training. The theoretical analysis and empirical results support this hypothesis.


## What is the main contribution of this paper?

This paper presents theoretical analysis and empirical results on using label smoothing regularization (LSR) when training deep neural networks with stochastic gradient descent. The main contributions are:- It provides convergence analysis showing that an appropriate amount of LSR can reduce the variance of stochastic gradients and speed up convergence of SGD to find an approximate stationary point of the non-convex loss function.- It proposes a two-stage training algorithm called TSLA that uses LSR in early training epochs but drops it off later. Theoretical analysis shows TSLA enjoys benefits of LSR for variance reduction early on and faster convergence without LSR later.- Empirical evaluations on image classification datasets demonstrate superior performance of the proposed TSLA algorithm compared to using SGD with or without LSR during the entire training.In summary, this is the first work providing theoretical justification for why LSR helps SGD training of neural nets. It also proposes a simple yet effective strategy of using LSR only during initial epochs, which is shown to outperform constant LSR or no LSR baselines. The analysis and algorithm together provide useful insights on how and when to use LSR.
