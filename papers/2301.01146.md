# [Rethinking Mobile Block for Efficient Attention-based Models](https://arxiv.org/abs/2301.01146)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is:"Can we build a lightweight Inverted Residual Block (IRB)-like infrastructure for attention-based models with only basic operators?"The key points are:- Inverted Residual Blocks (IRBs) are recognized as the core infrastructure for lightweight CNN models. However, there is no counterpart for attention-based models. - The paper aims to develop a lightweight IRB-like block for attention-based models, using only basic operators like depthwise convolutions and multi-head self-attention.- The goal is to integrate the efficiency of lightweight CNNs like MobileNet with the modeling capability of Transformers, resulting in an easy-to-use and high-performance mobile-friendly model.- To achieve this, the paper inductively abstracts a Meta Mobile Block that can instantiate IRB, Transformer blocks, etc. based on two parameters. - It then deduces an Inverted Residual Mobile Block (iRMB) from this meta block using efficient operators like depthwise conv and improved multi-head attention.- Finally, a lightweight attention-based model called Efficient MOdel (EMO) is built using only the iRMB blocks in a ResNet-like architecture.In summary, the core research question is about developing a MobileNet IRB-like infrastructure for attention models using basic building blocks, in order to get an efficient yet accurate mobile-friendly model. The iRMB and EMO are proposed to address this question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new building block called the Meta Mobile Block (MMB) for designing lightweight and efficient models. The MMB is an abstraction and unification of the Inverted Residual Block from MobileNets and the MHSA/FFN modules from Transformers. 2. Using the MMB, the authors deduce a novel Inverted Residual Mobile Block (iRMB) that only uses depthwise convolutions and an improved efficient window multi-head self-attention (EW-MHSA). 3. The iRMB is used to build a new lightweight model called Efficient MOdel (EMO) for computer vision tasks. EMO uses only iRMBs in a ResNet-like architecture.4. Extensive experiments on ImageNet, COCO, and ADE20K show EMO variants (EMO-1M to EMO-5M) achieve excellent performance compared to other lightweight CNN and Transformer models, with fewer parameters and FLOPs.5. The paper provides detailed ablation studies and analysis, offering insights into designing efficient attention-based models.In summary, the key contribution is the proposal of the MMB abstraction to unify efficient blocks from CNNs and Transformers, enabling the design of the new high-performance EMO model using only iRMBs. The simplicity yet effectiveness of EMO demonstrates the power of the MMB design principle.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a lightweight Meta Mobile Block infrastructure for attention-based models by extending the idea of CNN's efficient Inverted Residual Blocks, and uses it to build a simple yet effective Efficient MOdel (EMO) that achieves strong performance on image classification and dense prediction tasks with low parameters and FLOPs.
