# [ApiQ: Finetuning of 2-Bit Quantized Large Language Model](https://arxiv.org/abs/2402.05147)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Finetuning large language models (LLMs) like GPT-3 is very memory intensive due to the need to store the model weights, activations, and optimizer states in GPU memory. This poses challenges especially when using lower-end GPUs.
- Existing strategies like LoRA and QLoRA help reduce memory usage but suffer from inconsistent performance across different bit-widths and tasks. This is due to catastrophic forgetting stemming from the distortion of preserved knowledge in the LLM during aggressive quantization, which gets worse at lower bit-widths.

Proposed Solution:
- The paper proposes ApiQ, a novel quantization framework designed to restore lost information from quantization by concurrently initializing LoRA components and quantizing the LLM weights. 
- ApiQ minimizes the activation error rather than weight error during quantization by jointly optimizing the LoRA matrices A, B and quantized weights Q. This alignment of activations across layers mitigates error propagation.
- Quantization is done efficiently in a layer-wise manner using a gradient-based approach with straight-through estimator on a small calibration dataset.

Main Contributions:
- ApiQ ensures maintenance of original LLM's activation distribution and precision while quantizing the weights. This preserves the starting point for finetuning.
- It mitigates cumulative error propagation from shallower to deeper layers by consistently matching activations.
- Extensive experiments show ApiQ achieves superior and consistent finetuning performance across tasks and models especially for very low precision quantization of 2-3 bits.
- ApiQ also demonstrates promise as a competitive post-training quantization technique.

In summary, ApiQ introduces an activation preservation based quantization approach to enable effective and consistent finetuning of aggressively quantized LLMs across diverse bit-widths and tasks.
