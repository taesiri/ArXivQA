# [Network Pruning Spaces](https://arxiv.org/abs/2304.09453)

## What is the central research question or hypothesis that this paper addresses?

This paper explores the concept of "network pruning spaces" to study the structure of optimal subnetworks produced by pruning a large neural network. The key research questions/hypotheses are:1. There exists an optimal FLOPs-to-parameters ratio for a given level of pruning that leads to the best performing subnetworks.2. The best performing subnetworks tend to have pruning recipes that produce this optimal FLOPs-to-parameters ratio. 3. The optimal ratio and hence the maximum possible performance of the subnetworks can be predicted as a function of the level of pruning.4. Examining the distributions and ratios of FLOPs and parameters across populations of pruned subnetworks can reveal general principles about how to effectively prune neural networks.So in summary, the central hypothesis is that the optimal structure of pruned subnetworks, characterized by the FLOPs/parameters ratio, follows certain general principles that can be revealed through a "network pruning space" analysis across populations of pruned networks. The goal is to gain insight into what makes for effective pruning and improve pruning algorithms.
