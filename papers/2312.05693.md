# [Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs   on the Edge](https://arxiv.org/abs/2312.05693)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT-3 have very high computational and memory requirements, making it difficult to deploy them efficiently on edge devices. 
- Existing works focus primarily on weight quantization to 4-bits which helps with memory but still requires float16 activations, limiting speedups on edge hardware.  
- Quantizing activations is challenging as it introduces outliers that significantly hurt accuracy.

Proposed Solution:
- The paper proposes Agile-Quant, an activation-guided quantization framework to quantize both weights and activations of LLMs for faster inference on the edge.

Key ideas:
- Analyze token importance and activation outliers in LLMs to motivate quantization strategies.
- Propose basic activation quantization pipeline starting with 8-bits.
- Introduce activation-aware token pruning to minimize negative impact of outliers.
- Design two hardware optimizations: 
   - 4-bit SIMD multiplier to support efficient INT4 matrix multiplication
   - Efficient TRIP matrix multiplication to handle outliers

Contributions:
- Novel framework to quantize both weights and activations of LLMs to 4-8 bits.
- Maintain state-of-the-art accuracy compared to weight-only methods.
- Achieve up to 2.55x speedup on edge devices like phones and Raspberry Pis through hardware-aware optimization.
- Demonstrate effectiveness on popular LLMs like OPT, BLOOM and different LLaMA variants.

In summary, the paper enables efficient deployment of accurate LLMs on resource-constrained edge devices through a holistic approach of model and hardware co-design.
