# [BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis   via Bridging Image and Video Diffusion Models](https://arxiv.org/abs/2312.02813)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes BIVDiff, a training-free framework for general-purpose video synthesis by bridging image and video diffusion models. It first uses a task-specific image diffusion model (IDM) like ControlNet or InstructPix2Pix to generate a video frame-by-frame. Then it performs Mixed Inversion, mixing the latents from image and video DDIM inversion, to adjust the latent distribution. Finally, it inputs the inverted latents into a text-to-video diffusion foundation model (VDM) like VidRD for temporal smoothing. This decouples image and video models, allowing flexible IDM selection for different tasks. Experiments validate BIVDiff on controllable video generation, video editing, inpainting and outpainting. Quantitatively and qualitatively it compares well to baselines like Tune-A-Video and ControlVideo. Ablations study framework design choices like bridging strategies and the mixing ratio. The method is simple, efficient, and generalizable for various video synthesis tasks.
