# [Lights out: training RL agents robust to temporary blindness](https://arxiv.org/abs/2312.02665)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed one-paragraph summary of the key points from the paper:

In this paper, the authors propose a novel deep reinforcement learning method to train agents that are robust to temporary lack of state observations, or "blindness". They introduce a neural network architecture that encodes state observations into latent representations and uses an LSTM to make open-loop action selections based on those embeddings over multiple timesteps. Along with this, they contribute a specialized multi-step loss function to train the network to accurately predict future rewards and values after taking actions without access to updated state inputs. Through experiments in gridworld environments with different blindness masks, they demonstrate that their method can enable agents to reliably traverse blinded areas much longer than the lengths seen during training. Key results show the agent successfully switching between closed and open-loop control to solve mazes, as well as generalizing to blindness stretches over twice as long as the open-loop sequences it was trained on. While more extensive testing is needed, the method displays promise in improving robustness to temporary lack of observations in deep RL agents. The loss function specifically stands out for its ability to ground open-loop predictions to match closed-loop performance.
