# [JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings   for Retrieval: Early Technical Report](https://arxiv.org/abs/2312.16144)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Document retrieval is important for applications like retrieval-augmented generation (RAG), but current methods have limitations. Sparse methods like BM25 lack semantic understanding. Cross-encoder methods are expensive at scale. Dense methods require massive training data and struggle to capture all document aspects in one vector.

Solution:
- The paper introduces JaColBERT, a document retrieval model optimized for Japanese text. It is based on ColBERT architecture which represents documents as bags of vectors instead of a single vector.

- A hard negative augmented version of the Japanese MMArco dataset is released to help train retrieval models. Hard negatives that look similar to positives are generated using multilingual embeddings and BM25.

Contributions:
- Release of dataset with 10M Japanese hard negative passages for MMArco to support training retrieval models.

- Release of JaColBERT, a ColBERT-based model trained on this dataset. It outperforms prior Japanese retrieval models and competes with multilingual models that were trained on in-domain data.

- JaColBERT achieves average Recall@10 of 0.813 on benchmarks, beating prior Japanese models. This is achieved with a small Japanese-only dataset, showing promise for Japanese-first retrieval without massive multilingual data.

- Results highlight efficiency of ColBERT-style models. JaColBERT trains in 10 hours on 10M examples, yet competes with multilingual models trained on billions of examples. This greatly reduces data needs.

In summary, the paper presents a hard negative dataset and JaColBERT model to advance Japanese dense retrieval without reliance on other languages. Both dataset and model are released to support future work.
