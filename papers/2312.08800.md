# [Evaluating Large Language Models for Health-related Queries with   Presuppositions](https://arxiv.org/abs/2312.08800)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper evaluates the factual accuracy and consistency of large language models (LLMs) like InstructGPT, ChatGPT, and BingChat when answering health-related queries that contain varying degrees of presuppositions. The authors source a set of 1,779 verified health claims, pose them as questions to LLMs, and check if the LLM responses agree or disagree with claims using an entailment model. They find that while models rarely contradict true claims, they often fail to refute false ones - InstructGPT agrees with 32% of false claims, ChatGPT 26%, and BingChat 23%. As presuppositions in queries increase, agreements with claims also increase regardless of veracity, especially for InstructGPT and ChatGPT. When demands presume falsehoods and request evidence to support them, InstructGPT agrees 76% of the time, ChatGPT 62%, and BingChat 28%. Overall accuracy is low and consistency across presupposition levels is poor. The authors conclude that the inability of models to reliably correct false assumptions calls for careful assessment before deployment in high stakes scenarios like healthcare.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) like ChatGPT are integrated into search engines and other critical services, it is important that they provide factually accurate and robust responses. This is especially critical for health-related queries, where inaccurate information could cause harm. However, LLMs may fail to consistently refute false assumptions and could potentially reinforce misinformation. 

Methodology:
The authors evaluate the factual accuracy and consistency of InstructGPT, ChatGPT and BingChat on health-related queries with varying degrees of presupposition. They introduce a dataset called UPHILL with 8895 queries derived from fact-checked claims, with presupposition levels ranging from neutral to strong. They check if model responses agree/disagree with true/false claims using an entailment model.

Key Findings:
- Factual accuracy is concerningly low, with false claims supported in 26-32% of responses. Accuracy drops with increasing presupposition.  
- InstructGPT is most sensitive to presuppositions; BingChat most robust due to retrieval grounding.
- For strongest presupposition level demanding evidence for false claims, up to 76% InstructGPT responses, 62% ChatGPT responses and 28% BingChat responses agree to support the false claim.
- InstructGPT is least consistent (25%), ChatGPT moderately consistent (39%), BingChat most consistent (61%) across presupposition levels.

Main Contributions:  
- Quantify factual accuracy of LLMs for health queries and their robustness to presuppositions
- Introduce dataset UPHILL for evaluating presupposition handling 
- Analysis of different models shows concerning gaps in accuracy and consistency, calling for careful assessment before deployment in high-stakes scenarios

Let me know if you need any clarification or have additional questions on the summary!
