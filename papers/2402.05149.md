# [FlowPG: Action-constrained Policy Gradient with Normalizing Flows](https://arxiv.org/abs/2402.05149)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "FlowPG: Action-constrained Policy Gradient with Normalizing Flows":

Problem:
The paper addresses the challenge of action-constrained reinforcement learning (ACRL) where the agent's action at each time step should satisfy specified constraints. Using a projection layer to enforce constraints can be slow, get stuck in local optima, and suffer from zero gradients. Existing methods require solving an optimization program at each step.

Proposed Solution:
The key idea is to learn a mapping between a simple latent distribution (e.g. Gaussian) and the feasible action space using normalizing flows. The flow model transforms samples from the latent distribution into valid actions. This allows replacing the projection layer with the flow model to satisfy constraints. 

The authors propose multiple techniques to efficiently sample valid actions to train the flow model, including Hamiltonian Monte Carlo and probabilistic sentential decision diagrams. The trained normalizing flow is integrated with the DDPG algorithm by modifying the policy network to output a sample from the latent distribution, which is then mapped to a valid action via the flow model. This enables end-to-end training of policy parameters without requiring an optimization solver.

Main Contributions:
- Employ normalizing flows to learn mapping between a simple latent distribution and complex feasible action space
- Develop efficient sampling methods for non-convex action constraints 
- Integrate trained normalizing flow with DDPG while enabling end-to-end policy learning
- Empirically show superior performance over state-of-the-art on various continuous control tasks - significantly fewer constraint violations and faster training

The main advantage is avoiding optimization solvers during training and action execution while satisfying constraints. This results in faster training and reduced violations. The approach is general and can be integrated with other RL algorithms.
