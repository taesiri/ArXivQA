# [TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior prompt-based backdoor attacks require expensive full model fine-tuning or a large training dataset. They cannot achieve both high attack success rate (ASR) and high clean data accuracy (CDA) via few-shot prompt tuning.  
- Challenges in few-shot prompt backdoor attacks: (1) Imbalanced poisoned dataset issue where target class has more samples, hurting CDA (2) Overfitting due to high-dimensional prompt space (3) Lack of attention awareness between clean and poisoned samples.

Proposed Solution - TrojFSP:  
- Uses only 16-shot samples and freezes the language model parameters. Tunes a small set of prompt tokens.
- Target-Class Shrink (TC-Shrink): Dynamically reduces number of target class samples during data poisoning to balance classes.  
- Selective Token Poisoning: Only inserts Trojan into the least important prompt token based on expected sensitivity to combat overfitting.
- Trojan-Trigger Attention: Maximizes attention of poisoned token on triggers and minimizes it on clean samples.   

Main Contributions:
- First prompt backdoor attack via few-shot prompt tuning that keeps model frozen.
- Achieves over 97% ASR and less than 2% CDA loss across models and datasets. 
- Outperforms prior works by 9-48% higher ASR and 4-9% higher CDA in few-shot setting.
- Analyzes challenges in few-shot backdoor attacks and provides tailored solutions.

The paper raises awareness about emerging threat of few-shot prompt backdoor attacks and stimulates further research into defenses against such attacks.


## Summarize the paper in one sentence.

 This paper proposes TrojFSP, a new prompt-based backdoor attack against language models using few-shot prompt tuning, which achieves high attack success rates while minimizing clean data accuracy loss.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing TrojFSP, a new backdoor attack method against few-shot prompt tuning of pre-trained language models (PLMs). Specifically, the paper:

1) Identifies key challenges in launching effective backdoor attacks on few-shot prompt tuning, including the poisoned imbalance issue, overfitting, and lack of attention awareness. 

2) Proposes three techniques to address these challenges: Target-Class Shrink (TC-Shrink) to balance the poisoned data, Selective Token Poisoning to mitigate overfitting, and Trojan-Trigger Attention to improve attention allocation.

3) Demonstrates that TrojFSP achieves significantly higher attack success rates and higher clean data accuracy compared to prior prompt-based backdoor attacks across various PLMs and datasets. 

4) Discusses a potential defense method against TrojFSP and limitations of the current study, providing directions for future work.

In summary, the main contribution is the proposal and evaluation of TrojFSP, a novel and effective backdoor attack technique tailored for few-shot prompt tuning of PLMs. The paper provides new insights into security issues with few-shot prompt tuning.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- TrojFSP: The name of the backdoor attack method proposed in the paper for few-shot prompt tuning.

- Few-shot prompt tuning: Using a small number of examples to adapt a pre-trained language model to a new task by tuning prompt parameters while keeping the language model parameters frozen. Vulnerable to backdoor attacks.  

- Backdoor attack: An attack method where a model is poisoned so that it predictably misclassifies inputs containing a particular trigger.

- Target-class shrink (TC-Shrink): A technique proposed in the paper to balance the number of target class samples during trojan insertion to improve clean data accuracy. 

- Selective token poisoning: A technique proposed in the paper to poison only certain prompt tokens selected based on importance scores to mitigate overfitting.

- Trojan-trigger attention: An attention-based technique proposed to amplify attention on the trigger tokens.

So in summary, key terms cover the attack method itself (TrojFSP), the vulnerable tuning approach (few-shot prompt tuning), the general attack category (backdoor attack), and the techniques proposed in the paper to address challenges (TC-Shrink, selective token poisoning, trojan-trigger attention).


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a "Target-Class Shrink" technique to balance the poisoned dataset. Can you explain in detail how this technique works and why it is important for improving clean data accuracy? 

2. The selective token poisoning technique is used to mitigate overfitting. Why is overfitting a significant issue when generating backdoored prompts via few-shot prompt tuning? And how does selectively poisoning only certain tokens help address this?

3. What is the intuition behind using the L-infinity norm versus other norms like L1 in the Trojan-Trigger Attention loss function? How does the choice of norm impact optimizing attention allocation?

4. The paper demonstrates the effectiveness of TrojFSP across multiple models like RoBERTa and T5. What architectural differences between these models, if any, necessitated modifications in the attack methodology? 

5. What tradeoffs, if any, exist between achieving a high attack success rate versus a high clean data accuracy when constructing a backdoored prompt? How does TrojFSP balance these two objectives?

6. The threat model assumes an attacker has access to the target model and a small dataset. What are some real-world scenarios where this threat model would be applicable?  

7. How does the technique for generating syntactic triggers used in this paper differ from other trigger generation methods? What are the advantages of syntactic triggers?

8. The authors propose a potential defense by pruning unimportant prompt tokens. What are the limitations of this defense approach? How might it be circumvented?

9. What modifications would be required to apply TrojFSP to conditional text generation tasks rather than text classification tasks evaluated in the paper?

10. The authors note overfitting as a key challenge. What other challenges exist in generating effective backdoored prompts via few-shot tuning, and how does TrojFSP address them?
