# [SelfAugment: Automatic Augmentation Policies for Self-Supervised   Learning](https://arxiv.org/abs/2009.07724)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is: How can we evaluate self-supervised representation learning models and select effective data augmentation policies when labeled data is not available?The key hypotheses are:1) A self-supervised image rotation prediction task can serve as an effective proxy for evaluating representations learned via self-supervised contrastive learning, without requiring labels.2) Optimization algorithms can leverage this self-supervised evaluation to automatically select good augmentation policies for contrastive representation learning.In summary, the paper investigates using rotation prediction as an unsupervised way to evaluate and guide data augmentation selection for self-supervised learning. This removes the need for labeled data during unsupervised training.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a fully unsupervised method for evaluating and selecting data augmentation policies for self-supervised learning. The key points are:- They show that a self-supervised image rotation prediction task is highly correlated (Spearman rank correlation > 0.94) with standard supervised evaluations across datasets, tasks, architectures, and augmentation policies. This allows rotation prediction to be used as an evaluation metric without requiring labels.- Using rotation prediction, they adapt two data augmentation selection algorithms from the supervised domain (RandAugment and Fast AutoAugment) to work in a fully unsupervised manner. - Their unsupervised "SelfAugment" method finds augmentation policies that match or exceed the performance of policies found by exhaustive supervised search, while using a fraction of the compute time.- They demonstrate the effectiveness of SelfAugment across CIFAR-10, SVHN, ImageNet, Pascal VOC, COCO, and Places datasets on linear classification, detection, segmentation, and few-shot transfer tasks.In summary, the key contribution is presenting an unsupervised pipeline for representation learning that removes the need for labels during training, evaluation, and augmentation policy selection. This could enable more effective self-supervised learning when labels are unavailable.
