# [Learn To be Efficient: Build Structured Sparsity in Large Language   Models](https://arxiv.org/abs/2402.06126)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) like GPT-3 and OPT have achieved remarkable success but also incur extremely high inference costs due to their billion-scale parameters. Existing methods utilize the naturally occurring activation sparsity in LLMs to accelerate inference, but overlook the potential to further amplify this inherent sparsity. Moreover, they require replacing soft activations with ReLU, which can hurt performance.  

Proposed Solution:
This paper proposes Learn-To-Be-Efficient (LTE), a novel training algorithm that enables LLMs to learn structured sparsity awareness for efficient inference. LTE trains an additional efficiency loss to encourage models to activate fewer neurons. It also employs a threshold-based sigmoid routing strategy and a two-stage training process to train routers more stably. Unlike prior works, LTE works for both ReLU and soft-activation models.

Key Contributions:
- Propose LTE, a new method to train efficiency-aware LLMs with more structured sparsity for fast inference
- Introduce an efficiency loss term that enables models to activate fewer neurons with marginal accuracy drops
- Design a threshold-based sigmoid routing for flexible expert selection instead of a fixed number
- Achieve 1.83-2.59x FLOPs reductions on LLaMA via LTE, outperforming state-of-the-arts

In summary, this paper presents Learn-To-Be-Efficient to effectively build structured sparsity in LLMs by training them to be efficiency-aware, achieving better trade-offs between accuracy and inference efficiency. Evaluations on various datasets and models demonstrate the efficiency improvement of LTE over state-of-the-art methods.
