# [The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel   Size might be All You Need](https://arxiv.org/abs/2312.05695)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent self-supervised learning (SSL) methods show that Transformer architectures consistently outperform CNNs. This leads to a belief that self-attention in Transformers is crucial for the advances in SSL. However, most prior works use standard ResNets as the CNN backbones which have inferior designs compared to advanced Transformers like Swin Transformer. Therefore, it is unclear whether the performance gap is due to self-attention or the CNN architectures being outdated. This paper aims to investigate whether advanced CNN designs can match Transformers in SSL.

Method: 
The paper builds upon ConvNeXt, a recently proposed CNN architecture with design principles similar to Swin Transformer. Two modifications are made: (1) Adding batch normalization layers after depthwise convolutions in the backbone. (2) Increasing the kernel size from 7x7 to 9x9. The resulting architecture is called Big ConvNet for Self-Supervised Learning (BC-SSL). Experiments are done using DINO self-supervised framework.

Contributions:
1) BC-SSL with 9x9 kernels matches or outperforms Swin Transformer in ImageNet linear classification and k-NN evaluation, with 40% higher throughput. 

2) When transferred to downstream tasks, BC-SSL significantly outperforms both self-supervised and supervised Swin Transformers on COCO detection and segmentation, showing much better transferability.

3) BC-SSL shows monotonically increasing robustness with larger kernels, convincingly beating Swin Transformer on multiple robustness benchmarks.

4) Visualizations indicate larger kernels capture more context, leading to better localization of objects.

In summary, properly designed CNNs can match Transformers in SSL without self-attention. This highlights the promise of simple convolutional designs even in the era of Transformers. The paper provides SOTA CNN baselines for future SSL research.
