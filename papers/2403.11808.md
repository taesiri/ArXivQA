# [Dynamic Tuning Towards Parameter and Inference Efficiency for ViT   Adaptation](https://arxiv.org/abs/2403.11808)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing methods for Vision Transformer (ViT) adaptation focus on improving parameter efficiency during fine-tuning by updating only a small subset of parameters. However, they overlook the inference efficiency after adaptation. This poses challenges when deploying large ViTs to downstream tasks, especially when the model is computationally demanding.

Proposed Solution: 
The paper proposes Dynamic Tuning (DyT), a new approach to improve both parameter and inference efficiency for ViT adaptation. The key ideas are:

1) Insert lightweight adapter modules into ViT to maintain parameter efficiency.

2) Design a token dispatcher before each transformer block to distinguish informative tokens from redundant ones. Only informative tokens are processed by the original transformer block and adapter, while redundant tokens skip the block and are only processed by the adapter. This reduces computation at inference time.

3) Explore multiple variants (e.g. skipping tokens in attention or MLP blocks) to find the best practice of DyT.

4) Introduce a Mixture-of-Experts adapter to further enhance adaptation capability.

Main Contributions:

1) DyT achieves superior parameter efficiency over full fine-tuning and competitive accuracy compared with state-of-the-art methods on VTAB-1K benchmark, while only requiring 71%-85% FLOPs.

2) Experiments on image classification, video recognition and semantic segmentation show DyT's efficiency generalizes across domains and tasks. For example, on ADE20K segmentation, DyT surpasses full-tuning accuracy with 21 GFLOPs reduction.

3) Analysis on the impact of dispatch variants, adapter designs, and visualizations offer new insights into efficient ViT adaptation.

In summary, this work unifies parameter and inference perspectives for efficient ViT fine-tuning, achieving significant efficiency improvements with competitive accuracy across vision tasks. The design and evaluations of DyT provide a promising direction for efficient model deployment.
