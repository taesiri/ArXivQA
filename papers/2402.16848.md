# [InterroGate: Learning to Share, Specialize, and Prune Representations   for Multi-task Learning](https://arxiv.org/abs/2402.16848)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Multi-task learning (MTL) aims to jointly train a single model on multiple related tasks. This can improve accuracy and data efficiency by sharing representations across tasks. However, MTL often suffers from task interference, where optimizing one task degrades performance of other tasks. Existing solutions either balance task losses during training, or manually design model architectures to allocate some task-specific parameters. However, these methods focus mostly on accuracy while neglecting computational efficiency critical for real-world deployment.

Proposed Solution: 
The paper proposes InterroGate, a novel MTL architecture to mitigate task interference while optimizing for inference efficiency. It employs learnable gates to dynamically balance shared and specialized parameters per layer and task. The gate selections become static post-training, enabling pruning away unused parameters and collapsing the model into a simplified architecture. A sparsity regularization loss allows controlling trade-off between accuracy and efficiency.  

Key Contributions:
- Novel gating mechanism for flexible sharing-specialization between tasks, reducing interference while retaining cross-task knowledge transfer
- Gates automatically find optimal patterns allocating specialized vs shared computation budget per task and layer  
- Computational budget can be directly controlled via sparsity regularization
- Static post-training architecture optimized for efficient inference via pruning
- Demonstrates state-of-the-art trade-off between accuracy and efficiency on MTL benchmarks with varied backbones

In summary, the paper introduces an MTL architecture with learnable gates that dynamically balance parameter sharing across tasks during training. It then simplifies into an efficient static architecture post-training by pruning redundant parameters based on learned sharing patterns. A key benefit is the ability to directly control inference computation budget while retaining accuracy.
