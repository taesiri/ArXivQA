# [RegFormer: An Efficient Projection-Aware Transformer Network for   Large-Scale Point Cloud Registration](https://arxiv.org/abs/2303.12384)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes an end-to-end transformer network called RegFormer for large-scale point cloud registration. The central goal is to develop an accurate and efficient registration method that can directly process raw point clouds without any post-processing like RANSAC or explicit correspondences. 

The key research questions/hypotheses addressed are:

- How to design an efficient transformer architecture that can handle large numbers of unstructured 3D points for registration? They propose a projection-aware hierarchical transformer with linear complexity.

- How to effectively reduce mismatches and increase robustness to outliers for distant point cloud pairs? They introduce a Bijective Association Transformer module with cross-attention and all-to-all point gathering.

- Can a pure transformer network achieve state-of-the-art registration performance on large-scale outdoor datasets without any post-processing? Their experiments on KITTI and NuScenes datasets demonstrate competitive or superior results compared to previous methods in terms of both accuracy and efficiency.

In summary, the central goal is developing an end-to-end transformer for efficient and robust large-scale point cloud registration, which is validated through comprehensive experiments and ablation studies. The key novelty lies in the proposed network architecture designs to handle challenges like sparsity, outliers and mismatches for this task.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes RegFormer, an end-to-end transformer network for large-scale point cloud registration. RegFormer is both keypoint-free and RANSAC-free.

2. It introduces a projection-aware hierarchical transformer to extract point features globally and filter outliers effectively. This avoids relying on local descriptors or robust estimators like RANSAC. 

3. It designs a Bijective Association Transformer (BAT) module to reduce mismatches. BAT combines cross-attention for preliminary information exchange and an all-to-all point correlation strategy to find reliable matching points.

4. Experiments on KITTI and NuScenes datasets show RegFormer achieves state-of-the-art registration performance in terms of both accuracy and efficiency. It obtains 99.8% and 99.9% successful registration recall on KITTI and NuScenes respectively.

In summary, the key contribution is proposing an efficient, end-to-end transformer network RegFormer for large-scale point cloud registration. RegFormer eliminates the need for explicit keypoint matching or RANSAC post-processing. The global modeling of transformer and mismatch rejection strategy of BAT module enable RegFormer to achieve high registration accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes RegFormer, an end-to-end transformer network for large-scale point cloud registration that achieves high accuracy and efficiency by extracting global features to filter outliers and using a bijective association transformer to reduce mismatches.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of point cloud registration:

- Overall Approach: The end-to-end transformer architecture for point cloud registration is novel. Most prior works use a combination of CNNs and transformer, or rely on establishing explicit correspondences. The pure transformer approach in this paper is efficient and eliminates the need for correspondence estimation.

- Use of Projection: Projecting the 3D point clouds onto a 2D cylindrical surface is an effective way to structure the data for the transformer. The projection mask to deal with invalid pixels is also a nice technique. This allows processing of large unstructured point clouds.

- Global Modeling: Leveraging the global modeling capability of transformers for outlier rejection is clever. This avoids the need for post-processing steps like RANSAC for robustness. The comparisons show this gives better accuracy than local feature methods.

- Association Module: The Bijective Association Transformer for reducing mismatches is a key contribution. Using cross-attention and the all-to-all point gathering gives better performance than nearest neighbors.

- Results: The method achieves state-of-the-art results on KITTI and NuScenes datasets in terms of accuracy, robustness and efficiency. It handles large outdoor scenes better than prior indoor registration works.

- Limitations: The approach relies on having a structured 360 degree scan from a LiDAR. It may not work as well on partial scans or indoor scenes. More analysis on different scene types could be useful.

Overall, I think the paper makes excellent contributions in developing an efficient end-to-end transformer approach for large-scale point cloud registration. The innovations in projection, attention and association all help improve performance. It advances the state-of-the-art in accuracy and efficiency on this task.
