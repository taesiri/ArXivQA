# [Fair Active Ranking from Pairwise Preferences](https://arxiv.org/abs/2402.03252)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies the problem of actively and fairly ranking a set of n items by making adaptive pairwise comparison queries. 
- The items belong to disjoint groups based on sensitive attributes like race, gender etc.
- Existing ranking algorithms optimize for overall accuracy but can be unfair by concentrating errors on minority groups. 
- The paper proposes optimizing a new cascaded error metric that first aggregates errors within groups using lp norm and then across groups using lq norm. This allows controlling both within group and across group fairness.

Proposed Solution: 
- Paper defines the Probably Approximately Correct and Fair (PACF) ranker which should output a ranking with high probability such that the proposed cascaded error is below a threshold epsilon.

- Two classes of algorithms are studied: group-blind algorithms without access to group labels, and group-aware algorithms with access.

- For group-blind case, paper shows an existing algorithm can be adapted and proves tight sample complexity bounds. 

- For group-aware case, new merging based algorithm is proposed and sample complexity bounds are proved.

Main Contributions:
- Defines a flexible cascaded error metric that generalizes different notions of fairness by parametrizing p and q.

- Provides optimal sample complexity bounds for group-blind case.

- Proposes new group-aware algorithm and shows improved performance empirically.

- Comprehensive experiments validate superior accuracy and fairness of group-aware algorithm on real and synthetic datasets. 

In summary, the paper provides optimal theoretical guarantees and practical algorithms for the novel problem of adaptively and fairly ranking items based on noisy pairwise feedback.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper studies the problem of efficiently and fairly ranking a set of items by adaptively asking for pairwise comparisons, proposing algorithms for both when group information is available and unavailable, as well as providing matching sample complexity upper and lower bounds.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1) It introduces a new fair ranking metric that generalizes previous metrics by allowing flexible definitions of error within and across groups (Definition 2). This allows tailoring to different fairness preferences.

2) It studies the probably approximately correct and fair (PACF) ranking problem under this new metric, considering both group-blind and group-aware algorithms. 

3) For group-blind algorithms, it provides nearly matching upper and lower bounds on the sample complexity.

4) For group-aware algorithms, it provides an optimal algorithm and proves a reasonable lower bound for a restricted class of algorithms. 

5) It empirically evaluates the algorithms on real-world and synthetic data, showing the group-aware algorithm has better performance and that the algorithms achieve proportional errors across groups as expected.

In summary, the paper proposes a new flexible fair ranking metric, provides theoretical sample complexity results for learning to rank under this metric, and empirically demonstrates the usefulness of making algorithms group-aware.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Fair ranking
- Probably approximately correct (PAC) learning
- Pairwise comparisons
- Plackett-Luce model
- Sample complexity bounds
- Group-aware algorithms
- Group-blind algorithms
- Cascaded norm objectives
- Equal errors vs proportional errors across groups
- Real-world experiments (COMPAS, German Credit datasets)

The paper studies the problem of actively and fairly ranking a set of items by adaptively requesting pairwise comparisons between items. The goal is to find a ranking that minimizes errors across different groups, in order to avoid unfair outcomes for minority groups while being probably approximately correct. Key aspects include theoretically analyzing sample complexity bounds, designing group-aware and group-blind ranking algorithms, using cascaded norm objectives to allow flexible error criteria, and empirically evaluating the algorithms on real-world datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1) The paper defines a new fair ranking metric called $(\epsilon,p,q)$-Best Fair Ranking. How does this metric generalize previous definitions of fair rankings? What new capabilities does it provide?

2) The paper studies both group-blind and group-aware algorithms. What are the key differences in assumptions and approach between these two cases? What are the tradeoffs in sample complexity bounds?

3) For the group-blind algorithm, the paper provides matching upper and lower bounds on sample complexity. Walk through the key ideas used to prove the lower bound. What makes constructing the hard instance for this problem challenging?  

4) For the group-aware algorithm, there is a gap between the upper and lower bounds on sample complexity. What are some ideas proposed to close this gap? What restrictions were placed on the algorithm class for the lower bound result?

5) The group-aware algorithm has two main steps - finding group-wise rankings, and then merging the rankings. Explain how each step works, including choice of parameters. What drives the sample complexity of each step?

6) How does the algorithm deal with weighting groups differently, via the weight parameters $w(h)$? How does the choice of these parameters allow capturing different fairness notions like equal or proportional errors?

7) The experiments compare the group-blind and group-aware algorithms across different real datasets. Summarize the key practical advantages demonstrated by the group-aware algorithm. How do trends vary across choice of $p,q$?

8) The paper argues that previous fair ranking definitions have limitations in how they distribute error across groups. Use the illustrative example provided (Figure 1) to walk through limitations of previous work, and how the proposed metric addresses them.  

9) The proposed fair ranking definition is related to metric clustering objectives. Explain the analogy made to fair clustering problems, and how ideas are translated over to the ranking domain. What general techniques are leveraged?

10) The paper leaves open questions around alternative choice models beyond Plackett-Luce. What other models are suggested? What additional challenges might arise in extending this approach to other models? How could analysis change?
