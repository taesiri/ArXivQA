# [A Transformer Model for Symbolic Regression towards Scientific Discovery](https://arxiv.org/abs/2312.04070)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new Transformer model architecture tailored for symbolic regression, with a focus on scientific discovery applications. Three different encoder designs are introduced and analyzed - MLP, Att, and Mix - with the Mix encoder showing superior performance by allowing more flexible interactions between variables, albeit at the cost of losing column permutation equivariance. The model is trained on a large, synthetic dataset of mathematical expressions and corresponding numerical tabular data. Evaluation on the SRSD benchmark datasets demonstrates state-of-the-art performance using the normalized tree edit distance metric. Specifically, the Mix encoder Transformer model outperforms prior state-of-the-art methods on the medium and hard difficulty subsets of SRSD. The Transformer model benefits from almost instantaneous inference after training, providing an accuracy versus efficiency profile well-suited for symbolic regression. The work provides a strong baseline for future research, with opportunities highlighted including generating more diverse training data and exploring bigger model capacities. Code and models have been open-sourced to support further advances in this direction.
