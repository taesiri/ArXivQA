# [Fast and Unified Path Gradient Estimators for Normalizing Flows](https://arxiv.org/abs/2403.15881)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Normalizing flows are powerful generative models that can approximate complex target distributions. They are used in physics and other sciences to model phenomena where the probability density function is known up to a normalization constant.
- Training normalizing flows typically relies on two objectives - (1) reverse KL, which uses self-sampling and (2) forward KL, which uses samples from the target distribution. Both are needed for effective training.
- Computing gradients for these objectives can be challenging. Standard gradient estimators often have high variance which slows down training. 
- Recently, path gradient estimators have been proposed which have lower variance but are computationally more expensive by a factor of 2-5x. They also cannot be applied efficiently to maximum likelihood training or flows that require numerical inversion.

Proposed Solution
- This paper introduces a fast, unified path gradient estimator that works for both reverse and forward KL objectives.
- For reverse KL, they derive a recursive formula to compute path gradients alongside the sampling process. This avoids costly numerical inversions and reduces overhead to only 40% compared to standard gradients.
- For forward KL, they show path gradients correspond to reverse KL on the pullback density. This allows leveraging their efficient reverse KL estimator.
- Their estimator works for both explicitly and implicitly invertible flows, unlike prior path gradient methods. It scales to large systems and can be computed with low memory overhead.

Main Contributions
- A path gradient estimator that is 1.5-8x faster than previous estimators and works for all common normalizing flow architectures
- Demonstration that the same estimator can be used for maximum likelihood training by formulating it as a reverse KL problem
- Empirical evaluation showing faster convergence and lower variance across multiple target distributions from machine learning and physics
- Unified perspective linking path gradients for reverse and forward KL as well as continuous and discrete normalizing flows

The proposed innovations significantly improve the practical applicability of path gradients for normalizing flow training across application domains.
