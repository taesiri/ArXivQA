# [EVE: Efficient Vision-Language Pre-training with Masked Prediction and   Modality-Aware MoE](https://arxiv.org/abs/2308.11971)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we build an efficient and scalable vision-language foundation model with a simple yet effective architecture and training approach?The key hypotheses appear to be:1) A unified architecture with shared attention and modality-aware mixture-of-experts can effectively encode and fuse different modalities.2) Using a single unified pre-training task of masked signal modeling (masked image and language modeling) can significantly simplify and accelerate training compared to using multiple objectives like contrastive learning. 3) The combination of the unified architecture and training approach will result in a vision-language model that is easy to scale up and achieves strong performance on downstream tasks despite the simplicity.In summary, the main research direction is developing an efficient and scalable vision-language foundation model through architectural and training simplification while maintaining or improving performance. The hypotheses focus on using a unified multimodal Transformer with mixture-of-experts and unified masked signal modeling pre-training to achieve this goal.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors propose EVE, an efficient vision-language foundation model with a unified architecture and pre-training task. EVE uses a unified multimodal Transformer with shared attention and Modality-Aware Mixture-of-Experts (MoE). It is pre-trained with only masked signal modeling on raw image-text signals. 2. EVE incorporates Modality-Aware MoE modules in the Transformer to better capture modality-specific information while fusing different modalities in a unified manner. The proposed modality routing technique helps the router select appropriate experts.3. The masked signal modeling technique unifies masked image and language modeling into a single pre-training objective, reconstructing masked raw signals from visible signals. This simplifies pre-training, accelerates training, and improves scalability.4. Extensive experiments show that EVE achieves state-of-the-art performance on various downstream vision-language tasks including VQA, NLVR2, and image-text retrieval. EVE also demonstrates improved training speed and scalability compared to methods involving multiple pre-training objectives.In summary, the core contribution is an efficient and unified vision-language foundation model with a simple yet effective architecture and pre-training approach, which achieves strong performance while being easy to train and scale up. The simplicity of EVE enables better scalability and faster training speed.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes EVE, an efficient vision-language foundation model with a unified architecture using shared attention and modality-aware Mixture-of-Experts, pre-trained with masked signal modeling on raw image pixels and text tokens to reconstruct masked portions, achieving state-of-the-art performance while accelerating training by 3.5x.
