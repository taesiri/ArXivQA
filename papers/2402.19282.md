# [WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset](https://arxiv.org/abs/2402.19282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Constructing large-scale pre-training datasets for language models requires vast amounts of high-quality data. Common Crawl provides a large web-crawled dataset but contains inconsistent quality data like duplicates, errors, unsafe content etc. which are not useful for model training.  

Proposed Solution:
- Designed a comprehensive pipeline to process Common Crawl data:
    - Extracted 68 billion English docs 
    - Heuristic rule filtering to remove errors
    - Fuzzy deduplication using LSH to remove 90% duplicate data
    - Safety filtering using classifiers to remove toxic, pornographic content and PII masking
    - Quality filtering using BERT classifiers to remove ads, non-fluent content
- Obtained 2.22T tokens of safe data, selected 1T tokens as high quality data for WanJuan-CC dataset

Main Contributions:
- Open-sourced 300B tokens from WanJuan-CC dataset along with statistics related to data quality to help users select appropriate data
- Safety: Used Perspective API to show WanJuan-CC has lower toxicity than other datasets  
- Utility: Perplexity and downstream task accuracy better than RefinedWeb dataset
- Significant contribution for large language model research by providing vast, safe, high-quality open dataset processed from Common Crawl data

Let me know if you would like me to elaborate on any part of the summary further. I aimed to capture the core aspects and contributions clearly and concisely.


## Summarize the paper in one sentence.

 This paper presents WanJuan-CC, a safe and high-quality open-source English web text dataset derived from Common Crawl data through a comprehensive pipeline involving extraction, filtering, deduplication, and quality control.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. The authors designed and implemented a comprehensive process for handling Common Crawl data, including steps like data extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. 

2. From 68 billion original English documents in Common Crawl, they extracted 2.22 trillion tokens (9.71TB) of safe data, and selected 1 trillion tokens (4.45TB) of high-quality data as the WanJuan-CC dataset. They have open-sourced 300 billion tokens from this dataset.

3. The paper provides statistical information related to data quality in the dataset, allowing users to select appropriate data based on their needs. 

4. Experiments show that models trained on WanJuan-CC achieve better perplexity on validation sets and higher accuracy on downstream tasks compared to models trained on another dataset RefinedWeb. This demonstrates the higher quality of the WanJuan-CC dataset.

In summary, the main contribution is the construction of the large-scale WanJuan-CC dataset by processing Common Crawl data, and the demonstration of its safety, high quality, and utility through extensive experiments.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- WanJuan-CC dataset: The safe and high-quality open-sourced English webtext dataset derived from Common Crawl that is presented in the paper. 

- Data processing pipeline: The comprehensive process designed by the authors to handle Common Crawl data, including steps like extraction, heuristic rule filtering, deduplication, safety filtering, and quality filtering.

- Data safety: Ensuring the dataset does not contain toxic, pornographic, or personally identifiable information that could lead to unsafe model behaviors.

- Data quality: Enhancing the dataset to contain useful textual content that can effectively improve model performance, by filtering out things like advertisements and non-fluent text.

- Model evaluation: Assessing the utility of the dataset by training language models on it and evaluating perplexity on validation sets as well as accuracy on downstream tasks.

- Common Crawl: The freely available web crawl database that served as the original source of raw web page data for constructing the WanJuan-CC dataset.

- Pre-training data: The large amounts of high-quality data needed for pre-training powerful language models, which the WanJuan-CC dataset helps provide.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using a comprehensive process for handling Common Crawl data that includes several steps like extraction, filtering, deduplication etc. Can you elaborate more on why this specific order of steps was chosen in the pipeline? What would be the impact of changing this order?

2. In the heuristic rule filtering stage, you have augmented the rules from Gopher and C4 with additional rules based on deeper observation of data. Can you provide some examples of new rules you introduced and what specific issues they aim to address? 

3. For deduplication, you chose an LSH based method that eliminated over 90% of data as compared to 50% removal rate by other methods. What modifications did you make in the LSH implementation that resulted in far higher duplication detection?

4. The content safety filtering employs both blacklist based blocking as well as BERT models for classification. What were some challenges faced in creating high quality training datasets for fine-tuning these models? How did you overcome annotation noise?

5. Can you elaborate on the rationale behind choosing to filter advertisements and text fluency specifically in the quality filtering stage? What unique issues do these represent in Common Crawl data?

6. You have open sourced 300B tokens from the 1T high quality dataset. What criteria was used to select this subset? Is it representative of the overall data distribution w.r.t. quality signals? 

7. The data quality assessment mechanism employs multiple evaluation methods. When do the results from these methods diverge and how are the thresholds updated in such cases?

8. How frequently is the quality evaluation and feedback loop implemented as new Common Crawl dumps are released? Does it require retraining of filtering models?

9. The advantage of your pipeline is combining both rule based and model based filtering. What are some limitations of relying entirely on rules? When would you suggest introducing model based filtering?

10. A key contribution is the comprehensive data quality analysis presented. If another research group were to utilize this data, what specific quality attributes would you suggest them to further measure for their use case?
