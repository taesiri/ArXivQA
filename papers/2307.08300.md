# [ShiftNAS: Improving One-shot NAS via Probability Shift](https://arxiv.org/abs/2307.08300)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the performance of one-shot neural architecture search (NAS) by adjusting the sampling strategy during supernet training?

Specifically, the paper hypothesizes that:

- The commonly used uniform sampling strategy in one-shot NAS leads to insufficient training of subnets with extreme (very small or very large) computational complexity. This is because uniform sampling concentrates training on subnets with intermediate complexity.

- Adjusting the sampling probability during training based on the training sufficiency of subnets with different complexities can improve the performance of the final searched architecture.

To test this hypothesis, the paper proposes ShiftNAS, which:

- Quantifies the training sufficiency of subnets using their performance variation. Subnets with higher variation are sampled more frequently.

- Uses an architecture generator to efficiently obtain subnets with a desired complexity during training.

- Allows end-to-end training of the sampling probabilities and architecture generator along with the supernet.

The central hypothesis is that by shifting sampling probability towards insufficiently trained subnets, ShiftNAS can improve one-shot NAS performance without additional retraining costs. Experiments on ImageNet classification validate this hypothesis and demonstrate state-of-the-art results.

In summary, the key research question is whether a learnable, complexity-aware sampling strategy can improve one-shot NAS performance over uniform sampling. The paper proposes and validates the ShiftNAS method to address this question.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this paper are:

1. Proposing ShiftNAS, a novel one-shot neural architecture search (NAS) method that can dynamically adjust the sampling probability during supernet training to focus more on less trained subnets. 

2. Designing an architecture generator (AG) that can efficiently generate subnet architectures satisfying desired resource constraints. The AG can be trained jointly with the supernet in an end-to-end manner.

3. Introducing a "probability shift" mechanism that evaluates the training sufficiency of subnets and shifts the sampling probability distribution to sample more of the less trained subnets. This allows for more effective training resource allocation.

4. Achieving state-of-the-art results on multiple vision models including CNNs and Vision Transformers on ImageNet classification. The method is shown to be model-agnostic.

5. Demonstrating that the subnets found by ShiftNAS can directly inherit weights from the supernet without needing additional retraining or fine-tuning. This eliminates the extra search costs of some other NAS methods.

In summary, the core ideas are using a learnable sampling probability distribution to focus training on insufficiently trained subnets, along with a differentiable architecture generator to efficiently produce subnets satisfying resource constraints. Together these contributions enable effective one-shot NAS without incurring additional search or retraining costs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes ShiftNAS, a method to improve one-shot neural architecture search by dynamically adjusting the sampling probability of subnets during supernet training based on their training sufficiency, and using an LSTM-based architecture generator to efficiently obtain subnets with desired complexity constraints.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in neural architecture search:

- This paper focuses on improving one-shot NAS methods. One-shot NAS aims to find optimal architectures by training a supernet only once and inheriting weights, reducing overall search cost. This is compared to traditional NAS methods that require training each candidate model separately.

- The main contribution is proposing ShiftNAS, which dynamically adjusts the sampling probability during supernet training based on subnet complexity. This helps allocate training resources more efficiently compared to uniform sampling. 

- ShiftNAS uses an LSTM-based architecture generator to efficiently sample subnets meeting the complexity constraints. This enables end-to-end joint optimization of the generator and supernet.

- Experiments show ShiftNAS achieves state-of-the-art results on ImageNet using both CNN and ViT search spaces. The ability to work with different model types makes it more flexible than methods tailored to a specific architecture.

- Compared to other one-shot NAS methods like AutoFormer and AttentiveNAS, ShiftNAS reduces the performance gap between supernet training and final subnets without extra search costs. The learnable sampling strategy is a key difference.

- Unlike some weight sharing approaches, ShiftNAS does not require additional retraining or fine-tuning of the selected subnets. This further improves efficiency.

In summary, this paper pushes one-shot NAS forward by addressing the limitation of uniform sampling through a learnable sampling strategy and architecture generator. The end-to-end joint optimization and flexibility across model types are also advantages compared to prior arts. Reducing the performance gap without extra search costs is a notable improvement.
