# [ShiftNAS: Improving One-shot NAS via Probability Shift](https://arxiv.org/abs/2307.08300)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the performance of one-shot neural architecture search (NAS) by adjusting the sampling strategy during supernet training?

Specifically, the paper hypothesizes that:

- The commonly used uniform sampling strategy in one-shot NAS leads to insufficient training of subnets with extreme (very small or very large) computational complexity. This is because uniform sampling concentrates training on subnets with intermediate complexity.

- Adjusting the sampling probability during training based on the training sufficiency of subnets with different complexities can improve the performance of the final searched architecture.

To test this hypothesis, the paper proposes ShiftNAS, which:

- Quantifies the training sufficiency of subnets using their performance variation. Subnets with higher variation are sampled more frequently.

- Uses an architecture generator to efficiently obtain subnets with a desired complexity during training.

- Allows end-to-end training of the sampling probabilities and architecture generator along with the supernet.

The central hypothesis is that by shifting sampling probability towards insufficiently trained subnets, ShiftNAS can improve one-shot NAS performance without additional retraining costs. Experiments on ImageNet classification validate this hypothesis and demonstrate state-of-the-art results.

In summary, the key research question is whether a learnable, complexity-aware sampling strategy can improve one-shot NAS performance over uniform sampling. The paper proposes and validates the ShiftNAS method to address this question.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this paper are:

1. Proposing ShiftNAS, a novel one-shot neural architecture search (NAS) method that can dynamically adjust the sampling probability during supernet training to focus more on less trained subnets. 

2. Designing an architecture generator (AG) that can efficiently generate subnet architectures satisfying desired resource constraints. The AG can be trained jointly with the supernet in an end-to-end manner.

3. Introducing a "probability shift" mechanism that evaluates the training sufficiency of subnets and shifts the sampling probability distribution to sample more of the less trained subnets. This allows for more effective training resource allocation.

4. Achieving state-of-the-art results on multiple vision models including CNNs and Vision Transformers on ImageNet classification. The method is shown to be model-agnostic.

5. Demonstrating that the subnets found by ShiftNAS can directly inherit weights from the supernet without needing additional retraining or fine-tuning. This eliminates the extra search costs of some other NAS methods.

In summary, the core ideas are using a learnable sampling probability distribution to focus training on insufficiently trained subnets, along with a differentiable architecture generator to efficiently produce subnets satisfying resource constraints. Together these contributions enable effective one-shot NAS without incurring additional search or retraining costs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes ShiftNAS, a method to improve one-shot neural architecture search by dynamically adjusting the sampling probability of subnets during supernet training based on their training sufficiency, and using an LSTM-based architecture generator to efficiently obtain subnets with desired complexity constraints.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in neural architecture search:

- This paper focuses on improving one-shot NAS methods. One-shot NAS aims to find optimal architectures by training a supernet only once and inheriting weights, reducing overall search cost. This is compared to traditional NAS methods that require training each candidate model separately.

- The main contribution is proposing ShiftNAS, which dynamically adjusts the sampling probability during supernet training based on subnet complexity. This helps allocate training resources more efficiently compared to uniform sampling. 

- ShiftNAS uses an LSTM-based architecture generator to efficiently sample subnets meeting the complexity constraints. This enables end-to-end joint optimization of the generator and supernet.

- Experiments show ShiftNAS achieves state-of-the-art results on ImageNet using both CNN and ViT search spaces. The ability to work with different model types makes it more flexible than methods tailored to a specific architecture.

- Compared to other one-shot NAS methods like AutoFormer and AttentiveNAS, ShiftNAS reduces the performance gap between supernet training and final subnets without extra search costs. The learnable sampling strategy is a key difference.

- Unlike some weight sharing approaches, ShiftNAS does not require additional retraining or fine-tuning of the selected subnets. This further improves efficiency.

In summary, this paper pushes one-shot NAS forward by addressing the limitation of uniform sampling through a learnable sampling strategy and architecture generator. The end-to-end joint optimization and flexibility across model types are also advantages compared to prior arts. Reducing the performance gap without extra search costs is a notable improvement.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Improving the architecture generator to support more complex search spaces and generate architectures with higher accuracy. The current AG uses an LSTM which may not be optimal for very large and complex search spaces. Exploring more advanced sequence models or using reinforcement learning to train the AG could allow it to generate better architectures.

- Applying ShiftNAS to other vision tasks beyond image classification, such as object detection, segmentation, etc. The authors demonstrate promising segmentation results, but more extensive experiments on other tasks could be done.

- Adapting ShiftNAS to other model families besides CNNs and ViTs, such as MLP-based models. The authors claim ShiftNAS is model-agnostic but only evaluate it on CNN and ViT models. Testing it on different model types would verify this claim.

- Improving the way training sufficiency is quantified. Currently performance variance is used as a proxy, but more direct metrics could be explored. The gradient wrt the loss may more directly indicate if a subnet is undertrained.

- Developing methods to set the hyperparameters like step size and update frequency automatically rather than manually. For example, using validation set performance to adjust them on the fly.

- Extending ShiftNAS to multi-objective NAS settings where accuracy and efficiency are jointly optimized, rather than pre-specifying a constraint.

In summary, the main future directions are enhancing the architecture generator, applying ShiftNAS to more tasks and models, improving training sufficiency quantification, and automating more of the hyperparameters. Overall the authors propose ShiftNAS as a general framework for one-shot NAS that could be expanded in many ways.


## Summarize the paper in one paragraph.

 The paper proposes ShiftNAS, a neural architecture search method to improve one-shot NAS by addressing the issue of uniform sampling during supernet training. The key ideas are:

1) Rethink sampling in one-shot NAS: Uniform sampling leads to a normal distribution of subnet computational resources, concentrating training on moderate complexity subnets. This causes under/over-fitting for low/high complexity subnets. 

2) Probability shift: Dynamically adjust sampling probability based on subnet training sufficiency measured by performance variance. Increase probability for insufficiently trained subnets to allocate more resources.

3) Architecture generator: LSTM-based generator to efficiently produce subnets matching target complexity constraints. Trained jointly with supernet to provide optimal architectures.

4) Experiments on ImageNet show ShiftNAS improves one-shot NAS performance across CNN and ViT models without extra search/retraining cost. State-of-the-art results demonstrated. Approach is model-agnostic.

In summary, ShiftNAS improves one-shot NAS by shifting sampling probability to optimally allocate training resources across subnets and using an architecture generator to efficiently produce subnets matching constraints. Key innovation is dynamically adjusting sampling during supernet training.


## Summarize the paper in two paragraphs.

 Here is a 2 paragraph summary of the paper:

The paper proposes ShiftNAS, a method for improving one-shot neural architecture search (NAS). One-shot NAS trains a supernet where candidate architectures share weights, enabling efficient search. However, the paper notes that uniform sampling of architectures during training leads to a focus on medium-sized models, hampering performance of large/small models. 

To address this, ShiftNAS adjusts the sampling distribution during training based on architecture gradient. It calculates gradient variance across architectures to identify insufficiently trained regions of the search space. The sampling probability is increased for these regions to tilt resources towards them. ShiftNAS also uses an LSTM generator to efficiently produce architectures matching a target complexity. Experiments on CNN and vision transformer models demonstrate improved accuracy and wider efficiency/accuracy trade-offs compared to prior NAS methods. The approach requires no separate search or retraining stages.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes ShiftNAS, a neural architecture search method that improves one-shot NAS by adjusting the sampling probability of subnets during supernet training. ShiftNAS addresses the issue of uniform sampling used in previous one-shot NAS methods, which focuses training on subnets with intermediate complexity. Instead, ShiftNAS learns a sampling probability distribution by evaluating the performance variation of subnets across different complexities. Subnets with high variation are identified as insufficiently trained, and their sampling probability is increased. An LSTM-based architecture generator is also proposed to efficiently generate subnets meeting the desired complexity constraint. The sampling probabilities and architecture generator can be optimized end-to-end to allocate training resources more effectively. Experiments on ImageNet demonstrate that ShiftNAS improves one-shot NAS performance for both CNNs and ViTs without additional search costs.


## What problem or question is the paper addressing?

 Based on my reading, the key points about this paper are:

- It focuses on improving one-shot neural architecture search (NAS) methods through a technique called ShiftNAS. One-shot NAS aims to efficiently search for optimal neural network architectures by training a "supernet" only once and then searching over its weights.

- The paper identifies an issue with standard one-shot NAS methods - they use uniform sampling to train the supernet, which concentrates training on architectures of intermediate complexity. This leads to suboptimal performance when the final deployed architecture differs significantly in complexity. 

- To address this, ShiftNAS proposes two main ideas:

1) Adjust the sampling probability during supernet training based on the training sufficiency of subnets of different complexities. This tilts resources towards insufficiently trained subnets.

2) Use a learnable architecture generator to efficiently generate subnets of a target complexity for training.

- Experiments on ImageNet classification using CNNs and Vision Transformers demonstrate superior performance and wider exploration of the accuracy-efficiency tradeoff curve compared to prior NAS methods.

In summary, the key problem addressed is the inefficient training of the supernet in one-shot NAS methods, and the proposed ShiftNAS framework provides a way to improve it by shifting sampling probability and using an architecture generator.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- One-shot NAS: The paper focuses on a type of neural architecture search called one-shot NAS, where a supernet is trained once and subnets can inherit weights without additional training.

- Probability shift: The core idea proposed in the paper is shifting the sampling probability distribution during supernet training to focus more on undersampled architectures. 

- Architecture generator (AG): An LSTM-based generator is proposed to efficiently generate subnet architectures to meet target computational constraints.

- Computational resource: The paper analyzes subnets and shifts probability based on computational resource metrics like FLOPs.

- Weight sharing: The paper uses weight sharing/entanglement techniques to enable direct weight inheritance without retraining subnets.

- End-to-end training: The AG, supernet, and probability distribution are trained jointly in an end-to-end manner.

- Model agnostic: Experiments show the approach is effective for both CNN and ViT models, demonstrating its model agnosticism. 

- State-of-the-art: The found models achieve state-of-the-art accuracy under similar computational constraints compared to other NAS methods.

In summary, the key focus is on improving one-shot NAS by shifting sampling probability to address the limitations of uniform sampling during supernet training. The approach is model agnostic and achieves excellent accuracy without extra search costs.
