# [ShiftNAS: Improving One-shot NAS via Probability Shift](https://arxiv.org/abs/2307.08300)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the performance of one-shot neural architecture search (NAS) by adjusting the sampling strategy during supernet training?

Specifically, the paper hypothesizes that:

- The commonly used uniform sampling strategy in one-shot NAS leads to insufficient training of subnets with extreme (very small or very large) computational complexity. This is because uniform sampling concentrates training on subnets with intermediate complexity.

- Adjusting the sampling probability during training based on the training sufficiency of subnets with different complexities can improve the performance of the final searched architecture.

To test this hypothesis, the paper proposes ShiftNAS, which:

- Quantifies the training sufficiency of subnets using their performance variation. Subnets with higher variation are sampled more frequently.

- Uses an architecture generator to efficiently obtain subnets with a desired complexity during training.

- Allows end-to-end training of the sampling probabilities and architecture generator along with the supernet.

The central hypothesis is that by shifting sampling probability towards insufficiently trained subnets, ShiftNAS can improve one-shot NAS performance without additional retraining costs. Experiments on ImageNet classification validate this hypothesis and demonstrate state-of-the-art results.

In summary, the key research question is whether a learnable, complexity-aware sampling strategy can improve one-shot NAS performance over uniform sampling. The paper proposes and validates the ShiftNAS method to address this question.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this paper are:

1. Proposing ShiftNAS, a novel one-shot neural architecture search (NAS) method that can dynamically adjust the sampling probability during supernet training to focus more on less trained subnets. 

2. Designing an architecture generator (AG) that can efficiently generate subnet architectures satisfying desired resource constraints. The AG can be trained jointly with the supernet in an end-to-end manner.

3. Introducing a "probability shift" mechanism that evaluates the training sufficiency of subnets and shifts the sampling probability distribution to sample more of the less trained subnets. This allows for more effective training resource allocation.

4. Achieving state-of-the-art results on multiple vision models including CNNs and Vision Transformers on ImageNet classification. The method is shown to be model-agnostic.

5. Demonstrating that the subnets found by ShiftNAS can directly inherit weights from the supernet without needing additional retraining or fine-tuning. This eliminates the extra search costs of some other NAS methods.

In summary, the core ideas are using a learnable sampling probability distribution to focus training on insufficiently trained subnets, along with a differentiable architecture generator to efficiently produce subnets satisfying resource constraints. Together these contributions enable effective one-shot NAS without incurring additional search or retraining costs.
