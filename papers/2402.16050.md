# [LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form   Video-Text Understanding](https://arxiv.org/abs/2402.16050)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Understanding long-form videos and answering questions about them poses significant challenges due to the high dimensionality and complexity of video data as well as the misalignment between language and visual cues over space and time. Prior methods that sample only a few keyframes fail to capture sufficient context, while methods that process full videos have high computational costs. 

Proposed Solution: The authors propose a Language-guided Spatial-Temporal Prompt Learning (LSTP) framework with two key components:

1) Temporal Prompt Sampler (TPS): Uses optical flow to efficiently select relevant clips from long videos based on the language prompt/question. Incorporates rotary position embeddings to extrapolate to long videos.

2) Spatial Prompt Solver (SPS): Applies a lightweight Query Transformer over the keyframes from TPS to align them with language and feed into a frozen large language model to generate an answer.

The TPS and SPS modules are jointly optimized, with TPS improved using the spatial reasoning capacity of SPS.

Main Contributions:

- An efficient video understanding paradigm combining temporal sampling based on optical flow with spatial alignment using prompt tuning of language models

- A Temporal Prompt Sampler module that leverages optical flow for temporally-guided keyframe extraction

- Demonstrated state-of-the-art performance on the long-form AGQA 2.0 and NExT-QA videoQA datasets as well as the NExT-GQA temporal grounding dataset.

In summary, the paper presents an effective and versatile framework, LSTP, for long-form video understanding that strategically combines optical flow-based temporal sampling with spatial language-visual prompt tuning to balance efficiency and performance. Experiments validate its capabilities on multiple videoQA datasets.
