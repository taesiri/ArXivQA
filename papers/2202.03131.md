# [Transformers in Self-Supervised Monocular Depth Estimation with Unknown   Camera Intrinsics](https://arxiv.org/abs/2202.03131)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Self-supervised monocular depth estimation is important for autonomous vehicles and robotics. Most current methods rely on convolutional neural networks (CNNs) which have limited receptive fields.  
- Transformers have shown promise on image tasks due to their global context modeling, but have not been explored for self-supervised depth estimation.  
- Evaluating transformers on metrics beyond standard performance, like robustness and efficiency, is also lacking.

Method - MT-SfMLearner:
- Proposes Monocular Transformer Structure from Motion Learner (MT-SfMLearner) which adapts vision transformers for self-supervised depth and ego-motion estimation.
- Uses a transformer encoder-decoder architecture with losses from Monodepth2 to train depth and ego-motion networks.
- Also introduces a module to simultaneously estimate focal length and principal point for unknown camera intrinsics.
- Can flexibly use transformers, CNNs or both for the depth and ego-motion networks.

Experiments and Results:
- Compares MT-SfMLearner variants against state-of-the-art convolutional approaches on KITTI dataset. Gets comparable or better performance.
- Studies impact of using transformers in depth network - more coherent depths due to global context. CNNs better preserve local shapes.
- Analyzes model robustness to corruptions/attacks - transformers significantly improve robustness.
- Evaluates efficiency - transformers have lower speed and higher compute than CNNs.
- Shows estimated intrinsics do not degrade performance or robustness.

Contributions:
- First work exploring transformers for self-supervised depth learning
- Systematic analysis of accuracy, robustness and efficiency trade-offs with CNNs and transformers
- Novel method for jointly estimating intrinsics during self-supervised training
- State-of-the-art performance while improving robustness on KITTI benchmark
