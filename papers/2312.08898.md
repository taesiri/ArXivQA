# [Detection and Defense of Unlearnable Examples](https://arxiv.org/abs/2312.08898)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper demonstrates the detectability and defensibility of unlearnable examples, which are a type of data poisoning attack proposed for privacy preservation. The authors first prove that certain unlearnable attacks result in linear separability of the poisoned dataset. Based on this theoretical finding and the observation that unlearnable noises are robust to large bias shifts, they propose two effective detection algorithms - Simple Networks Detection and Bias-Shifting Noise Test. Experiments across CIFAR and TinyImageNet datasets show that these algorithms can identify all major unlearnable attacks with high accuracy. Furthermore, the authors develop a defense strategy focused on destroying detectability of unlearnable examples, using stronger data augmentations coupled with adversarial noises generated by simple networks. This approach not only degrades detectability but also achieves state-of-the-art defense performance against common unlearnable attacks based on the key insight that breaking detectability is essential. Finally, the paper establishes theoretical criteria on the relationship between poison and adversarial defense budgets that determine the success of robust unlearnable attacks and failure of adversarial training defenses. Overall, this work significantly advances understanding of the limitations of unlearnable examples.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Unlearnable examples are a type of data poisoning attack that slightly modifies training data to make machine learning models fail to generalize, while still achieving high accuracy on the poisoned validation set. This paper shows that existing unlearnable examples are easily detectable despite deceiving validation performance.  

Proposed Solution: 
The paper proposes two effective detection algorithms - Simple Networks Detection and Bias-Shifting Noise Test, that can identify all major unlearnable attacks with high accuracy. The key idea is that unlearnable examples introduce linear features that make them separable by simple linear models or neural networks, even under data augmentations. Moreover, they are robust to large bias-shifting noises that degrade original image features.  

The paper also proposes an effective defense by combining stronger data augmentations from contrastive learning with adversarial noises on two-layer neural networks. This destroys linear detectability of unlearnable attacks. Experiments show this achieves higher defense accuracy than adversarial training on complex models.

Main Contributions:
1) Proves linear separability for certain unlearnable attacks and shows all attacks are separable by two-layer neural networks  
2) Proposes two detection algorithms that identify unlearnable attacks by leveraging linear separability and bias-shift robustness
3) Establishes theoretical criteria relating the poisoning and adversarial defense budgets for existence of robust unlearnable attacks
4) Achieves state-of-the-art defense by augmenting training data to break detectability, instead of solely pursuing robustness.

The paper significantly advances understanding and defenses for unlearnable data poisoning attacks that aim to preserve privacy. The findings motivate creating more sophisticated attacks robust to proposed detection and defense methods.
