# [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity   within Large Language Models](https://arxiv.org/abs/2402.13516)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent large language models (LLMs) like LLaMA mostly use non-ReLU activations like GELU and Swish, which lack intrinsic activation sparsity. 
- Activation sparsity refers to the existence of considerable weakly-contributed elements in activation outputs, which can be leveraged to accelerate inference.
- Some works try to introduce ReLU activations to achieve sparsity, but fail to reach high sparsity without compromising performance.

Proposed Solution:
- The paper proposes ProSparse, an effective progressive ReLUfication method to introduce high activation sparsity into non-ReLU LLMs without decreasing performance.

Main Steps of ProSparse:
1) Replace original activation with ReLU and continue training to adapt to ReLU first.
2) Progressively increase L1 regularization factor on activations in multiple stages - starts low for warmup then increases smoothly over stages. This avoids radical distribution shifts.  
3) Shift ReLU threshold to positive value to further prune unimportant activations.

Contributions:
- Obtain 89.32% and 88.80% activation sparsity for LLaMA-7B and LLaMA-13B with ProSparse, without performance drop.
- Demonstrate practical acceleration effects of higher sparsity using approximate and accurate algorithms. Higher sparsity leads to more acceleration.
- Analyze sparsity over training process, dataset components and layers to explain ProSparse's effectiveness in boosting sparsity smoothly without comprising performance or load balance.

In summary, the paper proposes an effective progressive method ProSparse to introduce high activation sparsity into non-sparse LLMs, analyzes the sparsity deeply and demonstrates practical acceleration effects.
