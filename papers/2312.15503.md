# [Making Large Language Models A Better Foundation For Dense Retrieval](https://arxiv.org/abs/2312.15503)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Dense retrieval relies on learning discriminative text embeddings to capture semantic relationships between queries and documents. Large language models (LLMs) have strong semantic understanding capabilities that could benefit dense retrieval.  
- However, LLMs are pre-trained on text generation tasks, which is very different from producing semantic text embeddings needed for retrieval. So it's important to properly adapt LLMs to effectively initialize them as encoders for dense retrieval.

Proposed Solution:
- The paper proposes LLaRA (LLM adapted for Retrieval), a post-hoc adaptation method to improve LLM's ability to generate useful text embeddings for retrieval.
- LLaRA has two pre-training tasks:
   - EBAE (Embedding-Based Auto-Encoding): Text embeddings from LLM are used to reconstruct the original input sentence. This teaches the model to encode complete input semantics.
   - EBAR (Embedding-Based Auto-Regression): Text embeddings predict the next sentence after the input. This teaches associations between queries and relevant documents.
- The text embeddings are trained to accurately predict tokens in the input/next sentences. This forces them to capture complete semantic information.

Main Contributions:
- First work on adapting LLMs specifically for improving their utilization in dense retrieval
- Simple but effective - LLaRA gives major gains by pre-training on plain text with two self-supervised tasks
- Applied to adapt LLaMA-2-7B and achieved new SOTA results on MSMARCO and BEIR benchmarks, including zero-shot retrieval
- Will release model and code to facilitate future research
