# [Exploring the Impact of Dataset Bias on Dataset Distillation](https://arxiv.org/abs/2403.16028)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Dataset distillation (DD) aims to create a small synthetic dataset that retains essential information from a large dataset. It helps reduce data burden and training costs. 
- Existing DD methods assume the dataset is unbiased. However, real-world datasets often contain biases, which can negatively impact model performance.  
- This paper investigates, for the first time, the impact of dataset bias on DD.

Solution:
- Construct two new biased datasets for DD analysis - CMNIST-DD and CCFAR10-DD with varying bias ratios and severity levels.
- Evaluate performance of DD methods (DC, DSA, DM) on these datasets at different bias levels.
- Observe that bias generally degrades DD performance, especially at mid-range bias ratios. But DD benefits at very low and very high ratios.
- Severity of bias also impacts DD negatively.
- Propose "biased DD" formulation to retain unbiased attributes and minimize biased attributes during distillation.

Key Contributions:  
- First work exploring impact of dataset bias on DD.
- Create two new biased datasets for DD analysis.
- Empirically demonstrate that bias affects DD performance in most cases. 
- Identify bias levels and conditions where DD fails or succeeds.
- Redefine DD objective for biased datasets to extract unbiased attributes and reduce biased attributes.

In summary, this paper conducts an extensive empirical analysis to reveal how inherent dataset biases impact the dataset distillation process. The findings motivate a reformulation of DD that explicitly handles biases in the original training data.
