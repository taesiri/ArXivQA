# LaMemo: Language Modeling with Look-Ahead Memory

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we design a language model that can effectively model long-term dependencies in very long texts?The key hypothesis proposed in this paper is: Equipping Transformer language models with a look-ahead memory mechanism that allows bidirectional contextualization of memories can improve their ability to model long-term dependencies in long texts.In particular, the paper proposes two main ideas:1) A look-ahead memory mechanism that allows memory states to attend to future context tokens on their right side, in order to dynamically contextualize the memories with up-to-date information. 2) A memory interpolation technique that smoothly interpolates the look-ahead attention results with previous memory states, allowing information flow from long-term history while maintaining efficiency.The central hypothesis is that by enabling bidirectional interaction between memories and current context, as well as maintaining information flow from long-term history, the proposed look-ahead memory mechanism can improve language models' ability to capture long-range dependencies in very long texts. The experiments aim to test this hypothesis by evaluating the proposed LaMemo model on standard language modeling benchmarks.


## What is the main contribution of this paper?

This paper proposes LaMemo, a novel memory mechanism to enhance language modeling with Transformers. The key ideas and contributions are:- Proposes "look-ahead" attention that allows the memory states to attend to future tokens on the right side. This provides more up-to-date context for refreshing the memory representations. - Introduces memory interpolation to aggregate information from both left and right context when updating the memory states. This mimics bidirectional attention while keeping the memory complexity linear. - Presents a disentangled relative positional encoding scheme that is more stable for attending to future tokens compared to prior approaches like in Transformer-XL.- Evaluates LaMemo on word-level and character-level language modeling benchmarks. It outperforms strong Transformer baselines equipped with other memory mechanisms like recurrence memory, compressed memory, and continuous memory.- Analysis shows LaMemo improves memory utilization and is more beneficial when extrapolating to longer context lengths during inference.In summary, the main contribution is proposing an effective technique to learn better memory representations in Transformers by allowing the memory to interact with bidirectional context in an incremental fashion. The results demonstrate improved language modeling performance compared to prior memory-augmented Transformer approaches.
