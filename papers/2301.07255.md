# [Understanding the Role of Human Intuition on Reliance in Human-AI
  Decision-Making with Explanations](https://arxiv.org/abs/2301.07255)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How does human intuition affect reliance on AI predictions and explanations in human-AI decision making, and how does this differ for feature-based vs. example-based explanations?

Specifically, the authors aim to understand what types of human intuition are involved when interacting with AI predictions and explanations (RQ1), and whether human intuition comes into play differently with feature- vs example-based explanations in a way that impacts decision accuracy and appropriate reliance (RQ2). 

The key hypothesis seems to be that human intuition plays an important role in determining when people rely on or override AI predictions. The authors investigate whether feature- and example-based explanations interact with human intuition differently, which may explain why prior work has found mixed results on the benefits of explanations for improving appropriate reliance.


## What is the main contribution of this paper?

 This paper's main contribution is exploring how human intuition interacts with and affects reliance on AI predictions and explanations during human-AI decision making. Specifically, the authors conducted a mixed-methods study using a think-aloud protocol to investigate how people make decisions when provided with AI predictions and two types of explanations - feature-based and example-based. 

Through qualitative analysis of participants' think-aloud data, the authors identified three main types of intuition that people apply when deciding whether to rely on an AI prediction:

1. Intuition about the decision outcome - People may have a "gut feeling" about what the decision should be before considering the AI's input. 

2. Intuition about features - People apply domain knowledge and reasoning about specific features when evaluating AI explanations. 

3. Intuition about AI limitations - People may recognize signals that an AI prediction is unreliable based on its explanations.

The authors summarize these as three "intuition-driven pathways" that people use to override an AI prediction they believe is incorrect. 

The paper finds that example-based explanations were more compatible with human intuition and supported appropriate reliance better than feature-based explanations. The authors provide design recommendations for developing AI decision support systems that better accommodate human intuition.

In summary, the key contribution is providing new insights into the role of human intuition during the human-AI decision process, using think-aloud data and contrasting two types of explanations. This sheds light on when and how explanations do or do not improve appropriate reliance on AI.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper conducts a mixed-methods study using a think-aloud protocol to understand how human intuition affects reliance on AI predictions and explanations, finding that example-based explanations supported more appropriate reliance by being less disruptive of natural intuition and providing clearer signals of AI prediction unreliability compared to feature-based explanations.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in explainable AI for human-AI decision-making:

- The focus on understanding the role of human intuition is novel. Most prior work has focused on quantitative measures of decision outcomes, while this paper takes a more process-oriented approach using think-aloud methods and qualitative analysis. The identification of different types of intuition and intuition-driven pathways is a new theoretical contribution.

- The comparative evaluation of feature-based and example-based explanations builds on a body of prior work, adding new empirical evidence. The finding that example-based explanations can outperform feature-based explanations echoes some prior studies, though other studies have found mixed results. 

- The research questions on how explanations affect appropriate reliance and overreliance are well-motivated by issues raised in prior literature. The paper provides new explanations for the mixed effectiveness of explanations in prior work.

- The mixed-methods approach combining quantitative analysis of decision outcomes with qualitative think-aloud data provides richer insights compared to studies relying on one method alone.

- The sample size is relatively small compared to some prior work based on crowdsourcing. However, the in-depth qualitative data likely compensates for the smaller sample.

- The choice of prediction tasks and the experimental protocol follows conventions in prior work, making the results comparable. However, the tasks are still quite simplified compared to real-world settings.

Overall, this paper makes important theoretical contributions around the role of human intuition in this problem space. It also provides useful empirical results, though mainly for two simple prediction tasks. Follow-up work could further test the generalizability of the results in more complex real-world settings.


## What future research directions do the authors suggest?

 The authors suggest several directions for future research to develop more effective AI decision-support systems that better accommodate human intuition, based on the findings from their study:

1. Develop systems that can accommodate human intuition of varied strengths. For example, adaptive or personalized systems could provide different information support depending on the user's confidence in their own decision.

2. Design explanations that are more compatible with natural human reasoning patterns. For instance, start with qualitative narratives rather than quantitative feature importance scores. Also design explanations that leverage the types of feature-based reasoning people naturally engage in.

3. Develop explanations and interfaces that help people identify evidence in explanations that could discredit an AI prediction, since people look for such evidence to appropriately override AI.

4. Design features that effectively communicate AI limitations and signals of prediction unreliability. This could involve model documentation, confidence measures, or new evaluation metrics that test if explanations reflect errors.

5. Explore individual differences that affect the viability of different intuition-driven pathways to override AI. Then design support tailored for groups like low domain knowledge users.

6. Evaluate a broader range of explanation types, data types, tasks, and user populations to further generalize the role of intuition in human-AI decision making.

In summary, the authors call for explanations and interfaces that are more aligned with natural human reasoning, support users in applying their own intuition, and promote a critical understanding of the AI system's limitations in order to achieve appropriate reliance.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper conducted a mixed-methods study to understand how human decision-makers reconcile their own intuition with AI predictions and explanations when engaging with AI decision-support systems. The study compared two types of explanations - feature-based and example-based - across two prediction tasks. Through a think-aloud protocol and analysis, the study revealed three types of intuition that decision-makers apply to potentially override incorrect AI predictions: intuition about the decision outcome, intuition about features to reason about explanations, and intuition about AI limitations to recognize prediction unreliability. These intuition pathways were used to explain why example-based explanations led to better decision accuracy and appropriate reliance than feature-based explanations in this study. Specifically, example-based explanations were less disruptive of natural intuition, provided useful context to apply feature intuition, and gave clear signals about prediction unreliability. Based on these results, the paper provided design implications for developing AI decision-support systems and explanations that better leverage human intuition.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper conducted a mixed-methods study to understand how human decision-makers reconcile their own intuition with AI predictions and explanations. The authors evaluated two types of explanations — feature-based and example-based — across two prediction tasks. Their exploratory quantitative analysis found that feature-based explanations did not improve participants' decision outcomes and led to over-reliance on incorrect AI predictions. In contrast, example-based explanations reduced over-reliance and helped achieve complementary human-AI performance. The authors analyzed participants' think-aloud data during the decision tasks and identified three types of intuition that were involved: intuition about the task outcome, features, and AI limitations. These enabled three pathways that participants used to override incorrect AI predictions by applying their own intuition. The study suggests example-based explanations supported appropriate reliance better than feature-based explanations because they were less disruptive of natural intuition, provided additional context to form new intuition, and gave clear signals about prediction unreliability. Based on the findings, the authors make recommendations for developing AI decision support tools that better accommodate human intuition. They also discuss using the identified intuition pathways to explain mixed results in prior work and predict individual differences in who may benefit more from certain AI assistance.

In summary, this paper makes fundamental contributions to understanding the human decision-making process with AI predictions and explanations. It highlights the important role of human intuition and how different explanation types interact with intuition, which affects reliance on AI. The findings point to design needs for AI systems that are more compatible with human reasoning and support appropriate trust in AI. By moving beyond outcome metrics to study the decision process, this work provides actionable insights for developing human-centered AI assistance.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper conducted a mixed-methods study with 26 participants to understand the role of human intuition in human-AI decision-making when explanations are provided. The study involved two prediction tasks with different data types (tabular and text) where participants made decisions with and without AI assistance. Participants engaged with two types of AI explanations - feature-based and example-based. During the AI-assisted phase, participants were asked to think aloud while making decisions so the researchers could analyze their thought process. Quantitative analysis looked at the effect of explanation type on decision accuracy and reliance. Qualitative analysis involved open coding of think-aloud data to identify common themes around types of intuition participants applied. A comparative analysis was then done by separating cases of appropriate and inappropriate reliance on AI to understand how the explanations affected reliance differently. The mixed-methods approach provided both objective performance measures and insights into the decision-making process.


## What problem or question is the paper addressing?

 Based on my understanding, the key problem this paper is addressing is understanding the role of human intuition in determining reliance on AI predictions and explanations. Specifically, the paper investigates how different types of human intuition affect people's decision to rely on or override AI predictions when they are provided with explanations of the AI's reasoning. 

The paper notes that prior work has found mixed results on whether providing explanations of AI predictions improves human-AI collaborative decision-making, with some studies finding explanations can increase over-reliance on incorrect AI predictions. It points out a gap in understanding the thought process through which people determine whether to rely on the AI when given explanations. 

The key research questions are:

1) What types of human intuition are involved when people engage with AI predictions and explanations, and how does intuition affect reliance on the AI?

2) Do different types of explanations (feature-based vs example-based) interact with human intuition differently, and can this explain why they have different effects on decision accuracy and appropriate reliance in prior work?

So in summary, the main problem is a lack of understanding of the role of human intuition in determining reliance on AI when interacting with explanations. This limits our ability to design explanations that support appropriate reliance. The paper aims to address this by taking a closer look at the decision-making process and how different types of intuition come into play with different explanation types.


## What are the keywords or key terms associated with this paper?

 Based on a review of the paper, some of the key keywords and terms are:

- Explainable AI (XAI) - The paper focuses on explainable AI techniques for machine learning models. This encompasses methods to help people understand and interpret ML model predictions.

- Feature-based explanations - One type of local, post-hoc explanation method studied, which assigns an importance value or score to each input feature based on its contribution to a model prediction.

- Example-based explanations - Another type of local, post-hoc explanation studied, which provides similar examples from the training data along with the model's predictions. Aims to help understand how the model makes decisions and mistakes.

- Appropriate reliance - A goal when integrating AI assistance in decision-making. Means relying on the AI when it is likely correct but not relying on it when it is likely wrong. 

- Complementary performance - When combined human-AI performance exceeds that of either humans or AI alone. Depends on achieving appropriate reliance.

- Overreliance - The phenomenon where people overly rely on an incorrect AI prediction. Explanations have sometimes increased this in prior work.

- Intuition - Refers to people's beliefs, heuristics, pattern recognition, etc. brought into a decision task based on prior knowledge and experience. A focus of the study is understanding the interplay of human intuition and AI explanations.

- Think-aloud study - The method used, where people verbalize thoughts during decision tasks, to understand the real-time decision process.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main purpose or research question of the study?

2. What methods were used in the study (e.g., experiment, survey, interviews)? 

3. What were the key independent and dependent variables under investigation?

4. Who were the participants in the study and how were they selected?

5. What were the main findings or results of the study? 

6. Did the results support or contradict the original hypotheses or research questions?

7. What explanations or interpretations did the authors provide for the results?

8. What are the limitations of the study that could affect the validity or generalizability of the findings?

9. What are the theoretical and/or practical implications of the findings according to the authors? 

10. What future research do the authors suggest is needed based on this study?

Asking questions like these should help extract the key information needed to summarize the purpose, methods, findings, and implications of the study in a comprehensive way. The questions cover the critical elements of the introduction/background, methods, results, discussion, and conclusions sections. Focusing a summary around answering these types of questions will ensure all the important details are captured.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the methods proposed in the paper:

1. The paper utilizes a think-aloud protocol during the decision-making process. What are some of the advantages and limitations of using think-aloud methods to study decision-making processes? How might the think-aloud protocol have influenced participants' reasoning and reliance on the AI system?

2. The study uses a within-subjects design where each participant sees both feature- and example-based explanations. What are some potential carryover or order effects this design choice could introduce? How could the study design be modified to better isolate the effects of each explanation type?

3. The paper identifies three "intuition-driven pathways" for relying on or overriding the AI system based on the think-aloud data. While insightful, what are limitations of inductively identifying these pathways from observational data? How could the existence and effect of these pathways be more rigorously tested? 

4. The authors acknowledge their participant sample is not representative of the general population. How might the types of intuition identified differ in other populations less experienced with ML? How could the study recruit more diverse participants?

5. The example-based explanations used a simple nearest neighbor approach to select similar examples. What are some limitations of this approach? Would more sophisticated example selection methods further improve the benefits observed?

6. The study only looked at two types of post-hoc explanations on two relatively simple prediction tasks. How could the research be extended to study a wider range of explanation types and more complex real-world tasks?

7. The study focuses on cases where the AI system makes an incorrect prediction. How might the results differ for cases where the AI prediction is correct? Are there differences in how explanations support appropriate reliance vs. inappropriate overreliance?

8. How might the intuitions involved differ in collaborative settings where multiple human decision makers discuss and debate the AI prediction and explanations together rather than individually?

9. The paper argues explanations should be designed to support human intuition. What are some ways to further test whether new explanation methods are indeed more compatible with human reasoning?

10. What other experimental manipulations or measurements could help further unpack the relationship between different types of intuition and reliance on AI when interacting with different explanations?
