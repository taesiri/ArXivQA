# [AniFaceDrawing: Anime Portrait Exploration during Your Sketching](https://arxiv.org/abs/2306.07476)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: How can AI be used to assist users in creating high-quality anime portraits from rough freehand sketches throughout the entire sketching process?Specifically, the paper focuses on developing an AI system that can take a sequence of incomplete freehand sketches as input, and generate corresponding high-quality anime portraits as guidance to match the sketches and assist the user in creating the portrait. The key challenges are:- Maintaining high quality anime portrait generation from very rough and incomplete sketches during early sketching stages due to the ill-posed nature of conditional image generation.- Achieving stroke-level matching between the input sketches and generated portraits throughout the sketching process.To address these challenges, the paper proposes a novel approach based on latent space exploration of StyleGAN using a two-stage training strategy and stroke-level disentanglement learning, without requiring additional paired data. Experiments and user studies demonstrate the proposed system can generate high-quality matched portraits to assist users in creating anime portraits from scratch.In summary, the core hypothesis is that by using the proposed latent space exploration and disentanglement learning approach, an AI system can achieve stroke-level matching to assist in anime portrait creation from freehand sketches throughout the drawing process. The paper aims to demonstrate this capability.


## What is the main contribution of this paper?

 The main contribution of this paper is developing an AI system to assist users in creating high quality anime portraits from their freehand sketches. Specifically:- They propose a novel framework called "AniFaceDrawing" that can generate high quality anime portraits matching a user's sketch strokes throughout the entire drawing process. - They introduce the concept of "stroke-level disentanglement" which allows mapping sketch strokes to the corresponding parts of the anime portrait at a local level without needing semantic labels. This is achieved through a two-stage training strategy.- They simulate the drawing process on-the-fly during training to teach the system how sketches evolve from simple to complex. This allows it to handle progressive and incomplete sketches.- They conduct both qualitative and quantitative experiments to demonstrate the stability and effectiveness of the proposed method in generating high quality and locally matching anime portraits from rough sketches.- They validate the system through a user study, showing it can assist novice users in creating anime portraits following their intentions and improving their sketches.In summary, the key contribution is developing an end-to-end AI framework for interactive anime portrait drawing assistance that can generate high quality results matched to the user's sketch throughout the drawing process. The proposed "stroke-level disentanglement" concept and training strategy enables handling progressive rough sketches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes AniFaceDrawing, an interactive AI system that assists users in creating anime portraits by generating high-quality matching guidance images from users' freehand sketches throughout the sketching process using latent space exploration of StyleGAN with unsupervised stroke-level disentanglement learning.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of sketch-based anime portrait generation:- This paper focuses specifically on assisting users to create anime portraits during the sketching process, by generating high-quality anime portraits that match the user's rough sketches throughout. This is a novel application that I am not aware has been explored before in the sketch-based image synthesis literature. Most prior work has focused on generating complete images from finished sketches.- The proposed method adopts a latent space exploration approach with StyleGAN, which is different from prior sketch-to-image (S2I) synthesis techniques. Rather than training an end-to-end model, this work matches strokes to disentangled attributes in StyleGAN's latent space through a two-stage training strategy. This allows leveraging an existing high-quality StyleGAN model.- A key contribution is the unsupervised "stroke-level disentanglement" training that aligns sketch strokes with structural attributes in the latent space, without needing semantic labels. This is a creative way to achieve the desired stroke-level control for sketch guidance.- The paper presents both qualitative and quantitative experiments to demonstrate the method's ability to generate high-quality, matching portraits from progressive sketch inputs. The user study also provides evidence that their system assists anime portrait creation.- Compared to recent GAN inversion approaches like pSp, this method seems to perform better for incomplete sketch inputs, likely due to the proposed training strategy. The comparisons to pSp in the paper help highlight these advantages.- The interactive interface for providing sketch guidance during drawing seems well designed. The different guidance modes are useful for creativity support. The interface aspects are not a core contribution but help enable an effective drawing assistance system.Overall, I think the idea of interactive anime portrait generation during sketching and the proposed training approach are novel contributions to the fields of sketch-based interfaces and image synthesis. The results demonstrate effectiveness for the stated aim of assisting anime portrait creation.


## What future research directions do the authors suggest?

 The authors suggest several future research directions at the end of the paper:- Improving the matching of input sketches for hair: The current training strategy of treating hair as background limits the matching performance for hair. The authors suggest exploring new training strategies to improve hair matching.- Expanding the diversity of generated anime styles: Currently, the generated anime style is relatively constant as it depends on the pre-trained StyleGAN model. The authors suggest researching ways to extract more diverse anime styles from StyleGAN while maintaining control. - Expanding to more artistic styles like Ukiyo-e and painting: The current approach focuses on the anime style. The authors suggest expanding it to other artistic styles like Ukiyo-e and painting while preserving the stroke matching capability.- Reducing the computation time: The current response time after each stroke is around 1.65 seconds, which affects user experience. The authors suggest reducing the computation time to improve immersion.- Exploring new applications: The progressive sketch-based image generation approach could be explored for new applications beyond portrait drawing assistance.In summary, the main future directions are improving hair matching, expanding diversity of styles, reducing computation time, and exploring new applications based on the proposed progressive sketch-based image generation approach. The key is to maintain the stroke-level matching while improving quality and diversity.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper presents AniFaceDrawing, a system that provides high-quality anime portrait drawing assistance to users during freehand sketching. The key idea is to explore the latent space of a pre-trained StyleGAN model to generate anime portraits that match the user's incremental sketch strokes. The system trains an image encoder and sketch encoder in two stages - the image encoder learns GAN inversion from StyleGAN's decoder, while the sketch encoder learns to map sketches to the latent space guided by the image encoder in a stroke disentanglement manner. At inference time, the sketch encoder outputs a structural code from the input sketch which is combined with a color code for style mixing to generate the final anime portrait. Evaluations show the system can generate high-quality results consistently matched to rough input sketches. A user study demonstrates the effectiveness and usability of AniFaceDrawing for assisting users to create anime portraits based on their sketching.


## Summarize the paper in two paragraphs.

 Here is a 2 paragraph summary of the key points from the paper:Paragraph 1: This paper proposes AniFaceDrawing, an AI system to assist users in creating anime portraits during freehand sketching. The goal is to take a user's rough sketch strokes as input and generate a corresponding high-quality anime portrait image as guidance. Previous sketch-to-image (S2I) methods fail to generate good results from incomplete sketches, especially for the more abstract anime style. The key idea in AniFaceDrawing is "stroke-level disentanglement" which maps sketch strokes to specific facial attributes in the latent space of a pre-trained StyleGAN model. This is done through a two-stage training process: first an image encoder matches portraits to the latent space, then a sketch encoder learns to match strokes to disentangled facial attributes guided by the image encoder.Paragraph 2: Experiments demonstrate AniFaceDrawing can generate high-quality, locally-matching anime portraits even from sparse sketches with varying stroke order. Quantitative evaluation using FID and sketch-image matching metrics shows it outperforms a pSp baseline. A user study found it provides effective drawing assistance - users with little drawing skill could create reasonable sketches that were turned into high-quality anime portraits. The system was rated as having good usability and creativity support. Limitations are dependence on the pre-trained StyleGAN model and constant anime style. Future work includes expanding the styles and types of generated images while maintaining stroke-level matching.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a method for generating high-quality anime portraits from freehand sketches during the drawing process. The main idea is to learn a mapping from sketches to the latent space of a pre-trained StyleGAN model that achieves "stroke-level disentanglement". This allows matching individual strokes in the sketch to corresponding facial features in the generated anime portrait. The method involves two stages of training:1) An image encoder is trained to project anime portraits generated by StyleGAN back into the latent space. This performs GAN inversion. 2) A sketch encoder is trained to map sketches to the latent space. The training uses automatically generated sketch-image pairs from StyleGAN without needing real data. The key is simulating the drawing process by starting from a line drawing of a StyleGAN portrait and incrementally revealing parts. This forces the latent code to move in a locally consistent way that maintains image quality as strokes are added.The image encoder from stage 1 provides supervision for the sketch encoder in stage 2. At inference time, the sketch encoder outputs a structural latent code that is combined with a random style code from StyleGAN to generate an anime portrait matching the input sketch.So in summary, the main novelty is using the simulated drawing process to train the sketch encoder to achieve stroke-level disentanglement, which allows high-quality anime portraits to be generated from incomplete sketches. The method is unsupervised, relying only on StyleGAN samples.


## What problem or question is the paper addressing?

 The paper is addressing the challenging problem of creating high-quality anime portraits from freehand sketches during the sketching process. Specifically, it focuses on how AI can assist users in converting their rough sketches into anime portraits stroke-by-stroke as they refine their drawings. The key questions the paper tries to address are:1. How to learn a mapping that allows matching sketch strokes locally to the generated anime portrait image (stroke-level disentanglement)?2. How can this mapping be unaffected by the stroke order?The goal is to provide high-quality anime portrait guidance that consistently matches the user's input sketch throughout the sketching process, even when the sketch is very rough or sparse. This is difficult because incomplete sketches often lack enough detail and shape information to generate a matching high-quality anime portrait.So in summary, the paper is tackling the problem of generating high-quality anime portrait guidance images that match freehand sketches stroke-by-stroke during the sketching process. The key is developing a method for stroke-level disentanglement that can understand rough, sparse sketches and generate appropriately matching results.
