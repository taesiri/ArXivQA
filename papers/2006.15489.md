# [Video Representation Learning with Visual Tempo Consistency](https://arxiv.org/abs/2006.15489)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can visual tempo consistency serve as an effective self-supervision signal for video representation learning? The key hypotheses appear to be:- Videos of the same action instance but with different visual tempos (i.e. fast vs slow) share high similarity in terms of discriminative semantics.- Maximizing mutual information between representations of slow and fast videos of the same instance via contrastive learning can allow encoders to learn useful representations without labels.- A hierarchical contrastive learning scheme that matches representations from multiple network depths can provide stronger supervision, especially for deeper networks.The authors propose the visual tempo consistency as a novel pretext task for self-supervised video representation learning. Their method trains video encoders by maximizing agreement between representations of slow and fast videos of the same instances using contrastive learning. The goal is to learn representations that capture semantics shared across different visual tempos.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes to use visual tempo consistency as a self-supervision signal for unsupervised video representation learning. Specifically, it samples the same video instance at different frame rates to obtain slow and fast versions that share semantics but differ in visual tempo. - It introduces a hierarchical contrastive learning framework (VTHCL) to leverage the visual tempo consistency. The model maximizes mutual information between representations of slow and fast videos across multiple layers in the encoder network.- It achieves competitive performance for action recognition on UCF-101 and HMDB-51 under the standard self-supervised evaluation protocol. The learned representations also transfer reasonably well to other tasks like action detection and anticipation.- It proposes an Instance Correspondence Map (ICM) method to visualize and interpret what is captured by the contrastive learning framework, without needing any semantic category labels. The ICMs highlight temporally and spatially discriminative regions corresponding to informative objects.- It provides a more thorough evaluation and suggests future self-supervised video representation learning should be benchmarked across diverse architectures, datasets, and tasks to properly assess generalization ability.In summary, the key contribution is the proposal to exploit visual tempo consistency as a novel and effective self-supervision signal for unsupervised video representation learning, through a hierarchical contrastive learning framework. The learned representations achieve strong performance on action recognition and transfer reasonably to other video tasks.
