# [Task-customized Masked AutoEncoder via Mixture of Cluster-conditional   Experts](https://arxiv.org/abs/2402.05382)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The prevailing masked autoencoder (MAE) method for self-supervised learning suffers from negative transfer when applied to downstream tasks with different data distributions than the pre-training data.
- Using semantically irrelevant pre-training data can hurt transfer performance for downstream tasks.
- Naively adopting mixture of experts (MoE) to MAE does not improve performance due to lack of semantic correlation in routing tokens to experts.

Proposed Solution:
- Propose Mixture of Cluster-conditional Experts (MoCE) to achieve task-customized self-supervised pre-training.
- First cluster pre-training data using features from a pre-trained MAE model into semantically similar groups. 
- Construct MoCE network with multiple experts. Gate network routes images from same cluster to same expert using cluster embeddings.
- Additional loss introduced to stabilize training and enhance confidence of gate outputs.
- For new downstream task, select closest expert using clustering module on task images.

Main Contributions:
- Systematically analyze and demonstrate negative transfer phenomenon in MAE.
- Show limitations of naively adopting MoE to MAE.
- Propose novel MoCE method to train customized experts for downstream tasks and achieve state-of-the-art self-supervised transfer.
- Outperforms MAE by 2.45% averaged accuracy over 11 downstream tasks.
- Achieves new state-of-the-art self-supervised results on detection and segmentation.
