# [TOOLVERIFIER: Generalization to New Tools via Self-Verification](https://arxiv.org/abs/2402.14158)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) struggle to robustly use new tools from few demonstrations. This is due to context length limitations when including demonstrations for many tools.
- Existing methods that rely on fine-tuning or prompting LLMs on real or synthetic tools have issues scaling and generalizing to unseen tools.

Proposed Solution - ToolVerifier:
- Decomposes the task into tool selection and parameter generation sub-tasks. 
- Proposes a self-verification mechanism for both stages to reduce errors:
   - Generates contrastive questions between top 2 predicted tools/parameters.
   - Answers questions to discern between close candidates.
   - Appends answers to context to guide model.
- For tool selection, trains on high-quality synthetic data of model-generated tools and instructions. Enables generalization to unseen tools.

Key Contributions:
- Novel multi-step self-verification approach with contrastive questions tailored for tool use scenarios. 
- Synthetic dataset of 173 tools with 555 examples for tool selection.
- Experiments on 4 tasks with 17 unseen real-world tools. Achieves 22% average improvement over baselines. 
- Ablations demonstrate contributions from synthetic data (6% boost) and self-verification (8% boost).

In summary, the paper introduces an effective approach to enhance LLMs' ability to generalize to new tools by decomposing the task and employing self-verification, trained on synthetic model-generated data. The gains on real-world tool use highlight the promise of this method.
