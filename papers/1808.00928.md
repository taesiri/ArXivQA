# [Learning Actionable Representations from Visual Observations](https://arxiv.org/abs/1808.00928)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central research question this paper addresses is how to learn useful visual representations from observations that can be used for continuous control tasks in robotics, without requiring expensive supervised labels or pre-training on large classification datasets. The main hypothesis is that a multi-frame variant of Time-Contrastive Networks (mfTCN) can learn representations from visual observations that encode both static and motion attributes. This representation can serve as a drop-in replacement for true state representations and enable reinforcement learning agents to learn policies directly from pixels.In particular, the key hypotheses are:- mfTCN can capture both position and velocity attributes more accurately compared to single-frame methods like original TCN.- RL policies can be learned from pixels using mfTCN while outperforming policies learned from scratch on pixels or using other self-supervised approaches like PVEs. - The mfTCN-learned policies can be competitive with policies learned from true proprioceptive state representations.So in summary, the central research question is how to learn useful representations from visual observations alone, and the hypothesis is that a multi-frame contrastive approach can learn "actionable" representations that capture both static and motion attributes for continuous control.
