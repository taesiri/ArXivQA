# [Learning Actionable Representations from Visual Observations](https://arxiv.org/abs/1808.00928)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central research question this paper addresses is how to learn useful visual representations from observations that can be used for continuous control tasks in robotics, without requiring expensive supervised labels or pre-training on large classification datasets. The main hypothesis is that a multi-frame variant of Time-Contrastive Networks (mfTCN) can learn representations from visual observations that encode both static and motion attributes. This representation can serve as a drop-in replacement for true state representations and enable reinforcement learning agents to learn policies directly from pixels.In particular, the key hypotheses are:- mfTCN can capture both position and velocity attributes more accurately compared to single-frame methods like original TCN.- RL policies can be learned from pixels using mfTCN while outperforming policies learned from scratch on pixels or using other self-supervised approaches like PVEs. - The mfTCN-learned policies can be competitive with policies learned from true proprioceptive state representations.So in summary, the central research question is how to learn useful representations from visual observations alone, and the hypothesis is that a multi-frame contrastive approach can learn "actionable" representations that capture both static and motion attributes for continuous control.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a multi-frame variant of Time-Contrastive Networks (TCN) that can encode both static and motion attributes from visual observations. The key ideas are:- Proposing a multi-frame TCN (mfTCN) that jointly embeds multiple frames to capture motion information, unlike original single-frame TCN. - Showing that mfTCN embeddings can accurately encode position and velocity information through regression experiments.- Demonstrating that mfTCN embeddings can be used as input to learn continuous control policies with PPO, achieving comparable performance to policies trained on true state.- Evaluating mfTCN on the real-world Pouring dataset, showing significant improvements in classifying motion attributes compared to single-frame TCN.In summary, the main contribution is presenting mfTCN as a self-supervised approach to learn actionable visual representations that encode both static and dynamic information, and can be used for robotic control from pixels.
