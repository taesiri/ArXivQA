# Landmark Attention: Random-Access Infinite Context Length for   Transformers

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that landmark tokens can be used to train transformers to efficiently retrieve relevant context blocks directly through attention. The key ideas are:- Introducing special "landmark" tokens to represent each block of the input context. The landmark tokens are trained so their key vectors summarize the content of the corresponding block.- Modifying the attention mechanism during training to use landmark tokens to gate access to each block. - At inference time, using the attention scores to landmark tokens to directly retrieve the most relevant context blocks.In essence, the paper proposes landmark attention as a method to overcome the context length limitations in transformers by enabling flexible retrieval of informative context blocks directly through the standard attention mechanism. The central hypothesis is that this approach can allow transformers to operate on arbitrarily long contexts while retaining the capabilities of full attention.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method for training Transformer models to efficiently retrieve and attend to relevant context from long inputs during inference. The key ideas are:- Introducing landmark tokens to represent blocks of the input. The attention to these landmark tokens allows selecting relevant blocks.- Using a grouped softmax attention mechanism during training to learn representations for the landmark tokens. - An inference scheme that uses landmark attention scores to retrieve a small set of relevant blocks from a large context. This allows attending to long contexts using much fewer operations compared to standard full attention.- Showing that models trained with this method can effectively attend to context lengths much longer than those seen during training.- Demonstrating that the method can be used to efficiently extend pre-trained models like LLama to longer contexts through fine-tuning.In summary, the main contribution is an efficient attention-based retrieval mechanism for accessing relevant context from very long inputs, enabling transformers to go beyond their typical context length limitations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new method for training Transformers to efficiently retrieve and attend to relevant information from long contexts using special "landmark" tokens that act as representatives for blocks of tokens.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work on increasing context length for transformers:- The main novelty is using special "landmark" tokens to represent blocks of the input, and training attention to use these landmarks to retrieve relevant blocks. This provides an interpretable attention-based retrieval mechanism. - In comparison to methods like Transformer-XL that use recurrence, this approach retains the full flexibility of attention to access any token in the past context. However, it achieves comparable perplexity reductions to Transformer-XL.- Unlike retrieval augmentation methods like REALM, the retrieval is integrated directly into the attention mechanism rather than relying on a separate retriever module.- The method allows extrapolation to longer context lengths than seen during training, overcoming a key limitation of transformers. Fine-tuning experiments on LLaMA showcase extending context from 2K to 32K.- It provides computational benefits, with retrieval attention focused on a subset of landmark tokens. Authors report potential for up to 50x reduction in FLOPs.- There are similarities to sparse attention methods like Longformer in using representative tokens for blocks, but retention of fine-grained attention within retrieved blocks is a key difference.Overall, the landmark token approach seems like a promising way to extend context length while retaining the strengths of transformer attention. The results demonstrate the viability of this method and highlight opportunities for further optimization and combination with other techniques.
