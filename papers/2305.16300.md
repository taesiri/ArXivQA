# [Landmark Attention: Random-Access Infinite Context Length for   Transformers](https://arxiv.org/abs/2305.16300)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis is that landmark tokens can be used to train transformers to efficiently retrieve relevant context blocks directly through attention. The key ideas are:- Introducing special "landmark" tokens to represent each block of the input context. The landmark tokens are trained so their key vectors summarize the content of the corresponding block.- Modifying the attention mechanism during training to use landmark tokens to gate access to each block. - At inference time, using the attention scores to landmark tokens to directly retrieve the most relevant context blocks.In essence, the paper proposes landmark attention as a method to overcome the context length limitations in transformers by enabling flexible retrieval of informative context blocks directly through the standard attention mechanism. The central hypothesis is that this approach can allow transformers to operate on arbitrarily long contexts while retaining the capabilities of full attention.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel method for training Transformer models to efficiently retrieve and attend to relevant context from long inputs during inference. The key ideas are:- Introducing landmark tokens to represent blocks of the input. The attention to these landmark tokens allows selecting relevant blocks.- Using a grouped softmax attention mechanism during training to learn representations for the landmark tokens. - An inference scheme that uses landmark attention scores to retrieve a small set of relevant blocks from a large context. This allows attending to long contexts using much fewer operations compared to standard full attention.- Showing that models trained with this method can effectively attend to context lengths much longer than those seen during training.- Demonstrating that the method can be used to efficiently extend pre-trained models like LLama to longer contexts through fine-tuning.In summary, the main contribution is an efficient attention-based retrieval mechanism for accessing relevant context from very long inputs, enabling transformers to go beyond their typical context length limitations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a new method for training Transformers to efficiently retrieve and attend to relevant information from long contexts using special "landmark" tokens that act as representatives for blocks of tokens.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work on increasing context length for transformers:- The main novelty is using special "landmark" tokens to represent blocks of the input, and training attention to use these landmarks to retrieve relevant blocks. This provides an interpretable attention-based retrieval mechanism. - In comparison to methods like Transformer-XL that use recurrence, this approach retains the full flexibility of attention to access any token in the past context. However, it achieves comparable perplexity reductions to Transformer-XL.- Unlike retrieval augmentation methods like REALM, the retrieval is integrated directly into the attention mechanism rather than relying on a separate retriever module.- The method allows extrapolation to longer context lengths than seen during training, overcoming a key limitation of transformers. Fine-tuning experiments on LLaMA showcase extending context from 2K to 32K.- It provides computational benefits, with retrieval attention focused on a subset of landmark tokens. Authors report potential for up to 50x reduction in FLOPs.- There are similarities to sparse attention methods like Longformer in using representative tokens for blocks, but retention of fine-grained attention within retrieved blocks is a key difference.Overall, the landmark token approach seems like a promising way to extend context length while retaining the strengths of transformer attention. The results demonstrate the viability of this method and highlight opportunities for further optimization and combination with other techniques.


## What future research directions do the authors suggest?

 The paper suggests the following future research directions:- Extrapolating positional encoding: Developing methods that allow the model to extrapolate to context lengths much larger than those seen during training, while still allowing direct attention to long-range tokens. The paper briefly proposes a possible solution but leaves thorough investigation for future work.- Hierarchical landmarks: Exploring hierarchical landmark tokens, with higher level landmarks controlling attention to lower ones. This could improve retrieval performance and memory usage in large-scale settings. The paper provides an example of using a global token to gate all landmarks.- Training with cache: Investigating variants of training that incorporate caching, which may provide additional benefits over the standard training used in this work. - Efficient implementation: Optimizing implementation details like combining landmark tokens with flash attention, and using advanced data structures like FAISS for efficient nearest neighbor search over landmarks.- Applications as document retriever: Noting the potential to directly use the model's landmark attention for building a document retriever, without needing additional training.So in summary, future directions focus on improving context length handling, hierarchical retrieval, optimized training and implementation, and downstream applications. The core landmark token idea seems promising as a building block for future research on long-context transformers.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes a novel method to train Transformer models to access long context lengths during inference. It introduces special "landmark" tokens to represent blocks of the input context. The model learns to use attention to these landmarks to retrieve the most relevant context blocks. This allows the model to attend to any part of a long context, unlike recurrent memory approaches that compress past information. Experiments show the method achieves comparable perplexity to Transformer-XL on books and math papers, while using less computation by retrieving fewer context tokens. The approach also enables inference on context lengths much longer than training lengths. Fine-tuning a 7B parameter model with the method improves its accuracy in retrieving information from contexts over 32K tokens. Overall, the landmark attention mechanism provides an efficient way to access full context information for long documents.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a new method for allowing Transformer models to attend to much longer contexts than they are trained on. The key idea is to break the input into fixed-length blocks and introduce a special "landmark" token to represent each block. The landmark tokens are trained so their key vectors summarize the content of their block. At inference time, the model first uses attention scores to the landmark tokens to retrieve the most relevant blocks from the full context. It then attends only to the landmarks and retrieved blocks, ignoring other context. The experiments demonstrate that models trained with landmark tokens can achieve comparable perplexity to Transformer-XL models while using less computation. The results also show the approach enables extending context length far beyond training lengths. For example, fine-tuning LLaMA 7B with landmarks allows it to successfully retrieve information from contexts over 32k tokens, compared to its original 2k training length. The landmark method provides efficiency benefits since computation grows much slower with context length compared to standard attention, and allows tracking which context blocks are used. Overall, the landmark attention mechanism offers an efficient way to provide Transformer models with essentially unlimited context lengths for improved reasoning and knowledge integration.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method proposed in the paper:The paper proposes a novel method to enable transformers to efficiently access long context during inference. The key idea is to break the input into fixed-size blocks and introduce a special "landmark" token to represent each block. The landmark tokens are trained so that their attention scores indicate the relevance of their corresponding block. At inference time, the model can then directly retrieve relevant blocks by looking at the landmark attention scores, instead of having to attend to the entire context. This allows transforming an existing model to handle arbitrarily long contexts at test time, while reducing computation and memory usage by orders of magnitude compared to standard attention over the full context. The landmark tokens provide an attention-based retrieval mechanism that retains the flexibility of global attention while overcoming its computational limitations.
