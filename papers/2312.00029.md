# [Bergeron: Combating Adversarial Attacks through a Conscience-Based   Alignment Framework](https://arxiv.org/abs/2312.00029)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Modern large language models (LLMs) can still generate problematic responses that are misaligned with human values, despite advances in alignment methods. 
- Current alignment approaches have limitations - they are expensive, can degrade model performance if applied too aggressively, and still leave models vulnerable to attacks.
- As a result, many models exhibit bias, can be "jailbroken" into giving unsafe responses, undergo minimal alignment training, or overly aggressive unsafe training.

Proposed Solution:  
- The authors propose Bergeron, a modular framework designed to add an extra layer of safety to a model's existing alignment approach without compromising performance.  
- It has a two-tiered architecture - a primary user-facing LLM that interacts with users, and a secondary "conscience" LLM that oversees inputs/outputs of the primary LLM.
- The secondary LLM identifies potentially hazardous text and only intervenes when necessary to make adjustments to prompts or responses.  
- Bergeron aims to function as a wrapper for primary LLMs to enhance their alignment by providing redundant prompt and response protections.

Main Contributions:
- Introduction of Bergeron, a flexible framework to improve LLM robustness against attacks by monitoring and steering responses.
- Demonstration that Bergeron can enhance alignment and robustness of several popular LLMs without costly fine-tuning.
- Creation of a dataset of 84 adversarial prompts to evaluate model alignments using 13 templates covering different weaknesses.
- Empirical evaluation showing Bergeron reduces failure rates across models and a variety of attacks, especially for large models with extensive alignment training.
- Analysis providing insight into how models self-correct using disclaimers, and how this can be leveraged to increase framework effectiveness.

In summary, the paper introduces a novel conscience-based dual LLM architecture that demonstrably hardens language models against misalignment vulnerabilities and attacks by building on their existing knowledge.
