# [Accurate Block Quantization in LLMs with Outliers](https://arxiv.org/abs/2403.20137)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Inference on large language models (LLMs) is very computationally expensive, requiring efficient hardware and algorithms. A key bottleneck is the memory needed to store the large key and value matrices from the attention mechanism when generating long sequences. 

- Storing the keys and values in low-precision numeric formats like block floating point (BFP) can save memory, but the accuracy drops significantly if there are outlier values with much larger magnitudes than most elements. The outliers ruin the quantization for their whole block.

Method:
- The paper proposes a novel K-sort algorithm to reorder the rows of the key projection matrix Wk so that rows with large norms (that cause outliers) are grouped together into certain blocks. 

- The same reordering is applied to the query projection matrix Wq to compensate. This reshuffling happens once at compile time. During inference, the permutations are applied to the keys and queries before computing their product.

- For models using rotary embeddings, K-sort also correctly reorders the rotation frequencies and signs.

- After rearrangement, quantizing the keys into BFP format has much better accuracy, since each block contains values of similar magnitudes.

Results:
- Experiments on the Llama-7B model show perplexity degrades only slightly from 9.49 to 9.60 when storing keys in BFP12 format (4-bit mantissas) after applying K-sort, a 2x memory saving over float16 format.

- The gains are shown to be higher for smaller block sizes. For block size of 128, sorting channels made no difference, but for 32 the perplexity improved from 9.83 to 9.52 after sorting.

To summarize, the paper provides an efficient way to enable low-precision BFP quantization of the key cache in presence of outliers, through a smart reorder of projection matrix rows prior to inference. This allows much greater memory savings without significant loss of accuracy.
