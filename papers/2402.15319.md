# [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/abs/2402.15319)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have high computational costs due to their large number of parameters, requiring frequent data transfers during execution which is a key bottleneck.
- Prior work has focused on uniform quantization to compress LLM weights but there may be potential to achieve greater compression using non-uniform vector quantization (VQ).

Proposed Solution - GPTVQ:
- Proposes a fast and accurate post-training vector quantization algorithm called GPTVQ that interleaves VQ of weight columns with updates to unquantized weights using the Hessian information.
- Initializes VQ codebooks efficiently using a data-aware EM algorithm variant. 
- Further updates and compresses codebooks using gradient descent, integer quantization and SVD compression.

Contributions:
- Analysis and experiments show increasing dimensionality of VQ leads to improved accuracy vs model size tradeoffs for many LLMs.
- GPTVQ achieves state-of-the-art perplexity results for multiple LLM architectures (Llama-v2, Mistral etc) across different VQ dimensions.
- GPTVQ runtime is practical - quantizes a 70B parameter LLM in 3-11 hours on a single H100 GPU.
- Evaluated optimized VQ decoding on a mobile CPU and showed it has lower latency than 4-bit integer baseline, besides footprint savings.

In summary, the paper proposes an efficient vector quantization method for large language models that explores the blessing of dimensionality to achieve state-of-the-art accuracy/size tradeoffs across multiple models. The algorithm is fast, accurate, and hardware-feasible.
