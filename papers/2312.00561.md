# [Interior Point Constrained Reinforcement Learning with Global   Convergence Guarantees](https://arxiv.org/abs/2312.00561)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper develops an interior point policy gradient approach for safe reinforcement learning in constrained Markov decision processes (CMDPs). The goal is to find an optimal policy that maximizes cumulative reward while satisfying expected cumulative safety constraints, with a focus on ensuring constraint satisfaction during learning. The proposed algorithm employs a zeroth-order interior point method based on the log barrier function to incorporate constraints into the objective. Under common assumptions on the policy parameterization including Fisher non-degeneracy and bounded transfer error, the authors prove that their algorithm converges to an near-optimal policy with $\tilde{\mathcal{O}}(\varepsilon^{-6})$ sample complexity while guaranteeing safety throughout learning with high probability. This provides the first convergence and safe learning guarantees for policy gradient methods in CMDPs. The safety guarantee comes at the cost of additional $\mathcal{O}(\varepsilon^{-2})$ samples compared to prior work. Key results include explicitly bounding how close iterates can approach constraints, despite non-smoothness of the log barrier, as well as establishing gradient dominance to measure optimality gap.
