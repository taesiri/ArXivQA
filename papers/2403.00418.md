# [LLMs for Targeted Sentiment in News Headlines: Exploring Different   Levels of Prompt Prescriptiveness](https://arxiv.org/abs/2403.00418)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Targeted sentiment analysis (TSA) of news headlines is important but challenging due to the subjectivity and connotative nature of headlines. 
- Fine-tuned encoder models require labeled data and model retraining for new domains/languages. 
- Large language models (LLMs) offer an appealing solution via zero-shot in-context learning, but performance depends heavily on prompt design.

Proposed Solution: 
- Explore the effect of different levels of prompt prescriptiveness for LLMs on TSA of news headlines, drawing parallels with annotation paradigms.
- Evaluate LLM accuracy using prompts ranging from plain zero-shot to detailed few-shot prompts matching annotation guidelines.
- Assess calibration error to evaluate ability of LLMs to quantify uncertainty and mimic human disagreement.

Experiments:
- Use Croatian STONE dataset with ternary sentiment labels and detailed guidelines.
- Compare LLMs (Mistral, Neural Chat, GPT-3.5, GPT-4) to fine-tuned BERT models on English, Polish and Croatian datasets.  
- Design prompts with 6 levels of increasing prescriptiveness based on STONE guidelines.
- Apply 3 uncertainty quantification methods: self-consistency sampling, distribution sampling, verbal confidence scores.

Key Findings:
- GPT-4 achieves top accuracy on 2 of 3 multi-lingual datasets. BERT outperforms on Croatian data.  
- Except for few-shot prompting, accuracy rises with prompt prescriptiveness, but optimal level varies by LLM.
- Uncertainty is fairly well-calibrated but shows weak correlation to human disagreement. 
- GPT-3.5 balances predictive power and calibration best with highly prescriptive prompting.

Main Contributions:
- Analysis of zero- and few-shot LLMs for multi-lingual targeted sentiment analysis. 
- Evaluation of the impact of prompt prescriptiveness levels on LLM accuracy.
- Assessment of LLM uncertainty quantification for modeling label variation.
