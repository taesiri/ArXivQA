# [LLMs for Targeted Sentiment in News Headlines: Exploring Different   Levels of Prompt Prescriptiveness](https://arxiv.org/abs/2403.00418)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Targeted sentiment analysis (TSA) of news headlines is important but challenging due to the subjectivity and connotative nature of headlines. 
- Fine-tuned encoder models require labeled data and model retraining for new domains/languages. 
- Large language models (LLMs) offer an appealing solution via zero-shot in-context learning, but performance depends heavily on prompt design.

Proposed Solution: 
- Explore the effect of different levels of prompt prescriptiveness for LLMs on TSA of news headlines, drawing parallels with annotation paradigms.
- Evaluate LLM accuracy using prompts ranging from plain zero-shot to detailed few-shot prompts matching annotation guidelines.
- Assess calibration error to evaluate ability of LLMs to quantify uncertainty and mimic human disagreement.

Experiments:
- Use Croatian STONE dataset with ternary sentiment labels and detailed guidelines.
- Compare LLMs (Mistral, Neural Chat, GPT-3.5, GPT-4) to fine-tuned BERT models on English, Polish and Croatian datasets.  
- Design prompts with 6 levels of increasing prescriptiveness based on STONE guidelines.
- Apply 3 uncertainty quantification methods: self-consistency sampling, distribution sampling, verbal confidence scores.

Key Findings:
- GPT-4 achieves top accuracy on 2 of 3 multi-lingual datasets. BERT outperforms on Croatian data.  
- Except for few-shot prompting, accuracy rises with prompt prescriptiveness, but optimal level varies by LLM.
- Uncertainty is fairly well-calibrated but shows weak correlation to human disagreement. 
- GPT-3.5 balances predictive power and calibration best with highly prescriptive prompting.

Main Contributions:
- Analysis of zero- and few-shot LLMs for multi-lingual targeted sentiment analysis. 
- Evaluation of the impact of prompt prescriptiveness levels on LLM accuracy.
- Assessment of LLM uncertainty quantification for modeling label variation.


## Summarize the paper in one sentence.

 This paper explores the influence of prompt design on the performance of large language models for targeted sentiment analysis of news headlines, evaluating predictive accuracy and uncertainty quantification across prompts with varying levels of prescriptiveness.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Comparing the performance of large language models (LLMs) like GPT-3.5, GPT-4, Mistral, and Neural Chat to fine-tuned BERT models for targeted sentiment analysis (TSA) of news headlines in Croatian, English, and Polish.

2) Evaluating the effect of different levels of prompt prescriptiveness, ranging from basic to very detailed instructions matching annotation guidelines, on the predictive accuracy of LLMs for TSA of Croatian news headlines. 

3) Assessing the calibration error of LLM predictions across models and prompt prescriptiveness levels to determine their capability in quantifying predictive uncertainty for the subjective task of TSA.

In summary, the key contribution is an analysis of how prompt design impacts LLMs' accuracy and uncertainty quantification for TSA of news headlines across languages, using a newly introduced Croatian dataset with detailed annotation guidelines. Comparisons are also made to fine-tuned BERT models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords related to this work include:

- Targeted sentiment analysis (TSA)
- News headlines
- Large language models (LLMs) 
- Prompt design
- Prescriptiveness levels
- Annotation paradigms
- Descriptive vs prescriptive paradigms
- In-context learning
- Predictive accuracy 
- Calibration error
- Uncertainty quantification
- Self-consistency sampling
- Distribution sampling 
- Verbal confidence assessment
- Inter-annotator agreement
- Zero-shot learning
- Few-shot learning

The paper explores using LLMs for targeted sentiment analysis on news headlines, and studies the effect of different levels of prompt prescriptiveness, drawing parallels with annotation paradigms. It evaluates LLMs on metrics like predictive accuracy, calibration error, and correlation to human uncertainty. The key models analyzed are BERT, RoBERTa, GPT-3.5, GPT-4, Mistral and Neural Chat. The main datasets used are the STONE targeted sentiment dataset for Croatian news headlines, and the SEN dataset covering English and Polish headlines.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the level of prompt prescriptiveness parallel the descriptive vs prescriptive paradigms for data annotation? What are the key similarities and differences between these two frameworks? 

2. Why is targeted sentiment analysis of news headlines considered an especially challenging task? What unique complexities does it present compared to general sentiment analysis?

3. What motivated the authors to evaluate LLMs for targeted sentiment analysis using a range of prompt prescriptiveness levels? What benefits did they hypothesize this would provide?  

4. How did the authors construct the different levels of prompt prescriptiveness for their experiments, ranging from level 1 to level 6? What key information was incorporated at each level?

5. What trends did the authors observe regarding predictive accuracy as prompt prescriptiveness level increased? Did all LLMs exhibit the same patterns and optimal levels? Why might this be?  

6. How exactly did the authors evaluate the calibration error and uncertainty quantification capabilities of the LLMs? What specific metrics and analyses were used?  

7. What methods did the authors use to model LLM uncertainty and variability in a way that emulates human annotator disagreement and subjectivity? How successful were they?

8. Why is prompting sensitivity an issue when incorporating few-shot examples into higher prescriptiveness level prompts? How might this limitation be addressed?  

9. Could the optimal level of prompt prescriptiveness depend on language or domain? Why was only one dataset in Croatian used for evaluating this? How could additional multi-lingual experiments provide more insight?

10. How well did LLM uncertainty align with human inter-annotator disagreement? If this correlation was weak, what factors might be responsible and how could this alignment be improved?
