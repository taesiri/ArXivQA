# [TabMT: Generating tabular data with masked transformers](https://arxiv.org/abs/2312.06089)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes TabMT, a novel Masked Transformer model for generating high-quality synthetic tabular data. TabMT leverages bidirectional masked language modeling to effectively capture complex dependencies and patterns within heterogeneous tabular data. Through comprehensive experiments, TabMT demonstrates state-of-the-art performance across a diverse range of tabular datasets and metrics. Key advantages of TabMT include superior scalability from small to massive datasets, built-in handling of missing data, tunable privacy-utility tradeoffs, and high sample quality and diversity. The method is evaluated on privacy preservation, where it shows improved privacy over prior state-of-the-art, as well as on sample quality using a robust classifier-based metric, where it matches or exceeds previous methods on nearly all tested datasets. Additional experiments highlight TabMT's ability to scale to datasets orders of magnitude larger than prior work. The proposed model thus makes notable progress towards a robust and versatile tabular data generator through the cross-domain application of masked language modeling.
