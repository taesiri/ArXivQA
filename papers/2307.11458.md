# [Strip-MLP: Efficient Token Interaction for Vision MLP](https://arxiv.org/abs/2307.11458)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we design an efficient token interaction operation to improve the performance of MLP-based vision models?

Specifically, the authors identify a issue they term "the Token's interaction dilemma" - where the power of token interaction decreases in later layers as the spatial resolution is downsampled. To address this, the paper introduces a new MLP architecture called Strip-MLP with the following key components:

- Strip MLP layer - Allows tokens to interact in a cross-strip manner rather than just within rows/columns. This enriches the token interactions.

- Cascade Group Strip Mixing Module (CGSMM) - Splits features into patches and allows cross-patch token interactions, making it robust to smaller spatial sizes. 

- Local Strip Mixing Module (LSMM) - Captures local interactions more efficiently with a small Strip MLP.

Together, these components aim to improve the token interaction capabilities and power of MLP-based vision models, even as the spatial resolution decreases in deeper layers. The central hypothesis is that this will lead to improved performance on image classification tasks compared to prior MLP architectures.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper seem to be:

- Proposing a new MLP architecture called Strip-MLP to improve the token interaction power in vision MLP models. The key ideas include:

1) Strip MLP layer - Allows tokens to interact in a cross-strip manner, enabling each row/column to contribute differently to other rows/columns.

2) Cascade Group Strip Mixing Module (CGSMM) - Splits features into patches and allows cross-patch token interactions, making it robust to small spatial sizes. 

3) Local Strip Mixing Module (LSMM) - Captures local interactions more efficiently.

- Showing through experiments that Strip-MLP models achieve much better performance than other MLP models on small datasets like Caltech-101 and CIFAR-100.

- Demonstrating that Strip-MLP can achieve comparable or better accuracy than CNN and Transformer models on ImageNet with fewer parameters and FLOPs.

In summary, the key contribution appears to be proposing the Strip-MLP architecture that enriches token interactions in MLPs through strip-wise processing and patching, leading to accuracy gains especially on small datasets. The ablation studies seem to validate the design choices as well.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes Strip-MLP, a new MLP architecture for computer vision that enriches token interaction power through a Strip MLP layer for cross-strip aggregation, a Cascade Group Strip Mixing Module for patch-wise interaction, and a Local Strip Mixing Module for local aggregation.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in computer vision and MLP-based models:

- This paper introduces a new MLP-based vision model called Strip-MLP. It aims to improve the power of token interaction in MLP-based models, which has been an overlooked challenge.

- Previous MLP-based vision models like MLP-Mixer, gMLP, and SMLP have shown promising performance, but they struggle with effectively aggregating tokens when the spatial resolution becomes small in deeper layers. This paper identifies this issue as the "Token's interaction dilemma".

- To address this, Strip-MLP proposes three main contributions:

1) A new Strip MLP layer that allows more efficient cross-strip interaction between rows/columns of tokens.

2) A Cascade Group Strip Mixing Module (CGSMM) that splits features into channel groups to maintain interaction power independent of spatial size. 

3) A Local Strip Mixing Module (LSMM) to enhance local region aggregation.

- Compared to prior works, these components provide stronger global and local token interactions in a parameter-efficient way.

- Experiments show Strip-MLP significantly improves accuracy over previous MLP models on small datasets like Caltech-101 and CIFAR-100. On ImageNet, it achieves comparable or better performance than MLP competitors.

- The concepts introduced could potentially be integrated into other MLP architectures. The paper demonstrates this by applying Strip MLP to Wave-MLP and showing improved results.

- Overall, this paper makes important contributions to improving token interactions for vision MLPs. The techniques help overcome limitations of prior models and could become a useful component in future MLP architectures.

In summary, this paper advances the state-of-the-art in designing more powerful and efficient MLP-based vision models. The proposed methods meaningfully improve token interaction capabilities compared to previous works.
