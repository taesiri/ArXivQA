# [Rewarded soups: towards Pareto-optimal alignment by interpolating   weights fine-tuned on diverse rewards](https://arxiv.org/abs/2306.04488)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively align foundation models like large language models to diverse real-world tasks and human preferences. Specifically, the key hypotheses are:1. Linear mode connectivity holds between weights fine-tuned on diverse proxy rewards from a shared pre-trained initialization (Hypothesis 1). 2. Linearly interpolating weights fine-tuned independently on diverse proxy rewards yields a set of Pareto-optimal solutions that balances between those rewards (Hypothesis 2).The authors propose a new method called "rewarded soups" to test these hypotheses. The idea is to first fine-tune the weights of a foundation model on different proxy rewards representing different objectives or preferences. Then, interpolate between these specialized weights to create a continuum of models balancing between the objectives. The paper provides empirical validation of these two key hypotheses across a variety of tasks, including summarization, Q&A, image captioning, text-to-image generation, visual grounding, and locomotion control. The results consistently show that linear interpolation between weights fine-tuned on diverse rewards outperforms naive baselines, providing evidence that rewarded soups can mitigate reward misspecification and effectively handle the diversity of real-world preferences.In summary, the central research contribution is a new multi-policy strategy called rewarded soups to align foundation models to diverse objectives and human preferences by leveraging weight interpolation and the empirical finding of linear mode connectivity between specialized weights. The effectiveness of this approach is demonstrated across a range of language, vision, and control tasks.


## What is the main contribution of this paper?

The main contribution of this paper is rewarded soups, a new strategy to align foundation models with diverse human preferences and mitigate reward misspecification in reinforcement learning. The key ideas are:- Consider multiple proxy rewards capturing diverse user preferences and optimize one network independently for each reward.- Combine these expert networks by linearly interpolating their weights. This works surprisingly well empirically due to the linear mode connectivity between weights fine-tuned from a shared initialization.- The resulting rewarded soups define a continuous set of solutions that are close to Pareto-optimal. Users can select the best trade-off a posteriori based on their preferences.- This avoids the need for costly multi-objective reinforcement learning requiring one training per possible weighting of rewards. Instead, only one training per proxy reward is needed.The approach is validated extensively across various tasks (text, vision, control), models (LLMs, diffusion, locomotion) and types of rewards (human feedback, metrics). The interpolated solutions consistently mitigate reward misspecification and outperform single-policy strategies.Overall, rewarded soups offer an efficient and flexible strategy to handle the diversity of human preferences when aligning foundation models, while enhancing transparency. The hope is to better integrate AI systems in the complexity of the real world.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a new method called Rewarded Soup for fine-tuning foundation models with diverse reward functions. It shows that linearly interpolating between weights specialized for different rewards reveals a set of Pareto-optimal solutions that can mitigate reward misspecification. The key idea is that weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related work in this field:- This paper proposes a new technique called "rewarded soups" for training deep neural networks with diverse reward functions. Most prior work has focused on single reward functions or simple combinations through scalarization. The idea of independently training on different rewards and then interpolating the weights is novel. - The paper validates the effectiveness of their approach across a wide variety of tasks (text, vision, control), demonstrating the general applicability of the method. In contrast, much prior work on multi-objective RL or human feedback alignment has focused on narrow academic benchmarks.- The paper provides both empirical results and some theoretical analysis to support their hypotheses about linear mode connectivity and Pareto optimality when fine-tuning from a shared pre-trained initialization. This helps explain why their weight interpolation strategy works.- By embracing reward diversity rather than trying to find a single consensus reward, the paper aligns with an emerging viewpoint that handling human preferences requires multi-policy approaches. The paper cites related arguments made in Hayes et al. and Kirk et al.- The paper connects to the broader literature on weight interpolation, which has mostly considered supervised learning scenarios. Extending these ideas to RL training is novel and timely.- Compared to costly multi-policy RL techniques, rewarded soups is much more scalable by only requiring N trainings for N rewards. This makes it feasible to deploy the approach on huge modern foundation models.- The approach is compatible with existing RL algorithms and applicable after any standard pre-training/fine-tuning pipeline. This makes adoption easier compared to techniques requiring modifying the core training procedure.In summary, this paper makes both empirical and conceptual contributions to an important open problem of aligning AI with diverse human preferences and rewards. The generality of the approach across modalities is impressive.


## What future research directions do the authors suggest?

The authors suggest a few promising future research directions:- Developing theoretical guarantees for weight interpolation strategies like rewarded soups and linear mode connectivity between weights: The current work is primarily empirical. Theoretical guarantees could provide more robustness. In particular, the authors mention exploring settings beyond linear user rewards over proxy rewards. - Combining rewarded soups with more advanced weight interpolation techniques like MergeFisher that remove the constraint of a single interpolation coefficient: This could further improve results when combining many diverse rewards.- Enhancing pre-training strategies to facilitate weight interpolation, for instance through neuron permutation strategies: Better initialization could help maintain connectivity during fine-tuning.- Iteratively adding more proxy rewards over time to get an increasingly good coverage of possible user rewards: Rewarded soups is flexible to handle new rewards, enabling an iterative engineering process.- Applying rewarded soups in federated learning settings for personalized models while keeping data decentralized.- Developing open-sourced personalized models rewarded on diverse preferences to enable collaborative and continual improvements.In summary, the main future directions are: 1) Strengthening theoretical understanding, 2) Improving techniques to combine weights, 3) Enhancing pre-training connectivity, 4) Expanding the coverage of proxy rewards, and 5) Applying the approach to personalized and decentralized learning.


## Summarize the paper in one paragraph.

The paper proposes a method called rewarded soups for fine-tuning foundation models with diverse rewards. The key ideas are:1) Consider a family of diverse proxy rewards that evaluate different potentially conflicting criteria. 2) Independently optimize expert weights for each proxy reward using reinforcement learning. 3) Linearly interpolate the expert weights to define a continuous set of rewarded soup solutions. This relies on the empirical finding of linear mode connectivity between weights fine-tuned from a shared initialization.4) Select the interpolation coefficients at test time to match the user's preferences, enabling better alignment and mitigating reward misspecification issues.The method is validated across various tasks including text generation, image captioning, text-to-image, visual grounding, and locomotion. The interpolated weights reveal close to Pareto-optimal fronts, outperforming single-policy strategies. Overall, rewarded soups provide an efficient and flexible way to handle diverse rewards when fine-tuning foundation models.
