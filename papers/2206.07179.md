# [Proximal Splitting Adversarial Attacks for Semantic Segmentation](https://arxiv.org/abs/2206.07179)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract, the central research question this paper addresses is:

How to develop an effective white-box adversarial attack for deep semantic segmentation models that can produce minimal perturbations with small L-infinity norms?

The key points are:

- Prior work on adversarial attacks has focused mostly on classification tasks, with limited work on attacks for semantic segmentation models.

- Existing attacks for segmentation models do not accurately solve the adversarial segmentation problem and overestimate the size of perturbations needed to fool models. 

- The authors propose a new white-box attack using a proximal splitting method and augmented Lagrangian approach to handle the large number of pixel-wise constraints and minimize the L-infinity norm of perturbations.

- The attack is demonstrated to significantly outperform prior attacks, finding much smaller adversarial perturbations for segmentation models on Pascal VOC and Cityscapes datasets. 

So in summary, the main research goal is to develop a more effective adversarial attack for semantic segmentation that generates minimal perturbations, providing a better way to evaluate robustness of these models. The key hypothesis seems to be that prior attacks overestimate the perturbation sizes needed, which their new method aims to address.
