# EL-Attention: Memory Efficient Lossless Attention for Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research goals seem to be:- To propose a new attention method called EL-attention that can replace multi-head attention during inference for improved speed and reduced memory usage, without sacrificing accuracy.- To demonstrate that EL-attention can achieve equivalent results to multi-head attention, despite only expanding the query while keeping the key and value shared.- To show experimental results validating that EL-attention provides significant speedups (1.6-5.3x) across models like Transformer, BART, and GPT-2 on summarization and question generation tasks.- To highlight that the massive memory savings from EL-attention enable further optimizations like using larger batch sizes for additional speedups.So in summary, the central hypothesis appears to be that EL-attention can act as a drop-in replacement for multi-head attention at inference time, providing substantial gains in speed and memory efficiency without any loss in accuracy. The paper seems aimed at experimentally validating this hypothesis across diverse models and tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a new attention mechanism called EL-attention that can replace multi-head attention during inference to improve speed and reduce memory usage. Some key points:- EL-attention avoids the need to project the keys and values into multiple heads, and instead only expands the query. This saves computation and eliminates the need to cache keys/values per layer.- By expanding only the query, EL-attention can construct an ensemble of attention outputs similar to multi-head attention. The paper provides a proof showing EL-attention can produce identical results to multi-head attention.- Experiments demonstrate EL-attention speeds up inference by 1.6-5x across Transformer, BART, and GPT-2 models on summarization and question generation tasks, with no loss of accuracy.- The massive memory savings of EL-attention (96x less than multi-head attention in one experiment) allows using much larger batch sizes for additional speedups.In summary, the key contribution is proposing EL-attention as a drop-in replacement for multi-head attention at inference time, which provides gains in speed and memory efficiency. This could be useful for deploying Transformer models on memory-limited devices. The method does not require retraining models.
