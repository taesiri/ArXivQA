# [A Dataset of Open-Domain Question Answering with Multiple-Span Answers](https://arxiv.org/abs/2402.09923)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- There is a lack of publicly available Chinese multi-span question answering (MSQA) datasets compared to English. Existing Chinese MSQA datasets have limitations such as focusing only on specialized domains or formulating different tasks. 
- Current English MSQA datasets rely on Wikipedia for context retrieval, which leads to a bias towards factoid questions with entity answers. They struggle to represent open-domain questions that require longer, more descriptive answers.

Proposed Solution:
- The authors create a new Chinese MSQA dataset called CLEAN that covers diverse open-domain question topics and supports answer extraction. 
- CLEAN is constructed by extracting question-context pairs from the Baidu Zhidao platform rather than Wikipedia. Contexts are carefully chosen from long, high-quality answers to ensure they appropriately address question intents.
- CLEAN has 9,063 question-context-answer samples. 46% are multi-answer. It has a high proportion (76%) of descriptive answers compared to other datasets.

Contributions:
- CLEAN is the first large-scale, open-domain Chinese MSQA dataset with a focus on descriptive answers.
- Analysis shows CLEAN's descriptive answers are more challenging for current models. State-of-the-art models achieve much lower performance on CLEAN than English datasets.
- CLEAN drives further research into Chinese reading comprehension through its coverage of open-domain topics and prevalence of descriptive answers requiring deeper language understanding.

In summary, the paper introduces an innovative Chinese MSQA dataset called CLEAN to address limitations of existing resources. Experiments establish baseline performances and showcase the challenge presented by CLEAN's descriptive answers. CLEAN advances Chinese QA research.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes CLEAN, a new Chinese multi-span question answering dataset sourced from real-world user queries that features a high proportion of descriptive answers and establishes strong baselines, presenting new challenges for the research community.


## What is the main contribution of this paper?

 According to the paper, the main contributions are twofold:

1. The authors create a new Chinese multi-answer reading comprehension dataset named CLEAN, which covers a wide range of open-domain question topics and supports casting reading comprehension as an answer extraction task. 

2. The authors establish models drawn from related literature as baselines for the CLEAN dataset. Experiment results showcase the challenge of extracting longer descriptive answers, which constitute the majority in the CLEAN dataset.

In summary, the key contribution is the introduction of a new Chinese multi-span QA dataset called CLEAN to address the lack of such resources, along with baseline experiments that demonstrate the difficulty of this dataset, especially for extracting descriptive answer spans.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Chinese multi-span question answering
- Multi-answer reading comprehension
- Open-domain question answering
- Answer extraction
- Dataset construction
- Chinese datasets
- Descriptive answers
- Inter-annotator agreement
- Exact match (EM) 
- Partial match (PM)
- Baseline models (e.g. SSE, TASE, LIQUID, SpanQualifier, ITERATIVE, GPT-3.5)

The paper introduces a new Chinese multi-span question answering dataset called CLEAN. It discusses the dataset construction process, statistics, analysis, baseline model experiments, and challenges related to extracting longer, descriptive answer spans. The key focus is on multi-span extractive question answering for Chinese and the lack of publicly available datasets to support research in this area.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. What was the motivation behind creating the CLEAN dataset and how does it differ from existing Chinese multi-span RC datasets? 

2. Why did the authors choose to source question-context pairs from the Baidu Zhidao platform rather than using Wikipedia for context retrieval? What are the potential benefits of this approach?

3. The paper states that CLEAN has a high proportion (76%) of descriptive answer type instances. Why is this type of answer more challenging for current models and how could models be improved to better handle descriptive answers?  

4. What annotation guidelines and quality control processes were used to ensure reliable and consistent multi-span annotations when creating the CLEAN dataset?

5. The authors categorize questions in CLEAN into descriptive, entity, and numeric answer types. Why is this categorization useful and how do model performances differ across these question types?

6. SpanQualifier, the current SOTA model, shows lower performance on CLEAN compared to MultiSpanQA. What key differences between the datasets contribute to this performance gap?  

7. Iterative adaptation of single-span models struggles on CLEAN compared to MultiSpanQA. What characteristics of answers in CLEAN make the iterative approach less effective?

8. What are some potential reasons that the GPT-3.5 generative model underperforms on answer extraction compared to extractive models? How could it be improved?

9. What new model architectures, objectives, or training methods could be explored to better handle the complexity of descriptive answers in CLEAN?

10. The paper establishes strong baselines on CLEAN using models from related literature. What future work could be done to push state-of-the-art performance on this dataset?
