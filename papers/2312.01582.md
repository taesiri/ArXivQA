# [Explaining with Contrastive Phrasal Highlighting: A Case Study in   Assisting Humans to Detect Translation Differences](https://arxiv.org/abs/2312.01582)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces a new approach to generating contrastive phrasal highlights that explain the predictions of NLP models comparing two text inputs. The method aligns phrases across the inputs and highlights minimal phrase pairs whose removal increases the model's prediction of semantic equivalence. Through proxy evaluation on a dataset of human rationales and two IRB-approved user studies, the authors demonstrate that these highlights help bilingual speakers more reliably annotate fine-grained meaning differences in translations and detect critical machine translation errors. Specifically, the highlights assist crowdworkers with challenging tasks like identifying subtle translation divergences and local mistranslations that flip meaning. The paper argues that for translation and other tasks involving text comparison, explanations should be contrastive, i.e. show differences between inputs rather than just highlight important tokens. This work contributes the first rigorous human evaluation of highlight-based explanations on realistic applications like translation quality assurance. Key limitations are reliance on high-quality alignment and examining only high-resource languages. Overall, it provides evidence that properly-designed contrastive highlights can effectively guide people performing cognitively demanding NLP tasks.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a technique to generate contrastive phrasal highlights that explain the predictions of semantic divergence models by identifying minimal phrase pairs in two input texts that result in meaning differences.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new approach to generate "contrastive phrasal highlights" that explain the predictions of NLP models that compare two input texts, such as models for semantic textual similarity, machine translation evaluation, etc. 

Specifically, the key ideas are:

1) Explain predictions by highlighting minimal phrase pairs that differ across the two inputs, rather than highlighting independent salient tokens. This provides a more natural contrastive explanation.

2) Use phrase-alignment guided erasure to identify a minimal set of differing phrase pairs that most impact the model's predictions when erased.

3) Evaluate the usefulness of such highlights for assisting humans in tasks like detecting fine-grained meaning differences in translations or identifying critical machine translation errors. Human studies show the highlights help improve human performance.

In summary, the main contribution is a new approach for generating contrastive phrasal highlights to explain predictions of NLP models that compare two text inputs, along with evaluations showing these highlights are useful for assisting humans in related text analysis tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and skimming of the content, some of the key terms and concepts associated with this paper include:

- Contrastive phrasal highlighting
- Explainable NLP
- Translation differences
- Semantic divergence model
- Phrase-alignment-guided erasure
- Proxy evaluation
- Application-grounded evaluation
- Detecting fine-grained meaning differences
- Assisting humans 
- Human translations
- Machine translation errors

The paper introduces a technique called "contrastive phrasal highlighting" to explain predictions of NLP models that compare two text inputs, rather than just highlighting important words in a single text. This is applied to a semantic divergence model that detects differences between an original sentence and its translation. The highlights are generated using phrase-alignment-guided erasure. The paper includes both proxy evaluations against human rationales, and human studies focused on applications like detecting differences in translations and machine translation errors. So the key ideas focus on contrastive explanation of comparative NLP models, with a case study on explaining and assisting with translation difference tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions using phrase alignment to guide the extraction of phrase pairs for deletion. What are some potential challenges or limitations of relying on phrase alignment, especially for low resource language pairs? How could the approach be adapted?

2. When extracting multiple explanations, the paper iteratively deletes extracted highlight pairs. Could this lead to cascading errors or less reliable subsequent highlights? How might this iterative deletion process be improved? 

3. The brevity reward encourages shorter highlights. Is there a risk this could overly simplify explanations or fail to highlight longer but highly relevant phrases? How could the tradeoff between brevity and completeness be balanced?

4. The paper evaluates on semantic divergence detection between translations. How might the approach transfer to explaining predictions for other textual similarity tasks like paraphrase identification or textual entailment? What adaptations might be needed?

5. The highlight extraction method seems tightly coupled to the downstream divergence detection model. How difficult would it be to decouple the method to apply it to any black box textual similarity model? What challenges might arise?

6. The user studies suggest the highlights help detect some but not all types of translation errors. What adaptations could make the approach more sensitive to harder-to-spot errors like negations?  

7. Could the alignment and highlighting approach be modified to produce contrastive explanations for non-translation tasks by artificially generating a contrastive input? How might this contrastive input be produced?

8. The alignment model uses multilingual embeddings. How sensitive are the explanations to the choice of alignments? What is the error tolerance and could explanations degrade with lower quality alignments? 

9. The user studies had small sample sizes. What scale of study across tasks and language pairs would be needed to conclusively demonstrate the utility of contrastive highlights for human end tasks?

10. The paper studies human translation and machine translation errors. In what other applications could contrastively highlighting translation divergences provide value - for example in localization or studying systemic biases in systems?
