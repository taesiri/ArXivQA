# [Can multiple-choice questions really be useful in detecting the   abilities of LLMs?](https://arxiv.org/abs/2403.17752)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Multiple-choice questions (MCQs) are widely used to evaluate large language models (LLMs), but there are concerns about whether MCQs can truly measure LLMs' capabilities compared to long-form generation questions (LFGQs). 

- The misalignment between the MCQ evaluation format and real-world use cases that require LFG answers warrants an analysis of MCQs' efficacy in assessing LLMs.

Research Questions
- How does the arrangement of options in MCQs influence LLMs' selection of responses? 
- What methodologies can compare MCQs and LFGQs? What aspects should be considered in comparative tests?

Methods & Contributions 
- Showed LLMs have order sensitivity in MCQs, favoring answers in specific positions, challenging reliability.
- Proposed methods to quantify consistency and confidence of LLM outputs. Found higher consistency doesn't indicate better accuracy.  
- Compared MCQs & LFGQs in output, token logits & embedding spaces. Showed low correlation between MCQ & LFGQ answers for same questions.
- MCQs were less reliable than LFGQs in expected calibration error. The misalignment was also reflected in the embedding spaces.

Recommendations
- Choice of QA format should match knowledge type evaluated. LFGQs are better for specialized domains.  
- Adjusting number of MCQ options doesn't necessarily improve accuracy/confidence. But reordering helps.
- Don't rely on consistency to enhance LLM performance.
- Prioritize LFGQs for evaluation as it aligns with real use cases. Evaluate from multiple perspectives.

In summary, the paper conducts a comprehensive analysis highlighting limitations of MCQs in evaluating LLMs through comparative experiments with LFGQs, and provides recommendations for improving QA evaluation practices.
