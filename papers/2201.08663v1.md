# [Fast Differentiable Matrix Square Root](https://arxiv.org/abs/2201.08663v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we efficiently compute the differentiable matrix square root and its inverse in a way that is fast and stable for use in deep learning applications?

The key points are:

- Computing the differentiable matrix square root and its inverse is important for various computer vision tasks, such as covariance pooling, decorrelated batch normalization, and whitening and coloring transforms. 

- Existing methods using SVD or Newton-Schulz iteration have drawbacks in terms of speed, stability, or efficiency. 

- This paper proposes two new methods (Matrix Taylor Polynomial and Matrix Pade Approximants) for the forward pass that are considerably faster. It also proposes an iterative Lyapunov solver for the backward pass that is more efficient.

- Experiments demonstrate the speed and stability improvements over SVD and Newton-Schulz, while achieving competitive accuracy on tasks like decorrelated batch normalization and second-order vision transformers.

In summary, the central hypothesis is that the proposed methods can compute the differentiable matrix square root and its inverse more efficiently and stably compared to prior techniques, enabling improved performance on computer vision models that leverage these operations. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing two efficient methods to compute the differentiable matrix square root in deep learning:

- Matrix Taylor Polynomial (MTP) for fast forward propagation.

- Matrix Pade Approximants (MPA) for more accurate forward propagation. 

2. Developing an iterative Lyapunov solver using the matrix sign function for efficient backpropagation. This avoids expensive computations like SVD.

3. Demonstrating through experiments that the proposed MTP-Lya and MPA-Lya methods are faster than prior techniques like SVD and Newton-Schulz iteration, while achieving competitive or better accuracy on tasks like decorrelated batch normalization and second-order vision transformers.

4. Providing Pytorch implementations of the different matrix square root techniques to promote accessibility.

In summary, the key contributions are introducing faster and more accurate techniques for differentiable matrix square root calculation in deep learning, both in the forward and backward passes. This is shown to improve speed and accuracy on relevant applications compared to prior methods. The code release also makes these new techniques easy to apply in practice.
