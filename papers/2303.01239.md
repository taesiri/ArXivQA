# [MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource   Visual Question Answering](https://arxiv.org/abs/2303.01239)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How can we efficiently adapt pretrained vision-language models to low-resource visual question answering while achieving better performance than full finetuning?Specifically, the paper proposes a redundancy-aware parameter-efficient tuning method called MixPHM to address two key challenges:1) Reducing parameter redundancy while maintaining model capacity. The paper notes that existing parameter-efficient methods like adapters dramatically reduce tunable parameters but there is still a performance gap compared to full finetuning. The proposed MixPHM method aims to strike a balance between parameter efficiency and model capacity.2) Reducing task-irrelevant redundancy while promoting task-relevant correlation in representations. The paper argues that pretrained models contain redundant/irrelevant information that hinders performance on downstream tasks. MixPHM aims to reduce this task-irrelevant redundancy while promoting task-relevant correlation via a proposed "redundancy regularization".The central hypothesis is that by addressing these two challenges, the proposed MixPHM method can outperform full finetuning of vision-language models on low-resource VQA by tuning only a tiny fraction of parameters in a redundancy-aware manner. Experiments on several VQA datasets appear to validate this hypothesis.In summary, the paper proposes and evaluates a redundancy-aware parameter-efficient tuning approach to efficiently adapt vision-language models to low-resource VQA while achieving better performance than full finetuning.


## What is the main contribution of this paper?

The main contribution of this paper is proposing MixPHM, a redundancy-aware parameter-efficient tuning method to adapt pretrained vision-language models (VLMs) to the visual question answering (VQA) task under low-resource settings. Specifically, the key points are:1. MixPHM is a lightweight module inserted into pretrained VLMs, implemented with multiple Parameterized Hypercomplex Multiplication (PHM) experts in a mixture-of-experts manner.2. To reduce parameter redundancy, MixPHM reparameterizes expert weights into a low-rank subspace and shares weights globally across experts and locally within each expert. 3. Motivated by a redundancy analysis, MixPHM proposes a novel Redundancy Regularization to reduce task-irrelevant redundancy while promoting task-relevant correlation in representations.4. Extensive experiments show MixPHM achieves better performance and parameter efficiency compared to state-of-the-art methods and full finetuning. It consistently outperforms them on VQA v2, GQA and OK-VQA under various low-resource settings.5. The proposed MixPHM provides a new perspective on how to better adapt pretrained VLMs to downstream tasks by maximizing acquisition of task-relevant information and minimizing task-irrelevant redundancy from VLMs.In summary, the main contribution is proposing MixPHM, an effective and parameter-efficient method for low-resource VQA by reducing redundancy and improving correlation in representations. The experiments demonstrate its superiority over existing methods.
