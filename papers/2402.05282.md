# [TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing](https://arxiv.org/abs/2402.05282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Form understanding is an important task in visually rich document understanding (VRFU). However, current datasets like FUNSD have limitations: they decompose the task into separate labeling and linking tasks, have annotation inconsistencies, and don't fully capture hierarchical structures like tables. This makes end-to-end evaluation and complete form parsing difficult.

Proposed Solution: 
- The paper proposes new end-to-end evaluation metrics for FUNSD-style annotations, including a tree F1 score and node alignment accuracy (NAA) metric.
- It introduces a new tree-based annotation scheme called TreeForm that more fully captures form structure in a JSON format, including headers, questions/answers, and tables. 
- It details how to convert FUNSD annotations to TreeForm, using spatial information and heuristics to reconstruct details like tables.
- TreeForm predictions can be evaluated using a modified normalized tree edit distance called GAnTED.

Main Contributions:
- Novel end-to-end metrics to evaluate FUNSD annotation predictions
- New TreeForm annotation scheme that more completely captures form structure
- Method to convert FUNSD to TreeForm annotations 
- Evaluation of TreeForm using GAnTED scores
- Experiments applying state-of-the-art models on FUNSD and TreeForm, assessing tradeoffs and providing initial baselines

In summary, the paper tackles limitations of existing form understanding datasets by proposing improved evaluation schemes and a new TreeForm annotation format that could enable better form parsing. Initial experiments showcase some benefits of these proposals.


## Summarize the paper in one sentence.

 This paper proposes a novel tree-based annotation scheme called TreeForm to represent the hierarchical structure of forms, provides methods to convert existing annotations into TreeForm, and introduces new metrics to evaluate form parsing models on end-to-end understanding.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Providing novel end-to-end metrics for evaluating FUNSD-type annotations.

2. Presenting a novel tree-based representation of forms called TreeForm that is stored as a single JSON object. Also using the greedy-aligned tree-edit distance (GAnTED) to evaluate TreeForm predictions.

3. Detailing a method to transform any FUNSD-type annotations into TreeForm structures, which captures additional hierarchical and tabular information not contained in the original FUNSD annotations. 

4. Evaluating TreeForm and the new metrics using state-of-the-art models on the FUNSD and XFUND datasets. The baselines achieve a node-alignment accuracy of 0.22, end-to-end F1 score of 61.5, and TreeForm GAnTED score of 14.5 across all languages.

So in summary, the main contribution is proposing the new TreeForm annotation scheme along with end-to-end evaluation metrics and providing initial baselines for form parsing using this representation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Visually Rich Form Understanding (VRFU): Focus on understanding forms, which are documents containing hierarchical key-value pairs about specific entities.

- FUNSD dataset: A standard VRFU dataset that decomposes form parsing into entity labeling and linking tasks. Limitations include inconsistent annotations and inability to fully capture form structure. 

- End-to-end evaluation: The paper proposes a new F1 metric to evaluate models on their combined performance on entity labeling and linking for FUNSD-type datasets.

- TreeForm: A novel tree-based annotation scheme proposed that enables complete representation of forms using a single JSON object. Converts FUNSD annotations and evaluates predictions using a modified tree edit distance.

- LayoutXLM and Donut: State-of-the-art models evaluated using the proposed TreeForm scheme and end-to-end metrics. Experiments conducted on FUNSD and XFUND datasets across multiple languages.

- Greedy-aligned normalized tree edit distance (GAnTED): Metric used to evaluate the similarity between predicted and ground truth TreeForm annotations.

- Headers, question-answer pairs, tables: The key structures that comprise forms according to the TreeForm scheme.

So in summary, key terms cover the TreeForm annotation scheme, models evaluated, datasets used, and metrics proposed for end-to-end form understanding and evaluation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper proposes a new tree-based annotation scheme called TreeForm. How does TreeForm differ from previous annotation schemes like FUNSD in terms of capturing hierarchical structure and relationships in forms? What specific limitations does it address?

2. The paper describes converting FUNSD annotations to TreeForm annotations. What heuristics and rules are used to construct the more complex TreeForm structure from FUNSD? How are issues like incomplete information and inconsistencies in FUNSD handled? 

3. For evaluating TreeForm predictions, a variant of the normalized tree edit distance called GAnTED is used. How is GAnTED modified compared to the normal normalized tree edit distance? Why is it a more suitable metric for TreeForm?

4. What pre and post-processing steps were required to fine-tune and evaluate the LayoutXLM and Donut models on TreeForm annotations? How did you handle issues with the generative nature of Donut?

5. The results show Donut outperforms LayoutXLM on the GAnTED metric for TreeForm in most languages. Why would an end-to-end model like Donut be expected to do better on understanding hierarchical structure compared to a pipelined model?

6. The paper states the GAnTED scores seem similar across languages, attesting to the generalizability of TreeForm. Is the consistency across languages also seen in the other metrics? If not, why?

7. The end-to-end F1 score is lower than the labeling and linking F1 scores, indicating an issue combining the metrics. How could the end-to-end F1 metric be improved to better capture overall form understanding?

8. For the node alignment accuracy metric, could incorporating information like bounding boxes improve aligning entities beyond just using Levenshtein distance between text? What challenges would this introduce?

9. The heuristics used to convert FUNSD to TreeForm rely on assumptions about form structure that may not always hold. How could the conversion process be improved to handle more complex and irregular forms? 

10. The paper focuses on FUNSD-style annotations, but mentions other schemes like FUNSD+ and NAF. Could the TreeForm annotation and evaluation methods be extended to these other schemes? What modifications would be required?
