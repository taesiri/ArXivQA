# [Surfacing Biases in Large Language Models using Contrastive Input   Decoding](https://arxiv.org/abs/2305.07378)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can a new decoding strategy called Contrastive Input Decoding (CID) be used to help surface subtle biases and sensitivity to perturbations in large language models (LLMs)?2) Can CID highlight context-specific biases in LLMs that are difficult to detect using standard decoding strategies? 3) Can CID be used to quantify the relative effect of different types of perturbations to LLMs (e.g. syntactic vs semantic)?In particular, the authors propose CID as a way to generate text continuations that are likely under one input but unlikely under a contrastive/perturbed version of that input. By increasing the contrast level, CID aims to reveal differences in how the LLM treats the two inputs. The two main applications explored are:1) Using CID to detect subtle, context-specific biases related to notions like fairness and counterfactual fairness.2) Using CID to quantify the sensitivity of LLMs to different perturbation types, with the goal of testing alignment with user expectations.So in summary, the main research questions focus on whether CID can surface biases and quantify perturbation effects in large language models in order to audit them.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Contrastive Input Decoding (CID), a new decoding strategy for large language models. CID takes as input two sequences - the original input text and a "contrastive" perturbed version. It then generates text that is likely under the original input but unlikely under the contrastive input. The key ideas are:- Modify the next token probabilities during decoding based on the difference between the original and contrastive contexts. Tokens more likely under the original are upweighted, while tokens more likely under the contrastive are downweighted.  - A hyperparameter λ controls the degree of contrasting. Setting λ=0 recovers standard decoding.- Increasing λ surfaces subtle differences in how the language model responds to the two inputs.The authors show two applications of CID:1. Auditing language models for fairness. CID can reveal biases that are hard to detect with standard decoding.2. Quantifying the effect of different input perturbations. CID provides a way to measure the relative impact of various modifications like spelling mistakes or semantic changes.So in summary, the key contribution is proposing CID as a novel decoding algorithm for interpreting differences in language model behavior on contrastive inputs. This facilitates auditing models and understanding their sensitivity to perturbations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes a new decoding method called Contrastive Input Decoding (CID) to generate text continuations for a language model that are likely for one input text but unlikely for a contrastive perturbed version of that text. This allows surfacing subtle differences in how the language model treats the two inputs, which can be used for auditing biases or quantifying the impact of different input perturbations. The key idea is to modify the next-token probabilities during decoding based on the difference between the two inputs.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related work:- The use of contrastive input decoding (CID) to study potential biases in large language models is novel. Most prior work has focused on evaluating biases through downstream classification tasks or probing the internal representations of models. Using CID to generate contrastive text is an interesting way to surface biases in an open-ended, generative setting.- CID is related to prior work on contrastive explanations, but differs in that it contrasts inputs rather than outcomes. The goal is to understand model behavior rather than explain specific predictions. The idea of using contrastive decoding to improve generation quality has been explored before, but not for auditing biases.- The authors demonstrate applications of CID for studying context-specific biases and quantifying the impact of perturbations. These are practically motivated problems, as understanding model biases and robustness is important for real-world deployment. The "testing alignment with user expectations" experiment is especially creative.- Compared to work that analyzes model internals directly, this method has the benefit of being model-agnostic and focused on observable outputs. However, it does not provide the same level of insight into the root causes behind biases.- Overall, CID seems like a flexible and interpretable tool for auditing generative models. The experiments are nicely designed to highlight its potential. The results align with and complement prior findings using other methods. I think this represents an interesting and promising new technique for bias analysis.In summary, the CID framework for contrastive decoding seems novel and well-suited for studying biases and robustness in large language models. The paper makes a nice contribution to the growing literature on auditing these models. The proposed method is intuitive and can provide insights beyond what existing approaches offer.


## What future research directions do the authors suggest?

 The authors suggest several promising future research directions based on their work:- Using CID to aid prompt engineering by providing developers an interpretable way to understand the impact of modifications to the task description/prompt. This could help streamline the process of prompt engineering.- Exploring whether contrastive techniques like CID can help make models more robust to minor input variations. The authors suggest this could be done by augmenting the training data with contrastive examples. - Evaluating whether CID helps align models better with human intuitions about which input modifications should have a bigger vs smaller effect. The authors suggest doing this via larger user studies.- Applying CID to understand model biases and sensitivity to inputs beyond just text generation tasks. For example, the authors suggest it may be useful for interpreting image classification models.- Developing quantitative metrics based on CID to evaluate model robustness and alignment with human expectations. The authors' current approach relies more on qualitative assessment.- Exploring different choices of the scaling function used in CID beyond the exponential function. Different functions may surface different behaviors.- Applying CID to models beyond autoregressive LMs like GPT and T5. The authors suggest it may also be applicable to models like DALL-E or Codex.In summary, the main future directions are using CID to improve training and evaluation of LMs, and applying the ideas more broadly to understand model behaviors in different modalities and settings. Developing more quantitative metrics based on CID is also highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes Contrastive Input Decoding (CID), a new decoding strategy for large language models (LLMs) that generates text given two inputs: a regular input x and a “contrastive” input x’. The goal is to generate text that is likely under x but unlikely under x’. This is done by modifying the next-token probabilities during decoding based on the difference between the probabilities assigned to each token by the LLM under x vs x’. A hyperparameter λ controls the degree of contrasting. The authors show how CID can be used to surface subtle context-specific biases in LLMs that are hard to detect with standard decoding strategies. They also use CID to quantify the relative effect of different types of input perturbations, revealing for example that a LLM may be more sensitive to syntactic vs semantic changes. Overall, CID provides an interpretable way to understand differences in LLM behavior on similar inputs.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes Contrastive Input Decoding (CID), a new decoding strategy for large language models (LMs) like GPT-3. CID takes as input a regular text prompt x and a contrastive (modified) version x'. It then generates text that is likely under x but unlikely under x', in order to highlight differences in how the LM handles the two inputs. The authors demonstrate two applications of CID: (1) Detecting subtle biases in LMs. By using names associated with different genders/races as the modification between x and x', CID can reveal biases that are hard to surface with standard decoding. (2) Quantifying the effect of perturbations. CID can determine the minimum amount of "contrasting" needed to push model outputs far apart for different input perturbations. This allows assessing if perturbations align with user expectations (e.g. typos should have smaller effects than semantic changes). The authors show examples of using CID to uncover biases in medical QA and measure relative sensitivity to perturbation types. Overall, CID provides a simple but interpretable method for auditing generative LMs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper "Surfacing Biases in Large Language Models using Contrastive Input Decoding":The paper proposes a new decoding algorithm called Contrastive Input Decoding (CID) for generating text continuations from large language models (LMs). CID takes as input the original context along with a perturbed "contrastive" version of the context. It then generates a continuation that is likely under the original context but unlikely under the contrastive context. This is achieved by modifying the next-token probabilities during decoding, increasing probabilities for tokens that are more likely under the original context compared to the contrastive context, and decreasing probabilities for tokens that are less likely. A hyperparameter λ controls the degree of contrasting. In this way, CID can generate contrastive continuations that highlight differences in how the LM treats the original vs. contrastive contexts. The authors demonstrate using CID to surface subtle biases in LMs that are hard to detect with standard decoding, as well as to quantify the impact of different input perturbations.


## What problem or question is the paper addressing?

 The paper is addressing the challenge of evaluating how modifications to the input of large language models impact their behavior and outputs in open-ended text generation tasks. Specifically, it highlights two main issues:1. Surfacing biases in large language models: The paper proposes using "Contrastive Input Decoding" (CID) to reveal subtle context-specific biases that may be hard to detect using standard decoding strategies like greedy decoding or beam search. It shows how CID can reveal demographic biases in contexts like answering medical questions or providing justifications for failing a job interview. 2. Quantifying the impact of different input perturbations: The paper also demonstrates using CID to quantify and compare the effects of different types of input modifications like spelling mistakes, synonyms, irrelevant info, etc. This is important for testing if models conform to user expectations around robustness (e.g. that spelling mistakes should have a smaller effect than major semantic changes). Again, standard decoding makes it hard to quantify these effects in open-ended text generation.In summary, the key problem is finding better ways to understand how models behave in free-form text generation when modifications are made to the inputs, especially around fairness and robustness. The paper proposes CID as a decoding strategy to address these challenges.
