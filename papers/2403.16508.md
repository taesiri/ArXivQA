# [Return to Tradition: Learning Reliable Heuristics with Classical Machine   Learning](https://arxiv.org/abs/2403.16508)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Current deep learning approaches for learning for automated planning have issues with needing to tune many hyperparameters, lacking interpretability, and being computationally demanding. They also have not achieved competitive performance against classical planners.

- Prior classical machine learning methods for learning for planning predate deep learning approaches but have not been revisited.

Proposed Solution:
- The paper proposes WL-GOOSE, a new approach that leverages classical machine learning for learning for automated planning. 

- It introduces the Instance Learning Graph (ILG), a novel graph-based representation of lifted planning tasks. The nodes represent objects and facts (propositions) in the initial state and goal. Edges connect facts to the objects they contain.

- It runs a modified Weisfeiler-Lehman (WL) algorithm on the ILG to generate graph feature vectors. The features are domain-independent and do not require backpropagation.

- The WL features are then used to train classical machine learning models like support vector regression (SVR) and Gaussian processes (GP) to learn domain-specific heuristics.

Main Contributions:

- First learned heuristic to outperform h^FF and achieve high coverage across planning domains. The SVM/GP heuristics solve up to 490 problems, outperforming h^FF's 430 problems solved.

- First learned heuristic to match or exceed LAMA's performance on 4 out of 10 domains on coverage and 7 out of 10 on plan quality. 

- Up to 3 orders of magnitude faster training time than graph neural networks. Extremely sample efficient with under 100 training instances.

- Provides theoretical analysis showing WL features are equally powerful to GNNs for planning and incomparable to Description Logic features.

- Demonstrates interpretability of the classical ML models by visualizing most important WL features.

In summary, the paper presents a novel and efficient approach to learning for planning that matches and even exceeds the state-of-the-art in performance. It leverages classical ML to overcome limitations of deep learning techniques. The promising empirical results combined with theoretical connections make this an important contribution.


## Summarize the paper in one sentence.

 This paper introduces WL-GOOSE, a novel approach for efficiently learning domain-specific heuristics for planning using graph kernels generated by a modified Weisfeiler-Lehman algorithm operating on a new Instance Learning Graph representation of planning tasks.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) It introduces WL-GOOSE, a novel approach for learning for planning that leverages the efficiency of classical machine learning methods. This includes representing planning tasks as Instance Learning Graphs (ILGs) and using a modified Weisfeiler-Lehman algorithm to generate features from the graphs. 

2) It shows both theoretically and empirically that the WL features used by WL-GOOSE are highly expressive for representing planning tasks, more so than some previous approaches like Muninn.

3) When used with classical ML models like SVMs and Gaussian processes, WL-GOOSE is able to efficiently and reliably learn high-quality heuristics from scratch. The learned heuristics outperform both the h^FF heuristic and state-of-the-art learning for planning methods like GOOSE in terms of coverage. They also match or exceed the performance of the LAMA planner on several planning domains.

4) Compared to graph neural networks, the models learned by WL-GOOSE have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster, while still achieving better performance. This demonstrates the efficiency benefits of classical ML with WL features for learning for planning.

In summary, the main contribution is an efficient, reliable approach to learning for planning that leverages classical ML and achieves state-of-the-art performance compared to prior methods. The representations and features introduced by WL-GOOSE are shown to be highly useful for this task.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Weisfeiler-Leman (WL) algorithm - Used to generate features from graph representations of planning tasks. The paper proposes a modified version to handle edge labels.

- Instance Learning Graph (ILG) - A novel graph-based representation proposed for lifted planning tasks. WL features are generated from the ILG.

- Classical machine learning - The paper shows how classical ML models like SVMs and Gaussian processes can efficiently learn heuristics using WL features, contrasting with recent deep learning approaches.

- Expressivity - The paper analytically compares the expressive power of WL features to other approaches like GNNs and Description Logic features in distinguishing planning tasks. 

- Generalization - The learned heuristics using WL features and classical ML are shown to generalize well across planning domains compared to prior learned heuristics.

- Efficiency - Training times and number of parameters of the models using WL and classical ML are orders of magnitude better than neural approaches.

- Interpretability - The features generated by WL are more interpretable than neural approaches. An example is provided of understanding the semantic meaning of features.

In summary, the key ideas focus on a novel graph representation, use of the WL algorithm to generate efficient and expressive features, and leveraging classical ML for scalable and reliable learning of heuristics. The approach is shown to outperform prior neural and handcrafted methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper introduces a new graph representation called the Instance Learning Graph (ILG). How is this representation constructed and what information does it encode compared to previous graph-based representations for planning tasks?

2. The paper proposes using a variant of the Weisfeiler-Lehman (WL) algorithm to generate features from the ILG representation. How is the standard WL algorithm modified to handle the edge labels in the ILG? What is the complexity of computing these WL features?

3. The paper shows that the expressivity of the WL features is equivalent to that of Graph Neural Networks (GNNs) acting on the ILG representation. What does this result imply about the distinguishing power of the two methods? Can you outline the key ideas behind the proof?  

4. How exactly are the WL features from the ILG representation used to train classical machine learning models like SVMs and Gaussian Processes? What is the advantage of this approach over end-to-end learning with GNNs?

5. The paper argues that the WL features can be easily interpreted to understand what the learned models are capturing, unlike GNNs. Can you walk through Fig. 8 in the paper and describe how to deduce the semantic meaning behind some of the highlighted WL features?

6. What theoretical connections are established between the WL features and other methods like Description Logic features and the Muninn architecture? Can you summarize the key results regarding their expressivity using the notation defined in the paper?

7. Why does kernelizing the WL features with non-linear kernels generally lead to worse empirical performance even though it theoretically expands the feature space? Provide some hypotheses that may explain this phenomenon.

8. The Gaussian Process model outputs uncertainty estimates along with its predictions. How meaningful are these uncertainty bounds based on the analysis in Table 3? When and why do they correlate well with prediction errors?

9. The paper claims the WL method is more sample and computationally efficient compared to GNNs. Can you quantify these gains using some key statistics reported, especially regarding training time and number of parameters?

10. The counterexample constructed in Corollary 5 indicates all the feature generation methods considered still cannot guarantee learning optimal heuristics across all planning domains. What aspects are still missing and how might the expressivity be further improved?
