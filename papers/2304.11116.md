# Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via   Prompt Augmented by ChatGPT

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question it aims to address is:How to empower existing large language models (LLMs) with the ability to conduct reasoning on graph structured data?The paper proposes a framework called Graph-ToolFormer to teach LLMs to use external graph data loading and reasoning tools to handle various graph reasoning tasks. The goal is to give LLMs the capabilities to handle tasks involving complex graph data, such as computing graph properties, analyzing bibliographic networks, predicting molecular graph functions, making recommendations, detecting communities, and reasoning on knowledge graphs. The central hypothesis is that by fine-tuning LLMs on a dataset of prompts augmented with graph API calls, the models can learn to automatically generate appropriate API calls to external graph tools in order to accomplish diverse graph reasoning tasks. The prompts are generated based on a small number of hand-crafted examples, then expanded via ChatGPT.In essence, the paper explores methodologies to overcome weaknesses of current LLMs in areas like mathematical calculation, multi-step logic, spatial/topological reasoning, and temporal progression. The Graph-ToolFormer framework aims to imbue LLMs with enhanced capacities for graph data reasoning across various real-world domains.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper appear to be:1. Proposing Graph-ToolFormer (GTF), a framework to empower large language models (LLMs) like GPT with the ability to perform graph reasoning tasks. 2. Using ChatGPT to annotate and augment a large graph reasoning dataset with API calls to external graph reasoning tools. This allows teaching the LLM how and when to leverage these tools.3. Conducting extensive experiments on diverse graph reasoning tasks using real-world benchmark datasets. Tasks include graph property calculations, paper topic inference, molecular graph function prediction, recommendation, community detection, and knowledge graph reasoning.4. Releasing the source code for GTF, the annotated graph reasoning datasets, and pre-trained checkpoints for the graph reasoning models to facilitate further research.In summary, the key innovation seems to be developing GTF to adapt LLMs to handle graph reasoning, which they currently struggle with due to limitations in mathematical calculations, logic reasoning, and spatial/topological perception. Augmenting the training data using ChatGPT to provide examples of API usage allows efficient teaching of the LLM when and how to leverage external tools. The extensive experiments demonstrate the effectiveness of this approach across a variety of graph reasoning tasks. By open-sourcing the key components, the authors aim to bridge graph learning and LLMs to advance research at the intersection of these areas.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- The paper presents a new approach for empowering large language models (LLMs) with graph reasoning abilities. This is a novel contribution as most prior work on LLMs has focused on natural language and vision tasks, not graph reasoning. The idea of using prompt learning and external tools to teach LLMs graph reasoning capabilities is innovative.- Compared to other methods for graph reasoning, this approach leverages the power of pre-trained LLMs rather than training graph neural networks from scratch. Fine-tuning the LLMs is likely more practical and scalable than designing and training specialized graph networks. The framework allows incorporating diverse graph tools and datasets.- The variety of graph reasoning tasks tackled is quite broad, spanning basic property calculations, node/graph classification, link prediction, clustering, and search. This demonstrates the flexibility of the approach. Other papers usually focus on 1-2 graph reasoning applications.- The performance on some tasks like paper topic classification approaches state-of-the-art graph neural networks. For other complex reasoning tasks, the accuracy is lower but still reasonable given the early stage of this research direction. There is clear room for improvement.- The idea of using an LLM as a general interface for graph reasoning is novel and promising. This could expand the applicability of LLMs to structured graph data at scale. Most prior LLM research targets unstructured text.Overall, this paper pioneers a new research direction in empowering LLMs with structured graph reasoning abilities. The proposed methods are general and flexible. Performance is reasonably strong but could benefit from advances in prompt learning, tool integration, and continual learning to mitigate forgetting. The ideas open many possibilities to connect LLMs with graph learning.
