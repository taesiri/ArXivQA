# [An Experiment on Feature Selection using Logistic Regression](https://arxiv.org/abs/2402.00201)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Feature selection is important in machine learning to reduce dimensionality, improve model performance, and enhance interpretability. However, finding the optimal subset of features is a difficult problem. 
- The paper investigates using L1 and L2 regularization with logistic regression for feature selection. 

Proposed Solution:
- Use logistic regression with L1 regularization to rank features and select the top MAX_L1 features. 
- Similarly, use L2 regularization to select top MAX_L2 features.  
- Take the intersection of these L1 and L2 sets to find "common features".
- Evaluate machine learning models using just these common features.

Dataset:
- Experiments done on large real-world CIC-IDS2018 network intrusion detection dataset.
- Has 78 features over 12 classes, with over 50K samples.
- One "problematic class" is hard to distinguish from another "confounding class".

Models Compared:
- Logistic regression with L1 regularization (LR+L1)
- Logistic regression with L2 regularization (LR+L2)  
- Random forest (RF)
- Decision tree (DT)

Key Results:
- Found 22 "common features", a 72% reduction in features.
- Accuracy using common features was close to full set for RF and DT.  
- For LR, achieved baseline accuracy with 15% less features (L1) and 57% less (L2).
- Results hold even including problematic class, with <1% accuracy loss for RF/DT.

Main Contributions:
- Novel method to synthesize L1 and L2 feature selection
- Shows feature selection can greatly reduce dimensionality without significant accuracy loss
- Demonstrates approach on real-world dataset and with multiple ML models


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes and experimentally evaluates a feature selection method that synthesizes the findings of L1 and L2 regularization applied to logistic regression to obtain a small set of important features that performs well with linear and complex machine learning models on a large real-world network intrusion detection dataset.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution appears to be:

The authors propose and experimentally validate a method for feature selection that synthesizes the findings from L1 and L2 regularization applied to logistic regression. Specifically:

1) They obtain a ranked list of features using L1 regularization (LR+L1), which tends to zero out coefficients of unimportant features. 

2) They obtain another ranked list using L2 regularization (LR+L2), which shrinks coefficients more uniformly.

3) They take the intersection of these two lists to obtain a set of "common" important features.

4) They show experimentally on the CIC-IDS 2018 dataset that:

(a) This set of common features, though much smaller in size (22 vs 78 features), achieves a classification accuracy very close to using all features for multiple ML models - logistic regression, random forest and decision trees.

(b) It outperforms keeping the same number of top L1 or L2 features. 

(c) The accuracy is maintained even when including a problematic class that is hard to distinguish.

So in summary, the key contribution is an effective logistic regression-based feature selection approach that synthesizes L1 and L2 regularization, showing strong experimental results on a complex real-world dataset.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Feature selection
- Logistic regression 
- L1 regularization
- L2 regularization
- CIC-IDS2018 dataset
- Overfitting
- Sparsity 
- Classification
- Machine learning
- Accuracy
- Precision
- Recall
- F1-score
- Decision trees
- Random forest

The paper presents an experiment on feature selection for machine learning classification models using L1 and L2 regularized logistic regression. It analyzes the CIC-IDS2018 intrusion detection dataset, compares feature rankings and selections based on L1 versus L2 regularization, and evaluates the impact on model accuracy, precision, recall and F1 score when using Decision Trees and Random Forest classifiers. The goal is reducing overfitting and dimensionality in order to improve model performance. The key terms reflect this focus on feature selection, regularization methods, evaluation metrics, and machine learning models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the proposed feature selection method in this paper:

1) How exactly did you determine the parameters for the "SAGA" solver used in the logistic regression models? Did you use cross-validation, or some other method to tune them? 

2) You mention that the common feature set primarily followed the L1-rank ordering. Did you investigate why certain features were ranked much higher in the L2 regularization than the L1? What properties of those features might account for that?

3) For the experiments using just the top L1 or L2 features, why did you choose a fixed cutoff (MAX_L1 and MAX_L2) rather than using a validation set approach to determine the optimal number of features to use from each set?

4) In your analysis after including the problematic class, you focused only on the confusion between that class and the one "confounding" class. Did you investigate where the other misclassified examples for those two classes ended up? 

5) You attributed the performance drop after including the problematic class solely to issues separating that class and the confounding class. Did you do any additional experiments or analysis to confirm that diagnosis?

6) Have you compared the common feature set approach to any other feature selection methods, such as recursive feature elimination? If so, how did it compare?

7) For real-world deployment, would you recommend re-training the models completely or just using the fitted models from this study and only inputting the selected features?

8) Could the common features simply be acting as a dimensionality reduction technique rather than selecting more relevant features? Did you compare to a baseline like PCA? 

9) Did you experiment with any sampling or weighting techniques commonly used to deal with class imbalance, before settling on extracting a fixed 5K examples per class?

10) You mentioned the possibility of the common feature set matching the L1 feature set size as a limitation. In that case, what would your recommendation be for which set to use?
