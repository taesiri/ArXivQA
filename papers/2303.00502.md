# [On the Audio-visual Synchronization for Lip-to-Speech Synthesis](https://arxiv.org/abs/2303.00502)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How to address the audio-visual asynchrony issues in lip-to-speech synthesis, including both data asynchrony in the training datasets and model asynchrony caused by the lip-to-speech models themselves?The key hypotheses appear to be:1) Commonly used audio-visual datasets for lip-to-speech, such as GRID, TCD-TIMIT, and Lip2Wav, contain varying degrees of data asynchrony between the audio and video.2) This data asynchrony, if not addressed, can cause model asynchrony issues where the trained lip-to-speech models generate speech that is out of sync with the input video. 3) Explicitly modeling and correcting for data and model asynchrony during training can improve synchronization in lip-to-speech models and lead to performance gains on both objective metrics and subjective listening.To address these hypotheses, the paper proposes a synchronized lip-to-speech (SLTS) architecture with an automatic synchronization mechanism to correct data asynchrony and penalize model asynchrony during training. The effectiveness of this approach is then evaluated on various datasets and metrics.In summary, the central research question is how to handle audio-visual synchronization issues in lip-to-speech modeling, and the key hypotheses are that explicitly modeling synchronization in the training process can mitigate these issues and improve performance.
