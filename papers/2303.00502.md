# [On the Audio-visual Synchronization for Lip-to-Speech Synthesis](https://arxiv.org/abs/2303.00502)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How to address the audio-visual asynchrony issues in lip-to-speech synthesis, including both data asynchrony in the training datasets and model asynchrony caused by the lip-to-speech models themselves?

The key hypotheses appear to be:

1) Commonly used audio-visual datasets for lip-to-speech, such as GRID, TCD-TIMIT, and Lip2Wav, contain varying degrees of data asynchrony between the audio and video.

2) This data asynchrony, if not addressed, can cause model asynchrony issues where the trained lip-to-speech models generate speech that is out of sync with the input video. 

3) Explicitly modeling and correcting for data and model asynchrony during training can improve synchronization in lip-to-speech models and lead to performance gains on both objective metrics and subjective listening.

To address these hypotheses, the paper proposes a synchronized lip-to-speech (SLTS) architecture with an automatic synchronization mechanism to correct data asynchrony and penalize model asynchrony during training. The effectiveness of this approach is then evaluated on various datasets and metrics.

In summary, the central research question is how to handle audio-visual synchronization issues in lip-to-speech modeling, and the key hypotheses are that explicitly modeling synchronization in the training process can mitigate these issues and improve performance.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Identifying two types of audio-visual asynchrony issues in lip-to-speech synthesis: data asynchrony and model asynchrony. The paper analyzes common lip-to-speech datasets and finds varying degrees of audio-visual misalignment. It also explains how model asynchrony can arise during training on asynchronous data.

2. Proposing a synchronized lip-to-speech (SLTS) model architecture with an automatic synchronization mechanism (ASM) to address these asynchrony issues. The ASM contains two main components:

- Data synchronization module (DSM) - Estimates audio-visual offsets and uses them to correct misaligned training data, reducing data asynchrony.

- Self-synchronization module (SSM) - Penalizes model asynchrony by minimizing predicted offsets between generated audio and input video.

3. Demonstrating limitations of standard lip-to-speech evaluation metrics on asynchronous test data. The paper proposes using a time alignment frontend before these metrics to properly decouple synchronization from audio quality.

4. Extensive experiments showing the proposed model achieves state-of-the-art performance. The ASM is shown to be effective on datasets with both subtle and severe audio-visual misalignment.

In summary, the main contribution is identifying and providing solutions to address audio-visual synchronization issues in lip-to-speech synthesis, both during training and evaluation. The proposed SLTS model outperforms previous approaches when evaluated properly using time-aligned metrics.
