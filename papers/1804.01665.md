# [Learning to Separate Object Sounds by Watching Unlabeled Video](https://arxiv.org/abs/1804.01665)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn audio-visual object models from unlabeled video in order to perform audio source separation on novel videos. Specifically, the paper proposes an approach to learn what different objects sound like by looking at and listening to unlabeled videos containing multiple sounding objects. The key idea is that observing sounds in a variety of visual contexts can reveal cues to isolate individual audio sources, even though the sounds are mixed together in the audio track. The main hypothesis is that by discovering associations between audio frequency bases and visual objects in a large collection of unlabeled videos, the learned associations can be used to guide separation of object-level sounds in new videos.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing to enhance audio source separation in videos by using visual information from image recognition results as a form of "weak supervision". 2. Introducing a novel deep multi-instance multi-label learning framework to learn prototypical spectral patterns of different acoustic objects, and injecting this learned prior into an NMF source separation framework.3. Being the first to study audio source separation learned from a large scale of unlabeled "in the wild" online videos containing multiple audio sources per video. 4. Demonstrating state-of-the-art results on visually-aided audio source separation and audio denoising using this approach.In summary, the key innovation is using unlabeled video containing both visual and audio channels to learn audio-visual associations and models of how different objects sound. This learned prior knowledge is then used to separate and isolate sounds from individual objects in new videos with mixed audio. The main advantage is the ability to do audio source separation without clean labeled training data of isolated sounds.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an unsupervised approach using deep multi-instance multi-label learning on unlabeled videos to learn associations between visual objects and audio patterns, in order to perform audio source separation on novel videos by using the learned audio-visual models to guide separation.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on learning to separate object sounds by watching unlabeled video compares to other research on audio-visual source separation:- It proposes learning object-level sound models from a large number of unlabeled videos, rather than relying on low-level audio-visual correlations in a single video. This allows learning a generalizable prior from diverse training data.- The method operates on challenging "in the wild" videos with multiple objects and audio sources per video. In contrast, prior work has mostly focused on constrained domains like human speech or musical instruments. - It introduces a novel deep multi-instance multi-label learning framework to associate audio bases with visual objects, even without isolated examples of each object's sound. - The training data requirements are more flexible than prior work - it learns from unlabeled web videos rather than carefully collected matched audio-visual data.- It tackles a broader range of object-level sounds including instruments, animals, vehicles etc. Other recent work focuses specifically on speech or musical instruments.- The approach does not aim to synthesize sounds from silent video like some other audio-visual work, but rather separates real object sounds in videos.Overall, this work pushes audio-visual source separation in a less constrained, more naturalistic direction compared to prior work. The proposed learning framework and ability to discover object-level sound models from unlabeled video are notable advances.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Exploring ways to leverage scenes and ambient sounds, not just isolated objects. The current method focuses on separating sounds of individual objects, but extending it to utilize context from the surrounding scene and ambient sounds could be beneficial.- Integrating localized object detections and motion signals. The method currently relies on image-level object category recognition, but incorporating more precise spatially localized detections and motion cues could help further improve separation.- Improving the visual recognition component. The authors note they are constrained by the breadth of objects that current ImageNet-trained CNNs can recognize. Expanding the scope of recognizable visual concepts would allow separating a wider array of sounds.- Modeling sounds that are not visible or not within the camera's view. The paper notes their method can be robust to this to some extent already, but explicitly handling off-screen sounds is an area for improvement.- Adding grounding between the separated audio tracks and bounding boxes/segments in the visual frames. This could enable applications like audio-visual event indexing and retrieval.- Exploring self-supervised or weakly supervised training paradigms that require less manually labeled data. Reducing reliance on curated labeled datasets could allow scaling up more easily.In summary, the main future directions focus on incorporating more context, motion, and localization; improving the visual recognition component; explicitly handling off-screen sounds; adding finer grounding between modalities; and reducing manual labeling needs. Advancing these areas could move the method closer to real-world audio-visual separation and analysis applications.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes an unsupervised approach to learn how different objects sound by looking at and listening to unlabeled videos containing multiple sounding objects. The method first extracts audio frequency bases from the audio track of unlabeled videos using non-negative matrix factorization (NMF). It also detects visual objects in each video frame using an image recognition model. A multi-instance multi-label deep neural network is then trained to associate the audio bases with the detected visual objects. This allows the method to discover prototypical spectral patterns for each visual object category. Given a new video, the learned object-specific audio bases are used to guide NMF source separation and isolate the individual sound sources coming from each detected visual object. The authors demonstrate state-of-the-art performance on visually-guided audio source separation and audio denoising tasks using a large dataset of "in the wild" YouTube videos. A key advantage is the ability to learn from unlabeled multi-source audio, as opposed to methods that require cleanly isolated audio examples.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes an unsupervised approach to learn how different objects sound by looking at and listening to unlabeled videos containing multiple sounding objects. The method first uses image recognition models to detect objects in video frames and performs non-negative matrix factorization on the audio track to extract frequency basis vectors. A multi-instance multi-label learning framework is then used to associate the audio bases with the visually detected objects. This allows the method to recover prototypical spectral patterns for each object category from the ambiguously labeled training data. Given a new test video, the learned per-object audio bases are used to guide an NMF-based audio source separation to isolate the sound tracks corresponding to each detected visual object. The approach is evaluated on unlabeled videos from AudioSet and benchmark datasets. Quantitative experiments mixing single-source videos show the method outperforms baselines and prior work on metrics for audio source separation and denoising. Qualitative results on real videos demonstrate it can separate sounds belonging to diverse objects like musical instruments, animals, and vehicles. The idea of exploiting unlabeled video to learn cross-modal audio-visual associations is novel. The work could enable better audio understanding and editing for multimedia applications.


## Summarize the main method used in the paper in one paragraph.

The paper proposes an unsupervised approach to learn how different objects sound by looking at and listening to unlabeled videos containing multiple sounding objects. The key method is as follows:First, the authors use image recognition tools to detect objects in each video clip, and perform non-negative matrix factorization (NMF) on the audio track to extract frequency basis vectors. Then, they construct a multi-instance multi-label (MIML) neural network that maps the extracted audio bases to the distribution of visually detected objects. This allows disentangling which audio bases correspond to which objects. The learned per-object audio bases are collected to form prototypical spectral patterns for each object category. Finally, given a new test video, they detect objects in the frames and retrieve the corresponding learnt audio bases. These bases are used to guide NMF factorization of the test video's audio track in order to separate the sound for each detected object. Specifically, the retrieved bases are fixed as the dictionary, and NMF activation scores are estimated. The audio sources are then reconstructed using the bases and activation scores of each object.In summary, the key method is using a MIML framework to disentangle audio-visual object associations from unlabeled video, and exploiting the learned audio bases per object to guide source separation in new videos.
