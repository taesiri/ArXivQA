# [Learning to Separate Object Sounds by Watching Unlabeled Video](https://arxiv.org/abs/1804.01665)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn audio-visual object models from unlabeled video in order to perform audio source separation on novel videos. Specifically, the paper proposes an approach to learn what different objects sound like by looking at and listening to unlabeled videos containing multiple sounding objects. The key idea is that observing sounds in a variety of visual contexts can reveal cues to isolate individual audio sources, even though the sounds are mixed together in the audio track. The main hypothesis is that by discovering associations between audio frequency bases and visual objects in a large collection of unlabeled videos, the learned associations can be used to guide separation of object-level sounds in new videos.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing to enhance audio source separation in videos by using visual information from image recognition results as a form of "weak supervision". 2. Introducing a novel deep multi-instance multi-label learning framework to learn prototypical spectral patterns of different acoustic objects, and injecting this learned prior into an NMF source separation framework.3. Being the first to study audio source separation learned from a large scale of unlabeled "in the wild" online videos containing multiple audio sources per video. 4. Demonstrating state-of-the-art results on visually-aided audio source separation and audio denoising using this approach.In summary, the key innovation is using unlabeled video containing both visual and audio channels to learn audio-visual associations and models of how different objects sound. This learned prior knowledge is then used to separate and isolate sounds from individual objects in new videos with mixed audio. The main advantage is the ability to do audio source separation without clean labeled training data of isolated sounds.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an unsupervised approach using deep multi-instance multi-label learning on unlabeled videos to learn associations between visual objects and audio patterns, in order to perform audio source separation on novel videos by using the learned audio-visual models to guide separation.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on learning to separate object sounds by watching unlabeled video compares to other research on audio-visual source separation:- It proposes learning object-level sound models from a large number of unlabeled videos, rather than relying on low-level audio-visual correlations in a single video. This allows learning a generalizable prior from diverse training data.- The method operates on challenging "in the wild" videos with multiple objects and audio sources per video. In contrast, prior work has mostly focused on constrained domains like human speech or musical instruments. - It introduces a novel deep multi-instance multi-label learning framework to associate audio bases with visual objects, even without isolated examples of each object's sound. - The training data requirements are more flexible than prior work - it learns from unlabeled web videos rather than carefully collected matched audio-visual data.- It tackles a broader range of object-level sounds including instruments, animals, vehicles etc. Other recent work focuses specifically on speech or musical instruments.- The approach does not aim to synthesize sounds from silent video like some other audio-visual work, but rather separates real object sounds in videos.Overall, this work pushes audio-visual source separation in a less constrained, more naturalistic direction compared to prior work. The proposed learning framework and ability to discover object-level sound models from unlabeled video are notable advances.
