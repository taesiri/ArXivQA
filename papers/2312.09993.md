# [LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian   Language](https://arxiv.org/abs/2312.09993)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like LLaMA 2 have limited coverage of languages other than English. This makes them less effective for downstream tasks in those languages.  
- Adapting LLMs to new languages is important to improve their capabilities. However, continuing pre-training is computationally expensive, while adapter methods often reduce model quality.

Proposed Solution: 
- Use a parameter-efficient fine-tuning method called QLoRA to adapt LLaMA 2 to Italian language. This allows retaining most base model knowledge while adding Italian-specific understanding.
- Further fine-tune the adapted models on Italian dialog and instruction tuning datasets to specialize them for conversational and task-focused applications.  

Key Contributions:
- Introduce LLaMAntino family of Italian LLMs adapted from LLaMA 2 using QLoRA method. Models have 7B, 13B parameters.
- Also release LLaMAntino-Chat models fine-tuned on UltraChat dialog dataset translated to Italian.
- Additionally release instruction-tuned LLaMAntino models on Dolly and EVALITA datasets for improved task performance.
- Show qualitative improvements in Italian language understanding and generation abilities over original LLaMA 2.
- Provide weights for adapter-based models and weight-diffs for others to easily build all released models.
- Open source code and simplified model creation pipelines to enable further research.

In summary, the paper presents adapted Italian versions of LLaMA 2 to address limited multilingual transfer capabilities. The LLaMAntino models demonstrate stronger linguistic abilities in Italian across both conversational and task-focused domains.
