# [Making Pre-trained Language Models Better Continual Few-Shot Relation   Extractors](https://arxiv.org/abs/2402.15713)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Continual Few-shot Relation Extraction (CFRE) aims to continually learn to extract novel relations between entities in text while avoiding forgetting previously learned ones, using only a small number of labeled examples per relation. 
- CFRE faces two key challenges: catastrophic forgetting of old knowledge when learning new tasks, and overfitting to small training sets for new relations.

Proposed Solution - Contrastive Prompt Learning (CPL):
- Uses prompt learning to reformulate the extraction task into a cloze-style phrase prediction format, making it more similar to PLMs' pre-training and unlocking their capabilities.
- Introduces a novel margin-based contrastive loss to learn more generalized features focused on hard samples, alleviating overfitting and forgetting.  
- Augments memory using well-designed prompts for ChatGPT to generate diverse samples for low-resource relations.

Main Contributions:
- Proposes first exploration of prompt learning for CFRE to mitigate catastrophic forgetting and overfitting by better utilizing PLMs' knowledge.
- Designs effective prompt template and contrastive learning approach tailored for continual few-shot extraction.
- Introduces memory augmentation strategy with ChatGPT for sample generation, further remedying overfitting.
- Extensive experiments show state-of-the-art performance, significantly outperforming existing methods on two datasets. 

In summary, the key innovation is using prompt and contrastive learning to empower language models to become effective continual few-shot learners for extraction, mitigating key challenges. The overall framework and individual components are shown effective through extensive evaluations.
