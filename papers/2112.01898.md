# [Linear algebra with transformers](https://arxiv.org/abs/2112.01898)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can transformers learn to perform complex numerical computations, like operations of linear algebra, from examples only?The paper investigates whether transformers can be trained, using only input-output examples, to accurately compute solutions to various linear algebra problems like matrix addition, multiplication, inversion, and eigenvalue/eigenvector decomposition. The central hypothesis appears to be that transformers have the capability to learn these numerical computations to a high degree of accuracy, despite concerns that they may struggle with arithmetic and generalization. The experiments aim to demonstrate that transformers can achieve over 90% accuracy on these linear algebra tasks when trained on random matrix data. The paper also examines different encoding schemes for representing real numbers as tokens, and analyzes the models' robustness to noise and ability to generalize beyond their training distribution. Overall, the central research question seems to be whether transformers, as powerful sequence models, can be effectively applied to complex numerical computations in mathematics and science.


## What is the main contribution of this paper?

This paper presents an investigation into the capabilities of transformer models to perform numerical computations in linear algebra. The key contributions are:- Demonstrates that transformers can be trained to accurately perform a range of linear algebra computations on matrices, including basic operations like addition and multiplication as well as more complex tasks like eigenvalue decomposition and matrix inversion. - Introduces and compares different encoding schemes to represent real numbers as input sequences for transformers. Finds that a compact 15-bit floating point encoding works well for larger matrices while a base 1000 positional encoding performs better on smaller problems.- Shows that properly trained transformers can generalize well beyond their training distribution on linear algebra tasks. Selecting the right training data distribution is key - using "non-Wigner" random matrices with non-i.i.d. coefficients for training improves out-of-distribution generalization.- Provides an analysis of model failures and limitations. Shows transformers tend to produce approximations or solutions to related problems even when predictions are incorrect. Limitations arise from poor conditioning or large matrix sizes.- Benchmarked small transformer models, finding they can accurately solve problems on matrices up to 15x15 dimensions. Larger models can handle bigger matrix sizes. Overall demonstrates transformers are capable of complex numerical computations, not just symbolic manipulation.In summary, the key contribution is showing transformers can be effective on mathematical computations, if properly trained and configured. The analysis of training distributions and generalization provides insights into applying transformers more broadly in scientific domains.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper demonstrates that transformers can be trained to perform numerical computations like matrix operations and eigenvalue decomposition, achieving high accuracy on randomly generated matrices, and showing some ability to generalize beyond their training distribution if it is chosen carefully.


## How does this paper compare to other research in the same field?

This paper makes several notable contributions to research on using transformers for numerical computation:- It demonstrates that transformers can learn to perform a wide range of complex linear algebra operations, including matrix inversion and eigenvalue decomposition, not just basic arithmetic. Prior work has shown transformers struggling with even simple arithmetic like addition and multiplication. - The author systematically compares different encoding schemes for representing real numbers as tokens, finding tradeoffs between compactness and incorporating useful structure. This encoding is an important design choice lacking investigation in prior work.- Through training on different classes of random matrices, the paper provides new insights into how transformers can generalize outside their training distribution in numerical tasks. The connection to random matrix theory is novel.- The analysis of model failures by evaluating constraints like orthogonality of predictions provides techniques for interpretability and verification of results. This helps address common criticisms of neural networks being black boxes.- The comparison to optimized numerical libraries puts the efficiency of transformers on these tasks in context. The author is clear these models are not meant to replace optimized algorithms.Overall, this work significantly expands the known capabilities of transformers for numerical computation through extensive experiments and careful analysis. It also raises new research questions around scaling to larger matrices and smarter training distribution design. The connections to random matrix theory and model verification are innovative ways this work builds on past literature.
