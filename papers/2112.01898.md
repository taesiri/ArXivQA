# [Linear algebra with transformers](https://arxiv.org/abs/2112.01898)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can transformers learn to perform complex numerical computations, like operations of linear algebra, from examples only?The paper investigates whether transformers can be trained, using only input-output examples, to accurately compute solutions to various linear algebra problems like matrix addition, multiplication, inversion, and eigenvalue/eigenvector decomposition. The central hypothesis appears to be that transformers have the capability to learn these numerical computations to a high degree of accuracy, despite concerns that they may struggle with arithmetic and generalization. The experiments aim to demonstrate that transformers can achieve over 90% accuracy on these linear algebra tasks when trained on random matrix data. The paper also examines different encoding schemes for representing real numbers as tokens, and analyzes the models' robustness to noise and ability to generalize beyond their training distribution. Overall, the central research question seems to be whether transformers, as powerful sequence models, can be effectively applied to complex numerical computations in mathematics and science.
