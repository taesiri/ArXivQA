# [Towards Trustable Skin Cancer Diagnosis via Rewriting Model's Decision](https://arxiv.org/abs/2303.00885)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the trustworthiness and robustness of skin cancer diagnosis models by discovering and removing confounding factors that the models may rely on, through a human-in-the-loop approach?

The key points are:

- Deep learning models for skin cancer diagnosis can rely on confounding factors like image artifacts or bias rather than clinically relevant features, making their decisions untrustworthy. 

- The authors propose a framework to make models more explainable via concept mapping, allow human users to identify confounding behaviors, and enable interaction to correct the model's logic.

- They introduce a new dataset called ConfDerm to systematically evaluate model trustworthiness under controlled confounding factors.

- Experiments show their method can effectively detect and mitigate confounding factors without prior knowledge about the data, improving model performance on clinically-relevant concepts.

So in summary, the main hypothesis is that involving humans in the loop to identify and correct confounding model behaviors can improve the trustworthiness and robustness of skin cancer diagnosis systems. The ConfDerm dataset and experiments are designed to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, here are the main contributions:

1. The paper proposes a human-in-the-loop framework to improve the trustworthiness of skin cancer diagnosis models. The key ideas are to make the model explainable via concept mapping, allow humans to identify confounding behaviors, and enable humans to correct the model's logic through interaction. 

2. The paper introduces a new confounded skin lesion dataset called ConfDerm for systematically evaluating model trustworthiness. This dataset contains real images with controlled confounding factors in the training set, allowing rigorous analysis of model behaviors.

3. Through spectral relevance analysis on public datasets like ISIC, the paper reveals that deep learning models can be confounded by artifacts like dark corners, rulers, and hairs. This provides new insights into model weaknesses in skin lesion analysis.

4. The proposed human-in-the-loop framework is shown empirically to be effective in removing confounding factors and improving performance across different testing distributions. Experiments on ConfDerm and other public datasets demonstrate the ability to debias and enhance model trustworthiness.

5. The method does not require full concept annotations or prior knowledge about the concepts. It can discover and learn confounding concepts automatically and enable human interaction through simple first-order logic rules. This makes the framework more practical.

In summary, the core contribution is a human-in-the-loop approach to improving model trustworthiness in skin cancer diagnosis, with empirical validation on new datasets. The insights into model confounding factors and the ability to interactively debias the models are important for deploying trustworthy AI systems in healthcare.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a human-in-the-loop framework to improve the trustworthiness of skin cancer diagnosis models by discovering and removing confounding factors like image artifacts or bias via concept mapping and logical rule rewriting during the training process.


## How does this paper compare to other research in the same field?

 This paper focuses on improving the trustworthiness and reliability of skin cancer diagnosis models through detecting and removing confounding factors that can bias the models. Here are some key ways it compares to other research in this field:

- Explainability: It uses concept-based explanations to provide human-understandable interpretations of the model's decision process, rather than pixel-level attribution maps like CAMs. This allows better understanding of model behaviors.

- Concept learning: It discovers confounding concepts by analyzing model co-occurrence behaviors on datasets directly, without needing full concept annotations upfront like some prior works. This makes it more flexible.

- Interaction: It allows human-in-the-loop interaction to correct model behaviors via first-order logic rules on the concept space. This enables customized debiasing. 

- Evaluation: It systematically evaluates on a novel ConfDerm dataset designed specifically to analyze confounding factors. Most prior work lacks such rigorous evaluation.

- Applications: It focuses on the critical task of skin cancer diagnosis. Most related work has examined confounding factors in more general image classification domains.

Overall, a key distinction is the human-centered approach to promote trustworthiness in medical AI systems. The concept-based explanations, flexible discovery of confounds, and human-in-the-loop interaction for debiasing make this work uniquely well-suited to improving reliability in sensitive applications like cancer diagnosis versus more generic debiasing techniques. The new ConfDerm benchmark is also an important contribution for rigorously evaluating model biases in this space.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing more robust algorithms and evaluation metrics for spectral relevance analysis to better handle noise and discover more fine-grained concepts. The current algorithm still relies heavily on the quality of heatmaps and clustering.

- Expanding the concept bank with more clinical concepts defined by medical experts to cover more fine-grained clinical criteria. This could improve the explainability and faithfulness of the model. 

- Conducting more systematic evaluation on larger datasets to validate the effectiveness of the proposed method. The authors crafted a new dataset ConfDerm, but more evaluation on other public datasets would be useful.

- Exploring different ways to model the relationship between concepts for the explainable logic layer, such as logical formulas or decision trees. The current method uses a simple weighted sum.

- Investigating interactive learning with dynamic concept discovery and more advanced human-AI collaboration. The current work uses predefined concepts and simple global logic rules. Allowing users to introduce new concepts could be an interesting direction.

- Applying the approach to other interpretability tasks beyond classification such as segmentation or detection. Concept-based explanations may also help build trust in other medical AI applications.

- Validating the method for real clinical deployment, showing efficacy in actual usage by doctors and improvements in patient outcomes. Testing on live clinical systems would be important future work.

In summary, the main future directions focus on improving the robustness of concept discovery, expanding the concept bank, conducting more systematic evaluation, exploring different explainable logic modeling, advancing human-AI interaction, and clinical validation. Overall, it's a promising approach to improve trust in medical AI systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a human-in-the-loop framework to improve the trustworthiness of skin cancer diagnosis models. The authors find that modern deep learning models for skin cancer diagnosis can be confounded by irrelevant artifacts in the training data like rulers, dark corners, and hairs. To address this, they introduce a method to discover these confounding factors by analyzing the co-occurrence patterns in the training data using spectral clustering on GradCAM visualizations. They then build a concept bank with clinical concepts and the discovered confounding concepts, and project the model's features onto this concept space to make the model interpretable. The key idea is that they add an interactable logic layer on top that allows human users to provide feedback to the model by specifying rules about which concepts it should or should not focus on. For example, users can specify that the model should not focus on "rulers" to remove this confounding factor. They create a new dataset called ConfDerm to systematically evaluate model robustness to different confounding factors, and show improved performance and trustworthiness compared to baseline models without the human-in-the-loop interaction. The main strengths are the method's ability to discover and remove confounding factors without needing any prior concept annotations, and the interaction mechanism that allows users to rewrite the model's logic.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper explores the problem of confounding factors and bias in deep learning models for skin cancer diagnosis. The authors note that deep learning models can rely on artifacts like rulers, dark corners, or skin tone instead of meaningful clinical features when making predictions. This can make the models untrustworthy when deployed in real clinical settings. 

To address this issue, the authors propose a human-in-the-loop framework to make models more explainable and enable users to correct confounding behaviors. Their method can automatically discover confounding factors in datasets by analyzing co-occurrence patterns. It learns to map the modelâ€™s representations onto an explainable concept space defined by the user, enabling human interaction via logical rules to remove artifacts. The authors introduce a new skin lesion dataset called ConfDerm to systematically evaluate model trustworthiness under different confounds. Experiments on ConfDerm and other public datasets show their method can effectively detect and remove confounding factors, improving model performance and focusing predictions on clinical concepts. This enhances model transparency and trustworthiness.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a human-in-the-loop framework to improve the trustworthiness of skin cancer diagnosis models. First, they use an improved spectral relevance analysis algorithm to discover confounding factors like dark corners or rulers that can bias the model. Next, they construct a concept bank with clinical concepts from expert datasets and confounding concepts from the discovered clusters. Then, they project the feature representation of a model onto this concept space to get concept scores. By replacing the classifier with an explainable logic layer and mapping features to concepts, the model becomes interpretable. Finally, they allow human interaction during training by penalizing the input gradient of irrelevant concepts identified by the user. This enables correcting the model's logic to ignore confounding factors and focus on clinically relevant concepts, improving performance and trustworthiness. The key aspects are discovering confounding concepts without supervision, mapping features to an explainable concept space, and enabling human interaction to rewrite the model's decision logic.


## What problem or question is the paper addressing?

 Based on my reading, the key points about the problem and questions addressed in this paper are:

- Deep neural networks have shown promising performance on image recognition tasks, but they may rely on confounding factors or spurious correlations in the training data instead of meaningful clinical factors. This makes them untrustworthy for deployment in real-world medical scenarios like skin cancer diagnosis.

- The main problem is that deep learning models for skin cancer diagnosis can be confounded by irrelevant artifacts in the images (e.g. rulers, dark corners, hairs) or biases (e.g. image background, skin tone). When models rely on these factors rather than clinically relevant criteria, their predictions become untrustworthy, especially if the test data differs from the training data.

- The main questions are: How to make model decisions transparent and explainable to humans? And how to enable humans to correct the model's confounding behaviors when they are observed?

- The key goals are to: (1) Explain model decisions in human understandable concepts rather than pixel-level attributions. (2) Learn confounding concepts without full supervision or predefined labels. (3) Allow human users to intervene and correct the model via first-order logic instructions. 

In summary, the main problem addressed is improving model transparency and enabling human-guided correction of confounding factors, in order to make skin cancer diagnosis by deep learning models more trustworthy and reliable for real clinical deployment. The key questions revolve around explaining the model's logic and enabling human users to rewrite the model's decision process.
