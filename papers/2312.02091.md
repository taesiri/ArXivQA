# [Physics simulation capabilities of LLMs](https://arxiv.org/abs/2312.02091)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper presents an evaluation of the capabilities of state-of-the-art large language models (LLMs) like GPT-4 on challenging, graduate to research-level computational physics problems. Specifically, the authors design about 50 original problems spread across 4 simulation codebases - stellar physics, celestial mechanics, fluid dynamics, and non-linear dynamics - that would plausibly appear on a PhD qualifier exam. They find that while today's most capable LLM in GPT-4 is not able to satisfactorily solve these research problems, it is able to generate code containing 70-90% correct lines with a combination of physics and coding errors and placeholder code. About 40% of the solutions could plausibly pass at some minimal level. The most prominent failure modes identified include poor physical units handling, inconsistent code versioning, hallucinating plausible functions, inadequate justification of simulation parameters, and unreliable definition of stopping criteria. This analysis provides a snapshot of progress and limitations of LLMs on autonomous scientific simulation capabilities using classical physics problems, while also outlining promising directions for future improvements.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper explores the capabilities of large language models (LLMs) like GPT-4 in performing computational physics tasks, specifically in the domain of numerically simulating complex physics scenarios. This requires expertise in both coding and physics.

- The goal is to evaluate if LLMs can reliably generate code to simulate research-level physics problems, at the level of a graduate student or scientist. This tests their ability to autonomously perform core aspects of academic research in the physical sciences.

Methods
- The authors design ~50 original problems spanning four physics subdomains: stellar physics, celestial mechanics, fluid dynamics, and nonlinear dynamics. Well-established, open-source packages are used for each: MESA, REBOUND, Dedalus and SciPy.

- The problems are meant to be graduate-exam level and require generalization beyond textbook examples. Some "out-of-distribution" hypothetical problems are also included to test the boundaries of physical reasoning.

- Since the problems do not have definite solutions, several evaluation metrics are used: counts of different error types per code line (physics, coding, etc) and a pass/fail grade based on whether core aspects of the problem are addressed.

Results
- GPT-4 fails to solve most problems satisfactorily. About 40% pass upon lenient grading. 70-90% of code lines are valid, with physics and coding errors interspersed. Performance varies across problem difficulty and domain.

- Several failure modes are identified such as struggling with units, code versioning, hallucinating plausible functions, lacking justification for simulation parameters, and inability to reliably define stopping conditions. 

Conclusion
- The results showcase current limitations in autonomy for computational physics tasks but also define clear targets for improvement in future LLMs. With steady progress, more advanced scientific reasoning capabilities seem within reach.
