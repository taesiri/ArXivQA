# [Towards Open-ended Visual Quality Comparison](https://arxiv.org/abs/2402.16641)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing image quality assessment (IQA) methods suffer from ambiguity when evaluating the quality of a single image, as different people may have different standards. 
- Comparative settings (e.g. pairwise comparison) can provide more clear-cut judgements and have been widely adopted in subjective studies. 
- However, existing datasets and models are limited to simple overall quality comparison between image pairs. They lack the capability of responding to open-ended questions that compare quality attributes among multiple (more than 2) images and provide detailed reasoning.

Proposed Solution:
- Construct Co-Instruct-562K, the first instruction tuning dataset for open-ended, multi-image quality comparison. It contains human descriptions merged by language models, and responses from GPT-4V.
- Propose Co-Instruct, a model fine-tuned on this dataset. It adopts visual token reduction and an image-text interleaved format to handle multiple images.
- Construct MICBench, a new benchmark with 2,000 multi-choice questions comparing quality attributes among groups of 3 or 4 images.

Main Contributions:
- Co-Instruct-562K dataset to teach models for multi-image quality comparison.
- Co-Instruct model that outperforms all existing models, including GPT-4V, on quality comparison tasks.
- MICBench benchmark for evaluating multi-image comparison.

Overall, the paper moves visual quality assessment towards open-ended, multi-image comparison settings, with a new dataset, model and benchmark. It demonstrates superior capability of the Co-Instruct model over state-of-the-art methods on this practically useful but challenging problem.
