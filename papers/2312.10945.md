# [LaViP:Language-Grounded Visual Prompts](https://arxiv.org/abs/2312.10945)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Adapting large-scale pre-trained vision-language models (VLMs) like CLIP to downstream tasks is challenging due to a large number of tunable parameters. Finetuning risks catastrophic forgetting while prior visual prompting methods learn input-agnostic prompts lacking multimodal knowledge. The paper aims to address these limitations.

Proposed Solution:
The paper proposes Language-Grounded Visual Prompting (LaViP), a novel visual prompting method that leverages language supervision to generate input-aware visual prompts. Specifically, LaViP represents the visual prompt as a low-rank decomposition of two matrices derived from textual and visual encodings. This allows prompts to be tailored to each input image while reducing parameters. Further, a mechanism is introduced to embed novel class knowledge into prompts via Kronecker products, enabling generalization.

Main Contributions:
- Pioneers language-grounded visual prompts for adapting vision-language models without finetuning or accessing model parameters.
- Proposes a parameter-efficient strategy to generate input-aware prompts grounded in both modalities.
- Develops a method to expand visual prompts to novel classes without retraining.
- Demonstrates state-of-the-art performance on downstream tasks like few-shot learning and base-to-novel generalization through extensive experiments.
- Shows language integration consistently improves optimization speed and alignment of vision-language representations.
- Highlights the suitability of the approach for black-box model adaptation scenarios.

In summary, the paper puts forth LaViP, a novel multimodal visual prompting technique to efficiently adapt VLMs by effectively harnessing language semantics. Both the accuracy and flexibility of the algorithm are empirically verified across diverse settings.
