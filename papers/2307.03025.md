# [Style Over Substance: Evaluation Biases for Large Language Models](https://arxiv.org/abs/2307.03025)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How capable and reliable are human judges and large language models (LLMs) in evaluating machine-generated text, and what biases or limitations exist in their evaluation approaches? The key hypotheses examined in this study are:1) Both human judges and LLMs demonstrate biases and inconsistencies when evaluating machine-generated text across dimensions like accuracy, helpfulness, and language quality.2) Collapsing multiple evaluation dimensions into a single score results in an incomplete and potentially misleading assessment of machine-generated text. 3) Independently evaluating machine-generated text across multiple dimensions leads to more comprehensive and transparent insights into text quality.To test these hypotheses, the authors systematically generate machine text with controlled variations in accuracy, language errors, and length. They then have these texts evaluated by crowd workers, experts, and LLMs based on different scoring schemes. Their analysis of the results reveals intriguing differences in how humans and LLMs approach evaluation, including biases favoring longer text and limitations in detecting factual errors. Their proposed multi-dimensional rating system appears to improve certain aspects of evaluation, particularly factual accuracy. Overall, this study provides an insightful examination into the evaluation of machine-generated text.
