# [Style Over Substance: Evaluation Biases for Large Language Models](https://arxiv.org/abs/2307.03025)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How capable and reliable are human judges and large language models (LLMs) in evaluating machine-generated text, and what biases or limitations exist in their evaluation approaches? The key hypotheses examined in this study are:1) Both human judges and LLMs demonstrate biases and inconsistencies when evaluating machine-generated text across dimensions like accuracy, helpfulness, and language quality.2) Collapsing multiple evaluation dimensions into a single score results in an incomplete and potentially misleading assessment of machine-generated text. 3) Independently evaluating machine-generated text across multiple dimensions leads to more comprehensive and transparent insights into text quality.To test these hypotheses, the authors systematically generate machine text with controlled variations in accuracy, language errors, and length. They then have these texts evaluated by crowd workers, experts, and LLMs based on different scoring schemes. Their analysis of the results reveals intriguing differences in how humans and LLMs approach evaluation, including biases favoring longer text and limitations in detecting factual errors. Their proposed multi-dimensional rating system appears to improve certain aspects of evaluation, particularly factual accuracy. Overall, this study provides an insightful examination into the evaluation of machine-generated text.


## What is the main contribution of this paper?

This paper investigates the behavior of human and LLM judges in evaluating machine-generated text, specifically focusing on biases and limitations in assessing answer quality. The key contributions are:1. The study reveals several issues in current evaluation methods that rely on a single unified score, including the inability to thoroughly evaluate different factors like accuracy, helpfulness, and language quality individually. 2. Through a meticulously created set of machine-generated answers with controlled language proficiency, factual accuracy, and length, the analysis uncovers fascinating insights into human and LLM judges:- Humans exhibit hesitancy and inconsistencies compared to LLMs.- Humans overlook factual inaccuracies easily compared to LLMs. - Both prefer lengthy responses over succinct ones.- LLMs exhibit bias toward the first response.3. To address these limitations, the paper proposes the Multi-Elo Rating System to evaluate machine text across dimensions of Accuracy, Helpfulness, and Language independently. 4. Empirical results demonstrate this multi-dimensional approach significantly improves evaluation quality, especially factual accuracy evaluation by LLMs.In summary, the key contribution is highlighting the biases and flaws of current unified evaluation methods for machine text through a controlled study, and proposing an improved multi-dimensional evaluation framework to enable more comprehensive and transparent understanding. The paper provides valuable insights into human and machine evaluation behaviors.
