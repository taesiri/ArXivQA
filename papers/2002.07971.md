# [Gradient Boosting Neural Networks: GrowNet](https://arxiv.org/abs/2002.07971)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How can we combine the power of gradient boosting with the flexibility and versatility of neural networks? More specifically, the authors propose a novel modeling paradigm called GrowNet that builds neural networks layer-by-layer using the idea of gradient boosting. Instead of decision trees which are commonly used in gradient boosting methods like XGBoost, the authors use shallow neural networks as the weak learners that are added sequentially. The key ideas and hypotheses tested in this paper are:- Gradient boosting can be adapted to incrementally build deep neural networks by using shallow NNs as weak learners instead of decision trees. - Stacking features from the penultimate layer of each shallow NN as additional input for training subsequent learners improves information propagation.- A corrective step to update parameters of all previous learners prevents getting stuck in local optima. - Second-order optimization and dynamically updating the boosting rate further improves performance.- This approach can be generalized to various ML tasks like classification, regression and ranking.So in summary, the central hypothesis is around combining boosting with NNs to grow complex models, and the paper aims to demonstrate the viability of this approach across multiple tasks. The novelty lies in using boosting to construct DNNs layer-by-layer as opposed to standard end-to-end training.
