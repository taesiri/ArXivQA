# [Elephants Never Forget: Memorization and Learning of Tabular Data in   Large Language Models](https://arxiv.org/abs/2404.06209)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 and GPT-4 are being increasingly used for tasks involving tabular data. However, issues around data contamination and memorization are often overlooked when evaluating their performance. 
- Specifically, it's difficult to know if an LLM has seen parts of an evaluation dataset during pre-training. This could lead to inflated performance estimates due to memorization and overfitting.

Methods:
- The authors develop several tests to check if GPT-3.5 and GPT-4 have memorized parts of popular tabular datasets verbatim. This includes prompting the models to generate specific rows or values from the datasets.
- They then evaluate the few-shot learning performance of the models on memorized datasets vs novel datasets released after the model training cutoff. The datasets are presented in original and perturbed formats to test robustness.
- Additionally, they investigate the in-context statistical learning capabilities of the models using binary classification tasks with simulated data.

Key Findings:
- GPT-3.5 and GPT-4 had memorized parts of many popular datasets like Iris and Wine verbatim. The larger GPT-4 exhibited more memorization.
- Models performed better on memorized datasets, indicating overfitting due to contamination. Performance dropped on perturbed data.
- However, models showed decent generalization on novel datasets, relying on world knowledge instead of memorization.
- In-context statistical learning was limited, especially as problem complexity increased. Fine-tuning performed better.

Main Contributions:  
- Providing tests to detect if an LLM has seen specific tabular training data 
- Demonstrating overfitting effects due to memorization of tabular data
- Showing the limitations of statistical learning abilities for current LLMs
- Highlighting the need to control for data contamination when evaluating LLMs

The paper makes available the test suite as a Python package to check for table memorization in LLMs. Overall, it clearly reveals the significant impact data contamination can have on few-shot evaluations.
