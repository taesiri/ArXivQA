# Building and Interpreting Deep Similarity Models

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to explain similarity models in a human-interpretable manner. Specifically, the paper proposes a method called "BiLRP" to generate explanations for similarity models in terms of pairs of input features that contributed to the similarity score. The key ideas and contributions are:- Similarity models can be naturally decomposed and explained in terms of joint contributions of pairs of input features.- BiLRP provides a theoretically well-founded way to perform this decomposition, by propagating relevance scores backward through the layers of the model. - BiLRP can be expressed as a composition of standard LRP computations, inheriting robustness and broad applicability.- BiLRP is applied to explain similarity models based on VGG-16 features and a specialized model for matching historical tables. The explanations provide useful insights and model validation capabilities.In summary, the central hypothesis is that decomposing and explaining similarity models in terms of joint contributions of input features can provide greater model transparency and interpretability. The BiLRP method is proposed to address this problem and is evaluated on deep neural networks and a specialized application.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called BiLRP for explaining similarity predictions made by deep neural networks and other machine learning models. Specifically, the paper introduces an approach to decompose and attribute a model's similarity score to pairs of input features from the two examples being compared. This allows identifying which specific input patterns jointly contribute to or contradict the predicted similarity.The BiLRP method is developed based on the layer-wise relevance propagation (LRP) framework for explaining neural network classifiers. It inherits LRP's ability to handle complex nonlinear models in a robust manner.Through experiments, the paper demonstrates how BiLRP can provide useful insights into similarity models and reveal their strengths and limitations, for example when assessing transferability to new tasks or invariance properties.The method is also applied to a real-world problem in digital humanities of modeling similarities between historical documents. Here again, BiLRP helps validate and debug the similarity model based on limited data.Overall, the key contribution is developing an explanation technique tailored to similarity predictions, that reveals relevance based on pairs of input features. This brings interpretability to an important ingredient of many machine learning systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method called BiLRP to explain similarity scores predicted by deep neural networks in terms of relevant pairs of input features; BiLRP is derived using deep Taylor decomposition and can be implemented efficiently as a composition of standard layer-wise relevance propagation computations.
