# [Building and Interpreting Deep Similarity Models](https://arxiv.org/abs/2003.05431)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to explain similarity models in a human-interpretable manner. Specifically, the paper proposes a method called "BiLRP" to generate explanations for similarity models in terms of pairs of input features that contributed to the similarity score. The key ideas and contributions are:- Similarity models can be naturally decomposed and explained in terms of joint contributions of pairs of input features.- BiLRP provides a theoretically well-founded way to perform this decomposition, by propagating relevance scores backward through the layers of the model. - BiLRP can be expressed as a composition of standard LRP computations, inheriting robustness and broad applicability.- BiLRP is applied to explain similarity models based on VGG-16 features and a specialized model for matching historical tables. The explanations provide useful insights and model validation capabilities.In summary, the central hypothesis is that decomposing and explaining similarity models in terms of joint contributions of input features can provide greater model transparency and interpretability. The BiLRP method is proposed to address this problem and is evaluated on deep neural networks and a specialized application.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called BiLRP for explaining similarity predictions made by deep neural networks and other machine learning models. Specifically, the paper introduces an approach to decompose and attribute a model's similarity score to pairs of input features from the two examples being compared. This allows identifying which specific input patterns jointly contribute to or contradict the predicted similarity.The BiLRP method is developed based on the layer-wise relevance propagation (LRP) framework for explaining neural network classifiers. It inherits LRP's ability to handle complex nonlinear models in a robust manner.Through experiments, the paper demonstrates how BiLRP can provide useful insights into similarity models and reveal their strengths and limitations, for example when assessing transferability to new tasks or invariance properties.The method is also applied to a real-world problem in digital humanities of modeling similarities between historical documents. Here again, BiLRP helps validate and debug the similarity model based on limited data.Overall, the key contribution is developing an explanation technique tailored to similarity predictions, that reveals relevance based on pairs of input features. This brings interpretability to an important ingredient of many machine learning systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method called BiLRP to explain similarity scores predicted by deep neural networks in terms of relevant pairs of input features; BiLRP is derived using deep Taylor decomposition and can be implemented efficiently as a composition of standard layer-wise relevance propagation computations.


## How does this paper compare to other research in the same field?

This paper introduces a new method called BiLRP for explaining similarity predictions from deep neural networks. Here are a few key ways it compares to prior work on explaining similarity and deep models:- Most prior work on explainable AI has focused on explaining classifications rather than similarities. This paper extends explanation techniques like layer-wise relevance propagation (LRP) to the novel task of explaining pairwise similarities. - Compared to methods that explain classifiers, BiLRP produces explanations specifically in terms of pairs of input features that jointly contribute to the similarity score. This provides new insights into how the model perceives similarity between two examples.- The proposed method can handle complex neural network architectures like VGG-16. The modular formulation makes it efficient to apply on deep models compared to approaches that require computing second-order derivatives.- They demonstrate the approach on real-world applications like analyzing similarities of facial images and historical documents. This showcases the usefulness of the technique for practical tasks where interpretability is important.- The work provides both theoretical analysis of BiLRP in terms of conservation and connection to other methods, as well as empirical validation on models where ground truth explanations are available. This helps establish BiLRP as a principled approach.Overall, this paper makes both theoretical and practical contributions for opening up the "black box" of similarity predictions. Explainable similarity has applications across many domains where understanding model behavior is key. The experiments show this approach can provide meaningful insights into complex deep similarity models.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the paper:- Extend the proposed techniques from binary to n-ary similarity structures to incorporate different levels of reliability of the input features. - Use the proposed research tool to gain insight into large data collections, specifically to ground historical networks to interpretable domain-specific concepts.- Design better explanation-based invariance measures, potentially in combination with optical flows, to better distinguish between matching structures that should and should not contribute to the invariance score.- Apply the proposed methods to gain insight into practical similarity models in other application domains beyond the ones considered in the paper.Overall, the authors suggest extending the capabilities of the proposed explanation techniques, applying them to large real-world datasets to gain new insights, and designing new explanation-based validation measures for similarity models. The techniques could be applied to a variety of domains beyond the examples presented in the paper.


## Summarize the paper in one paragraph.

The paper introduces BiLRP, a method to explain similarity models by attributing the similarity score to pairs of input features. BiLRP is based on composing multiple layer-wise relevance propagation (LRP) computations. It propagates relevance backward through the layers of the similarity model, redistributing it from pairs of neurons to pairs of neurons in the layer below. This results in a second-order deep Taylor decomposition of the similarity function. BiLRP is applied to interpret different types of models including similarities based on VGG-16 features and a specialized similarity model for comparing historical tables. The method provides insights into the strengths and limitations of the similarity models. For example, it reveals when the model focuses on unexpected patterns to determine similarity between images. BiLRP also allows to efficiently validate and compare similarity models even when limited ground truth data is available.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new method called BiLRP for explaining similarity models such as dot products built on deep neural networks. The key idea is to decompose the similarity score in terms of pairs of input features that jointly contribute to it. This is done by propagating relevance backwards through the network's layers using second-order relevance propagation rules derived from a deep Taylor decomposition framework. The method can be implemented efficiently as a composition of standard LRP computations on each branch of the similarity model. Experiments demonstrate that BiLRP produces insightful explanations that reveal the matching structures captured by similarity models. It is applied to inspect deep similarity models built using VGG-16 features and a specialized model for assessing similarity of historical textbooks. The method brings interpretability to these complex nonlinear similarities and helps validate them.Overall, the paper introduces a principled and scalable approach to explain similarity predictions. This helps inspect what makes the model consider two inputs as similar. The method works for complex neural network models unlike previous approaches. It also enables more efficient debugging and validation of similarity models using the human-interpretable explanations produced. This can be useful in applications like information retrieval, kernel methods, and analysis of historical data. The modular composition of LRP computations makes the method easy to implement.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a method called BiLRP to explain similarity models that compute dot products between deep feature representations of two inputs. The key idea is to decompose the dot product similarity score into contributions from pairs of input features. This is done by propagating relevance signals through the layers of the neural network using second-order variants of layer-wise relevance propagation (LRP) rules. Specifically, the relevance at each layer is modeled as a product between activations of neuron pairs. By taking the second-order Taylor expansion of this relevance model, propagation rules can be derived that redistribute relevance to pairs of neurons in the layer below. The resulting BiLRP explanation scores can be expressed as compositions of standard LRP computations applied independently to each input.In summary, BiLRP extends the robustness and broad applicability of LRP to explaining nonlinear similarity models built on deep neural networks. It produces explanations in terms of relevant pairs of input features that jointly contribute to the predicted similarity score.
