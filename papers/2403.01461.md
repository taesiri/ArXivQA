# [Answerability in Retrieval-Augmented Open-Domain Question Answering](https://arxiv.org/abs/2403.01461)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Open-domain question answering (ODQA) systems can retrieve text excerpts that vary in relevance when answering a question. Many ODQA datasets lack examples for identifying irrelevant excerpts. 
- Previous approaches rely on simplistically pairing questions with random text, but models trained this way may not generalize to irrelevant excerpts with high semantic overlap.

Proposed Solution:
- Investigate if models trained on random text can recognize semantically related but practically irrelevant excerpts and abstain from answering.
- Explore if models are more likely to hallucinate or incorrectly extract an answer due to confirmation bias when faced with such excerpts. 
- Use ChatGPT to efficiently generate synthetic excerpts that are related or unrelated to questions.
- Train models on unanswerable questions from SQuAD 2.0 to recognize such excerpts.

Main Contributions:
- Show that models trained on random text perform poorly (98% to 1% accuracy) on semantically related but irrelevant excerpts.
- Find that models exhibit confirmation bias and tend to extract factual answers more than other entities when no answer is possible.
- Demonstrate that fine-tuning on SQuAD 2.0 unanswerable questions enables models to achieve near perfect accuracy on recognizing various types of irrelevant excerpts.
- Highlight the importance of abstaining from answering for trustworthy QA systems.

In summary, the paper studies model behavior on irrelevant texts in ODQA using efficient data generation, and shows SQuAD 2.0 fine-tuning is an effective approach to improve performance. Recognizing unanswerable questions is critical for robust ODQA systems.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper investigates models' ability to recognize irrelevant text extracts in open-domain question answering and finds that models trained on randomized irrelevant extracts fail to generalize, while models trained on unanswerable questions from SQuAD 2.0 perform much better.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1) Identifying a key limitation with existing strategies for training open-domain question answering (ODQA) models to recognize when a retrieved text excerpt does not contain an answer. Specifically, prior work has relied on simply pairing questions with random irrelevant text excerpts. This paper shows that models trained this way fail to generalize to more realistic cases where the excerpt is semantically related to the question but still unanswerable.

2) Proposing a more effective training strategy that leverages unanswerable questions from the SQuAD 2.0 dataset. Models trained this way are able to recognize semantically related but unanswerable excerpts with near perfect accuracy.

3) Analyzing model behavior when confronted with unanswerable yet related excerpts. The analysis reveals issues like a confirmation bias, where models tend to incorrectly extract factual answers mentioned out of context. The authors also quantify rates of answer hallucination vs extraction.

In summary, the main contribution is identifying limitations of existing strategies for handling unanswerable questions in ODQA, and proposing an improved training approach that results in models better able to recognize when no answer is contained in a retrieved text excerpt.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Open-Domain Question Answering (ODQA)
- Answerability
- Retrieval augmentation 
- Unanswerable questions
- Natural Questions (NQ) dataset
- SQuAD 2.0 dataset
- Confirmation bias
- Hallucination vs extraction
- Abstaining from answering

The paper examines issues with answerability in open-domain QA systems that rely on retrieved text passages, including the system's inability to recognize irrelevant passages and abstain from answering. It analyzes different training strategies for handling unanswerable questions and evaluates model performance on semantically related but unanswerable passages generated by ChatGPT. Key findings relate to limitations in existing strategies and evidence of confirmation bias and hallucination behaviors when models lack the proper training. The concepts of answerability, unanswerable questions, abstaining from answering, and mitigating biases seem most central to the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper relies on using the unanswerable questions from SQuAD 2.0 dataset to train models to recognize unanswerable questions. Why is this dataset particularly suitable for this task compared to other existing QA datasets? What are some potential limitations of solely relying on this dataset?

2. When using the random text excerpt augmentation method, the authors observe a significant drop in accuracy when evaluating on semantically related texts compared to random texts. What factors could explain this discrepancy in performance? How can this issue be addressed?

3. The paper hypothesizes that models exhibit a confirmation bias when answering questions, preferring to extract familiar answers. What analyses could be done to further validate this hypothesis? Are there other types of biases that should also be investigated? 

4. ChatGPT is used to generate synthetic text excerpts for evaluating model performance. What are some potential issues with using a language model for this purpose? How does this impact the generalizability of the results?

5. The authors mainly experiment with T5 model architectures. How would the results differ if other model architectures like BERT or GPT were used instead? What architectural properties affect the answerability behavior?

6. Beyond extracting the gold answer or hallucinating an incorrect answer, what other types of bad behaviors could models exhibit when faced with unanswerable questions? How can these failure modes be identified and addressed?

7. The methodology relies on generating several variants of unanswerable questions. What other challenging variants could be created to evaluate model robustness? Are there any gaps or limitations in the types of questions examined?

8. How do the answerability results translate into real-world open domain QA scenario? What additional experiments could be done to measure practical impact? Are there issues applying this method at scale?

9. The paper examines answerability in a reading comprehension setup. How different would the conclusions be for conversational QA systems? What changes would be needed to adapt this methodology?

10. How does fine-tuning vs counterfactual training impact model answerability? What are the tradeoffs between the two methods and can they be combined?
