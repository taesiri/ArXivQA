# [vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations](https://arxiv.org/abs/1910.05453)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that learning discrete representations of speech through self-supervised context prediction can enable direct application of powerful NLP algorithms like BERT to speech data and improve performance on speech tasks like phoneme classification and speech recognition. Specifically, the paper proposes vq-wav2vec, an algorithm that learns discrete representations of fixed length audio segments by predicting future time steps, similar to word2vec in NLP. This is done by introducing a vector quantization module into the wav2vec model to convert the continuous representations into discrete codes. The main experiments then show that pre-training BERT on the discretized speech representations from vq-wav2vec sets a new state of the art on TIMIT phoneme classification and Wall Street Journal speech recognition. This demonstrates that the discrete representations learned by vq-wav2vec enable transferring knowledge from large unlabeled speech datasets to downstream tasks through powerful NLP models like BERT.In summary, the central hypothesis is that learning discrete speech representations in a self-supervised way enables better transfer learning through the direct application of discrete NLP models like BERT, thus improving performance on speech tasks. The experiments provide evidence to support this hypothesis.
