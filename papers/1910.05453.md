# [vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations](https://arxiv.org/abs/1910.05453)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that learning discrete representations of speech through self-supervised context prediction can enable direct application of powerful NLP algorithms like BERT to speech data and improve performance on speech tasks like phoneme classification and speech recognition. Specifically, the paper proposes vq-wav2vec, an algorithm that learns discrete representations of fixed length audio segments by predicting future time steps, similar to word2vec in NLP. This is done by introducing a vector quantization module into the wav2vec model to convert the continuous representations into discrete codes. The main experiments then show that pre-training BERT on the discretized speech representations from vq-wav2vec sets a new state of the art on TIMIT phoneme classification and Wall Street Journal speech recognition. This demonstrates that the discrete representations learned by vq-wav2vec enable transferring knowledge from large unlabeled speech datasets to downstream tasks through powerful NLP models like BERT.In summary, the central hypothesis is that learning discrete speech representations in a self-supervised way enables better transfer learning through the direct application of discrete NLP models like BERT, thus improving performance on speech tasks. The experiments provide evidence to support this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing vq-wav2vec, a self-supervised method to learn discrete speech representations by quantizing the outputs of a wav2vec model trained on a context prediction task. This enables directly applying algorithms from NLP that require discrete inputs to speech data.2. Introducing two quantization methods - Gumbel softmax and online k-means clustering - for vq-wav2vec. The Gumbel softmax enables fully differentiable training while k-means is closer to traditional vector quantization. 3. Showing that pre-training a BERT model on the discrete speech representations from vq-wav2vec and using it as input features for an acoustic model leads to state-of-the-art results on TIMIT phoneme classification and Wall Street Journal speech recognition benchmarks.4. Demonstrating a direct application of standard NLP sequence-to-sequence models to the discrete speech representations for speech recognition, without having to go through a separate acoustic model.5. Analyzing the trade-off between compression rate and accuracy by varying the number of codewords in vq-wav2vec, and showing it outperforms standard audio compression algorithms.In summary, the main contribution is presenting a method to effectively discretize raw speech in a self-supervised way to unlock direct applications of discrete NLP models to speech tasks, leading to improved accuracy. The discretization approach is shown to be more effective than standard compression algorithms.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in learning discrete speech representations:- The approach of using a self-supervised context prediction task like wav2vec to learn representations is similar to previous work such as wav2vec and vq-vae. This paper innovates by quantizing the wav2vec representations to make them discrete.- Using Gumbel-softmax and k-means clustering for quantization has been explored before in other domains like images, but this paper is one of the first to apply it to speech representation learning.- Applying BERT pre-training to the discrete speech representations is novel. Most prior speech representation learning has focused just on the encoder model. Leveraging BERT demonstrates the benefits of also pre-training the aggregator/context model.- State-of-the-art results are achieved on both phoneme recognition (TIMIT) and speech recognition (WSJ) by combining discrete speech representations with BERT. This shows the strength of the overall approach.- Discretization enables directly applying a variety of NLP models like BERT and Transformers to speech. Prior work required modifications of NLP models to handle continuous speech inputs.- The experiments on bitrate vs accuracy for different vector quantized models provide an analysis of the tradeoffs between model compression and performance. This helps understand the limitations of discrete representations.Overall, the paper builds nicely on prior work on speech representation learning, while making innovations in quantization, using BERT, and achieving strong empirical results. The techniques open up many possibilities for transferring NLP methods to speech directly.
