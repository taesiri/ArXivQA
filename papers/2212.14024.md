# [Demonstrate-Search-Predict: Composing retrieval and language models for   knowledge-intensive NLP](https://arxiv.org/abs/2212.14024)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we design composable functions that allow complex interactions between frozen language models (LMs) and retrieval models (RMs) for knowledge-intensive NLP tasks using only natural language instructions and operations on texts? The key hypothesis seems to be that by relying entirely on passing natural language texts and scores between a frozen LM and RM, it is possible to build sophisticated task-specific strategies that outperform existing "retrieve-then-read" pipelines for in-context learning on knowledge-intensive tasks. The paper introduces the Demonstrate-Search-Predict (DSP) framework to address this question. DSP provides composable functions to bootstrap training examples (Demonstrate), gather relevant information (Search), and generate grounded predictions (Predict). By composing these functions into deliberate programs tailored to a task, DSP aims to enable more reliable and effective use of frozen LMs and RMs compared to simple retrieve-then-read pipelines. The results on question answering datasets appear to validate the hypothesis, showing gains over vanilla LMs and standard pipelines.In summary, the central research question is how to unlock sophisticated interactions between LMs and RMs using only natural language, and DSP represents a proposed approach and framework based on the hypothesis that composable functions can enable complex, task-specific strategies that improve on existing methods. The gains demonstrated provide evidence supporting the validity of the hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introducing the Demonstrate-Search-Predict (DSP) framework for retrieval augmented in-context learning. DSP provides composable functions to implement in-context learning systems as programs that systematically decompose complex problems into smaller steps/transformations that the language model (LM) and retrieval model (RM) can handle more reliably. 2. Showing how DSP can express sophisticated task-specific strategies by composing basic techniques like bootstrapping annotations, multi-hop search, query rewriting, passage fusion, etc. This reveals new possibilities for retrieval augmented in-context learning.3. Implementing DSP programs for question answering in open-domain, multi-hop, and conversational settings. Despite low development effort, these programs establish new state-of-the-art in-context learning results, delivering considerable gains over vanilla LMs, retrieve-then-read pipelines, and contemporaneous self-ask pipelines.4. Arguing conceptually that the ability to pass natural language texts between frozen LMs and RMs creates opportunities for sophisticated interactions between them. If realized, this could enable rapid development of grounded AI systems at a high level of abstraction.In summary, the main contribution is introducing the DSP framework to reveal and leverage the potential for composing retrieval and language models via deliberative programs operating on natural language texts. The gains on various QA tasks demonstrate this potential.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work:- This paper introduces the DSP (Demonstrate-Search-Predict) framework for composably building retrieval-augmented language models. Other recent work has explored retrieving passages to augment language model prompts, but DSP provides a more structured way to integrate retrieval with several reusable stages.- A key contribution is the "demonstrate" stage which can automatically annotate training examples with intermediate steps like search queries. This provides a form of weak supervision to train retrieval-based pipelines without hand-labeling each intermediate step. Other related work on multi-hop QA and conversational search typically requires more hand-labeled data.- For search, the paper emphasizes deliberate strategies where the LM and retriever cooperate, compared to standard "retrieve-then-read" pipelines. The composability of DSP makes it convenient to explore techniques like multi-hop reasoning, query rewriting, and result fusion.- For prediction, the paper discusses aggregating information across passages and sampling multiple pipelines. Related work has looked at some of these ideas like marginalizing across passages, but DSP provides a unified framework.- The empirical results are very competitive, achieving new SOTA for GPT-3.5 on SQuAD, HotpotQA, and QReCC with minimal tuning. Other comparable in-context learning results require more expensive LMs or more hand-engineering.- An open question is whether the DSP framework can be generalized to other tasks besides QA. The composable structure seems flexible enough to apply more broadly but this hasn't been demonstrated yet.Overall, the paper makes nice conceptual and practical contributions over existing retrieval-augmented in-context learning techniques. The framework and reusable capabilities like automatic annotation set it apart from prior efforts.
