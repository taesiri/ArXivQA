# [Automated Search for Resource-Efficient Branched Multi-Task Networks](https://arxiv.org/abs/2008.10292)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we automatically design resource-efficient branching (tree-like) structures in the encoder of a neural network to perform well across multiple dense prediction tasks?

The key points are:

- The paper proposes an approach called Branched Multi-Task Architecture Search (BMTAS) to automatically determine encoder branching structures for multi-task learning. 

- BMTAS is based on differentiable neural architecture search, allowing end-to-end training to find optimal branching structures.

- A novel resource-aware loss function is introduced to find compact models that balance performance vs computational efficiency. 

- Experiments show BMTAS consistently finds high-performing branching structures within a given resource budget across tasks like semantic segmentation and surface normal estimation.

So in summary, the central hypothesis is that using differentiable NAS with the proposed resource-aware loss can automate the design of efficient multi-task network architectures, eliminating the need for manual exploration.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an automated approach called Branched Multi-Task Architecture Search (BMTAS) to find efficient branching structures in the encoder of multi-task neural networks. Specifically:

- They formulate the search space to encompass all possible branching structures for a given set of tasks. 

- They use a differentiable neural architecture search algorithm to traverse this search space in an end-to-end trainable manner. 

- They introduce a novel proxyless, resource-aware loss function that allows flexibility in balancing performance vs efficiency during the architecture search.

- Evaluations on multiple dense prediction datasets show their method can consistently find high-performing branching structures within constrained resource budgets. 

In summary, the key contribution is developing a principled neural architecture search approach tailored to automatically determining encoder branching points in multi-task learning based on both performance and efficiency criteria. This automates a process that typically requires extensive manual exploration.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an automated approach called Branched Multi-Task Architecture Search (BMTAS) that uses differentiable neural architecture search to find efficient tree-like branching structures in the encoder stage of a neural network for performing multiple visual perception tasks concurrently.


## How does this paper compare to other research in the same field?

 This paper on automated search for branched multi-task networks makes several key contributions to the field of neural architecture search and multi-task learning:

- It proposes a novel differentiable architecture search method to automatically find branching structures in the encoder of a multi-task network. This allows more flexibility compared to prior works where branching points were manually predetermined.

- The search space directly encompasses all possible branching configurations for a given set of tasks. This is more expansive than typical NAS which only searches over components like cells. 

- A new proxyless, resource-aware loss function is introduced to navigate the trade-off between performance and efficiency. This allows finding compact models suitable for resource-constrained applications.

- Evaluations demonstrate strong performance across various datasets and backbones. The method consistently finds efficient branching structures compared to alternatives like brute force search.

- The approach focuses specifically on dense prediction tasks like segmentation and depth estimation. Most NAS works have targeted image classification instead.

Overall, this paper pushes NAS into new territory by tackling the very relevant problem of automated multi-task architecture search for dense prediction. The novel search algorithm and resource-aware loss enable finding efficient branching structures in an end-to-end manner. This reduces the need for extensive manual architecture engineering in multi-task learning.

The key limitations are the growing search space complexity for many tasks, and the focus only on encoder branching rather than more extensive architecture search. But within its scope, the paper makes significant contributions to automated multi-task architecture design. It opens up new possibilities for NAS to have impact in multi-modal perception problems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring NAS techniques to also search the decoder/prediction heads of multi-task networks, in addition to the encoder branching structures focused on in this work. The authors state this could lead to further performance improvements.

- Extending the approach to search over more complex graph-based search spaces, rather than just tree-like branching structures. This could allow discovering more intricate relationships between tasks.

- Applying the method to additional tasks beyond the dense prediction tasks studied in the paper, such as other computer vision tasks or multimodal tasks combining vision and language. This would demonstrate the general applicability of the approach.

- Developing more efficient search algorithms to handle settings with larger numbers of tasks. The authors note the search complexity increases substantially as the number of tasks grows, limiting the approach's scalability currently. New techniques could improve this.

- Incorporating additional resource constraints beyond just computational cost/MAdds into the search, such as memory footprint or latency. This could produce models tailored for more diverse resource-limited applications.

- Exploring the combination of the automated architecture search approach with other MTL techniques like knowledge distillation. Integrating multiple complementary MTL methods could further boost performance.

In summary, the main future directions focus on expanding the search space, applying the approach to new tasks, improving search efficiency for larger scale problems, and incorporating additional constraints and complementary techniques into the overall framework.


## Summarize the paper in one paragraph.

 The paper proposes an automated approach to design branching structures in the encoder of a multi-task neural network architecture. It builds a differentiable neural architecture search method with a search space encompassing all possible branching configurations for a given set of tasks. The goal is to find resource-efficient structures that balance performance across tasks with computational efficiency. To achieve this, they introduce a proxyless resource-aware loss function that minimizes the expected multiply-add operations during inference. Evaluations on semantic segmentation, human parts segmentation, saliency estimation, surface normal estimation, and edge detection tasks demonstrate that their method consistently finds high-performing branching structures within a specified computational budget. The key novelty is the end-to-end trainable search algorithm and the resource-aware objective function for finding computationally efficient multi-task architectures.


## Summarize the paper in two paragraphs.

 The paper proposes an automated approach called Branched Multi-Task Architecture Search (BMTAS) to find efficient branching structures in the encoder stage of multi-task neural networks. The key ideas are:

1. The search space encompasses all possible branching structures for a given set of tasks. Candidate operations are duplicated in parallel branches and "warmed up" on each task before the search. During search, subgraphs are sampled for each task using learned masks and combined into full branching structures. 

2. A differentiable neural architecture search algorithm is used to learn the sampling masks. It alternates between updating architecture parameters and operation parameters, and uses Gumbel-Softmax to enable gradients.  

3. A novel proxyless, resource-aware loss function is introduced. It dynamically controls model size by combining a weighted sum of task losses with a term that minimizes multiply-add operations based on the task groupings.

Evaluations on PASCAL and NYUD datasets show BMTAS can effectively balance performance vs efficiency tradeoffs. It consistently finds high-performing branching structures within limited resource budgets. The method is flexible across tasks, datasets and backbones. Limitations include slower search times for many tasks due to the large search space. But overall, BMTAS provides an automated way to generate efficient MTL network architectures.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes an automated approach called Branched Multi-Task Architecture Search (BMTAS) to find efficient branching structures in the encoding stage of multi-task neural networks. The key ideas are:

- They define a search space that encompasses all possible branching structures for a given set of tasks. The search space is represented as a directed acyclic graph where vertices are feature tensors and edges are operations. 

- They use a differentiable neural architecture search method based on Gumbel-Softmax to directly learn routing probabilities through the graph for each task. The routings are combined to form branching structures.

- They introduce a novel proxyless resource-aware loss function that dynamically controls model size. The loss penalizes multiply-add operations of sampled architectures using a recursive formulation.

- By jointly optimizing for task performance and resource efficiency, BMTAS can automatically find high-performing branching structures within a target resource budget. Experiments on semantic segmentation, depth estimation etc. show models balancing efficiency vs performance.

In summary, the main contribution is an end-to-end trainable neural architecture search approach to automatically generate resource-efficient branching structures for multi-task learning problems.


## What problem or question is the paper addressing?

 The paper "Automated Search for Resource-Efficient Branched Multi-Task Networks" is addressing the problem of how to automatically design neural network architectures that can perform multiple visual tasks concurrently while remaining efficient. 

Specifically, the paper proposes an approach called Branched Multi-Task Architecture Search (BMTAS) to automatically determine branching structures in the encoder portion of a multi-task neural network. The goal is to find architectures that share computations for general features across tasks, while decoupling task-specific computations, in order to balance performance and efficiency. 

The key points are:

- Multi-task learning is important for real-world computer vision problems, but designing good multi-task architectures is challenging.

- Manual architecture design for multi-task networks likely exceeds human abilities, so automated architecture search is needed.

- BMTAS uses a differentiable search strategy to explore the space of possible branching structures for a given set of tasks. 

- A novel resource-aware loss function is introduced to directly optimize for efficiency during the architecture search.

- Experiments show BMTAS can find branching structures that effectively balance performance vs computational cost across various datasets and backbones.

In summary, the paper tackles the problem of automatically designing resource-efficient branched networks for multi-task learning in computer vision. The proposed BMTAS approach automates architecture search to find high-performing yet efficient branching structures.
