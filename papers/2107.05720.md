# [SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking](https://arxiv.org/abs/2107.05720)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we learn sparse representations for queries and documents that are effective for first-stage retrieval while being efficient enough to allow inverted index search?The paper proposes a new model called SPLADE that aims to address this question. The key ideas are:- Using a log-saturation activation function and sparse regularization to induce sparsity in the learned representations. This allows retrieval using inverted indexes.- Modeling query/document expansion within the sparse lexical space. This reduces vocabulary mismatch and improves effectiveness. - Training end-to-end with in-batch negatives and ranking loss. This simplifies training compared to prior work.- Controlling the sparsity via the regularization. This allows trading off effectiveness vs efficiency.So in summary, the central hypothesis is that with the right modeling choices (log saturation, expansion, regularization) they can learn sparse representations that are both effective and efficient for first-stage retrieval. The SPLADE model is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes SPLADE, a new sparse first-stage ranker for neural information retrieval based on explicit sparsity regularization and a log-saturation effect on term weights. This leads to highly sparse query and document representations.- SPLADE performs efficient document expansion, allowing it to fight vocabulary mismatch. The results are competitive with state-of-the-art dense models like ANCE even though SPLADE uses a simple single-stage training approach.- The paper shows how the sparsity regularization in SPLADE can be controlled to influence the trade-off between efficiency (in terms of floating point operations) and effectiveness. More regularization leads to greater sparsity and efficiency at the cost of some effectiveness.In summary, the main contribution is presenting SPLADE, a simple yet effective sparse neural ranking model for first-stage retrieval that can balance efficiency and accuracy via its sparsity regularization. The results are state-of-the-art for sparse models and competitive with more complex dense models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes SPLADE, a simple yet effective sparse neural ranking model for first-stage retrieval that rivals state-of-the-art dense models through a combination of in-batch negatives, logarithmic activation, and FLOPS regularization to learn sparse query and document representations.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of sparse neural representations for information retrieval:- This paper builds on prior work like SparTerm and SNRM in using sparse representations and regularization for efficient retrieval. However, it makes several key modifications like using in-batch negatives and log saturation that improve performance.- Compared to dense retrieval methods like ANCE, this paper shows competitive performance can be achieved with sparse models that allow inverted index search. This is an important finding as dense methods have dominated recently. - The simplicity of the model and training is a strength compared to other state-of-the-art methods that require multiple training stages, distillation etc. The simplicity could make SPLADE an attractive baseline to build on.- Analyzing the efficiency-effectiveness trade-off by tuning the regularization is novel. This provides guidance on model selection based on computational constraints.- The expansions learned seem more semantically meaningful than prior work like doc2query or SparTerm. This indicates the regularization helps learn a better expansion mechanism.- The results significantly outperform the original SparTerm paper, indicating the techniques like in-batch negatives and log saturation have a big impact. This is an important improvement on prior lexical matching models.Overall, I think this paper makes several notable contributions that advance sparse neural retrieval methods and demonstrate they can be competitive with more complex dense methods on benchmark datasets. The simplicity of the model is a major advantage, and the expansions learned appear higher quality than prior work.
