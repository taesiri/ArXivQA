# [A Text-to-Text Model for Multilingual Offensive Language Identification](https://arxiv.org/abs/2312.03379)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces FT5 and mFT5, the first ever transformer-based models using the T5 architecture that are pre-trained specifically for offensive language identification. FT5 is trained on English data from the SOLID and CCTK datasets, while mFT5 uses the same data to train a multilingual model. The authors employ semi-supervised learning techniques to optimize the selection and combination of training data. Extensive experiments on benchmark English and multilingual datasets demonstrate superior performance over previous state-of-the-art models like fBERT and HateBERT for both sentence-level and token-level offensive language identification. The multilingual mFT5 model also shows strong zero-shot cross-lingual transfer capabilities. Overall, the T5 architecture allows amalgamating multiple datasets with different taxonomies without dependency on label sets. By releasing FT5 and mFT5, the authors significantly advance the state-of-the-art in offensive language detection while enabling new research opportunities in multilingual and low-resource scenarios.
