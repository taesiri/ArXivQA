# [One Timestep is All You Need: Training Spiking Neural Networks with   Ultra Low Latency](https://arxiv.org/abs/2110.05929)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train deep spiking neural networks (SNNs) to perform inference using only a single timestep, thereby minimizing latency and maximizing energy efficiency? The key hypothesis is that an iterative training approach called "IIR-SNN" can enable deep SNNs to perform well with unit timestep latency. Specifically, the IIR-SNN method involves:1) Starting with an SNN trained for multiple timesteps (e.g. 5)2) Gradually reducing the number of timesteps, retraining the SNN at each stage using the weights from the prior higher timestep network as initialization.3) Repeating this process until reaching a single timestep SNN that performs accurately due to the gradual training approach.So in summary, the central question is how to obtain efficient single timestep SNNs, and the core hypothesis is that the proposed IIR-SNN technique can enable this through iterative initialization and retraining to compress the SNN along the temporal dimension. The paper aims to demonstrate that deep SNNs can perform complex tasks like ImageNet classification using just a single spike propagation.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing an iterative training technique called IIR-SNN to obtain deep spiking neural networks that can perform inference using just a single timestep. The key ideas are:- Starting with an SNN trained for 5 timesteps (the minimum required for good performance on ImageNet), they gradually reduce the latency by retraining the network initialized from the previous higher timestep network.- This allows them to reduce the latency all the way down to 1 timestep, while maintaining accuracy comparable to the corresponding ANN architecture. - Single timestep inference provides maximum possible temporal compression of SNNs and removes the memory access overhead of fetching membrane potentials at each timestep.- They demonstrate SNNs with 1 timestep achieving 93.05% accuracy on CIFAR-10 and 67.71% on ImageNet using VGG16, which is comparable to the original ANN versions.- Compared to other state-of-the-art SNNs, their method reduces latency by 5-2500x while achieving equal or better accuracy.- The single timestep SNNs are 25-33x more energy efficient compared to equivalent ANNs due to the sparsity resulting from low spike rates.- They also show the applicability of their training technique for obtaining low latency SNN solutions for reinforcement learning tasks like Cartpole and Atari Pong.In summary, the key contribution is proposing a sequential training scheme to obtain extremely low latency (single timestep) yet high performing SNNs for image classification and reinforcement learning applications. This addresses a major bottleneck of SNNs compared to ANNs in terms of inference latency while providing significant efficiency benefits.
