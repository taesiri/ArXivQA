# [One Timestep is All You Need: Training Spiking Neural Networks with   Ultra Low Latency](https://arxiv.org/abs/2110.05929)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train deep spiking neural networks (SNNs) to perform inference using only a single timestep, thereby minimizing latency and maximizing energy efficiency? The key hypothesis is that an iterative training approach called "IIR-SNN" can enable deep SNNs to perform well with unit timestep latency. Specifically, the IIR-SNN method involves:1) Starting with an SNN trained for multiple timesteps (e.g. 5)2) Gradually reducing the number of timesteps, retraining the SNN at each stage using the weights from the prior higher timestep network as initialization.3) Repeating this process until reaching a single timestep SNN that performs accurately due to the gradual training approach.So in summary, the central question is how to obtain efficient single timestep SNNs, and the core hypothesis is that the proposed IIR-SNN technique can enable this through iterative initialization and retraining to compress the SNN along the temporal dimension. The paper aims to demonstrate that deep SNNs can perform complex tasks like ImageNet classification using just a single spike propagation.
