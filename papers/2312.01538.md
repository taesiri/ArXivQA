# [Recurrent Distance-Encoding Neural Networks for Graph Representation   Learning](https://arxiv.org/abs/2312.01538)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Classical message passing neural networks (MPNNs) for graph representation learning struggle to utilize information from distant nodes in the graph effectively. They require many rounds of message passing to reach distant nodes, leading to exponential growth of neighborhoods and over-squashing of information.

- Recent graph transformers allow nodes to attend directly to all other nodes, but rely on ad-hoc positional encodings to incorporate graph structure inductive bias. The attention mechanism also leads to high computational complexity.

Proposed Solution:
- The paper proposes Graph Recurrent Encoding by Distance (GRED), a new architecture that reconciles the challenges with MPNNs and graph transformers. 

- For a target node, GRED categorizes other nodes into sets based on their shortest distance to the target. A permutation-invariant neural network generates representations for each set. 

- These set representations are fed into a linear recurrent neural network, ordered from farthest to closest set. This naturally encodes the neighborhood hierarchy and distance information without needing positional encodings.

- The recurrence output is transformed through MLPs to obtain the target node's new representation. Multiple GRED layers can be stacked.

Main Contributions:

- Combines strengths of MPNNs and graph transformers: effectively propagates long-range information without positional encoding, while being computationally efficient.

- Strictly more powerful than 1-hop MPNNs. Linear RNN injectively encodes neighborhood hierarchy.

- Outperforms MPNN baselines on various benchmarks. Comparable to state-of-the-art graph transformers but with 8.8x speedup.

- Naturally encodes graph structure through distance-based recurrence, without needing positional encodings.

In summary, GRED is a principled and powerful new architecture for graph representation learning that reconciles the challenges with prior approaches. It leverages linear RNNs to effectively and efficiently harness long-range dependencies in graph data.
