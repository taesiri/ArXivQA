# [Anomaly Detection via Learning-Based Sequential Controlled Sensing](https://arxiv.org/abs/2312.00088)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper addresses the problem of anomaly detection among a set of processes. The goal is to identify which processes are anomalous. To do this, a decision-making agent can sequentially select a subset of the processes to observe at each time instant. However, each observation is noisy (can be flipped with some probability) and incurs a cost. Therefore, the agent must balance the trade-off between accurately detecting anomalies (delay and accuracy) while minimizing the total sensing cost.

Proposed Solution:
The authors formulate the problem as a Markov decision process (MDP). The belief vector on the state of the processes is defined as the state. Choosing which subset of processes to observe next is the action. Two reward functions are proposed based on the Bayesian log likelihood ratio and entropy to encourage building confidence on the true state. The problem reduces to finding a policy (mapping from belief state to action) that maximizes long-term cumulative reward.  

Two approaches are presented to obtain good policies:

1) Deep reinforcement learning algorithms: Two RL algorithms are designed - deep Q-learning and actor-critic. These optimize policies based on learned value functions and directly optimize the policy respectively.

2) Deep active inference: Models the agent's internal model of environment statistics. Optimizes a policy by minimizing variational free energy, which is the divergence between the internal model and external observations.   

Main Contributions:

- Novel problem formulation for anomaly detection using Markov decision processes and suitable reward functions

- Two customized deep reinforcement learning algorithms using deep Q-learning and actor-critic 

- A deep active inference algorithm that relies on learning a generative model

- Extensive experiments comparing algorithms in terms of accuracy, delay and cost

- Key insights on adapting algorithms to problem statistics and recommendations on selecting suitable algorithms

In summary, the paper provides a comprehensive framework and deep learning solutions for sequential decision making in anomaly detection problems, balancing accuracy, delay and sensing costs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper formulates the problem of detecting anomalies among a set of processes as a Markov decision process, develops deep reinforcement learning and deep active inference algorithms to sequentially select processes for sensing to minimize detection delay and cost, and shows through simulations that the algorithms achieve similar accuracy but differ in their sensitivity to problem parameters like correlation among processes.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Formulation of anomaly detection as a Markov decision process (MDP). The paper defines the posterior belief vector on the conditions of the processes as the state of the MDP, the subset of processes chosen by the decision-making agent as the action, and two different types of reward functions: an average Bayesian log-likelihood ratio (LLR) based reward and an entropy-based reward. 

2. Development of deep reinforcement learning (RL) algorithms based on deep Q-learning and actor-critic frameworks to learn policies that maximize long-term rewards of the MDP.

3. Development of a deep active inference algorithm as an alternative solution strategy. This formulates anomaly detection as an active inference problem of minimizing free energy. 

4. Empirical evaluation and comparison of the deep RL and deep active inference algorithms in terms of detection accuracy, delay, and sensing cost. Key findings are that the active inference algorithm is more robust and adapts better to statistical dependence, while deep Q-learning is more sensitive to sensing costs.

In summary, the main contribution is the novel formulation and learning-based solutions for anomaly detection using sequential controlled sensing. Both model-based Bayesian updates and data-driven deep neural networks are leveraged.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms associated with it are:

- Anomaly detection
- Sequential controlled sensing 
- Markov decision processes
- Bayesian log-likelihood ratio reward
- Entropy-based reward 
- Deep reinforcement learning
- Deep Q-learning
- Policy gradient actor-critic
- Deep active inference
- Variational free energy
- Quickest state estimation 
- Sequential decision-making
- Sequential sensing

The paper formulates the anomaly detection problem as a sequential controlled sensing problem using Markov decision processes. It designs Bayesian log-likelihood ratio and entropy-based reward functions and develops deep reinforcement learning algorithms like deep Q-learning and actor-critic, as well as a deep active inference algorithm. The goal is to balance tradeoffs like accuracy, delay, and sensing cost for quickest state estimation. So the key terms revolve around anomaly detection, sequential decision-making under uncertainties, active sensing, and deep learning based solutions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the anomaly detection method proposed in the paper:

1. How does the Markov decision process (MDP) formulation balance the trade-off between detection accuracy, stopping time, and sensing cost? What are the key components of the MDP that enable this?

2. Explain the difference between the Bayesian log-likelihood ratio (LLR) based reward and the entropy based reward used in the MDP formulation. How do they encourage the agent to achieve the desired confidence level quickly?

3. The deep Q-learning method uses a dueling architecture to estimate both state-value and advantage functions. How does separating these two functions help learn a better policy?

4. Contrast the policy learning methods used in deep Q-learning versus the actor-critic framework. What are the relative advantages and disadvantages? 

5. Explain the concept of active inference and how it relates to reinforcement learning methods. In particular, how does active inference differ in its approach to policy optimization?

6. What is meant by the free energy function used in the active inference formulation? How is this function approximated and optimized by the neural networks?

7. How do the statistical dependencies between process states get exploited by the proposed algorithms to reduce the number of observations required?

8. Analyze the relative strengths and weaknesses of each algorithm – Q-learning, actor-critic, and active inference – based on the simulation results.

9. How does the Chernoff test baseline compare to the proposed deep learning methods? Under what conditions does it perform poorly in comparison?

10. If you had to deploy one of the proposed methods in a real-world system, which would you choose and why? What practical considerations would influence algorithm selection?
