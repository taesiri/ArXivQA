# [Variance-Dependent Regret Bounds for Non-stationary Linear Bandits](https://arxiv.org/abs/2403.10732)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies non-stationary stochastic linear bandits, where the reward distribution changes over time. Existing algorithms only characterize the non-stationarity using the total variation budget $B_K$, which measures changes in the expected rewards. 

- This is insufficient in many real applications where both the mean and variance of rewards can change over time. The key open question is whether algorithms can be improved by also utilizing information about the variance.

Main Contributions:
- The paper proposes two novel algorithms, Restarted-WeightedOFUL+ and Restarted-SAVE+, that utilize both the total variation budget $B_K$ and the total variance $V_K$ to achieve tighter regret bounds. 

- Restarted-WeightedOFUL+ assumes known per-round variances. It achieves regret $\tilde{O}(d^{7/8}(B_KV_K)^{1/4}\sqrt{K} + d^{5/6}B_K^{1/3}K^{2/3})$, which is better than state-of-the-art when $V_K$ is small.

- Restarted-SAVE+ allows unknown per-round variances. It maintains multi-layer weighted linear models and achieves regret $\tilde{O}(d^{4/5}V_K^{2/5}B_K^{1/5}K^{2/5} + d^{2/3}B_K^{1/3}K^{2/3})$. This is still better when $V_K$ is small.

- A third algorithm Restarted-SAVE+-BOB is proposed to tackle fully unknown $V_K, B_K$ using bandits-over-bandits, with slight degradation in theoretical bounds.

- Experiments validate the superiority of the proposed algorithms over prior methods that do not use variance information.

To summarize, the key contribution is in designing more informative non-stationary bandit algorithms by further utilizing the variance information, which lead to provably tighter regret bounds and empirical improvements.
