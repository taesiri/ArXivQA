# [FBNetV5: Neural Architecture Search for Multiple Tasks in One Run](https://arxiv.org/abs/2111.10007)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new neural architecture search (NAS) framework called FBNetV5 for efficiently searching for architectures for multiple computer vision tasks in a single run. The main research questions/goals addressed in this paper are:

1. How to design a NAS framework that can efficiently search neural architectures for multiple vision tasks (image classification, object detection, semantic segmentation) simultaneously in a single run? 

2. How to reduce the computational cost and human effort required for NAS when dealing with multiple tasks compared to prior NAS methods?

3. How to design a search space that is simple yet inclusive enough to produce strong architectures for different vision tasks?

4. How to disentangle the NAS search process from the individual training pipelines of each task to avoid repeatedly integrating NAS into new tasks?

5. How to develop a NAS algorithm with constant computational cost independent of the number of target tasks?

To address these challenges, the key ideas proposed in this paper include:

- A simple yet inclusive search space based on extending FBNetV3 to have multi-resolution parallel paths.

- A proxy multi-task dataset and disentangled search process to avoid integrating NAS into each task's pipeline. 

- A novel search algorithm using importance sampling and REINFORCE to enable single-run multi-task NAS.

- Empirical evaluation showing FBNetV5 can achieve state-of-the-art efficiency and accuracy on ImageNet classification, ADE20K segmentation, and COCO object detection simultaneously in one run.

So in summary, the main research contribution is proposing a novel NAS framework and techniques to significantly improve the efficiency and reduce the human effort needed to apply NAS to multiple vision tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FBNetV5, a neural architecture search (NAS) framework that can efficiently search for backbone topologies for multiple computer vision tasks in a single run. The key ideas include:

1. A simple yet inclusive and transferable search space extended from FBNetV3.

2. A disentangled search process using a multitask proxy dataset to avoid integrating NAS into each task's training pipeline. 

3. A search algorithm based on importance sampling and REINFORCE that can simultaneously produce architectures for multiple tasks with constant computational cost.

The experiments show FBNetV5 can search architectures in one run that outperform previous state-of-the-art task-specific models in ImageNet classification, COCO object detection, and ADE20K semantic segmentation. The main advantage is the improved efficiency and reduced human effort compared to doing NAS separately for each task. Overall, this work provides an effective framework for multitask NAS that is more scalable and transferable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes FBNetV5, a neural architecture search framework that can efficiently search for compact neural network architectures for multiple vision tasks (image classification, object detection, semantic segmentation) in a single run, outperforming prior state-of-the-art task-specific models.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in neural architecture search:

- The key focus of this paper is developing a NAS method that can efficiently search for architectures for multiple vision tasks in one run. This sets it apart from most prior NAS works that focus on searching architectures for a single task, usually image classification.

- The paper proposes a simple yet inclusive search space that can represent architectures suitable for different tasks. Many prior works designed specialized search spaces for each individual task instead. The transferable search space allows architectures found for one task to transfer to another. 

- The paper uses a proxy dataset and disentangled search process. This avoids having to integrate the search into every new task's training pipeline. Many recent works do "proxyless" NAS which requires non-trivial effort to couple NAS with each task's training.

- The proposed search algorithm can simultaneously optimize for multiple tasks in one run. This is more efficient than running search separately per task. The algorithm is based on differentiable NAS with innovations in importance sampling and policy gradients.

- Experiments show architectures found by the proposed method in one search run can surpass state-of-the-art task-specific architectures designed manually or by specialized per-task NAS methods. This demonstrates the promise of multi-task NAS.

In summary, this paper makes NAS more practical for real applications by making the search space transferable across tasks, disentangling search from training, and enabling efficient multi-task search. This contrasts with most prior works that customize NAS specifically for one task like image classification. The results demonstrate competitive performance, indicating multi-task NAS is a promising research direction.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring more granular search spaces, such as searching for block-wise channel sizes, which could further improve the performance of the searched models. The current work focused on a more macro search space of selecting blocks.

- Supporting the incremental addition of new tasks to the framework. Currently, the framework can search for multiple tasks in one run, but does not allow easy addition of new tasks later on. Enabling this could further improve the task scalability of the framework.

- Transferring searched architectures from one task (e.g. segmentation) to similar tasks (e.g. depth estimation) without rerunning the full search. This could reduce the compute needed when dealing with new but related tasks.

- Exploring alternate search spaces and search algorithms to further improve the efficiency and accuracy of the models found. The current framework provides a strong baseline, but there is room for innovation on the core search methodology.

- Evaluating the framework on a wider range of vision tasks beyond classification, segmentation, and detection. Extending to other tasks like human pose estimation could demonstrate broader applicability.

- Deploying the models found by the framework in real-world perception systems and quantifying the efficiency and performance gains in applied settings. This could reveal benefits and limitations not apparent from pure academic benchmarking.

In summary, the main directions seem to focus on improving the flexibility, scalability, and applicability of the framework to handle more tasks more efficiently, as well as quantifying performance in real-world systems. The core idea of multi-task architecture search is promising, but can likely be extended and refined further.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for neural network architectures for multiple computer vision tasks in a single run. The key ideas are 1) using a simple yet inclusive search space based on extending FBNetV3 to have parallel multi-resolution paths, 2) disentangling the search from individual tasks' training by using a proxy multi-task dataset, and 3) an algorithm to simultaneously optimize architectures for all tasks that reduces computational cost. Evaluated on ImageNet classification, COCO object detection, and ADE20K segmentation, FBNetV5 models searched in one run achieve state-of-the-art accuracy and efficiency trade-offs, outperforming prior task-specific NAS and hand-designed models. The framework reduces the effort of applying NAS to new tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for architectures for multiple computer vision tasks in one run. Previous NAS methods have focused mainly on image classification and require significant effort to apply to new tasks. FBNetV5 aims to address this by designing 1) a simple yet inclusive search space, 2) a disentangled multitask search process using a proxy dataset, and 3) an algorithm to simultaneously search for architectures for multiple tasks. 

FBNetV5 is evaluated on image classification, object detection, and semantic segmentation. Architectures searched by FBNetV5 in one run achieve state-of-the-art results on all three tasks, outperforming both manually designed and NAS models specialized for each task. For example, FBNetV5 models achieve 1.3% higher ImageNet accuracy than FBNetV3 under the same FLOPs, 1.8% higher ADE20K mIoU than SegFormer with 3.6x fewer FLOPs, and 1.1% higher COCO mAP than YOLOX with 1.2x fewer FLOPs. The disentangled search process and multitask algorithm allow FBNetV5 to efficiently produce high-performing architectures for multiple tasks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for backbone architectures for multiple computer vision tasks in a single run. The key ideas include: 1) Constructing a supernet with parallel paths and multiple resolutions extended from a state-of-the-art image classification model FBNetV3. This provides a simple yet inclusive search space. 2) Using a proxy multitask dataset with classification, detection and segmentation labels to train the supernet and search for optimal architectures for each task. This disentangles the search from downstream training. 3) Deriving a search algorithm based on importance sampling and REINFORCE that can search for multiple tasks simultaneously with constant compute cost. After one run of supernet training, task-specific architectures are sampled and trained individually using existing pipelines, achieving state-of-the-art accuracy and efficiency for image classification, object detection and semantic segmentation.


## What problem or question is the paper addressing?

 This paper proposes a neural architecture search (NAS) framework called FBNetV5 for searching neural network architectures for multiple computer vision tasks with reduced computational cost and human effort. The key issues it aims to address are:

1. Previous NAS research has focused too much on image classification while ignoring other vision tasks. This has led to suboptimal architectures for non-classification tasks. 

2. Many NAS methods optimize task-specific components that are not transferable to other tasks. This does not help reduce overall human design effort across tasks.

3. Existing NAS methods require integrating the search process into each task's training pipeline. This makes it hard to scale NAS to new tasks.

To address these issues, FBNetV5 proposes:

1. A simple yet inclusive search space that can represent strong architectures for multiple vision tasks.

2. A disentangled search process using a multitask proxy dataset. This avoids having to integrate NAS into each task's training.

3. An algorithm to simultaneously search architectures for multiple tasks in one run with cost independent of number of tasks.

So in summary, FBNetV5 aims to make NAS more efficient, scalable and transferable across multiple vision tasks compared to prior NAS techniques.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract, some of the key terms and concepts are:

- Neural Architecture Search (NAS): The paper focuses on using NAS to design neural network architectures for computer vision tasks. NAS aims to automate and optimize the neural architecture design process.

- Image classification: One of the key computer vision tasks that the paper targets. Prior NAS research has focused a lot on image classification.

- Object detection: Another computer vision task targeted by the paper. The goal is to use NAS to find optimal architectures for this task.

- Semantic segmentation: A third computer vision task considered in the paper. Again, the goal is to use NAS to design high-performing architectures. 

- Computational cost: The paper aims to reduce the computational resources and human effort needed for NAS by searching for multiple tasks jointly.

- Multitask learning: The method proposed trains one supernet on a dataset with multiple vision tasks. This allows jointly optimizing and searching architectures for all tasks.

- Transferable architectures: The goal is to find architectures that transfer well and achieve state-of-the-art results on multiple target tasks.

- Disentangled search: The paper proposes a search process decoupled from the downstream training to reduce engineering effort.

So in summary, the key focus is using NAS in a multi-task way to find efficient, high-performance architectures for major computer vision tasks like classification, detection and segmentation. The aim is to make NAS more scalable.
