# [FBNetV5: Neural Architecture Search for Multiple Tasks in One Run](https://arxiv.org/abs/2111.10007)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new neural architecture search (NAS) framework called FBNetV5 for efficiently searching for architectures for multiple computer vision tasks in a single run. The main research questions/goals addressed in this paper are:

1. How to design a NAS framework that can efficiently search neural architectures for multiple vision tasks (image classification, object detection, semantic segmentation) simultaneously in a single run? 

2. How to reduce the computational cost and human effort required for NAS when dealing with multiple tasks compared to prior NAS methods?

3. How to design a search space that is simple yet inclusive enough to produce strong architectures for different vision tasks?

4. How to disentangle the NAS search process from the individual training pipelines of each task to avoid repeatedly integrating NAS into new tasks?

5. How to develop a NAS algorithm with constant computational cost independent of the number of target tasks?

To address these challenges, the key ideas proposed in this paper include:

- A simple yet inclusive search space based on extending FBNetV3 to have multi-resolution parallel paths.

- A proxy multi-task dataset and disentangled search process to avoid integrating NAS into each task's pipeline. 

- A novel search algorithm using importance sampling and REINFORCE to enable single-run multi-task NAS.

- Empirical evaluation showing FBNetV5 can achieve state-of-the-art efficiency and accuracy on ImageNet classification, ADE20K segmentation, and COCO object detection simultaneously in one run.

So in summary, the main research contribution is proposing a novel NAS framework and techniques to significantly improve the efficiency and reduce the human effort needed to apply NAS to multiple vision tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FBNetV5, a neural architecture search (NAS) framework that can efficiently search for backbone topologies for multiple computer vision tasks in a single run. The key ideas include:

1. A simple yet inclusive and transferable search space extended from FBNetV3.

2. A disentangled search process using a multitask proxy dataset to avoid integrating NAS into each task's training pipeline. 

3. A search algorithm based on importance sampling and REINFORCE that can simultaneously produce architectures for multiple tasks with constant computational cost.

The experiments show FBNetV5 can search architectures in one run that outperform previous state-of-the-art task-specific models in ImageNet classification, COCO object detection, and ADE20K semantic segmentation. The main advantage is the improved efficiency and reduced human effort compared to doing NAS separately for each task. Overall, this work provides an effective framework for multitask NAS that is more scalable and transferable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes FBNetV5, a neural architecture search framework that can efficiently search for compact neural network architectures for multiple vision tasks (image classification, object detection, semantic segmentation) in a single run, outperforming prior state-of-the-art task-specific models.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in neural architecture search:

- The key focus of this paper is developing a NAS method that can efficiently search for architectures for multiple vision tasks in one run. This sets it apart from most prior NAS works that focus on searching architectures for a single task, usually image classification.

- The paper proposes a simple yet inclusive search space that can represent architectures suitable for different tasks. Many prior works designed specialized search spaces for each individual task instead. The transferable search space allows architectures found for one task to transfer to another. 

- The paper uses a proxy dataset and disentangled search process. This avoids having to integrate the search into every new task's training pipeline. Many recent works do "proxyless" NAS which requires non-trivial effort to couple NAS with each task's training.

- The proposed search algorithm can simultaneously optimize for multiple tasks in one run. This is more efficient than running search separately per task. The algorithm is based on differentiable NAS with innovations in importance sampling and policy gradients.

- Experiments show architectures found by the proposed method in one search run can surpass state-of-the-art task-specific architectures designed manually or by specialized per-task NAS methods. This demonstrates the promise of multi-task NAS.

In summary, this paper makes NAS more practical for real applications by making the search space transferable across tasks, disentangling search from training, and enabling efficient multi-task search. This contrasts with most prior works that customize NAS specifically for one task like image classification. The results demonstrate competitive performance, indicating multi-task NAS is a promising research direction.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring more granular search spaces, such as searching for block-wise channel sizes, which could further improve the performance of the searched models. The current work focused on a more macro search space of selecting blocks.

- Supporting the incremental addition of new tasks to the framework. Currently, the framework can search for multiple tasks in one run, but does not allow easy addition of new tasks later on. Enabling this could further improve the task scalability of the framework.

- Transferring searched architectures from one task (e.g. segmentation) to similar tasks (e.g. depth estimation) without rerunning the full search. This could reduce the compute needed when dealing with new but related tasks.

- Exploring alternate search spaces and search algorithms to further improve the efficiency and accuracy of the models found. The current framework provides a strong baseline, but there is room for innovation on the core search methodology.

- Evaluating the framework on a wider range of vision tasks beyond classification, segmentation, and detection. Extending to other tasks like human pose estimation could demonstrate broader applicability.

- Deploying the models found by the framework in real-world perception systems and quantifying the efficiency and performance gains in applied settings. This could reveal benefits and limitations not apparent from pure academic benchmarking.

In summary, the main directions seem to focus on improving the flexibility, scalability, and applicability of the framework to handle more tasks more efficiently, as well as quantifying performance in real-world systems. The core idea of multi-task architecture search is promising, but can likely be extended and refined further.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for neural network architectures for multiple computer vision tasks in a single run. The key ideas are 1) using a simple yet inclusive search space based on extending FBNetV3 to have parallel multi-resolution paths, 2) disentangling the search from individual tasks' training by using a proxy multi-task dataset, and 3) an algorithm to simultaneously optimize architectures for all tasks that reduces computational cost. Evaluated on ImageNet classification, COCO object detection, and ADE20K segmentation, FBNetV5 models searched in one run achieve state-of-the-art accuracy and efficiency trade-offs, outperforming prior task-specific NAS and hand-designed models. The framework reduces the effort of applying NAS to new tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for architectures for multiple computer vision tasks in one run. Previous NAS methods have focused mainly on image classification and require significant effort to apply to new tasks. FBNetV5 aims to address this by designing 1) a simple yet inclusive search space, 2) a disentangled multitask search process using a proxy dataset, and 3) an algorithm to simultaneously search for architectures for multiple tasks. 

FBNetV5 is evaluated on image classification, object detection, and semantic segmentation. Architectures searched by FBNetV5 in one run achieve state-of-the-art results on all three tasks, outperforming both manually designed and NAS models specialized for each task. For example, FBNetV5 models achieve 1.3% higher ImageNet accuracy than FBNetV3 under the same FLOPs, 1.8% higher ADE20K mIoU than SegFormer with 3.6x fewer FLOPs, and 1.1% higher COCO mAP than YOLOX with 1.2x fewer FLOPs. The disentangled search process and multitask algorithm allow FBNetV5 to efficiently produce high-performing architectures for multiple tasks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes FBNetV5, a neural architecture search (NAS) framework that can efficiently search for backbone architectures for multiple computer vision tasks in a single run. The key ideas include: 1) Constructing a supernet with parallel paths and multiple resolutions extended from a state-of-the-art image classification model FBNetV3. This provides a simple yet inclusive search space. 2) Using a proxy multitask dataset with classification, detection and segmentation labels to train the supernet and search for optimal architectures for each task. This disentangles the search from downstream training. 3) Deriving a search algorithm based on importance sampling and REINFORCE that can search for multiple tasks simultaneously with constant compute cost. After one run of supernet training, task-specific architectures are sampled and trained individually using existing pipelines, achieving state-of-the-art accuracy and efficiency for image classification, object detection and semantic segmentation.


## What problem or question is the paper addressing?

 This paper proposes a neural architecture search (NAS) framework called FBNetV5 for searching neural network architectures for multiple computer vision tasks with reduced computational cost and human effort. The key issues it aims to address are:

1. Previous NAS research has focused too much on image classification while ignoring other vision tasks. This has led to suboptimal architectures for non-classification tasks. 

2. Many NAS methods optimize task-specific components that are not transferable to other tasks. This does not help reduce overall human design effort across tasks.

3. Existing NAS methods require integrating the search process into each task's training pipeline. This makes it hard to scale NAS to new tasks.

To address these issues, FBNetV5 proposes:

1. A simple yet inclusive search space that can represent strong architectures for multiple vision tasks.

2. A disentangled search process using a multitask proxy dataset. This avoids having to integrate NAS into each task's training.

3. An algorithm to simultaneously search architectures for multiple tasks in one run with cost independent of number of tasks.

So in summary, FBNetV5 aims to make NAS more efficient, scalable and transferable across multiple vision tasks compared to prior NAS techniques.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract, some of the key terms and concepts are:

- Neural Architecture Search (NAS): The paper focuses on using NAS to design neural network architectures for computer vision tasks. NAS aims to automate and optimize the neural architecture design process.

- Image classification: One of the key computer vision tasks that the paper targets. Prior NAS research has focused a lot on image classification.

- Object detection: Another computer vision task targeted by the paper. The goal is to use NAS to find optimal architectures for this task.

- Semantic segmentation: A third computer vision task considered in the paper. Again, the goal is to use NAS to design high-performing architectures. 

- Computational cost: The paper aims to reduce the computational resources and human effort needed for NAS by searching for multiple tasks jointly.

- Multitask learning: The method proposed trains one supernet on a dataset with multiple vision tasks. This allows jointly optimizing and searching architectures for all tasks.

- Transferable architectures: The goal is to find architectures that transfer well and achieve state-of-the-art results on multiple target tasks.

- Disentangled search: The paper proposes a search process decoupled from the downstream training to reduce engineering effort.

So in summary, the key focus is using NAS in a multi-task way to find efficient, high-performance architectures for major computer vision tasks like classification, detection and segmentation. The aim is to make NAS more scalable.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the motivation for the work? Why is neural architecture search for multiple tasks in one run important? 

2. What are the key limitations of prior NAS methods that this work aims to address?

3. What is the proposed framework called and what are its key components or techniques?

4. What search space does the framework use? What are its key properties?

5. How does the framework conduct search to disentangle it from target tasks' training pipelines? 

6. What is the proposed supernet training algorithm? How does it enable efficient simultaneous search for multiple tasks?

7. What is the multitask proxy dataset used for search and how is it constructed?

8. What are the target tasks used to evaluate the framework and dataset used for each task?

9. What are the main results? How do the models searched by the framework compare to prior state-of-the-art models for each task?

10. What are the limitations discussed and future work suggested by the authors?

In summary, the key questions cover the motivation and problem definition, details of the proposed framework and techniques, experimental setup and results, and limitations and future work. Asking these types of questions can help create a comprehensive yet concise summary of the key contributions and findings presented in the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a simple yet inclusive and transferable search space. Can you explain in more detail how the search space is constructed and why it meets those three criteria? What limitations might this search space have?

2. The paper uses a proxy multitask dataset for the search process. Can you explain why this dataset was created and how it allows for a disentangled search process? What are the tradeoffs of using a proxy dataset versus real labeled data? 

3. The proposed search algorithm reduces computational cost by using importance sampling and REINFORCE. Can you walk through how those techniques work and why they reduce cost? What hyperparameters or implementation details are most important for making this work?

4. The experiments show the method can find architectures that beat state-of-the-art task-specific models. Why do you think this simultaneous multitask search works so well? Does it contradict the common wisdom that specialized architectures are best?

5. The method claims to reduce human effort compared to task-specific NAS techniques. In what ways does it reduce human effort? What engineering challenges might still exist in applying this method?

6. How does the proposed method compare to other recent works on multi-task NAS like HR-NAS and ScaleNAS? What are the key similarities and differences?

7. The ablation study shows multitask search outperforms single task search. Why might sharing information across tasks lead to better models than specializing? When might single task search be more appropriate?

8. The segmentation model found has a U-Net like topology while the classification model is more unorthodox. Why might the method find such different architectures for each task?

9. The method searches over block-level topological choices. How might exploring more fine-grained search spaces affect the results? Would the cost savings of the method remain?

10. The method currently searches over 3 tasks. How could the approach be extended to handle more tasks? What challenges might arise as the number of tasks scales up?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points in the paper:

This CVPR 2022 paper proposes FBNetV5, a neural architecture search (NAS) framework that can simultaneously search for efficient architectures for multiple computer vision tasks with reduced computational cost. The authors design a simple yet inclusive search space extended from FBNetV3 and conduct search by training a supernet on a multitask proxy dataset once. They propose a novel search algorithm based on importance sampling and REINFORCE that finds architectures for multiple tasks simultaneously in one run. Extensive experiments demonstrate FBNetV5's effectiveness - architectures found for image classification, semantic segmentation, and object detection in a single run of FBNetV5 outperform previous state-of-the-art task-specific models in terms of accuracy under the same computational budgets. For example, the FBNetV5 search backbone achieves 1.3% higher ImageNet top-1 accuracy than FBNetV3, 1.8% higher ADE20K mIoU than SegFormer, and 1.1% higher COCO AP than YOLOX. By disentangling the search from individual task training, FBNetV5 significantly reduces the engineering efforts of applying NAS to new tasks.


## Summarize the paper in one sentence.

 This paper proposes FBNetV5, a neural architecture search framework that can simultaneously search for architectures for multiple computer vision tasks like image classification, object detection, and semantic segmentation in a single run.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points in the paper:

The paper proposes FBNetV5, a neural architecture search framework that can simultaneously search for optimal backbone topologies for multiple computer vision tasks like image classification, object detection, and semantic segmentation. FBNetV5 features a simple yet inclusive search space, a disentangled multitask search process using a proxy dataset, and an efficient search algorithm with constant compute cost independent of the number of tasks. Experiments show FBNetV5 can find compact models surpassing previous state-of-the-art in all three tasks with a single run of architecture search. For example, it achieves 1.3% higher ImageNet accuracy than FBNetV3, 1.8% higher ADE20K mIoU than SegFormer, and 1.1% higher COCO mAP than YOLOX, demonstrating its ability to efficiently produce high-performance architectures for diverse vision tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the FBNetV5 paper:

1. The paper claims the search space is simple, inclusive and transferable. What makes the search space designed in this work simple compared to other NAS works? Why is it more inclusive and transferable?

2. The paper proposes a disentangled search process using a multitask proxy dataset. What are the key advantages of conducting search in this way compared to integrating NAS into each task's training pipeline? How does the multitask proxy dataset help enable an efficient search?

3. The simultaneous search algorithm for multiple tasks uses importance sampling and REINFORCE. Walk through the key steps that led to reducing the number of forward and backward passes per iteration to 1. Why is this important for scalability?

4. The experiments show the searched models achieve state-of-the-art performance on image classification, object detection and segmentation. Analyze the architectures found for each task. What interesting or surprising observations can you make about the optimal topology discovered for each task?

5. The classification architecture found contains many blocks from higher resolutions, contrasting most existing NAS works. What implications might this have on future classification model design?

6. How well does the proposed approach address the limitations of prior NAS techniques discussed in the introduction? What limitations still remain?

7. The method currently searches architectures for predefined tasks. How could the framework potentially be extended to enable incremental addition of new tasks without rerunning the full search?

8. What further enhancements could be made to the search space design to potentially find even better models? Would searching block-level hyperparameters like channel size help?

9. The paper focuses on CV tasks. What changes would need to be made to apply this method to search architectures for NLP or other domains? Would the overall approach still be effective?

10. The method trains one multi-task supernet. How does training a single supernet compare to independently training multiple task-specific supernets in terms of efficiency and result quality? What are the key trade-offs?
