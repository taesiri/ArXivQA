# [Brain-Inspired Machine Intelligence: A Survey of   Neurobiologically-Plausible Credit Assignment](https://arxiv.org/abs/2312.09257)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper provides a comprehensive survey and taxonomy of biologically-inspired algorithms for credit assignment in artificial neural networks. Credit assignment refers to the problem of determining how to assign "credit" or "blame" to individual units in a neural network based on their contribution to the final output. 

The paper first provides background on the popular backpropagation algorithm, which is the default method for credit assignment in most modern deep neural networks. However, backpropagation has been criticized over the years as being biologically implausible. The paper summarizes the main problems with backpropagation, including requiring symmetric weights between layers, a separate backward pass, and non-local weight updates.

The core of the paper is a taxonomy that organizes biologically-inspired credit assignment algorithms based on the source of the learning signals that drive weight updates. The taxonomy has two main branches: implicit target signals, where no explicit targets are used, and explicit target signals, which are further divided into global versus local and synergistic versus non-synergistic signals. 

The paper reviews six families of algorithms within this taxonomy:

- Implicit signal algorithms like Hebbian learning that use only local pre- and post-synaptic activity
- Explicit global signal algorithms like neuromodulated Hebbian learning driven by global reward signals 
- Non-synergistic explicit local signal algorithms like synthetic gradient methods
- Synergistic explicit local signal algorithms based on discrepancy reduction (e.g. predictive coding), energy minimization (e.g. contrastive Hebbian learning), and forward-only passes (no backpropagation).

For each family, the paper summarizes key algorithms, how they address deficiencies of backpropagation, and evidence of their application to problems like computer vision and reinforcement learning. 

The paper concludes by arguing that no single algorithm likely explains learning in the brain, but combinations across the taxonomy may lead to more capable and efficient artificial learning systems. Future challenges include scaling up algorithms to complex tasks and ensuring algorithms are compatible with spiking neural networks and neuromorphic hardware.
