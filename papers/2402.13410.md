# [Bayesian Neural Networks with Domain Knowledge Priors](https://arxiv.org/abs/2402.13410)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Bayesian neural networks (BNNs) allow quantifying predictive uncertainty, but specifying good priors that incorporate domain knowledge is challenging. Existing BNN priors (e.g. Gaussian, Gaussian processes) cannot easily encode complex domain knowledge like physics rules, feature importance, fairness constraints. This makes BNNs susceptible to undesirable biases and suboptimal predictions.  

Solution:
The paper proposes a framework to incorporate broad forms of domain knowledge into BNN priors. The key ideas are:

1) Represent domain knowledge as a loss function $\phi(h,x)$ that measures how well a model $h$ aligns with the knowledge on input $x$. Many types of knowledge like physics rules, invariances, fairness can be expressed this way.

2) Learn an informative prior by optimizing a variational lower bound on the posterior $p(w|\phi,X')$ that incorporates the loss $\phi$ on unlabeled data $X'$. The learned prior assigns high probability to models with low domain knowledge loss. 

3) Use a low-rank Gaussian approximation for the informative prior to enable efficient posterior inference. The low-rank structure scales as $O(r \cdot n)$ where $r$ is the rank and $n$ is the number of parameters.

4) Transfer learned priors to new architectures by matching samples from old and new priors in function space using maximum mean discrepancy. This allows reusing informative priors when the loss $\phi$ is no longer accessible.

Contributions:

- General framework to incorporate diverse domain knowledge into BNN priors based on losses 

- Use of variational inference to learn priors encoding domain knowledge  

- Low-rank Gaussian prior enables tractable posterior inference

- Techniques to transfer learned priors across architectures

- Empirically demonstrates improved predictive performance and better satisfaction of domain knowledge constraints versus baselines

The paper makes BNNs more practical by developing methods to include complex and useful domain knowledge through priors. This can lead to models that are more reliable, fair and calibrated.
