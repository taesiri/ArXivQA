# [Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel   Prediction](https://arxiv.org/abs/1611.09842)

## What is the central research question or hypothesis that this paper addresses?

After reviewing the paper, it appears the central research question is:Can we improve feature representations learned by autoencoders for transfer learning tasks by modifying the architecture to perform cross-channel prediction instead of reconstruction?The key hypothesis is that forcing the network to solve complementary prediction problems by splitting the architecture will result in representations that transfer better to unseen tasks compared to traditional autoencoders trained on reconstruction objectives. The authors propose a "split-brain autoencoder" architecture composed of two disjoint sub-networks. Each sub-network is trained to predict one subset of the data channels (e.g. color) from another subset (e.g. grayscale). This architectural change from traditional autoencoders makes the pre-training task one of cross-channel prediction rather than reconstruction. The central hypothesis is that this induction of cross-channel prediction tasks will force the network to learn representations with greater abstraction and semantic meaning, which will transfer better to new tasks. The authors demonstrate state-of-the-art performance on several representation learning benchmarks compared to prior unsupervised learning methods.In summary, the key research question is whether architectural modifications to autoencoders can improve transferability of learned features by changing the pre-training objective from reconstruction to cross-channel prediction. The authors propose split-brain autoencoders to test this hypothesis and demonstrate improved performance.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a modified autoencoder architecture called a "split-brain autoencoder" for unsupervised representation learning. The key ideas are:- The autoencoder is split into two disjoint sub-networks. - Each sub-network is trained to perform a difficult prediction task - predicting one subset of the data channels from another subset (called "cross-channel prediction").- By forcing the network to solve these complementary prediction tasks, it learns an internal representation that transfers well to other unseen tasks. - This method avoids some weaknesses of regular autoencoders, like the inherent tradeoff between forced abstraction and information bottlenecking.- The split-brain architecture allows extracting features from the full input, unlike some prior cross-channel prediction techniques.- The method achieves state-of-the-art performance on several representation learning benchmarks compared to previous unsupervised approaches.So in summary, the key contribution is proposing this split-brain autoencoder architecture that trains sub-networks to perform cross-channel prediction tasks, which induces a useful representation for transfer learning while overcoming some limitations of prior techniques. The effectiveness is demonstrated through experiments on benchmark datasets.
