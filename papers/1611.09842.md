# [Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel   Prediction](https://arxiv.org/abs/1611.09842)

## What is the central research question or hypothesis that this paper addresses?

After reviewing the paper, it appears the central research question is:Can we improve feature representations learned by autoencoders for transfer learning tasks by modifying the architecture to perform cross-channel prediction instead of reconstruction?The key hypothesis is that forcing the network to solve complementary prediction problems by splitting the architecture will result in representations that transfer better to unseen tasks compared to traditional autoencoders trained on reconstruction objectives. The authors propose a "split-brain autoencoder" architecture composed of two disjoint sub-networks. Each sub-network is trained to predict one subset of the data channels (e.g. color) from another subset (e.g. grayscale). This architectural change from traditional autoencoders makes the pre-training task one of cross-channel prediction rather than reconstruction. The central hypothesis is that this induction of cross-channel prediction tasks will force the network to learn representations with greater abstraction and semantic meaning, which will transfer better to new tasks. The authors demonstrate state-of-the-art performance on several representation learning benchmarks compared to prior unsupervised learning methods.In summary, the key research question is whether architectural modifications to autoencoders can improve transferability of learned features by changing the pre-training objective from reconstruction to cross-channel prediction. The authors propose split-brain autoencoders to test this hypothesis and demonstrate improved performance.
