# [Non-Markovian Reward Modelling from Trajectory Labels via Interpretable   Multiple Instance Learning](https://arxiv.org/abs/2205.15367)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can reward modelling for reinforcement learning be generalized to handle non-Markovian rewards that exhibit temporal dependencies?

The standard assumption in reward modelling is that human evaluators assess each state-action pair independently when giving feedback on trajectories. This paper argues that in reality, human assessment of trajectories is temporally extended, so earlier events can influence the evaluation of later ones. The main contribution is developing new models based on multiple instance learning that can capture these temporal dependencies and reconstruct non-Markovian reward functions.

The key hypotheses tested are:

1) The proposed LSTM-based multiple instance learning models can accurately predict trajectory returns and timestep rewards for non-Markovian tasks with temporal dependencies.

2) The learnt models provide interpretable hidden state representations that capture the temporal dependencies. 

3) The predicted rewards and hidden states from the models can be used to train reinforcement learning agents to solve non-Markovian tasks.

So in summary, the central research question is how to do reward modelling when rewards are non-Markovian, and the main hypothesis is that multiple instance learning provides a promising approach to handle the temporal dependencies.


## What is the main contribution of this paper?

 This paper presents a method to handle non-Markovian rewards in reinforcement learning (RL) through reward modelling. The main contributions are:

1. It generalizes reward modelling to allow for temporal dependencies in how humans evaluate agent trajectories, by modelling hidden state information that accumulates over time. 

2. It identifies a connection between reward modelling and multiple instance learning (MIL), allowing MIL concepts and models to be applied to this problem.

3. It proposes novel LSTM-based MIL architectures to model the hidden state dynamics and make reward predictions from trajectory return labels.

4. It evaluates the proposed models on RL tasks with non-Markovian rewards, showing they can accurately reconstruct rewards and hidden states. The models can also effectively augment RL training.

5. It provides interpretability analysis of the learnt models, visualizing and explaining the recovered hidden states and rewards.

In summary, the key contribution is a general framework and set of techniques for handling temporal dependencies in reward modelling, enabling the training of RL agents for non-Markovian tasks. This is achieved by modelling hidden states with MIL and LSTM architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes new methods for reward modeling in reinforcement learning that can capture temporal dependencies in how humans evaluate and provide feedback on agent behavior over entire trajectories, rather than assuming each state-action pair is assessed independently.


## How does this paper compare to other research in the same field?

 This paper presents a novel approach to generalizing reward modeling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Here are some key ways it compares to other related work:

- Most prior work in RM assumes rewards are Markovian, i.e. depend only on the current state-action pair. This paper relaxes that assumption by allowing rewards to depend on historical trajectory information via a hidden state.

- A few other papers have tried to capture temporal dependencies in RM, but in fairly restricted ways such as using a discrete psychological mode or monotonic bias term. This paper proposes a more general framework using LSTM models to represent complex hidden state dynamics.

- The idea of connecting RM and multiple instance learning is novel. Viewing trajectories as bags and state-action pairs as instances enables transferring concepts like LSTM aggregation between the fields.  

- The proposed LSTM architectures for non-Markovian RM are new. Prior LSTM MIL models don't separate hidden states and rewards, while this work designs models tailored for RM with disentangled components.

- Evaluating the ability to reconstruct ground truth rewards, augment RL training, and interpret learned models on a diverse set of non-Markovian RL tasks is more comprehensive than most prior work.

- The model analysis and concept of "trajectory probing" to verify hidden state transitions provides a level of interpretability lacking in most prior LSTM-based approaches.

Overall, this paper makes multiple novel contributions to extending RM to sequential and potentially biased human feedback. The connections made to MIL and recurring themes of interpretability also help push the field forward. The range of experiments provides a solid proof of concept for the proposed techniques.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions the authors suggest include:

- Applying the methods to human labelling rather than just synthetic oracle labelling. The paper focuses on using oracle labels for validation, but notes that evaluating with real human labels is an important next step. This could reveal new challenges around label uncertainty, sparsity, different preference formats, etc.

- Testing the methods on more complex environments. The paper proposes some new non-Markovian benchmark tasks, but suggests adapting other established RL environments like Atari games could provide further insights. Image observations in particular may pose new challenges.

- Iterative bootstrapping of reward modelling and RL agent training. Rather than separate offline training of the reward model then online use for RL, the authors propose interleaving these steps could improve performance. 

- Transfer of concepts and techniques between reward modelling and multiple instance learning. The paper identifies a link between these fields, and suggests further exchange of ideas could be mutually beneficial.

- Alternative model architectures to prevent negative reward predictions. For the Lunar Lander task, some models struggled due to predicting impossible negative rewards. The authors suggest several mechanisms to address this issue.

- Analysis of what causes the "better than oracle" phenomenon sometimes seen. In certain cases, agents trained using the learnt reward model outperformed those trained on the ground truth. Understanding this counterintuitive result could further improve performance.

So in summary, applying the methods to real human data, scaling up to more complex tasks, iterative training procedures, tighter integration with MIL, improving model training, and analysing cases where models improve on ground truth are noted as interesting directions for future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach for reward modelling in reinforcement learning that can handle non-Markovian rewards. Existing reward modelling methods assume the human evaluator assesses each step in a trajectory independently when giving feedback. This paper removes that assumption and allows for temporal dependencies in the human's assessment of trajectories. They formulate the problem as an instance of multiple instance learning, where trajectories are treated as bags of state-action pairs (instances) that collectively contribute to the overall trajectory label (bag label) in complex ways. They propose LSTM-based architectures to model these temporal dependencies and show they can accurately reconstruct rewards on non-Markovian RL tasks. The learnt models provide interpretable representations of the hidden dynamics and can successfully augment RL agent training. Overall, the paper generalizes reward modelling to handle settings where the human's preferences have temporal dependencies that make rewards dependent on historical context alongside visible state/action information.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper generalizes the problem of reward modeling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Existing RM work assumes that human evaluators observe each step in a trajectory independently when providing feedback on agent behavior. This paper removes that assumption, extending RM to include hidden state information that captures temporal dependencies in human assessment of trajectories. The authors identify a connection between RM and multiple instance learning (MIL). Trajectories are treated as “bags” of state-action pairs (“instances”), with labels provided at the bag level. This inspires novel LSTM-based MIL model architectures to recover hidden state dynamics and learn instance-level reward predictions from labeled trajectories. 

Experiments use synthetic oracle labels for five RL tasks. The proposed models accurately reconstruct hidden states and rewards for non-Markovian tasks. They are integrated into RL agent training, achieving performance matching or exceeding agents with direct access to true states and rewards. Analysis provides insight into the representations learned by the models. Overall, the work generalizes RM to handle temporal reward dependencies, leverages MIL concepts for this problem, and demonstrates the feasibility of learning and utilizing non-Markovian reward functions.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new approach for reward modelling (RM) in reinforcement learning that can handle non-Markovian rewards, where the reward for a state-action pair depends on historical context. It formulates RM as a multiple instance learning (MIL) problem, where trajectories are treated as bags with return labels, and state-action pairs within trajectories are instances with unseen reward labels. Long short-term memory (LSTM) architectures are used to model temporal dependencies between instances in a bag. Specifically, the paper develops concatenated skip connection instance space LSTM models that learn hidden state dynamics to capture historical dependencies, alongside reward predictions for individual state-action instances. These models are shown to effectively reconstruct rewards for non-Markovian tasks, and can be straightforwardly integrated into RL agent training by augmenting states with their hidden representations. The LSTM models outperform baseline MIL architectures lacking temporal modelling, and enable agents to solve tasks that are impossible with only the visible state.


## What problem or question is the paper addressing?

 The paper is addressing the problem of reward modelling in reinforcement learning when the reward is non-Markovian. Specifically, it is looking at the case where human evaluators provide trajectory-level feedback that depends on temporal structure and hidden state information. 

The key questions the paper tries to address are:

1) How can we extend reward modelling to handle non-Markovian rewards that depend on temporal dependencies and hidden states?

2) How can we leverage ideas from multiple instance learning to develop models that can learn from trajectory-level feedback and uncover the hidden dynamics? 

3) How can the learnt models be used to help train reinforcement learning agents on tasks with non-Markovian rewards?

4) How can we interpret what the models have learned about the hidden states and dynamics?

So in summary, it is generalizing reward modelling to handle more complex temporal dependencies, using concepts from multiple instance learning, with the end goal of enabling reinforcement learning in non-Markovian environments. The key innovation is in modeling trajectory-level rewards as "bags" with temporal instance dependencies.
