# [Non-Markovian Reward Modelling from Trajectory Labels via Interpretable   Multiple Instance Learning](https://arxiv.org/abs/2205.15367)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can reward modelling for reinforcement learning be generalized to handle non-Markovian rewards that exhibit temporal dependencies?

The standard assumption in reward modelling is that human evaluators assess each state-action pair independently when giving feedback on trajectories. This paper argues that in reality, human assessment of trajectories is temporally extended, so earlier events can influence the evaluation of later ones. The main contribution is developing new models based on multiple instance learning that can capture these temporal dependencies and reconstruct non-Markovian reward functions.

The key hypotheses tested are:

1) The proposed LSTM-based multiple instance learning models can accurately predict trajectory returns and timestep rewards for non-Markovian tasks with temporal dependencies.

2) The learnt models provide interpretable hidden state representations that capture the temporal dependencies. 

3) The predicted rewards and hidden states from the models can be used to train reinforcement learning agents to solve non-Markovian tasks.

So in summary, the central research question is how to do reward modelling when rewards are non-Markovian, and the main hypothesis is that multiple instance learning provides a promising approach to handle the temporal dependencies.


## What is the main contribution of this paper?

 This paper presents a method to handle non-Markovian rewards in reinforcement learning (RL) through reward modelling. The main contributions are:

1. It generalizes reward modelling to allow for temporal dependencies in how humans evaluate agent trajectories, by modelling hidden state information that accumulates over time. 

2. It identifies a connection between reward modelling and multiple instance learning (MIL), allowing MIL concepts and models to be applied to this problem.

3. It proposes novel LSTM-based MIL architectures to model the hidden state dynamics and make reward predictions from trajectory return labels.

4. It evaluates the proposed models on RL tasks with non-Markovian rewards, showing they can accurately reconstruct rewards and hidden states. The models can also effectively augment RL training.

5. It provides interpretability analysis of the learnt models, visualizing and explaining the recovered hidden states and rewards.

In summary, the key contribution is a general framework and set of techniques for handling temporal dependencies in reward modelling, enabling the training of RL agents for non-Markovian tasks. This is achieved by modelling hidden states with MIL and LSTM architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes new methods for reward modeling in reinforcement learning that can capture temporal dependencies in how humans evaluate and provide feedback on agent behavior over entire trajectories, rather than assuming each state-action pair is assessed independently.


## How does this paper compare to other research in the same field?

 This paper presents a novel approach to generalizing reward modeling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Here are some key ways it compares to other related work:

- Most prior work in RM assumes rewards are Markovian, i.e. depend only on the current state-action pair. This paper relaxes that assumption by allowing rewards to depend on historical trajectory information via a hidden state.

- A few other papers have tried to capture temporal dependencies in RM, but in fairly restricted ways such as using a discrete psychological mode or monotonic bias term. This paper proposes a more general framework using LSTM models to represent complex hidden state dynamics.

- The idea of connecting RM and multiple instance learning is novel. Viewing trajectories as bags and state-action pairs as instances enables transferring concepts like LSTM aggregation between the fields.  

- The proposed LSTM architectures for non-Markovian RM are new. Prior LSTM MIL models don't separate hidden states and rewards, while this work designs models tailored for RM with disentangled components.

- Evaluating the ability to reconstruct ground truth rewards, augment RL training, and interpret learned models on a diverse set of non-Markovian RL tasks is more comprehensive than most prior work.

- The model analysis and concept of "trajectory probing" to verify hidden state transitions provides a level of interpretability lacking in most prior LSTM-based approaches.

Overall, this paper makes multiple novel contributions to extending RM to sequential and potentially biased human feedback. The connections made to MIL and recurring themes of interpretability also help push the field forward. The range of experiments provides a solid proof of concept for the proposed techniques.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions the authors suggest include:

- Applying the methods to human labelling rather than just synthetic oracle labelling. The paper focuses on using oracle labels for validation, but notes that evaluating with real human labels is an important next step. This could reveal new challenges around label uncertainty, sparsity, different preference formats, etc.

- Testing the methods on more complex environments. The paper proposes some new non-Markovian benchmark tasks, but suggests adapting other established RL environments like Atari games could provide further insights. Image observations in particular may pose new challenges.

- Iterative bootstrapping of reward modelling and RL agent training. Rather than separate offline training of the reward model then online use for RL, the authors propose interleaving these steps could improve performance. 

- Transfer of concepts and techniques between reward modelling and multiple instance learning. The paper identifies a link between these fields, and suggests further exchange of ideas could be mutually beneficial.

- Alternative model architectures to prevent negative reward predictions. For the Lunar Lander task, some models struggled due to predicting impossible negative rewards. The authors suggest several mechanisms to address this issue.

- Analysis of what causes the "better than oracle" phenomenon sometimes seen. In certain cases, agents trained using the learnt reward model outperformed those trained on the ground truth. Understanding this counterintuitive result could further improve performance.

So in summary, applying the methods to real human data, scaling up to more complex tasks, iterative training procedures, tighter integration with MIL, improving model training, and analysing cases where models improve on ground truth are noted as interesting directions for future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach for reward modelling in reinforcement learning that can handle non-Markovian rewards. Existing reward modelling methods assume the human evaluator assesses each step in a trajectory independently when giving feedback. This paper removes that assumption and allows for temporal dependencies in the human's assessment of trajectories. They formulate the problem as an instance of multiple instance learning, where trajectories are treated as bags of state-action pairs (instances) that collectively contribute to the overall trajectory label (bag label) in complex ways. They propose LSTM-based architectures to model these temporal dependencies and show they can accurately reconstruct rewards on non-Markovian RL tasks. The learnt models provide interpretable representations of the hidden dynamics and can successfully augment RL agent training. Overall, the paper generalizes reward modelling to handle settings where the human's preferences have temporal dependencies that make rewards dependent on historical context alongside visible state/action information.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper generalizes the problem of reward modeling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Existing RM work assumes that human evaluators observe each step in a trajectory independently when providing feedback on agent behavior. This paper removes that assumption, extending RM to include hidden state information that captures temporal dependencies in human assessment of trajectories. The authors identify a connection between RM and multiple instance learning (MIL). Trajectories are treated as “bags” of state-action pairs (“instances”), with labels provided at the bag level. This inspires novel LSTM-based MIL model architectures to recover hidden state dynamics and learn instance-level reward predictions from labeled trajectories. 

Experiments use synthetic oracle labels for five RL tasks. The proposed models accurately reconstruct hidden states and rewards for non-Markovian tasks. They are integrated into RL agent training, achieving performance matching or exceeding agents with direct access to true states and rewards. Analysis provides insight into the representations learned by the models. Overall, the work generalizes RM to handle temporal reward dependencies, leverages MIL concepts for this problem, and demonstrates the feasibility of learning and utilizing non-Markovian reward functions.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new approach for reward modelling (RM) in reinforcement learning that can handle non-Markovian rewards, where the reward for a state-action pair depends on historical context. It formulates RM as a multiple instance learning (MIL) problem, where trajectories are treated as bags with return labels, and state-action pairs within trajectories are instances with unseen reward labels. Long short-term memory (LSTM) architectures are used to model temporal dependencies between instances in a bag. Specifically, the paper develops concatenated skip connection instance space LSTM models that learn hidden state dynamics to capture historical dependencies, alongside reward predictions for individual state-action instances. These models are shown to effectively reconstruct rewards for non-Markovian tasks, and can be straightforwardly integrated into RL agent training by augmenting states with their hidden representations. The LSTM models outperform baseline MIL architectures lacking temporal modelling, and enable agents to solve tasks that are impossible with only the visible state.


## What problem or question is the paper addressing?

 The paper is addressing the problem of reward modelling in reinforcement learning when the reward is non-Markovian. Specifically, it is looking at the case where human evaluators provide trajectory-level feedback that depends on temporal structure and hidden state information. 

The key questions the paper tries to address are:

1) How can we extend reward modelling to handle non-Markovian rewards that depend on temporal dependencies and hidden states?

2) How can we leverage ideas from multiple instance learning to develop models that can learn from trajectory-level feedback and uncover the hidden dynamics? 

3) How can the learnt models be used to help train reinforcement learning agents on tasks with non-Markovian rewards?

4) How can we interpret what the models have learned about the hidden states and dynamics?

So in summary, it is generalizing reward modelling to handle more complex temporal dependencies, using concepts from multiple instance learning, with the end goal of enabling reinforcement learning in non-Markovian environments. The key innovation is in modeling trajectory-level rewards as "bags" with temporal instance dependencies.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Reward modeling/modelling (RM) - The process of inferring an agent's reward function from sources of preference information provided by humans, such as demonstrations, pairwise comparisons, corrections, etc. The goal is to learn the underlying reward function that explains demonstrated preferences.

- Non-Markovian rewards - Rewards that depend not just on the current state-action pair but also on some hidden state that captures historical dependencies. This generalizes the standard Markovian reward assumption.

- Hidden state - Extra state information that is not contained in the explicit state representation but affects rewards. It captures temporal dependencies and can represent things like human memory or biases.  

- Multiple instance learning (MIL) - A machine learning paradigm where training data is organized into bags of instances. Bags have labels but instances do not. The aim is to learn from bag labels.

- Long short-term memory (LSTM) - A type of recurrent neural network well-suited to modeling sequential data like trajectories, by maintaining an internal hidden state.

- Interpretability - The ability to explain and visually understand what a machine learning model has learned. For example, visualizing and probing the LSTM hidden states.

- Non-Markovian decision processes (NMDPs) - Decision processes where rewards have temporal dependencies and are non-Markovian. Solvable by augmenting the state with supplementary information.

- Reinforcement learning (RL) - Training agents by rewarding/penalizing actions and iteratively improving a policy. MIL-based reward models can be integrated into RL training.

In summary, the key focus is using MIL and LSTMs to perform more general non-Markovian reward modeling for RL agents, in order to better capture the temporal aspects of how humans evaluate trajectories.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are some potential questions to ask to create a comprehensive summary of the paper:

1. What is the key problem or research gap that the paper aims to address?

2. What are the main contributions or innovations presented in the paper? 

3. What is the theoretical background or related prior work that provides context for the paper? 

4. What methodology does the paper use to address the research problem? What datasets or experiments were conducted?

5. What were the main results or findings from the experiments/analysis conducted in the paper?

6. Did the results support or contradict the original hypotheses or expectations of the authors?

7. What are the limitations of the work presented in the paper?

8. How does this work advance the overall field or state-of-the-art? What are the broader impacts?

9. What directions for future work does the paper suggest?

10. What are the key takeaways or conclusions from the paper? What are the main insights?

11. How does the paper relate to other papers I have read on this topic? What comparisons can be made?

12. What parts of the paper were unclear or confusing? What questions do I still have?

By thoroughly answering questions like these that cover the key elements of the paper (motivation, methods, results, implications, etc.), it should be possible to develop a comprehensive summary and understanding of the work. Additional domain-specific questions may also be relevant depending on the field. The goal is to analyze the paper from different angles.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel approach to reward modelling that can capture temporal dependencies in how humans evaluate agent trajectories. How does framing the problem as multiple instance learning enable modelling of these dependencies, and what are the key advantages over prior Markovian reward modelling techniques?

2. The paper introduces concatenated skip connections in the instance space LSTM architecture. What is the motivation behind this modification compared to a standard LSTM, and how does it lead to more interpretable and disentangled hidden state representations? 

3. The CSC instance space LSTM model generally achieved the best performance on the RL tasks. What architectural properties make it well-suited to non-Markovian reward modelling, and how might it be further improved or adapted to different problem settings?

4. The hidden state analysis provides insight into what temporal dependencies each model architecture was able to capture. How do the hidden state visualizations for the CSC instance space LSTM compare to the other models, and what does this suggest about its representational capacity?

5. The trajectory probing analysis verifies that the CSC instance space LSTM recovers valid hidden state transitions. How diagnostic are these probing results in understanding what the model has learned, and how could the probing methodology be expanded or improved?

6. For the Lunar Lander task, the CSC instance space LSTM enabled better RL agent performance than using the oracle rewards directly. What explanations are proposed for this counterintuitive result, and what are the broader implications for using learnt reward models in RL training?

7. The paper argues that modelling temporal dependencies is important when reward modelling from human feedback. What factors could induce dependencies beyond those studied here, and what new challenges might they introduce?

8. How suitable are the proposed LSTM-based architectures for online reward learning alongside agent training, compared to batch learning from fixed datasets? What modifications would be needed to effectively interleave reward modelling and RL?

9. What impact did the addition of label noise have on model performance, and how well did the CSC instance space LSTM architecture cope compared to the baseline? How might the robustness be further improved?

10. The paper focuses on trajectory return labels for reward modelling. How readily could the methodology be applied to other types of human preference data, such as demonstrations or pairwise choices? What adaptations would need to be made?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper generalizes the problem of reward modeling (RM) in reinforcement learning to handle non-Markovian rewards, where rewards depend not just on the current state-action pair but also on historical information. They frame RM as a multiple instance learning (MIL) problem, with trajectories as bags and state-action pairs as instances. The authors propose novel LSTM-based MIL architectures to model temporal dependencies and recover hidden state dynamics and reward functions from trajectory return labels. Experiments on synthetic oracle-labeled datasets for five RL tasks show their models accurately reconstruct rewards and outperform baselines, with the concatenated skip connection model achieving the best results. The learned models are used to train RL agents, in some cases exceeding the performance of agents trained on true rewards and states. Interpretability analysis reveals the models capture meaningful temporal dependencies related to subtask structure and human psychology. This work removes unrealistic independence assumptions in RM and demonstrates MIL-inspired neural architectures that recover interpretable latent structure from non-Markovian trajectory preferences.


## Summarize the paper in one sentence.

 The paper proposes novel LSTM-based multiple instance learning models to perform non-Markovian reward modelling, allowing the reconstruction of rewards that depend on historical information.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach to reward modeling that relaxes the common assumption that human evaluators assign rewards to each state-action independently when judging agent behavior over temporally extended trajectories. Instead, the authors develop models that allow for temporal dependencies in human assessment of trajectories, capturing the idea that earlier events can influence the perception of later ones. They draw a connection between reward modeling and multiple instance learning, allowing them to leverage recurrent neural network architectures from MIL to handle temporal dependencies. The authors evaluate their proposed models on several reinforcement learning tasks with non-Markovian reward functions, demonstrating improved performance in reconstructing rewards and returns compared to baselines. They also show the models can be used to successfully train RL agents by providing reward signals and state augmentations. The work concludes by analyzing what the models have learned through visualizations and probing, lending some interpretability to the captured temporal dynamics. Overall, the paper presents a novel way of handling temporal effects in human preference modeling for RL agents.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methodology proposed in the paper:

1. The paper proposes using multiple instance learning (MIL) for non-Markovian reward modeling. How does framing trajectories as "bags" and steps as "instances" allow temporal dependencies to be captured? What modifications to standard MIL architectures were required?

2. The paper introduces a concatenated skip connection (CSC) to the instance space LSTM architecture. How does adding this connection aid in disentangling the hidden state representation and improving performance? What is the intuition behind using skip connections in this context?

3. The CSC instance space LSTM model generally outperformed the other proposed architectures. Why do you think it struggled more on the Dial toy task in terms of reward prediction, despite better return prediction? How could the model be improved?

4. The paper demonstrates using the trained MIL models to provide rewards and state augmentations during RL training. What are the potential benefits and downsides of using the learned models in this way rather than the true oracles?

5. For the Lunar Lander environment, the CSC instance space LSTM model enabled better performance than even direct access to the oracle rewards. What explanations are proposed for this counterintuitive result? How was the model's reward function analyzed and interpreted?

6. The paper identifies an issue during Lunar Lander training where a proportion of models become stuck in a local minimum, predicting high negative rewards. What architectural modifications are suggested to overcome this limitation? How would you evaluate the proposed solutions?

7. What mechanisms were used to generate diverse training datasets covering a wide distribution of outcomes for each task? How were trajectory classifiers defined and used during data collection?

8. The paper focuses on trajectory return labels as the training signal. How could the methodology be extended to incorporate other forms of human preference feedback like rankings or corrections? What challenges might arise?

9. For interpretability, the paper constrains the hidden state size to 2D and visualizes the embeddings. What are the limitations of this approach for analyzing more complex environments? How else could the hidden dynamics be interpreted?

10. The methodology relies on oracle reward functions for initial training and evaluation. What steps would need to be taken to deploy the approach with real human feedback? What open challenges remain in that area?
