# Non-Markovian Reward Modelling from Trajectory Labels via Interpretable   Multiple Instance Learning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can reward modelling for reinforcement learning be generalized to handle non-Markovian rewards that exhibit temporal dependencies?The standard assumption in reward modelling is that human evaluators assess each state-action pair independently when giving feedback on trajectories. This paper argues that in reality, human assessment of trajectories is temporally extended, so earlier events can influence the evaluation of later ones. The main contribution is developing new models based on multiple instance learning that can capture these temporal dependencies and reconstruct non-Markovian reward functions.The key hypotheses tested are:1) The proposed LSTM-based multiple instance learning models can accurately predict trajectory returns and timestep rewards for non-Markovian tasks with temporal dependencies.2) The learnt models provide interpretable hidden state representations that capture the temporal dependencies. 3) The predicted rewards and hidden states from the models can be used to train reinforcement learning agents to solve non-Markovian tasks.So in summary, the central research question is how to do reward modelling when rewards are non-Markovian, and the main hypothesis is that multiple instance learning provides a promising approach to handle the temporal dependencies.


## What is the main contribution of this paper?

This paper presents a method to handle non-Markovian rewards in reinforcement learning (RL) through reward modelling. The main contributions are:1. It generalizes reward modelling to allow for temporal dependencies in how humans evaluate agent trajectories, by modelling hidden state information that accumulates over time. 2. It identifies a connection between reward modelling and multiple instance learning (MIL), allowing MIL concepts and models to be applied to this problem.3. It proposes novel LSTM-based MIL architectures to model the hidden state dynamics and make reward predictions from trajectory return labels.4. It evaluates the proposed models on RL tasks with non-Markovian rewards, showing they can accurately reconstruct rewards and hidden states. The models can also effectively augment RL training.5. It provides interpretability analysis of the learnt models, visualizing and explaining the recovered hidden states and rewards.In summary, the key contribution is a general framework and set of techniques for handling temporal dependencies in reward modelling, enabling the training of RL agents for non-Markovian tasks. This is achieved by modelling hidden states with MIL and LSTM architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes new methods for reward modeling in reinforcement learning that can capture temporal dependencies in how humans evaluate and provide feedback on agent behavior over entire trajectories, rather than assuming each state-action pair is assessed independently.


## How does this paper compare to other research in the same field?

This paper presents a novel approach to generalizing reward modeling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Here are some key ways it compares to other related work:- Most prior work in RM assumes rewards are Markovian, i.e. depend only on the current state-action pair. This paper relaxes that assumption by allowing rewards to depend on historical trajectory information via a hidden state.- A few other papers have tried to capture temporal dependencies in RM, but in fairly restricted ways such as using a discrete psychological mode or monotonic bias term. This paper proposes a more general framework using LSTM models to represent complex hidden state dynamics.- The idea of connecting RM and multiple instance learning is novel. Viewing trajectories as bags and state-action pairs as instances enables transferring concepts like LSTM aggregation between the fields.  - The proposed LSTM architectures for non-Markovian RM are new. Prior LSTM MIL models don't separate hidden states and rewards, while this work designs models tailored for RM with disentangled components.- Evaluating the ability to reconstruct ground truth rewards, augment RL training, and interpret learned models on a diverse set of non-Markovian RL tasks is more comprehensive than most prior work.- The model analysis and concept of "trajectory probing" to verify hidden state transitions provides a level of interpretability lacking in most prior LSTM-based approaches.Overall, this paper makes multiple novel contributions to extending RM to sequential and potentially biased human feedback. The connections made to MIL and recurring themes of interpretability also help push the field forward. The range of experiments provides a solid proof of concept for the proposed techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest include:- Applying the methods to human labelling rather than just synthetic oracle labelling. The paper focuses on using oracle labels for validation, but notes that evaluating with real human labels is an important next step. This could reveal new challenges around label uncertainty, sparsity, different preference formats, etc.- Testing the methods on more complex environments. The paper proposes some new non-Markovian benchmark tasks, but suggests adapting other established RL environments like Atari games could provide further insights. Image observations in particular may pose new challenges.- Iterative bootstrapping of reward modelling and RL agent training. Rather than separate offline training of the reward model then online use for RL, the authors propose interleaving these steps could improve performance. - Transfer of concepts and techniques between reward modelling and multiple instance learning. The paper identifies a link between these fields, and suggests further exchange of ideas could be mutually beneficial.- Alternative model architectures to prevent negative reward predictions. For the Lunar Lander task, some models struggled due to predicting impossible negative rewards. The authors suggest several mechanisms to address this issue.- Analysis of what causes the "better than oracle" phenomenon sometimes seen. In certain cases, agents trained using the learnt reward model outperformed those trained on the ground truth. Understanding this counterintuitive result could further improve performance.So in summary, applying the methods to real human data, scaling up to more complex tasks, iterative training procedures, tighter integration with MIL, improving model training, and analysing cases where models improve on ground truth are noted as interesting directions for future work.
