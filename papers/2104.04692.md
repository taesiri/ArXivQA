# [Not All Attention Is All You Need](https://arxiv.org/abs/2104.04692)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design an effective dropout method for self-attention based pre-trained language models to alleviate overfitting and enable more robust task-specific tuning. The key points are:- Self-attention networks are prone to co-adaptation and overfitting due to the severe dependencies between attention elements. Existing dropout methods like vanilla dropout are not effective enough for self-attention models.- The authors propose a novel dropout method called AttendOut that uses self-attention to dynamically generate dropout patterns for each attention layer and sample in an end-to-end manner. - AttendOut consists of three modules: Defender, Attacker, and Generator. The Generator learns to generate dropout masks by being rewarded when the Attacker model outperforms the Defender model on a task.- Experiments on various NLP datasets show AttendOut consistently improves strong BERT and RoBERTa models across different tasks by alleviating overfitting. The learned dropout patterns are shown to be task-specific and dynamic.In summary, the paper introduces AttendOut, a novel and effective dropout technique tailored for self-attention models, to address the limitation of existing dropout methods and enable more robust fine-tuning. The core idea is leveraging self-attention itself to guide the dropout.
