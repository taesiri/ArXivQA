# [AttentionViz: A Global View of Transformer Attention](https://arxiv.org/abs/2305.03210)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we gain a deeper understanding of the self-attention mechanism in transformer models through interactive visualization techniques? More specifically, the authors propose a new visualization approach to help researchers explore and analyze the patterns learned by the query and key vectors used in transformer self-attention calculations. Their technique involves creating a joint embedding space for the queries and keys from multiple input sequences, then visualizing this space to reveal global attention trends across many examples. The main hypothesis behind this work seems to be that visualizing query and key vectors in this joint embedding will provide new insights into transformer self-attention that previous visualization methods, focused on single input sequences, could not offer. The authors implement their technique in an interactive tool called AttentionViz and apply it to study attention in major language models like BERT and GPT as well as vision transformers.Through case studies and expert interviews, they aim to demonstrate that their visualization approach and AttentionViz tool can:- Help researchers better understand how self-attention informs model behavior in general- Allow easy comparison and contrasting of attention heads - Facilitate the identification of attention anomalies or unexpected model behavior- Provide a flexible platform to study attention across various models, tasks, and modalitiesIn summary, the central research question is about gaining insights into transformer self-attention through a new multi-sequence visualization approach, embodied in the AttentionViz system. The utility of this technique is evaluated through usage scenarios and domain expert feedback.
