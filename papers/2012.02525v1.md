# [Practical No-box Adversarial Attacks against DNNs](https://arxiv.org/abs/2012.02525v1)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis that this paper addresses is whether effective adversarial attacks can be generated against deep neural networks without either white-box access (to model architecture, parameters, training data) or black-box access (ability to query the model). Specifically, the paper hypothesizes that effective "no-box" adversarial attacks can be generated using only a very limited amount of auxiliary data from the same domain as the victim model.

The key research questions investigated are:

1) Can effective adversarial attacks be generated using only a small dataset (on the order of tens of examples from a few classes)? 

2) What training mechanisms allow useful discriminative features to be learned from such limited data?

3) How can adversarial examples crafted on such limited-data models transfer well to real victim models trained on large datasets?

4) How does the performance of such no-box attacks compare to white-box and black-box attacks?

To summarize, this paper introduces and evaluates a new no-box threat model for generating adversarial attacks without access to the victim model or its training data. The central hypothesis is that effective attacks are possible using only a small auxiliary dataset.
