# Revisiting the Weaknesses of Reinforcement Learning for Neural Machine   Translation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Recent work has identified weaknesses of reinforcement learning (RL) for neural machine translation (NMT), finding that performance gains may not actually be due to the reward signal. The authors revisit and empirically evaluate these claims.- The authors hypothesize that improvements from RL for NMT cannot solely be explained by the "peakiness effect", i.e. the most likely tokens gaining more probability mass. They test this by manipulating the peakiness of models. - The paper investigates whether the upwards mobility of lower-ranked tokens is possible with RL, contrary to suspicions that RL may be unable to move such tokens to higher ranks.- The surprising finding that constant rewards yield similar gains to meaningful rewards is re-examined. The authors hypothesize that rewards and their scaling do matter, especially for domain adaptation.- The hypothesis that RL methods like policy gradients can mitigate exposure bias and the beam search curse in NMT is evaluated.In summary, the overarching research questions are whether previously identified weaknesses of RL for NMT hold up under further empirical investigation, and whether RL can be effective for adapting NMT models, especially in domain adaptation settings. The authors hypothesize some of the prior negative findings may not apply with proper training configurations.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It revisits some of the recent criticisms and alleged weaknesses of using reinforcement learning (RL) for neural machine translation (NMT). Specifically, it examines the claims by Choshen et al. (2020) that cast doubt on the effectiveness of policy gradient methods for NMT. 2. It provides new experimental results that counter some of the conclusions from Choshen et al. In particular, the authors show that:- Improvements in BLEU score with policy gradient cannot be solely explained by increased "peakiness" of the output distribution, as claimed by Choshen et al. By modifying the temperature parameter, the authors show you can change peakiness while still getting BLEU gains.- Simple methods like increasing the temperature can successfully move lower-ranked tokens into higher ranks, countering the claim that upwards mobility of tokens is not possible. - Reward functions and their scaling do matter for domain adaptation with policy gradients, unlike the surprising constant reward results from Choshen et al.3. The new experiments on in-domain and cross-domain adaptation highlight the importance of exploration and reward scaling for making policy gradients effective for NMT.4. The results help re-establish policy gradients as a viable approach for adapting NMT models, for example to new domains or user preferences, by reinforcing desired model outputs.In summary, the main contribution is providing empirical evidence to counter some recent doubts about using RL/policy gradients for NMT, and showing these methods can still be effective when configured properly. The new experiments provide a more holistic perspective on the alleged "weaknesses" from prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper revisits criticisms of reinforcement learning for neural machine translation, providing empirical evidence that improvements are not solely due to increased peakiness of output distributions, and that meaningful rewards and proper exploration do matter, especially for domain adaptation.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in reinforcement learning for neural machine translation:- It re-examines and provides counter-evidence to recent claims that policy gradient methods have limited effectiveness for NMT. Some recent papers like Choshen et al. (2020) suggested policy gradient only works due to increased peakiness of output distributions, not the reward signal. This paper shows with careful experimental design that rewards do matter.- It systematically studies the impact of different policy gradient algorithm variants, such as the use of baselines and modified rewards, on both in-domain and cross-domain adaptation scenarios. Much prior work focused only on in-domain adaptation. The domain adaptation experiments better reveal the importance of exploration and scaled rewards.- It evaluates the ability of policy gradient methods to mitigate exposure bias and the beam search curse in NMT. Prior work like Wang et al. (2020) showed this for minimum risk training, and this paper extends the finding to policy gradient.- It aims to isolate the impact of different factors like rewards, exploration, and variance reduction through controlled experiments. Many prior works studied policy gradient for NMT without this sort of analysis to understand why certain variants succeed.Overall, this paper provides a more careful empirical analysis of policy gradient for NMT than most prior work, while also showing its effectiveness in domain adaptation scenarios that are critically important for real-world deployment. The systematic experiments and evidence combat some recently raised doubts about policy gradient's efficacy for NMT.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Further exploring different reward functions and scaling methods in RL for NMT. The authors show the importance of rewards and scaling for domain adaptation, but suggest more work is needed on designing appropriate reward functions that capture translation quality or human preferences.- Applying RL to bridge the gap between training and inference for other sequence generation tasks beyond NMT, such as summarization, dialogue, etc. The authors show RL helps mitigate exposure bias in NMT.- Combining RL with other methods like data augmentation to further improve domain adaptation for NMT. The authors demonstrate RL improves out-of-domain adaptation but suggest combining it with other techniques.- Developing more stable and sample-efficient RL algorithms tailored to NMT. The authors note challenges with training stability and variance, suggesting further adaptations of RL algorithms to this problem setting.- Testing RL for NMT with human-in-the-loop experiments, since the potential benefits are highest when adapting to human preferences. The simulated rewards here could be replaced with human feedback.- Analyzing model behavior changes during RL, to better understand adaptation effects and peakiness. The authors suggest further probing of model internals.In summary, the main directions are enhancing reward design, applying RL more broadly for sequence generation, combining RL with other methods, adapting RL algorithms, conducting human-in-the-loop experiments, and analyzing model internals during RL fine-tuning. The authors lay out an agenda for better understanding and extending RL for NMT.
