# [FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level   Gradient Calibration](https://arxiv.org/abs/2307.16617)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question this paper tries to address is: 

How to effectively integrate multi-modality fusion and multi-task learning for 3D perception tasks in autonomous driving scenarios?

Specifically, the paper identifies two key issues that arise when naively combining multi-modality fusion and multi-task learning:

1) Modality bias: Different modalities (e.g. LiDAR, camera images) provide complementary information useful for different tasks. Without proper regularization, one task may favor or bias towards its best compatible modality, leaving other modalities under-optimized. 

2) Task conflict: When training on multiple tasks simultaneously, the gradient signals of different tasks can be imbalanced. A task with smaller gradients may get overwhelmed by another task with larger gradients during joint training.

To address these issues, the paper proposes a multi-level gradient calibration technique applied during end-to-end training:

1) Inter-gradient calibration to balance gradients between tasks and prevent task conflict. 

2) Intra-gradient calibration to ensure different modalities are learned at a balanced pace and prevent modality bias.

By applying multi-level gradient calibration, the paper aims to effectively integrate multi-modality fusion and multi-task learning within a unified framework for robust 3D perception in autonomous driving. Experiments on nuScenes dataset demonstrate clear improvements over baseline methods.

In summary, the core research contribution is a novel optimization technique based on multi-level gradient calibration to enable effective joint multi-modality multi-task learning for 3D perception tasks.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a novel framework called Fuller that unifies multi-modality fusion and multi-task learning for 3D perception in autonomous driving scenarios. The key idea is to use multi-level gradient calibration to address the issues of modality bias and task conflict that arise in naively combining these approaches. 

2. It introduces a lightweight transformer-based design for the 3D detection and map segmentation heads, which achieves strong performance while saving significant parameters compared to prior designs.

3. It provides extensive experiments and analysis on the nuScenes dataset to demonstrate the effectiveness of Fuller. The results show substantial improvements over strong baselines, e.g. a 14.4% mIoU boost on map segmentation and 1.4% mAP increase on 3D detection. 

4. The paper offers insights into the links between modalities and tasks, and how their gradients interact during end-to-end training. It is the first work to systematically study multi-modality multi-task learning for 3D perception in self-driving.

In summary, the main contribution lies in proposing the Fuller framework that can effectively unify multi-modality fusion and multi-task learning, two increasingly important paradigms in 3D perception for autonomous driving. The paper offers convincing experimental validation and provides useful analysis into the optimization challenges.
