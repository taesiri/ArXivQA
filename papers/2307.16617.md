# [FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level   Gradient Calibration](https://arxiv.org/abs/2307.16617)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question this paper tries to address is: 

How to effectively integrate multi-modality fusion and multi-task learning for 3D perception tasks in autonomous driving scenarios?

Specifically, the paper identifies two key issues that arise when naively combining multi-modality fusion and multi-task learning:

1) Modality bias: Different modalities (e.g. LiDAR, camera images) provide complementary information useful for different tasks. Without proper regularization, one task may favor or bias towards its best compatible modality, leaving other modalities under-optimized. 

2) Task conflict: When training on multiple tasks simultaneously, the gradient signals of different tasks can be imbalanced. A task with smaller gradients may get overwhelmed by another task with larger gradients during joint training.

To address these issues, the paper proposes a multi-level gradient calibration technique applied during end-to-end training:

1) Inter-gradient calibration to balance gradients between tasks and prevent task conflict. 

2) Intra-gradient calibration to ensure different modalities are learned at a balanced pace and prevent modality bias.

By applying multi-level gradient calibration, the paper aims to effectively integrate multi-modality fusion and multi-task learning within a unified framework for robust 3D perception in autonomous driving. Experiments on nuScenes dataset demonstrate clear improvements over baseline methods.

In summary, the core research contribution is a novel optimization technique based on multi-level gradient calibration to enable effective joint multi-modality multi-task learning for 3D perception tasks.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a novel framework called Fuller that unifies multi-modality fusion and multi-task learning for 3D perception in autonomous driving scenarios. The key idea is to use multi-level gradient calibration to address the issues of modality bias and task conflict that arise in naively combining these approaches. 

2. It introduces a lightweight transformer-based design for the 3D detection and map segmentation heads, which achieves strong performance while saving significant parameters compared to prior designs.

3. It provides extensive experiments and analysis on the nuScenes dataset to demonstrate the effectiveness of Fuller. The results show substantial improvements over strong baselines, e.g. a 14.4% mIoU boost on map segmentation and 1.4% mAP increase on 3D detection. 

4. The paper offers insights into the links between modalities and tasks, and how their gradients interact during end-to-end training. It is the first work to systematically study multi-modality multi-task learning for 3D perception in self-driving.

In summary, the main contribution lies in proposing the Fuller framework that can effectively unify multi-modality fusion and multi-task learning, two increasingly important paradigms in 3D perception for autonomous driving. The paper offers convincing experimental validation and provides useful analysis into the optimization challenges.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a method called Fuller that calibrates the gradients between tasks and modalities in a multi-task, multi-modal 3D perception model to address issues with task conflict and modality bias, improving performance on nuScenes benchmarks for 3D detection and map segmentation.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in the field of multi-modality multi-task learning for 3D perception:

- This paper proposes a novel framework called Fuller that unifies multi-modality fusion and multi-task learning for 3D perception tasks like 3D object detection and semantic segmentation. Other works have focused on either multi-modality fusion or multi-task learning, but not both together.

- The key contribution is using multi-level gradient calibration to address the issues of modality bias and task conflict that arise when naively combining multiple modalities and tasks. This is a new approach not explored by prior works. 

- Experiments are conducted on the large-scale nuScenes dataset using point cloud and images as input. Most prior multi-modality works focused only on 3D detection, while this paper tackles 3D detection and segmentation.

- Compared to BEVFusion, which also fused point cloud and images, this paper explicitly handles the problems of modality bias and task conflict that BEVFusion did not address. Fuller improves segmentation mIoU by 14.4% and detection mAP by 1.4% over BEVFusion.

- The lightweight transformer-based task heads used in Fuller improve accuracy while reducing parameters by ~40% compared to prior designs. This is a novel architecture contribution.

- Evaluations under different dataset distributions and loss weight settings demonstrate Fuller's effectiveness in various conditions. This rigorous evaluation goes beyond what most prior works have done.

Overall, this paper makes significant research contributions in multi-modality multi-task learning for 3D perception by proposing the new Fuller framework with multi-level gradient calibration and lightweight transformer task heads. The thorough experiments demonstrate clear improvements over existing methods.
