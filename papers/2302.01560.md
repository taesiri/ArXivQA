# Describe, Explain, Plan and Select: Interactive Planning with Large   Language Models Enables Open-World Multi-Task Agents

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we enable planning-based agents to successfully accomplish long-horizon, multi-step tasks in open-ended environments like Minecraft?Specifically, the authors identify two key challenges when using planning for tasks in open worlds:1) Long-term planning requires precise and multi-step reasoning, but planners often fail to produce completely error-free plans upfront for complex tasks.2) Planning efficiency can be compromised as vanilla planners do not consider the proximity/accessibility of parallel sub-goals when ordering them.To address these challenges, the paper proposes an interactive planning approach called "Describe, Explain, Plan and Select" (DEPS) that leverages large language models (LLMs). The main ideas are:- Use a "descriptor" module to get feedback on plan execution and summarize the current state when failures occur. - Treat the LLM as an "explainer" to identify bugs in previous plans based on the feedback.- Re-plan tasks with the LLM "planner" using the explanation to correct errors.- Use a learned "selector" module to pick the most efficient among parallel sub-goals based on proximity.The central hypothesis is that this interactive planning approach with LLMs and the learned selector will enable more robust planning and execution for complex, long-horizon tasks in open-ended environments. The experiments aim to test this hypothesis in the Minecraft domain.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an interactive planning approach called "Describe, Explain, Plan and Select" (DEPS) to enable multi-task agents to accomplish a diverse set of tasks in open-ended environments like Minecraft. The key ideas are:1) Using a large language model (LLM) as an interactive planner that can receive feedback from the agent, explain failures, and replan accordingly. This allows handling long and complex planning required in open worlds. 2) Introducing a "Selector" module that chooses the most efficient sub-goal to execute next from parallel goals based on predicting horizon/proximity. This improves planning efficiency.3) Comprehensive experiments in Minecraft with strong baselines show DEPS significantly improves performance and planning reliability. It nearly doubles the success rate across 70+ tasks compared to prior methods.4) DEPS marks the first planning-based agent that can accomplish the challenging ObtainDiamond task in Minecraft.In summary, the main contribution is an interactive planning approach that combines large language model, explainability, and proximity-aware goal selection to enable more reliable and efficient planning for multi-task agents in open-ended environments. The comprehensive validation in Minecraft demonstrates the advantages over existing methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an interactive planning approach called "Describe, Explain, Plan and Select" (DEPS) based on large language models to address challenges in planning for open-world environments like Minecraft; DEPS leverages agent feedback to iteratively correct plans and selects efficient sub-goals based on proximity, achieving state-of-the-art performance on a suite of 71 Minecraft tasks.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in interactive planning for embodied agents:- This paper tackles two key challenges in open-world planning: long-term multi-step reasoning and incorporating a sense of proximity/efficiency. Many prior works in embodied planning do not focus on these issues or long-horizon tasks. So this represents an advance over methods that do one-shot planning without feedback.- The interactive planning loop with "describe, explain, plan, and select" is novel compared to prior work. The additional components for explanation and selection based on proximity go beyond most existing interactive planning methods like saycan or inner monologue. This allows the agent to correct flawed plans and make them more efficient.- Evaluating on a large suite of 70+ Minecraft tasks is much more comprehensive than previous embodied planning works which tend to use simpler domains. The Minecraft environment poses a greater challenge.- The gains in performance over strong baselines like inner monologue and code as policies are significant, nearly doubling success rates. This supports the value of the proposed techniques.- Using an LLM as the interactive planner is an interesting choice compared to specialized planners in prior work. Leveraging the knowledge and reasoning of the LLM for replanning seems powerful, though it relies on a large private model.- The interactive planning process is intuitive and human-like with the dialogue format. But it may face issues scaling up compared to learned planning models. There is a tradeoff between flexibility and scalability.Overall, the work makes excellent progress on reliable planning for complex embodied tasks. The advances on long-term reasoning, efficiency, and leveraging LLMs help move the field forward in tackling more open-ended environments. The comprehensive experiments support the value of the techniques proposed.


## What future research directions do the authors suggest?

The paper suggests the following future research directions:- Expanding the interactive planning approach with LLMs to other open-world environments beyond Minecraft. The authors note that their planner operates on demonstration prompts with a fixed LLM, making it easy to adapt to other long-horizon open worlds.- Exploring end-to-end trainable goal-conditioned policies that can amortize the planning within the model itself. The authors acknowledge the planning bottleneck in their explicit planning system and suggest amortized planning could allow further scaling. - Using more accessible open-sourced LLMs like OPT and BLOOM instead of privately-held models like ChatGPT/GPT-3. The authors recognize the limited accessibility of their current models and aim to democratize the approach.- General methods to inject common sense and physics into LLMs to improve multi-step reasoning without needing environment feedback. The authors' interactive approach helps correct LLM errors over time, but injecting knowledge upfront could improve one-shot planning.- Studying how to dynamically expand the goal/skill space of agents online to handle new objectives discovered at test time. The predefined goal space limits generalization so enabling open-ended growth is important.- Improving sample efficiency and generalization of the learned goal-conditioned policies. The authors use imitation learning but RL or self-supervised approaches may help.- Validating the approach on real-world robotics tasks, not just simulated environments like Minecraft. Testing the transferability is key for real-world impact.In summary, the main directions are 1) expanding the approach to new domains 2) improving accessibility of models 3) injecting knowledge into LLMs 4) increasing goal flexibility 5) advancing low-level learning and 6) applying to robotics.
