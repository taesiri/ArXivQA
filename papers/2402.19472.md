# [Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid   Progress](https://arxiv.org/abs/2402.19472)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Standard benchmarks like ImageNet and CIFAR10 have been critical in driving progress in machine learning. However, repeated testing on these static benchmarks leads to overfitting as models exploit idiosyncrasies. This reduces their ability to measure real-world generalization.

Proposed Solution: 
- Introduce the concept of "Lifelong Benchmarks" - ever expanding pools of test samples that aim to enhance representativeness and resist overfitting.

- Specifically, construct two lifelong benchmarks: Lifelong-CIFAR10 (1.69M samples) and Lifelong-ImageNet (1.98M samples) by combining multiple datasets spanning distribution shifts.

- To tackle the high evaluation cost on such large benchmarks, develop a framework called "Sort & Search (S&S)" that selectively ranks test samples by difficulty and chooses a subset to evaluate new models on. This allows reuse of past model evaluations.

Key Contributions:
- Formalize lifelong benchmarks as a practical solution to benchmark overfitting. 

- Curate Lifelong-CIFAR10 and Lifelong-ImageNet benchmarks encompassing natural, synthetic, clean and corrupted samples across distribution shifts.

- Propose Sort & Search for efficient benchmark evaluation, reducing compute from 180 GPU days to 5 GPU hours, a ~1000x cost saving, with low error. Enables scalable lifelong benchmarking.

In summary, the paper makes a case for expanding benchmarks continually over time to avoid overfitting. The proposed lifelong benchmarking paradigm powered by the Sort & Search evaluation framework provides a practical solution to enable efficient and robust benchmarking at scale.
