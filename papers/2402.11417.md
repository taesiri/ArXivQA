# [LoRETTA: Low-Rank Economic Tensor-Train Adaptation for   Ultra-Low-Parameter Fine-Tuning of Large Language Models](https://arxiv.org/abs/2402.11417)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Fine-tuning large language models (LLMs) like BERT and LLaMA on downstream tasks requires updating all model parameters, which is computationally expensive. 
- Existing parameter-efficient fine-tuning (PEFT) methods like adapters and LoRA still have too many trainable parameters for large models. For example, LoRA needs to update over 16 million parameters for LLaMA-2-70B.
- There is a need for an ultra parameter-efficient method with minimal trainable parameters yet maintains performance.

Proposed Solution:
- The authors propose LoRETTA, a framework with two variants - LoRETTA_adp and LoRETTA_rep for efficient LLM fine-tuning.
- LoRETTA employs tensor train (TT) decomposition to compress large weight matrices into small TT factors. Only these factors are updated during fine-tuning.
- LoRETTA_adp uses lightweight tensorized adapters with TT layers inserted into self-attention blocks. 
- LoRETTA_rep reparameterizes weight matrices as the product of TT factors and updates these TT factors.

Main Contributions:
- LoRETTA achieves comparable or better performance than widely used PEFT methods on LLaMA-2 with up to 100x fewer trainable parameters. 
- LoRETTA_adp outperforms all existing PEFT methods given the same number of parameters.
- LoRETTA requires substantially lower memory for parameter storage and training. LoRETTA_rep uses less than 1MB storage.
- LoRETTA shows better multi-task learning ability and handles overfitting more effectively.
- Code is provided for easy adoption of LoRETTA. Experiments cover diverse tasks and models including BERT, RoBERTa, DeBERTa and LLaMA.

In summary, LoRETTA enables highly efficient LLM fine-tuning with ultra-low trainable parameters yet maintains performance across models and tasks. The tensor train formulation proves to be a compact and effective way to update large model weights.
