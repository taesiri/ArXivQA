# [Watermarking Makes Language Models Radioactive](https://arxiv.org/abs/2402.14904)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Language models are often fine-tuned on synthetic instruction datasets generated by other models. This raises questions around derivative works and detecting when model outputs have been used as training data.
- Existing methods like membership inference attacks have limitations in detecting model contamination across realistic scenarios with limited access to models or data.  

Proposed Solution: 
- The paper introduces the concept of making language models "radioactive" by watermarking model outputs before releasing them. 
- The watermarks are specially designed to leave detectable traces when used in fine-tuning other models, even with limited data/model access.

Key Contributions:
- Formalizes language model "radioactivity" and derives new detection tests for supervised/unsupervised and open/closed access scenarios.
- Shows watermarked text contamination can be detected with very high confidence (p-value < 10^-5) even when only 5% of fine-tuning data is watermarked. 
- Demonstrates the approach in a real-world setup fine-tuning on Self-Instruct data.
- Analyzes factors influencing contamination like fine-tuning process, watermarking methods, text distributions.

In summary, the key insight is that watermarking designed for synthetic text detection also makes models "radioactive", allowing model owners to reliably detect fine-tuning on their data across practical scenarios with limited access. The paper provides a rigorous analysis of this phenomenon.
