# [High-Fidelity Image Generation With Fewer Labels](https://arxiv.org/abs/1903.02271)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that recent advances in self-supervised and semi-supervised learning can be leveraged to significantly reduce the dependence of high-fidelity generative adversarial networks (GANs) on vast quantities of labeled data. Specifically, the paper investigates whether techniques from self-supervised learning (to extract semantic features from unlabeled data) and semi-supervised learning (to infer labels for unlabeled data) can allow GANs to match the sample quality of fully supervised models trained on ImageNet while using only a fraction of the full labeled dataset.The key questions explored are:- Can self-supervised pre-training provide useful semantic representations to guide GAN training without labels?- Can labels inferred via semi-supervised learning provide sufficient conditional information to GANs, despite being less accurate than true labels? - How do different degrees of label availability affect sample quality?The central hypothesis is that by combining these techniques, high fidelity GANs can be trained with significantly fewer labels, closing the gap with fully supervised models. The experiments aim to validate this hypothesis and quantify the reduction in label dependence achieved.
