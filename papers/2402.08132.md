# [On the Resurgence of Recurrent Models for Long Sequences -- Survey and   Research Opportunities in the Transformer Era](https://arxiv.org/abs/2402.08132)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Processing very long sequences of data remains a key challenge in machine learning. Transformer models have shown outstanding performance but their quadratic complexity in sequence length is a concern. This has renewed interest in recurrent models like RNNs.  

- However, most current methods still focus on offline training on finite-length sequences. The setting of online continual learning on infinite streams of data is still largely unexplored.

Proposed Solution:
- The paper surveys recent work on the "resurgence of recurrence" for long sequence modeling. This includes Transformer variants that incorporate recurrence to improve efficiency, as well as Deep State Space Models which offer robust sequence modeling.

- The paper highlights how these trends have led to new recurrent units like the Linear Recurrent Unit (LRU) that match state-of-the-art performance on benchmark datasets.

- As a case study, the paper empirically evaluates RNNs, LSTMs, convolutional models, and Transformers on a lifelong online learning task of time series forecasting.

Main Contributions:
- Provides an overview connecting recent advances in efficient Transformers, State Space Models and recurrent units. 

- Identifies online continual learning on infinite streams as a key open challenge for future recurrent models.

- Emphasizes how concepts from State Space Models can inform the design of new recurrent units like LRUs.

- Shares empirical evidence on the difficulty of online continual learning on streaming time series data.

In summary, the paper surveys the resurgence of recurrence for long sequence modeling and points to online continual learning on infinite streams as an important direction for developing more powerful and scalable sequential models.
