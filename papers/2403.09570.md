# [Multi-Fidelity Bayesian Optimization With Across-Task Transferable   Max-Value Entropy Search](https://arxiv.org/abs/2403.09570)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of Bayesian optimization when faced with a sequence of related expensive-to-evaluate black-box optimization tasks. For example, tuning neural network hyperparameters over a sequence of similar learning tasks. The goal is to find the optimal solution for each task while minimizing the overall evaluation cost across tasks. Existing methods select candidates to maximize information only about the current task's optimum. However, since tasks are related through some common underlying parameters, information useful for future tasks can potentially be extracted.

Proposed Solution: 
The paper proposes a novel acquisition function called Multi-Fidelity Transferable Max-Value Entropy Search (MFT-MES) that balances gathering information about the current task and collecting transferable information useful for future tasks. This is done by modeling shared inter-task latent variables that capture the relatedness across tasks. These variables are integrated into the acquisition function and transferred across tasks in a Bayesian way.

Specifically, the acquisition function contains two terms - one for information about the current task's optimum as in standard MF-MES, and another for the transferable information quantified by the mutual information between observations and shared parameters. The latent parameters are represented using particles that are updated via Stein Variational Gradient Descent.   

Main Contributions:
- A new MFT-MES acquisition function that accounts for transferable information useful for future tasks by modeling inter-task dependencies 
- An efficient Bayesian particle-based implementation to represent, update and transfer shared parameters across tasks
- Experiments on synthetic and real-world tasks demonstrating significant gains from transfer learning, with over 3x simple regret reduction compared to state-of-the-art

The key insight is that taking actions to extract transferable knowledge across the task sequence can greatly improve overall optimization performance despite possibly sacrificing some short-term performance on individual tasks.
