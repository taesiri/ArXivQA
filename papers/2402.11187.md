# [LaCo: Large Language Model Pruning via Layer Collapse](https://arxiv.org/abs/2402.11187)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are rapidly increasing in size, leading to high costs for training and inference. 
- Existing methods to reduce costs like quantization, distillation and pruning have drawbacks: impacts performance, requires extensive training, alters model structure.

Proposed Solution:
- The paper proposes a new layer-wise structured pruning method called "Layer Collapse" (LaCo). 
- Rear layers of the LLM collapse into a prior layer, aiming to preserve the model's output representation as closely as possible. This directly removes 30-50% of layers without retraining.
- The method is based on "Reserving-Differences-while-Seeking-Common" (RDSC) layer merge. This involves parameter differencing between layers and merging the differences to substitute multiple layers with one layer.

Main Contributions:
- LaCo outperforms state-of-the-art structured pruning methods by over 10% in maintaining average task performance at 25-30% pruning ratios.
- It preserves the model structure and intermediate dimensions, enabling quick adaptation.
- Post-training experiments show pruned models via LaCo rapidly converge, indicating effective inheritance of original model parameters.
- Analysis of layer similarity and evaluations across pruning ratios motivate and validate the approach.

In summary, the paper introduces LaCo, a straightforward yet powerful layer collapse method to effectively prune large language models while maintaining performance and model structure. Experiments and analysis demonstrate clear improvements over existing methods.
