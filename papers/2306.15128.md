# [MIMIC: Masked Image Modeling with Image Correspondences](https://arxiv.org/abs/2306.15128)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an effective method to curate large-scale multi-view image datasets for self-supervised pretraining of vision models, especially for dense prediction tasks like depth estimation and semantic segmentation?The key hypotheses appear to be:1) Multi-view image datasets with varying perspectives of the same scenes can enable models to learn useful representations of 3D structure. This could benefit dense prediction tasks that require reasoning about 3D geometry.2) Existing multi-view datasets rely on 3D annotations like meshes, point clouds, etc. which limit scalability. The authors propose a method to generate multi-view datasets without needing such 3D supervision. 3) Larger dataset scale leads to better self-supervised pretraining and downstream task performance. Their proposed data curation methodology can scale arbitrarily to generate even bigger datasets.4) Self-supervised pretraining on their proposed multi-view dataset outperforms existing datasets like ImageNet and MV-Habitat on multiple dense prediction tasks.So in summary, the main research question is how to develop a scalable multi-view dataset curation methodology for self-supervised pretraining, and the key hypotheses are around its advantages over existing datasets and pretraining methods.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors propose MIMIC, a novel data curation method to generate large-scale multi-view image datasets for self-supervised learning without requiring any 3D annotations like meshes or camera parameters. This allows scaling up multi-view datasets more easily.2. Using MIMIC, the authors curate two multi-view datasets - MIMIC-1M with 1.3M image pairs and MIMIC-3M with 3.1M image pairs, sourced from videos and synthetic 3D environments.3. The authors train masked image modeling methods like MAE and CroCo on MIMIC datasets and evaluate on downstream tasks. They show representations trained on MIMIC-3M outperform those trained on other datasets like ImageNet-1K and MV-Habitat on tasks like depth estimation, semantic segmentation, surface normals prediction, etc.4. The authors demonstrate larger pretraining datasets, longer pretraining and their curation method's ability to scale leads to performance gains on downstream tasks.5. The authors show MIMIC representations perform better on few-shot fine-tuning and on reconstruction quality compared to other datasets.In summary, the key contribution is proposing a scalable multi-view data curation approach MIMIC and using it to generate large datasets to train representations that outperform prior datasets on several dense prediction tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes MIMIC, a new method to generate large-scale multi-view image datasets from videos and 3D environments using traditional computer vision techniques like SIFT and homography, and shows these datasets can effectively train masked image modeling objectives like MAE and CroCo for self-supervised representation learning on dense prediction tasks.
