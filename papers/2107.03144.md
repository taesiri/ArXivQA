# [Neural Contextual Bandits without Regret](https://arxiv.org/abs/2107.03144)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to design contextual bandit algorithms based on neural networks that achieve sublinear regret bounds. Specifically, the paper proposes two new algorithms - NN-UCB (for fully-connected networks) and CNN-UCB (for convolutional networks) - and theoretically analyzes their regret bounds. The key contributions are proving that:- NN-UCB achieves a regret bound of Õ(T^{(2d-1)/2d}) for arbitrary context sequences, where d is the context dimension. This is the first sublinear regret bound proven for a neural network-based contextual bandit algorithm.- CNN-UCB achieves the same Õ(T^{(2d-1)/2d}) regret bound when the number of channels is sufficiently large. This is the first analysis showing convolutional bandits can have sublinear regret. The paper's main hypothesis is that neural network-based bandit algorithms can achieve sublinear regret guarantees by leveraging properties of the Neural Tangent Kernel and training networks via gradient descent. The theoretical analysis of NN-UCB and CNN-UCB supports this hypothesis and provides regret bounds that hold for general context sequences.In summary, the central question is how to design and analyze neural network contextual bandit algorithms to attain sublinear regret, and the paper hypothesizes this is possible by exploiting NTK properties and network training dynamics. The regret analysis for the proposed NN-UCB and CNN-UCB algorithms confirms this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is developing neural network-based contextual bandit algorithms with theoretical guarantees on their regret. Specifically:- The paper proposes two new algorithms: NN-UCB which uses a fully-connected neural network, and CNN-UCB which uses a convolutional neural network. - The key theoretical result is proving sublinear regret bounds for these algorithms, i.e. showing that their cumulative regret grows slower than linearly with the number of steps T. This implies the algorithms converge to the optimal policy.- The regret bounds hold for general context sequences, without any additional assumptions. Previous neural bandit algorithms typically had regret guarantees only under extra assumptions on the context.- To analyze the neural network algorithms, the paper first studies kernelized bandits with the Neural Tangent Kernel (NTK). Bounds on the NTK maximum information gain are leveraged to prove regret bounds for the neural methods.- The analysis covers both fully-connected and convolutional neural networks. Regret bounds for convolutional bandits were an open problem previously. In summary, the main contribution is developing the first neural contextual bandit algorithms with sublinear regret guarantees for general context sequences. This helps bridge the gap between theory and practice for deep contextual bandits.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR for the paper "Neural Contextual Bandits without Regret":The paper introduces novel neural network based algorithms for contextual bandits and proves sublinear regret bounds, establishing convergence to the optimal policy under general assumptions for both fully-connected and convolutional architectures.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other research in the field of contextual bandits and neural networks:- The main contribution of this paper is providing the first theoretical regret bounds for neural network-based contextual bandit algorithms like NN-UCB and CNN-UCB. Prior work had proposed similar algorithms but lacked theoretical guarantees on regret, unless making restrictive assumptions on the context distribution.- The paper builds on a large body of prior work in kernelized contextual bandits, especially GP-UCB style algorithms. It adapts the proof techniques used for kernelized bandits to analyze neural bandit algorithms, via their associated Neural Tangent Kernels.- A key theoretical contribution is bounding the information gain of Neural Tangent Kernels. This allows regret bounds for NTK-UCB that hold for any context distribution. Prior work on information gain focused on kernels with greater smoothness.- For neural networks, most prior analysis focused on simple linear models or fully-connected networks. This paper also provides the first analysis of CNNs for bandits. It links 2-layer CNNs to NTKs to derive regret bounds.- Compared to other neural bandit papers, this work makes weaker assumptions on the reward function, only requiring it lie in the RKHS of the NTK. It does not assume linearity or a particular parametric form.- The rates match or improve upon prior kernelized and neural bandit algorithms. But the analysis currently applies only for wide networks, a common limitation in NTK-based study of NNs.- Empirically, the paper only evaluates the algorithms on a simple MNIST task. More extensive empirical validation of the theory on larger problems would be useful future work.In summary, the paper significantly advances the theory of neural contextual bandits by providing the first regret bounds using NTK analysis. It also expands our understanding of CNNs in bandit settings. The techniques could potentially impact other kernelized sequential decision-making problems.


## What future research directions do the authors suggest?

Here are some key future research directions suggested in the paper:- Developing neural network bandit algorithms with regret guarantees that hold for deeper convolutional networks beyond the 2-layer case analyzed in the paper. The authors state that their analysis relies on the 2-layer convolutional network being invariant to cyclic shifts of the input, which does not hold for deeper CNNs. Extending the regret bounds to deeper CNN architectures is an open problem.- Improving the regret bounds to be fully non-asymptotic, i.e. removing the requirement for the network width or number of channels to tend to infinity. The paper analyzes the large width asymptotic behavior, but providing finite width guarantees is an important direction. - Investigating whether the $\mathcal{O}(T^{(2d-1)/2d})$ regret rate proven in the paper is information-theoretically optimal. The authors conjecture their bound may match an information-theoretic lower bound up to logarithmic factors, but proving this is left for future work.- Exploring connections between neural network training and Bayesian optimization more broadly, beyond the bandit setting. The links identified in this paper between NN training and GP inference could potentially be extended to other sequential decision making problems.- Analyzing the effects of the finite width on the approximation capabilities of the neural tangent kernel and information gain bounds. Taking the infinite width limit induces rotation invariance which could reduce expressiveness. Understanding this trade-off is an open question.- Developing more neural network bandit algorithms using the "kernelized analysis" approach introduced in the paper. The insights on information gain and regret could facilitate designing new NN-based methods with convergence guarantees.So in summary, key directions are: extending the analysis to deeper networks, removing asymptotic assumptions, proving lower bounds, broadening connections to Bayesian optimization, understanding trade-offs in finite width effects, and developing more algorithms based on the kernelization approach.
