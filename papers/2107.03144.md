# [Neural Contextual Bandits without Regret](https://arxiv.org/abs/2107.03144)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to design contextual bandit algorithms based on neural networks that achieve sublinear regret bounds. Specifically, the paper proposes two new algorithms - NN-UCB (for fully-connected networks) and CNN-UCB (for convolutional networks) - and theoretically analyzes their regret bounds. The key contributions are proving that:- NN-UCB achieves a regret bound of Õ(T^{(2d-1)/2d}) for arbitrary context sequences, where d is the context dimension. This is the first sublinear regret bound proven for a neural network-based contextual bandit algorithm.- CNN-UCB achieves the same Õ(T^{(2d-1)/2d}) regret bound when the number of channels is sufficiently large. This is the first analysis showing convolutional bandits can have sublinear regret. The paper's main hypothesis is that neural network-based bandit algorithms can achieve sublinear regret guarantees by leveraging properties of the Neural Tangent Kernel and training networks via gradient descent. The theoretical analysis of NN-UCB and CNN-UCB supports this hypothesis and provides regret bounds that hold for general context sequences.In summary, the central question is how to design and analyze neural network contextual bandit algorithms to attain sublinear regret, and the paper hypothesizes this is possible by exploiting NTK properties and network training dynamics. The regret analysis for the proposed NN-UCB and CNN-UCB algorithms confirms this hypothesis.
