# [CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot   Object Navigation](https://arxiv.org/abs/2203.10421)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How capable are open-vocabulary models like CLIP at executing embodied navigation tasks, even without any additional training? Specifically, can they perform well on language-driven zero-shot object navigation (L-ZSON)?

The authors investigate this by proposing a simple framework called CLIP on Wheels (CoW) that combines classical semantic mapping techniques with open-vocabulary models like CLIP to do zero-shot object navigation. The CoW framework uses CLIP or similar models to localize goal objects specified in natural language, without any navigation training.

To evaluate the capabilities of this approach on L-ZSON, the authors also introduce a new benchmark called Pasture that tests uncommon objects, appearance/spatial descriptions, and hidden objects. 

The main hypothesis seems to be that open-vocabulary models can effectively perform L-ZSON when incorporated into the CoW framework, even without any task-specific fine-tuning. The results generally validate this on uncommon objects, but also reveal limitations in fully leveraging language descriptions. Overall, it provides an empirical study of adapting these powerful models to embodied tasks.
