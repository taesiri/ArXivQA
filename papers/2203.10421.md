# [CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot   Object Navigation](https://arxiv.org/abs/2203.10421)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How capable are open-vocabulary models like CLIP at executing embodied navigation tasks, even without any additional training? Specifically, can they perform well on language-driven zero-shot object navigation (L-ZSON)?

The authors investigate this by proposing a simple framework called CLIP on Wheels (CoW) that combines classical semantic mapping techniques with open-vocabulary models like CLIP to do zero-shot object navigation. The CoW framework uses CLIP or similar models to localize goal objects specified in natural language, without any navigation training.

To evaluate the capabilities of this approach on L-ZSON, the authors also introduce a new benchmark called Pasture that tests uncommon objects, appearance/spatial descriptions, and hidden objects. 

The main hypothesis seems to be that open-vocabulary models can effectively perform L-ZSON when incorporated into the CoW framework, even without any task-specific fine-tuning. The results generally validate this on uncommon objects, but also reveal limitations in fully leveraging language descriptions. Overall, it provides an empirical study of adapting these powerful models to embodied tasks.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Introducing a new benchmark called Pasture for evaluating language-driven zero-shot object navigation (L-ZSON). Pasture focuses on three capabilities: finding uncommon objects, using appearance/spatial descriptions, and finding hidden objects. 

2. Proposing a simple baseline approach called CLIP on Wheels (CoW) for adapting existing open-vocabulary models like CLIP to the L-ZSON task without any fine-tuning. The CoW approach uses classical exploration techniques combined with off-the-shelf object localization models.

3. Conducting an empirical study evaluating 21 different CoW variants on the Pasture, Habitat, and RoboTHOR benchmarks. Key findings are that CoWs can find uncommon objects well but struggle to fully leverage language descriptions. A simple CoW matches state-of-the-art zero-shot navigation methods that require millions of training steps.

4. Providing analysis of CoW failure modes, showing exploration and object localization are key bottlenecks. This suggests future progress in these areas could further improve CoW.

In summary, the paper introduces a new benchmark, proposes a simple but strong baseline, and provides extensive empirical analysis to elucidate capabilities and limitations of existing models for the L-ZSON task. The CoW analysis and Pasture benchmark aim to motivate and guide future research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper introduces CLIP on Wheels, a collection of baselines leveraging open-vocabulary vision models for language-driven zero-shot object navigation, evaluates these baselines on a new Pasture benchmark, and shows they can match or exceed methods requiring millions of training steps, while being more broadly applicable.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in the field of language-driven zero-shot object navigation:

- This paper introduces a new benchmark called Pasture to better evaluate language-driven zero-shot object navigation. Most prior work evaluates on existing datasets like RoboTHOR or Habitat which have a limited set of common objects. Pasture provides a more comprehensive benchmark by including uncommon objects, spatial/appearance descriptions, and hidden objects. This allows testing a wider range of capabilities relevant to real-world settings.

- The paper proposes a simple but effective baseline approach called CLIP on Wheels (CoW) for this task. It adapts open-vocabulary models like CLIP for embodied navigation without any navigation training. Most prior work on zero-shot navigation uses models that require substantial training in the simulator. CoW matches or exceeds the performance of these methods without any navigation training.

- CoW decomposes the task into classical exploration when uncertain and goal-driven planning when the target is localized. This is similar to older ideas like semantic mapping but CoW uses modern perceptual modules. Other learning-based methods train end-to-end policies which can be less interpretable.

- The paper provides an extensive empirical study, evaluating over 90k episodes across multiple simulators. They ablate many design choices like exploration strategy, object localization model, backbones etc. This provides useful insights, for example simpler exploration works best, and more compute does not always help.

- The only limitation compared to some other work is CoW has not been evaluated in real physical environments. But the use of photorealistic simulators helps close the reality gap. Overall, the paper pushes forward language-driven embodied AI through comprehensive benchmarking and strong baselines.
