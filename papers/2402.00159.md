# [Dolma: an Open Corpus of Three Trillion Tokens for Language Model   Pretraining Research](https://arxiv.org/abs/2402.00159)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper introduces Dolma, a new open corpus of three trillion tokens for pretraining language models. The goal is to facilitate research into understanding how training data composition impacts model capabilities and limitations. 

The authors argue that lack of access to pretraining data has been a major obstacle for language model research. Most state-of-the-art models release little to no information about their training data. Even for models that are open, the training data is rarely released. This makes it very difficult to study the effects of different data compositions on model behavior.

To address this, the authors construct Dolma, which they release publicly along with all data processing tools. Dolma contains text extracted from diverse sources including web pages, scientific papers, code, books, social media posts and encyclopedic content. In total, it comprises over 3 trillion tokens spanning 5 billion documents from 7 data sources.

The authors use a high-performance framework they built to efficiently process and transform large amounts of raw text into a cleaned corpus suitable for pretraining. This involved language identification, quality filtering, content filtering, deduplication and mixing procedures tailored to each data source. Numerous ablation experiments are conducted to evaluate the impact of different configurations.

To validate Dolma and demonstrate its utility, the authors use it to pretrain Olmo-Tiny, a 1 billion parameter autoregressive decoder model. They show it achieves state-of-the-art results compared to similarly sized models on a suite of language tasks.

In conclusion, this paper addresses the lack of available pretraining data that hinders language model research. Dolma pushes the boundaries of scale among open corpora both in raw size and diversity of content. Combined with full documentation and open-sourced tools, this enables deeper investigation into the science of data curation for language model pretraining.


## Summarize the paper in one sentence.

 This paper introduces Dolma, a new open corpus of 3 trillion English tokens for language model pretraining, sourced from diverse domains like web pages, code, scientific papers, books, and social media.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) The release of the Dolma corpus, an open corpus of 3 trillion tokens for language model pretraining research. Dolma is one of the largest open corpora available and consists of diverse data from web pages, academic papers, books, code, social media, and encyclopedic sources.

2) The open sourcing of the Dolma toolkit, a high-performance data curation toolkit to efficiently process large datasets for language model pretraining. The toolkit enables researchers to reproduce the Dolma corpus and develop their own data pipelines.

3) Documentation of the design, construction, and content summary of the Dolma corpus. The paper provides details on Dolma's principles, data sources, processing pipelines, toxicity filtering, and other aspects of its creation. 

4) Analyses and experiments on intermediate states of Dolma during its construction, providing insights into data curation practices like deduplication, content filtering, and mixing of data sources.

5) The release of OLMo Tiny, a 1B parameter autoregressive language model trained on Dolma, which demonstrates its utility for pretraining competitive models.

So in summary, the main contributions are the release of the large-scale Dolma corpus, the Dolma toolkit, documentation of Dolma's creation, analyses of data curation techniques, and a pretrained model to showcase Dolma's effectiveness.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the main keywords and key terms associated with this paper include:

- Dolma corpus
- Language model pretraining
- Data curation
- Open corpus
- Data transparency
- Data filtering
- Toxicity filtering
- Common Crawl
- PeS2o
- The Stack 
- Gutenberg
- Wikipedia
- OLMo

The paper introduces and documents the Dolma corpus, a new open corpus of 3 trillion tokens designed to support language model pretraining research. It provides details on the corpus construction, filtering, and release to facilitate transparency and reproducibility in language model training data. Key aspects covered include acquiring and mixing data from diverse sources like Common Crawl, scientific papers, code, books, and Wikipedia; applying different quality and toxicity filters; and experiments on training language models like OLMo on variants of Dolma. So the main keywords revolve around language model pretraining data, curation processes, the specific Dolma corpus contents and sources, model training, and goals around openness and transparency.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper introduces the Dolma corpus for language model pretraining research. What were some of the key design goals and principles behind creating this corpus? How do they compare to goals behind other popular pretraining corpora?

2. The paper documents putting together a toolkit for efficient large-scale data curation. What are some of the main challenges in building infrastructure to process hundreds of terabytes of raw text content? What design choices were made in Dolma's toolkit to balance performance and portability?

3. The paper ablates the effects of different data filtering techniques like quality filtering, content filtering and deduplication. Can you summarize some of the key findings? How do you think findings might be different for other domains like multilingual data?  

4. The paper experiments with different mixing strategies and rates for Dolma's diverse data sources. What did they find about the relationship between amount of in-domain data and domain fit? What open questions remain about optimal mixing?

5. The paper shares details about toxicity filtering approaches used. What are some limitations of current methods? How might the landscape change as new techniques and benchmarks emerge?

6. The paper evaluates the effects of benchmark contamination and decontamination. What are some arguments for and against attempting to remove contamination? How might findings change with different definitions of contamination?

7. The Dolma corpus has been used to train the OLMo family of models. What are some examples of how open access to pretraining corpora can advance language model research directions? What barriers still exist?

8. The paper interleaves corpus construction details with ablation experiments using perplexity and downstream tasks. What are some alternatives or complements to these model-based analysis techniques that could provide additional insights?

9. The paper highlights legal and ethical considerations made during Dolma's construction. What are some areas where norms and best practices are still emerging or debated? How can we balance openness with responsible data practices?

10. The paper shares Dolma's data processing toolkit along with the corpus itself. What opportunities does this create for the research community and for language model developers? What extensions or improvements would you want to see in future toolkit iterations?
