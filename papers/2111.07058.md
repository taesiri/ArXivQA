# [Bolstering Stochastic Gradient Descent with Model Building](https://arxiv.org/abs/2111.07058)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:How can stochastic gradient descent (SGD) be improved by incorporating second-order information to adaptively adjust both step size and search direction? The key hypothesis is that an approach based on stochastic model building can achieve faster convergence and better generalization compared to standard SGD and adaptive SGD methods like Adam.Specifically, the paper proposes a new algorithm called Stochastic Model Building (SMB) that builds a local quadratic model using the latest stochastic gradient information to compute steps that incorporate curvature. This allows SMB to dynamically adapt step size and direction without needing to accumulate curvature information over iterations like quasi-Newton methods. The central claims are:- SMB will require less hyperparameter tuning compared to SGD and adaptive SGD methods.- SMB will show comparable or faster convergence compared to methods like SGD, Adam and stochastic line search.- SMB will generalize better due to the adaptive tuning of step size and direction based on local curvature.The paper provides theoretical convergence analysis for a variant of SMB and empirical evaluations on test problems and neural network models to demonstrate these claims.


## What is the main contribution of this paper?

Here are the key contributions of this paper:- It proposes a new adaptive optimization algorithm called Stochastic Model Building (SMB) for stochastic optimization problems. - SMB adjusts the step size and direction by building a quadratic model using the latest curvature information around the current iterate. This is in contrast to stochastic quasi-Newton methods that use information from previous steps. - The algorithm builds separate models for each group of parameters, making the step lengths adaptive. This is suitable for implementation with deep neural networks where parameters come in groups (layers).- It provides convergence rate analysis for a modified version of SMB called SMBi, where the curvature information is obtained from an independent random batch. This allows the analysis under standard assumptions.- Experiments on multi-class classification problems demonstrate that SMB requires less tuning and achieves faster convergence and better generalization compared to SGD, Adam, and a stochastic line search method.In summary, the key contribution is an adaptive stochastic optimization algorithm with convergence guarantees that outperforms baseline methods and requires less tuning. The adaptive modeling of local curvature information and the diagonalization for parameter groups make SMB particularly suitable and efficient for training deep neural networks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points of the paper:The paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB) that adjusts both step size and search direction by building quadratic models along the search path, analyzes the convergence properties of a variant called SMBi, and shows experimentally that SMB requires less tuning and achieves faster convergence compared to SGD and adaptive methods like Adam on several deep learning test problems.


## How does this paper compare to other research in the same field?

Here are some key points of comparison between this paper and other related research:- This paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB), which adaptively adjusts the step size and direction by building local quadratic models. This is a novel approach compared to other stochastic optimization methods like SGD, Adam, etc. - Most prior work on adaptive stochastic optimization has focused on adjusting the step size only, through techniques like stochastic line search. This paper adjusts both step size and direction, making it more adaptive.- The paper provides a theoretical convergence analysis for a variant of SMB where the curvature pairs are collected independently. This analysis relates SMB to stochastic quasi-Newton methods. Prior work has not analyzed model-building approaches in a stochastic setting.- The numerical experiments compare SMB against SGD, Adam and a recent stochastic line search method (SLS). The results show SMB achieves faster convergence on some problems like MNIST, especially in terms of wall clock time. On harder problems like CIFAR, SMB is competitive with the baselines.- The model building approach is designed to be compatible with existing deep learning frameworks, by building separate models for each parameter group (layer). This modularity is different from typical approaches.- Overall, SMB demonstrates a novel model-based approach for stochastic optimization that is theoretically motivated and shows promising empirical performance. The modular design is notable. Key limitations are the limited experiments and lack of analysis for the original non-modular SMB algorithm.In summary, this paper introduces a new model-based stochastic optimization technique with some attractive properties, but further work is needed to extensively benchmark it against alternatives and establish stronger theoretical guarantees. The approach seems promising and aligns well with recent interest in adaptive methods for deep learning.
