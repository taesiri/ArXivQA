# [Bolstering Stochastic Gradient Descent with Model Building](https://arxiv.org/abs/2111.07058)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:How can stochastic gradient descent (SGD) be improved by incorporating second-order information to adaptively adjust both step size and search direction? The key hypothesis is that an approach based on stochastic model building can achieve faster convergence and better generalization compared to standard SGD and adaptive SGD methods like Adam.Specifically, the paper proposes a new algorithm called Stochastic Model Building (SMB) that builds a local quadratic model using the latest stochastic gradient information to compute steps that incorporate curvature. This allows SMB to dynamically adapt step size and direction without needing to accumulate curvature information over iterations like quasi-Newton methods. The central claims are:- SMB will require less hyperparameter tuning compared to SGD and adaptive SGD methods.- SMB will show comparable or faster convergence compared to methods like SGD, Adam and stochastic line search.- SMB will generalize better due to the adaptive tuning of step size and direction based on local curvature.The paper provides theoretical convergence analysis for a variant of SMB and empirical evaluations on test problems and neural network models to demonstrate these claims.


## What is the main contribution of this paper?

Here are the key contributions of this paper:- It proposes a new adaptive optimization algorithm called Stochastic Model Building (SMB) for stochastic optimization problems. - SMB adjusts the step size and direction by building a quadratic model using the latest curvature information around the current iterate. This is in contrast to stochastic quasi-Newton methods that use information from previous steps. - The algorithm builds separate models for each group of parameters, making the step lengths adaptive. This is suitable for implementation with deep neural networks where parameters come in groups (layers).- It provides convergence rate analysis for a modified version of SMB called SMBi, where the curvature information is obtained from an independent random batch. This allows the analysis under standard assumptions.- Experiments on multi-class classification problems demonstrate that SMB requires less tuning and achieves faster convergence and better generalization compared to SGD, Adam, and a stochastic line search method.In summary, the key contribution is an adaptive stochastic optimization algorithm with convergence guarantees that outperforms baseline methods and requires less tuning. The adaptive modeling of local curvature information and the diagonalization for parameter groups make SMB particularly suitable and efficient for training deep neural networks.
