# [Bolstering Stochastic Gradient Descent with Model Building](https://arxiv.org/abs/2111.07058)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:How can stochastic gradient descent (SGD) be improved by incorporating second-order information to adaptively adjust both step size and search direction? The key hypothesis is that an approach based on stochastic model building can achieve faster convergence and better generalization compared to standard SGD and adaptive SGD methods like Adam.Specifically, the paper proposes a new algorithm called Stochastic Model Building (SMB) that builds a local quadratic model using the latest stochastic gradient information to compute steps that incorporate curvature. This allows SMB to dynamically adapt step size and direction without needing to accumulate curvature information over iterations like quasi-Newton methods. The central claims are:- SMB will require less hyperparameter tuning compared to SGD and adaptive SGD methods.- SMB will show comparable or faster convergence compared to methods like SGD, Adam and stochastic line search.- SMB will generalize better due to the adaptive tuning of step size and direction based on local curvature.The paper provides theoretical convergence analysis for a variant of SMB and empirical evaluations on test problems and neural network models to demonstrate these claims.


## What is the main contribution of this paper?

Here are the key contributions of this paper:- It proposes a new adaptive optimization algorithm called Stochastic Model Building (SMB) for stochastic optimization problems. - SMB adjusts the step size and direction by building a quadratic model using the latest curvature information around the current iterate. This is in contrast to stochastic quasi-Newton methods that use information from previous steps. - The algorithm builds separate models for each group of parameters, making the step lengths adaptive. This is suitable for implementation with deep neural networks where parameters come in groups (layers).- It provides convergence rate analysis for a modified version of SMB called SMBi, where the curvature information is obtained from an independent random batch. This allows the analysis under standard assumptions.- Experiments on multi-class classification problems demonstrate that SMB requires less tuning and achieves faster convergence and better generalization compared to SGD, Adam, and a stochastic line search method.In summary, the key contribution is an adaptive stochastic optimization algorithm with convergence guarantees that outperforms baseline methods and requires less tuning. The adaptive modeling of local curvature information and the diagonalization for parameter groups make SMB particularly suitable and efficient for training deep neural networks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points of the paper:The paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB) that adjusts both step size and search direction by building quadratic models along the search path, analyzes the convergence properties of a variant called SMBi, and shows experimentally that SMB requires less tuning and achieves faster convergence compared to SGD and adaptive methods like Adam on several deep learning test problems.


## How does this paper compare to other research in the same field?

Here are some key points of comparison between this paper and other related research:- This paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB), which adaptively adjusts the step size and direction by building local quadratic models. This is a novel approach compared to other stochastic optimization methods like SGD, Adam, etc. - Most prior work on adaptive stochastic optimization has focused on adjusting the step size only, through techniques like stochastic line search. This paper adjusts both step size and direction, making it more adaptive.- The paper provides a theoretical convergence analysis for a variant of SMB where the curvature pairs are collected independently. This analysis relates SMB to stochastic quasi-Newton methods. Prior work has not analyzed model-building approaches in a stochastic setting.- The numerical experiments compare SMB against SGD, Adam and a recent stochastic line search method (SLS). The results show SMB achieves faster convergence on some problems like MNIST, especially in terms of wall clock time. On harder problems like CIFAR, SMB is competitive with the baselines.- The model building approach is designed to be compatible with existing deep learning frameworks, by building separate models for each parameter group (layer). This modularity is different from typical approaches.- Overall, SMB demonstrates a novel model-based approach for stochastic optimization that is theoretically motivated and shows promising empirical performance. The modular design is notable. Key limitations are the limited experiments and lack of analysis for the original non-modular SMB algorithm.In summary, this paper introduces a new model-based stochastic optimization technique with some attractive properties, but further work is needed to extensively benchmark it against alternatives and establish stronger theoretical guarantees. The approach seems promising and aligns well with recent interest in adaptive methods for deep learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing convergence analysis for the original SMB algorithm where the scaling matrix Hk is dependent on the gradient gk. The convergence analysis in the paper is for the modified SMBi algorithm where Hk is constructed with an independent batch. Analyzing the convergence of the original SMB method is noted as an open problem.- Further exploring the robustness of SMB empirically and theoretically. The preliminary experiments in the paper suggest SMB is robust to the choice of learning rate in deep neural networks. More investigation into this property is needed. - Incorporating an automatic stepsize scheduling routine into SMB. The authors note SMB could benefit from a learning rate adjustment scheme like other adaptive methods. - Additional numerical testing of SMB on a wider range of problem types and neural network architectures. The experiments in the paper are limited to image classification tasks with MLP, ResNet and DenseNet models. Expanding the empirical evaluation could further demonstrate the capabilities of SMB.- Analyzing the computational and memory costs of SMB more thoroughly. The model building steps require additional gradient computations which need more analysis, especially for very large neural networks.- Developing second-order versions of SMB using approximations to the Hessian matrix rather than only gradient information. This could improve the convergence speed and sample complexity.- Comparing SMB to other related stochastic optimization methods besides SGD, Adam and SLS. Testing against methods like AdaGrad, RMSProp, etc. would help better situate SMB.In summary, the main directions are: further theoretical analysis, expanded numerical testing, modifications like second-order and stepsize scheduling, and additional comparisons against other stochastic optimization algorithms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB) as an alternative to stochastic gradient descent. The key idea is to incorporate second-order information to adjust not just the step size but also the search direction. Unlike stochastic quasi-Newton methods that accumulate curvature information over iterations, SMB uses the latest curvature at each iteration to build a quadratic model and take a model step. The algorithm builds a separate model for each group of parameters, making the step sizes adaptive. Convergence analysis is provided for a modified version SMBi where the curvature information is estimated independently from the gradient. Experiments on neural network models show SMB requires less tuning and achieves faster convergence compared to SGD, Adam, and a recent stochastic line search method. A key advantage is robustness - SMB maintains good performance across different step size choices.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB) as an alternative to stochastic gradient descent. The key idea is to use a model building strategy to adjust both the step size and direction of the stochastic gradient steps. Specifically, after taking a trial stochastic gradient step, if sufficient decrease in the objective is not achieved, a quadratic model is built using the latest stochastic gradient information. The model's minimizer provides the next step, adapting its length and direction. Theoretical convergence analysis is provided for a variant of SMB where the model information is constructed independently of the gradient step. Experiments on neural network training demonstrate that SMB requires less tuning and achieves faster convergence compared to SGD, Adam, and a recent stochastic line search method. The adaptive steps of SMB are shown to be more stable and less sensitive to the step size choice. Overall, SMB incorporates second-order curvature information while retaining the low per-iteration cost of first-order stochastic methods.
