# [Large Language Models Are Unconscious of Unreasonability in Math   Problems](https://arxiv.org/abs/2403.19346)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 show impressive capabilities in solving math problems. However, they tend to produce nonsensical or hallucinated responses when given mathematically unreasonable questions that contain errors.  
- This poses risks if LLMs are used for applications like tutoring systems, where producing inaccurate responses could negatively impact students.
- Prior work has not studied how LLMs behave when confronted with inherently unreasonable math problems.

Proposed Solution:
- The authors construct a benchmark of unreasonable math problems called UMP to systematically test LLMs' ability to detect errors. UMP contains reasonable problems from GSM8K along with altered unreasonable variants in categories like incorrect assumptions and inconsistent conditions.
- Experiments show models fail to consistently identify unreasonableness when directly prompted, but have inherent capabilities.
- The authors design a prompt template called "Critical Calculation and Conclusion" (CCC) that guides models to scrutinize problems first, then solve reasonable ones directly while correcting and re-evaluating unreasonable ones.  

Key Contributions:
- Creation of UMP benchmark to evaluate math problem unreasonableness detection.
- Demonstration that models struggle with unreasonable problems but have latent detection abilities.
- Introduction of CCC prompting strategy to stimulate models' critical thinking and enhance ability to identify and amend errors.
- Results show CCC prompts help models discern and handle unreasonable problems better across model sizes.
- Provides techniques to improve reliability of math models for applications like tutoring.

In summary, the key insight is that LLMs have underlying skills for detecting unreasonable math problems but need explicit prompting to leverage that capability. The CCC approach better activates these skills to make model math problem solving more robust.
