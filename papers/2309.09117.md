# [Contrastive Decoding Improves Reasoning in Large Language Models](https://arxiv.org/abs/2309.09117)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is whether Contrastive Decoding, a text generation method originally proposed for improving long-form text generation, can also improve the performance of large language models on reasoning tasks. The key hypothesis seems to be that Contrastive Decoding, which searches for strings that maximize the difference in likelihood between a stronger "expert" model and a weaker "amateur" model, can prevent reasoning errors and improve logical reasoning in large LMs. Specifically, the authors hypothesize that Contrastive Decoding will outperform greedy decoding for solving reasoning problems.The paper tests this hypothesis across a variety of reasoning benchmarks, including math word problems, commonsense reasoning, and multiple choice question answering. The results generally support the hypothesis, showing improved performance over greedy decoding and other baseline methods on tasks like arithmetic reasoning and multiple choice ranking.In summary, the central research question is whether Contrastive Decoding can enhance reasoning abilities in large LMs, and the key hypothesis is that it will outperform greedy decoding on logical reasoning tasks. The paper presents experiments across different models and datasets to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is showing that Contrastive Decoding, a simple text generation method, can improve the reasoning abilities of large language models. Specifically:- Contrastive Decoding improves performance on a variety of reasoning benchmarks, including math word problems (GSM8K) and commonsense reasoning (HellaSwag). It leads to state-of-the-art results on some tasks.- The gains occur across different model sizes, from 7B to 65B parameters, suggesting the method could benefit even larger models.- The authors analyze why Contrastive Decoding improves results. It seems to reduce surface-level copying from the input and prevent some reasoning errors. - Contrastive Decoding is efficient, providing gains with minimal computational overhead compared to other reasoning enhancement techniques.- The method works well for both open-ended generation (using sampling) and selecting answers for multiple choice questions. This makes it a flexible approach applicable to diverse tasks.Overall, Contrastive Decoding provides a simple but surprisingly effective way to improve reasoning in large language models, without requiring fine-tuning. The results suggest it could be a generally useful technique across models and tasks.
