# [Quick Back-Translation for Unsupervised Machine Translation](https://arxiv.org/abs/2312.00912)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Quick Back-Translation for Unsupervised Machine Translation":

Problem:
- Unsupervised machine translation (UMT) relies on back-translation (BT) to generate synthetic parallel data from monolingual corpora. However, BT with autoregressive models like Transformer is slow due to sequential decoding. 
- UMT models also suffer from lack of diversity in the synthetic parallel data generated during BT.

Proposed Solution:
- Propose Quick Back-Translation (QBT) framework that improves both speed and data diversity of BT for UMT.
- QBT utilizes the Transformer encoder as a non-autoregressive translation model to generate sequences in parallel during BT. This is called Encoder Back-Translation (EBT).
- EBT sequences are then used to train the Transformer decoder in an Encoder Back-Translated Distillation (EBTD) step. 
- EBT provides speedup and EBTD increases diversity of training data.
- QBT combines EBT, EBTD along with standard BT in an iterative framework.

Main Contributions:
- Novel usage of Transformer encoder as a fast non-autoregressive translation model for generating diverse synthetic parallel data.
- EBT provides constant time parallel sequence generation during BT.
- EBTD leverages encoder outputs for augmented training signal to the decoder.
- QBT combines EBT, EBTD and BT to improve efficiency and performance of UMT.
- Experiments show QBT improves state-of-the-art UMT models with better speed and data efficiency. On long sequences, QBT outperforms BT baselines.

In summary, the paper proposes an efficient BT framework QBT that repurposes the Transformer encoder for fast diverse synthetic data generation to enhance UMT learning.
