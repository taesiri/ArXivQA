# [Vision-Language Model-based Physical Reasoning for Robot Liquid   Perception](https://arxiv.org/abs/2404.06904)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Grounding large language models (LLMs) in the physical world remains challenging as they can only process textual input. 
- Large vision-language models (LVLMs) allow incorporating visual input but their ability to reason about multimodal physical interaction feedback for robotic tasks is unexplored.
- Liquid perception is difficult due to variable visual attributes and requires reasoning over physical interactions.

Proposed Solution:
- Propose a paradigm to leverage GPT-4V, a state-of-the-art LVLM, to enable robots to perceive and recognize liquids by reasoning over image-based haptic feedback.
- Design an action space of "Look", "Shake", and "Finish" for the robot to actively gather visual and haptic feedback about liquid objects. 
- Convert haptic signals to visual plots and feed to GPT-4V along with visual scene images to exploit its physical understanding to estimate liquid viscosity.
- Select liquid with properties most consistent with GPT-4V's predictions.

Contributions:
- Demonstrate GPT-4V can interpret image-based haptic feedback using physical understanding to estimate liquid viscosity.
- Show combining visual and haptic reasoning enables recognizing 10 household liquids with 86% accuracy, improving over 69% for vision-only method.  
- Establish a new paradigm for grounding LVLMs in the physical world by reasoning over multimodal interaction feedback.

In summary, the paper explores using large vision-language models for robotic liquid perception by actively gathering and reasoning over visual and haptic feedback. A key contribution is demonstrating how image-based interaction signals can be exploited by LVLMs for physical understanding without any training.
