# [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is whether a supervised contrastive loss function can consistently outperform the traditional cross-entropy loss for large-scale image classification tasks. The key hypotheses are:1) A contrastive loss function adapted for supervised learning by using label information to determine positives (same class) and negatives (different classes) can improve upon self-supervised contrastive losses and cross-entropy.2) Using multiple positives and negatives per anchor, rather than just one of each as in triplet loss, will lead to better performance without needing hard negative mining. 3) The proposed supervised contrastive (SupCon) loss will achieve state-of-the-art accuracy on standard image classification benchmarks compared to cross-entropy.4) The SupCon loss representations will demonstrate increased robustness to image corruptions compared to cross-entropy.5) The SupCon loss will be more stable to hyperparameters like optimizers, learning rates, and data augmentations than cross-entropy.In summary, the central research question is whether a supervised adaptation of contrastive learning can consistently beat cross-entropy on large-scale image classification across multiple criteria like accuracy, robustness, and hyperparameter sensitivity. The experiments and results aim to validate the hypotheses that a properly formulated SupCon loss can achieve this.
