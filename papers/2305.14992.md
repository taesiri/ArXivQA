# Reasoning with Language Model is Planning with World Model

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper aims to address is: Can integrating planning techniques by leveraging language models as world models enhance the reasoning abilities of large language models? The key hypothesis is that explicitly modeling world states and rewards during the reasoning process will allow language models to better simulate and anticipate outcomes of potential reasoning paths. This will enable more deliberate, strategic reasoning compared to autoregressive generation of reasoning chains.To test this hypothesis, the paper proposes the RAP framework which repurposes a language model as both a world model to predict states and rewards, and as a reasoning agent. By applying Monte Carlo tree search guided by the world model, the agent can explore the reasoning space more efficiently.The paper evaluates RAP on a diverse set of reasoning tasks including plan generation, math word problems, and logical inference. The results demonstrate substantial improvements over strong baselines like chain-of-thought prompting, providing evidence supporting the hypothesis that planning with world models can unlock more advanced reasoning abilities in language models.In summary, the central research question is whether integrating planning techniques and world models can boost language models' reasoning capabilities, which the paper examines through the proposed RAP framework and experiments across multiple reasoning domains.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a new framework called Reasoning via Planning (RAP) that enables large language models (LLMs) to reason in a more strategic, human-like manner. Specifically, the key ideas and contributions are:- Repurposing the LLM as both a world model to simulate states and provide rewards, and as a reasoning agent. This allows the LLM to anticipate action outcomes and get feedback.- Incorporating Monte Carlo Tree Search, a principled planning algorithm, to explore the vast reasoning space and balance exploitation vs exploration. This enables deliberate planning to refine and improve the reasoning over multiple iterations. - Formulating reasoning tasks as Markov decision processes with flexible definitions of states, actions, and rewards catered to different reasoning contexts. This makes RAP a general framework applicable to diverse reasoning problems.- Demonstrating substantial improvements of RAP over chain-of-thought and other prompting baselines on a range of challenging tasks including plan generation, math reasoning, and logical inference.In summary, the key contribution is proposing RAP as an innovative reasoning framework that combines planning algorithms with LLMs to achieve more strategic reasoning. RAP brings LLMs closer to human-level planning and reasoning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Unfortunately, I do not have enough context to provide a meaningful TL;DR or one-sentence summary of this paper draft, since it seems to be incomplete. The paper introduces a new framework called "Reasoning via Planning (RAP)" that aims to improve reasoning capabilities of large language models, but many key details are missing, such as the problem formulation, specific techniques used, experiments, results, and conclusions. From the abstract and introduction, it seems RAP incorporates planning algorithms like Monte Carlo Tree Search to help language models better explore and evaluate different reasoning paths. But without seeing the full technical exposition and evaluation, I cannot confidently summarize the core contribution or effectiveness of this proposed approach. Perhaps you could provide more context about the paper's goals and contents? That would help me understand it at a high-level and highlight the key takeaways.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in the same field:- This paper proposes a new framework called Reasoning via Planning (RAP) that enables large language models (LLMs) to reason in a more deliberate, human-like manner by integrating planning techniques. In contrast, most prior work has focused on autoregressive or chain-of-thought style reasoning with LLMs. The integration of planning and explicit state/reward modeling makes RAP unique.- A core contribution is repurposing the LLM as both the world model and the reasoning agent within a planning framework. This differs from prior work where world models are learned separately or planning is done externally. The authors show the flexibility of using the LLM for both roles across diverse reasoning tasks.- The paper demonstrates state-of-the-art results on challenging reasoning tasks like Blocksworld planning, math reasoning on GSM8k, and logical inference on PrOntoQA. The consistent improvements across very different tasks highlight the generality of RAP.- Compared to concurrent work on search-guided reasoning like guided beam search or tree-of-thoughts, RAP offers a more comprehensive solution with explicit state modeling, look ahead via the world model, and efficient exploration via advanced Monte Carlo tree search.- RAP moves beyond just reasoning toward integrating deeper planning and search capabilities in LLMs. The results provide evidence these techniques can overcome limitations of current LLMs and push towards more human-like strategic reasoning.In summary, the paper makes significant contributions through the novel RAP framework, its flexible repurposing of LLMs as world models, strong empirical results across diverse reasoning tasks, and integration of planning with state-of-the-art LLMs. It pushes forward the state of research on reasoning and planning with large language models.
