# [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge   Transfer](https://arxiv.org/abs/2107.02681)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that visually-grounded multi-modal knowledge from videos can improve language understanding, beyond what is possible with text-only pretraining. Specifically, the authors propose a cross-modal knowledge distillation method called VidLanKD to transfer knowledge from a video-text pretrained teacher model to a student text-only language model. The key ideas are:1) Videos provide richer grounded world knowledge compared to images, especially action-based and physical commonsense knowledge.2) A large-scale video dataset like HowTo100M has more diverse vocabulary and examples compared to typical image-text datasets. 3) Using different knowledge distillation objectives like NST and CRD can avoid the approximation error of assigning finite image/video "vokens" to tokens.4) Transferring knowledge from a multi-modal teacher to a text-only student maintains the scalability of standard language model pretraining.The central hypothesis is that by distilling knowledge from a teacher model pretrained on diverse video-text data, the student language model can gain improved language understanding abilities, even without seeing any visual input. The authors evaluate this through extensive experiments on language understanding benchmarks like GLUE, SQuAD, SWAG etc.In summary, the key hypothesis is that grounding language learning on large-scale, diverse video data can provide benefits for general natural language understanding, by transferring visual knowledge to text-only models. The proposed VidLanKD method aims to effectively distill this visually-grounded knowledge.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing VidLanKD, a novel video-language knowledge distillation method to improve language understanding. The key idea is to first pretrain a teacher model on a large-scale video-text dataset, then transfer its knowledge to a student language model on a text dataset via new distillation objectives.2. Using a large-scale video dataset (HowTo100M) which provides diverse vocabulary and rich world knowledge compared to image datasets commonly used in prior works. The video grounding helps the model learn better physical and temporal reasoning.3. Proposing to use Neuron Selectivity Transfer (NST) and Contrastive Representation Distillation (CRD) objectives for knowledge transfer, which avoid the approximation error of using a fixed set of image/video labels (vokens) as in prior work.4. Empirical improvements over strong baselines like BERT and vokenization on several language understanding tasks including GLUE, SQuAD, SWAG. Detailed ablation studies are provided.5. Analysis of the improved linguistic knowledge and physical/temporal reasoning abilities learned from video grounding, using GLUE-diagnostics, PIQA, and TRACIE datasets. Text-to-video retrieval visualization further shows the learned grounding.In summary, the key contribution is presenting a way to effectively transfer visual knowledge from large-scale video data to improve language understanding, via a new cross-modal knowledge distillation method and objectives. Both empirical and analysis results demonstrate the benefits of the video grounding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes VidLanKD, a novel video-language knowledge distillation method to improve language understanding by first pretraining a teacher model on a video-text dataset using contrastive learning and masked language modeling objectives, then transferring its knowledge to a student language model on a text dataset using neuron selectivity transfer and contrastive representation distillation objectives.
