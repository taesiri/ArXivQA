# [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge   Transfer](https://arxiv.org/abs/2107.02681)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that visually-grounded multi-modal knowledge from videos can improve language understanding, beyond what is possible with text-only pretraining. Specifically, the authors propose a cross-modal knowledge distillation method called VidLanKD to transfer knowledge from a video-text pretrained teacher model to a student text-only language model. The key ideas are:1) Videos provide richer grounded world knowledge compared to images, especially action-based and physical commonsense knowledge.2) A large-scale video dataset like HowTo100M has more diverse vocabulary and examples compared to typical image-text datasets. 3) Using different knowledge distillation objectives like NST and CRD can avoid the approximation error of assigning finite image/video "vokens" to tokens.4) Transferring knowledge from a multi-modal teacher to a text-only student maintains the scalability of standard language model pretraining.The central hypothesis is that by distilling knowledge from a teacher model pretrained on diverse video-text data, the student language model can gain improved language understanding abilities, even without seeing any visual input. The authors evaluate this through extensive experiments on language understanding benchmarks like GLUE, SQuAD, SWAG etc.In summary, the key hypothesis is that grounding language learning on large-scale, diverse video data can provide benefits for general natural language understanding, by transferring visual knowledge to text-only models. The proposed VidLanKD method aims to effectively distill this visually-grounded knowledge.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing VidLanKD, a novel video-language knowledge distillation method to improve language understanding. The key idea is to first pretrain a teacher model on a large-scale video-text dataset, then transfer its knowledge to a student language model on a text dataset via new distillation objectives.2. Using a large-scale video dataset (HowTo100M) which provides diverse vocabulary and rich world knowledge compared to image datasets commonly used in prior works. The video grounding helps the model learn better physical and temporal reasoning.3. Proposing to use Neuron Selectivity Transfer (NST) and Contrastive Representation Distillation (CRD) objectives for knowledge transfer, which avoid the approximation error of using a fixed set of image/video labels (vokens) as in prior work.4. Empirical improvements over strong baselines like BERT and vokenization on several language understanding tasks including GLUE, SQuAD, SWAG. Detailed ablation studies are provided.5. Analysis of the improved linguistic knowledge and physical/temporal reasoning abilities learned from video grounding, using GLUE-diagnostics, PIQA, and TRACIE datasets. Text-to-video retrieval visualization further shows the learned grounding.In summary, the key contribution is presenting a way to effectively transfer visual knowledge from large-scale video data to improve language understanding, via a new cross-modal knowledge distillation method and objectives. Both empirical and analysis results demonstrate the benefits of the video grounding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes VidLanKD, a novel video-language knowledge distillation method to improve language understanding by first pretraining a teacher model on a video-text dataset using contrastive learning and masked language modeling objectives, then transferring its knowledge to a student language model on a text dataset using neuron selectivity transfer and contrastive representation distillation objectives.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on visually grounded language learning:- It proposes a novel cross-modal knowledge distillation method, VidLanKD, for transferring knowledge from a multi-modal teacher model (video + text) to a student language model (text only). This is a new approach compared to prior works like vokenization that directly supervise the language model with vision labels.- It uses large-scale video datasets like HowTo100M for the teacher pretraining, instead of smaller-scale image captioning datasets used in prior works. Videos provide richer information and vocabulary coverage.- It experiments with different knowledge distillation objectives like NST and CRD to avoid the approximation error issue suffered by vokenization. The objectives better transfer the contextual representations.- It demonstrates strong performance improvements on diverse NLU tasks like GLUE, SQuAD, SWAG over text-only models and vokenization models.- It provides extensive ablation studies and analysis to understand where the improvements are coming from - whether it's the video grounding, knowledge distillation objectives, or architectures. - It analyzes the acquired knowledge qualitatively by showing improved commonsense reasoning on datasets like PIQA and TRACIE. It also visualizes the learned grounding through text-video retrieval.In summary, this paper makes significant advances over prior visually-grounded language learning methods by using videos, exploring new cross-modal distillation methods, and comprehensive empirical analysis. The gains on multiple language understanding benchmarks highlight the promise of this research direction.
