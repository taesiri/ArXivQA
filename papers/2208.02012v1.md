# [Character Generation through Self-Supervised Vectorization](https://arxiv.org/abs/2208.02012v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can we develop a drawing agent that can generate and parse characters in a stroke-based, self-supervised manner, without relying on any vector supervision?

In particular, the paper aims to tackle the generative and parsing tasks in the Omniglot challenge using only raster images, without any stroke-level labels during training. The key ideas explored are:

- Training a reinforcement learning agent to sequentially generate strokes to form characters, using appropriate reward functions for unconditional generation and parsing/reconstruction.

- Leveraging the trained parsing agent for exemplar generation and type-conditioned concept generation without any further training, by sampling stroke sequences from the policy network.

So in summary, the main research question is how to do stroke-based character generation and parsing in a completely self-supervised way, which no prior work has accomplished. The proposed drawing agent with appropriate training setups is their solution. Evaluating this agent on unconditional generation, parsing, exemplar generation, and type-based generation on Omniglot is their way of demonstrating the capabilities enabled by their approach.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a self-supervised reinforcement learning approach for stroke-based image generation, where a drawing agent is trained to generate images stroke-by-stroke. The key aspect is that only raster images are used for training, without any stroke-level supervision.

- Demonstrating that the agent can be trained for unconditional image generation on MNIST and Omniglot datasets. Novel characters are generated while conforming to the overall statistics of the dataset.

- Demonstrating that the same agent can be trained for the parsing (reconstruction) task, where it must redraw a given raster image using minimal strokes. This model learns to vectorize images in a self-supervised manner.

- Showing that the parsing model can be used for exemplar generation (generating new variations of a character) and type-conditioned generation (generating novel characters given an alphabet), without any further training.

- Providing quantitative evaluation of exemplar and type-conditioned generation using perceptual similarity metrics like LPIPS, SSIM and L2 distance.

- Achieving all of the above with a simple model consisting of a policy network and non-differentiable renderer, trained end-to-end with reinforcement learning and appropriate reward functions.

In summary, the key contribution is a self-supervised stroke-based drawing agent that can handle unconditional, conditional and parsing tasks on characters using only raster image supervision. This is the first model of its kind according to the authors.
