# [Character Generation through Self-Supervised Vectorization](https://arxiv.org/abs/2208.02012v1)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we develop a drawing agent that can generate and parse characters in a stroke-based, self-supervised manner, without relying on any vector supervision?In particular, the paper aims to tackle the generative and parsing tasks in the Omniglot challenge using only raster images, without any stroke-level labels during training. The key ideas explored are:- Training a reinforcement learning agent to sequentially generate strokes to form characters, using appropriate reward functions for unconditional generation and parsing/reconstruction.- Leveraging the trained parsing agent for exemplar generation and type-conditioned concept generation without any further training, by sampling stroke sequences from the policy network.So in summary, the main research question is how to do stroke-based character generation and parsing in a completely self-supervised way, which no prior work has accomplished. The proposed drawing agent with appropriate training setups is their solution. Evaluating this agent on unconditional generation, parsing, exemplar generation, and type-based generation on Omniglot is their way of demonstrating the capabilities enabled by their approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a self-supervised reinforcement learning approach for stroke-based image generation, where a drawing agent is trained to generate images stroke-by-stroke. The key aspect is that only raster images are used for training, without any stroke-level supervision.- Demonstrating that the agent can be trained for unconditional image generation on MNIST and Omniglot datasets. Novel characters are generated while conforming to the overall statistics of the dataset.- Demonstrating that the same agent can be trained for the parsing (reconstruction) task, where it must redraw a given raster image using minimal strokes. This model learns to vectorize images in a self-supervised manner.- Showing that the parsing model can be used for exemplar generation (generating new variations of a character) and type-conditioned generation (generating novel characters given an alphabet), without any further training.- Providing quantitative evaluation of exemplar and type-conditioned generation using perceptual similarity metrics like LPIPS, SSIM and L2 distance.- Achieving all of the above with a simple model consisting of a policy network and non-differentiable renderer, trained end-to-end with reinforcement learning and appropriate reward functions.In summary, the key contribution is a self-supervised stroke-based drawing agent that can handle unconditional, conditional and parsing tasks on characters using only raster image supervision. This is the first model of its kind according to the authors.
