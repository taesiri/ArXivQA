# [ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on   Class-level Code Generation](https://arxiv.org/abs/2308.01861)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How do large language models (LLMs) perform on the more challenging task of class-level code generation compared to simpler function-level code generation benchmarks like HumanEval?The key hypotheses examined in the paper are:1) LLMs will show worse performance on class-level code generation tasks compared to standalone function-level tasks.2) The rankings of LLM performance on function-level tasks may not hold for class-level tasks. 3) Different code generation strategies (holistic, incremental, compositional) may be better suited for different LLMs on class-level tasks.4) LLMs may have difficulty generating code that depends on other contexts like fields and methods within a class.To test these hypotheses, the authors construct a new benchmark of class-level coding tasks called ClassEval and evaluate 11 state-of-the-art LLMs using different generation strategies. The key findings validate the worse performance of LLMs on class-level tasks, changes in LLM rankings, and limitations in generating dependent code. Overall, the paper provides new insights into LLM capabilities on more complex, class-level code generation compared to simpler function-level benchmarks.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The paper presents the first benchmark for evaluating large language models (LLMs) on class-level code generation tasks. Specifically, the authors manually construct a new benchmark called ClassEval with 100 class-level Python code generation tasks. Constructing this benchmark involves significant manual effort (around 500 person-hours) to create class skeletons, test cases, and solutions for each task. 2. The paper performs an empirical study using the ClassEval benchmark to evaluate 11 state-of-the-art LLMs on class-level code generation. The study explores different code generation strategies like holistic, incremental, and compositional generation. The results reveal insights about the capabilities and limitations of current LLMs for more complex, class-level code generation compared to simpler function-level tasks.3. The key findings from the study are:- LLMs demonstrate much lower performance on class-level tasks compared to function-level tasks. Their function-level abilities do not necessarily reflect class-level abilities. This highlights the need for class-level benchmarks like ClassEval.- GPT-3 and GPT-4 still dominate other LLMs on class-level generation, while models like Codex, Instruct-CodeGen, and WizardCoder form the next tier.- Holistic generation works best only for GPT models. Other LLMs perform better with incremental or compositional method-by-method generation.- Models struggle to generate code dependent on other methods, compared to field accesses. - Common errors include AttributeError and TypeError, indicating difficulties in satisfying constraints.Overall, the paper presents the first dedicated class-level code generation benchmark ClassEval, and uses it to provide novel insights into evaluating more complex coding abilities of LLMs compared to simpler function-level tasks. The benchmark and findings highlight promising future research directions.
