# [ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on   Class-level Code Generation](https://arxiv.org/abs/2308.01861)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How do large language models (LLMs) perform on the more challenging task of class-level code generation compared to simpler function-level code generation benchmarks like HumanEval?The key hypotheses examined in the paper are:1) LLMs will show worse performance on class-level code generation tasks compared to standalone function-level tasks.2) The rankings of LLM performance on function-level tasks may not hold for class-level tasks. 3) Different code generation strategies (holistic, incremental, compositional) may be better suited for different LLMs on class-level tasks.4) LLMs may have difficulty generating code that depends on other contexts like fields and methods within a class.To test these hypotheses, the authors construct a new benchmark of class-level coding tasks called ClassEval and evaluate 11 state-of-the-art LLMs using different generation strategies. The key findings validate the worse performance of LLMs on class-level tasks, changes in LLM rankings, and limitations in generating dependent code. Overall, the paper provides new insights into LLM capabilities on more complex, class-level code generation compared to simpler function-level benchmarks.
