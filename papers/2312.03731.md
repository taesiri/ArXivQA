# [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs](https://arxiv.org/abs/2312.03731)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) rely on large-scale labeled data for end-to-end supervised training, which can be expensive to obtain. Pre-training GNNs on self-supervised tasks and prompting have emerged as promising solutions but existing works have limitations:
(1) Most employ only a single pretext task, limiting the breadth of pre-trained knowledge. 
(2) Transferring knowledge from multiple pretext tasks to downstream tasks is challenging.

Proposed Solution - MultiGPrompt
- A novel framework for multi-task pre-training and prompting on graphs for few-shot learning. It has two key stages:

1. Multi-Task Pre-training
- Employs multiple self-supervised pretext tasks: DGI, GraphCL, link prediction
- Introduces pretext tokens (learnable vectors) to reformulate input and reduce interference between diverse pretext tasks 
- Different tokens used for different tasks and layers to capture task-specific signals
- Losses from all tasks aggregated synergistically 

2. Downstream Prompting 
- Proposes composed prompts: Learnable compositions of pretext tokens to extract task-specific knowledge
- Introduces open prompts: Directly learnable vectors focusing on global inter-task knowledge  
- Dual prompts applied to pretrained model separately; outputs combined for downstream task

Main Contributions:
- First multi-task pre-training and prompting framework on graphs for few-shot learning
- Pretext tokens in pre-training stage to enable collaborative training of multiple self-supervised tasks  
- Dual-prompt design using composed and open prompts to transfer both task-specific and global pre-trained knowledge
- Extensive experiments on 6 datasets demonstrating efficacy over state-of-the-art methods
