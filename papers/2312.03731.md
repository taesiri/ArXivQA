# [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs](https://arxiv.org/abs/2312.03731)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) rely on large-scale labeled data for end-to-end supervised training, which can be expensive to obtain. Pre-training GNNs on self-supervised tasks and prompting have emerged as promising solutions but existing works have limitations:
(1) Most employ only a single pretext task, limiting the breadth of pre-trained knowledge. 
(2) Transferring knowledge from multiple pretext tasks to downstream tasks is challenging.

Proposed Solution - MultiGPrompt
- A novel framework for multi-task pre-training and prompting on graphs for few-shot learning. It has two key stages:

1. Multi-Task Pre-training
- Employs multiple self-supervised pretext tasks: DGI, GraphCL, link prediction
- Introduces pretext tokens (learnable vectors) to reformulate input and reduce interference between diverse pretext tasks 
- Different tokens used for different tasks and layers to capture task-specific signals
- Losses from all tasks aggregated synergistically 

2. Downstream Prompting 
- Proposes composed prompts: Learnable compositions of pretext tokens to extract task-specific knowledge
- Introduces open prompts: Directly learnable vectors focusing on global inter-task knowledge  
- Dual prompts applied to pretrained model separately; outputs combined for downstream task

Main Contributions:
- First multi-task pre-training and prompting framework on graphs for few-shot learning
- Pretext tokens in pre-training stage to enable collaborative training of multiple self-supervised tasks  
- Dual-prompt design using composed and open prompts to transfer both task-specific and global pre-trained knowledge
- Extensive experiments on 6 datasets demonstrating efficacy over state-of-the-art methods


## Summarize the paper in one sentence.

 This paper proposes a novel framework called MultiGPrompt for multi-task pre-training and prompting of graph neural networks to improve performance on downstream few-shot learning tasks.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing MultiGPrompt, a novel multi-task pre-training and prompting framework for few-shot learning on graphs. 

2. Introducing pretext tokens during pre-training to reduce task interference and enable multiple pretext tasks to be optimized in a synergistic manner.

3. Proposing a dual-prompt mechanism with both composed prompts and open prompts for downstream adaptation, in order to leverage both task-specific and global inter-task pre-trained knowledge.

4. Conducting extensive experiments on six public datasets which demonstrate that MultiGPrompt significantly outperforms various state-of-the-art baselines.

In summary, the key contribution is a new framework that combines multi-task pre-training using pretext tokens and a dual-prompt design for effective few-shot learning on graphs. Both the pre-training and prompting components have novel designs to enable better transfer of knowledge from diverse self-supervised signals.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and concepts associated with this paper include:

- Graph neural networks (GNNs): The paper focuses on developing a framework for pre-training and prompting graph neural networks. GNNs are used for graph representation learning. 

- Pre-training: The paper proposes a multi-task pre-training framework to learn general graph knowledge without task-specific labels. Multiple self-supervised pretext tasks are used.

- Prompting: The paper employs prompt-based learning to align the downstream tasks with the pre-trained model, which is especially useful in few-shot settings. A dual-prompt mechanism with composed and open prompts is proposed.  

- Multi-task learning: The goal is to leverage multiple diverse pretext tasks synergistically to cover comprehensive graph knowledge, avoiding task interference. 

- Few-shot learning: The pre-trained model is adapted to downstream tasks with limited labeled data, e.g. few-shot node classification and graph classification tasks.

- Parameter efficiency: Prompting only updates a small number of parameters compared to fine-tuning the entire pre-trained model, making it suitable for few-shot adaptation.

In summary, the key terms cover multi-task pre-training and prompting of GNNs for few-shot learning on graphs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the proposed method use pretext tokens to reduce task interference during multi-task pre-training? What is the intuition behind using pretext tokens for this purpose?

2. The proposed dual-prompt mechanism employs both composed prompts and open prompts. What is the difference in their objectives and how do they complement each other?

3. How does the composed prompt transfer task-specific pre-trained knowledge to the downstream tasks? Explain the working and significance of composed prompts.  

4. What is the purpose of open prompts? How do they help transfer global inter-task knowledge that composed prompts may miss out?

5. The paper mentions that different pretext tasks focus on different aspects of graphs, such as connectivity, features, or patterns. Can you analyze what specific aspects each of the 3 pretext tasks (DGI, GraphCL, Link Prediction) focus on?

6. How does the proposed method address the two key challenges mentioned: (i) leveraging diverse pretext tasks synergistically, and (ii) transferring both task-specific and global pre-trained knowledge downstream?

7. What were the different variants of the model analyzed in the ablation study? What was learned about the importance of different components from this analysis?

8. How do the trends for selecting the hyperparameter Î±0 vary between node classification and graph classification tasks? What could be the potential reasons behind this?

9. How does the parameter efficiency of the proposed method compare to the baselines? Why is it more suitable for few-shot downstream tasks?

10. The cross-data experiments pre-train on one dataset and prompt on another. What key observations indicate the robustness of the proposed approach from these results?
