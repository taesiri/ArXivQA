# [KICGPT: Large Language Model with Knowledge in Context for Knowledge   Graph Completion](https://arxiv.org/abs/2402.02389)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Knowledge graph completion (KGC) aims to complete incomplete triples in knowledge graphs to address their inherent incompleteness. This is crucial for downstream applications.
- Existing KGC methods can be categorized as triple-based methods and text-based methods. 
    - Triple-based methods struggle with long-tail entities due to limited structural information.  
    - Text-based methods alleviate this issue but require costly training of language models and task-specific fine-tuning.
- Directly applying large language models (LLMs) to KGC also has several challenges:
    - Outputs may fall outside entity scope
    - Length limits hinder describing full KGC task
    - No effective prompt design for KGC

Proposed Solution:
- Propose a framework called KICGPT that integrates an LLM with a triple-based KGC retriever
    - For a query, retriever generates scores for all possible tails. LLM reranks the top candidates.
- Use an in-context learning strategy called Knowledge Prompt to encode structural knowledge into demonstrations that guide the LLM
- Knowledge Prompt has specially designed demonstration pools and ordering strategies
- Also propose Text Self-Alignment to transform raw text descriptions into more understandable ones  

Main Contributions:
- First work combining LLMs with triple-based KGC methods
- Novel in-context learning strategy Knowledge Prompt tailored for KGC
- KICGPT achieves state-of-the-art performance on benchmarks with low training overhead
- Effective for handling long-tail entities

In summary, the paper proposes a novel and efficient KGC framework KICGPT that integrates LLMs with traditional KGC methods. A specially tailored in-context learning strategy is used to inject structural knowledge and guide the LLM. Experiments show state-of-the-art performance, especially on long-tail entities, with low training overhead.
