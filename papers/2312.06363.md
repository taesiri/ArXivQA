# [MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples](https://arxiv.org/abs/2312.06363)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper proposes Multi-Modal In-Context Tuning (MMICT), a new fine-tuning paradigm that leverages the in-context learning capabilities of multi-modal large language models (MM-LLMs) to boost performance on downstream multi-modal tasks. MMICT allows models to learn from visual-guided textual features extracted from demonstration examples, and to generate outputs conditioned on textual-guided visual features from the input queries. A key contribution is the Multi-Modal Hub module, which can capture both uni-modal and multi-modal fused features within a unified architecture. Extensive experiments across diverse multi-modal tasks and datasets demonstrate MMICT's effectiveness - it consistently outperforms both traditional fine-tuning and a vanilla in-context tuning baseline. Further analysis provides insights into optimal demonstration design choices and confirms MMICT's robustness. Overall, MMICT represents an innovative integration of in-context learning and multi-modal fine-tuning that successfully leverages the complementary strengths of both paradigms to advance the state-of-the-art in multi-modal language models.
