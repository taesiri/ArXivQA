# [Accuracy Prediction with Non-neural Model for Neural Architecture Search](https://arxiv.org/abs/2007.04785)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: Can non-neural models like gradient boosting decision trees (GBDT) be effectively used as the accuracy predictor in neural architecture search (NAS)?

The key hypotheses appear to be:

1) Architectures represented as sequences of discrete symbols are more like tabular data, which is better handled by non-neural models like GBDT compared to neural network architectures. 

2) GBDT can achieve comparable or better accuracy prediction for NAS compared to commonly used neural predictors like RNNs, CNNs, GCNs.

3) GBDT's interpretability can be leveraged to automatically prune the search space and identify unimportant architecture components, making NAS more efficient.

4) Using GBDT as both the pruner and predictor in a two-stage NAS approach (pruning then search) can lead to better architectures compared to using GBDT solely as the predictor.

The paper aims to demonstrate the viability of using GBDT for NAS via experiments on NASBench-101 and ImageNet datasets. The results appear to confirm the hypotheses, showing GBDT can effectively predict architecture accuracy, prune search spaces, and discover high-performing neural net architectures.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes to use gradient boosting decision trees (GBDT) as the accuracy predictor for neural architecture search (NAS). Previous works usually use neural network models like RNN or GCN to build the predictor. This paper shows that tree-based GBDT can achieve better prediction accuracy than neural predictors. 

2. It further proposes to leverage GBDT to prune the search space by identifying unimportant architecture operations according to the SHAP values from GBDT. This allows searching in a more compact and promising space. 

3. Based on the two aspects above, it develops two NAS algorithms: GBDT-NAS which uses GBDT as the predictor, and GBDT-NAS-S3 which additionally prunes the search space using GBDT.

4. Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of the proposed GBDT-NAS and GBDT-NAS-S3. GBDT-NAS is much more sample efficient than previous methods like random search and regularization evolution in finding the optimal architecture on NASBench-101. GBDT-NAS-S3 achieves strong performance on ImageNet classification.

In summary, the key contribution is using non-neural GBDT models for neural architecture search, including accuracy prediction and search space pruning. This provides a simple yet effective approach for NAS.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in neural architecture search (NAS):

- It focuses on using non-neural models like gradient boosting decision trees (GBDT) as the accuracy predictor, rather than neural networks which are commonly used. The motivation is that the tabular, discrete representation of architectures may be better suited to tree-based models than neural networks. 

- The GBDT predictor is shown to achieve higher prediction accuracy than neural network predictors like RNNs, CNNs, and GCNs on the NASBench-101 dataset. This demonstrates the potential of non-neural predictors.

- The paper proposes not just using GBDT to predict architecture accuracy, but also using it to prune the search space by identifying unimportant architecture factors. This improves NAS efficiency and result quality. Most works focus just on architecture prediction.

- Experiments show GBDT-NAS finds optima faster than methods like random search, regularized evolution, and Monte Carlo tree search on NASBench-101. On ImageNet it achieves competitive accuracy to other NAS techniques.

- Overall, using non-neural GBDT for architecture prediction and search space pruning is a novel approach in NAS. The simplicity and interpretability of GBDT is leveraged compared to complex neural network predictors commonly used. Empirical results validate the effectiveness of this idea.

So in summary, the key novelty is using GBDT for NAS, which performs well empirically. The work also proposes innovations like search space pruning that further improve NAS. The results demonstrate GBDT's potential as an alternative to standard neural network based approaches in this field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Applying GBDT-NAS and GBDT-NAS-S3 to more general search spaces and more complex tasks beyond ImageNet classification. The authors mention they plan to explore using their methods on larger and more diverse search spaces and for tasks like object detection and semantic segmentation. 

- Using higher-order SHAP values for pruning the search space in GBDT-NAS-S3. The authors showed that using 2nd order SHAP values for pruning lead to better results than 1st order, so they suggest exploring using even higher orders.

- Enhancing the search space pruning process. The authors mention trying more advanced ways to prune the search space beyond just removing individual operations, such as removing entire layers or blocks.

- Improving the efficiency and scalability of the methods. The authors suggest exploring ways to further improve the computational efficiency and scalability of GBDT-NAS and GBDT-NAS-S3 to even larger search spaces and datasets.

- Applying the ideas to other neural architecture search spaces and representations beyond the chain-like architectures focused on in this paper.

- Exploring other non-neural predictors besides GBDT for modeling and searching architectures.

- Developing better ways to leverage both neural and non-neural predictors/representations for neural architecture search.

So in summary, the key directions are around enhancing the methods and applying them to larger and more complex scenarios, improving efficiency and scalability, and exploring how to best combine neural and non-neural techniques for architecture search.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

The paper proposes using gradient boosting decision trees (GBDT) as an accuracy predictor for neural architecture search (NAS). Most prior works use neural network based predictors like RNNs or GCNs, but the authors argue that representing architectures as tabular data with one-hot encodings is better suited to tree-based models like GBDT. They introduce GBDT-NAS which trains a GBDT to predict architecture accuracy and searches the space guided by its predictions. They further propose GBDT-NAS-S3 which leverages the interpretability of GBDT to prune unpromising operations from the search space before searching, making the process more efficient. Experiments on NASBench-101 and ImageNet show GBDT-NAS finds better architectures with 6-22x fewer samples than baselines. GBDT-NAS-S3 further improves results, discovering a network with 23.4% top-1 error on ImageNet. The paper demonstrates GBDT is an effective and interpretable approach for accuracy prediction and search space pruning in NAS.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes using a gradient boosting decision tree (GBDT) model as the accuracy predictor for neural architecture search (NAS). Most prior NAS methods use neural network based predictors, but the authors argue that GBDT may be better suited for predicting architecture accuracy since architectures can be represented as tabular data with one-hot encodings. The GBDT predictor achieved higher accuracy than RNN, CNN, and GCN predictors on a NAS benchmark dataset. Based on this, the authors propose GBDT-NAS, which trains a GBDT to predict architecture accuracies, samples architectures from the space, predicts their accuracy with the GBDT, and validates the top predictions. They further propose GBDT-NAS-S3, which leverages the GBDT model to prune the search space by removing operations that contribute negatively to accuracy prediction based on SHAP values. Experiments on NASBench-101 and ImageNet show GBDT-NAS achieves better sample efficiency than baselines like random search and regularized evolution. GBDT-NAS-S3 further improves results by pruning the search space, achieving 23.4% top-1 error on ImageNet.

In summary, this paper demonstrates the effectiveness of using a GBDT architecture accuracy predictor for NAS, outperforming neural network predictors. The authors also show how the GBDT can be leveraged to prune the search space and improve NAS efficiency and performance further. The proposed GBDT-NAS and GBDT-NAS-S3 methods achieve strong results on NAS benchmarks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes using a gradient boosting decision tree (GBDT) as the accuracy predictor for neural architecture search (NAS). The key idea is to represent candidate architectures as tabular data with one-hot encoding, rather than complex neural network inputs. This allows the tree-based GBDT model to effectively learn the mapping from architecture encoding to accuracy. The GBDT predictor is first trained on some sampled architecture-accuracy pairs. It is then used to predict the accuracy of many more candidate architectures, from which the most promising ones are selected for evaluation. The paper further improves this GBDT-NAS approach by leveraging the GBDT model itself to prune the search space. Specifically, architectural building blocks that contribute negatively to accuracy prediction are removed, based on SHAP value analysis of the GBDT. This focuses the search on a smaller but higher-quality search space. Experiments on NASBench and ImageNet show GBDT-NAS can find architectures using far fewer samples than baselines, and the search space pruning provides further benefits.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes using gradient boosting decision trees (GBDT) as a non-neural alternative to predict architecture accuracy for neural architecture search (NAS), showing it achieves better prediction accuracy than neural network predictors. The paper also proposes a NAS method called GBDT-NAS-S3 that first prunes the search space using GBDT feature importance, and then performs NAS in the pruned space, achieving state-of-the-art results on NASBench-101 and ImageNet datasets.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the key problem it is addressing is how to build an effective neural architecture search (NAS) method based on using an accuracy predictor to predict the performance of candidate architectures. 

Specifically, it points out some limitations of existing NAS methods that use neural network based predictors (like RNN, CNN, GCN etc), including:

- Neural predictors can be prone to overfitting and require careful design/tuning. 

- They may not fit the discrete, tabular-like representations of architectures as well as non-neural models like tree-based methods. 

To address this, the paper explores using gradient boosted decision trees (GBDT) as the accuracy predictor, treating architecture representations as tabular data. 

The main contributions are:

- Proposing GBDT-NAS which uses GBDT as the predictor to perform NAS. Experiments show it achieves better prediction accuracy than neural predictors.

- Further proposing GBDT-NAS-S3 which uses GBDT to first prune the search space by identifying unimportant architecture components, and then perform NAS in the pruned space. This makes the search more efficient. 

- Evaluating the methods on NASBench-101 and ImageNet datasets, showing GBDT-NAS is much more sample efficient than baselines like random search, and GBDT-NAS-S3 achieves state-of-the-art ImageNet accuracy.

In summary, the key focus is using non-neural GBDT models effectively for neural architecture search, to address limitations of neural predictors. The main ideas are using GBDT as the predictor, and pruning the search space prior to search.


## What are the keywords or key terms associated with this paper?

 Based on my reading, some of the key terms and concepts in this paper include:

- Neural Architecture Search (NAS): The paper focuses on automatically searching for optimal neural network architectures using NAS methods.

- Accuracy predictor: A core component of many NAS methods is an accuracy predictor model that predicts the performance of candidate architectures to guide the search.

- Non-neural model: The paper proposes using a gradient boosting decision tree (GBDT), a non-neural tree-based model, as the accuracy predictor. This is in contrast to prior works that use neural network models.

- Tabular data representation: The paper represents architectures as tabular, one-hot encoded features, instead of graph representations. This fits better with tree models like GBDT.

- Search space pruning: The paper proposes pruning the search space using the GBDT model to find suboptimal operations and narrow the search.

- NASBench-101: A NAS benchmark dataset used to evaluate NAS algorithms. Experiments here demonstrate efficiency of the GBDT predictor.

- ImageNet: Image classification benchmark used to demonstrate GBDT-NAS methods finding efficient architectures.

- Sample efficiency: The proposed GBDT methods require significantly fewer architecture evaluations to find optimal architectures compared to baselines.

- Effectiveness: The discovered architectures achieve competitive accuracy on ImageNet compared to other NAS methods.

In summary, the key ideas are using GBDT instead of neural networks for architecture search, representing architectures as tabular data, and pruning the search space - leading to more sample efficient and effective NAS.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing this paper:

1. What is the motivation for using non-neural models like GBDT for neural architecture search? Why might they be better than neural network based predictors? 

2. How does the paper represent architectures as features for the GBDT predictor? What type of input representation is used?

3. What is the proposed GBDT-NAS algorithm? How does it work to search architectures using a GBDT predictor?

4. How does the paper propose to prune the search space using SHAP values from the GBDT model? What are the first-order and second-order pruning algorithms?

5. What experiments does the paper conduct to evaluate GBDT-NAS and GBDT-NAS-S3? What datasets are used?

6. What are the main results on NASBench-101? How does GBDT-NAS compare to other NAS methods?

7. What ImageNet results are achieved by GBDT-NAS and GBDT-NAS-S3? How do they compare to other approaches?

8. What analysis does the paper provide on the hyperparameters and performance of GBDT as a predictor? What affects accuracy?

9. What architectures are discovered by the different methods? How does pruning the search space impact the results?

10. What are the main conclusions and takeaways from the paper? What future work is suggested?
