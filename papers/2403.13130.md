# [Self-generated Replay Memories for Continual Neural Machine Translation](https://arxiv.org/abs/2403.13130)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern neural machine translation (NMT) models exhibit catastrophic forgetting when trained continually on new languages. This limits their ability to adapt to new data over time.

- Previous works have mainly focused on domain adaptation rather than continual learning across languages. Recently some works have looked at incremental language learning but solutions are still limited.

Proposed Solution:
- The paper proposes a novel data-based approach called Self-Generated Replay (SG-Rep) to mitigate catastrophic forgetting in continual NMT training. 

- Core idea is to use a fixed-size replay buffer populated with pseudo-parallel sentences generated by the NMT model itself in previous experiences. This acts as supplemental training data.

- SG-Rep has 3 main steps: 
   1) Generate target sentences using the NMT decoder and an encoder input signal
   2) Filter low quality sentences
   3) Translate target sentences back to source language to create pseudo-parallel data

- The pseudo-parallel sentences are used alongside new task data to train the model on subsequent experiences. This replays previous knowledge.

Contributions:
- First application of self-generated replay memories for continual learning in NMT.

- Shows SG-Rep can effectively mitigate catastrophic forgetting without requiring real training data storage.

- Extensive experiments conducted on multiple language pairs from IWSLT17 and UNPC datasets.

- Demonstrates superior performance over several strong continual learning baselines like EWC, A-GEM, and LAMOL.

- Analyzes impact of different model architectures and hyper-parameters. Also studies pseudo-sample quality.

In summary, the paper makes important contributions in advancing continual learning for NMT models by generating synthetic training data on-the-fly for replay. This alleviates the need for storing previous real training data.
