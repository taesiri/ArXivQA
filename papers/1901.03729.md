# [Automated Rationale Generation: A Technique for Explainable AI and its   Effects on Human Perceptions](https://arxiv.org/abs/1901.03729)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions and hypotheses appear to be:

Research Questions:

- Can human explanations for actions in a video game be used to train a system to generate explanations for the actions of autonomous agents?

- How do humans perceive different types of automatically generated rationales compared to human-provided explanations and randomly generated rationales? 

- What factors contribute to human preferences for certain types of rationales over others?

Hypotheses:

- Rationales generated by the proposed technique will be rated higher in quality by humans compared to randomly generated rationales (H1). 

- Rationales generated by the proposed technique will be rated close in quality to human-provided exemplary rationales (H2).

The authors introduce a pipeline for collecting a corpus of human explanations for actions in the game Frogger. They use this corpus to train neural networks to generate natural language rationales explaining the actions of AI agents playing Frogger. 

Through user studies, they test if the generated rationales are perceived as higher quality than randomly generated ones (H1) and close in quality to human rationales (H2). They also investigate factors that influence human preferences between different types of generated rationales.

So in summary, the central research questions focus on using human explanations to train AI rationale generation, evaluating the quality of the generated rationales, and understanding human perceptions and preferences regarding different types of rationales. The main hypotheses are that the generated rationales will be better than random and close to human quality.


## What is the main contribution of this paper?

 The main contribution of this paper appears to be introducing an automated technique for generating natural language rationales to explain the actions of AI agents in sequential environments. Specifically:

- They propose the idea of using "automated rationale generation" to have AI agents generate human-like natural language explanations for their actions in real-time. This differs from some other explainable AI techniques that focus more on visualization or exposing internal representations.

- They introduce a methodology to automatically collect a corpus of human think-aloud explanations paired with game states, using a remote data collection setup.

- They train encoder-decoder neural network models on this corpus to translate game states and actions into natural language rationales. They show two configurations ("focused-view" and "complete-view") that produce different styles of rationales.

- They conduct user studies evaluating how people perceive and prefer the different types of generated rationales along various dimensions like confidence, human-likeness, justification, etc. This provides insights into what makes rationales effective.

- Overall, they demonstrate an end-to-end pipeline for training systems to produce human-understandable rationales in sequential environments, evaluate the rationales, and gain insights to further improve the approach. The methodology and findings represent contributions in making AI systems more explainable in a human-centered way.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents a technique for training neural networks to generate natural language rationales that explain the actions of autonomous agents in video games, using a corpus of human think-aloud explanations collected through a novel pipeline, and evaluates the plausibility and human perceptions of the generated rationales.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in explainable AI:

- Focuses on sequential decision making tasks: Much prior work on explainable AI has focused on non-sequential tasks like image classification. This paper tackles the more challenging problem of generating explanations for an agent's behavior over time in a dynamic environment.

- Uses natural language rationales: The approach trains models to generate natural language rationales that sound human-like but do not necessarily reveal the true internal decision process. This differs from some interpretability methods that focus on visualizing internal representations.

- Leverages human explanation data: A key contribution is the pipeline for collecting a corpus of human think-aloud explanations as they play a game. This natural language dataset is then used to train the rationale generation models.

- Evaluates with user studies: The paper includes substantial evaluation of the generated rationales with real human subjects, assessing qualities like plausibility, confidence, preference between different styles of rationales. This user-centered evaluation is still relatively rare in XAI research.

- Compares different input configurations: An interesting aspect is showing how tweaking the inputs to the rationale generator (e.g. focused vs complete view) leads to qualitative differences in the rationales (concise vs detailed).

Overall, the use of human explanation data, focus on sequential tasks, and evaluation via user studies help differentiate this work from much of the existing research in explainable AI. The paper makes a nice contribution in advancing XAI techniques for dynamic environments.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Testing the approach in more interactive settings where users can contest or ask for alternative rationales. The current system is non-interactive. Adding interactivity could improve user satisfaction.

- Extending the data collection pipeline to work in continuous time/action environments. Currently it is designed for discrete action games with natural breakpoints.

- Deploying the system in a longitudinal study to see how perceptions change over time and control for novelty effects. The current study only looks at one-time use. 

- Testing scalability to larger state/action spaces. The Frogger environment has a relatively small state space.

- Looking at how mistake criticality affects confidence drops. Currently all mistakes are weighted equally but mistakes during critical moments may have an outsized impact.

- Incorporating checks for grammar/spelling errors which can sometimes get propagated from the training data.

- Exploring multimodal explanations by combining rationales with other modes like visualization.

- Applying the approach to other sequential decision making domains beyond games.

In summary, the main directions are around interactivity, scaling, longitudinal studies, multimodality, mistake criticality, grammar quality, and testing the approach in other domains. The authors lay out a clear research agenda for building on this work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper presents a technique for generating natural language rationales to explain the actions of autonomous agents in sequential decision making environments. The authors collect a dataset of human think-aloud explanations for actions taken in the game Frogger. They then train a neural network to generate rationales from the game state and action representations. Two types of rationales are produced: focused-view rationales that are localized and concise, and complete-view rationales that incorporate more contextual information. Through user studies, the authors evaluate the plausibility and perceptions of the generated rationales. They find that the intended design characteristics of the rationale types align with user perceptions. Overall, the paper demonstrates a novel approach for training AI agents to generate human-understandable rationales to explain their actions in sequential environments like games.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a technique for generating natural language rationales to explain the behaviors of autonomous agents acting in sequential environments. The authors propose automated rationale generation as an approach to explainable AI that focuses on producing plausible, human-understandable justifications for agent actions, rather than trying to expose the inner workings of the agent's decision making process. To train a rationale generator, the authors first collect a corpus of human think-aloud explanations for actions taken in the game Frogger. They then use this corpus to train an encoder-decoder neural network to translate an agent's internal state representations into natural language rationales. 

The authors conduct two user studies evaluating the generated rationales. The first study establishes the plausibility of the rationales compared to random and human-authored baselines, and characterizes how people perceive the rationales along dimensions like confidence, human-likeness, adequacy of justification, and understandability. The second study validates alignment between the intended features of two types of generated rationales and actual user perceptions, and further explores user preferences between the rationale types in contexts like communicating failure. Overall, the work demonstrates an end-to-end pipeline for training neural rationale generators on human explanation data, and provides insights on designing rationales to effectively communicate agent behavior to human users.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents an automated rationale generation technique for explaining the behavior of autonomous agents in sequential environments. The authors first collected a corpus of human think-aloud explanations for actions taken in the game Frogger using a novel data collection interface. This corpus was then used to train an encoder-decoder neural network to translate an agent's internal state representations into natural language rationales explaining its actions. The authors experimented with two different input configurations for the network - focused-view and complete-view - to generate different styles of rationales. The plausibility and perceived quality of the generated rationales were then evaluated in two user studies. The first study compared the candidate rationales to upper and lower baselines, while the second study directly compared user perceptions of the focused-view and complete-view rationales. Overall, the main contribution is a pipeline for training neural models to generate human-like natural language rationales to explain agent behavior in sequential environments.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the main problem it is addressing is how to generate plausible, human-understandable explanations for the actions of autonomous agents operating in sequential environments. 

Specifically, the paper focuses on two key challenges:

1. Generating human-like explanations in sequential environments is difficult because an agent's actions depend not just on the current state but also on previous states and actions. So the explanations need to take into account this temporal dependence and reasoning.

2. Most prior work on explainable AI has focused on non-sequential problems like image classification. Techniques developed for those domains don't directly translate to providing good explanations in sequential environments. 

To address these challenges, the paper introduces the concept of "automated rationale generation" - using natural language generation techniques to produce explanations that justify an agent's actions in a way a human would explain themselves. 

The key contributions are:

- A methodology to collect a corpus of human explanations by having people explain their actions while playing a game. 

- Using this corpus to train neural network models to translate an agent's internal states into natural language rationales.

- Configuring the inputs to these models to generate different styles of rationales (e.g. localized vs holistic).

- User studies evaluating the perceived quality and preferences between rationale styles.

So in summary, the main problem is producing human-understandable rationales to explain sequential agent behaviors, for which they develop a data-driven rationale generation technique. The human studies then provide insights into people's perceptions of the resulting rationales.
