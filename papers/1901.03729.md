# Automated Rationale Generation: A Technique for Explainable AI and its   Effects on Human Perceptions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions and hypotheses appear to be:Research Questions:- Can human explanations for actions in a video game be used to train a system to generate explanations for the actions of autonomous agents?- How do humans perceive different types of automatically generated rationales compared to human-provided explanations and randomly generated rationales? - What factors contribute to human preferences for certain types of rationales over others?Hypotheses:- Rationales generated by the proposed technique will be rated higher in quality by humans compared to randomly generated rationales (H1). - Rationales generated by the proposed technique will be rated close in quality to human-provided exemplary rationales (H2).The authors introduce a pipeline for collecting a corpus of human explanations for actions in the game Frogger. They use this corpus to train neural networks to generate natural language rationales explaining the actions of AI agents playing Frogger. Through user studies, they test if the generated rationales are perceived as higher quality than randomly generated ones (H1) and close in quality to human rationales (H2). They also investigate factors that influence human preferences between different types of generated rationales.So in summary, the central research questions focus on using human explanations to train AI rationale generation, evaluating the quality of the generated rationales, and understanding human perceptions and preferences regarding different types of rationales. The main hypotheses are that the generated rationales will be better than random and close to human quality.


## What is the main contribution of this paper?

The main contribution of this paper appears to be introducing an automated technique for generating natural language rationales to explain the actions of AI agents in sequential environments. Specifically:- They propose the idea of using "automated rationale generation" to have AI agents generate human-like natural language explanations for their actions in real-time. This differs from some other explainable AI techniques that focus more on visualization or exposing internal representations.- They introduce a methodology to automatically collect a corpus of human think-aloud explanations paired with game states, using a remote data collection setup.- They train encoder-decoder neural network models on this corpus to translate game states and actions into natural language rationales. They show two configurations ("focused-view" and "complete-view") that produce different styles of rationales.- They conduct user studies evaluating how people perceive and prefer the different types of generated rationales along various dimensions like confidence, human-likeness, justification, etc. This provides insights into what makes rationales effective.- Overall, they demonstrate an end-to-end pipeline for training systems to produce human-understandable rationales in sequential environments, evaluate the rationales, and gain insights to further improve the approach. The methodology and findings represent contributions in making AI systems more explainable in a human-centered way.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a technique for training neural networks to generate natural language rationales that explain the actions of autonomous agents in video games, using a corpus of human think-aloud explanations collected through a novel pipeline, and evaluates the plausibility and human perceptions of the generated rationales.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in explainable AI:- Focuses on sequential decision making tasks: Much prior work on explainable AI has focused on non-sequential tasks like image classification. This paper tackles the more challenging problem of generating explanations for an agent's behavior over time in a dynamic environment.- Uses natural language rationales: The approach trains models to generate natural language rationales that sound human-like but do not necessarily reveal the true internal decision process. This differs from some interpretability methods that focus on visualizing internal representations.- Leverages human explanation data: A key contribution is the pipeline for collecting a corpus of human think-aloud explanations as they play a game. This natural language dataset is then used to train the rationale generation models.- Evaluates with user studies: The paper includes substantial evaluation of the generated rationales with real human subjects, assessing qualities like plausibility, confidence, preference between different styles of rationales. This user-centered evaluation is still relatively rare in XAI research.- Compares different input configurations: An interesting aspect is showing how tweaking the inputs to the rationale generator (e.g. focused vs complete view) leads to qualitative differences in the rationales (concise vs detailed).Overall, the use of human explanation data, focus on sequential tasks, and evaluation via user studies help differentiate this work from much of the existing research in explainable AI. The paper makes a nice contribution in advancing XAI techniques for dynamic environments.
