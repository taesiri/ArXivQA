# [Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large   Language Models](https://arxiv.org/abs/2404.00884)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT-3 show impressive few-shot learning capabilities, adapting to new tasks with just a few demonstrations. However, performance heavily relies on the quality and relevance of provided demos. 
- When faced with out-of-demonstration (OOD) queries that differ from available demos, existing methods may fail as they cannot cover all potential inputs.

Proposed Solution: 
- The paper proposes Self-Demos, a novel prompting method to elicit the inherent OOD generalizability in LLMs. 
- It generates query-aware demos by strategically interpolating between existing demos and the OOD query. This transforms the query to be in-demonstration.
- The method has pre- and post-processing around demo generation:
   - Pre-process: Guide model to give query understanding
   - Generate demos with seed demos and understanding  
   - Post-process: Select best demos via best-of-N sampling

Main Contributions:
- Proposes Self-Demos, an effective prompting method for handling OOD queries in LLMs
- Constructs OOD-Toolset, a new dataset with 300+ APIs and 1000 queries in tool-using domain specifically for OOD evaluation 
- Shows superior performance over baselines in addressing OOD queries on OOD-Toolset and math datasets 
- Performs extensive analyses like ablation studies to provide insights into the method

In summary, the paper presents Self-Demos to elicit OOD generalizability in LLMs by strategic demo generation and selection, achieving state-of-the-art performance on OOD queries. The constructed dataset and analyses also facilitate better understanding of models' generalization capability.
