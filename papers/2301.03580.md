# [Designing BERT for Convolutional Networks: Sparse and Hierarchical   Masked Modeling](https://arxiv.org/abs/2301.03580)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we extend the success of BERT-style masked image modeling pre-training from vision transformers to convolutional neural networks (convnets)?

The key challenges identified are:

1) Convolution operations cannot handle irregular, randomly masked input images like transformers can. 

2) The single-scale nature of BERT pre-training is inconsistent with the hierarchical structure of convnets.

To address these challenges, the main contributions of the paper are:

1) Using sparse convolution to encode the unmasked image patches, treating them as sparse voxels in a 3D point cloud. This allows handling of irregular masked inputs.

2) Designing a hierarchical decoder to reconstruct images from multi-scale encoded features. This takes advantage of the hierarchical representation in convnets. 

3) Proposing the SparK (Sparse masKed modeling) framework that integrates these solutions into a BERT-style pre-training approach for convnets.

By evaluating SparK on ImageNet classification and COCO detection/segmentation, the authors demonstrate superior transfer learning performance compared to prior self-supervised methods for both convnets and transformers. This shows the promise of extending masked modeling to convnets through solutions tackling their architectural differences from transformers.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes SparK (Sparse masked modeling with hierarchy), the first BERT-style pre-training method that can be directly applied to convolutional neural networks (convnets) without any backbone modifications. 

2. It identifies and overcomes two key obstacles in applying masked image modeling to convnets: (a) convnets cannot handle irregular, randomly masked input images; (b) the single-scale nature of BERT pre-training is inconsistent with the hierarchical structure of convnets.

3. It introduces two key techniques to overcome the above issues:

- Uses sparse convolution to encode the unmasked image patches, treating them as sparse voxels in 3D point clouds. This is the first work using sparse convolution for 2D masked modeling.

- Employs a hierarchical decoder to reconstruct images from multi-scale encoded features, embracing the advantage of convnet's hierarchy.

4. Extensive experiments show SparK significantly outperforms previous state-of-the-art contrastive learning and transformer-based masked modeling methods on various downstream tasks. It demonstrates the promise of extending BERT-style pre-training to convnets.

5. Ablation studies validate the importance of the two key techniques (sparse convolution and hierarchical decoding) introduced in SparK.

In summary, the key contribution is proposing a general framework SparK that makes BERT-style masked modeling suitable for convolutional networks, achieving new state-of-the-art results and showing the potential of generative pre-training on convnets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes SparK, a method to adapt BERT-style masked image modeling pre-training to convolutional neural networks by using sparse convolutions to handle irregular masked inputs and a hierarchical decoder to reconstruct multi-scale features.
