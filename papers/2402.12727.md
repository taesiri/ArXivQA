# [Diffusion Posterior Sampling is Computationally Intractable](https://arxiv.org/abs/2402.12727)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Diffusion models have become very popular for unconditional sampling and generative modeling of complex distributions like images. Many works have proposed using them as priors for downstream tasks like super-resolution, inpainting etc.

- Several algorithms have been recently proposed for posterior sampling from the diffusion model conditioned on observations like noisy image patches. However, none of these algorithms have theoretical guarantees of fast convergence to the true conditional distribution. 

- This paper studies if efficient and accurate posterior sampling is theoretically possible given approximate smoothed score representations of the diffusion model.

Key Contributions:

1. The paper proves that under standard cryptographic assumptions (existence of one-way functions), there exist well-modeled diffusion model priors for which posterior sampling requires superpolynomial time.

2. They construct an explicit distribution over which the smoothed score is representable by a small ReLU network, allowing efficient unconditional sampling. However, posterior sampling given partial linear observations requires inverting a cryptographic one-way function embedded in the distribution.

3. Consequently, rejection sampling, which takes exponential time in general, turns out to be nearly optimal. Under a stronger assumption that there exist one-way functions requiring exponential time to invert, rejection sampling is optimal up to log factors.

4. Therefore, the paper establishes fundamental limitations in conditional sampling for diffusion models - no general algorithm can efficiently sample from the true posterior in the worst case over well-modeled priors. Additional assumptions are necessary to make theoretical progress.

In summary, the key contribution is a negative result - establishing limitations of posterior sampling by relating it to cryptographic hardness assumptions, and showing that rejection sampling is essentially optimal.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper shows that under basic cryptographic assumptions, posterior sampling from diffusion models is computationally intractable in the worst case, with rejection sampling being nearly optimal.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proving computational intractability of posterior sampling for diffusion models. Specifically, the paper shows that under basic cryptographic assumptions like the existence of one-way functions, there exist instances of diffusion models where every algorithm for posterior sampling takes superpolynomial time, even though unconditional sampling is efficient. The paper also shows that rejection sampling is nearly optimal by giving a matching exponential lower bound under a slightly stronger assumption. Overall, this is an important negative result that rules out the existence of general efficient algorithms for posterior sampling with diffusion models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's content, some of the key terms and concepts associated with it include:

- Diffusion models - The paper focuses on diffusion models for image generation and how to sample from them. These models learn distributions over images.

- Posterior sampling - A key problem studied is how to sample from the posterior distribution over images given a noisy measurement, which is useful for tasks like super-resolution or inpainting. 

- Computational intractability - The main result is that posterior sampling is computationally intractable in general, under basic cryptographic assumptions.

- One-way functions - The hardness of posterior sampling is shown by constructing an instance based on a one-way function, which is believed to take superpolynomial time to invert.

- Well-modeled distributions - Distributions that have smoothed scores accurately representable by a polynomial-size neural network. It is shown the hardness holds even for such "nice" distributions.

- Rejection sampling - An algorithm for posterior sampling that takes exponential time in general. The paper shows this exponential time dependence is inevitable.

- Unconditional sampling - Sampling from the original distribution without any conditioning measurements, for which diffusion models give efficient algorithms.

So in summary, key terms revolve around diffusion models, the problem of posterior sampling, computational complexity, cryptographic assumptions, and the contrast with unconditional sampling.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper shows that posterior sampling from diffusion models is computationally intractable in general. Can you describe the construction of the "well-modeled" distribution used in the lower bound proof that makes posterior sampling hard? What properties does it satisfy?

2. One of the key components in the lower bound proof is translating a one-way function into the phase offset of a discretized Gaussian. Can you walk through why this connection makes posterior sampling hard, while still allowing efficient unconditional sampling? 

3. The paper analyzes the time complexity of rejection sampling for the posterior. Can you derive the time complexity formula step-by-step? What are the key lemmas used in the analysis? How tight is the bound?

4. The upper bound proof relies on analyzing the acceptance probability at each round of rejection sampling. Walk through the details of how the acceptance probability is lower bounded as a function of the noise level $\beta$. What is the intuition?

5. The paper shows that the exponential-time rejection sampling algorithm is nearly optimal under certain cryptographic assumptions. Can you describe what those assumptions are and why they imply the optimality of rejection sampling? 

6. The lower bound distribution has the property that its scores are well-approximated by a neural network. Walk through how this approximation is constructed and analyze its accuracy. Why is this property important?

7. One of the paper's contributions is making a formal distinction between unconditional sampling and posterior sampling. Contrast these two tasks and explain why unconditional sampling is easy while posterior sampling is hard for diffusion models.

8. The paper focuses on the linear Gaussian measurement model. How might the analysis change for different observation models such as low-rank matrix sensing or 1-bit compressed sensing?

9. The lower bound suggests that approximations are necessary for efficient posterior sampling. What types of approximation algorithms are currently used in practice and what are their limitations?

10. The paper shows limitations of diffusion models for reconstructing images from partial observations. How might these limitations be addressed - via better algorithms, better modeling, or both? What are promising future directions?
