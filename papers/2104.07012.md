# [Sparse Attention with Linear Units](https://arxiv.org/abs/2104.07012)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper aims to address is how to develop a sparse attention mechanism that is easy to implement, computationally efficient, and performs comparably to dense attention in terms of model accuracy. Specifically, the paper proposes a new attention mechanism called Rectified Linear Attention (ReLA) that replaces the softmax activation in standard attention with a ReLU activation. This results in automatic sparsity as the ReLU drops negative attention scores. The key research questions explored are:- Can simply replacing softmax with ReLU and adding normalization produce a working sparse attention mechanism for Transformer models? - How does ReLA compare to dense attention and other sparse attention methods like sparsemax in terms of model accuracy, training efficiency, inference speed, and induced sparsity?- Does the automatic sparsity from ReLA provide any benefits in terms of interpretability or model analysis compared to dense attention?The central hypothesis is that ReLA can achieve comparable accuracy to dense attention while being simpler and more efficient than other sparse attention methods. The experiments aim to validate whether this hypothesis holds across various machine translation tasks and model configurations.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes Rectified Linear Attention (ReLA), a novel sparse attention mechanism that replaces the softmax activation in standard attention with a ReLU activation. This results in automatic sparsity without needing complex inference algorithms like prior sparse attention methods.- ReLA is shown to achieve comparable translation performance to strong baselines on five machine translation tasks, while having similar training and inference speed as standard softmax attention.- Analysis shows that ReLA induces sparse and diverse attention heads, with accuracy on word alignment that is better than prior sparse attention methods. - Uniquely, ReLA allows attention heads to assign a total attention of zero to some queries, leading to highly specialized heads. The paper shows this "null attention" can be indicative of translation quality.In summary, the key novelty is the simple but effective ReLU formulation for sparse attention, which gives both efficiency and interesting properties like null attention. ReLA seems to be an interpretable and efficient drop-in replacement for standard softmax attention.
