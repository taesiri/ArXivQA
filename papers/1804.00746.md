# [The simple essence of automatic differentiation](https://arxiv.org/abs/1804.00746)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can reverse-mode automatic differentiation (RAD) be specified and implemented in a simple, correct-by-construction manner without mutation, graphs, tapes, etc?The paper aims to develop a simple, generalized AD algorithm calculated from a natural specification in terms of elementary category theory. It then specializes this algorithm by varying the representation of derivatives, obtaining much simpler RAD algorithms than previously known. In particular, the paper explores using linear maps, generalized matrices, continuations, and dual spaces as representations of derivatives. Applying continuations and dual spaces yields RAD algorithms that are free of mutation and hence naturally parallelizable. The dual space version also appears efficient as it does not require matrix computations.Overall, the paper seeks to develop RAD algorithms that are simpler, more parallel, and usable directly from an existing programming language compared to conventional approaches involving graphs, tapes, and mutation. The goal is a cleaner foundation for machine learning and other uses of gradient-based optimization.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Developing a simple, general algorithm for automatic differentiation (AD) by specifying it homomorphically in terms of elementary category theory abstractions like functors. This allows calculating correct implementations.2. Showing how this algorithm can be specialized and made more efficient by varying the representation of derivatives, including functions, generalized matrices, continuations, and dual spaces. 3. Demonstrating that using a dual space representation of derivatives leads to a particularly simple reverse mode AD algorithm. This algorithm is simpler than conventional reverse mode AD algorithms and naturally parallel without requiring graphs or mutation.4. Providing a precise mathematical specification of reverse mode AD using continuations and duality. This allows calculating a correct implementation.5. Showing how this categorical framework allows generalizing AD, such as by replacing linear maps with other categories. This results in a family of algorithms for various notions of "derivative."6. Illustrating through examples how this approach allows implementing AD in terms of familiar functional programming constructs, without the need for special AD-specific data types or programming styles.In summary, the key innovations seem to be using category theory to specify and calculate simple yet general AD algorithms, and using dual spaces and continuations to arrive at a new, simplified perspective on reverse mode AD. The overall approach provides a clean mathematical foundation for AD in functional programming.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Unfortunately I cannot provide a full summary of the paper without reading it. However, based on the title "The Simple Essence of Automatic Differentiation", it seems the paper is about presenting a simplified understanding of automatic differentiation, which is a method for efficiently computing derivatives of functions. The key point may be that the author has found a way to explain automatic differentiation that removes unnecessary complexity. But this is just speculation without reading the full paper. Please provide the paper text if you would like me to summarize the key points in more detail.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper takes a programming languages/functional programming perspective on automatic differentiation (AD), especially reverse mode AD. Much prior work has come from the numerical analysis and machine learning communities. The functional programming viewpoint provides a fresh look at AD.- The key insight is to specify AD using basic categorical concepts like functors, then derive correct implementations from these high-level specifications. This approach is elegant and provides important theoretical guarantees, but is fairly uncommon in the AD literature.- Most AD research focuses on efficient incremental algorithms that propagate derivatives backwards through computations. This paper shows reverse mode AD can be specified and implemented in a simpler, non-mutating, and more parallel style.- The generalized AD algorithm based on replacing linear maps with an arbitrary cartesian category is novel. It enables swapping in different representations of derivatives while reusing the overall AD derivation.- The dual number approach to reverse mode AD seems much simpler than conventional approaches based on taping, graphs, etc. It removes the need to build intermediate data structures. However, its efficiency compared to state-of-the-art AD is unclear.- The emphasis on compositionality, functoriality, and calculation/derivation of programs is quite different than most AD papers, which focus more on algorithms and applications.- The idea of "differentiable functional programming" and use of a compiler plugin to automate lifting code to a differential setting is powerful, though hasn't gotten much attention in the AD community.Overall, I'd say the paper brings a fresh programming languages perspective to AD, with new conceptual insights and techniques. The theoretical development is thorough. Whether the ideas translate to practical efficiency gains remains an open question.
