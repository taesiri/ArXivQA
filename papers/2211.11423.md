# [Blur Interpolation Transformer for Real-World Motion from Blur](https://arxiv.org/abs/2211.11423)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

- How can we effectively utilize the temporal information present in motion blur to reconstruct a high frame rate sharp video sequence from a low frame rate blurry video? The authors aim to address the challenging problem of joint deblurring and interpolation or "blur temporal super-resolution".

- How can we improve upon current methods, which still leave room for improvement in visual quality and struggle to generalize to real-world blurry videos? The authors propose a new model architecture and training strategies to enhance performance. 

- How can we collect a real-world dataset to enable models to generalize to real blurry videos rather than just performing well on synthetic data? The authors present a new real-world dataset of aligned blurry and sharp video pairs.

In summary, the main research focuses seem to be:

1) Proposing a new transformer-based model architecture and training strategies (dual-end temporal supervision, temporally symmetric ensembling) to improve blur interpolation performance, especially for arbitrary time motion reconstruction.

2) Introducing the first real-world blurry/sharp video dataset to address the generalization issue from synthetic to real data that prior works face.

3) Validating the proposed model and real-world dataset through extensive experiments, showing state-of-the-art results on public benchmarks and real-world data.

So in essence, the paper aims to push the state-of-the-art in joint deblurring and interpolation by improving model architecture and leveraging a new real-world dataset. The ability to extract arbitrary time motions from blur in high quality is a key focus.
