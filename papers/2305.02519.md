# [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning   over Untrimmed Videos](https://arxiv.org/abs/2305.02519)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we build a large-scale video question answering benchmark that supports fine-grained compositional reasoning over untrimmed videos? 

The key points are:

- Existing video QA benchmarks have limitations in generating fine-grained questions that require reasoning about detailed video semantics. 

- The authors introduce ANetQA, a new benchmark built on top of ActivityNet videos, to address this limitation.

- The key features of ANetQA are:

1) Untrimmed long videos with rich semantics 

2) Fine-grained spatio-temporal scene graph annotations

3) Massive and diverse QA pairs generated from fine-grained templates

4) Benchmark size is an order of magnitude larger than previous ones

- Experiments show current QA models are far from human performance, indicating challenge of ANetQA and room for improvement.

In summary, the main research question is how to build a large-scale video QA benchmark that can effectively evaluate fine-grained compositional reasoning, which ANetQA aims to address through its design and scale.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing ANetQA, a large-scale video question answering benchmark that supports fine-grained compositional reasoning over untrimmed videos. Key highlights include:

- The benchmark is built on top of over 11K untrimmed videos from ActivityNet, with fine-grained annotations of objects, relationships, attributes, and actions over spatio-temporal scene graphs. 

- It contains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than previous video QA benchmarks like AGQA.

- The questions are automatically generated from handcrafted templates based on the scene graph annotations, enabling fine-grained compositional reasoning abilities to be measured. 

- New question types involving attributes are introduced compared to previous benchmarks like AGQA, requiring more detailed video understanding.

- Comprehensive experiments and analyses are provided using state-of-the-art video QA models, with substantial room for improvement over the human performance.

In summary, the key contribution is the introduction and thorough evaluation of ANetQA as a new large-scale benchmark to assess fine-grained reasoning abilities for video question answering over complex real-world videos.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper introduces ANetQA, a new large-scale video question answering benchmark built on top of fine-grained video scene graph annotations from ActivityNet to enable compositional reasoning over complex untrimmed web videos.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related research:

- This paper introduces a new VideoQA benchmark ANetQA for fine-grained compositional reasoning over untrimmed web videos. Other major VideoQA benchmarks use either synthetic videos (e.g. CLEVRER, MarioQA) or simpler real-world videos (e.g. MovieQA, TVQA, ActivityNet-QA). ANetQA uses more complex videos from ActivityNet.

- The QA pairs in ANetQA are automatically generated from spatio-temporal scene graphs annotated by humans. This is similar to the recent AGQA benchmark, but ANetQA uses more fine-grained scene graph annotations with additional attributes. 

- With 1.4B QA pairs, ANetQA is orders of magnitude larger than previous real-world VideoQA benchmarks like MovieQA (150K QA pairs) and ActivityNet-QA (58K QA pairs). The only larger benchmark is the synthetic HowTo100M.

- ANetQA claims to have better control over language biases compared to benchmarks that rely on captions or free-form human-generated questions. The scene graph based automatic QA generation helps reduce biases.

- The paper presents comprehensive experiments and analysis of several state-of-the-art VideoQA models on ANetQA. Performance is significantly lower compared to other benchmarks, showing ANetQA is more challenging.

- The best model obtains 44.5% accuracy on ANetQA while human performance is 84.5%, indicating significant room for improvement. Other benchmarks like ActivityNet-QA have smaller human-model gaps.

In summary, ANetQA pushes VideoQA to more complex real-world videos and fine-grained reasoning compared to prior benchmarks, while utilizing scene graphs and automatic QA generation to provide control and scale. The large performance gap shows it poses new challenges to current models.
