# [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning   over Untrimmed Videos](https://arxiv.org/abs/2305.02519)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we build a large-scale video question answering benchmark that supports fine-grained compositional reasoning over untrimmed videos? 

The key points are:

- Existing video QA benchmarks have limitations in generating fine-grained questions that require reasoning about detailed video semantics. 

- The authors introduce ANetQA, a new benchmark built on top of ActivityNet videos, to address this limitation.

- The key features of ANetQA are:

1) Untrimmed long videos with rich semantics 

2) Fine-grained spatio-temporal scene graph annotations

3) Massive and diverse QA pairs generated from fine-grained templates

4) Benchmark size is an order of magnitude larger than previous ones

- Experiments show current QA models are far from human performance, indicating challenge of ANetQA and room for improvement.

In summary, the main research question is how to build a large-scale video QA benchmark that can effectively evaluate fine-grained compositional reasoning, which ANetQA aims to address through its design and scale.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing ANetQA, a large-scale video question answering benchmark that supports fine-grained compositional reasoning over untrimmed videos. Key highlights include:

- The benchmark is built on top of over 11K untrimmed videos from ActivityNet, with fine-grained annotations of objects, relationships, attributes, and actions over spatio-temporal scene graphs. 

- It contains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than previous video QA benchmarks like AGQA.

- The questions are automatically generated from handcrafted templates based on the scene graph annotations, enabling fine-grained compositional reasoning abilities to be measured. 

- New question types involving attributes are introduced compared to previous benchmarks like AGQA, requiring more detailed video understanding.

- Comprehensive experiments and analyses are provided using state-of-the-art video QA models, with substantial room for improvement over the human performance.

In summary, the key contribution is the introduction and thorough evaluation of ANetQA as a new large-scale benchmark to assess fine-grained reasoning abilities for video question answering over complex real-world videos.
