# [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning   over Untrimmed Videos](https://arxiv.org/abs/2305.02519)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we build a large-scale video question answering benchmark that supports fine-grained compositional reasoning over untrimmed videos? 

The key points are:

- Existing video QA benchmarks have limitations in generating fine-grained questions that require reasoning about detailed video semantics. 

- The authors introduce ANetQA, a new benchmark built on top of ActivityNet videos, to address this limitation.

- The key features of ANetQA are:

1) Untrimmed long videos with rich semantics 

2) Fine-grained spatio-temporal scene graph annotations

3) Massive and diverse QA pairs generated from fine-grained templates

4) Benchmark size is an order of magnitude larger than previous ones

- Experiments show current QA models are far from human performance, indicating challenge of ANetQA and room for improvement.

In summary, the main research question is how to build a large-scale video QA benchmark that can effectively evaluate fine-grained compositional reasoning, which ANetQA aims to address through its design and scale.
