# [Multi-modal preference alignment remedies regression of visual   instruction tuning on language model](https://arxiv.org/abs/2402.10884)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-modal large language models (MLLMs) suffer from performance degradation on language tasks after being fine-tuned on visual tasks. This happens because visual datasets like VQA lack the complexity and diversity compared to language datasets the model was originally trained on.

- Existing methods to mitigate this issue either require mixing large amounts of text data while fine-tuning or complex training pipelines like in LLaVA-RLHF.

Solution:
- The paper proposes using distillation and direct preference optimization (DPO) to align MLLMs with human preferences and reconcile language and visual capabilities. 

- A small multi-modal preference dataset is collected where answers are annotated by Gemini on 5 quality metrics. DPO, rejection sampling and steerLM are evaluated as alignment strategies.

Key Findings:
- DPO is found to be highly effective, surpassing language capabilities of the original LM Vicuna, achieving 6.73 on MT-Bench compared to Vicuna's 6.57 and LLaVA's 5.99.

- DPO also improved visual instruction performance (+4.9% on MM-Vet, +6% on LLaVA-Bench) with minimal impact on visual knowledge benchmarks.

Main Contributions:
- First work to identify and address multi-modal performance degradation in MLLMs due to catastrophic forgetting.

- Proposes innovative preference alignment framework using distillation and DPO that restores language capabilities and enhances both text and visual performance.

- Introduces efficient annotation scheme and demonstrates aligning with only 5k examples using DPO, unlike prior work requiring >80k samples.

In summary, the paper makes significant contributions in reconciling language and visual capabilities in MLLMs via efficient preference alignment. DPO is shown to be highly promising for multi-modal alignment.
