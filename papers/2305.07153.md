# [Towards best practices in AGI safety and governance: A survey of expert   opinion](https://arxiv.org/abs/2305.07153)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: What safety and governance practices do leading experts think AGI labs should implement? The authors surveyed 51 experts from AGI labs, academia, and civil society organizations. They presented these experts with 50 statements about potential safety and governance practices that AGI labs could implement. The experts were then asked to indicate their level of agreement with each statement. The central goal was to gauge expert opinions on what AGI labs should do to safely develop artificial general intelligence (AGI). The results provide insights into areas of consensus and disagreement among experts regarding best practices for AGI labs. This can help inform efforts by the industry, regulators, and standard-setting bodies to establish safety protocols, regulations, and standards for organizations pursuing AGI development.In summary, the key research question is focused on eliciting expert opinions on safety and governance best practices that should be implemented by companies/labs trying to develop AGI. The authors aggregate these expert views to provide guidance on emerging consensus in this important area.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is presenting the results of a survey of expert opinions on safety and governance practices that AGI labs should implement. The key findings are:- There was broad consensus among the 51 experts surveyed that AGI labs should implement most of the 50 safety and governance practices listed in the survey. On average, 85% of respondents agreed that AGI labs should follow each practice. - Practices receiving the highest levels of agreement (98% agreement) were: conducting pre-deployment risk assessments, evaluating models for dangerous capabilities, commissioning third-party model audits, establishing safety restrictions on model usage, and red teaming.- Respondents from AGI labs showed significantly higher overall agreement with the practices than respondents from academia or civil society. However, no significant differences were found at the item-level.- Respondents suggested an additional 50 unique practices, indicating the list in the survey was not comprehensive. The authors suggest these findings can serve as an initial foundation to develop best practices and standards for AGI labs. The results indicate areas of consensus to build on as well as areas needing further discussion and research.In summary, the key contribution is presenting expert opinions on AGI safety and governance practices, revealing a high level of consensus on implementing most practices as well as areas needing further research and discussion. The results provide an initial evidence base to inform efforts to develop standards and regulations for AGI labs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents the results of a survey of 51 leading experts in AI safety and governance, finding broad consensus that AGI labs should implement a wide range of safety and governance practices to reduce risks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on AI safety and governance practices:- Scope: This paper focuses specifically on practices for organizations trying to build artificial general intelligence (AGI). Much of the existing research looks more broadly at AI safety for narrow AI or machine learning systems. There are some parallel efforts to create standards for "general purpose" AI systems, but not focused solely on AGI. So this paper has a fairly unique and focused scope.- Methodology: The methodology of surveying experts is common in AI safety research. However, this survey has a larger sample size (51 respondents) compared to previous expert surveys in this field. The sample also seems more systematically constructed rather than a convenience sample. The main limitations are the small overall sample size and potential biases in the sampling frame.- Results: The finding of high agreement with most proposed practices is fairly unique. Many past surveys show more disagreement or heterogeneity of views. This could be partly due to the framing of abstract practices instead of concrete mechanisms. There are also no major differences found between groups, whereas past work often identifies divides between researchers from different backgrounds.- Policy implications: The paper clearly spells out actionable implications from the results for AGI labs, regulators, and standard-setting bodies. Many papers in this field are more conceptual without concrete policy recommendations. However, others like the Partnership on AI's process also aim to develop shared practices.- Theory: The paper does not engage deeply with academic theories or concepts, which much published research does. However, it is oriented towards a practical impact goal. The motivation draws on governance theory around best practices informing standards and regulations.Overall, I would say the paper makes a novel and useful contribution due to its focused scope, systematic approach, actionable findings, and applied goal of informing policy. The direct expert elicitation method is fairly standard, but implemented rigorously. Compared to pure academic research, the paper prioritizes practical impact over theoretical novelty.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:- Conducting future surveys and expert elicitation work to address the acknowledged limitations of their study, including surveying a larger and more comprehensive sample in a more systematic way. The surveys could also include the additional practices suggested by participants. - Exploring the rationale behind experts' views on each practice through more in-depth discussion and research. This could help determine the key considerations and concerns towards implementation of the practices.- Creating more concrete and specific instantiations of each practice, figuring out the practical details of how to implement them effectively.- Conducting research on the proposals and open questions identified during the workshop, such as how to adapt existing frameworks like NIST's to an AGI context, how to test AI safety in a falsifiable way, and what constitutes a robust auditing ecosystem.- Promoting inclusive processes to develop best practices, including public surveys and participatory methods to include diverse stakeholders. - Conducting detailed analysis of existing practices at AGI labs to enable gap analyses and benchmarking.- Research into an idealized system card for AGI labs as a tool for governance.- Developing appropriate enforcement and auditing mechanisms to ensure adherence to established best practices.So in summary, the authors call for more research across all aspects of the AGI safety and governance landscape - from further eliciting expert views, to making practices concrete, to implementation and enforcement. They highlight the need for continued collaborative efforts between technical and governance experts to make progress.
