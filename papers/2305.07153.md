# [Towards best practices in AGI safety and governance: A survey of expert   opinion](https://arxiv.org/abs/2305.07153)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: What safety and governance practices do leading experts think AGI labs should implement? The authors surveyed 51 experts from AGI labs, academia, and civil society organizations. They presented these experts with 50 statements about potential safety and governance practices that AGI labs could implement. The experts were then asked to indicate their level of agreement with each statement. The central goal was to gauge expert opinions on what AGI labs should do to safely develop artificial general intelligence (AGI). The results provide insights into areas of consensus and disagreement among experts regarding best practices for AGI labs. This can help inform efforts by the industry, regulators, and standard-setting bodies to establish safety protocols, regulations, and standards for organizations pursuing AGI development.In summary, the key research question is focused on eliciting expert opinions on safety and governance best practices that should be implemented by companies/labs trying to develop AGI. The authors aggregate these expert views to provide guidance on emerging consensus in this important area.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is presenting the results of a survey of expert opinions on safety and governance practices that AGI labs should implement. The key findings are:- There was broad consensus among the 51 experts surveyed that AGI labs should implement most of the 50 safety and governance practices listed in the survey. On average, 85% of respondents agreed that AGI labs should follow each practice. - Practices receiving the highest levels of agreement (98% agreement) were: conducting pre-deployment risk assessments, evaluating models for dangerous capabilities, commissioning third-party model audits, establishing safety restrictions on model usage, and red teaming.- Respondents from AGI labs showed significantly higher overall agreement with the practices than respondents from academia or civil society. However, no significant differences were found at the item-level.- Respondents suggested an additional 50 unique practices, indicating the list in the survey was not comprehensive. The authors suggest these findings can serve as an initial foundation to develop best practices and standards for AGI labs. The results indicate areas of consensus to build on as well as areas needing further discussion and research.In summary, the key contribution is presenting expert opinions on AGI safety and governance practices, revealing a high level of consensus on implementing most practices as well as areas needing further research and discussion. The results provide an initial evidence base to inform efforts to develop standards and regulations for AGI labs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents the results of a survey of 51 leading experts in AI safety and governance, finding broad consensus that AGI labs should implement a wide range of safety and governance practices to reduce risks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on AI safety and governance practices:- Scope: This paper focuses specifically on practices for organizations trying to build artificial general intelligence (AGI). Much of the existing research looks more broadly at AI safety for narrow AI or machine learning systems. There are some parallel efforts to create standards for "general purpose" AI systems, but not focused solely on AGI. So this paper has a fairly unique and focused scope.- Methodology: The methodology of surveying experts is common in AI safety research. However, this survey has a larger sample size (51 respondents) compared to previous expert surveys in this field. The sample also seems more systematically constructed rather than a convenience sample. The main limitations are the small overall sample size and potential biases in the sampling frame.- Results: The finding of high agreement with most proposed practices is fairly unique. Many past surveys show more disagreement or heterogeneity of views. This could be partly due to the framing of abstract practices instead of concrete mechanisms. There are also no major differences found between groups, whereas past work often identifies divides between researchers from different backgrounds.- Policy implications: The paper clearly spells out actionable implications from the results for AGI labs, regulators, and standard-setting bodies. Many papers in this field are more conceptual without concrete policy recommendations. However, others like the Partnership on AI's process also aim to develop shared practices.- Theory: The paper does not engage deeply with academic theories or concepts, which much published research does. However, it is oriented towards a practical impact goal. The motivation draws on governance theory around best practices informing standards and regulations.Overall, I would say the paper makes a novel and useful contribution due to its focused scope, systematic approach, actionable findings, and applied goal of informing policy. The direct expert elicitation method is fairly standard, but implemented rigorously. Compared to pure academic research, the paper prioritizes practical impact over theoretical novelty.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:- Conducting future surveys and expert elicitation work to address the acknowledged limitations of their study, including surveying a larger and more comprehensive sample in a more systematic way. The surveys could also include the additional practices suggested by participants. - Exploring the rationale behind experts' views on each practice through more in-depth discussion and research. This could help determine the key considerations and concerns towards implementation of the practices.- Creating more concrete and specific instantiations of each practice, figuring out the practical details of how to implement them effectively.- Conducting research on the proposals and open questions identified during the workshop, such as how to adapt existing frameworks like NIST's to an AGI context, how to test AI safety in a falsifiable way, and what constitutes a robust auditing ecosystem.- Promoting inclusive processes to develop best practices, including public surveys and participatory methods to include diverse stakeholders. - Conducting detailed analysis of existing practices at AGI labs to enable gap analyses and benchmarking.- Research into an idealized system card for AGI labs as a tool for governance.- Developing appropriate enforcement and auditing mechanisms to ensure adherence to established best practices.So in summary, the authors call for more research across all aspects of the AGI safety and governance landscape - from further eliciting expert views, to making practices concrete, to implementation and enforcement. They highlight the need for continued collaborative efforts between technical and governance experts to make progress.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper reports the results of a survey of 51 leading experts in AGI safety and governance from AGI labs, academia, and civil society. The survey asked respondents to indicate their level of agreement with 50 statements about what practices AGI labs should implement to reduce risks, covering areas like development, deployment, monitoring, risk management, external scrutiny, information security, communication, and culture. The main finding is that there was a broad consensus in favor of implementing most of the 50 practices. On average, 85% of respondents either somewhat or strongly agreed that AGI labs should follow each practice. There were extremely high agreement levels for practices like pre-deployment risk assessments, dangerous capabilities evaluations, third-party audits, safety restrictions, and red teaming. The paper concludes that the list of practices with broad expert endorsement serves as a helpful foundation for efforts to develop best practices, standards, and regulations for responsible AGI development. It suggests the results can inform voluntary industry initiatives, standard-setting processes, and regulatory efforts.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper reports the results of a survey of 51 leading experts in artificial general intelligence (AGI) safety and governance. The goal was to identify consensus around best practices that AGI labs like OpenAI, Google DeepMind, and Anthropic should implement to reduce risks. Participants were presented with 50 statements about AGI safety and governance practices and asked their level of agreement. The main finding is that there was broad consensus in favor of nearly all practices presented. On average, 85% of respondents agreed AGI labs should implement each practice. Extremely high agreement (98%) was found for practices like conducting pre-deployment risk assessments, evaluating models for dangerous capabilities, having third-party audits, imposing safety restrictions on models, and commissioning red teams. This suggests the presented list could serve as a foundation for developing standards and regulations for AGI labs. The paper discusses implications for AGI labs and regulators. It also acknowledges limitations like the small sample size and lack of precision around how practices would be implemented. Overall, it makes a valuable contribution to the debate around safety practices for organizations pursuing artificial general intelligence.


## Summarize the main method used in the paper in one paragraph.

The main method used in this paper is an expert survey. The authors surveyed 51 leading experts from AGI labs, academia, and civil society. They selected experts based on their knowledge and experience in AGI safety and governance. The survey asked experts to what extent they agree or disagree with 50 statements about what AGI labs should do to reduce risks. After completing the survey, respondents could also suggest additional practices they felt were missing. The survey used a 5-point Likert scale for the level of agreement. It also asked about respondents' gender and sector of work. The authors analyzed the results by calculating the level of overall agreement and differences between groups. They tested for statistically significant differences using Mann-Whitney U tests and Chi-squared tests.In summary, the main method was an expert survey of 51 leading researchers and practitioners. It elicited their level of agreement with 50 proposed AGI safety and governance practices, as well as suggestions for additional practices. The results were analyzed to determine the overall level of agreement and differences between demographic groups.
