# ZipIt! Merging Models from Different Tasks without Training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that it is possible to merge models trained on completely different tasks without any additional training. Specifically, the paper introduces a method called "ZipIt!" that aims to merge models with different initializations that were trained on disjoint datasets and label spaces. The key ideas and contributions are:- Prior work on model merging assumes the models lie in the same "loss basin" modulo permutation. This fails when merging models trained on different tasks, as they may lie in different loss basins.- ZipIt introduces two main strategies to enable merging models from different tasks:   1) Merging redundant features both within and across models, not just permuting one model onto the other.   2) Partially zipping models only up to a certain layer to create a multi-head model, rather than merging the entire network.- Experiments show ZipIt significantly outperforms baselines like weight averaging and prior work like Git Re-Basin when merging models trained on disjoint subsets of CIFAR and ImageNet categories.- Analysis provides insight into the method, showing the benefits of within-model merging, partial zipping, and increased model capacity.In summary, the main hypothesis is that models trained on completely different tasks can be merged without additional training using the proposed ZipIt approach, which is demonstrated through extensive experiments and analysis. The key ideas are merging within and across models and partial zipping.


## What is the main contribution of this paper?

This paper introduces "ZipIt!", a method for merging models trained on completely different tasks without any additional training. The key contributions are:- Proposes a general framework to merge models layer-by-layer by exploiting redundancy in their features. This allows merging models with different architectures trained on separate tasks.- Introduces two main strategies: 1) Allows merging features both within and across models, instead of just permuting one model to align with the other. 2) Partial zipping, where only initial layers are merged to create a multi-head model, avoiding issues when later layer features diverge. - Shows strong experimental results merging models trained on disjoint subsets of CIFAR and ImageNet categories. Significantly outperforms prior work like Git Re-Basin, and approaches ensemble performance.- Demonstrates merging models trained on completely separate datasets (Stanford Dogs, Oxford Pets, etc) without any training. Again outperforms baselines.- Analyzes the approach, showing the benefits of within-model merging and partial zipping. Reveals it makes effective use of model capacity.In summary, the main contribution is a general framework for merging models trained on separate tasks without any additional training. The key ideas are merging features within models and partial zipping to create multi-head models. Experiments show large improvements over prior work in this challenging setting.
