# [Context-aware Decoding Reduces Hallucination in Query-focused   Summarization](https://arxiv.org/abs/2312.14335)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Query-focused summarization (QFS) aims to generate a summary from a document that is relevant to answering a given query. 
- Applying large language models (LLMs) to QFS can lead to hallucination, where the generated text contradicts the source document.
- There is interest in developing new decoding methods that can improve faithfulness in conditional text generation like QFS.

Method:
- The paper studies Context-Aware Decoding (CAD), a recently proposed decoding method that uses pointwise mutual information to make generation more conditioned on the input context. 
- CAD modifies the decoding distribution by multiplying the vanilla logit with a term that measures association between predicted token and input context.

Experiments:
- Experiments compare CAD to vanilla decoding over 8 LLMs on 2 QFS and 2 news summarization datasets.
- CAD reduces factual errors/hallucinations measured by FactKB while retaining lexical similarity measured by ROUGE.
- There is a tradeoff between factuality and abstractiveness when varying the CAD hyperparameter.
- CAD improves quality but with extra compute cost and slower decoding.

Contributions:
- Provides large-scale study on effectiveness of CAD for improving faithfulness in QFS task.
- Shows CAD reduces hallucinations while maintaining level of abstraction.
- Analyzes computational overhead and sensitivity to hyperparameters of CAD.
- Implementation based on HuggingFace Transformers is open-sourced.
