# [RoadRunner -- Learning Traversability Estimation for Autonomous Off-road   Driving](https://arxiv.org/abs/2402.19341)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Autonomous high-speed off-road navigation is challenging as it requires accurately assessing terrain traversability and geometry from onboard sensors only. Degraded sensor inputs (e.g. motion blur, lighting changes) and limited update rates (especially for LiDAR) make this difficult. Existing methods rely on hand-crafted heuristics or semantics, have high latency, and don't leverage experience well. 

Proposed Solution:
The paper proposes RoadRunner, a deep neural network that takes camera and LiDAR inputs to simultaneously predict a traversability cost map and elevation map from a bird's eye view perspective. It is trained self-supervised on real-world driving data by using the existing X-Racer autonomy stack to generate pseudo-ground truth labels in hindsight. RoadRunner builds on top of a LiDAR backbone (PointPillars) and image backbone (EfficientNet-B0) by lifting image features into 3D and fusing modalities. Separate decoders are used for predicting elevation and traversability.

Main Contributions:
- RoadRunner network architecture that fuses cameras and LiDAR to estimate traversability costs and elevation 
- Framework to generate training data through hindsight by aggregating sensor data over time using X-Racer
- Overview and details of the sophisticated X-Racer autonomy software stack
- RoadRunner reduces latency 4x (to 140ms) and improves traversability MSE by 52.3% compared to X-Racer
- Evaluation on 16.5 km of off-road driving data demonstrating accuracy improvements

In summary, RoadRunner pushes the state-of-the-art in real-time traversability perception for high-speed off-road autonomy by effectively fusing visual and LiDAR cues in an end-to-end trained deep network.
