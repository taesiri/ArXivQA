# [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is:Given a fixed compute budget (i.e. number of accelerators and target training duration), how should one trade off model size and the number of training tokens when training a large language model?The key hypothesis is that current large language models are significantly undertrained, as there has been a focus on scaling up model size while keeping the training data size constant. The paper investigates the optimal balance between model size and training data size for a given compute budget.The authors approach this question by:- Training over 400 language models of varying sizes (70M to 16B parameters) on different amounts of training data (5B to 500B tokens)- Modeling the training loss as a function of model size and data size to estimate the optimal configuration- Predicting that given the compute used for a 280B model (Gopher), a 70B model trained on 4x more data would be optimal- Verifying this by training such a model (Chinchilla) and showing it outperforms Gopher across many benchmarksThe key conclusion is that model size and data size should be scaled equally - for every doubling of model size, the data size should also double. Current models are too large given their training data, and smaller models trained on more data can be more compute-optimal.


## What is the main contribution of this paper?

This paper investigates the optimal model size and amount of training data for training large transformer language models under a fixed computational budget. The key findings are:- Current large language models like GPT-3 and Jurassic-1 are significantly undertrained, as there has been a focus on scaling up model size without increasing the training data. - The authors train over 400 language models ranging from 70M to 16B parameters on 5B to 500B training tokens. Based on analyzing the training losses, they find that for optimal training efficiency, model size and training data should be scaled up equally - for every doubling of model size, the training data should also be doubled. - This is in contrast to prior work like Kaplan et al. 2020 which suggested model size should be increased much more than the amount of training data as compute budgets grow.- The authors validate their analysis by training Chinchilla, a 70B parameter model trained on 4x more data (1.4T tokens) than Gopher. Despite being 4x smaller, Chinchilla matches or exceeds Gopher's performance across a wide range of benchmarks.- Chinchilla's smaller size also makes fine-tuning and inference more efficient. The results suggest that current trends of massively scaling up model size with fixed data are suboptimal, and more effort should go into scaling up high-quality training datasets.In summary, the key contribution is an analysis showing model size and training data should be scaled equally for optimal training of large language models, validated by a smaller but better-trained model outperforming much larger models. The results highlight the need to create larger high-quality training datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper appears to be on optimizing the compute efficiency and performance of large language models. The key takeaway seems to be that current large language models are significantly undertrained, and that for optimal efficiency, model size and training data size should be scaled equally - for every doubling of model size, the training data size should also double. The authors put this into practice by training a smaller but more compute-efficient 70 billion parameter model called Chinchilla on much more data than Gopher, resulting in better performance. In summary: Larger language models are undertrained; scaling model size and data size equally is more compute-efficient.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of training large language models:- This paper focuses on optimizing the training of large language models by balancing model size and amount of training data. Much recent work has focused solely on scaling up model size, keeping training data fixed. So this provides a useful counterpoint and investigation into also scaling up training data. - The paper introduces a new model called Chinchilla which is smaller but trained on more data than Gopher. Testing this model is a novel contribution compared to other work that has not explored this direction of smaller but longer trained models.- The paper aims to provide guidance on how to optimally trade off model size and training data given a fixed compute budget. This kind of analysis has been done before (e.g. Kaplan et al. 2020) but the authors argue previous work has underestimated the value of training on more data. The analysis here to determine optimal scaling rules is more comprehensive.- The comparison of Chinchilla against models like GPT-3 and Turing NLG on various benchmarks moves beyond just training set metrics to rigorously test capabilities on language tasks. This allows for more meaningful comparisons to other recent models.- The model card provides transparency about model development, evaluation, and ethical considerations. Publishing this information allows for accountability and reproducibility, following recommendations like Mitchell et al. 2019.Overall, I would say this paper makes excellent contributions through its focus on data scaling, extensive scaling analysis, introduction of the novel Chinchilla model, and rigorous benchmark evaluations. The compute-optimal training perspective and findings appear novel compared to related work. The scale and thoroughness of the experiments and analysis are impressive.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Continued investigation into optimal model scaling with larger compute budgets. The authors note there is uncertainty in extrapolating their compute-optimal frontier by orders of magnitude. Training and evaluating more large models could further refine the understanding of how to balance model size, training data size, and compute budget.- Training models for multiple epochs over the data. The analysis in this paper focuses on the single-epoch regime, but future work could explore optimally training models for multiple epochs. - Incorporating recent advances like mixture-of-experts models into the scaling analysis. The authors suggest applying a similar methodology to understand the compute-optimal frontier for MoE models.- Focus on scaling up high-quality datasets. The analysis predicts models will need trillion-scale datasets going forward. Responsibly collecting and scaling up datasets is an important challenge.- Mitigating risks like toxicity and bias. The authors emphasize the need for more work on evaluating and mitigating potential harms in large language models.- Applying similar methodology to model scaling in other modalities beyond text. The authors propose their compute-optimal training framework could extend to other data types like images, video, etc.In summary, key directions are further refinement on model scaling laws, multi-epoch training, applying to new model types and modalities, scaling up datasets, and improving safety. The analysis highlights how much room there still is to improve language models with more optimally allocated compute and data.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper investigates the optimal model size and amount of training data for training a transformer language model under a fixed computational budget. The authors find that current large language models are significantly undertrained, as recent work has focused on scaling up model size while keeping training data constant. By training over 400 language models from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they conclude that for compute-optimal training, model size and training data should be scaled equally - for every doubling of model size, the amount of training data should also double. Based on this, they train a predicted optimal 70 billion parameter model called Chinchilla on 1.4 trillion tokens, using the same compute as the 280 billion parameter Gopher model. Chinchilla outperforms Gopher and other large models across many evaluations, despite being much smaller, showing the benefits of more balanced scaling. The results suggest that further improvements will require collecting more high-quality training data in addition to engineering advances enabling larger models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper investigates the optimal model size and amount of training data for training a transformer language model under a fixed computational budget. The authors model the training loss as a function of model size and training data size. They find that current large language models are significantly undertrained, as recent focus has been on scaling up model size while keeping training data constant. By training over 400 models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they determine that for compute-optimal training, model size and training data should be scaled equally - for every doubling of model size, the amount of training data should also double. Based on this analysis, the authors train a predicted compute-optimal 70 billion parameter model called Chinchilla on 1.4 trillion tokens, matching the compute budget used for the 280 billion parameter Gopher model. Despite being 4x smaller, Chinchilla outperforms Gopher, GPT-3, Jurassic-1 and Megatron-Turing NLG across a wide range of tasks. For example, Chinchilla achieves 67.5% accuracy on MMLU, over 7% higher than Gopher. This shows the benefits of more compute-optimal training, as Chinchilla also has reduced fine-tuning and inference costs. The results underscore the importance of scaling up training data in addition to model size.
