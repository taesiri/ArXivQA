# [Benchmarking and Analyzing In-context Learning, Fine-tuning and   Supervised Learning for Biomedical Knowledge Curation: a focused study on   chemical entities of biological interest](https://arxiv.org/abs/2312.12989)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem Addressed:
- Biomedical knowledge graphs (KGs) like ChEBI suffer from incompleteness and require periodic updating. Manual curation is time-consuming and not scalable. 
- This paper explores automated approaches for curating and enriching the ChEBI ontology using natural language processing (NLP), comparing three paradigms:
  1) Large language model (LLM) in-context learning 
  2) Fine-tuning a pretrained biomedical BERT model
  3) Supervised machine learning on distributed word representations

Methods:
- Experiments conducted on the ChEBI ontology using 3 simulated curation tasks:
  1) Classifying true versus random false triples 
  2) True versus reversed triples 
  3) True versus closely related false triples
- Models tested: GPT-3.5, GPT-4, BioGPT (in-context learning); PubmedBERT (fine-tuning); GloVe, W2V-Chem, BioWordVec etc. (supervised learning)  
- Also tested: effects of training data size and imbalance

Key Findings:
- GPT-4 achieved best accuracy of 91.6%, 76.6% and 87.4% on tasks 1-3 via in-context learning
- Supervised learning outperformed in-context learning in all tasks when trained on full dataset 
- Fine-tuned PubmedBERT performance on par with best supervised learning models
- Simulation experiments showed both fine-tuning and supervised learning deteriorate with less or imbalanced data
- With <6K triples for training, GPT-4 superior to other paradigms in tasks 1 and 3 but not 2

Key Contributions:
- Comprehensive comparison of modern NLP paradigms for biomedical ontology curation
- Evidence that supervised learning still relevant and outperforms in-context learning given sufficient high-quality training data
- Guidelines provided on best NLP approach to use based on availability and characteristics of training data
