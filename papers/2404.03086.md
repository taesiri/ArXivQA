# [Auditing the Use of Language Models to Guide Hiring Decisions](https://arxiv.org/abs/2404.03086)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Regulators are increasingly calling for "audits" of AI systems, especially in high-stakes contexts like hiring, but there is little clarity on what auditing entails. 
- Large language models (LLMs) are seeing rapid adoption for screening job candidates based on text materials like resumes and interview transcripts, raising concerns about potential discrimination.
- There are many proposed metrics for algorithmic fairness but no consensus on best practices for auditing potential bias, especially for complex models like LLMs.

Proposed Solution:
- The authors adapt correspondence experiments, a method used in social sciences to detect hiring discrimination against minority groups, to audit LLMs. 
- They manipulate names/pronouns in real teacher job applications to signal race/gender, elicit hiring recommendations from LLMs, and check if assessments vary by group.
- They find moderate evidence of favoritism towards women and minorities, suggesting bias in LLM-based hiring algorithms.

Main Contributions:
- Demonstrates correspondence experiments as a concrete auditing procedure called for under emerging regulations on algorithmic accountability.
- Tests multiple state-of-the-art LLMs on a novel dataset of real teacher job applications; finds gender/racial biases persist across models. 
- Shows detected disparities are fairly robust to changes in input data, task framing, and model choice.
- Discusses limitations of correspondence experiments for audits, like lack of ground truth on applicant qualifications and context-specificity of results.

In summary, the paper proposes adapting correspondence experiments from social sciences to audit LLMs for bias in high-stakes hiring decisions, piloting this method on teacher applications to uncover evidence of moderate race/gender effects across several models. The authors discuss caveats around interpreting their findings and using this approach for audits.
