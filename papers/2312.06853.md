# [LLF-Bench: Benchmark for Interactive Learning from Language Feedback](https://arxiv.org/abs/2312.06853)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Interactive agents that can learn from natural language feedback would be more aligned and efficient to train compared to using numeric rewards. However, existing benchmarks either use rewards, focus on non-learning tasks like planning, or do not properly evaluate robustness to language variations. 

Solution: The paper introduces LLF-Bench, a benchmark for evaluating interactive agents on their ability to learn from language feedback across diverse tasks. LLF-Bench formalizes the Learning from Language Feedback (LLF) paradigm which replaces rewards with instructions and feedback. It consists of 8 sets of decision-making problems with configurable instructions and feedback:

- Bandits: Multi-armed bandit problems with randomized actions/rewards across episodes 

- Poem: Constrained text generation tasks for writing poems 

- Movie: Movie recommendation based on user preferences  

- Optimization: Minimizing loss functions based on verbalized gradient feedback

- Parking: Continuous control for parking vehicles without collisions  

- Gridworld: Graph-based navigation to find treasure

- Alfworld: Multi-step language-based tasks in text adventure games

- Metaworld: Low-dimensional robot arm manipulation tasks


Main Contributions:

- Formalizes the LLF paradigm as an alternative to reward-based RL

- Provides diverse set of problems with configurable instructions and feedback

- Implements paraphrasing and environment randomization for robust evaluation 

- Adopts Gym interface for interacting via reset, step etc. and is compatible with RL agents

- Classifies feedback into performance, suggestion and explanation types inspired by education research

The benchmark aims to measure and drive progress in building interactive agents that can efficiently learn from language feedback across diverse tasks.
