# [Is It a Free Lunch for Removing Outliers during Pretraining?](https://arxiv.org/abs/2402.12102)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Quantization is important for deploying large language models (LLMs) on resource-constrained devices, but the presence of outliers in weights/activations harms quantized model performance. 
- Recently, a clipped softmax method was proposed to prevent outliers during pretraining, improving quantizability. However, this degrades full-precision performance on downstream tasks and for causal LM pretraining.

Methods:
- Investigate if outlier-free pretraining via clipped softmax universally benefits LLMs. Find it improves quantizability but hurts full-precision finetuning/varying sequence lengths.
- Propose normalized clipped softmax (NCS) which normalizes probabilities invariantly to sequence length. Sets hyperparameters without grid search.
- Apply NCS to pretraining BERT and OPT (causal LM). Evaluate on GLUE, quantization, and varying sequence lengths.

Contributions:
- Clipped softmax preprocessing does not offer a one-size-fits-all soln for LLMs. NCS boosts clipped softmax's full-precision performance.
- NCS gives better quantization and is more robust to hyperparameters than clipped softmax during pretraining.
- NCS enables effective outlier-free pretraining of causal LMs like OPT, which vanilla clipped softmax failed at.
- Analysis and experiments on how sequence length discrepancy between pretraining and downstream usage causes clipped softmax's inferior full-precision performance.

In summary, the paper investigates issues with existing clipped softmax pretraining, proposes a normalized variant (NCS) that bridges gaps for full-precision usage, causal LM compatibility, and hyperparameter sensitivity, while preserving the quantization benefits.
