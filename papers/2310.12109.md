# [Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture](https://arxiv.org/abs/2310.12109)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using Monarch matrices, a class of structured matrices that generalize the FFT, as a sub-quadratic mixing operator in machine learning architectures. How do Monarch matrices achieve sub-quadratic computational complexity? What are the tradeoffs in expressiveness vs efficiency with different choices of the order p?

2. The paper builds architectures like BERT and GPT entirely out of Monarch matrices. What modifications were required to make Monarch matrices causal for autoregressive modeling? How does the theory of polynomial manipulation developed in the paper ensure causality? 

3. For non-causal modeling like BERT, the paper shows improved throughput over Transformers. What are the relative bottlenecks in Monarch matrix architectures compared to Transformers? How could optimizations like kernel fusion help improve throughput further?

4. The paper evaluates Monarch matrices on a range of modalities like text, image, speech, and synthetic data. Are there any modalities where Monarch architectures seem particularly suitable or unsuitable? How do design choices affect suitability?

5. The paper proposes using Monarch matrices for both mixing across sequence length and across model dimension. What are the tradeoffs between using the same vs different classes of matrices along each axis?

6. How does the performance of Monarch matrix architectures compare to other approaches for efficient Transformers, like sparse attention, kernel methods, or linear attention? What are relative advantages and disadvantages?

7. The paper focuses on replacement of attention and MLP blocks. What other neural network components could potentially be replaced by Monarch operations? What would be required?

8. What are the implications of the strong empirical results on causal language modeling? Does this indicate potential promise for attention-free architectures in generative settings?

9. What modifications would be needed to scale Monarch architectures to even longer sequences or larger models than studied in the paper? Are there any fundamental limitations?

10. The paper provides a simple PyTorch implementation of Monarch layers. What would be required to build an optimized high-performance Monarch matrix library? What optimizations beyond kernel fusion could help?
