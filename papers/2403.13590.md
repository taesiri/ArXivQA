# [Teacher-Student Training for Debiasing: General Permutation Debiasing   for Large Language Models](https://arxiv.org/abs/2403.13590)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) show impressive versatility and performance when prompted, but can be sensitive to input orderings (e.g. question/answer options). This permutation sensitivity leads to inconsistent and unreliable predictions.  
- Debiasing approaches like permutation ensembling can mitigate these issues but have high computational cost.

Proposed Solution: 
- A teacher-student framework to distill a computationally cheaper student model that emulates the debiased capabilities of a teacher.  
- Two student variants: (1) Direct knowledge distillation (2) Error correction where student corrects a single biased teacher sample.
- Applicable to both white-box and black-box LLMs with manageable data requirements.

Contributions:
- Metrics proposed to quantify permutation sensitivity and positional bias. Experiments show LLMs can have high sensitivity.
- Debiasing is shown to improve performance and reliability. Addresses neglected invariances.
- On RACE++ and SummEval tasks, small 110-330M student models outperform larger biased teachers after teacher-student training, while maintaining invariances.
- Black-box training demonstrated to be sample efficient. Student models can implicitly infer systematic biases using noisy approximations.

In summary, the paper introduces an effective and practical teacher-student framework to impart beneficial capabilities from a debiased teacher to an efficient student model for improved reliability and performance.
