# [Stable Distillation: Regularizing Continued Pre-training for   Low-Resource Automatic Speech Recognition](https://arxiv.org/abs/2312.12783)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Stable Distillation: Regularizing Continued Pre-training for Low-Resource Automatic Speech Recognition":

Problem:
- Self-supervised learning (SSL) models for speech suffer from poor generalization when there is a domain shift between pre-training and target fine-tuning data. 
- Continued pre-training (CP) on target data helps adapt models but leads to overfitting due to violation of IID assumption.

Proposed Solution: 
- Stable Distillation: A simple and effective CP strategy to improve ASR performance on target domain while preventing overfitting.
- Uses self-distillation to regularize CP and constrain distance between initial and final weights after CP.
- Has a teacher model pretrained on target data using vanilla CP. Student model is trained on same data jointly optimizing SSL pretext task loss and MSE loss between teacher and student representations.

Main Contributions:
- Proposes Stable Distillation - a novel self-distillation method to regularize continued pretraining and prevent overfitting.
- Achieves 0.8-7 WER gain over baselines by better utilizing knowledge from both initial pretraining and target domain CP. 
- Also helps in effective cross-lingual to mono-lingual SSL model adaptation by preventing catastrophic forgetting.
- Evaluated on multiple low-resource target domain datasets differing from source pretrain data. Consistently outperforms baselines.

In summary, the paper presents Stable Distillation as an effective CP regularization technique via self-distillation to boost ASR performance on low-resource target domains while preventing overfitting.
