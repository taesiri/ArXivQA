# [Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual   Reinforcement Learning](https://arxiv.org/abs/2403.05066)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Continual reinforcement learning (CRL) aims to enable RL agents to continuously learn new tasks without forgetting previously learned ones. However, one major challenge is the negative transfer issue that frequently occurs when transitioning between dissimilar tasks. 
- Recent work has focused on mitigating plasticity loss in RL agents, but does not effectively address negative transfer in CRL. Experiments on Meta-World tasks demonstrate severe performance drops when fine-tuning agents on new tasks due to negative transfer.

Proposed Solution:
- The authors propose Reset & Distill (R&D), a simple yet effective strategy to overcome negative transfer in CRL. 
- R&D utilizes an online actor-critic that interacts with the environment and learns the current task, and an offline actor that retains knowledge.
- After the online networks learn a task, their parameters are reset before learning the next task. This prevents negative transfer.  
- Offline actor then distills knowledge from the online actor's policy into its own policy using behavioral cloning, retaining prior knowledge.
- An expert replay buffer stores state-action samples from previous online actors to regularize the offline actor.

Contributions:
- Demonstrate through analysis and experiments that negative transfer frequently occurs in CRL and cannot be addressed by methods tackling plasticity loss.
- Propose R&D that combines resetting online networks to prevent negative transfer and distilling knowledge into persistent offline network to avoid catastrophic forgetting.
- Achieve state-of-the-art performance on long sequences of Meta-World tasks. R&D consistently outperforms baselines, showing much higher success rates.
- Results highlight the need to explicitly consider negative transfer in designing effective CRL algorithms. Findings also emphasize resetting and distillation as a robust strategy against it.

In summary, the paper clearly identifies negative transfer as a key challenge in CRL and puts forth a simple yet powerful R&D technique to overcome it through resetting and distillation. Experiments extensively demonstrate the effectiveness of R&D over strong baselines.
