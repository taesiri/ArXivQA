# Don't Forget Your ABC's: Evaluating the State-of-the-Art in   Chat-Oriented Dialogue Systems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we develop a dimensional human evaluation method for open-domain chatbots that reliably measures key aspects of dialogue quality?The key points are:- The paper presents a novel human evaluation method called ABC-Eval that measures the rate of various chatbot behaviors that impact conversation quality.- It provides a detailed validation comparing ABC-Eval to other popular evaluation methods like Likert scales and pairwise comparisons. The analyses suggest ABC-Eval metrics are more reliable, sensitive, and provide better coverage of distinct dialogue characteristics.- The paper applies ABC-Eval and other methods to comprehensively evaluate four state-of-the-art open-domain chatbots. This evaluation quantifies and highlights several key challenges facing current chatbots, especially regarding consistency, relevance, common sense, and emotional understanding. - Based on the results, the paper recommends that future work adopt the validated ABC-Eval methodology, or a relevant subset, to gain more nuanced insights into chatbot strengths/weaknesses and to drive progress on key dialogue challenges.In summary, the central research contribution is a new methodology for dimensional human evaluation of chatbots, along with analyses to validate its merits and application of the methodology to illuminate pressing issues with state-of-the-art chatbots.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new human evaluation method called ABC-Eval that involves turn-level annotation of chatbot behaviors. The paper argues that ABC-Eval provides more reliable, sensitive, and interpretable metrics compared to existing methods like Likert scales and pairwise comparisons.2. Performing a detailed validation study that compares ABC-Eval to other evaluation methods. The analysis looks at interpretability, importance, sensitivity, coverage, and distinctness of the metrics. 3. Using the proposed ABC-Eval method along with existing methods to comprehensively evaluate four state-of-the-art open-domain chatbots. The evaluation quantifies and highlights several key challenges faced by current chatbots, especially regarding consistency, relevance, common sense, and emotional understanding.4. Providing guidance to future work on choosing suitable evaluation methods based on the results of the validation study. The paper recommends using ABC-Eval or a relevant subset of it to complement existing methods like dialogue-level Likert ratings.5. Releasing an evaluation platform, data, and analyses to enable further research on human evaluation of chatbots. This includes the annotation tool, instructions, conversation dataset, and code for the analyses.In summary, the key contributions are proposing and validating a new dimensional human evaluation method for chatbots, using it to highlight key challenges facing current chatbots, and providing guidance and resources to enable future progress in this area. The paper makes a case for ABC-Eval being an improvement over existing methods.
