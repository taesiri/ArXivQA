# [Trainable Projected Gradient Method for Robust Fine-tuning](https://arxiv.org/abs/2303.10720)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the out-of-distribution (OOD) robustness/generalization of fine-tuned models, so they retain more of the generalization capability of the original pre-trained model?

The key hypothesis appears to be:

Enforcing customized distance constraints between the fine-tuned model and pre-trained model for each layer will allow the model to retain more generalization capability from pre-training, leading to better OOD performance.

The paper proposes the trainable projected gradient method (TPGM) to automatically learn these per-layer distance constraints during fine-tuning to improve OOD robustness. The bi-level optimization formulation enables TPGM to balance fitting the training data and generalizing to new data when learning the projection ratios. Theoretically and empirically, TPGM is shown to learn differing constraints for each layer which helps improve OOD performance while maintaining ID accuracy.

So in summary, the central research question is how to improve OOD generalization during fine-tuning, with the key hypothesis being that optimized per-layer distance constraints can achieve this goal. TPGM is the proposed method to automatically learn these constraints.
