# [SUDS: Scalable Urban Dynamic Scenes](https://arxiv.org/abs/2303.14536)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to scale up neural radiance field (NeRF) models to represent large dynamic urban scenes across multiple videos. The key hypotheses appear to be:1) By decomposing the scene into separate branches for static, dynamic, and far-field radiance, and using hash tables to represent them efficiently, NeRFs can be scaled to large environments.2) By using unlabeled input signals like optical flow, sparse LiDAR, and 2D descriptors, the model can learn to disentangle static and dynamic components without needing manual annotations.3) The combination of the multi-branch architecture and unlabeled supervisory signals will allow reconstructing complex urban scenes with moving objects across multiple videos covering large spatial areas.In summary, the central research question is how to scale neural radiance fields to city-scale dynamic environments in an unsupervised manner using the proposed multi-branch architecture and unlabeled input signals. The key hypotheses relate to the scalability and capability of this approach for large urban scene modeling and decomposition.


## What is the main contribution of this paper?

The main contribution of this paper is presenting SUDS, a method for scaling neural radiance field (NeRF) reconstructions to large dynamic urban scenes. The key innovations are:- A scalable representation that factorizes the scene into three branches - static, dynamic, and far-field - which are modeled efficiently using hash tables. This allows scaling up to city-scale environments. - Leveraging unlabeled data like optical flow, sparse LiDAR, and 2D feature descriptors as supervision signal, avoiding the need for difficult to obtain labels like 3D bounding boxes. This enables tasks like scene flow estimation and semantic manipulation.- Evaluation on a new large-scale dynamic dataset City-1M spanning over 100 km^2 and 1.2 million frames. SUDS outperforms baselines and achieves state-of-the-art results on KITTI/Virtual KITTI while being faster to train.- Demonstrating various applications enabled by the representation like novel view synthesis, unsupervised 3D instance segmentation, 3D cuboid detection, and semantic segmentation using only a small set of labeled data.In summary, the main contribution is developing a scalable dynamic neural scene representation that leverages diverse unlabeled data to achieve compelling reconstructions of urban environments for various tasks. This pushes the boundary on the scale and complexity of scenes that can be modeled with neural radiance fields.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces SUDS, a method to scale neural scene representations to city-wide reconstructions across thousands of videos by using a multi-branch hash table structure and unlabeled inputs like optical flow and 2D features to decompose scenes into static geometry, dynamic objects, and their motions.
