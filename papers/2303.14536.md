# [SUDS: Scalable Urban Dynamic Scenes](https://arxiv.org/abs/2303.14536)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to scale up neural radiance field (NeRF) models to represent large dynamic urban scenes across multiple videos. The key hypotheses appear to be:1) By decomposing the scene into separate branches for static, dynamic, and far-field radiance, and using hash tables to represent them efficiently, NeRFs can be scaled to large environments.2) By using unlabeled input signals like optical flow, sparse LiDAR, and 2D descriptors, the model can learn to disentangle static and dynamic components without needing manual annotations.3) The combination of the multi-branch architecture and unlabeled supervisory signals will allow reconstructing complex urban scenes with moving objects across multiple videos covering large spatial areas.In summary, the central research question is how to scale neural radiance fields to city-scale dynamic environments in an unsupervised manner using the proposed multi-branch architecture and unlabeled input signals. The key hypotheses relate to the scalability and capability of this approach for large urban scene modeling and decomposition.


## What is the main contribution of this paper?

The main contribution of this paper is presenting SUDS, a method for scaling neural radiance field (NeRF) reconstructions to large dynamic urban scenes. The key innovations are:- A scalable representation that factorizes the scene into three branches - static, dynamic, and far-field - which are modeled efficiently using hash tables. This allows scaling up to city-scale environments. - Leveraging unlabeled data like optical flow, sparse LiDAR, and 2D feature descriptors as supervision signal, avoiding the need for difficult to obtain labels like 3D bounding boxes. This enables tasks like scene flow estimation and semantic manipulation.- Evaluation on a new large-scale dynamic dataset City-1M spanning over 100 km^2 and 1.2 million frames. SUDS outperforms baselines and achieves state-of-the-art results on KITTI/Virtual KITTI while being faster to train.- Demonstrating various applications enabled by the representation like novel view synthesis, unsupervised 3D instance segmentation, 3D cuboid detection, and semantic segmentation using only a small set of labeled data.In summary, the main contribution is developing a scalable dynamic neural scene representation that leverages diverse unlabeled data to achieve compelling reconstructions of urban environments for various tasks. This pushes the boundary on the scale and complexity of scenes that can be modeled with neural radiance fields.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces SUDS, a method to scale neural scene representations to city-wide reconstructions across thousands of videos by using a multi-branch hash table structure and unlabeled inputs like optical flow and 2D features to decompose scenes into static geometry, dynamic objects, and their motions.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in neural radiance fields and dynamic scene reconstruction:- Scale: This paper focuses on reconstructing large urban areas spanning over 100 km2 from thousands of videos. This is much larger in scale than most prior work on neural radiance fields, which has focused on reconstructing single rooms or small outdoor scenes. The scalability comes from the partitioned multi-branch hash table representation.- Dynamism: The method models dynamic scenes across multiple videos captured at different times, as opposed to just a single video sequence. This allows it to disentangle more complex object motions like transiently parked cars. The scene flow supervision is crucial to making this work.- Inputs: The method relies on "free" supervision from sparse LiDAR, optical flow, and 2D feature descriptors, instead of requiring manual labels like segmentation maps or bounding boxes. This makes it more practical for large real-world datasets.- Semantics: It incorporates semantic feature distillation to enable tasks like segmentation, while most neural radiance field papers focus only on view synthesis. The features come from a self-supervised 2D model without human labeling.- Benchmarks: For single scene datasets like KITTI, it achieves state-of-the-art view synthesis results compared to category-specific methods that use ground truth bounding boxes during training.Overall, the key innovations appear to be in scaling to large environments, handling multi-video dynamism through scene flow, and incorporating diverse data like optical flow and 3D features to avoid costly manual labeling. The tradeoff is that instance-level understanding is not as strong as dedicated category-specific models trained on annotated data. But the paper shows impressive results on view synthesis and geometry for its scale.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest are:- Improving the disentanglement of static and dynamic components. The paper mentions limitations in properly attributing shadows to the static or dynamic branch. Further improvements in decomposition could enhance the quality of the representation.- Developing more sophisticated scene flow representations. The paper currently assumes linear flow between observed frames. Modeling acceleration or higher-order dynamics could enable better novel view synthesis and tracking.- Generalizing to new scenes and videos. The current approach does not allow extrapolation of object motions beyond the boundaries of the training videos. Exploring ways to transfer and adapt models to new sequences could extend the applicability. - Handling camera inaccuracies. The authors mention camera calibration as being crucial to rendering quality. Jointly optimizing intrinsics and extrinsics within the model could make it more robust.- Reducing compute requirements. Significant resources were needed for dataset preprocessing and model training. Finding ways to reduce this cost through efficient data loading, lower-precision computations, etc could enable broader adoption.- Improving instance-level understanding. While qualitative results on 3D detection and segmentation were shown, performance lags conventional methods. Developing the radiance field to be more competitive for 3D instance tasks is noted as an open challenge.- Enhancing semantic capabilities. The distillation of 2D features provides a start towards semantics, but more sophisticated reasoning, perhaps through self-supervision on unlabeled video, could unlock new applications.So in summary, key directions involve improving decomposition and dynamics, generalizing across scenes, reducing compute costs, and enhancing the model's semantic understanding at both the instance and category levels. Advancing these aspects could help scale neural radiance fields to even larger and more complex environments.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces SUDS, a method for scaling neural radiance fields to large dynamic urban scenes. The key innovations are (1) factorizing the scene into three branches - static, dynamic, and far-field - which are modeled using efficient multi-resolution hash tables, and (2) using unlabeled data like optical flow and 2D feature descriptors as free supervision signals. This allows scaling to city-wide reconstructions with tens of thousands of objects across multiple videos covering hundreds of kilometers. The method is demonstrated to perform novel view synthesis, unsupervised instance segmentation, and other tasks on a new 1.2 million frame dataset spanning 105 sq km. It also achieves state-of-the-art performance on KITTI and Virtual KITTI benchmarks while being 10x faster than prior methods relying on ground truth annotations. Overall, the work represents an important step towards scalable 4D neural reconstructions of dynamic real-world environments.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes SUDS, a method to scale neural radiance field reconstructions to large dynamic urban scenes. The key innovation is a factorization of the scene into three components - a static branch containing non-moving objects like buildings, a dynamic branch for transient and moving objects like cars and people, and a far-field branch for distant objects like sky. Each branch is implemented efficiently using hash tables. The method makes use of unlabeled data like optical flow and sparse LiDAR to provide supervision signals and decomposition losses to learn scene flow and disentangle the static and dynamic components. This allows the model to scale to city-wide datasets of over 1 million images across thousands of videos covering over 100km, which the authors claim is the largest dynamic neural radiance field built. The model demonstrates strong performance on novel view synthesis, unsupervised 3D instance segmentation, 3D cuboid detection, and other tasks without requiring manually annotated supervision. Comparisons on KITTI and Virtual KITTI datasets also show the method surpasses prior work reliant on ground truth bounding boxes while training much faster.In summary, the key contributions are:1) To the authors' knowledge, the first large-scale dynamic neural radiance field covering a whole city.2) A scalable 3-branch hash table representation for modeling static, dynamic, and far-field components. 3) State-of-the-art reconstruction results on multiple datasets without requiring manual supervision.4) Qualitative demonstrations of various downstream tasks like free viewpoint rendering, 3D scene flow estimation, unsupervised instance segmentation and 3D bounding boxes.


## Summarize the main method used in the paper in one paragraph.

The paper presents SUDS (Scalable Urban Dynamic Scenes), a method for reconstructing large-scale dynamic urban environments with neural radiance fields (NeRFs). The key ideas are:1) The scene is factorized into three branches - a static branch for consistent background geometry, a dynamic branch for transient objects, and a far-field branch for distant radiance like the sky. 2) The branches are represented efficiently using hash tables indexed by spatial location (static), spatiotemporal location (dynamic), and viewing direction (far-field). This allows scaling to city-wide areas.3) The method makes use of various unlabeled inputs like optical flow, LiDAR depth, and 2D feature descriptors. These provide supervision for scene flow, geometry, and semantics without manual annotation.4) The scene is partitioned spatially into sub-models that are trained separately. Visibility reasoning using depth measurements reduces the training data size.The combination of the multi-branch radiance field, hash table scene representation, diverse inputs, and spatial partitioning allows SUDS to reconstruct city-scale dynamic scenes with state-of-the-art quality. It can synthesize novel views, estimate 3D scene flow, and enable tasks like unsupervised 3D instance segmentation.
