# [A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on   NVIDIA Hopper Architecture using the CUTLASS Library](https://arxiv.org/abs/2312.11918)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Attention mechanisms are a core part of transformer models like large language models (LLMs), but standard implementations require saving intermediate matrices to global memory, hurting performance. 
- Fusing the steps of attention (two GEMM operations and a row-wise softmax) into a single CUDA kernel can improve performance by eliminating these intermediate writes. However, fusing softmax with GEMM is challenging.

Proposed Solution:
- Implement the FlashAttention-2 algorithm, which is the current state-of-the-art fused multi-head attention (FMHA) kernel, using the NVIDIA CUTLASS library and target the new Hopper GPU architecture.

- Use Hopper-specific features like the Tensor Memory Accelerator (TMA) for fast async copies and Warp Matrix Multiply Accumulate (WGMMA) instructions for fast matrix multiplication.

- Leverage CUTLASS abstractions like Layouts and Tensors to correctly define the mappings between logical and physical coordinates needed for the GEMM and copy operations.

- Use layout transformations like taking the transpose and reshaping the GEMM-I accumulator layout to the GEMM-II operand layout.

- Overlap TMA copy operations with WGMMA GEMM operations to hide memory latency.

- Choose tile sizes to balance utilization, register pressure and shared memory usage.

Main Contributions:

- First published implementation and analysis of FMHA targeting Hopper's new architecture.

- 20-50% higher GFLOPs/s over FlashAttention-2 implementation for Ampere architecture by using Hopper-specific features.

- Demonstrates techniques like reshaping layouts, using online softmax with shuffle instructions, and overlapping copy and GEMM that serve as a case study for CUDA kernel fusion challenges.

- Provides detailed explanation and heavily commented code to complement the high-level discussion in the FlashAttention papers.
