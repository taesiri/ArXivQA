# [ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](https://arxiv.org/abs/2402.10930)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Softmax operation bottlenecks transformer-based large language model (LLM) inference accelerators due to low computing parallelism and hardware-unfriendly non-linear operations. 
- It requires synchronization across the entire input sequence to find the maximum score and calculate score summation. This prevents parallel computation of the subsequent output probabilities and value vectors.
- Existing methods like approximate computing reduce accuracy, while partial Softmax still requires synchronization.

Proposed Solution: 
- Propose Constant Softmax (ConSmax) to replace Softmax using two learnable parameters beta and gamma instead of maximum score and denominator.
- Avoids synchronization for max score search and score summation, allowing parallel computation of output probabilities. 
- Custom ConSmax hardware uses bitwidth-split look-up tables for lossless, non-linear operations and supports mixed-precision computing.

Main Contributions:
- ConSmax enables high parallelism in LLM inference accelerators without accuracy loss.
- Bitwidth-split ConSmax hardware is scalable, achieves 3.35x power and 2.75x area savings compared to state-of-the-art.
- Evaluated on GPT-2 model and WikiText103 dataset, ConSmax hardware consumes only 0.2mW power and 0.0008mm^2 area in 16nm technology.

In summary, ConSmax is a hardware-software co-designed Softmax alternative that avoids synchronization bottlenecks, enhances parallelism, and enables efficient acceleration of large language models while maintaining accuracy.
