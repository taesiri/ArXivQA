# [Learning Procedure-aware Video Representation from Instructional Videos   and Their Narrations](https://arxiv.org/abs/2303.17839)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is to learn video representations that encode both the concepts of individual action steps as well as their temporal ordering, for the task of understanding procedural activities. 

The central hypothesis is that jointly learning these two aspects - step concepts and their temporal dependencies - from unlabeled instructional videos and narrations can enable new capabilities like step classification, forecasting, and sequence modeling, without needing manual annotations.

Specifically, the key research questions addressed are:

- How to learn step concept representations from instructional videos and narrations in a self-supervised manner? The paper uses a video-text matching objective with CLIP.

- How to model temporal dependencies and orderings of steps? The paper proposes a novel deep probabilistic model based on diffusion processes. 

- What capabilities does this joint learningframework enable? The paper demonstrates strong performance on step classification, forecasting, zero-shot inference, and generating diverse predictions.

- How does modeling temporal order compare with learning individual steps? Ablations show modeling order reinforces step concept learning and enables forecasting.

In summary, the central focus is on developing and evaluating an approach to jointly learn step concepts and their temporal dynamics from unlabeled video-text pairs, to support new capabilities in understanding procedural activities. The paper presents strong empirical validation of this idea.


## What is the main contribution of this paper?

 The main contribution of this paper is learning video representations that encode both action steps and their temporal ordering from instructional videos and narrations, without using human annotations. Specifically:

- They propose jointly learning a video representation to encode individual step concepts, and a deep probabilistic model to capture temporal dependencies and variations in step ordering. 

- The video representation is learned by matching video clips to narrations. The probabilistic model, built with a diffusion process, is tasked to predict the distribution of the video representation for a missing step given steps in its vicinity.

- The model is trained using only videos and narrations from automatic speech recognition, without requiring manual annotations. 

- Once trained, the model supports zero-shot inference for step classification and forecasting, and can sample multiple video representations when predicting a missing step to provide diverse, plausible options.

- They demonstrate the model achieves state-of-the-art results on step classification and forecasting on the COIN and EPIC-Kitchens datasets. It also enables promising zero-shot inference and generation of diverse step predictions.

In summary, the key innovation is a joint learning framework and probabilistic model for learning video representations that capture both individual step concepts as well as their temporal ordering from unlabeled instructional videos and narrations. This enables new capabilities for procedure understanding and reasoning.
