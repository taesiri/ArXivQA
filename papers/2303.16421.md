# ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of   Commonsense Problem in Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can GPTs effectively answer commonsense questions? 2) Are GPTs knowledgeable in commonsense? 3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? 4) Can GPTs effectively leverage commonsense for answering questions?The authors seem to be evaluating and analyzing the commonsense abilities of large language models (LLMs) like ChatGPT. In particular, they are investigating whether these models can accurately answer commonsense questions, if they actually possess and can generate the necessary commonsense knowledge, and whether they can effectively identify and utilize the relevant commonsense knowledge needed to answer specific questions. The overarching goal seems to be assessing the strengths and limitations of current LLMs when it comes to representing and reasoning about commonsense knowledge across different domains (physical, social, temporal, etc.). The authors design experiments using commonsense QA datasets to analyze these capabilities of ChatGPT.In summary, the central research questions focus on evaluating ChatGPT's abilities in: 1) answering commonsense questions, 2) generating/possessing commonsense knowledge, 3) identifying necessary knowledge for questions, and 4) leveraging commonsense knowledge for reasoning and QA. The experiments aim to provide insights into these aspects to determine how well LLMs can currently perform on commonsense tasks.


## What is the main contribution of this paper?

The main contribution of this paper is conducting a detailed investigation and analysis of the commonsense abilities of large language models, specifically ChatGPT. The key points are:- The paper evaluates ChatGPT's ability to answer commonsense questions across 11 datasets covering 8 knowledge domains. It finds that ChatGPT can achieve high accuracy on most datasets, but still struggles with certain types of knowledge like social and temporal reasoning. - The paper assesses whether ChatGPT can identify the necessary commonsense knowledge to answer specific questions. Through manual evaluation, it shows ChatGPT struggles to precisely pinpoint the relevant knowledge, often generating high noise.- The paper evaluates whether ChatGPT contains accurate commonsense knowledge by querying it with prompts based on the identified knowledge. It finds ChatGPT can generate correct descriptions for most knowledge, indicating it is knowledgeable. - The paper tests whether ChatGPT can leverage generated knowledge to answer questions. It discovers providing the knowledge as context does not significantly improve ChatGPT's reasoning and answering abilities.In summary, the key contribution is providing a comprehensive evaluation of the strengths and weaknesses of ChatGPT's commonsense capabilities through multiple experiments. The paper demonstrates that while knowledgeable, ChatGPT is an inexperienced problem solver when it comes to effectively using commonsense knowledge for reasoning and question answering.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper investigates the commonsense abilities of large language models like ChatGPT and finds that while they can achieve good accuracy on commonsense QA datasets and generate accurate commonsense knowledge, they still struggle to precisely identify the knowledge needed to answer specific questions and cannot effectively leverage that knowledge in reasoning.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work on evaluating commonsense reasoning abilities in large language models:- This paper provides a comprehensive evaluation of commonsense reasoning abilities in ChatGPT across multiple dimensions - answering questions, generating relevant knowledge, and leveraging knowledge in context. Most prior work has focused on only one or two aspects.- The paper uses a broad set of 11 diverse QA datasets covering 8 types of commonsense knowledge. This provides a more comprehensive analysis compared to prior work like Zhou et al. (2020) that used a narrower set of tasks. - The analysis of ChatGPT's ability to generate relevant commonsense knowledge and leverage it in context provides novel insights not explored deeply in prior work. For instance, Ma et al. (2021) studied knowledge in LLMs but did not evaluate leveraging it.- The findings that ChatGPT struggles with precisely identifying necessary knowledge and utilizing knowledge in reasoning aligns with several prior studies showing limitations in commonsense reasoning abilities (Davison et al. 2019, Tamborrino et al. 2020).- The correlation analysis between knowledge awareness and QA accuracy provides new evidence about the importance of knowledge identification for commonsense QA, extending insights from Wang et al. (2021).- The comparison of different LLMs (GPT-3 vs ChatGPT) and analysis of the impacts of tuning (GPT-3 vs GPT-3.5) aligns with findings in Ouyang et al. (2022) about the benefits of tuning for commonsense.- Major limitations compared to some prior work are the focus only on ChatGPT rather than multiple LLMs, and lack of analysis on few-shot prompting.Overall, this paper provides a comprehensive analysis of ChatGPT's commonsense abilities using diverse QA datasets and evaluation methods that complements and extends key findings from several recent studies in this area. The novel aspects include evaluating knowledge generation and leveraging abilities.
