# ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of   Commonsense Problem in Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can GPTs effectively answer commonsense questions? 2) Are GPTs knowledgeable in commonsense? 3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? 4) Can GPTs effectively leverage commonsense for answering questions?The authors seem to be evaluating and analyzing the commonsense abilities of large language models (LLMs) like ChatGPT. In particular, they are investigating whether these models can accurately answer commonsense questions, if they actually possess and can generate the necessary commonsense knowledge, and whether they can effectively identify and utilize the relevant commonsense knowledge needed to answer specific questions. The overarching goal seems to be assessing the strengths and limitations of current LLMs when it comes to representing and reasoning about commonsense knowledge across different domains (physical, social, temporal, etc.). The authors design experiments using commonsense QA datasets to analyze these capabilities of ChatGPT.In summary, the central research questions focus on evaluating ChatGPT's abilities in: 1) answering commonsense questions, 2) generating/possessing commonsense knowledge, 3) identifying necessary knowledge for questions, and 4) leveraging commonsense knowledge for reasoning and QA. The experiments aim to provide insights into these aspects to determine how well LLMs can currently perform on commonsense tasks.


## What is the main contribution of this paper?

The main contribution of this paper is conducting a detailed investigation and analysis of the commonsense abilities of large language models, specifically ChatGPT. The key points are:- The paper evaluates ChatGPT's ability to answer commonsense questions across 11 datasets covering 8 knowledge domains. It finds that ChatGPT can achieve high accuracy on most datasets, but still struggles with certain types of knowledge like social and temporal reasoning. - The paper assesses whether ChatGPT can identify the necessary commonsense knowledge to answer specific questions. Through manual evaluation, it shows ChatGPT struggles to precisely pinpoint the relevant knowledge, often generating high noise.- The paper evaluates whether ChatGPT contains accurate commonsense knowledge by querying it with prompts based on the identified knowledge. It finds ChatGPT can generate correct descriptions for most knowledge, indicating it is knowledgeable. - The paper tests whether ChatGPT can leverage generated knowledge to answer questions. It discovers providing the knowledge as context does not significantly improve ChatGPT's reasoning and answering abilities.In summary, the key contribution is providing a comprehensive evaluation of the strengths and weaknesses of ChatGPT's commonsense capabilities through multiple experiments. The paper demonstrates that while knowledgeable, ChatGPT is an inexperienced problem solver when it comes to effectively using commonsense knowledge for reasoning and question answering.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper investigates the commonsense abilities of large language models like ChatGPT and finds that while they can achieve good accuracy on commonsense QA datasets and generate accurate commonsense knowledge, they still struggle to precisely identify the knowledge needed to answer specific questions and cannot effectively leverage that knowledge in reasoning.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work on evaluating commonsense reasoning abilities in large language models:- This paper provides a comprehensive evaluation of commonsense reasoning abilities in ChatGPT across multiple dimensions - answering questions, generating relevant knowledge, and leveraging knowledge in context. Most prior work has focused on only one or two aspects.- The paper uses a broad set of 11 diverse QA datasets covering 8 types of commonsense knowledge. This provides a more comprehensive analysis compared to prior work like Zhou et al. (2020) that used a narrower set of tasks. - The analysis of ChatGPT's ability to generate relevant commonsense knowledge and leverage it in context provides novel insights not explored deeply in prior work. For instance, Ma et al. (2021) studied knowledge in LLMs but did not evaluate leveraging it.- The findings that ChatGPT struggles with precisely identifying necessary knowledge and utilizing knowledge in reasoning aligns with several prior studies showing limitations in commonsense reasoning abilities (Davison et al. 2019, Tamborrino et al. 2020).- The correlation analysis between knowledge awareness and QA accuracy provides new evidence about the importance of knowledge identification for commonsense QA, extending insights from Wang et al. (2021).- The comparison of different LLMs (GPT-3 vs ChatGPT) and analysis of the impacts of tuning (GPT-3 vs GPT-3.5) aligns with findings in Ouyang et al. (2022) about the benefits of tuning for commonsense.- Major limitations compared to some prior work are the focus only on ChatGPT rather than multiple LLMs, and lack of analysis on few-shot prompting.Overall, this paper provides a comprehensive analysis of ChatGPT's commonsense abilities using diverse QA datasets and evaluation methods that complements and extends key findings from several recent studies in this area. The novel aspects include evaluating knowledge generation and leveraging abilities.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Investigating better mechanisms for utilizing commonsense knowledge in large language models (LLMs), such as instruction tuning and better commonsense-guided reasoning. The authors state that current LLMs are knowledgeable but inexperienced problem solvers, so better methods are needed to leverage their commonsense knowledge effectively.2. Designing knowledge injection approaches for types of commonsense knowledge that are currently missing or inadequate in LLMs, such as social and temporal commonsense. The authors found LLMs struggled more with these types of knowledge.3. Developing lightweight commonsense updating methods to keep the knowledge in LLMs up-to-date over time.4. Designing new benchmarks with wider coverage of commonsense skills and new evaluation methods to provide a more comprehensive assessment of large language models. The authors argue the full capabilities of models like ChatGPT and GPT-4 are difficult to evaluate.5. Exploring the incorporation of other capabilities beyond commonsense knowledge that are needed for complex reasoning, such as the ability to handle negation and make inferences under uncertainty.In summary, the main directions are enhancing commonsense utilization, expanding knowledge coverage, keeping knowledge updated, improving evaluation methods, and integrating additional reasoning skills into LLMs. The authors highlight commonsense as an ongoing challenge for LLMs that requires further research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper investigates the commonsense abilities of large language models (LLMs) like ChatGPT by conducting a series of experiments to evaluate their performance on commonsense question answering (QA), knowledge identification, knowledge generation, and knowledge-based reasoning. The key findings are: (1) LLMs can achieve good accuracy on commonsense QA datasets but still struggle with certain knowledge types like social and temporal reasoning. (2) While ChatGPT contains a lot of commonsense knowledge and can generate accurate descriptions when queried, it struggles to precisely identify the necessary knowledge to answer specific questions. (3) ChatGPT cannot effectively leverage its own generated commonsense knowledge descriptions to improve reasoning and QA performance. Overall, the paper concludes that current LLMs are knowledgeable but inexperienced when it comes to effectively utilizing commonsense knowledge for reasoning and QA. The authors suggest further research is needed on instruction tuning, commonsense injection, and benchmark design to address these limitations.
