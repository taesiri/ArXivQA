# [Less is More: Mitigating Multimodal Hallucination from an EOS Decision   Perspective](https://arxiv.org/abs/2402.14545)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large multimodal models (LMMs) often suffer from multimodal hallucinations, where the text they generate contains content not actually present in the visual inputs. This reduces reliability.
- One source of hallucinations is training data that contains overly detailed text exceeding the models' visual perception capabilities. This hinders models' ability to timely terminate generation based on what they can actually perceive.

Solution: 
- The paper investigates models' inner behavior regarding end-of-sentence (EOS) token prediction. It finds models tend to assess completeness of text relative to visual inputs when deciding whether to stop generation.  
- Leveraging this finding, the paper proposes two methods to enhance models' capability to conclude generation appropriately:
   1) A selective EOS supervision training objective that allows models to learn to terminate generation timely.
   2) A scoring strategy to filter out overly-detailed training data that impairs models' EOS decision ability.

Main Contributions:
- Identifies a new factor contributing to multimodal hallucinations: excessively detailed training data hindering models' innate ability to end generation based on visual perception limits
- Through analysis of models' EOS prediction, discovers the potential of models to terminate generation appropriately based on visual inputs
- Develops two techniques to reduce hallucinations by preserving models' capability to conclude generation in a timely manner
- Provides effective and practical solutions to enhance reliability of LMMs without needing additional data or supervision

The key insight is that models inherently can stop generation based on visual perception, but overly detailed training data compromises this ability. The proposed techniques help models regain this capability to mitigate hallucinations.
