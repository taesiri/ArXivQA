# [Countering Noisy Labels By Learning From Auxiliary Clean Labels](https://arxiv.org/abs/1905.13305)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions/hypotheses addressed in this paper are:1. Can learning from noisy labels be improved by leveraging auxiliary clean labels from self-supervised learning? 2. Can pseudo-labels in semi-supervised learning be treated as a type of noisy label and improved via self-supervision?3. Can a unified framework be developed that improves robustness to both synthetic noisy labels and pseudo-labels by exploiting rotation self-supervision?4. Does the proposed Rotational-Decoupling Consistency Regularization (RDCR) framework outperform existing state-of-the-art methods, especially under high noise levels?In summary, the central hypothesis is that noisy labels, including synthetic noise and pseudo-labels, can be countered by learning from auxiliary clean labels generated via self-supervision. The proposed RDCR framework integrates consistency regularization with self-supervised rotation predictions to decouple the model from noisy labels and enforce noise-tolerant representations. The key questions are whether this approach is effective for both types of noise and if it improves over current state-of-the-art methods. The experiments aim to demonstrate the superiority of RDCR, particularly under high noise levels.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a unified framework called Rotational-Decoupling Consistency Regularization (RDCR) to handle two types of label noise - synthetic noise in simplified noisy label (NL) settings and pseudo labels in semi-supervised learning (Semi-SL). 2. It argues that the generalization performance of existing methods is still highly coupled with the quality of noisy labels. To counter this, RDCR introduces auxiliary clean rotation labels to avoid overfitting to the noise while exploiting additional training signals from all input images.3. It integrates the consistency-based methods with the self-supervised rotation task to encourage more noise-tolerant feature representations. 4. It demonstrates through experiments that RDCR achieves comparable or better performance than state-of-the-art methods under low noise levels, and significantly outperforms them under high noise levels for both synthetic noise and pseudo labels.In summary, the key contribution is proposing a unified framework RDCR that leverages auxiliary clean labels from self-supervision to counter the two types of label noise in NL and Semi-SL. It shows superior robustness across different noise types and levels compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a unified framework called Rotational-Decoupling Consistency Regularization (RDCR) that leverages an auxiliary self-supervised rotation task to combat two types of label noise - synthetic noise in simplified noisy label learning and pseudo labels in semi-supervised learning - by decoupling from the noisy labels to stimulate data cleansing and exploit extra supervisions from all inputs regardless of noise level.
