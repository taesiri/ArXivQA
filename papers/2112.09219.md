# [All You Need is RAW: Defending Against Adversarial Attacks with Camera   Image Pipelines](https://arxiv.org/abs/2112.09219)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is: How can we exploit the raw image domain and image processing pipeline statistics as an empirical prior to defend against adversarial attacks on neural networks?In particular, the authors propose a new defense method that maps adversarially perturbed RGB images to raw sensor measurements and then reconstructs clean RGB images via learned and conventional image processing pipelines. The key hypothesis is that mapping through the intermediate raw image space, which captures natural image statistics before processing, can help remove adversarial perturbations more effectively compared to existing defenses that operate solely in RGB space.The experiments validate this hypothesis by showing their method outperforms previous input transformation defenses across different vision tasks and datasets. The ablation studies provide further analysis on how the raw image domain helps defend against adversarial attacks.In summary, the main novelty is using the raw image distribution and image processing pipelines as an empirical prior, rather than relying only on RGB image statistics as most prior work. The results demonstrate the effectiveness of this approach for adversarial defense.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new adversarial defense method that exploits the raw image domain to defend against adversarial attacks. The key ideas are:- Instead of learning a direct RGB-to-RGB mapping like existing defense methods, they propose learning a mapping via the intermediate raw image space. - They rely on the natural distribution of raw images, before being processed into RGB images, as an empirical prior to help remove adversarial perturbations.- They use both learned neural networks and conventional image processing pipelines to map between RGB, raw, and reconstructed RGB spaces. Using conventional non-differentiable pipelines enhances robustness.- Their defense method is model-agnostic, requiring no adversarial images for training, and acts as an off-the-shelf preprocessing module that generalizes to diverse tasks.- They provide analysis and experiments showing that exploiting the raw image domain helps defend against adversarial attacks. The "rawer" the intermediate space is, the more effective the defense is.- They demonstrate state-of-the-art defense performance on ImageNet classification, COCO semantic segmentation, and PASCAL VOC object detection, significantly outperforming prior defense methods.In summary, the key contribution is a new defense approach that exploits the raw image domain and distribution as an empirical prior to filter out adversarial perturbations, through a mapping via the raw space. This defense generalizes across tasks and datasets without needing adversarial training examples.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new adversarial defense method that maps perturbed RGB images to RAW space and back to RGB using learned camera image processing pipelines to eliminate adversarial patterns, achieving state-of-the-art defense performance without needing adversarial images for training.


## How does this paper compare to other research in the same field?

This paper proposes a new defense method against adversarial attacks by exploiting the raw image distribution as an empirical prior. Here are some key aspects in comparing it to prior work:- Most prior defense methods operate in RGB image space, while this paper utilizes the raw image space as an intermediate mapping space. Mapping through raw space allows exploiting statistics like sensor noise and optical aberrations that are lost in RGB images. This is a novel approach not explored before.- The method is model-agnostic, not requiring access to the target model or adversarial examples for training. Many prior defenses require retraining the model with adversarial data. In contrast, this acts as an off-the-shelf preprocessing module.- It does not rely on obfuscated gradients or randomness like some prior input transformation defenses. So it may be more robust to attacks circumventing such defenses.- The defense is comprehensively evaluated on multiple datasets (ImageNet, COCO, VOC) and tasks (classification, segmentation, detection). Most prior works only evaluate on classification on smaller datasets.- It significantly outperforms existing input transformation defenses across different attack methods and metrics. The gains are especially large on stronger attacks.- The paper provides useful analysis and ablations on the role of raw space, which provides insight into why the defense works.Overall, exploiting raw image statistics appears to be a promising new direction for building robust defenses. The comprehensive evaluation and strong performance make a good case this could be an impactful approach in the adversarial defense field. The model-agnostic nature and lack of reliance on adversarial data for training are also useful properties.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Explore RAW natural image statistics as an unsupervised prior for image reconstruction and generative neural rendering tasks. The authors propose using the RAW image data distribution as an empirical latent space in their defense method. They suggest this RAW data prior could be useful for other image processing tasks like reconstruction and rendering.- Evaluate the proposed defense method on additional datasets and tasks beyond those tested in the paper. The authors validate their method on ImageNet, COCO, and Pascal VOC for classification, segmentation, and detection. They suggest evaluating on more datasets and tasks. - Extend the method to video frames and 3D tasks. The current method operates on individual images. The authors propose applying it to video frames over time and 3D perception tasks.- Investigate using the proposed principles for undefended model training. The defense method relies on mapping via the RAW domain at inference time. The authors suggest exploring if similar principles could be incorporated during model training for inherent robustness.- Analyze how the method transfers under different training and test domain shifts. The authors use a street scene dataset for training but test on ImageNet object images, suggesting further domain shift experiments.- Evaluate defense robustness against adaptive attacks that target the method's components. Though robust to current attacks, designing attacks that target the RAW mapping specifically could reveal vulnerabilities.In summary, the main future directions are: exploring RAW data as a general prior for vision tasks, evaluating on more data domains, extending to video and 3D, using similar principles during model training, analyzing domain shift impacts, and testing against attacks targeting the defense components.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new model-agnostic defense method against adversarial attacks by leveraging raw image data as an empirical prior. Instead of learning a direct mapping between adversarially perturbed images and natural RGB images like existing defenses, the proposed method maps images to an intermediate raw image space and then converts them back to RGB images using both a learned ISP pipeline and a conventional ISP pipeline. By exploiting the natural statistics in raw images that are removed during image processing, the proposed defense is able to effectively eliminate adversarial perturbations. The method does not require any adversarial images for training and can be applied as an off-the-shelf preprocessing module for different models and tasks. Experiments on ImageNet, COCO, and Pascal VOC datasets validate that the proposed method significantly outperforms existing input transformation defenses across different vision tasks and attack methods. Ablation studies provide insights into how the raw image distribution helps defend against adversarial attacks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a new defense method against adversarial attacks on deep neural networks. The key idea is to exploit the raw image data distribution as an empirical prior for mapping adversarially perturbed images back to the natural image distribution. Existing defense methods try to directly map from the adversarial RGB image distribution to the natural RGB image distribution. However, RGB images are the result of processing raw sensor data through an image signal processing pipeline. This pipeline removes useful statistical information about the raw captures, like noise patterns and optical aberrations. By using the raw data as an intermediate mapping space, the proposed method can leverage this extra information to more effectively remove adversarial perturbations. The defense method involves learning three mappings - RGB to raw ($F$), raw to RGB ($G$), and raw to RGB ($S$) - on natural images only without any adversarial examples needed. At inference, an adversarial RGB image is mapped to raw via $F$, processed separately by $G$ and $S$ to get two natural RGB images, which are weighted-summed to get the final cleaned image. Extensive experiments on ImageNet classification and COCO/Pascal segmentation/detection validate the effectiveness of the proposed approach over existing methods. Ablation studies provide insights into how the raw image statistics help defend against adversarial attacks.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes an adversarial defense method that exploits raw image data as an intermediate empirical distribution prior. Instead of learning a direct mapping from adversarially perturbed RGB images to clean RGB images like existing defenses, the method learns a mapping via the raw capture space. It uses three operators - F maps perturbed RGB images to raw space, G maps raw images to clean RGB images using a learned neural network, and S maps raw images to clean RGB images using a non-differentiable conventional ISP pipeline. At inference, it maps a perturbed image to raw space with F, processes it through G and S separately, and combines the outputs in a weighted sum to get the final clean image. By using the raw image distribution as an intermediate space, it can leverage natural image statistics from raw captures that are lost when forming RGB images. The defense is model-agnostic, doesn't require adversarial images for training, and generalizes to unseen tasks.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- Existing neural networks for computer vision are vulnerable to adversarial attacks, where small perturbations to images can fool the models. The paper aims to defend against such attacks.- The paper proposes a new defense method that exploits the raw image domain as an empirical prior. Instead of learning an RGB-to-RGB mapping like prior defenses, they learn a mapping via the intermediate raw image space. - They use three operators: F maps perturbed RGB to raw, G maps raw to RGB using a learned ISP, and S maps raw to RGB using a conventional ISP. By combining G and S, the method is robust.- The defense is model-agnostic, requiring no adversarial images for training. It acts as an off-the-shelf preprocessing module that can be transferred to any task.- Experiments validate the method outperforms prior defenses on ImageNet classification, COCO segmentation, and VOC detection. Analysis provides insights into how the raw image domain helps defend attacks.In summary, the key innovation is using the raw image domain as an empirical prior to map perturbed images to benign ones, instead of just using the RGB image domain like prior work. This allows the defense to exploit raw image statistics to remove adversarial patterns.
