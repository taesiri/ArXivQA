# [Rebalancing Batch Normalization for Exemplar-based Class-Incremental   Learning](https://arxiv.org/abs/2201.12559)

## What is the central research question or hypothesis that this paper addresses?

 This paper titled "Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning" explores the research question of how to design an appropriate Batch Normalization (BN) mechanism for exemplar-based offline class-incremental learning (CIL). 

The key hypothesis is that the commonly used BN causes issues in exemplar-based offline CIL due to the imbalanced mini-batches between the abundant current task data and the limited exemplar data from previous tasks. Specifically, the paper hypothesizes that both the sample mean/variance and affine transformation parameters computed by BN become heavily biased towards the current task, which harms the model's ability to retain knowledge from previous tasks.

To address this, the paper proposes a new normalization scheme called Task-Balanced Batch Normalization (TBBN) that can calculate task-balanced statistics and learn the BN parameters in a balanced manner across all tasks. The core hypothesis is that by preventing BN statistics/parameters from being biased to the current task, TBBN can alleviate forgetting of previous tasks in offline CIL.

So in summary, the central hypothesis is that a properly designed task-balanced BN can effectively mitigate catastrophic forgetting in exemplar-based offline CIL by reducing the bias in model towards the current task. The paper aims to demonstrate this via the proposed TBBN method.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new normalization layer called Task-Balanced Batch Normalization (TBBN) for exemplar-based offline class-incremental learning (CIL). 

Specifically, the key contributions are:

1. The paper analyzes the problem of standard Batch Normalization (BN) in exemplar-based offline CIL, which is that the mean and variance statistics computed by BN during training become biased towards the current task due to imbalanced data. This causes issues during both training and inference.

2. To address this problem, the paper proposes TBBN which computes task-balanced mean and variance statistics during training via adaptive reshape and repeat operations on the input feature map. This allows proper normalization of features from both current and previous tasks.

3. The paper also proposes a method to train the affine transformation parameters (gamma, beta) of BN in a task-balanced manner during backpropagation. This is done by propagating the gradients through adapted reshape/repeat operations.

4. TBBN does not require any hyperparameter tuning as the reshape/repeat operations are determined adaptively based on the relative batch sizes of current vs previous tasks.

5. Extensive experiments on CIFAR-100, ImageNet-100 show that simply replacing BN with TBBN improves performance of various CIL methods by reducing forgetting. The gains are robust across datasets, architectures and even dissimilar tasks.

In summary, the paper identifies issues with BN in offline CIL and proposes a simple yet effective solution via a novel normalization layer TBBN, which computes proper statistics and trains parameters in a task-balanced way. The strength and wide applicability of TBBN is demonstrated through extensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new batch normalization method called Task-Balanced Batch Normalization (TBBN) that is designed to work well in exemplar-based offline class-incremental learning by calculating task-balanced mean/variance statistics and training the affine transformation parameters in a less biased way compared to standard batch normalization.

In a bit more detail:

The paper argues that standard batch normalization causes problems in exemplar-based offline class-incremental learning due to the imbalance between current and past tasks' data in each mini-batch. This imbalance biases the mean/variance statistics and gradients used to train the affine transformation parameters. The proposed TBBN method addresses this issue by using reshape/repeat operations to create balanced mini-batches and computing task-balanced mean/variance statistics. It also trains the affine transformation parameters in a less biased way using the balanced mini-batches. Experiments on CIFAR-100, ImageNet-100, and dissimilar tasks demonstrate that simply replacing batch normalization layers with the proposed TBBN improves performance of several state-of-the-art class-incremental learning methods.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in the field of continual learning:

- The paper focuses on class-incremental learning (CIL), which is an active area of research in continual learning. CIL aims to learn new classes continually without forgetting previous ones, which is a challenging problem. 

- The paper specifically looks at CIL in the context of computer vision tasks using CNN models. It points out that most prior CIL methods do not scrutinize the effect of batch normalization (BN) layers in the CNN models, even though BN can become problematic with the class imbalance between current and previous tasks.

- The key novelty is proposing a new normalization layer called Task-Balanced Batch Normalization (TBBN) that properly handles the class imbalance by computing task-balanced statistics and affine transformation parameters in the BN layers.

- This direction of modifying normalization layers for continual learning has been relatively less explored compared to other approaches like regularization methods, replay strategies, etc. A related work is Continual Normalization (CN) which combines BN with GroupNorm for online continual learning. However, the paper shows CN is not effective for offline CIL which is more realistic.

- The paper demonstrates strong empirical results by plugging TBBN into various CIL methods like EEIL, LUCIR, SS-IL, etc. and shows consistent gains over standard BN, especially in terms of reducing catastrophic forgetting. This highlights the wide applicability of their proposal.

- The results are shown on diverse datasets like CIFAR-100, ImageNet-100, five distinct tasks, etc. This evaluates TBBN under different data distributions and degrees of task similarity.

- Overall, by identifying an understudied aspect of normalization in lifelong learning and proposing a principled solution, the paper makes a valuable contribution. The consistent gains across models and datasets underscore the significance of proper handling of BN for CIL.

In summary, the paper carves out an interesting and underexplored research direction and offers useful practical insights for improving existing CIL techniques. The strength of the empirical analysis and general applicability of TBBN are noteworthy aspects.
