# [Rebalancing Batch Normalization for Exemplar-based Class-Incremental   Learning](https://arxiv.org/abs/2201.12559)

## What is the central research question or hypothesis that this paper addresses?

 This paper titled "Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning" explores the research question of how to design an appropriate Batch Normalization (BN) mechanism for exemplar-based offline class-incremental learning (CIL). 

The key hypothesis is that the commonly used BN causes issues in exemplar-based offline CIL due to the imbalanced mini-batches between the abundant current task data and the limited exemplar data from previous tasks. Specifically, the paper hypothesizes that both the sample mean/variance and affine transformation parameters computed by BN become heavily biased towards the current task, which harms the model's ability to retain knowledge from previous tasks.

To address this, the paper proposes a new normalization scheme called Task-Balanced Batch Normalization (TBBN) that can calculate task-balanced statistics and learn the BN parameters in a balanced manner across all tasks. The core hypothesis is that by preventing BN statistics/parameters from being biased to the current task, TBBN can alleviate forgetting of previous tasks in offline CIL.

So in summary, the central hypothesis is that a properly designed task-balanced BN can effectively mitigate catastrophic forgetting in exemplar-based offline CIL by reducing the bias in model towards the current task. The paper aims to demonstrate this via the proposed TBBN method.
