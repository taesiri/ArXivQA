# [Rebalancing Batch Normalization for Exemplar-based Class-Incremental   Learning](https://arxiv.org/abs/2201.12559)

## What is the central research question or hypothesis that this paper addresses?

 This paper titled "Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning" explores the research question of how to design an appropriate Batch Normalization (BN) mechanism for exemplar-based offline class-incremental learning (CIL). 

The key hypothesis is that the commonly used BN causes issues in exemplar-based offline CIL due to the imbalanced mini-batches between the abundant current task data and the limited exemplar data from previous tasks. Specifically, the paper hypothesizes that both the sample mean/variance and affine transformation parameters computed by BN become heavily biased towards the current task, which harms the model's ability to retain knowledge from previous tasks.

To address this, the paper proposes a new normalization scheme called Task-Balanced Batch Normalization (TBBN) that can calculate task-balanced statistics and learn the BN parameters in a balanced manner across all tasks. The core hypothesis is that by preventing BN statistics/parameters from being biased to the current task, TBBN can alleviate forgetting of previous tasks in offline CIL.

So in summary, the central hypothesis is that a properly designed task-balanced BN can effectively mitigate catastrophic forgetting in exemplar-based offline CIL by reducing the bias in model towards the current task. The paper aims to demonstrate this via the proposed TBBN method.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new normalization layer called Task-Balanced Batch Normalization (TBBN) for exemplar-based offline class-incremental learning (CIL). 

Specifically, the key contributions are:

1. The paper analyzes the problem of standard Batch Normalization (BN) in exemplar-based offline CIL, which is that the mean and variance statistics computed by BN during training become biased towards the current task due to imbalanced data. This causes issues during both training and inference.

2. To address this problem, the paper proposes TBBN which computes task-balanced mean and variance statistics during training via adaptive reshape and repeat operations on the input feature map. This allows proper normalization of features from both current and previous tasks.

3. The paper also proposes a method to train the affine transformation parameters (gamma, beta) of BN in a task-balanced manner during backpropagation. This is done by propagating the gradients through adapted reshape/repeat operations.

4. TBBN does not require any hyperparameter tuning as the reshape/repeat operations are determined adaptively based on the relative batch sizes of current vs previous tasks.

5. Extensive experiments on CIFAR-100, ImageNet-100 show that simply replacing BN with TBBN improves performance of various CIL methods by reducing forgetting. The gains are robust across datasets, architectures and even dissimilar tasks.

In summary, the paper identifies issues with BN in offline CIL and proposes a simple yet effective solution via a novel normalization layer TBBN, which computes proper statistics and trains parameters in a task-balanced way. The strength and wide applicability of TBBN is demonstrated through extensive experiments.
