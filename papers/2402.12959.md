# [Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)

## Summarize the paper in one sentence.

 This paper proposes novel prompt stealing attacks to reverse engineer the original prompts used with large language models based on their generated responses, and explores defenses against such attacks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing the first prompt stealing attacks against large language models (LLMs). Specifically:

1) The paper proposes a novel attack methodology containing two key components - a parameter extractor and a prompt reconstructor. The parameter extractor predicts properties about the original prompt (e.g. whether it's a direct, role-based, or in-context prompt). The prompt reconstructor then uses this information to reconstruct a similar prompt that generates comparable responses from the LLM.

2) The paper conducts extensive experiments evaluating the attack on two LLMs (ChatGPT and LLaMA) using two question-answering datasets (RetrievalQA and Alpaca-GPT4). Results demonstrate the attack can effectively steal prompts, with high similarity between original and reconstructed prompts/responses.

3) The paper also explores potential defense strategies, finding that techniques like adding perturbations to original prompts/responses can mitigate but not completely defend against attacks. This highlights the need for more research into robust defenses. 

Overall, the key contribution is introducing and evaluating the first methodology tailored to stealing prompts from LLMs based on their generated responses. The paper makes a case these models remain vulnerable to such attacks despite their capabilities, emphasizing the importance of further security research on LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Prompt stealing attacks 
- Large language models (LLMs)
- Prompt engineering 
- Parameter extractor
- Prompt reconstructor
- Direct prompts
- Role-based prompts  
- In-context prompts
- Prompt similarity 
- Answer similarity
- Defenses against prompt stealing attacks

The paper proposes a novel attack called "prompt stealing attacks" against large language models (LLMs) like ChatGPT. The goal is to reverse engineer or steal the original prompts used to generate responses from the LLMs, based only on the generated answers. 

The attack involves two main components - a "parameter extractor" to identify features of the original prompt, and a "prompt reconstructor" to generate a similar reconstructed prompt. Three main types of prompts are considered - direct, role-based, and in-context. 

The performance of the attacks is evaluated using metrics like prompt similarity and answer similarity between the original and reconstructed prompts. Potential defenses against such attacks are also explored.

So in summary, the key focus is on stealing prompts from LLMs, evaluating attack performance, and defenses against such novel prompt stealing attacks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a hierarchical design for the parameter extractor comprising a primary classifier and two sub-classifiers. What is the rationale behind this hierarchical architecture? How does it improve performance over a single classifier baseline?

2. The primary classifier categorizes prompts into three types - direct, role-based and in-context. What textual features of the generated answers allow this classification? How does the performance vary across prompt types?  

3. For role-based prompts, a sub-classifier predicts the role used in the original prompt. How are the most common roles identified for training this classifier? Does semantic relevance between role and prompt text improve classification accuracy?

4. For in-context prompts, a sub-classifier predicts the number of contexts used. What patterns in the generated text allow estimating context length? How does context length estimation performance degrade as the number of contexts increases?

5. The prompt reconstructor uses the primary classifier's predictions to modify a naively reconstructed prompt from ChatGPT. How do the different prompt types necessitate different modifications to the naively reconstructed text?

6. Prompt similarity and answer similarity are used to evaluate reconstructed prompts. Why is answer similarity a better metric than prompt similarity? What factors affect the gap between prompt and answer similarities?

7. How does the performance of the attacks vary between ChatGPT and LLaMA? What differences in the models' architectures explain this variation? 

8. How does training data size affect the performance of the primary and sub-classifiers? Is there a point of diminishing returns as more training data is added?

9. What is the impact of using randomly assigned versus semantically relevant roles in the role-based prompt dataset? How does this validate assumptions about role-based prompts?

10. How effective are the proposed defense strategies of perturbing prompts versus answers? What factors explain the tradeoff between defense efficacy and utility?
