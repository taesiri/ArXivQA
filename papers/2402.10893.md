# [RLVF: Learning from Verbal Feedback without Overgeneralization](https://arxiv.org/abs/2402.10893)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
As large language models (LLMs) are adopted in more diverse use cases, the ability to customize their behaviors based on nuanced user preferences becomes increasingly important. However, common approaches like supervised context distillation or reinforcement learning from human feedback require extensive example-level annotations, and fine-tuning on preferences provided in one context can lead to unpredictable and unintended behavior changes in other contexts. The paper studies the challenge of adapting LLMs to high-level verbal feedback without such overgeneralization.

Proposed Solution:
The paper proposes Contextualized Critiques with Constrained Preference Optimization (C3PO), a method that translates a piece of verbal feedback into a small synthetic preference dataset. It generates hypothetical prompts where the feedback should and should not apply, samples completions from the base LLM, and revises the completions to adhere to the feedback. It then fine-tunes the LLM to match the revised, feedback-adherent responses for relevant contexts while regularizing to preserve the original model's behavior for irrelevant contexts.   

Main Contributions:
- Identifies and quantifies the overgeneralization problem when incorporating high-level verbal feedback into LLMs using existing approaches
- Proposes a novel data generation scheme to translate verbal feedback into fine-tuning data denoting desired behavior changes and contexts where behavior should be preserved  
- Introduces a new fine-tuning objective that adheres to feedback for relevant contexts while minimizing divergence from the original model for other contexts
- Experiments show the method effectively applies feedback when relevant while reducing unintended behavior changes by over 30% compared to baselines

The key insight is that overgeneralization can be reduced by using synthetic data to discriminate contexts where feedback should and should not apply, paired with a regularization loss that preserves original model behavior for out-of-scope contexts.
