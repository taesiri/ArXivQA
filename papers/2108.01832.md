# [Offline Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2108.01832)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is:How can we develop an effective framework for offline decentralized multi-agent reinforcement learning, where each agent learns from an independent dataset without interacting with other agents? Specifically, the paper aims to tackle two key challenges:1) Transition bias: The mismatch between the transition dynamics in the offline dataset versus the true dynamics induced by the learned policies of other agents during execution. This can lead to large errors in value estimates.2) Miscoordination: Due to decentralized training on diverse datasets, agents may arrive at very different state value estimates, leading to uncoordinated policies. To address these issues, the paper proposes a framework with two key components:1) Value deviation: Deliberately increasing the transition probabilities of high-value next states during training. This helps correct for the underestimation caused by poor behavior policies of other agents.2) Transition normalization: Normalizing the transition probabilities to be more uniform across agents. This helps build consensus on value estimates and improve coordination.The key hypothesis is that combining value deviation and transition normalization will enable effective decentralized multi-agent learning from offline datasets, leading to higher-performing and more coordinated policies.In summary, this paper provides a novel framework to enable offline decentralized MARL by handling the key challenges of transition bias and miscoordination between independently learned agents.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for offline decentralized multi-agent reinforcement learning. The key ideas are:- Introducing value deviation and transition normalization to address the mismatch between the transition dynamics in the offline dataset and the real ones during execution. - Value deviation increases the transition probabilities of high-value next states to correct the underestimation of potential good actions hidden by poor behavior policies. - Transition normalization enforces a uniform distribution over next states to build consensus on value estimates among agents and promote coordination.- Theoretically proving the convergence of Q-learning under the purposely controlled non-stationary transition dynamics.- Showing that value deviation and transition normalization can be integrated into existing offline RL algorithms by taking effect as weights of the objective function. - Providing an instantiation of the framework on BCQ, termed MABCQ, and experimentally demonstrating that it substantially outperforms baselines on a variety of multi-agent tasks.In summary, this is the first work to address the challenging problem of offline decentralized multi-agent RL. It proposes a principled framework to tackle the mismatch of transition dynamics and miscoordination of policies. Theoretically and empirically, it shows the framework enables agents to learn coordinated policies from decentralized offline datasets.
