# [GiT: Towards Generalist Vision Transformer through Universal Language   Interface](https://arxiv.org/abs/2403.09394)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Vision and language models still have different architectures, hindering progress towards universal foundation models like GPT. Vision models typically need task-specific modules, whereas language models use a simple transformer stack.
- Unifying object, pixel, and image-level vision tasks is challenging due to their varying output representations.

Proposed Solution: 
- Introduce GiT, a generalist vision transformer, with only a vanilla vision transformer (ViT) stack.
- Design a flexible parallel decoding template to unify tasks across image, object, and pixel levels using point prompts.
- Convert all inputs/outputs to tokens, enabling sequence modeling. Use a standard vocabulary and compress multi-word concepts into single tokens.  
- Jointly train on 27 datasets across 5 core vision tasks without task-specific tuning.

Main Contributions:
- Propose the first simple transformer-only model for various vision tasks through a universal language interface.
- Achieve strong generalist capability by weight sharing and multi-task learning, like LLMs. Outperform prior generalist vision models.  
- Demonstrate strong zero-shot transfer and adaptation with just 27-dataset pre-training, highlighting simplicity.
- Simplify model design while boosting versatility, moving towards aligning vision and language architectures.

In summary, the paper introduces GiT, a straightforward yet powerful vision transformer that can handle diverse vision tasks through a flexible parallel decoding approach and token-based interfacing. By simplifying model design for vision, it takes a step towards universal foundation models akin to LLMs.
