# [Implicit Bias of Policy Gradient in Linear Quadratic Control:   Extrapolation to Unseen Initial States](https://arxiv.org/abs/2402.07875)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies the implicit bias of policy gradient methods in reinforcement learning/optimal control problems, specifically in terms of their ability to extrapolate to unseen initial states. 
- Extrapolation is important in real-world control problems where the controller may encounter new situations during deployment. However, current RL algorithms often generalize poorly to new initial states.
- The paper focuses its analysis on the linear quadratic regulator (LQR) problem as a testbed for studying this question theoretically.

Main Contributions:

1. The paper proves theoretically that the extent of extrapolation by policy gradient in underdetermined LQR problems depends on the "exploration" induced by the system dynamics when starting from the initial states used during training.

2. In settings without sufficient exploration, extrapolation does not occur and the learned controller behaves similarly to a "no-extrapolation" baseline. However, with an exploration-encouraging system and initial state choice, extrapolation can become perfect.

3. For a typical setting with random system dynamics and an arbitrary training initial state, the paper shows (in expectation and with high probability) that some non-trivial extrapolation occurs, owing to the generic exploratory behavior.

4. Experiments on LQR and complex nonlinear control problems corroborate the theory and demonstrate its applicability beyond linear systems.

5. Through advanced mathematical tools, the paper also reveals differences from implicit bias results in supervised learning. This contrasts the common belief that such results translate directly to reinforcement learning.

Proposed Solution:

- The paper introduces a mathematical framework to quantify extrapolation via new optimality and cost measures tailored to RL/control.

- The analysis links these measures to the degree of "exploration" occurring during training, formalized as the overlap between trajectories from training initial states and directions orthogonal to them.

- By constructing appropriate LQR examples, the paper bounds extrapolation in different exploration regimes, thereby elucidating the interplay between system dynamics and extrapolation.

The paper hypothesizes that explicitly optimizing exploration may significantly improve generalization in real-world control problems. Overall, this work opens promising directions at the intersection of implicit bias and reinforcement learning.
