# [Robustness via Retrying: Closed-Loop Robotic Manipulation with   Self-Supervised Learning](https://arxiv.org/abs/1810.03043)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to use self-supervised predictive models of raw visual observations for more complex and temporally extended robotic manipulation tasks. Specifically, it aims to tackle the challenges of using imperfect video prediction models for closed-loop control and long-horizon planning.The key hypotheses are:1) Even an imperfect video prediction model can complete complex manipulation tasks if it is used in a closed-loop fashion with continuous retrying.2) Image-to-image registration can provide a grounded mechanism for evaluating the planning costs of predicted futures, enabling persistent retrying.3) Self-supervised registration of current and goal images allows tracking of user-specified objects for long durations.4) Combining short-horizon planning, continuous replanning (MPC), and registration-based cost evaluation enables temporally extended manipulation skills from video prediction models.In summary, the central research question is how self-supervised video prediction models can be utilized for complex, long-horizon robotic manipulation, which requires addressing challenges in tracking, cost evaluation, and closed-loop control. The key hypotheses provide possible solutions through image registration, short-horizon replanning, and persistent retrying.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method for closed-loop control of robotic manipulation tasks using self-supervised video prediction models. Specifically:- They propose using learned image-to-image registration to define a planning cost function for model predictive control with raw image observations. This allows tracking and retrying during execution to accomplish long-horizon tasks.- They demonstrate a self-supervised method to train the registration model using the same dataset collected for training the video prediction model.- They show this approach can enable complex robotic manipulation skills like grasping and non-prehensile pushing from raw visual inputs, without manual engineering or object models.- They demonstrate real-world robotic manipulation on long-horizon tasks with objects not seen during training. The method acquires skills like repositioning objects after 150 hours of autonomous data collection.- They extend the approach to handle multiple camera views and 3D space.In summary, the key contribution is developing a method to close the control loop for visual MPC, enabling persistent retrying and complex manipulation skills to be acquired through autonomous interaction and video prediction models. The self-supervised registration technique is the main element that unlocks the ability to perform long-horizon tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method for closed-loop robotic manipulation using self-supervised learning, where an imperfect video prediction model can complete complex long-horizon tasks by continuously retrying through the use of a learned image registration cost function that tracks progress towards goal images.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in robotic manipulation and self-supervised learning:- This paper builds on prior work on learning robotic manipulation skills from video prediction models trained with self-supervised data. In particular, it extends methods like Visual Foresight and SNA by proposing a new mechanism for tracking progress towards goals over long time horizons. - The key novelty is the use of a learned image registration model to dynamically update the goal/cost representations during visual model predictive control. This allows persistent retrying and recovery from failures. Prior self-supervised manipulation methods lacked this capability.- For training, the self-supervised data collection process is similar to prior work, with some additions like the grasping reflex to increase prehensile behaviors. The focus is more on how the model is used at test time.- Compared to visual servoing methods, this work uses learning and multi-step predictive models rather than reactive control laws. It also handles non-prehensile manipulation unlike most servoing approaches.- For goal specification, using designated pixels in goal images is related to prior work on learning from play and image goals. The key difference is the use of registration rather than feature distances.- The experiments demonstrate substantially longer and more complex tasks compared to prior self-supervised manipulation work, thanks to the retrying capability. The approach also combines pushing and grasping skills.- Limitations include reliance on short-horizon planning, lack of semantics or abstraction, and issues scaling to multi-object tasks. The method still requires some manual specification.In summary, this work makes important advances in self-supervised robotic manipulation by introducing a way to achieve persistent retrying during visual MPC, enabling more complex long-horizon behaviors. The comparisons show meaningful improvements over prior self-supervised methods.
