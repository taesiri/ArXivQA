# [Robustness via Retrying: Closed-Loop Robotic Manipulation with   Self-Supervised Learning](https://arxiv.org/abs/1810.03043)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to use self-supervised predictive models of raw visual observations for more complex and temporally extended robotic manipulation tasks. Specifically, it aims to tackle the challenges of using imperfect video prediction models for closed-loop control and long-horizon planning.The key hypotheses are:1) Even an imperfect video prediction model can complete complex manipulation tasks if it is used in a closed-loop fashion with continuous retrying.2) Image-to-image registration can provide a grounded mechanism for evaluating the planning costs of predicted futures, enabling persistent retrying.3) Self-supervised registration of current and goal images allows tracking of user-specified objects for long durations.4) Combining short-horizon planning, continuous replanning (MPC), and registration-based cost evaluation enables temporally extended manipulation skills from video prediction models.In summary, the central research question is how self-supervised video prediction models can be utilized for complex, long-horizon robotic manipulation, which requires addressing challenges in tracking, cost evaluation, and closed-loop control. The key hypotheses provide possible solutions through image registration, short-horizon replanning, and persistent retrying.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method for closed-loop control of robotic manipulation tasks using self-supervised video prediction models. Specifically:- They propose using learned image-to-image registration to define a planning cost function for model predictive control with raw image observations. This allows tracking and retrying during execution to accomplish long-horizon tasks.- They demonstrate a self-supervised method to train the registration model using the same dataset collected for training the video prediction model.- They show this approach can enable complex robotic manipulation skills like grasping and non-prehensile pushing from raw visual inputs, without manual engineering or object models.- They demonstrate real-world robotic manipulation on long-horizon tasks with objects not seen during training. The method acquires skills like repositioning objects after 150 hours of autonomous data collection.- They extend the approach to handle multiple camera views and 3D space.In summary, the key contribution is developing a method to close the control loop for visual MPC, enabling persistent retrying and complex manipulation skills to be acquired through autonomous interaction and video prediction models. The self-supervised registration technique is the main element that unlocks the ability to perform long-horizon tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method for closed-loop robotic manipulation using self-supervised learning, where an imperfect video prediction model can complete complex long-horizon tasks by continuously retrying through the use of a learned image registration cost function that tracks progress towards goal images.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in robotic manipulation and self-supervised learning:- This paper builds on prior work on learning robotic manipulation skills from video prediction models trained with self-supervised data. In particular, it extends methods like Visual Foresight and SNA by proposing a new mechanism for tracking progress towards goals over long time horizons. - The key novelty is the use of a learned image registration model to dynamically update the goal/cost representations during visual model predictive control. This allows persistent retrying and recovery from failures. Prior self-supervised manipulation methods lacked this capability.- For training, the self-supervised data collection process is similar to prior work, with some additions like the grasping reflex to increase prehensile behaviors. The focus is more on how the model is used at test time.- Compared to visual servoing methods, this work uses learning and multi-step predictive models rather than reactive control laws. It also handles non-prehensile manipulation unlike most servoing approaches.- For goal specification, using designated pixels in goal images is related to prior work on learning from play and image goals. The key difference is the use of registration rather than feature distances.- The experiments demonstrate substantially longer and more complex tasks compared to prior self-supervised manipulation work, thanks to the retrying capability. The approach also combines pushing and grasping skills.- Limitations include reliance on short-horizon planning, lack of semantics or abstraction, and issues scaling to multi-object tasks. The method still requires some manual specification.In summary, this work makes important advances in self-supervised robotic manipulation by introducing a way to achieve persistent retrying during visual MPC, enabling more complex long-horizon behaviors. The comparisons show meaningful improvements over prior self-supervised methods.


## What future research directions do the authors suggest?

The paper suggests several future research directions:- Apply the method to more complex multi-object manipulation tasks. The current method was demonstrated on single object manipulation tasks. Extending it to multi-object tasks may require incorporating high-level planning on top of the visual MPC framework.- Explore sharing parameters between the video prediction model and image registration model. Currently these are separate models trained on the same data, but sharing parameters might improve efficiency and performance. - Investigate adaptive schemes for deciding when to register to the start image versus the goal image, rather than using the photometric distance heuristic. This could further improve tracking robustness.- Evaluate the benefits of learning shared latent state representations between the predictor and controller, as has been done in some model-based RL works. The current method trains the predictor and controller separately.- Experiment with more advanced planning and exploration techniques on top of the visual MPC framework. For example, information-theoretic planning for exploration or hybrid model-free and model-based methods.- Apply the method to manipulating deformable objects like cloth, where precise pixel-level tracking is more difficult. New techniques may be needed to handle such objects.- Explore whether the registration technique could enable imitation learning from human demonstrations, for example by registering demonstration images to the robot's current view.In summary, the main future directions are developing extensions for more complex tasks, improving the efficiency and robustness of the self-supervised models, and incorporating the approach into more sophisticated planning and exploration frameworks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a method for closed-loop robotic manipulation using self-supervised learning and video prediction models. The key ideas are: 1) An action-conditioned video prediction model is trained on unlabeled robot interaction data to predict future frames. 2) This prediction model is used in a model predictive control framework to plan actions to accomplish user-specified goals, represented as changes in pixel positions. 3) To enable accurate tracking of goals throughout an episode, even when predictions are imperfect, the method learns an image registration function that matches the current image to both the start and goal image. This registration provides a planning cost measuring distance to the goal image. 4) With retrying enabled by the registration, temporally extended manipulation tasks can be accomplished. 5) The same unlabeled robot interaction data is used to train both the video prediction and image registration models in a self-supervised fashion, with no human labeling required. The method is shown to enable pushing, grasping, and non-prehensile manipulation of novel objects not seen during training.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes a method for learning robotic manipulation skills from raw image observations, using only autonomously collected, unlabeled data. The approach uses a video prediction model to forecast future frames conditioned on candidate action sequences. A key challenge is that inevitable inaccuracies in the video prediction model make it difficult to perform temporally extended tasks. The main contribution is a method to enable persistent retrying and error correction by registering the current image to designated source pixels in the initial frame and goal frame. This allows computing the distance to the goal state for visual model predictive control, enabling tasks to be completed successfully even with imperfect predictions. A self-supervised registration model is trained on the same unlabeled robotic experience as the predictor. The registration network puts the current image in correspondence with the initial and goal image in order to locate the designated pixels. Experiments demonstrate that this approach substantially improves performance on long-horizon manipulation tasks, and enables learning of combined prehensile and non-prehensile behaviors like grasping and pushing. The self-supervised visual MPC method achieves comparable performance to a baseline using a supervised object tracker on real-world object manipulation tasks.In summary, the key ideas presented are:1) A video prediction model for planning robotic manipulation from pixels.2) Persistent retrying and error correction during task execution using a learned image registration network to keep track of goal state. 3) Self-supervised training of the prediction and registration models using only unlabeled robotic experience.4) Demonstrated ability to acquire complex prehensile and non-prehensile manipulation skills.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper proposes a method for learning robotic manipulation skills from raw image observations using self-supervised predictive models. The key idea is to use model predictive control (MPC) with a learned video prediction model to plan actions to achieve goal images specified by a user. To enable persistent retrying even when predictions are inaccurate, they propose learning an image registration model in a self-supervised manner to align the current image with the goal image. This allows computing distances to the goal for use in the MPC cost function. By registering to both the initial and goal image, the method can recover if the tracking fails with respect to one of the images. The video prediction and registration models are trained on the same dataset collected autonomously by the robot through random exploration. Experiments on real robotic manipulation tasks demonstrate that, by continuous retrying, the approach can achieve complex long-horizon tasks more robustly than prior self-supervised manipulation methods.


## What problem or question is the paper addressing?

The paper is addressing the challenge of using self-supervised video prediction models for closed-loop control of robotic manipulation tasks. Specifically, it aims to enable video prediction models to perform longer, more complex manipulation tasks by proposing a method that allows the robot to continuously correct its behavior and retry when predictions are inaccurate.The key problems/questions it tackles are:- How can video prediction models be used for closed-loop control when predictions will inevitably be imperfect?- How can the robot keep track of task objectives (e.g. objects to be moved) over long time horizons, so it can retry tasks until successful completion? - How can video prediction models acquired through self-supervision learn both prehensile (grasping) and non-prehensile (pushing) manipulation skills?- How can video prediction models scale to multiple camera views to enable full 3D control?The main contribution is a method that uses learned image registration to define a planning cost function that allows persistent retrying and task tracking, enabling video prediction models to succeed at long-horizon manipulation tasks.
