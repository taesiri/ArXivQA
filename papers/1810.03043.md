# [Robustness via Retrying: Closed-Loop Robotic Manipulation with   Self-Supervised Learning](https://arxiv.org/abs/1810.03043)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to use self-supervised predictive models of raw visual observations for more complex and temporally extended robotic manipulation tasks. Specifically, it aims to tackle the challenges of using imperfect video prediction models for closed-loop control and long-horizon planning.

The key hypotheses are:

1) Even an imperfect video prediction model can complete complex manipulation tasks if it is used in a closed-loop fashion with continuous retrying.

2) Image-to-image registration can provide a grounded mechanism for evaluating the planning costs of predicted futures, enabling persistent retrying.

3) Self-supervised registration of current and goal images allows tracking of user-specified objects for long durations.

4) Combining short-horizon planning, continuous replanning (MPC), and registration-based cost evaluation enables temporally extended manipulation skills from video prediction models.

In summary, the central research question is how self-supervised video prediction models can be utilized for complex, long-horizon robotic manipulation, which requires addressing challenges in tracking, cost evaluation, and closed-loop control. The key hypotheses provide possible solutions through image registration, short-horizon replanning, and persistent retrying.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a method for closed-loop control of robotic manipulation tasks using self-supervised video prediction models. Specifically:

- They propose using learned image-to-image registration to define a planning cost function for model predictive control with raw image observations. This allows tracking and retrying during execution to accomplish long-horizon tasks.

- They demonstrate a self-supervised method to train the registration model using the same dataset collected for training the video prediction model.

- They show this approach can enable complex robotic manipulation skills like grasping and non-prehensile pushing from raw visual inputs, without manual engineering or object models.

- They demonstrate real-world robotic manipulation on long-horizon tasks with objects not seen during training. The method acquires skills like repositioning objects after 150 hours of autonomous data collection.

- They extend the approach to handle multiple camera views and 3D space.

In summary, the key contribution is developing a method to close the control loop for visual MPC, enabling persistent retrying and complex manipulation skills to be acquired through autonomous interaction and video prediction models. The self-supervised registration technique is the main element that unlocks the ability to perform long-horizon tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a method for closed-loop robotic manipulation using self-supervised learning, where an imperfect video prediction model can complete complex long-horizon tasks by continuously retrying through the use of a learned image registration cost function that tracks progress towards goal images.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in robotic manipulation and self-supervised learning:

- This paper builds on prior work on learning robotic manipulation skills from video prediction models trained with self-supervised data. In particular, it extends methods like Visual Foresight and SNA by proposing a new mechanism for tracking progress towards goals over long time horizons. 

- The key novelty is the use of a learned image registration model to dynamically update the goal/cost representations during visual model predictive control. This allows persistent retrying and recovery from failures. Prior self-supervised manipulation methods lacked this capability.

- For training, the self-supervised data collection process is similar to prior work, with some additions like the grasping reflex to increase prehensile behaviors. The focus is more on how the model is used at test time.

- Compared to visual servoing methods, this work uses learning and multi-step predictive models rather than reactive control laws. It also handles non-prehensile manipulation unlike most servoing approaches.

- For goal specification, using designated pixels in goal images is related to prior work on learning from play and image goals. The key difference is the use of registration rather than feature distances.

- The experiments demonstrate substantially longer and more complex tasks compared to prior self-supervised manipulation work, thanks to the retrying capability. The approach also combines pushing and grasping skills.

- Limitations include reliance on short-horizon planning, lack of semantics or abstraction, and issues scaling to multi-object tasks. The method still requires some manual specification.

In summary, this work makes important advances in self-supervised robotic manipulation by introducing a way to achieve persistent retrying during visual MPC, enabling more complex long-horizon behaviors. The comparisons show meaningful improvements over prior self-supervised methods.


## What future research directions do the authors suggest?

 The paper suggests several future research directions:

- Apply the method to more complex multi-object manipulation tasks. The current method was demonstrated on single object manipulation tasks. Extending it to multi-object tasks may require incorporating high-level planning on top of the visual MPC framework.

- Explore sharing parameters between the video prediction model and image registration model. Currently these are separate models trained on the same data, but sharing parameters might improve efficiency and performance. 

- Investigate adaptive schemes for deciding when to register to the start image versus the goal image, rather than using the photometric distance heuristic. This could further improve tracking robustness.

- Evaluate the benefits of learning shared latent state representations between the predictor and controller, as has been done in some model-based RL works. The current method trains the predictor and controller separately.

- Experiment with more advanced planning and exploration techniques on top of the visual MPC framework. For example, information-theoretic planning for exploration or hybrid model-free and model-based methods.

- Apply the method to manipulating deformable objects like cloth, where precise pixel-level tracking is more difficult. New techniques may be needed to handle such objects.

- Explore whether the registration technique could enable imitation learning from human demonstrations, for example by registering demonstration images to the robot's current view.

In summary, the main future directions are developing extensions for more complex tasks, improving the efficiency and robustness of the self-supervised models, and incorporating the approach into more sophisticated planning and exploration frameworks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a method for closed-loop robotic manipulation using self-supervised learning and video prediction models. The key ideas are: 1) An action-conditioned video prediction model is trained on unlabeled robot interaction data to predict future frames. 2) This prediction model is used in a model predictive control framework to plan actions to accomplish user-specified goals, represented as changes in pixel positions. 3) To enable accurate tracking of goals throughout an episode, even when predictions are imperfect, the method learns an image registration function that matches the current image to both the start and goal image. This registration provides a planning cost measuring distance to the goal image. 4) With retrying enabled by the registration, temporally extended manipulation tasks can be accomplished. 5) The same unlabeled robot interaction data is used to train both the video prediction and image registration models in a self-supervised fashion, with no human labeling required. The method is shown to enable pushing, grasping, and non-prehensile manipulation of novel objects not seen during training.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes a method for learning robotic manipulation skills from raw image observations, using only autonomously collected, unlabeled data. The approach uses a video prediction model to forecast future frames conditioned on candidate action sequences. A key challenge is that inevitable inaccuracies in the video prediction model make it difficult to perform temporally extended tasks. The main contribution is a method to enable persistent retrying and error correction by registering the current image to designated source pixels in the initial frame and goal frame. This allows computing the distance to the goal state for visual model predictive control, enabling tasks to be completed successfully even with imperfect predictions. A self-supervised registration model is trained on the same unlabeled robotic experience as the predictor. The registration network puts the current image in correspondence with the initial and goal image in order to locate the designated pixels. Experiments demonstrate that this approach substantially improves performance on long-horizon manipulation tasks, and enables learning of combined prehensile and non-prehensile behaviors like grasping and pushing. The self-supervised visual MPC method achieves comparable performance to a baseline using a supervised object tracker on real-world object manipulation tasks.

In summary, the key ideas presented are:
1) A video prediction model for planning robotic manipulation from pixels.
2) Persistent retrying and error correction during task execution using a learned image registration network to keep track of goal state. 
3) Self-supervised training of the prediction and registration models using only unlabeled robotic experience.
4) Demonstrated ability to acquire complex prehensile and non-prehensile manipulation skills.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper proposes a method for learning robotic manipulation skills from raw image observations using self-supervised predictive models. The key idea is to use model predictive control (MPC) with a learned video prediction model to plan actions to achieve goal images specified by a user. To enable persistent retrying even when predictions are inaccurate, they propose learning an image registration model in a self-supervised manner to align the current image with the goal image. This allows computing distances to the goal for use in the MPC cost function. By registering to both the initial and goal image, the method can recover if the tracking fails with respect to one of the images. The video prediction and registration models are trained on the same dataset collected autonomously by the robot through random exploration. Experiments on real robotic manipulation tasks demonstrate that, by continuous retrying, the approach can achieve complex long-horizon tasks more robustly than prior self-supervised manipulation methods.


## What problem or question is the paper addressing?

 The paper is addressing the challenge of using self-supervised video prediction models for closed-loop control of robotic manipulation tasks. Specifically, it aims to enable video prediction models to perform longer, more complex manipulation tasks by proposing a method that allows the robot to continuously correct its behavior and retry when predictions are inaccurate.

The key problems/questions it tackles are:

- How can video prediction models be used for closed-loop control when predictions will inevitably be imperfect?

- How can the robot keep track of task objectives (e.g. objects to be moved) over long time horizons, so it can retry tasks until successful completion? 

- How can video prediction models acquired through self-supervision learn both prehensile (grasping) and non-prehensile (pushing) manipulation skills?

- How can video prediction models scale to multiple camera views to enable full 3D control?

The main contribution is a method that uses learned image registration to define a planning cost function that allows persistent retrying and task tracking, enabling video prediction models to succeed at long-horizon manipulation tasks.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some key terms and keywords that seem most relevant are:

- Visual model-predictive control (visual MPC) - Using predicted future video frames for visual feedback control.

- Image registration - Finding correspondences between images, e.g. warping one image to match another. Used to track objects across frames. 

- Self-supervised learning - Learning from unlabeled data without human supervision, e.g. by predicting future frames.

- Retrying - Continuously re-attempting a task using closed-loop control until successful. Enabled by image registration for tracking.

- Object manipulation - Using robotic manipulation (pushing, grasping) to reposition objects. 

- Prehensile/non-prehensile manipulation - Grasping vs pushing objects.

- Autonomous robot learning - Robots learning skills and models without human supervision, through self-generated experiences.

Some other potentially relevant terms: video prediction models, deep learning, model-predictive control, visual servoing, robotic manipulation.

The key ideas seem to be using self-supervised video prediction models for control, and enabling persistent retrying of tasks by tracking objects through learned image registration. The method is applied to object manipulation tasks using both prehensile and non-prehensile strategies.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 questions that could help create a comprehensive summary of the paper:

1. What is the goal or purpose of this research? What problem is it trying to solve?

2. What is the proposed approach or method? How does it work at a high level? 

3. What are the key innovations or novel contributions? How is this approach different from prior work?

4. What is the robotic system used for experiments? What are the inputs, outputs, actions, etc.?

5. How is the data collected and preprocessed? What datasets are used?

6. How are the models trained? What loss functions, architectures, hyperparameters, etc. are used? 

7. How is the method evaluated? What metrics are used to measure performance?

8. What are the main experimental results? How does the proposed method compare to baselines or prior work?

9. What are the limitations of the current approach? What issues remain unsolved?

10. What are the major conclusions? What directions for future work are suggested?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using image-to-image registration to keep track of the object of interest during visual model predictive control. How does this help with long-horizon tasks compared to prior methods like propagating pixel probabilities forward? What are the limitations of propagation that registration addresses?

2. The paper trains the registration model in a self-supervised manner using the same dataset collected for training the video prediction model. What is the advantage of this approach compared to using an off-the-shelf registration or tracking method? How does the registration network architecture and training process enable this self-supervised training? 

3. The method registers the current image to both the initial and goal image. Why is registering to both images helpful compared to just one? In what cases might each of these registrations fail during a task execution?

4. Explain how the weighting factors Î»i are calculated from the photometric distance between true and warped images. Why does this provide a reasonable estimate of local registration success? What are limitations of using photometric distance to determine weighting?

5. The method combines pushing and grasping skills in a unified framework by incorporating a grasping reflex during data collection. How does this allow emergence of prehensile behaviors from the self-supervised predictive model? What are the advantages over sampling grasping actions randomly?

6. The experiments show significant improvements on long-horizon tasks compared to prior visual MPC methods. Analyze the results and explain why registration-based cost evaluation enables persistent retrying and task completion. Are there still limitations?

7. The paper scales up the approach to multi-view control and 3D tasks. Explain how pixel distances in 2D image space can be combined across views to define 3D manipulation tasks. What are the main benefits of multi-view modeling?

8. Compare the proposed visual MPC method to traditional visual servoing approaches. What are key differences in problem formulation, assumptions, and capabilities? When might each approach be more suitable?

9. The method relies on short-horizon action predictions and replanning. Explain how this model predictive control scheme compensates for inaccuracies in long-term prediction. Are there still limitations in handling compounded prediction errors over many timesteps?

10. The experiments focus on object pushing and grasping. How might the approach extend to more complex, multi-object tasks? What new challenges arise in pickup and placement tasks requiring reasoning over multiple objects?


## Summarize the paper in one sentence.

 The paper proposes a method for using learned image registration within model predictive control frameworks to enable robotic manipulation tasks with raw visual inputs.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes a method for enabling robots to learn complex manipulation skills from raw image observations in a self-supervised manner. The key idea is to use model predictive control with a learned video prediction model to plan actions to achieve a user-specified goal image. To enable persistent retrying and recovery from failures, they propose learning an image registration model to track the objects of interest throughout the task execution. This allows computing distances to the goal image to define the cost function for model predictive control. They demonstrate that, by continuously re-planning using the learned video prediction and image registration models, their approach can successfully perform long-horizon manipulation tasks including pushing, grasping, and object relocation. The models are trained on unlabeled robot-collected data without human supervision. Experiments on real robotic systems indicate their method can manipulate new objects not seen during training more reliably than prior self-supervised manipulation methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using learned image registration to enable retrying and closed-loop control in visual model-predictive control (MPC). How does this registration-based cost function compare to prior methods like using pixel-wise image distances or flow-based tracking? What are the key benefits of the registration approach?

2. The registration network is trained in a self-supervised manner on the same data used to train the video prediction model. How does this self-supervised training procedure work? What loss functions are used to train the registration network? 

3. The paper proposes registering the current image to both the start and goal image. Why is this beneficial compared to registering to just one of these images? How does the weighting scheme help handle cases where one registration succeeds and the other fails?

4. The registration network is used to find the current pixel locations corresponding to designated pixels in the start and goal images. How are these pixel correspondences used to define the MPC cost function for planning? 

5. The method is extended to use multiple camera views to enable full 3D manipulation tasks. How does the multi-view registration scheme work? How are the costs from different views combined?

6. Prehensile manipulation skills like grasping are incorporated by using a simple reflex during data collection. How does this allow the emergence of combined pushing and grasping skills? How does the method choose between pushing and grasping during test time?

7. The experiments show that the method outperforms using an off-the-shelf tracker for long-horizon pushing tasks. Why does the tracker perform poorly on these tasks compared to the learned registration approach?

8. For prehensile and non-prehensile manipulation, the method achieves comparable performance to using OpenCV tracking. What are the advantages of the self-supervised registration approach compared to supervised tracking?

9. The paper focuses on relatively simple planar manipulation tasks. What challenges need to be addressed to scale the approach to more complex 3D tasks and environments?

10. An interesting direction for future work is incorporating long-term planning on top of the visual MPC framework. What benefits could a hierarchical planning approach provide? How could high-level planning be integrated algorithmically?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a method for closed-loop robotic manipulation using self-supervised learning from raw image observations. The key idea is to use model predictive control (MPC) with a learned video prediction model to plan actions, while utilizing a learned image registration model to track progress towards goals specified by images. Image registration enables "retrying" by keeping track of objects over long horizons, overcoming limitations of prior work on visual MPC which could only perform short tasks. The video prediction and image registration models are trained in a completely self-supervised manner using autonomously collected experience. Experiments demonstrate that this approach can manipulate objects more accurately over longer horizons compared to prior methods. The method is also extended to utilize multiple cameras to specify 3D goals and combine grasping and pushing actions without any manual engineering. Results show that the robot can choose to grasp or push objects and retry actions multiple times until completing long-horizon rearrangement tasks. The key insight is that, through persistent retrying guided by learned image registration, even imperfect predictive models can perform complex temporally extended manipulation skills.
