# [Robustness via Retrying: Closed-Loop Robotic Manipulation with   Self-Supervised Learning](https://arxiv.org/abs/1810.03043)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to use self-supervised predictive models of raw visual observations for more complex and temporally extended robotic manipulation tasks. Specifically, it aims to tackle the challenges of using imperfect video prediction models for closed-loop control and long-horizon planning.The key hypotheses are:1) Even an imperfect video prediction model can complete complex manipulation tasks if it is used in a closed-loop fashion with continuous retrying.2) Image-to-image registration can provide a grounded mechanism for evaluating the planning costs of predicted futures, enabling persistent retrying.3) Self-supervised registration of current and goal images allows tracking of user-specified objects for long durations.4) Combining short-horizon planning, continuous replanning (MPC), and registration-based cost evaluation enables temporally extended manipulation skills from video prediction models.In summary, the central research question is how self-supervised video prediction models can be utilized for complex, long-horizon robotic manipulation, which requires addressing challenges in tracking, cost evaluation, and closed-loop control. The key hypotheses provide possible solutions through image registration, short-horizon replanning, and persistent retrying.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method for closed-loop control of robotic manipulation tasks using self-supervised video prediction models. Specifically:- They propose using learned image-to-image registration to define a planning cost function for model predictive control with raw image observations. This allows tracking and retrying during execution to accomplish long-horizon tasks.- They demonstrate a self-supervised method to train the registration model using the same dataset collected for training the video prediction model.- They show this approach can enable complex robotic manipulation skills like grasping and non-prehensile pushing from raw visual inputs, without manual engineering or object models.- They demonstrate real-world robotic manipulation on long-horizon tasks with objects not seen during training. The method acquires skills like repositioning objects after 150 hours of autonomous data collection.- They extend the approach to handle multiple camera views and 3D space.In summary, the key contribution is developing a method to close the control loop for visual MPC, enabling persistent retrying and complex manipulation skills to be acquired through autonomous interaction and video prediction models. The self-supervised registration technique is the main element that unlocks the ability to perform long-horizon tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a method for closed-loop robotic manipulation using self-supervised learning, where an imperfect video prediction model can complete complex long-horizon tasks by continuously retrying through the use of a learned image registration cost function that tracks progress towards goal images.
