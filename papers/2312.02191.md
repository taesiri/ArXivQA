# [Prompt Tuning for Zero-shot Compositional Learning](https://arxiv.org/abs/2312.02191)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Prompt Tuning for Zero-shot Compositional Learning":

Problem:
The paper tackles the problem of open world compositional zero-shot learning (OW-CZSL). The goal is to recognize novel compositions of seen attributes and objects given an image, without any prior assumptions about the output space. This is very challenging as the compositions are unseen during training. 

Proposed Solution: 
The paper proposes a framework called Multi-Modal Prompt Tuning (MMPT) to tackle this problem. MMPT is built on top of the CoOp vision-language model and has three key components:

1) Attribute branch and Object branch: These process the textual descriptions of attributes and objects using language transformers. 

2) Vision branch: This processes the input image using a visual transformer.

3) Shared prompts: Unique learnable prompt tokens are injected into the front layers of all three branches to better align the modalities.

4) Visual patch prompts: Additional prompts are injected into the vision branch for better generalization.

The branches are trained jointly using a composite loss function. During inference, the attribute and object predictions are combined to predict unseen compositions.

Main Contributions:

1) MMPT achieves new state-of-the-art results on the UT-Zappos and challenging MIT-States datasets for OW-CZSL, significantly outperforming prior works.

2) Detailed ablation studies demonstrate the benefits of using both visual and shared cross-modal prompts over using just the CoOp model.

3) The paper explores using prompt tuning for bridging vision and language modalities for the first time.

4) Analysis shows that OW-CZSL is a difficult task that requires models to be both "smart" in reasoning about seen compositions and "knowledgeable" about the open world. MMPT combines the compositional reasoning ability of specialized CZSL models with the broad knowledge of vision-language models.

In summary, the paper presents a novel prompt tuning approach to inject knowledge into vision-language models for tackling the highly challenging task of open world compositional zero-shot learning.
