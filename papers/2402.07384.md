# [Exploring Perceptual Limitation of Multimodal Large Language Models](https://arxiv.org/abs/2402.07384)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Recent multimodal large language models (MLLMs) like GPT-4V have shown remarkable capability in visual question answering. However, their ability to perceive small visual details is limited. 
- The paper provides anecdotal evidence of MLLMs struggling with small objects and text, but notes that the extent of this limitation and underlying factors have not been explored systematically.

Main Contributions:  
- Conducts quantitative analysis of 7 MLLMs on visual QA datasets GQA and TextVQA - groups data by relative target object size and observes performance drops with smaller sizes
- Identifies and studies 4 factors affecting MLLMs' perception of small objects:
    1. Object quality (sampling rate) - finds threshold-dependent trend, with performance stabilizing past rate of 8
    2. Object size - at fixed quality, smaller size hurts performance except for model trained on small object data
    3. Distractors - more distractors consistently reduce MLLMs' performance 
    4. Location 
        - Global: performance fluctuates significantly across different object locations
        - Local: dividing objects across patches helps, horizontal cuts hurt more
- Proposes new evaluation protocol and data to analyze MLLMs' perceptual limitations 

Impact:
- Caution against using MLLMs when tasks rely on discerning small visual details
- Provides insights to develop more robust MLLMs in handling lower quality data and smaller objects
- Lays groundwork to enhance and evaluate future MLLMs' perception across factors like distractors and location

In summary, the paper systematically studies limitations of current MLLMs in perceiving small visual details, considering the roles of multiple underlying factors. It offers a new experimental framework to advance and assess MLLMs on this key capability.
