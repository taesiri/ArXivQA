# [Exploiting Symmetric Temporally Sparse BPTT for Efficient RNN Training](https://arxiv.org/abs/2312.09391)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
Recurrent neural networks (RNNs) like LSTMs are useful for processing temporal sequence data, but training RNNs requires many compute operations and memory accesses. This makes deploying and incrementally training RNNs challenging on resource-constrained edge devices. 

Proposed Solution:
The paper proposes using Delta RNNs, which exploit temporal sparsity during inference by skipping neuron updates when their activations change little. The key insight is that the same sparsity exists during backpropagation, allowing 3x sparse matrix-vector multiplies. This greatly reduces overall training operations.

Key Contributions:

1) First mathematical formulation showing Delta RNN training is a type of sparse backpropagation through time (BPTT), reusing identical forward pass sparsity. Hardware speedups seen during Delta RNN inference directly apply to training.

2) Empirical evaluation on speech datasets like Fluent Commands, showing 7.3x fewer training operations for Delta LSTMs vs standard LSTMs for a small 1.16x increase in error rate. Up to 80% sparsity observed during incremental keyword training.  

3) RTL simulation of a custom Delta RNN training accelerator, demonstrating 2-10x speedups for 50-90% sparsity levels. This matches theoretical maximum speedups.

4) Overall, the paper enables efficient on-device incremental training of RNNs by exploiting temporal sparsity in both forward and backward passes. This reduces overall operations without harming model accuracy.

In summary, the key innovation is formulating Delta RNN training as a sparse BPTT process reusing forward pass sparsity. Empirical and hardware results validate this allows RNN training with much lower resource requirements.
