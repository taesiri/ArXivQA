# Self-Consistency Improves Chain of Thought Reasoning in Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is: Can introducing self-consistency over diverse reasoning paths improve the performance of chain-of-thought prompting for complex reasoning tasks? The key ideas are:- Chain-of-thought prompting has shown promise for improving language models' reasoning abilities, but still uses greedy decoding to generate a single chain of reasoning.- The authors propose that complex reasoning tasks often admit multiple valid reasoning paths leading to the correct solution. - They introduce a new decoding strategy called "self-consistency" that samples diverse reasoning paths from the language model, then aggregates answers by choosing the most frequent final answer.- The intuition is that if multiple reasoning paths give the same answer, there is greater confidence the answer is correct.- This approach is evaluated extensively across arithmetic, commonsense, and symbolic reasoning tasks.- Results show large gains over greedy chain-of-thought prompting across tasks and model scales, achieving new state-of-the-art on many benchmarks.In summary, the central hypothesis is that sampling diverse reasoning paths and aggregating the most consistent answer can substantially improve performance of chain-of-thought prompting for complex reasoning. The results appear to validate this hypothesis across a range of models and reasoning tasks.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a new decoding strategy called "self-consistency" to improve the performance of chain-of-thought prompting with large language models on reasoning tasks. Specifically, the key ideas are:- Chain-of-thought prompting has shown promise for improving reasoning capabilities of language models by prompting them to generate an explicit reasoning process before outputting the final answer. - The paper proposes self-consistency as an improved decoding strategy over the greedy decoding used in prior chain-of-thought prompting work. - Self-consistency involves sampling multiple diverse reasoning paths from the language model's decoder, then aggregating the answers by choosing the most frequent final answer. This exploits the intuition that correct reasoning processes tend to agree on the final answer.- Empirically, self-consistency is shown to substantially boost accuracy over greedy decoding for chain-of-thought prompting on a range of arithmetic and commonsense reasoning benchmarks, across multiple language models.- The method is simple, unsupervised, and does not require any additional training or annotation.In summary, the key contribution is proposing and demonstrating the effectiveness of the self-consistency decoding strategy to improve reasoning performance of language models prompted with chains of thought. The simplicity and unsupervised nature of the approach are also notable.
