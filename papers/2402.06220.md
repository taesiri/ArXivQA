# [A Unified Causal View of Instruction Tuning](https://arxiv.org/abs/2402.06220)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "A Unified Causal View of Instruction Tuning":

Problem:
- Instruction tuning on large language models has shown promise for improving generalization on new tasks, but current methods exploit superficial statistical correlations rather than learning the underlying causal relationships. This can lead to fragile models that fail when adapting to new distributions.

Proposed Solution:
- Develop a meta Structural Causal Model (meta-SCM) that provides a unified causal graph to integrate different NLP tasks. The model includes multiple latent factors to represent linguistic properties, only some of which causally influence target labels.  
- Propose the Uniform Identifiability Condition (UIC), which provides theoretic guarantees that the latent factors can be identified from observed data without mixing of information.
- Develop Structural Instruction Tuning (SIT), a novel tuning method guided by the meta-SCM and UIC. Key ideas:
  - Learn causal factor selection mechanisms and task-specific causal representations 
  - Learn causal generative mechanisms to produce target labels from selected causal factors
  - Architecture includes four components: SCM Latent Module, Task-guided Latent Encoder, Source Reconstruction Decoder, Target Prediction Decoder

Main Contributions:
- Theoretically, provide the UIC which ensures identifiability of latent factors for a range of SCM topologies, enabling incorporation of causality
- Methodically, propose the meta-SCM and SIT method to unify NLP tasks under a causal view and perform instruction tuning on causal representations
- Experimentally, demonstrate SIT's effectiveness on in-domain and out-of-domain datasets across a range of NLP tasks, outperforming baselines in terms of metrics like accuracy and ROUGE-L
