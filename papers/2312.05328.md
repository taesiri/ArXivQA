# [Bad Students Make Great Teachers: Active Learning Accelerates   Large-Scale Visual Understanding](https://arxiv.org/abs/2312.05328)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training large vision models requires massive compute and data, following a power law relationship where each gain in accuracy requires an order of magnitude more compute. Reducing this training cost is an important challenge.

- Prior work on model-based data filtering and active learning can accelerate training but incur heavy overheads for scoring/filtering data, making overall training less efficient than standard IID training. 

Method - ClassAct & ActiveCLIP:
- Propose a model-based data selection framework where small "actor" models score the train data and prioritize examples for training a larger "learner" model. This allows scaling down the cost of scoring while still accelerating the learner.

- Actor models assign a learnability score to each example based on its training loss on the current online model versus a reference model. This focuses training on examples that are learnable by the current model.

- The online actor model tracks the learner while the reference actor stabilizes scores. Both actors are much smaller than the learner (e.g. 1,000x fewer parameters) to minimize scoring costs.

Contributions:
- First demonstration of a "compute-positive" active learning method, requiring less total compute (scoring + training) than standard IID training.

- Shows robust generalization of selection policies, with actor models up to 1,000x smaller still providing 26% acceleration of the learner model.

- Applies method successfully to large-scale image classification (JFT) and multimodal pretraining (ALIGN dataset), demonstrating universality.

- Establishes that scaling laws from prior work on IID training carry over to the active learning setting.

- Shows learned selection policies transfer across datasets and tasks, enabling amortization of scoring costs.

In summary, the paper presents an efficient and scalable active learning framework to accelerate training of large vision models, with empirically demonstrated benefits across model sizes, tasks, and datasets. The core innovation is efficiently decoupling scoring models from learner models.
