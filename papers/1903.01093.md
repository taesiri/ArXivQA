# [Differentiable Causal Computations via Delayed Trace](https://arxiv.org/abs/1903.01093)

## What is the central research question or hypothesis that this paper addresses?

 This paper seems to address the question of how to formally model and differentiate causal/stateful computations, particularly those arising in machine learning contexts like recurrent neural networks. The key ideas appear to be:

- Introducing a "causal extension" construction Caus(-) that takes a cartesian category C and returns a new category Caus(C) whose morphisms represent causal/stateful computations extended over discrete time steps. This allows modeling things like recurrent neural networks.

- Defining a "delayed trace" operator on Caus(C) that allows feedback loops while enforcing a certain "guardedness" condition. This models feedback loops with delay like those in signal flow graphs.

- Showing that if C is equipped with a cartesian differential operator D, then Caus(C) can also be equipped with a differential operator SeqD derived from D. This allows differentiating causal/stateful computations, providing an abstract justification for techniques like backpropagation through time.

So in summary, the central research contribution seems to be introducing the Caus(-) construction and delayed trace operator to model causal computations, and then equipping this with a differential structure to enable differentiating such causal computations in a principled categorical fashion. This provides new formal foundations and tools for analyzing and differentiating stateful models in machine learning.


## What is the main contribution of this paper?

 Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:

1. Developing a categorical construction called the "causal extension" (denoted caus(C)) that extends any cartesian category C to represent stateful/causal functions on sequences. This caus construction allows modeling of various stateful systems like Mealy machines and recurrent neural networks. A key feature is a "delayed trace" operator that forms feedback loops with an implicit guardedness property.

2. Showing that if C is equipped with a cartesian differential operator D, then caus(C) can also be given a differential operator defined using an abstract version of backpropagation through time. This allows differentiation and gradient computations for stateful functions, obtaining properties like a chain rule and Schwartz theorem that justify backpropagation through time as true differentiation rather than just an algorithmic process. The differential operator matches unroll-then-differentiate but without requiring explicit unrolling.

3. More broadly, the paper develops categorical tools and semantics to bring notions of causality, stateful computation, and differentiability together in a formal framework. This provides a foundation for studying machine learning techniques like recurrent neural networks from a mathematical perspective.

In summary, the main contribution appears to be developing categorical semantics to capture stateful and causal computations in a differentiable setting, with applications to mathematically justifying and extending techniques used in machine learning such as backpropagation through time. The abstraction provides both theoretical understanding and potential for further applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper develops a categorical framework for modeling and differentiating causal computations on sequences, with applications to recurrent neural networks and backpropagation through time.

In slightly more detail:

The paper introduces a "causal extension" construction that takes a Cartesian category and produces a new category whose morphisms represent causal functions on sequences. This construction captures general discrete-time feedback systems like recurrent neural networks. The key tool is a "delayed trace" operator that inserts an implicit delay into feedback paths. 

The paper then shows this causal extension construction preserves Cartesian differential structure, allowing differentiation of causal computations. This yields an abstract understanding of backpropagation through time and properties like a chain rule. The differentiation can be done without unrolling recurrent networks, matching gradients computed by unrolling.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related research:

- The paper presents a novel categorical construction called "delayed trace" that adds feedback loops with delay to causal/stateful computations. This is related to prior work on trace operators and feedback in categories, but the delayed trace is a new variant tailored for modeling computations through discrete time steps. 

- The paper connects this categorical construction to machine learning by showing how to differentiate through the feedback loops, matching the backpropagation through time technique from RNNs. This categorical perspective on differentiating stateful functions is novel. Prior ML work has focused more on algorithms and implementations rather than abstract theory.

- The paper links the categorical semantics to graphical languages like signal flow graphs. Other research has studied signal flow graphs coalgebraically or using symmetric monoidal categories. This work offers a new Cartesian-based semantics using double categories.

- For differentiation, the paper leverages the theory of Cartesian differential categories. This fits into a line of research applying Cartesian differential categories to understand differentiation and backpropagation abstractly. The novelty is extending differentiation to handle stateful computations.

- Overall, the main innovations seem to be the delayed trace operator itself, the application of these categorical semantics to stateful neural networks/ML systems, and the results connecting differentiation to backpropagation through time. The paper relates all this to prior work, but approaches the problems from a novel categorical perspective.

In summary, the paper offers new categorical constructions and semantics to understand stateful computation and differentiation, connecting abstraction with applications in ML in ways that differ from prior approaches. The delayed trace and differentiation results appear to be the main novel contributions relating to other work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Formalizing the diagrammatic equational system for reasoning about morphisms in $\causz\CC$. The authors mention this is an interesting direction for future work.

- Investigating other possible delayed trace operators beyond the one described for $\caus\CC$. The authors say there seem to be quite a few other interesting delayed traces to explore.

- Adding extra structure to Cartesian differential categories to allow for more explicit representation of Jacobians/gradients. The authors discuss that their abstract notion of differentiation doesn't directly provide gradients, which could be useful for machine learning applications.

- Enhancing the theory with differential restriction categories to handle non-smooth and discontinuous functions. The authors note this would be useful for extending the theory to handle functions commonly used in machine learning. 

- Further iterating the $\causz-$ construction, such as looking at $\causz{\causz\CC}$, to potentially model things like meta-learning and hyperparameter tuning. The authors suggest this as an interesting direction related to potential machine learning applications.

- Formally comparing delayed trace with other related work such as guarded trace operators. The authors mention investigating the precise relationship between guarded trace and delayed trace could be interesting future work.

- Connecting the categorical constructions back to coalgebraic models like stream systems/signal flow graphs. The authors suggest this as an interesting topic to explore further.

In summary, the main suggested future directions seem to be: further developing the theory of delayed traces, enhancing the differential aspects, and exploring applications of the overall framework to machine learning topics like meta-learning.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper investigates causal computations that take sequences of inputs to sequences of outputs, where the nth output depends only on the first n inputs. The authors model these computations in category theory using a construction called "delayed trace", which forms feedback loops with implicit guardedness. They show this construction extends any Cartesian category to another category of causal computations. Furthermore, if the original Cartesian category has a differential operator, the causal extension also inherits a differential operator defined via an abstract form of backpropagation through time. This allows differentiation of causal computations like recurrent neural networks. The differential operator satisfies properties like a chain rule and Schwartz theorem, and can compute derivatives without unrolling recurrent networks. Overall, the paper provides a categorical understanding of differentiation for causal/stateful computations.


## Summarize the paper in two paragraphs.

 The paper introduces a categorical construction called the delayed trace, which provides a feedback mechanism for building causal computations in a Cartesian category $\mathcal{C}$. The key ideas are:

- The construction takes $\mathcal{C}$ to a new category $\text{Caus}(\mathcal{C})$ whose morphisms represent stateful computations that take sequences of inputs to sequences of outputs in a causal manner (i.e. each output depends only on past inputs). 

- This is achieved by composing basic computations from $\mathcal{C}$ vertically across time steps, passing state between them horizontally. The delayed trace operator forms feedback loops with an implicit delay, avoiding issues with guardedness.

- When $\mathcal{C}$ is equipped with a Cartesian differential operator, the construction induces one on $\text{Caus}(\mathcal{C})$. This differential structure supports an abstract form of backpropagation that works directly on stateful networks, without needing to unroll them.

- As a result, key properties of differentiation like chain rules and Schwartz theorems carry over to stateful computations. The induced differential operator matches the derivatives computed by backpropagation through time, providing a formal grounding for this technique.

Overall, the paper develops categorical machinery to cleanly talk about causal and differentiable stateful computations. This provides a foundation to further investigate things like recurrent neural networks through the lens of category theory.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces a categorical framework for modeling causal (stateful) computations and differentiating them. The key ideas are:

- They define a construction Caus(-) that takes a cartesian category C and returns a new category Caus(C) whose morphisms represent causal functions on sequences. Caus(C) has a "delayed trace" operator that allows feedback loops while maintaining guardedness. 

- When C is equipped with a cartesian differential operator D, they show how to construct a differential operator D' for Caus(C) using an abstract form of backpropagation through time (BPTT). D' allows differentiating causal functions directly, without needing to unroll them first like in traditional BPTT.

- They show that if C is a cartesian differential category, then so is Caus(C). This allows importing many useful properties of derivatives, like the chain rule. The differential in Caus(C) matches the result of standard BPTT.

In summary, the key method is using categorical constructions to model causal computations and their derivatives. Leveraging cartesian differential categories allows deriving a well-behaved derivative for stateful functions. Overall, this provides a principled framework for differentiation and backpropagation in systems with feedback.
