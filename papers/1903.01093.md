# [Differentiable Causal Computations via Delayed Trace](https://arxiv.org/abs/1903.01093)

## What is the central research question or hypothesis that this paper addresses?

This paper seems to address the question of how to formally model and differentiate causal/stateful computations, particularly those arising in machine learning contexts like recurrent neural networks. The key ideas appear to be:- Introducing a "causal extension" construction Caus(-) that takes a cartesian category C and returns a new category Caus(C) whose morphisms represent causal/stateful computations extended over discrete time steps. This allows modeling things like recurrent neural networks.- Defining a "delayed trace" operator on Caus(C) that allows feedback loops while enforcing a certain "guardedness" condition. This models feedback loops with delay like those in signal flow graphs.- Showing that if C is equipped with a cartesian differential operator D, then Caus(C) can also be equipped with a differential operator SeqD derived from D. This allows differentiating causal/stateful computations, providing an abstract justification for techniques like backpropagation through time.So in summary, the central research contribution seems to be introducing the Caus(-) construction and delayed trace operator to model causal computations, and then equipping this with a differential structure to enable differentiating such causal computations in a principled categorical fashion. This provides new formal foundations and tools for analyzing and differentiating stateful models in machine learning.


## What is the main contribution of this paper?

Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:1. Developing a categorical construction called the "causal extension" (denoted caus(C)) that extends any cartesian category C to represent stateful/causal functions on sequences. This caus construction allows modeling of various stateful systems like Mealy machines and recurrent neural networks. A key feature is a "delayed trace" operator that forms feedback loops with an implicit guardedness property.2. Showing that if C is equipped with a cartesian differential operator D, then caus(C) can also be given a differential operator defined using an abstract version of backpropagation through time. This allows differentiation and gradient computations for stateful functions, obtaining properties like a chain rule and Schwartz theorem that justify backpropagation through time as true differentiation rather than just an algorithmic process. The differential operator matches unroll-then-differentiate but without requiring explicit unrolling.3. More broadly, the paper develops categorical tools and semantics to bring notions of causality, stateful computation, and differentiability together in a formal framework. This provides a foundation for studying machine learning techniques like recurrent neural networks from a mathematical perspective.In summary, the main contribution appears to be developing categorical semantics to capture stateful and causal computations in a differentiable setting, with applications to mathematically justifying and extending techniques used in machine learning such as backpropagation through time. The abstraction provides both theoretical understanding and potential for further applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper develops a categorical framework for modeling and differentiating causal computations on sequences, with applications to recurrent neural networks and backpropagation through time.In slightly more detail:The paper introduces a "causal extension" construction that takes a Cartesian category and produces a new category whose morphisms represent causal functions on sequences. This construction captures general discrete-time feedback systems like recurrent neural networks. The key tool is a "delayed trace" operator that inserts an implicit delay into feedback paths. The paper then shows this causal extension construction preserves Cartesian differential structure, allowing differentiation of causal computations. This yields an abstract understanding of backpropagation through time and properties like a chain rule. The differentiation can be done without unrolling recurrent networks, matching gradients computed by unrolling.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper presents a novel categorical construction called "delayed trace" that adds feedback loops with delay to causal/stateful computations. This is related to prior work on trace operators and feedback in categories, but the delayed trace is a new variant tailored for modeling computations through discrete time steps. - The paper connects this categorical construction to machine learning by showing how to differentiate through the feedback loops, matching the backpropagation through time technique from RNNs. This categorical perspective on differentiating stateful functions is novel. Prior ML work has focused more on algorithms and implementations rather than abstract theory.- The paper links the categorical semantics to graphical languages like signal flow graphs. Other research has studied signal flow graphs coalgebraically or using symmetric monoidal categories. This work offers a new Cartesian-based semantics using double categories.- For differentiation, the paper leverages the theory of Cartesian differential categories. This fits into a line of research applying Cartesian differential categories to understand differentiation and backpropagation abstractly. The novelty is extending differentiation to handle stateful computations.- Overall, the main innovations seem to be the delayed trace operator itself, the application of these categorical semantics to stateful neural networks/ML systems, and the results connecting differentiation to backpropagation through time. The paper relates all this to prior work, but approaches the problems from a novel categorical perspective.In summary, the paper offers new categorical constructions and semantics to understand stateful computation and differentiation, connecting abstraction with applications in ML in ways that differ from prior approaches. The delayed trace and differentiation results appear to be the main novel contributions relating to other work.
