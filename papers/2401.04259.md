# [MARG: Multi-Agent Review Generation for Scientific Papers](https://arxiv.org/abs/2401.04259)

## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is the proposal of a novel method called MARG-S (Multi-Agent Review Generation with Specialized Agents) for automatically generating high-quality peer-review feedback for scientific papers. The key ideas behind MARG-S are:

1) Using multiple "agents" (instances of an LLM like GPT-4) and having them communicate to collectively reason about long papers that exceed the input size limits of a single model. 

2) Specializing different agents into "experts" focused on generating certain types of comments (experiments, clarity, impact), which is more effective than having a single agent try to handle all comment types.

3) Distributing different sections of the paper across agents so that the full content is available during reasoning.

4) Refining generated comments in a separate stage to resolve errors and improve quality.

The authors evaluate MARG-S against baseline methods and find that it substantially increases the number of "good" comments rated by users, reduces the rate of generic comments, and produces more specific suggestions. So in summary, the main contribution is the proposal and evaluation of the MARG-S approach for improving review generation through multi-agent modeling and specialization.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the key advantage of using flow imbalance as the objective for clustering compared to more common objectives based on edge density? How is flow imbalance able to reveal clusters that density-based methods would miss?

2. The directed mixed path aggregation (DIMPA) scheme seems crucial for capturing long-range dependencies in the network. Can you explain in more detail how the self-loops and weighting schemes allow it to effectively propagate information across longer paths?

3. The paper introduces a probabilistic treatment of the imbalance score. What is the motivation behind this probabilistic formulation compared to a deterministic version? How does it change the optimization objective?

4. What assumptions does the imbalance score make about the flow distributions in the network, if any? How might violations of those assumptions impact the performance of clustering?  

5. One of the claims is that DIGRAC works well even without node attributes. However, the method does construct an input feature matrix from the eigenvectors when attributes are absent. What information do these eigenvector features capture and how does that aid clustering?

6. How does the paper determine the number of eigenvectors, K, to use for the feature mapping when no attributes are available? Is there a systematic way to choose K or is it mostly heuristically determined?

7. What limits the scalability of DIGRAC to extremely large networks? The paper mentions sampling - how might that help mitigate complexity and what challenges does it introduce?

8. The DIMPA scheme seems similar in spirit to techniques used in GCNs and GATs. What modifications were made to handle directed edges rather than undirected ones? How does ignoring edge direction fail?

9. The experimental results indicate good performance even with sparse networks and ambient nodes. Are there any assumptions baked into the method that might cause problems if violated in a sparser regime? 

10. The pairwise imbalance scores used seem somewhat arbitrarily chosen for the loss function. Is there a systematic way to determine the set of optimal pairs to consider that isn't simply combinatorial search?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: Large language models like GPT-4 have tremendous capabilities but face limitations in the amount of context they can process at once, making it challenging for them to generate high-quality reviews for long scientific papers.

Proposed Solution: The authors propose a multi-agent approach called \textbf{MARG} (\textbf{M}ulti-\textbf{A}gent \textbf{R}eview \textbf{G}eneration) to distribute text across agents that collectively retain and reason over the full paper. Specialized "expert" agents focus on generating certain types of comments (experiments, clarity, impact). 

Key Contributions:
- MARG enables review generation for papers longer than the context limit of the base LLM
- Specializing agents and sub-tasks tailored to different comment types substantially improves review quality 
- In a user study, a GPT-4 baseline produces mostly generic comments (60%), while MARG generates mostly specific comments (71%)
- MARG generates 2.2x as many helpful comments per paper as the best GPT-4 baseline

The core ideas are:
(1) Distribute long papers across agents 
(2) Add specialized sub-tasks/experts for key comment types  
(3) Substantial gains in specificity and quantity of useful feedback

In summary, the proposed MARG approach demonstrates how to effectively apply LLMs to generate helpful, nuanced feedback even for long technical papers through careful decomposition into specialized sub-tasks and a communication framework to retain context. The gains highlight the importance of problem formulation in unlocking the capabilities of LLMs.
