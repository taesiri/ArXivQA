# [Holmes: Towards Distributed Training Across Clusters with Heterogeneous   NIC Environment](https://arxiv.org/abs/2312.03549)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces Holmes, a framework optimized for training large language models (LLMs) across multiple GPU clusters with heterogeneous network interface cards (NICs). Holmes employs intelligent scheduling to allocate computational tasks to GPUs based on their NIC types, maximizing the capabilities of each network technology. Key innovations include cross-cluster pipeline parallelism to enable scaling across clusters without high-speed interconnects, self-adapting pipeline partitioning tailored to heterogeneous speeds, automatic NIC selection for data parallelism, and an overlapped distributed optimizer. Comprehensive experiments demonstrate Holmes achieving performance in heterogeneous NIC environments close to that of homogeneous RDMA networks, and exceeding pure Ethernet environments. Comparisons to Megatron-LM, Megatron-DeepSpeed, and Megatron-LLaMA validate considerable improvements in training efficiency and scalability. The proposed scheduling and optimized parallelism in Holmes unlocks efficient heterogeneous system utilization for accessible and cost-effective LLM training.
