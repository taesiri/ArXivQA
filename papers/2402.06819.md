# [Monitored Markov Decision Processes](https://arxiv.org/abs/2402.06819)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Monitored Markov Decision Processes":

Problem:
- In reinforcement learning (RL), agents typically assume they can always observe the rewards generated by the environment in response to their actions. However, in many real-world settings, rewards may not always be observable due to lack of instrumentation, unavailability of human experts, faulty sensors, etc.
- Existing RL frameworks like POMDPs, sparse rewards, cautious RL, etc. do not fully capture such complexities where rewards are generated but not always observable. 

Proposed Solution:
- The paper proposes a new RL framework called "Monitored Markov Decision Processes (Mon-MDPs)" to model such situations. 
- In Mon-MDPs, the observability of rewards from the environment is controlled by a separate stochastic process called the "monitor". The monitor has its own states, actions and dynamics.  
- At each timestep, the agent selects joint actions for both the environment and the monitor. The environment generates a reward but the agent observes a "proxy" reward based on the monitor state and action. The proxy can be the actual reward or a special "unobservable" value.
- The agent must learn to maximize long-term rewards from the environment, while only observing proxy rewards from the monitor.

Contributions:
- Formalizes the problem of unobservable rewards in RL through Mon-MDPs.
- Discusses theoretical consequences like non-existence of optimal policies.
- Empirically demonstrates challenges through gridworld experiments, e.g. ignoring unobservable rewards can lead to dangerous behaviors.
- Proposes model-based RL algorithm with reward prediction that can converge to optimal policies under certain conditions.
- Positions Mon-MDPs as a powerful new formalism unifying problems like active RL, partial monitoring, etc. while enabling modeling of more complex real-world situations.
- Lays theoretical and empirical foundation for future work in learning under limited reward observability.
