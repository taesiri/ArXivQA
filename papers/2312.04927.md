# [Zoology: Measuring and Improving Recall in Efficient Language Models](https://arxiv.org/abs/2312.04927)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summarizing paragraph of the key findings from the paper:

This paper analyzes attention-free language models that combine gating and convolutions. Despite recent advances claiming competitive performance, experiments reveal these models still underperform attention models in language modeling perplexity on the Pile dataset. Through analysis, the paper finds that 82% of the gap is attributed to inferior ability in associative recall - predicting tokens based on associations previously seen in context. For example, predicting "worries" after seeing the bigram "no worries" earlier in the text. The paper terms this phenomenon multi-query associative recall (MQAR) and shows theoretically and empirically that the model dimensionality required for gated convolutions to solve MQAR scales poorly compared to attention. Based on this analysis, the paper evaluates convolution-attention hybrids using sparse, input-dependent attention patterns only on exact repeating bigrams. Results show such hybrids can recover 97.4% of the gap while maintaining efficiency advantages over pure attention models. Overall, this careful analysis of state-of-the-art efficient language models elucidates fundamental limitations and opportunities for improvement via input-dependent sequence mixing.
