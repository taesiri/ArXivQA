# [EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for   Evaluating Vision Language Models](https://arxiv.org/abs/2403.10378)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating vision-language models (VLMs) have limitations - they are mostly English-centric, not from school exams, and keep text and images separate. This fails to properly challenge models on more complex reasoning tasks.

- There is a need for a new challenging multimodal multilingual exam benchmark to better assess sophisticated VLMs.

Solution:
- The authors introduce EXAMS-V, a new benchmark consisting of 20,932 multiple choice questions across 11 languages and 20 subjects, compiled from school exams in different countries.

- The questions feature integrated text, images, tables, figures, diagrams, maps, scientific symbols, and equations, requiring joint reasoning over both visual and textual content.

- The benchmark has broader linguistic scope than prior datasets, with increased representation of low-resource languages like Croatian and Hungarian. It also features higher complexity questions from grades 4-12.

Contributions:
- A novel dimension of unified visual-textual reasoning for benchmarking VLMs, through integrated question images containing both modalities.

- A new large-scale, multidisciplinary, multimodal, multilingual exam dataset that is more realistic and challenging for assessing state-of-the-art VLMs.

- Analysis of multiple VLMs shows there is significant room for improvement, indicating the complexity of EXAMS-V.

In summary, EXAMS-V advances VLM evaluation through a more diverse, intricately reasoning visual-textual benchmark mirroring real-world complexity. This can catalyze progress towards advanced multilingual VLMs with expert-level perception and understanding.
