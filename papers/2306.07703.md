# [E2E-LOAD: End-to-End Long-form Online Action Detection](https://arxiv.org/abs/2306.07703)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we design an end-to-end framework based on Transformers to achieve both high efficiency and effectiveness for online action detection?

The key points are:

- Most prior work relies on fixed backbone networks for feature extraction, which constrains further improvements. The authors propose an end-to-end learning framework to overcome this limitation.

- They introduce novel components like a stream buffer, asymmetric spatiotemporal modeling branches, and efficient inference to balance performance and speed. 

- The overall framework aims to effectively model long-term dependencies while maintaining high computational efficiency for online processing.

In summary, the paper focuses on developing an end-to-end Transformer approach tailored for online action detection that can achieve superior accuracy and speed compared to prior feature-based methods. The core hypothesis seems to be that end-to-end training and their proposed techniques can unlock greater efficiency and effectiveness.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes E2E-LOAD, an end-to-end learning framework for online action detection (OAD) that is based on Transformers. 

2. It introduces several novel components in the architecture design:

- A Stream Buffer module that reuse computed frame features to improve efficiency. 

- An asymmetric design with separate Long-term Compression and Short-term Modeling branches to handle different lengths of historical context.

- A Long-Short-term Fusion module to integrate long-term and short-term representations.

- An Efficient Inference technique to accelerate spatiotemporal attention through token reuse.

3. Extensive experiments show E2E-LOAD achieves superior performance and efficiency compared to prior methods on three benchmark datasets, using only RGB frames as input. It obtains 17.3 FPS speed while achieving 1.2% higher mAP on THUMOS14.

4. Ablation studies validate the contribution of each proposed component. The end-to-end training framework significantly reduces memory consumption compared to prior methods.

In summary, the key contribution is an end-to-end Transformer framework for online action detection that achieves new state-of-the-art results in both accuracy and efficiency through several novel designs. The experiments comprehensively demonstrate its effectiveness.
