# [GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can generate impressive text but also easily produce toxic, offensive content. This is a major issue limiting their usage.
- Existing controllable text generation (CTG) methods reduce toxicity but hurt other aspects of LLM performance like topic accuracy, grammar quality, fluency and slow down generation.

Proposed Solution: 
- The paper proposes a new method called "Gated Toxicity Avoidance (GTA)" that selectively applies CTG only when toxicity is detected to preserve LLM performance.  
- GTA uses a toxicity classifier as a "gate". If toxicity probability exceeds a threshold, it applies the CTG method to alter the next token. Otherwise, it allows the original LLM distribution to continue unchecked.

Main Contributions:
- First study analyzing holistic performance degradation (topic, grammar, fluency, speed) of applying CTG methods to LLMs. Reveals tradeoffs.
- Proposes simple yet effective Gated Toxicity Avoidance method that preserves LLM performance while achieving comparable toxicity reduction to state-of-the-art CTG methods. 
- Model-agnostic approach compatible with any CTG method and LLM.
- Empirically demonstrates effectiveness of proposed GTA method across multiple models, datasets and metrics compared to applying CTG alone.
- GTA also speeds up guided-decoding style CTG methods by selective application.

In summary, the paper identifies an important problem in applying toxicity control methods to LLMs and contributes a practical gating based solution that reduces toxicity while preserving language generation quality.
