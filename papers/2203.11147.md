# [Teaching language models to support answers with verified quotes](https://arxiv.org/abs/2203.11147)

## What is the central research question or hypothesis that this paper addresses?

Based on a close reading of the introduction, it seems this paper focuses on developing language models that can generate answers to factual questions alongside supporting evidence in the form of verbatim quotes. The key objectives appear to be:1) Teaching language models to support their answers by citing relevant quotes from source documents, in order to increase trustworthiness. 2) Using reinforcement learning from human preferences to align the models to produce high quality evidence that verifies and substantiates their claims.3) Enabling the models to abstain from answering questions when unsure, rather than generating potentially incorrect responses. 4) Evaluating these "open book QA" models on their ability to provide plausible, supported answers on subsets of the Natural Questions and ELI5 datasets.5) Analyzing the strengths and limitations of supporting claims with evidence quotes, including failures on more adversarial datasets, to motivate further research into ensuring truthful responses.In summary, the central hypothesis seems to be that training language models to provide supporting evidence can increase the plausibility and trustworthiness of their factual claims, although this alone does not guarantee truthfulness. The paper aims to demonstrate successes on this self-supported QA task while also delineating the need for further techniques to achieve fully truthful and reliable language agents.


## What is the main contribution of this paper?

The main contribution of this paper is developing a language model called GopherCite that can generate answers to factual questions along with supporting evidence from documents. The key ideas are:- Using an "inline evidence" approach where the model generates the answer and supporting quote together in a single string. This allows scoring and conditional generation.- Retrieving supporting documents with a search engine rather than relying solely on the model's parameters. - Training with a combination of supervised learning and reinforcement learning from human preferences to align the model to produce plausible, supported answers.- Enabling the model to abstain from answering questions it is uncertain about, to improve reliability.The authors demonstrate that GopherCite can produce answers that human evaluators rate as supported by evidence 80% of the time on a subset of Natural Questions, approaching human performance. The inline evidence approach provides a way to make large generative models more transparent. Limitations are that evidence quoting alone doesn't ensure truthfulness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points from the paper:The paper describes an open-book question answering system called GopherCite that is trained to provide factual answers supported by verbatim quotes from retrieved documents, which enables users to verify the system's responses.


## How does this paper compare to other research in the same field?

This paper on teaching language models to support answers with verified quotes makes several notable contributions compared to prior work:- It presents a new task formulation called "Self-Supported Question Answering" (SQA) which requires models to provide both an answer to a question and a direct supporting quote from a document. This is more demanding than typical QA settings which only require an answer.- The paper introduces a straightforward but effective "Inline Evidence" approach to SQA which formats the answer and evidence quote into a single response for conditional language modeling. This contrasts with more complex pipeline or dual encoder architectures.  - The method trains 280B parameter models that can provide high quality evidence from web documents. This scales up evidence provision substantially compared to prior works with smaller models like 16B WebGPT.- The paper demonstrates that reinforcement learning from human preferences leads to better alignment than pure supervised learning, mirroring results in concurrent work.- It studies model abstention, showing the system can decline to answer when uncertain. Prior work focused more on compulsory answering.- The paper thoroughly investigates limitations around truthfulness, highlighting that evidence quoting alone is insufficient to ensure truthful responses. This level of critical analysis is rare.Overall, this paper makes solid incremental progress in scaling up and improving the quality and reliability of evidence provision from large language models. The limitations analysis provides a thoughtful guide to open problems for future work on truthfulness.


## What future research directions do the authors suggest?

Based on my reading of the introduction, some future research directions the authors suggest include:- Enriching the setting with debate or recursive reward modeling to address limitations like misleading quotations or cherry-picking evidence. They suggest debate could help detect when evidence is cherry-picked.- Investigating protocols that go beyond quoting evidence to support statistical or negative claims, which are difficult to support with brief quotations. - Addressing the reliability and potential biases of sources. The authors note that even sources like Wikipedia can contain errors or biases. They suggest techniques like amplification could help users make good judgments about sources.- Providing more nuanced, qualified answers rather than binary true/false claims. The authors note this could help with contentious claims.- Combining citation of evidence with other techniques like common sense reasoning to provide stronger justification and avoid limitations of the evidence quoting approach alone.- Exploring more subtle alignment approaches like debate that could provide better training signals compared to quoting evidence. This could help ensure quotes are not misleading or cherry-picked.In summary, the main future directions relate to handling unreliable sources, providing more nuanced justification beyond quotation, and combining evidence quoting with other methods like debate and common sense reasoning for better training and evaluation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper describes a system called GopherCite that is trained to generate factual answers to questions along with supporting evidence from documents. GopherCite is built on top of the 280B parameter Gopher language model and uses a combination of supervised learning and reinforcement learning from human preferences to teach the model to produce high quality, supported answers. The authors introduce an "inline evidence" approach where the model outputs an answer and a verbatim quote from a document in a single string. This allows easy verification of the evidence by users. GopherCite retrieves multiple documents related to a question using Google Search and conditions the model on long contexts from these documents. The authors demonstrate that GopherCite can produce plausible, supported answers 80% of the time on a subset of NaturalQuestions and 67% on a subset of ELI5 when attempting all questions. By declining to answer the 30% of questions it is least confident on, quality can be further improved, exceeding human performance on the subsets. The authors discuss limitations around cherry-picking evidence and truthfulness, and situate inline evidence as one component of developing more trustworthy AI systems.
