# [Zero-Shot Composed Image Retrieval with Textual Inversion](https://arxiv.org/abs/2303.15247)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we perform composed image retrieval (CIR) without requiring expensive labeled training data?

The key points are:

- Composed image retrieval (CIR) involves retrieving a target image by providing a reference image and a text caption describing the differences between them. 

- Current CIR methods rely on supervised learning, which requires expensive manually annotated datasets.

- This paper proposes a new task called "zero-shot CIR" that aims to remove the need for labeled training data.

- The paper introduces a novel approach called "SEARLE" that can perform zero-shot CIR by mapping the reference image into a learned pseudo-word token and combining it with the relative caption.

- SEARLE involves training a textual inversion network on unlabeled images to map images to pseudo-word tokens.

- The paper also introduces a new benchmark dataset called CIRCO for evaluating zero-shot CIR.

In summary, the central hypothesis is that zero-shot CIR can be achieved without labeled training data by using textual inversion to map reference images to pseudo-words that can be combined with relative captions. The SEARLE method is proposed to test this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new task called Zero-Shot Composed Image Retrieval (ZS-CIR) and an approach to tackle it called SEARLE. 

Specifically, the key contributions are:

- Introducing the ZS-CIR task, which aims to perform composed image retrieval without requiring expensive labeled training data. This removes the need for high effort in creating datasets with query triplets.

- Proposing SEARLE, a novel approach involving a textual inversion network to map images to pseudo-words. It has two main stages: an optimization-based textual inversion to generate pseudo-words, and training the textual inversion network via distillation.

- Introducing CIRCO, a new benchmark dataset for ZS-CIR containing multiple ground truth images per query to enable more comprehensive evaluation. The annotation leverages SEARLE to ease collecting multiple ground truths.

- Experiments showing SEARLE achieves significant improvements over baselines on existing CIR datasets like FashionIQ and CIRR, and on the proposed CIRCO.

In summary, the main contribution is proposing the ZS-CIR task to remove the need for expensive training data in CIR, and SEARLE as a novel approach to tackle this new task in a zero-shot manner. The introduction of CIRCO also supports benchmarking progress on ZS-CIR.
