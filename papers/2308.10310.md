# [DVGaze: Dual-View Gaze Estimation](https://arxiv.org/abs/2308.10310)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve gaze estimation accuracy using a dual-view camera system. Specifically, the authors hypothesize that using images from two cameras can provide more complete facial information compared to a single camera, and allow for better gaze estimation.The key hypothesis is that fusing information from two camera views can compensate for facial occlusion and missing details that occur in single view gaze estimation, thereby improving accuracy. The authors propose and evaluate a dual-view gaze estimation network called DV-Gaze to test this hypothesis.In summary, the main research question is: Can using dual-view images improve gaze estimation accuracy compared to single-view methods? The paper introduces a dual-view gaze estimation method to address this question.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a dual-view gaze estimation network called DV-Gaze. This is the first work to explore dual-view gaze estimation using deep learning. 2. It introduces a dual-view interactive convolution (DIC) block that exchanges information between the two view features during convolution. Multiple DIC blocks are stacked to fuse features at different scales.3. It proposes a dual-view transformer module that takes the dual-view features and camera poses as input to estimate the gaze directions. 4. It uses a dual-view gaze consistency loss that enforces geometric consistency between the predicted gazes. This acts as a self-supervised loss.5. Experiments on ETH-XGaze and EVE datasets show DV-Gaze outperforms state-of-the-art single-view and multi-view gaze estimation methods. This demonstrates the benefits of leveraging dual-view information.In summary, the main contribution is the proposal of the first deep learning based dual-view gaze estimation method DV-Gaze, which intelligently fuses information from two views and outperforms prior arts. The dual-view interactive convolution block and consistency loss are key components leading to the performance gains.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a dual-view gaze estimation network called DV-Gaze that extracts and fuses features from two camera views to improve gaze estimation accuracy compared to using a single camera view.
