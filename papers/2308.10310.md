# [DVGaze: Dual-View Gaze Estimation](https://arxiv.org/abs/2308.10310)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve gaze estimation accuracy using a dual-view camera system. Specifically, the authors hypothesize that using images from two cameras can provide more complete facial information compared to a single camera, and allow for better gaze estimation.The key hypothesis is that fusing information from two camera views can compensate for facial occlusion and missing details that occur in single view gaze estimation, thereby improving accuracy. The authors propose and evaluate a dual-view gaze estimation network called DV-Gaze to test this hypothesis.In summary, the main research question is: Can using dual-view images improve gaze estimation accuracy compared to single-view methods? The paper introduces a dual-view gaze estimation method to address this question.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a dual-view gaze estimation network called DV-Gaze. This is the first work to explore dual-view gaze estimation using deep learning. 2. It introduces a dual-view interactive convolution (DIC) block that exchanges information between the two view features during convolution. Multiple DIC blocks are stacked to fuse features at different scales.3. It proposes a dual-view transformer module that takes the dual-view features and camera poses as input to estimate the gaze directions. 4. It uses a dual-view gaze consistency loss that enforces geometric consistency between the predicted gazes. This acts as a self-supervised loss.5. Experiments on ETH-XGaze and EVE datasets show DV-Gaze outperforms state-of-the-art single-view and multi-view gaze estimation methods. This demonstrates the benefits of leveraging dual-view information.In summary, the main contribution is the proposal of the first deep learning based dual-view gaze estimation method DV-Gaze, which intelligently fuses information from two views and outperforms prior arts. The dual-view interactive convolution block and consistency loss are key components leading to the performance gains.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a dual-view gaze estimation network called DV-Gaze that extracts and fuses features from two camera views to improve gaze estimation accuracy compared to using a single camera view.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in gaze estimation:- This paper explores dual-view gaze estimation, using two cameras to capture images of a person's face. Most prior work has focused on single-view gaze estimation with one camera. Using two cameras provides more facial information and can help compensate for limitations of a single view.- The proposed DV-Gaze network fuses information from the two views throughout the model, with a novel dual-view interactive convolution block. Other multi-view gaze papers have simply concatenated features from different views, rather than deeply integrating them.- Experiments show DV-Gaze outperforms state-of-the-art single-view and multi-view gaze estimation methods. On ETH-XGaze and EVE datasets, it reduces error by 10-30% compared to leading single-view methods.- The consistency loss between the two predicted gaze directions is a nice way to incorporate geometric constraints in a self-supervised manner. This is a fairly unique idea not seen in other papers.- The transformer architecture for combining dual-view features is also novel for this problem. Most gaze papers use CNNs or MLPs for feature fusion.- The analysis of how performance varies across different camera distances provides useful insights. Larger distances increase occlusion and perspective differences, which dual-view helps compensate for.Overall, this paper makes several nice contributions in exploring and evaluating dual-view gaze estimation. The dual-view interactive convolutions and consistency loss stand out as innovative ideas. The experiments demonstrate clear improvements over single-view methods. This points towards dual-view being a promising research direction.
