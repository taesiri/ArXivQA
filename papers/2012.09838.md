# Transformer Interpretability Beyond Attention Visualization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate improved explanations for the decisions made by Transformer models? Specifically, the authors note that prior methods for explaining Transformer models like attention maps or relevance propagation have various limitations. Attention maps only look at one component of the self-attention heads and don't consider the full model. Existing relevance propagation methods like LRP don't work well with Transformers due to challenges like skip connections, attention mechanisms, and non-ReLU activations. To address these limitations, the authors propose a new method to compute and propagate relevance scores through Transformer networks in a way that handles attention layers, skip connections, and maintains the total relevance. Their central hypothesis is that this new propagation method will produce superior explanations compared to prior approaches.The paper then validates this hypothesis through experiments on visual Transformer models like ViT and BERT models for NLP. They benchmark their approach on perturbation tests, segmentation, and a language reasoning task and demonstrate improved performance over attention maps, raw LRP, and other existing methods like GradCAM and rollout. Overall, the central thrust is developing an improved way to explain the predictions of Transformer models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel method to compute relevance for Transformer networks that assigns local relevance based on the Deep Taylor Decomposition principle and propagates these relevance scores through the layers. 2. It handles the challenges of propagating relevance through Transformer networks by proposing solutions for non-parametric layers like attention and skip connections. This includes a normalization technique to maintain the total relevance.3. It integrates the attention and relevance scores in a class-specific manner to produce visual explanations. This results in the method being inherently class-specific.4. It demonstrates the effectiveness of the proposed method through extensive experiments on visual and textual Transformer models. The method outperforms existing explainability techniques like attention maps, LRP, GradCAM etc. on tasks like image segmentation, positive/negative perturbation and language reasoning.In summary, the key contribution is a new methodology to generate class-specific explanations for Transformer models by propagating and integrating relevance in a principled manner. This enables better interpretation of these powerful models. The strength of the approach is shown through superior results on vision and language tasks compared to previous techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method for visualizing and explaining the decisions of Transformer models by assigning local relevance scores based on Deep Taylor Decomposition and propagating them through the layers while handling challenges like skip connections and attention mechanisms.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of transformer interpretability:- The main contribution of this paper is developing a new method for assigning relevance scores and visualizing decisions in transformer models like BERT. Most prior work has focused on using attention weights directly as a way to explain transformer models. However, as this paper argues, attention weights provide an incomplete picture and can highlight irrelevant tokens.- This paper proposes propagating relevance scores through all layers of a transformer using principles like Deep Taylor Decomposition. Their method handles unique challenges of transformers like skip connections and attention layers. Other methods like Layer-wise Relevance Propagation (LRP) have struggled with some aspects of transformer architectures.- A key advantage claimed is that this new method produces class-specific visualizations, unlike attention weights and other methods like LRP that are class-agnostic in practice. Producing class-specific explanations is important for understanding decisions. The paper shows examples where their method highlights different parts of an image depending on the target class.- The paper evaluates the new method on a diverse set of vision and NLP datasets, including segmentation, perturbation analysis, and a language rationale extraction task. The proposed approach outperforms baselines like attention, LRP, GradCAM, and rollout across these benchmarks.- Compared to other propagation-based methods like LRP, this paper handles unique aspects of transformers like skip connections and multiple attention layers. It also integrates both relevance and gradient information to improve explanations. The ablation studies demonstrate the importance of these contributions.- Overall, this paper makes significant advances in transformer interpretability by developing a relevance propagation method tailored to key properties of transformers. The thorough quantitative and qualitative evaluation on vision and language tasks shows the promise of this approach compared to existing techniques.In summary, this paper moves beyond using just attention weights for transformer interpretability, handles unique challenges of transformers, produces class-specific explanations, and demonstrates effectiveness on diverse vision and NLP tasks. It makes important progress compared to prior work in developing relevance propagation techniques for state-of-the-art transformer models.
