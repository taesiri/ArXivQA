# Transformer Interpretability Beyond Attention Visualization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate improved explanations for the decisions made by Transformer models? Specifically, the authors note that prior methods for explaining Transformer models like attention maps or relevance propagation have various limitations. Attention maps only look at one component of the self-attention heads and don't consider the full model. Existing relevance propagation methods like LRP don't work well with Transformers due to challenges like skip connections, attention mechanisms, and non-ReLU activations. To address these limitations, the authors propose a new method to compute and propagate relevance scores through Transformer networks in a way that handles attention layers, skip connections, and maintains the total relevance. Their central hypothesis is that this new propagation method will produce superior explanations compared to prior approaches.The paper then validates this hypothesis through experiments on visual Transformer models like ViT and BERT models for NLP. They benchmark their approach on perturbation tests, segmentation, and a language reasoning task and demonstrate improved performance over attention maps, raw LRP, and other existing methods like GradCAM and rollout. Overall, the central thrust is developing an improved way to explain the predictions of Transformer models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel method to compute relevance for Transformer networks that assigns local relevance based on the Deep Taylor Decomposition principle and propagates these relevance scores through the layers. 2. It handles the challenges of propagating relevance through Transformer networks by proposing solutions for non-parametric layers like attention and skip connections. This includes a normalization technique to maintain the total relevance.3. It integrates the attention and relevance scores in a class-specific manner to produce visual explanations. This results in the method being inherently class-specific.4. It demonstrates the effectiveness of the proposed method through extensive experiments on visual and textual Transformer models. The method outperforms existing explainability techniques like attention maps, LRP, GradCAM etc. on tasks like image segmentation, positive/negative perturbation and language reasoning.In summary, the key contribution is a new methodology to generate class-specific explanations for Transformer models by propagating and integrating relevance in a principled manner. This enables better interpretation of these powerful models. The strength of the approach is shown through superior results on vision and language tasks compared to previous techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method for visualizing and explaining the decisions of Transformer models by assigning local relevance scores based on Deep Taylor Decomposition and propagating them through the layers while handling challenges like skip connections and attention mechanisms.
