# [ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity   Recognition](https://arxiv.org/abs/2403.17385)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current NER settings require impractical amounts of annotation (e.g. 10K+ tokens for 5% of CoNLL-2003 data). This is unrealistic for many real-world applications that need custom models quickly.
- Recent NER methods relying on decoder-based large language models (LLMs) don't scale well due to high inference overhead. 
- Most methods rely solely on neural networks, ignoring potentially useful linguistic rules.

Proposed Solution:
- The paper proposes an extremely light supervision setting using only a lexicon with 10 examples per class created by a domain expert without looking at any labels.
- They introduce ELLEN, a simple, modular, neuro-symbolic method combining fine-tuned language models with linguistic rules and insights. 
- ELLEN uses a 3-stage framework balancing between avoiding pitfalls of classic self-training and still conceptually being similar to self-training.
- Key components include: Masked LM Heuristic (fully unsupervised NER), Dynamic Window Filtering (controls false negatives), Global Rules (disambiguation), One Sense Per Discourse, and Confidence-Based Rules.

Main Contributions:
- Demonstrate combining language models and linguistic rules under a self-training, modular, neuro-symbolic architecture.
- Introduce the Masked LM Heuristic for "free" supervision, which achieves 55%+ precision on CoNLL-2003 as fully unsupervised NER.
- Evaluate ELLEN on CoNLL-2003 under varying supervision levels. Under proposed extreme light supervision, achieve 76.87% F1. Outperform state-of-the-art semi-supervised methods under their common 5% labeled data setting.
- Evaluate ELLEN in zero-shot scenario on WNUT-17. Achieve performance comparable to GPT-3.5 and GPT-4. Also achieve over 75% of performance of model trained on WNUT-17 gold data.

In summary, the paper demonstrates an effective approach for extremely lightly supervised NER by harmoniously combining neural networks and linguistic rules under a novel self-training framework. The method requires minimal human effort and achieves strong performance across varying low resource scenarios.
