# [ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity   Recognition](https://arxiv.org/abs/2403.17385)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current NER settings require impractical amounts of annotation (e.g. 10K+ tokens for 5% of CoNLL-2003 data). This is unrealistic for many real-world applications that need custom models quickly.
- Recent NER methods relying on decoder-based large language models (LLMs) don't scale well due to high inference overhead. 
- Most methods rely solely on neural networks, ignoring potentially useful linguistic rules.

Proposed Solution:
- The paper proposes an extremely light supervision setting using only a lexicon with 10 examples per class created by a domain expert without looking at any labels.
- They introduce ELLEN, a simple, modular, neuro-symbolic method combining fine-tuned language models with linguistic rules and insights. 
- ELLEN uses a 3-stage framework balancing between avoiding pitfalls of classic self-training and still conceptually being similar to self-training.
- Key components include: Masked LM Heuristic (fully unsupervised NER), Dynamic Window Filtering (controls false negatives), Global Rules (disambiguation), One Sense Per Discourse, and Confidence-Based Rules.

Main Contributions:
- Demonstrate combining language models and linguistic rules under a self-training, modular, neuro-symbolic architecture.
- Introduce the Masked LM Heuristic for "free" supervision, which achieves 55%+ precision on CoNLL-2003 as fully unsupervised NER.
- Evaluate ELLEN on CoNLL-2003 under varying supervision levels. Under proposed extreme light supervision, achieve 76.87% F1. Outperform state-of-the-art semi-supervised methods under their common 5% labeled data setting.
- Evaluate ELLEN in zero-shot scenario on WNUT-17. Achieve performance comparable to GPT-3.5 and GPT-4. Also achieve over 75% of performance of model trained on WNUT-17 gold data.

In summary, the paper demonstrates an effective approach for extremely lightly supervised NER by harmoniously combining neural networks and linguistic rules under a novel self-training framework. The method requires minimal human effort and achieves strong performance across varying low resource scenarios.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces ELLEN, an extremely lightly supervised neural-symbolic method for named entity recognition that blends transformer models with linguistic rules and achieves strong performance using only a small lexicon with 10 examples per class as supervision.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Demonstrating the effectiveness of combining language models with commonsense linguistic rules under a self-training, modular, neuro-symbolic architecture for semi-supervised NER. The approach is considerably simpler than other complex statistical methods.

2) Introducing a novel component called the Masked Language Model (MLM) Heuristic, which is a fully unsupervised NER method that achieves over 55% precision on the CoNLL-2003 dataset and complements other heuristics. 

3) Evaluating the method on CoNLL-2003 under three supervision settings and in a zero-shot setting on WNUT-17. Under extremely limited supervision, the method achieves 76.87% F1 on CoNLL-2003. It also outperforms most existing semi-supervised methods under the same 5% labeled data setting. In zero-shot evaluation, the method is comparable to GPT-3.5 and GPT-4 on WNUT-17 while obtaining over 75% of the performance of a supervised model.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Extremely light supervision
- Named entity recognition (NER)
- Neuro-symbolic methods
- Modular architectures
- Self-training
- Language models
- Linguistic rules
- One sense per discourse
- Masked language modeling heuristic
- Dynamic window filtering
- Confidence-based rules
- Zero-shot learning
- Transfer learning

The paper proposes an extremely lightly supervised NER method called ELLEN that combines fine-tuned language models with linguistic rules under a neuro-symbolic, modular, self-training framework. Key aspects include using a masked language model as an unsupervised NER, leveraging insights like "one sense per discourse", and evaluating the method in zero-shot and low-resource scenarios. The key terms reflect this focus on semi-supervised NER, integrating neural and symbolic techniques, and limited supervision.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper argues that annotating the equivalent amount of data as used in typical semi-supervised NER (5% of CoNLL-2003) would take approximately 9 person hours. Could you elaborate more on how you arrived at this time estimate? What were the assumptions made?

2. Dynamic window filtering is proposed to control the effect of false negatives in sparsely annotated data. How does this algorithm specifically work to identify and eliminate false negatives during training? Could you provide more implementation details?

3. The masked language model (MLM) heuristic is shown to achieve over 55% F1 score on CoNLL-2003 in a fully unsupervised manner. What is the intuition behind using the MLM objective for unsupervised NER? How exactly does the masking and filling strategy allow labeling of spans?

4. The paper mentions blending fine-tuned language models with linguistic rules under a neuro-symbolic architecture. Could you expand more on why this hybrid approach was chosen over using just neural networks? What are the pros and cons?

5. One of the ideas presented is "curriculum learning" where models are trained on gradually noisier examples. How exactly was this implemented in determining the data augmentation order? What thresholds dictated noise levels?

6. The paper argues that annotating the lexicon requires minimum effort from experts. But doesn't the criteria outlined for selecting unambiguous entities already require domain knowledge and multiple iterations? Please elaborate.

7. Error analysis shows confusion between ORG and LOC entities. The paper attributes it to four possible reasons. Could you expand on why the global rule was not adequate in disambiguating such entities in nuanced contexts?

8. Dynamic window filtering uses POS tags from an off-the-shelf tagger trained on a different dataset. Doesn't this introduce more noise due to distribution shift? How can this issue be alleviated? 

9. The method seems to have many hyperparmeters that require tuning like noise level, window size, thresholds etc. Was there any guide or intuition followed for setting these or was it mostly empirical search?

10. For practical deployments, what would be your recommendations in terms of where the proposed approach might need improvements or additional research to make it more robust?
