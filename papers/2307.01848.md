# Embodied Task Planning with Large Language Models

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on the task of embodied task planning for robots and agents using large language models (LLMs). The key research question is: How can we enable embodied agents to generate executable action plans for complex tasks in general indoor environments based on natural language instructions and visual perception of the surroundings? The central hypothesis is that by combining the knowledge and reasoning capabilities of LLMs with grounded visual scene information, the agent can produce more feasible and plausible action sequences to complete human instructions compared to using just the LLM alone.Specifically, the paper proposes:1) A method to construct a large-scale multimodal dataset of visual scenes, instructions, and action plans for training the task planner. This is done by prompting the GPT-3.5 LLM to generate grounded instructions and plans based on object lists representing the scenes.2) A framework called TaPA that finetunes a pretrained LLM (LLaMA) on this dataset to serve as the task planner. It takes as input the instruction and predicted objects from an open-vocabulary detector over multi-view images. 3) An evaluation benchmark measuring task planning success on complex instructions (more steps compared to prior datasets like ALFRED).The key hypothesis is that by grounding the LLM's knowledge with visual perceptions, the TaPA agent can produce more executable plans than using the LLM alone or other large multimodal models. The experiments aim to validate this hypothesis and the advantages of the overall approach.In summary, the paper tackles the research problem of task planning for embodied agents by grounding LLMs with visual scene information, via a new dataset, model architecture, and benchmark. The central hypothesis is that this combination will yield more feasible plans.
