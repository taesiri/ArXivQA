# Embodied Task Planning with Large Language Models

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on the task of embodied task planning for robots and agents using large language models (LLMs). The key research question is: How can we enable embodied agents to generate executable action plans for complex tasks in general indoor environments based on natural language instructions and visual perception of the surroundings? The central hypothesis is that by combining the knowledge and reasoning capabilities of LLMs with grounded visual scene information, the agent can produce more feasible and plausible action sequences to complete human instructions compared to using just the LLM alone.Specifically, the paper proposes:1) A method to construct a large-scale multimodal dataset of visual scenes, instructions, and action plans for training the task planner. This is done by prompting the GPT-3.5 LLM to generate grounded instructions and plans based on object lists representing the scenes.2) A framework called TaPA that finetunes a pretrained LLM (LLaMA) on this dataset to serve as the task planner. It takes as input the instruction and predicted objects from an open-vocabulary detector over multi-view images. 3) An evaluation benchmark measuring task planning success on complex instructions (more steps compared to prior datasets like ALFRED).The key hypothesis is that by grounding the LLM's knowledge with visual perceptions, the TaPA agent can produce more executable plans than using the LLM alone or other large multimodal models. The experiments aim to validate this hypothesis and the advantages of the overall approach.In summary, the paper tackles the research problem of task planning for embodied agents by grounding LLMs with visual scene information, via a new dataset, model architecture, and benchmark. The central hypothesis is that this combination will yield more feasible plans.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors propose a task planning agent called TaPA for generating executable action plans grounded in physical scenes. This allows the agent to accomplish complex household tasks based on natural language instructions. 2. They construct a large-scale multimodal dataset containing triplets of visual scenes, instructions, and corresponding action plans. This is used to fine-tune pre-trained large language models as the task planner.3. They evaluate different large language models (LLMs) and large multimodal models (LMMs) on embodied task planning using their proposed benchmark. Their method outperforms baselines by a significant margin.4. They study different strategies for representing the visual scene, such as collecting multi-view RGB images and using open-vocabulary object detection. This allows grounded action plans to be generated based on the objects that exist in the real environment.5. The embodied tasks considered are more complex and diverse compared to prior work, involving longer action sequences for tasks like making sandwiches. The proposed method can generate plausible plans for such complex instructions.In summary, the main contribution is proposing a framework and benchmark for embodied task planning that can leverage large pre-trained models and ground them in physical scenes via multi-view perception. This enables generating executable plans for complex household tasks based on natural language instructions.
