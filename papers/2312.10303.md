# [Online Restless Multi-Armed Bandits with Long-Term Fairness Constraints](https://arxiv.org/abs/2312.10303)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies the restless multi-armed bandit (RMAB) problem, which is used to model sequential decision making under constraints. In the basic RMAB model, there are N "arms" (options to choose from), each following a Markov decision process (MDP). At each time step, the decision maker can activate up to B arms based on their states, with the goal of maximizing long-term rewards. However, this model does not provide any fairness guarantees in how activations are distributed among the arms over time. 

To address this, the paper introduces a new RMAB model called RMAB-F that imposes additional "long-term fairness constraints". Specifically, it requires that each arm n must be activated for at least a minimum fraction Î·n of time steps in the long run. The goal now is to maximize long-term rewards while satisfying both a constraint on instantaneous activations per time step, and long-term fairness constraints.

Proposed Solution:
The paper focuses on the online setting where the MDPs of the arms are initially unknown. It develops a reinforcement learning algorithm called Fair-UCRL to solve this online RMAB-F problem. Fair-UCRL has two key features:

1. It provides a probabilistic sublinear bound on both the reward regret (loss in rewards due to lack of knowledge of MDPs) and fairness violation regret (loss in long-term fairness) that holds with high probability.

2. It is computationally efficient, leveraging an index policy for decision making instead of solving complicated Bellman equations. This allows satisfying instantaneous activation constraints.

Main Contributions:

1. Introduces a new RMAB model with long-term fairness constraints (RMAB-F) and formalizes the online learning problem.

2. Develops a novel RL algorithm, Fair-UCRL, with order-optimal regret bounds on both reward and fairness violation.

3. Fair-UCRL uses an efficient index policy for decisions that provably meets instantaneous activation constraints.

4. Experiments on real-world applications demonstrate Fair-UCRL's effectiveness in balancing maximizing rewards and providing fairness guarantees.

In summary, the paper makes important contributions in constrained RMAB problems by formalizing and providing an efficient learning algorithm for RMAB with long-term fairness guarantees. The performance is supported by theoretical regret analysis and empirical evaluations.
