# [Adversarial Counterfactual Visual Explanations](https://arxiv.org/abs/2303.09962)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that denoising diffusion probabilistic models (DDPMs) can be used to turn adversarial attacks into semantically meaningful and perceptually realistic counterfactual explanations. The key questions/goals addressed are:- How to generate counterfactual explanations that make minimal but perceptually realistic changes to flip a classifier's prediction.- How to generate such explanations without needing to modify or retrain the classifier being explained.- How to create counterfactuals that are valid (flip the prediction), sparse/proximal to the input, diverse, and realistic.To summarize, the paper proposes using DDPMs as a "polishing" step to take adversarial attacks and turn them into semantically meaningful and realistic counterfactual explanations that can help users understand classifiers. The key innovation is using DDPMs to robustify classifiers without changing their weights, enabling counterfactual generation via adversarial attacks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Adversarial Counterfactual Explanations (ACE) to generate counterfactual visual explanations. The key points are:- ACE uses adversarial attacks to flip the prediction of a model and generate semantic changes in images to produce counterfactual explanations. This allows generating meaningful modifications even for non-robust models.- ACE robustifies brittle classifiers by using a denoising diffusion probabilistic model (DDPM) as a filtering function. This lets ACE keep classifier performance untouched while enabling counterfactual generation through adversarial attacks. - ACE refines the generated explanations to only modify relevant regions and keep unimportant parts identical to the input image. This makes the explanations more interpretable.- Experiments across multiple datasets show ACE outperforms previous state-of-the-art methods on various metrics. The diversity, validity, sparsity, and realism of ACE's counterfactuals are improved.- ACE can generate actionable modifications that successfully fool classifiers in real scenarios, demonstrating its ability to expose model weaknesses.In summary, the key contribution is a new approach to generate semantically meaningful and interpretable counterfactual visual explanations even for non-robust models, outperforming previous methods. The use of DDPMs and refinement makes ACE's explanations higher quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Adversarial Counterfactual Explanations (ACE), a new method to generate semantically meaningful counterfactual explanations by using denoising diffusion probabilistic models to robustify classifiers and enable adversarial attacks to produce interpretable image edits, outperforming previous state-of-the-art methods.
