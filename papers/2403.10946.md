# [The Fallacy of Minimizing Local Regret in the Sequential Task Setting](https://arxiv.org/abs/2403.10946)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- In reinforcement learning (RL), algorithms typically aim to minimize cumulative regret in a stationary environment. However, real-world RL tasks often arrive sequentially with substantial changes between tasks. 
- Two key differences from theoretical setups: (1) Significant changes between tasks in reward design, policy spaces due to technological/human factors (2) Necessity of static/non-adaptive policies in some tasks due to practical constraints.
- Applying algorithms focused narrowly on minimizing local regret can perform poorly on subsequent tasks. There is a tradeoff between minimizing local vs global regret.

Proposed Solution:
- Formulates a sequential multitask contextual bandit problem with changes in policy spaces and reward functions between tasks.
- Provides a lower bound that characterizes the tradeoff between local and global regret in a two task setting. Shows conditions on changes that lead to a strong tension, with order $\Omega(1)$ minimax lower bound on $\sqrt{\E[\Reg_1]} (\E[\Reg_2])/T_2$.
- Analyzes optimal exploration rate to balance regrets in two tasks. Shows different regimes based on task horizons where different exploration rates are needed.
- Extends analysis to multiple tasks, showing similar tension holds between a task's regret and preceding tasks' regret when adaptivity in that task is not allowed.
- Considers shifts in outcome distribution between tasks, proposes 'robust simple regret' metric. Shows similar tension still holds.  

Key Contributions:
- Formalizes and provides fundamental limits on adapting RL algorithms myopically focused on local regret minimization to complex sequential tasks with multiple changes.
- Key insight on need for additional exploration compared to stationary settings to account for unpredictable changes over sequences of tasks. 
- Analysis guides exploration rate selection based on task horizons to optimally balance local vs global regret.
- Results suggest common practices like clipped policies, fixed $\epsilon$-greedy provide a reasonable heuristic.
