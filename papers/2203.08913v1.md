# [Memorizing Transformers](https://arxiv.org/abs/2203.08913v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it aims to address is: 

How can language models be extended to memorize and retrieve long-range dependencies in text, instead of needing to be trained or fine-tuned to acquire new knowledge?

The key ideas explored in the paper to address this question are:

- Using an approximate k-Nearest Neighbors (kNN) lookup into a non-differentiable external memory of (key, value) pairs to allow the model to attend to a much longer context. 

- Not backpropagating gradients into the external memory, allowing it to scale to very large sizes (e.g. 262k tokens) by reusing computed keys/values instead of recomputing on every training step.

- Demonstrating how this memorization and retrieval of long-range dependencies improves perplexity on diverse language modeling tasks involving long-form text like books, LaTeX documents, code, and formal proofs.

- Showing the model is able to look up and use definitions, function names, etc. that were provided much earlier in the context, enabling a form of rapid learning or acquisition of knowledge at test time.

So in summary, the key hypothesis is that adding an external memory with efficient kNN lookup will enable language models to memorize and retrieve long-range dependencies instead of needing to learn them through extensive training. The experiments and analyses aim to demonstrate these capabilities across different domains.


## What is the main contribution of this paper?

 Based on my reading of the paper abstract, the main contribution seems to be:

1. Extending language models with the ability to memorize the internal representations of past inputs using an approximate kNN lookup into a non-differentiable memory. 

2. Demonstrating that this kNN-augmented attention mechanism improves language modeling performance across various benchmarks and tasks, including web text, math papers, books, code, and formal theorems.

3. Showing that performance steadily improves as the memory size is increased up to 262K tokens, indicating the model is capable of effectively utilizing a large external memory.

4. Providing evidence that models can make use of this memory in intended ways, such as looking up function definitions in code and mathematical lemma definitions in proofs. 

5. Demonstrating these memory improvements are maintained even when scaling up to larger transformer models, with the memory providing gains greater than increasing model size 5X.

In summary, the key contribution is augmenting transformers with a scalable long-term memory that allows models to rapidly memorize and retrieve new facts and definitions, leading to improved language modeling, especially on tasks requiring long-range reasoning or rapid incorporation of new knowledge. The simplicity of the approach allows it to be integrated into existing models.
