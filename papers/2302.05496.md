# [MaskSketch: Unpaired Structure-guided Masked Image Generation](https://arxiv.org/abs/2302.05496)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to generate realistic images conditioned on a spatial layout specified by an input sketch, without requiring paired training data or model finetuning. 

The key ideas and contributions are:

- They propose MaskSketch, a sketch-guided image generation method that leverages the self-attention maps from a pre-trained masked generative transformer (MaskGIT) to encode spatial structure.

- They show the self-attention maps are robust to the domain shift between sketches and real images, allowing their use as a structure similarity metric. 

- They introduce a structure-based parallel sampling procedure that balances optimizing the self-attention structure similarity and the natural image prior of MaskGIT.

- Their method achieves high fidelity to input sketches of varying abstraction levels, while generating highly realistic and diverse results, outperforming prior unsupervised sketch-to-image and image-to-image translation methods.

- Their approach does not require fine-tuning or paired training data, thanks to building on top of a powerful pre-trained generator.

In summary, the key hypothesis is that self-attention maps can enable sketch-conditioned image generation without paired training data by capturing spatial layout in a domain invariant way. Their experiments validate this approach leads to state-of-the-art sketch-based image synthesis.
