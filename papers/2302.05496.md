# [MaskSketch: Unpaired Structure-guided Masked Image Generation](https://arxiv.org/abs/2302.05496)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to generate realistic images conditioned on a spatial layout specified by an input sketch, without requiring paired training data or model finetuning. 

The key ideas and contributions are:

- They propose MaskSketch, a sketch-guided image generation method that leverages the self-attention maps from a pre-trained masked generative transformer (MaskGIT) to encode spatial structure.

- They show the self-attention maps are robust to the domain shift between sketches and real images, allowing their use as a structure similarity metric. 

- They introduce a structure-based parallel sampling procedure that balances optimizing the self-attention structure similarity and the natural image prior of MaskGIT.

- Their method achieves high fidelity to input sketches of varying abstraction levels, while generating highly realistic and diverse results, outperforming prior unsupervised sketch-to-image and image-to-image translation methods.

- Their approach does not require fine-tuning or paired training data, thanks to building on top of a powerful pre-trained generator.

In summary, the key hypothesis is that self-attention maps can enable sketch-conditioned image generation without paired training data by capturing spatial layout in a domain invariant way. Their experiments validate this approach leads to state-of-the-art sketch-based image synthesis.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing MaskSketch, an unpaired sketch-guided image generation method that allows control over the spatial layout of the generated image without requiring paired training data or model finetuning. 

Specifically, the key contributions are:

- Showing that the self-attention maps of a masked generative transformer (MaskGIT) encode important structural information about the input image, and are robust to the domain shift between sketches and natural images. 

- Proposing a sampling method based on self-attention similarity that balances following the structural guidance of the input sketch and the natural image prior captured by MaskGIT.

- Demonstrating that MaskSketch outperforms state-of-the-art methods in unpaired sketch-to-photo translation, enabling photorealistic generation guided by sketches of varying abstraction levels using only class label supervision.

- To the best of the authors' knowledge, MaskSketch is the first method for sketch-to-photo translation that produces realistic results with only class label supervision, without requiring sketch-photo paired training data.

In summary, the key contribution is introducing an unpaired sketch-guided image generation approach that leverages the structural information encoded in a pre-trained masked generative transformer's attention maps to enable fine-grained spatial control over the generated image. This allows photorealistic sketch-to-image translation without requiring model finetuning or paired training data.
