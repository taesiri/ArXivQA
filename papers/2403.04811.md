# [Quantifying Contamination in Evaluating Code Generation Capabilities of   Language Models](https://arxiv.org/abs/2403.04811)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) have shown remarkable performance on code generation benchmarks. However, there are growing concerns about potential contamination of these benchmarks from the pretraining and finetuning data.
- While contamination has been studied for natural language tasks, there has been limited research quantifying it for code generation. This is critical to understand the robustness and reliability of LLMs for programming.
- Code has key differences from natural language that warrant a deeper examination - syntax and naming of functions/variables can vary across equivalent programs. So traditional surface-level comparisons may be inadequate.

Proposed Solution:
- The authors develop a pipeline to precisely measure overlap between popular code generation benchmarks (MBPP, HumanEval) and pretraining corpora (\textsc{Pile}, \textsc{Stack}) using both surface-level and semantic-level similarity.
- Surface similarity is computed using Levenshtein distance. Semantic similarity uses the Dolos toolkit to compare abstract syntax trees.
- An aggregated similarity score combines both surface and semantic similarity. Overlapping programs are identified across different similarity thresholds.
- Data contamination is quantified based on similarity scores and the frequency of observing similar solutions during training.

Key Findings:
- There is significant contamination in both MBPP (up to 20.8\%) and HumanEval (up to 18.9\%) by the \textsc{Stack}. Relatively less for \textsc{Pile} (3.6\% on MBPP).
- Models perform much better on questions with seen vs unseen solutions, indicating memorization. The gaps can be over 50\% in accuracy.
- After de-contamination, model rankings don't change but performance gaps reduce. Contamination may thus exaggerate differences.
- Larger models memorize better. But there's no clear correlation between problem difficulty/length and memorization.

Contributions:
- Precisely quantified data contamination in popular code generation benchmarks using both surface and semantic similarity matching. 
- Showed significant memorization effects from seeing solutions during training through accuracy improvements.
- Released matched files to enable further research on understanding generalization of code LLMs.
