# [Longitudinal Counterfactuals: Constraints and Opportunities](https://arxiv.org/abs/2403.00105)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Counterfactual explanations are often proposed as a way to provide recourse to individuals regarding algorithmic decisions. However, current methods often generate counterfactuals that are implausible or unachievable for a given individual. This makes justifying their use for recourse difficult.
- While plausibility is agreed to be an important quality for counterfactuals aimed at providing recourse, quantifying ground truth plausibility continues to be challenging. 

Proposed Solution:  
- The paper proposes using longitudinal data as a way to assess and improve the plausibility of counterfactual explanations. 
- They introduce a longitudinal distance metric that compares the difference between an individual's current state and their counterfactual state to differences seen in the longitudinal data. This allows them to quantify how similar a counterfactual change is to changes that have been previously observed.
- This metric can then be used to re-score/rank existing counterfactuals by plausibility. It can also be incorporated into the counterfactual generation process itself as an additional optimization constraint.

Main Contributions:
- Conceptualizing counterfactuals as potential future states and using longitudinal data as a proxy for plausibility 
- Introduction of a longitudinal distance metric to compare counterfactual changes to observed historical changes
- Demonstration that the metric can effectively discriminate between more and less plausible counterfactuals
- Finding that requiring plausibility significantly reduces the proportion of individuals for which valid counterfactuals can be generated
- Discussion of inherent difficulties of providing true recourse through counterfactuals alone due to constraints on achievability


## Summarize the paper in one sentence.

 This paper proposes using longitudinal data to assess and improve the plausibility of counterfactual explanations, which are often used to provide recourse to individuals subject to algorithmic decisions, though there are inherent difficulties in offering genuine recourse through counterfactuals alone.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new method to evaluate and improve the plausibility of counterfactual explanations using longitudinal data. Specifically:

1) The paper introduces a novel longitudinal distance metric that compares the difference between an individual's current features and their counterfactual features to observed longitudinal changes in the data. This allows assessing how similar a counterfactual change is to changes that have been previously observed.

2) The metric can be used to re-score/rank existing counterfactuals by plausibility. Experiments on a health dataset show it is effective at discriminating plausible from implausible explanations.

3) The metric can also be incorporated into the counterfactual generation process itself as an additional optimization constraint. Experiments compare a genetic algorithm that optimizes proximity and longitudinal distance vs. one that optimizes proximity and sparsity.

4) Analysis and discussion emphasize difficulties of using counterfactuals for recourse in practice. Even plausible counterfactuals may not be achievable for individuals. The paper argues more philosophical/practical work is needed to bridge explainability and recourse.

In summary, the main contribution is using longitudinal data to quantify plausibility of counterfactuals, and showing this can be used to both evaluate and improve (genetic generation of) counterfactual explanations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Counterfactual explanations - The paper focuses on using counterfactual explanations to provide recourse to data subjects. Counterfactuals explain a prediction by finding a small change to get a desired outcome.

- Plausibility - A key concept is improving the plausibility of counterfactual explanations so they respect constraints of reality and are possible states for a data subject.

- Longitudinal data - The paper proposes using longitudinal data as a proxy for assessing and improving plausibility of counterfactuals. Compares proposed changes to observed historical changes.

- Recourse - Motivation of paper is providing recourse (a path to a desired decision) through counterfactual explanations. Requires counterfactuals be both plausible and achievable.

- Proximity - Objective function to keep counterfactuals close to original data point. Tradeoff between proximity and plausibility.

- Genetic algorithm - Method used to generate counterfactuals by optimizing proximity and longitudinal distance objectives.

- Validity - Whether a counterfactual explanation has the desired prediction. Tradeoff between validity and plausibility.

- Achievability - Subjective perspective on whether a data subject can realistically achieve the change proposed by a counterfactual.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a longitudinal distance metric to assess the plausibility of counterfactual explanations. How exactly is this metric defined and what are some of the key components it compares between the counterfactual and longitudinal data?

2. The paper normalizes the longitudinal distance metric in several ways, including using the median absolute deviation (MAD) and average absolute deviation (AAD). What is the motivation behind normalizing the metric in these ways and how does it help assess plausibility? 

3. The paper explores using the longitudinal distance metric both for scoring/ranking existing counterfactuals and during the generation process itself. What are the potential advantages and disadvantages of using it in each of these ways?

4. What types of longitudinal data would be most useful for assessing plausibility with this metric? What are some key properties or characteristics it should have? What are limitations of using longitudinal data in this way?

5. The genetic algorithm proposed generates counterfactuals by optimizing both proximity and longitudinal distance objectives. How do these two objectives interact and potentially compete during the optimization process? How is the balance determined?

6. In the experiment with MIMIC-III data, what causes the large differences observed between allowing changes to only vital signs versus all features? What does this suggest about constraints and plausibility?

7. The Adult Income simulation constructs longitudinal data by allowing changes to certain features like occupation and education level. Why were these specific features chosen? What impact did that choice have on the plausibility assessment?  

8. The paper discusses difficulties balancing validity and plausibility of counterfactuals. What causes this tension? How do constraints aimed at improving plausibility potentially degrade validity?

9. The discussion raises issues around whether counterfactuals should aim for justification or for actual recourse. What is the difference between these two goals? Why does aiming for recourse pose challenges?

10. What are some ways the longitudinal distance metric could be enhanced in future work, such as by modeling the longitudinal data? What methodology could help address some of the limitations raised?
