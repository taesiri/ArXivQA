# [Make Landscape Flatter in Differentially Private Federated Learning](https://arxiv.org/abs/2303.11242)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper seeks to address is: how can we alleviate the severe performance degradation issue caused by dropped model information and exacerbated model inconsistency in differentially private federated learning (DPFL)? 

Specifically, existing DPFL methods tend to suffer from poor performance compared to federated learning without privacy protection. The authors hypothesize this is due to two key issues:

1) Useful information is dropped when clipping the norms of local updates to enforce DP. This loses important information contained in the local updates.

2) Adding random noise to local updates damages the updates and leads to greater inconsistency between local models, exacerbating the performance degradation.

To address these issues, the authors propose a novel DPFL algorithm called DP-FedSAM. The key idea is to use the Sharpness Aware Minimization (SAM) optimizer in each client to generate flatter local loss landscapes. This results in better model stability and robustness to the noise from DP. Aggregating several flatter local models creates a "potentially global flat model" with improved generalization ability and robustness. 

In summary, the central hypothesis is that using SAM to create flatter loss landscapes will mitigate the negative impacts of DP such as dropped information and model inconsistency. This will alleviate performance degradation compared to prior DPFL methods. The paper provides theoretical analysis and extensive experiments to evaluate this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a new algorithm called DP-FedSAM for differentially private federated learning (DPFL) to alleviate the performance degradation issue caused by dropped model information and exacerbated model inconsistency in existing DPFL methods. 

2. It provides theoretical analysis on the convergence rate, sensitivity, and privacy for DP-FedSAM. The convergence rate bound is tighter than previous works. The analysis also combines the impacts of the on-average norm and consistency of local updates on the convergence.

3. It empirically evaluates DP-FedSAM on datasets like EMNIST, CIFAR-10/100, showing its superiority over state-of-the-art DPFL baselines. The results also confirm the theoretical analysis on the norm and consistency of local updates.

4. It visualizes the loss landscapes and contours to provide insights on how DP-FedSAM generates flatter landscape for better generalization and robustness against noise compared to baseline DP-FedAvg.

In summary, the main contribution is proposing the DP-FedSAM algorithm along with theoretical and empirical analysis to address the performance degradation issue in DPFL. The integration of SAM optimizer is novel to make the landscape flatter and mitigate the negative impacts of operations in DP.
