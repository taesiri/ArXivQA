# [Are LLMs Robust for Spoken Dialogues?](https://arxiv.org/abs/2401.02297)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-2 and T5 have shown great performance on written dialogues, but their robustness on spoken dialogues is unknown. 
- There is a lack of proper spoken dialogue datasets to train and evaluate models. Existing spoken dialogue test sets are small and only provide transcripts, limiting analysis.

Method:
- Authors automatically transcribed a subset of spoken task-oriented dialogues and analyzed the types and distribution of speech recognition errors.
- They injected these error patterns into the MultiWOZ written dialogue training set to simulate spoken noise. 
- GPT-2 and T5 models were fine-tuned on clean and noisy versions of MultiWOZ for response generation and state tracking.
- Models were evaluated on verbatim and paraphrased spoken test sets on perplexity, goal accuracy and human judgments.

Key Findings:
- Models fine-tuned on clean written dialogues perform much worse on spoken test sets.  
- Injecting speech errors into training data leads to minor perplexity gains but hurts goal accuracy.
- However, noisy training data significantly improves appropriateness and contextualization of responses per human judges.
- Results show importance of proper spoken training data, and limitations of automatic metrics vs human ratings.

In summary, the key contributions are collecting and analyzing a spoken dialogue test set, simulating speech errors to show limitations of LLMs on spoken conversations, and conducting extensive human experiments to highlight gaps between automatic and human evaluation.


## Summarize the paper in one sentence.

 This paper evaluates the robustness of large language models like GPT-2 and T5 for spoken task-oriented dialogues by fine-tuning them on clean and noise-injected dialogues, finding that they are not robust by default but training on proper spoken dialogue data can improve robustness.


## What is the main contribution of this paper?

 The main contributions of this paper can be summarized as:

1. The authors transcribed a development set of spoken task-oriented dialogues and studied the error types and distributions observed in ASR transcriptions. They then generated a noisy TOD dataset by injecting these ASR error distributions into a large dataset of written TODs. 

2. The authors fine-tuned two LLMs (GPT-2 and T5) for response generation and dialogue state tracking using both clean and noise-injected TOD datasets.

3. The authors evaluated the performance of the fine-tuned models on two spoken test sets - Human-Verbatim and Human-Paraphrased - for the tasks of response generation and dialogue state tracking. Their analysis showed that while LLMs are not robust to spoken noise by default, fine-tuning on proper spoken TOD data can improve robustness. The human evaluation results in particular showed improved appropriateness and contextualization for the model fine-tuned on noisy data.

So in summary, the main contribution is studying the robustness of LLMs for spoken TODs, creating simulated spoken training data by injecting ASR errors, and showing both quantitatively and via human evaluation that fine-tuning on such noisy data can improve model robustness.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Large language models (LLMs) - The paper evaluates the robustness of large pre-trained language models like GPT-2 and T5 for spoken dialogues.

- Spoken dialogues - The focus is on evaluating LLMs on spoken task-oriented dialogues rather than written dialogues. Key properties of spoken dialogues are discussed.

- Robustness - The paper investigates if LLMs are inherently robust to properties of spoken dialogues like speech errors and disfluencies. 

- Automatic speech recognition (ASR) errors - ASR transcriptions of spoken dialogues are analyzed to characterize errors. These errors are simulated in written dialogues for model training.

- Dialogue state tracking - One subtask used to evaluate model robustness is tracking dialogue states. The Joint Goal Accuracy metric is used.

- Response generation - The other subtask for evaluation is end-to-end response generation given context. Metrics like perplexity and human evaluations are used.

- Fine-tuning - Models pre-trained on written text are fine-tuned on spoken dialogues to evaluate if it improves robustness.

So in summary, the key themes are evaluating LLM robustness for spoken dialogues, using ASR error simulation, and fine-tuning methods to potentially improve performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 suggested in-depth questions about the method proposed in this paper:

1. The paper mentions developing a methodology to replicate ASR error patterns observed in a small dataset of spoken dialogues. Can you elaborate more on this methodology? How exactly did you categorize and quantify the error patterns?

2. You evaluated two subtasks - dialogue state tracking and response generation. What motivated the choice of these two subtasks to study robustness of LLMs for spoken dialogues? Would the findings extend to other dialogue tasks like dialogue act prediction? 

3. For dialogue state tracking, introducing noise on slot values specifically showed a slight increase in performance compared to just overall noisy dialogues. Why do think targeting noise towards slots had this effect? Does it suggest slot values are more vulnerable in spoken dialogues?

4. The human evaluation results highlight considerable improvement from noisy fine-tuning for appropriateness and contextualization. What error patterns were reduced that led to this? Can you share some dialogue examples demonstrating this?

5. You studied the robustness of GPT-2 and T5 models. How would transformer architectures like BART, BlenderBot or even non-transformer models compare? Would the overall conclusions still hold?

6. The choice of test sets seems crucial - Human Verbatim vs Human Paraphrased. What were the key differences you observed between these test sets and the challenges they pose?

7. You automatically aligned ASR transcriptions with ground truth to categorize errors. What alignment techniques did you use? What other semi-automated ways could help speed up annotation for a spoken dialogue dataset?  

8. Why do you think Joint Goal Accuracy showed contradicting results to human evaluation for state tracking? Could metrics be improved or adapted to better capture robustness?

9. How difficult was collecting multi-annotations for human evaluation? Any insights on improving agreement rates? Were crowdworkers specifically trained?

10. Do you think findings would hold up on full spoken dialogues instead of just spoken user turns? What new challenges might arise on handling ASR errors on system turns?
