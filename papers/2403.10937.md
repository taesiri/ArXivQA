# [Initial Decoding with Minimally Augmented Language Model for Improved   Lattice Rescoring in Low Resource ASR](https://arxiv.org/abs/2403.10937)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Low resource languages like Telugu and Kannada have high out-of-vocabulary (OOV) rates due to limited training data, leading to high word error rates (WER) in automatic speech recognition (ASR) systems.
- Rescoring lattices generated from small baseline language models (LMs) with larger LMs gives marginal gains as lattices may not contain all probable words earlier missing.
- Building decoding graphs with large LMs has high memory needs, not feasible for low resource setups.

Proposed Solution:
- Augment baseline LM with unigram counts of out-of-train (OOT) words from larger text corpus before initial decoding. 
- This "minimally augmented" LM helps generate more comprehensive lattices containing earlier OOV words.
- Rescore these lattices with larger LM (e.g. trained on full Wikipedia text) to significantly reduce WER.

Contributions:
- Proposed computationally inexpensive method to leverage available large text corpora for low resource ASR.
- Achieves 21.8% (Telugu) and 41.8% (Kannada) relative WER reduction, comparable to decoding with full Wikipedia LM.
- Consumes only 1/8th the memory of full Wikipedia LM decoding.
- Applicable across different text selection methods for LM augmentation and data subsets of varying sizes.
- Eliminates need to empirically determine amount of text selection.
- Improves OOV recovery and overall ASR accuracy.

In summary, the paper proposes an effective and efficient method to build ASR systems for low resource languages by minimally augmenting baseline LMs before decoding to generate better lattices that give significant WER reduction when rescored with larger LMs.
