# [Zero-shot Generative Large Language Models for Systematic Review   Screening Automation](https://arxiv.org/abs/2401.06320)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Systematic reviews are crucial in evidence-based medicine to analyze published research on specific questions, but conducting them is resource- and time-intensive, especially the screening phase where candidate documents are assessed for inclusion. 

- Methods have been proposed to assist with optimizing systematic review creation, but there is interest in minimizing the number of documents needing manual screening.

Solution:
- This paper investigates using zero-shot large language models (LLMs) for automatic screening of documents for inclusion in systematic reviews. 

- Two approaches are examined - an uncalibrated one that compares yes/no token probabilities from the LLM to decide on inclusion, and a calibrated one that introduces a threshold for the difference in probabilities to determine inclusion, calibrated to target a minimum recall level.

- Ensembling of screening decisions from top LLMs and a BERT-based baseline is also explored.

Key Contributions:
- Evaluation of 8 different generative LLM architectures in zero-shot setup for document screening, highlighting best current model - LlaMa2-7b-ins.

- Demonstration that instruction-based fine-tuning consistently improves effectiveness over base models.

- Introduction and evaluation of calibrated screening method that reliably achieves target recall levels.

- Finding that ensemble of methods outperforms individual models and is competitive with state-of-the-art fine-tuned baseline while saving labeling effort.

Overall, the paper shows promise for practical use of zero-shot LLMs to significantly reduce manual screening burden in systematic reviews while maintaining high recall. Calibrated ensembling of models is an effective approach.


## Summarize the paper in one sentence.

 This paper evaluates the effectiveness of zero-shot large language models and ensembling for automatic document screening in systematic reviews, finding that proper calibration can reliably achieve high recall targets while significantly reducing screening workload.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing and evaluating a zero-shot approach using generative large language models (LLMs) for automatic document screening in systematic reviews. Specifically, the key contributions are:

1) Investigating the effectiveness of various zero-shot LLMs, including different architectures (LlaMa, Alpaca, Guanaco, Falcon) and sizes (7B to 13B parameters), for the screening task without requiring any task-specific tuning.

2) Introducing a calibration technique to tune the LLM's output decision threshold based on a target recall level. This allows flexibility in meeting review-specific needs and quality standards.

3) Demonstrating that an ensemble of the top zero-shot LLMs combined with a strong baseline model (BioBERT) outperforms current state-of-the-art fine-tuned methods after calibration.

4) Showing that zero-shot LLMs can reliably achieve high recall targets, saving significant manual screening effort. The best LLM ensemble obtains over 90% work savings on multiple datasets while still capturing 95% of relevant documents.

In summary, the key innovation is leveraging recent advances in generative LLMs to automate the important but laborious document screening phase in conducting systematic reviews, without expensive fine-tuning. The introduced calibration technique also makes this approach practical.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the main keywords and key terms associated with this paper include:

- Systematic review 
- Document screening
- Large language models (LLMs)
- Zero-shot learning
- Instruction fine-tuning
- Calibration 
- Ensemble methods
- Recall
- Effectiveness
- Automation

The paper focuses on using zero-shot large language models for automating the document screening process in systematic reviews. It evaluates different LLMs, with and without instruction fine-tuning, on their effectiveness at this task. The paper also introduces a calibration technique to tune the models' output to target a specified recall level. Finally, it explores ensembling LLMs with existing methods to further improve screening effectiveness. The key goal is developing methods to automate screening to save the significant human effort this normally requires when conducting systematic reviews.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes both an uncalibrated and a calibrated setting for using generative language models for systematic review screening. What are the key differences between these two settings and what is the rationale behind exploring a calibrated approach?

2. When determining the decision threshold Theta in the calibrated setting, two approaches are introduced: extrapolation from collection and calibration with seed studies. What are the relative merits and limitations of each approach? When would you recommend using one vs the other?  

3. Ensemble methods are explored by combining predictions from multiple models. What is the rationale behind ensembling in this application and what specific combination of models is used? How does the ensemble approach compare to using predictions from a single top-performing model?

4. Both model architecture and model size are evaluated as factors influencing effectiveness. What are the key tradeoffs observed between model architecture and model size? Which factor seems to have a greater influence on performance?

5. Instruction-based fine-tuning is compared against base LLMs without fine-tuning. What trends are observed regarding the impact of fine-tuning on metrics like recall and work saved over sampling? Are there differences depending on the base LLM architecture?  

6. The calibrated LLM approach is compared to a supervised fine-tuned baseline, Bio-SIEVE. How does the performance compare, especially with respect to meeting a minimum recall threshold? What does this suggest about the practical utility of each method?

7. Besides the metrics reported in the paper like recall and balanced accuracy, what other evaluation metrics would be informative to assess for this task? What limitations exist with the current set of metrics being used?

8. How sensitive is the method likely to be to the specific prompt formulation provided as input to the LLMs? What steps could be taken to design optimal prompts and account for variation across topics? 

9. What factors likely contribute to the volatility in performance observed across systematic review topics when using the LLMs? How can this volatility be accounted for if deploying such models in practice?

10. What additional experiments or analyses would help further validate the real-world viability of using zero-shot LLMs for systematic review screening automation? What are the next steps needed before translation into practice?
