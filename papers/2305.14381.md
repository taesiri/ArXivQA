# [Connecting Multi-modal Contrastive Representations](https://arxiv.org/abs/2305.14381)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis appears to be:

Can we learn robust multi-modal contrastive representations between modalities with limited paired data by connecting and transferring knowledge from existing multi-modal contrastive representation (MCR) models pre-trained on other modality pairs with abundant data?

The key hypotheses seem to be:

1) Existing MCR models (like CLIP for image-text and CLAP for audio-text) encode strong semantic alignment between their respective modalities that can be transferred.

2) By projecting representations of an overlapping modality (text) from two MCRs into a shared space, we can align them based just on that overlapping modality. 

3) The alignment between MCRs established via the overlapping modality can transfer to the non-overlapping modalities (image and audio in this case).

So in summary, the central hypothesis is that we can learn an effective image-audio multi-modal contrastive representation without direct paired supervision by connecting and transferring knowledge from CLIP and CLAP models through their overlapping text modality.
