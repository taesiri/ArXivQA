# [Fast Model Editing at Scale](https://arxiv.org/abs/2110.11309)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we efficiently edit the behavior of very large pre-trained neural network models using only a single example of a desired model change?The paper proposes an approach called Model Editor Networks with Gradient Decomposition (MEND) to address this question. The key ideas appear to be:- Editing should modify a pre-trained model's behavior for a specific input, but leave the model's behavior on other unrelated inputs unchanged. This is referred to as making "local" edits.- Existing approaches like fine-tuning or prior meta-learning methods are inefficient or ineffective for very large models with billions of parameters. - MEND learns a collection of small "model editor networks" that transform the standard fine-tuning gradient for a given edit into a better update.- By factorizing the fine-tuning gradient into low rank components, MEND makes this gradient transformation computationally feasible even for huge models.So in summary, the central hypothesis seems to be that MEND can enable fast, localized editing of very large pre-trained models using only a single input-output example, which is not possible with prior techniques. Experiments test this hypothesis on models like GPT-Neo, GPT-J, T5-XL, and T5-XXL.
