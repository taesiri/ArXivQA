# [Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in   Entropy Minimization](https://arxiv.org/abs/2308.06879)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question/hypothesis of this paper seems to be: How can test-time adaptation (TTA) models be improved to better handle noisy signals originating from incorrect or open-set predictions, in order to enable more stable long-term adaptation?The key points are:- Existing TTA methods that rely on the model's own predictions as targets (e.g. entropy minimization) are prone to utilizing noisy signals from incorrect or open-set predictions. This hampers long-term stable adaptation.- The authors propose a simple yet effective sample selection method to filter out such noisy samples, using the confidence difference between the original model and adapted model. - They find that noisy signals tend to show decreased confidence on originally predicted labels compared to correct signals, due to misalignment with "wisdom of crowds".- Their proposed method consistently improves existing TTA methods on image classification and segmentation, especially for long-term adaptation and open-set scenarios.So in summary, the main hypothesis is that filtering noisy samples using confidence difference can significantly improve existing TTA methods to enable more robust and stable adaptation. Evaluating this on various TTA methods and tasks is the key contribution.


## What is the main contribution of this paper?

The main contribution of this paper appears to be proposing a simple yet effective sample selection method for test-time adaptation that filters out noisy samples using the confidence difference between the original model and the adapted model. The key points are:- They make an empirical observation that with entropy minimization, noisy samples including wrong predictions and open-set samples tend to show decreased confidence values on the originally predicted class from the original model to the adapted model. - They propose to use the confidence difference to filter out such noisy samples. Samples achieving lower confidence on the originally predicted class with the adapted model compared to the original model are discarded.- This method is widely applicable to various existing test-time adaptation methods like ENT, TENT, EATA etc. and improves their adaptation performance.- They propose a new evaluation setting called open-set test-time adaptation which includes unknown classes not seen during training. Most prior work focused on closed-set adaptation.- The proposed method improves performance on both closed-set and open-set test-time adaptation, for both image classification and semantic segmentation tasks, demonstrating its wide applicability.In summary, the key contribution is a simple yet effective sample selection technique to filter noisy samples in test-time adaptation using confidence difference, which also enables open-set adaptation. The wide applicability to existing methods and gains across tasks are also important outcomes.
