# [Nonparametric Variational Regularisation of Pretrained Transformers](https://arxiv.org/abs/2312.00662)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a method to reinterpret pretrained Transformer models like BERT as nonparametric variational Bayesian models using a novel identity initialization of Nonparametric Variational Information Bottleneck (NVIB) layers. The identity initialization achieves empirical equivalence with the original Transformer while allowing the introduction of uncertainty into the latent representations. The paper also contributes a data-efficient empirical prior estimated from the model's embeddings. Increasing the uncertainty post-training acts as an information-theoretic regularizer without additional training. Experiments on out-of-domain text summarization tasks demonstrate that this Bayesian reinterpretation and post-training regularization can improve generalization. For example, the reinterpreted model summarizing Wikipedia articles achieves higher Rouge scores compared to the original fine-tuned BERT. The success of this variational Bayesian framework provides insights into how pretrained Transformers represent information in their embeddings. This interpretation and regularization technique may enable better understanding and development of Transformer architectures.
