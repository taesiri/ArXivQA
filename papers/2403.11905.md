# [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Humans spend a lot of time interacting with web pages to accomplish tasks, but AI systems still struggle with complex, multi-modal web-based reasoning. 
- There is a lack of standardized benchmarks to evaluate web agents' ability to understand instructions embedded in web pages and interactively complete tasks by filling out forms.

Proposed Solution:
- Introduce a new benchmark called TurkingBench containing over 32K instances across 158 real-world crowdsourcing tasks formatted as complex web pages.
- Tasks require understanding multi-modal context like text, tables, images to follow the instructions on filling out forms. 
- Implement an evaluation framework to connect model responses to modifications of the web pages.

Key Contributions:
- Release diverse benchmark of complex web tasks requiring interactive reasoning.
- Data comes from real Amazon Mechanical Turk tasks designed for human workers. 
- Introduce standard evaluation protocol for models that interact with web forms.
- Benchmark state-of-the-art models like GPT-4 and Claude and analyze performance - models significantly outperform random but still far below human ceiling.
- Identify key challenges like multi-modality, long context, interaction as areas for improvement.

In summary, the paper presents a novel benchmark to measure progress in web-based agents that can follow instructions situated in complex web pages and highlights current limitations of state-of-the-art models, motivating further research into assistive, interactive AI systems.
