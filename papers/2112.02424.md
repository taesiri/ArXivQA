# [Variational Wasserstein gradient flow](https://arxiv.org/abs/2112.02424)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a numerical algorithm to compute Wasserstein gradient flows for objectives expressed as f-divergences. The key ideas are:1. Using a variational formulation of the f-divergence objectives that allows evaluating them with samples without density estimation. 2. Applying the JKO scheme with neural network parametrization of the transport map. This casts each JKO step as a min-max stochastic optimization problem.3. Theoretical analysis showing the variational objective satisfies certain properties like moment matching.4. Demonstrating the effectiveness and scalability of the proposed method on sampling tasks, PDEs, Bayesian inference, and image generation compared to prior works. The central hypothesis is that the variational formulation of f-divergences combined with neural network parametrization can lead to an effective and scalable algorithm for Wasserstein gradient flows applicable to high-dimensional problems. The paper provides empirical evidence through experiments and some theoretical justification to support this hypothesis.


## What is the main contribution of this paper?

This paper proposes a scalable numerical algorithm to implement the Wasserstein gradient flow for objective functions that can be expressed as an f-divergence. The key ideas are:1. Utilizing a variational form of the f-divergence objective, which allows evaluating it using only samples without density estimation. 2. Applying the JKO scheme on the variational objective and reformulating each step as a min-max optimization over a transport map and a dual function parameterized by neural networks.3. Theoretical analysis showing the variational objective satisfies certain properties like moment matching.4. Demonstrating the effectiveness of the proposed method on high-dimensional sampling problems and image generation tasks. The main advantage compared to prior works is avoiding density estimation and approximating log-determinant of Hessians, making the algorithm scalable. Overall, it provides a practical way to realize Wasserstein gradient flows for a class of objectives.
