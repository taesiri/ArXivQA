# [Variational Wasserstein gradient flow](https://arxiv.org/abs/2112.02424)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a numerical algorithm to compute Wasserstein gradient flows for objectives expressed as f-divergences. The key ideas are:1. Using a variational formulation of the f-divergence objectives that allows evaluating them with samples without density estimation. 2. Applying the JKO scheme with neural network parametrization of the transport map. This casts each JKO step as a min-max stochastic optimization problem.3. Theoretical analysis showing the variational objective satisfies certain properties like moment matching.4. Demonstrating the effectiveness and scalability of the proposed method on sampling tasks, PDEs, Bayesian inference, and image generation compared to prior works. The central hypothesis is that the variational formulation of f-divergences combined with neural network parametrization can lead to an effective and scalable algorithm for Wasserstein gradient flows applicable to high-dimensional problems. The paper provides empirical evidence through experiments and some theoretical justification to support this hypothesis.


## What is the main contribution of this paper?

This paper proposes a scalable numerical algorithm to implement the Wasserstein gradient flow for objective functions that can be expressed as an f-divergence. The key ideas are:1. Utilizing a variational form of the f-divergence objective, which allows evaluating it using only samples without density estimation. 2. Applying the JKO scheme on the variational objective and reformulating each step as a min-max optimization over a transport map and a dual function parameterized by neural networks.3. Theoretical analysis showing the variational objective satisfies certain properties like moment matching.4. Demonstrating the effectiveness of the proposed method on high-dimensional sampling problems and image generation tasks. The main advantage compared to prior works is avoiding density estimation and approximating log-determinant of Hessians, making the algorithm scalable. Overall, it provides a practical way to realize Wasserstein gradient flows for a class of objectives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a scalable method to numerically implement Wasserstein gradient flows for objective functions in the form of f-divergences, using a variational reformulation of the objective within a primal-dual optimization framework.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work:- The paper proposes a numerical algorithm for computing Wasserstein gradient flows, which model gradient dynamics on the space of probability densities with respect to the Wasserstein metric. This is an active area of research with applications in PDEs, sampling, optimal transport, and other fields.- Most prior work on numerically computing Wasserstein gradient flows relies on finite difference methods applied to the PDE representation. These methods require discretizing the underlying space and have computational complexity that scales poorly to high dimensions. - Recent neural network-based methods like JKO-ICNN utilize the JKO scheme with input convex neural networks to avoid spatial discretization. However, they still have limitations such as needing to estimate density for entropy terms or compute determinants of Hessians which scale cubically with dimension.- This paper proposes a novel variational formulation of the objective function that avoids density estimation and determinant calculations. By optimizing over a parameterized class of test functions, the objectives can be approximated using only samples.- The proposed primal-dual optimization algorithm to implement the JKO steps has complexity independent of dimension, unlike prior methods.- Experiments show the approach can scale to sample from high-dimensional distributions and generate complex image datasets like CIFAR-10. Comparisons illustrate advantages over JKO-ICNN in computation time and stability.In summary, the key novelty is the variational formulation that leads to a scalable primal-dual algorithm for Wasserstein gradient flows applicable to a range of problems. The experiments validate the effectiveness for high-dimensional tasks compared to state-of-the-art methods.
