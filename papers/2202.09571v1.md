# [Bit-wise Training of Neural Network Weights](https://arxiv.org/abs/2202.09571v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

Why doesn't gradient descent naturally prune neural network weights to zero during training?

The authors hypothesize that an important factor is the over-parametrization in terms of the number of bits used to represent the weights. Weights are usually represented with 32-bit floating point values, making it very unlikely for weights to be pruned to exact zero. 

To test this hypothesis, the authors develop an algorithm to train individual bits representing the weights rather than the full floating point values. This allows them to train networks with lower bit depths, which increases the chance weights will be pruned to zero during training. They then analyze the relationship between bit depth, sparsity, and accuracy.

In essence, the main research question is why neural network pruning is not naturally uncovered by standard training, and the central hypothesis is that the high bit-depth for representing weights is a key factor preventing this. The bit-wise training algorithm is introduced to test this hypothesis.
