# [Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as   an Alternative to Attention Layers in Transformers](https://arxiv.org/abs/2311.10642)

## Summarize the paper in one sentence.

 The paper explores replacing attention mechanisms in Transformers with shallow feedforward networks trained via knowledge distillation, finding they can emulate attention well for self-attention but struggle with cross-attention.
