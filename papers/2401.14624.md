# [Query of CC: Unearthing Large Scale Domain-Specific Knowledge from   Public Corpora](https://arxiv.org/abs/2401.14624)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of open-source large language models (LLMs) and data tailored for specific domains, which limits development of domain-specific LLMs. 
- Existing methods rely on manual data collection which is time-consuming and labor-intensive. It is also challenging to collect diverse and scalable domain-specific data.

Proposed Solution:
- The authors propose an automated data collection framework called "Query of CC" (\querycc) to gather domain-specific data from public corpora.  
- It has two main stages:
   1) Query Bootstrapping: Uses an LLM to expand seed keywords into diverse queries covering the breadth and depth of the target domain.
   2) Data Retrieval: Retrieves documents from public corpora relevant to the queries using BM25 algorithm.
- Using this framework, the authors collect a 300B-tokens dataset called "\knowledgepile" spanning STEM, humanities, social sciences and medicine.

Contributions:
- Proposes an efficient automated framework \querycc to collect domain-specific data and knowledge at scale from public corpora.
- Releases \knowledgepile, a 300B high-quality open dataset to enhance LLMs' capabilities in mathematical and knowledge-based reasoning.  
- Shows LLMs enhanced with \knowledgepile (Llama2-QoC, Mistral-QoC) achieve significant performance gains on benchmarks like MATH, GSM8K, MMLU, AGIEval and BIG-Bench compared to baseline models.
- The framework and dataset aim to provide valuable support for research into domain-specific LLMs.

In summary, the paper offers an automated data collection solution to obtain domain-specific data at scale for training LLMs. The released dataset enhances performance on reasoning tasks. The framework and dataset facilitate research into specialized LLMs.


## Summarize the paper in one sentence.

 The paper proposes an efficient method called Query of CC (QoC) to automatically collect high-quality domain-specific data from public corpora, and uses this data to train enhanced versions of Llama and Mistral models that demonstrate significant improvements on mathematical and knowledge-based reasoning tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, I would summarize the main contributions as:

1. Proposing \querycc, an efficient self-bootstrapping strategy for automatically collecting domain-specific data from public corpora. This involves using a large language model to expand seed queries in breadth (question extension) and depth (thought generation) to retrieve relevant data. 

2. Collecting and releasing \knowledgepile, a high-quality knowledge dataset spanning STEM, humanities, social sciences, and other domains. The 300B token dataset is shown to significantly enhance language models' capabilities in mathematical and knowledge-based reasoning tasks.

3. Introducing the Llama-QoC and Mistral-QoC models trained on \knowledgepile. These models demonstrate substantial improvements over baselines on benchmarks like MATH, GSM8K, MMLU, AGIEval, and BIG-Bench Hard.

In summary, the main contributions are: (1) an efficient automated data collection strategy, (2) a new high-quality knowledge dataset, and (3) enhanced language models trained on this dataset that show significant performance gains on reasoning tasks. The goal is to provide better methods for knowledge accumulation and to advance language models' reasoning abilities.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Query of CC (QueryCC): The proposed method for automatically collecting domain-specific data by querying large language models and retrieving relevant data from public corpora.

- Knowledge Pile: The name of the high-quality dataset collected using the Query of CC method, spanning domains like STEM, humanities, social sciences, etc. 

- Query Bootstrapping: A key component of QueryCC that expands queries in breadth and depth using question extension and thought generation with large language models.

- Data Retrieval: The other major component of QueryCC that retrieves relevant documents from public corpora using the bootstrapped queries and BM25 algorithm. 

- Llama2-QoC and Mistral-QoC: Variants of Llama2 and Mistral models further pretrained on the Knowledge Pile dataset.

- Mathematical reasoning: A key application area of the Knowledge Pile dataset to enhance mathematical reasoning abilities of models.

- Knowledge-based reasoning: Another major set of capabilities improved in models like Llama2-QoC and Mistral-QoC after training on the Knowledge Pile dataset.

Some other terms include seed information, query analysis, data statistics, model training, evaluation benchmarks like MATH, GSM8K, MMLU, AGIEval, etc.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. How does the Query Bootstrapping stage work to extend the initial seed queries both in terms of breadth (Question Extension) and depth (Thought Generation)? What are the specific techniques used?

2. What is the motivation behind using both Question Extension and Thought Generation? How do they complement each other in expanding the scope and depth of the queries? 

3. What post-processing operations are conducted on the generated queries before they enter the seed pool and final query pool? Why are these cleaning and filtering steps important?

4. What is the advantage of using BM25 for document retrieval over other relevance scoring methods like Dense Retriever? What efficiency vs accuracy trade-off does BM25 present?

5. What types of filtering and deduplication techniques are used in the Data Post-processing stage? Why are these steps critical for ensuring data quality? 

6. How does the distribution of queries used to collect Knowledge Pile differ across the categories like STEM, humanities etc. as shown in Figure 3? What inferences can be drawn from this?

7. Analyze the top web domains contributing to Knowledge Pile as listed in Table 2. What does this distribution indicate about the data sources?

8. Why does Mistral-QoC exhibit a more pronounced performance gain on the MMLU-STEM category versus other MMLU categories as observed? Provide possible explanations.

9. Compare and contrast the behavior of Llama2-QoC versus Mistral-QoC during continued pretraining on Knowledge Pile based on Figure 5. What accounts for the differences?

10. How scalable is the proposed automated data collection framework for expanding into other specialized domains beyond the ones covered currently? Discuss the possibilities and challenges.
