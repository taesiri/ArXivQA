# [VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning](https://arxiv.org/abs/2312.08774)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper proposes a novel Visual-Spatial Fusion Transformer (VSFormer) for correspondence pruning in relative camera pose estimation. The key idea is to introduce visual cues of an image pair, representing the abstract inlier ratio, to guide pruning. The VSFormer extracts visual cues using cross-attention between local image features. It then fuses the visual and spatial correspondence cues using a joint fusion module involving projection, transformer modeling, and summation. To further capture correspondence consistency, a Context Transformer is proposed to explicitly model local context via a graph attention block on a KNN graph, and global context using a transformer. Comparative and ablation studies on indoor and outdoor datasets demonstrate state-of-the-art performance, with gains of up to 15.79% in pose accuracy over current approaches. The visual-spatial fusion and transformer modeling effectively exploit scene priors and contextual information to improve correspondence discrimination, especially for challenging cases, while maintaining efficiency.
