# [VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning](https://arxiv.org/abs/2312.08774)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper proposes a novel Visual-Spatial Fusion Transformer (VSFormer) for correspondence pruning in relative camera pose estimation. The key idea is to introduce visual cues of an image pair, representing the abstract inlier ratio, to guide pruning. The VSFormer extracts visual cues using cross-attention between local image features. It then fuses the visual and spatial correspondence cues using a joint fusion module involving projection, transformer modeling, and summation. To further capture correspondence consistency, a Context Transformer is proposed to explicitly model local context via a graph attention block on a KNN graph, and global context using a transformer. Comparative and ablation studies on indoor and outdoor datasets demonstrate state-of-the-art performance, with gains of up to 15.79% in pose accuracy over current approaches. The visual-spatial fusion and transformer modeling effectively exploit scene priors and contextual information to improve correspondence discrimination, especially for challenging cases, while maintaining efficiency.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of correspondence pruning, which aims to identify correct feature matches (inliers) from an initial set of putative correspondences between two images. This is a fundamental task for many computer vision applications like SLAM and 3D reconstruction. The key challenge is handling varying inlier ratios across scenes due to differences in texture, illumination, occlusion etc. Existing methods lack a scene-level visual perception to guide the pruning process.

Proposed Solution - Visual-Spatial Fusion Transformer (VSFormer):

1. Visual Cues Extractor: Extracts scene-level visual cues by applying cross-attention between local features of the two view images. Provides an abstract representation of scene properties like textures and illumination.

2. Visual-Spatial Fusion Module: Models relationship between visual and spatial cues using a transformer, and fuses them through summation after separate encodings. Helps embed scene visual cues into each correspondence to guide pruning.  

3. Context Transformer: Stacks graph neural network and transformer to explicitly capture both local neighbourhood and global contexts of correspondences. Also proposes a graph attention block to enhance representation ability of the KNN graph.

Main Contributions:

- First work to introduce scene-level visual cues to guide correspondence pruning 
- Novel visual-spatial fusion module to jointly model and embed scene cues into correspondences
- Context transformer with graph attention block to effectively combine local and global context
- Outperforms state-of-the-art on both outdoor and indoor benchmarks. Achieves precision improvement of 15.79% on outdoor and 4.45% on indoor scenes.

In summary, the paper proposes a Visual-Spatial Fusion Transformer to leverage scene-level visual cues along with local and global correspondence contexts to significantly improve correspondence pruning performance. The extensive experiments demonstrate state-of-the-art results on challenging outdoor and indoor benchmarks.
