# [Do Large Language Model Understand Multi-Intent Spoken Language ?](https://arxiv.org/abs/2403.04481)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Multi-intent spoken language understanding (SLU) is challenging but critical for practical dialog systems. Existing models have limitations in handling the complexity of multi-intent utterances.  

- The potential of large language models (LLMs) is unexplored in multi-intent SLU. Integrating LLMs is non-trivial due to uncontrolled generation length and error propagation in sequence tagging.

Proposed Solution:
- Propose using LLMs for multi-intent SLU by:
    - Reconfiguring entity slots specifically for LLM application
    - Introducing concept of Sub-Intent Instruction (SII) to enhance dissection of multi-intent utterances
    - Conducting supervised fine-tuning of LLMs using task-specific labeled data
- Employ efficient fine-tuning with Quantized Low-Rank Adapters (QLoRA) for scalability
- Inference procedure with parsing and transformation to ensure precise intent and slot prediction

Main Contributions:
- Pioneer application of LLMs for multi-intent SLU with tailored methodology
- Craft entity slots and sub-intent instructions for LLM datasets LM-MixATIS and LM-MixSNIPS  
- Demonstrate competitiveness (if not superiority) over state-of-the-art models
- Introduce novel evaluation metrics: Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA)

The paper makes notable advancements in modeling, data, performance and metrics for enabling LLMs in the complex domain of multi-intent SLU.
