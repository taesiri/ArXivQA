# [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- In-context learning (ICL), where LLMs are adapted to downstream tasks using a few input-output examples, has become very popular. However, existing approaches only learn from the correct examples. 
- In contrast, humans learn effectively from mistakes. Further, humans can verbalize principles from mistakes to avoid them.

Proposed Solution: 
- The paper proposes LEAP (Learning Principles from Mistakes), a novel ICL approach. 
- First, LEAP intentionally induces mistakes by the LLM on the given examples. 
- Next, LEAP asks the LLM to reflect on its own mistakes and articulate corrective principles.
- Finally, LEAP prompts the LLM to solve new test questions using the original examples and learned principles.

Main Contributions:
- Proposes a new ICL paradigm - learning more from few given examples by inducing and learning from mistakes
- Generates explicit task-specific principles from mistakes in a self-supervised way
- Evaluated on HotpotQA, DROP, GSM8K, MATH and 27 BigBench tasks
- Outperforms standard few-shot prompting of GPT-3.5 Turbo to GPT-4 across most benchmarks
- Shows principles can generalize to new test questions not used to derive principles
- Demonstrates new abilities of LLMs - structured self-reflection and articulation

In summary, the paper introduces an innovative approach called LEAP that allows LLMs to learn corrective principles from their own mistakes on a few examples, and leverage the principles to improve accuracy on new test questions. It outperforms standard few-shot prompting on several reasoning tasks.
