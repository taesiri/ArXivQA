# [cantnlp@LT-EDI-2024: Automatic Detection of Anti-LGBTQ+ Hate Speech in   Under-resourced Languages](https://arxiv.org/abs/2401.15777)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
The paper addresses the problem of developing systems to automatically detect anti-LGBTQ+ hate speech in comments on social media, specifically focusing on 10 under-resourced language varieties - English, Spanish, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, Tulu, and Telugu. The data has class imbalance issues, with a lack of homophobic/transphobic examples in some languages. There is also code-mixing with Latin script in many languages.

Proposed Solution:
The authors propose using an XLM-Roberta transformer model as the base, then conducting domain adaptation by retraining the model on additional appropriate data featuring synthetic and organic script mixing. They then fine-tune a classification model on the domain-adapted transformer to classify social media comments. They evaluate language-specific and multilingual variants.

Key Contributions:
- Show that adding synthetic and organic script-mixed data during domain adaptation improves performance over the base model in 7/10 languages, indicating the value of adding this type of data.
- Compare synthetic and organic script mixing for domain adaptation. Organic gave better improvements overall.  
- Develop language-specific and multilingual classification models after domain adaptation. Multilingual model had higher average F1-score but by a small margin.
- Address highly imbalanced and low-resource language varieties in this task.
- Discuss insights around variation in performance across language varieties in relation to data characteristics like size and balance.

In summary, the key novelty is in handling low resource and imbalanced language varieties by using script-mixed data for domain adaptation of transformer models to improve hate speech detection performance.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper describes a transformer-based multiclass classification system to detect anti-LGBTQ+ hate speech in social media comments across 10 languages, using synthetic and organic script-switching data to improve performance for under-resourced languages.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

"The main contribution of the current paper is the proposal to use synthetic and organic script-switching examples of during domain adaptation to improve the down-stream performance for under-resourced languages. We demonstrated that our methodology improved the model performance for \textsc{guj}, \textsc{kan}, \textsc{mal}, \textsc{mar}, and \textsc{tam} even though the improvement was only marginal."

So in summary, the main contribution is using examples of script-switching (mixing scripts within the same text) in the training data to improve performance of models for detecting anti-LGBTQ speech in under-resourced languages. They showed this helped for several Indian languages like Gujarati, Kannada, Malayalam, Marathi and Tamil.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Homophobia/transphobia detection
- Social media comments
- Multiclass classification 
- Under-resourced languages
- Indo-Aryan languages
- Dravidian languages
- Script-switching
- Domain adaptation
- Transformer models
- XLM-RoBERTa
- Synthetic data
- Organic data
- Paralinguistic information

The paper describes the development of a system for detecting homophobia and transphobia in social media comments across multiple under-resourced language varieties. It utilizes transformer models like XLM-RoBERTa and incorporates synthetic and organic script-switching data during domain adaptation. The languages focused on are Indo-Aryan (English, Spanish, Gujarati, Hindi, Marathi) and Dravidian (Kannada, Malayalam, Tamil, Tulu, Telugu). Key goals are handling multiclass classification and leveraging paralinguistic information like code-switching to improve performance, especially for lower-resource languages.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using synthetic and organic script-switching during domain adaptation. Can you explain in more detail how the synthetic and organic script-switching data was generated? What were the differences in approach?

2. Figure 1 shows the amount of labeled training data available for each language. How might the relative sizes of the training sets impact model performance across languages? Does the paper take any steps to address potential issues caused by differing sized training sets?

3. Table 2 shows the class distribution is highly imbalanced for some languages like English and Hindi. What impact could this imbalance have on model training and performance? Does the paper attempt to address the class imbalance?

4. For the synthetic script-switching data, Wikipedia articles were randomly sampled and transliterated. What potential issues could this simple approach cause in terms of realistically modelling code-switching? How could the synthetic data generation be improved?

5. What specifically does Figure 3 show regarding script-switching rates in the training data? Why is accounting for varying script-switching rates important? 

6. The organic script-switching data relies on social media data from the Global Corpus of Language Use. What potential biases could this external dataset introduce? How does geographic bias in social media impact code-switching rates?

7. Table 3 shows introducing script-switching helps most languages but hurts Telugu performance. Why might the impact be different across languages? How could this be analyzed further?  

8. For languages like English, Spanish and Tulu, script-switching data did not help. Why would low script-switching languages see no benefit? What improvements could be made?

9. The results show the approach works well for some languages but not as well for others. What sociolinguistic frameworks could help better contextualize when the approach is applicable? 

10. The paper acknowledges limitations around biases in the training data and PLMs. What steps could be taken to quantify these biases and their impact on model development and performance?
