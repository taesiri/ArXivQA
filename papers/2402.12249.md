# [Analysis of Levenshtein Transformer's Decoder and Its Variants](https://arxiv.org/abs/2402.12249)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper analyzes the quality of translations from the Levenshtein Transformer (LevT) machine translation model and its variants. Specifically, it looks at potential deficiencies in LevT's iterative refinement procedure during decoding to identify areas for future improvement. 

Methods:
The authors compare translations from the original LevT model, a knowledge-distilled LevT model, a LevT model trained with translation memory, and a knowledge-distilled LevT model trained with translation memory. They analyze the translations across three key aspects:

1. Length prediction 
2. Subword versus complete word generation
3. Capabilities of the deletion module

Key Findings:

Length:
- The original LevT model tends to produce shorter translations due to underestimating length in the first decoding iteration. Using an external length predictor helps alleviate this issue.

Subwords vs words: 
- All models generate fewer subwords compared to references. The original LevT performs worse at predicting subwords but better for complete words.
- All models achieve high precision for word prediction in the first iteration, but original LevT degrades more with missing subwords or when decoding from scratch.

Deletion module:
- For original and distilled LevT, the deletion module acts primarily as a language model, not considering the source sentence. The translation memory-enhanced LevT does consider the source.
- All models are confident in theirdeletion decisions, especially the knowledge distilled ones.

Proposed Improvements:
- Add specialized length predictor
- Additional training on decoding from scratch 
- Alternative subword tokenization
- Improve cooperation between deletion and token prediction modules
