# [Distilling Reinforcement Learning Policies for Interpretable Robot   Locomotion: Gradient Boosting Machines and Symbolic Regression](https://arxiv.org/abs/2403.14328)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent advances in reinforcement learning (RL) have enabled remarkable progress in robot locomotion capabilities. However, the complexity and "black box" nature of neural network policies hinder their interpretability and acceptance, especially in applications demanding safety and reliability. There is a need to transform opaque neural policies into transparent "glass box" models.

Proposed Solution: 
The paper introduces a novel framework to distill neural RL locomotion policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs), and Symbolic Regression. Leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, opaque neural policies are transformed into transparent "glass box" models.

The key aspects are:

1) Train expert neural network policies with RL.
2) Distill policies into GBMs, EBMs, and symbolic expressions using a modified Dataset Aggregation (DAgger) method with curriculum policy alternation.
3) Evaluate performance and interpretability of distilled policies on various locomotion gaits.

Main Contributions:

1) A policy distillation framework with episode-dependent alternation incorporated into DAgger for efficient distillation of feedback control policies.

2) Effective locomotion policies distilled via GBMs, EBMs, and Symbolic Regression.

3) Interpretability insights into the observation-action mapping in distilled policies, providing both global and local explanations. Policies are distilled with only 10 minutes of interaction per gait after 205 hours of RL expert training.

4) Analysis of distilled policy performance in walking, trotting, pacing and bounding tasks. GBMs and EBMs match or exceed neural policy performance when alternating actions.

In summary, the paper demonstrates that interpretable models can serve as effective locomotion policies, contributing to increased transparency of learned behaviors. The framework enables distilling opaque neural networks into "glass box" policies, facilitating trustworthy adoption of intelligent robots.
