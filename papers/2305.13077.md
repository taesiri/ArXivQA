# [ControlVideo: Training-free Controllable Text-to-Video Generation](https://arxiv.org/abs/2305.13077)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:How can we enable efficient and high-quality controllable text-to-video generation without requiring heavy training of large-scale video generative models? The key hypothesis is that by leveraging pre-trained text-to-image diffusion models and incorporating motion sequences as coarse structure guidance, it is possible to synthesize high-quality and temporally consistent videos without extensive training. Specifically, the paper proposes that by adapting the image diffusion model ControlNet to the video domain through techniques like cross-frame interaction and adding modules like the interleaved-frame smoother, training-free controllable text-to-video generation can be achieved.In summary, the core hypothesis is that training-free text-to-video generation is feasible by building upon pre-trained text-to-image models and using motion sequences, rather than requiring end-to-end training of massive video generative models. The research aims to validate this hypothesis through the proposed ControlVideo framework.
