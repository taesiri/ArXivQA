# [Unified Static and Dynamic Network: Efficient Temporal Filtering for   Video Grounding](https://arxiv.org/abs/2403.14174)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Temporal video grounding (TVG) is challenging as it requires understanding video content and properly associating it with textual or spoken language queries. 
- Existing methods have limitations in effectively modeling both the static and dynamic associations between video content and queries.

Proposed Solution:
- The paper proposes a unified static and dynamic network (UniSDNet) inspired by human visual perception mechanisms.
- It has two key components:
   1) Static Semantic Supplement Network (S3Net): Models global interactions between video clips and multiple queries to supplement their semantics.
   2) Dynamic Temporal Filtering Network (DTFNet): Dynamically filters important spatiotemporal contexts in the video using a graph network.
- S3Net handles static associations via comprehensive multimodal feature aggregation.
- DTFNet captures dynamic associations by simulating three key properties of human visual perception over video timeline.

Key Contributions:
- Novel unified architecture consisting of complementary static and dynamic networks for addressing video grounding.
- New dynamic temporal filtering approach using graph networks with Gaussian filtering to model visual perception over video. 
- Collected two new datasets, Charades-STA Speech and TACoS Speech to facilitate spoken language video grounding research.
- Extensive experiments showing state-of-the-art performance of the proposed UniSDNet on multiple standard benchmarks for both natural language video grounding and spoken language video grounding.

In summary, the key novelty is in unifying efficient static and dynamic modeling tailored for video grounding based on insights from human visual perception. The new datasets and strong empirical results further demonstrate the effectiveness of the proposed techniques.
