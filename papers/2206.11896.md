# [EventNeRF: Neural Radiance Fields from a Single Colour Event Camera](https://arxiv.org/abs/2206.11896)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether an event stream from an event camera moving around a static scene is sufficient to reconstruct a dense volumetric 3D representation of that scene. 

The key hypothesis is that a neural radiance field (NeRF) volume can be inferred from only a monocular color event stream, enabling 3D-consistent, dense, and photorealistic novel view synthesis in RGB space at test time.

Some key points:

- Existing event-based 3D reconstruction methods produce sparse point clouds. The authors aim to achieve dense novel view synthesis, which is useful for computer vision and graphics. 

- The authors adopt NeRF as the 3D scene representation and present the first approach to infer a NeRF volume purely from a color event stream.

- Their method EventNeRF is designed for event-based supervision while preserving the resolution of individual RGB event channels.

- They introduce an event-based ray sampling strategy for efficient training and tailor it to avoid artefacts.

- Experiments on synthetic and real datasets, including numerical and subjective comparisons, demonstrate EventNeRF produces significantly denser and more visually appealing renderings than prior work.

In summary, the key hypothesis is that a dense NeRF volume enabling high-quality RGB novel view synthesis can be learned in a self-supervised, event-based manner from only a monocular color event stream. The experiments aim to validate this capability not shown by prior event-based methods.
