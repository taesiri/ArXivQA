# [ConSept: Continual Semantic Segmentation via Adapter-based Vision   Transformer](https://arxiv.org/abs/2402.16674)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Continual semantic segmentation aims to learn new visual concepts over time without forgetting previous knowledge. However, existing methods have two key issues: 1) Reliance on heavy segmentation decoders limits efficiency; 2) Anti-catastrophic forgetting ability on old classes remains insufficient. The paper investigates using Vision Transformers (ViTs), which have shown strong performance on segmentation, for continual segmentation.

Method: 
Through analysis, the paper finds vanilla ViTs inherently possess anti-catastrophic forgetting ability on old classes, while adapters enhance generalization to novel classes. This motivates an adapter-based ViT called ConSept for continual segmentation. ConSept inserts lightweight attention-based adapters into a pretrained ViT and adopts a dual-path architecture with a simple linear decoder for segmentation.

Main Contributions:
1) ConSept integrates adapters into ViTs for continual segmentation for the first time. With <10% parameters, it achieves better segmentation on old classes and promising quality on novel classes.

2) A distillation method with frozen old-class decoder boundary is proposed to further exploit ConSept's anti-catastrophic forgetting ability.

3) Dual dice losses are used to regularize segmentation maps and enhance overall quality.

Experiments:
Extensive experiments on PASCAL VOC and ADE20K benchmarks show ConSept achieves new state-of-the-art under both overlapped and disjoint settings. With only a simple decoder, it outperforms methods with heavier decoders. ConSept also shows strong anti-catastrophic forgetting ability on both old and novel classes.

In summary, the paper proposes an adapter-based ViT called ConSept for effective and efficient continual semantic segmentation without relying on heavy decoders, and demonstrates its state-of-the-art performance.
