# [AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for   Preconditioning Matrix](https://arxiv.org/abs/2312.01658)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Adaptive optimizers such as Adam have shown remarkable success in deep learning. A key component is the preconditioning matrix that provides enhanced gradient information and regulates the step size along each dimension. However, computing the full Hessian matrix to approximate the curvature is prohibitively expensive. Therefore, it is important to design the preconditioning matrix to balance approximation accuracy and computational efficiency.

Proposed Solution:
This paper proposes a novel optimizer called AGD that enhances the approximation of curvature information and enables automatic switching between SGD and adaptive optimization. 

The key ideas are:

1) Approximate the diagonal Hessian using the difference between gradients of two successive steps. This serves as the inner product between the Hessian row vectors and difference of parameters.

2) Introduce an auto-switching capability to toggle the preconditioning matrix between SGD and adaptive optimizer, governed by a threshold hyperparameter. This allows seamless switching on a per-parameter basis during training.

Main Contributions:

- Proposed a new optimizer AGD that efficiently approximates curvature via gradient differences and automatically switches between SGD and adaptive modes.

- Provided theoretical analysis to establish convergence guarantees for AGD in both non-convex and convex settings.  

- Performed comprehensive experiments on 6 datasets across NLP, CV and Recommendation tasks. Results demonstrate AGD achieves highly competitive or significantly better performance than state-of-the-art optimizers.

- Analyzed the auto-switching mechanism to understand how AGD transitions between SGD and adaptive optimizer, and the impact of key hyperparameter controlling this process.

In summary, the paper presented a novel optimizer AGD with gradient difference based approximation of curvature information and automatic switching capability. Both theoretical and empirical results prove its superiority over existing methods across diverse deep learning tasks.
