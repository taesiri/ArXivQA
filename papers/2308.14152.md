# [Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code   Diffusion using Transformers](https://arxiv.org/abs/2308.14152)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to perform high-quality 2D to 3D image translation for complex volumetric data exhibiting varying internal and external topologies, from different domains, in a unified framework. Specifically, the paper investigates translating between 2D views (e.g. X-rays) to 3D volumes (e.g. CT scans) where there are differences in imaging devices, modalities, and geometric misalignment between the 2D and 3D data. The key hypothesis is that modeling this as a conditional generative modeling problem with a likelihood-based model in a discrete latent space parameterized by transformers can achieve high quality 3D image synthesis without requiring aligned training data.In summary, the main research question is how to perform unaligned 2D to 3D translation with a conditional generative model to generate complex 3D data from 2D views of different domains. The key hypothesis is that a conditional discrete diffusion model parameterized by transformers in a compressed latent space can achieve this effectively.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- Proposing a novel 2D to 3D image translation approach using conditional diffusion with transformers. The key idea is to first compress the 2D and 3D data into discrete vector quantized (VQ) spaces independently using VQ-VAEs. Then a diffusion model parameterized by a transformer is used to generate the 3D data conditioned on the 2D data in the compressed VQ space.- Showing that operating in the discrete VQ space allows the model to scale easily to high-resolution 3D data. The global context provided by the transformer's attention mechanism also allows using unaligned 2D and 3D data, since any part of the 2D inputs can contribute to 3D voxel predictions.- Demonstrating state-of-the-art performance on two complex volumetric image datasets (chest CT scans and baggage screening CTs) compared to GAN and NeRF baselines. The model gives higher fidelity in terms of density and coverage metrics, while also providing likelihood estimates.In summary, the key contribution is a new conditional discrete diffusion approach for 2D to 3D translation that works directly in an information-rich VQ space and does not require aligned 2D-3D data pairs. This allows generating high quality 3D volumes conditioned on 2D views in a simple and scalable way.
