# [Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free   Optical PINN Training](https://arxiv.org/abs/2401.00413)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Solving partial differential equations (PDEs) numerically often requires large computing time, energy, and hardware resources, limiting their use in real-time applications like autonomous systems. 
- Training physics-informed neural networks (PINNs) to solve PDEs is still expensive, preventing real-time safety-aware decision making.
- Training PINNs on photonic chips is challenging due to: (1) large photonic device size limiting integration density, (2) lack of scalable on-chip training methods, and (3) higher-order derivatives in loss requiring multiple backpropagation passes.

Proposed Solution: 
- Develop an on-chip training framework for PINNs to solve high-dimensional PDEs with ultra-low fJ/MAC power and latency using optical computing.
- Employ a backpropagation-free approach with additional inferences to estimate gradients and derivatives, avoiding error feedback and intermediate result storage.
- Utilize a tensor-compressed format to reduce photonic devices and improve convergence of the training framework.
- Design uses tensorized optical neural networks (TONN) for scalable inference and in-situ optimization via phase-domain tuning.

Main Contributions:
- First backpropagation-free optical training framework to handle large-size PINN training on photonic chips
- Tensor-compressed method to reduce devices and enhance optical PINN training convergence/scalability 
- Demonstration of proposed method on 20-dim HJB PDE, reducing MZIs by 117x and solving PDE with only 1.36 J and 1.15 s
- Framework can scale to 1024x1024 optical neural networks, paving way for real-time, ultra-low power PDE solvers

In summary, the paper develops a novel tensor-compressed, backpropagation-free on-chip training approach to enable large-scale PINN training on photonic chips for the first time. This provides a path to ultra-low energy and real-time PDE solving in applications like autonomous systems.


## Summarize the paper in one sentence.

 This paper proposes the first optical training framework for physics-informed neural networks that enables solving high-dimensional partial differential equations with ultra-low energy consumption and latency by utilizing a tensor-compressed backpropagation-free approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing the first optical training framework that can handle realistic large-size physics-informed neural networks (PINNs) on an integrated photonic platform. Specifically, the paper:

1) Employs a backpropagation-free approach using only additional inferences to estimate gradients and derivatives, enabling training PINNs and solving realistic PDEs on a photonic chip. 

2) Utilizes a tensor-compressed format to reduce the number of photonic devices and improve the convergence of the backpropagation-free optical PINN training framework.

3) Demonstrates numerical simulation of the optical PINN training method to solve a 20-dimensional Hamiltonian-Jacobi-Bellman (HJB) partial differential equation (PDE). The method achieves competitive performance while reducing the number of Mach-Zehnder interferometers (MZIs) by a factor of 1.17Ã—10^3 and requiring only 1.36 J and 1.15 s to solve this PDE.

In summary, the key contribution is developing the first scalable on-chip training framework for large-size PINNs that can be applied to solve high-dimensional PDEs with ultra-low energy consumption and latency. This paves the way for future real-time, ultra-efficient optical computing for complex scientific computing problems.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with this paper include:

- Physics-informed neural networks (PINNs)
- Partial differential equations (PDEs) 
- Optical neural networks (ONNs)
- Tensorized optical neural networks (TONNs)
- Tensor-train (TT) decomposition
- Backpropagation-free training
- Mach-Zehnder interferometers (MZIs)
- Hamiltonian-Jacobi-Bellman (HJB) PDE
- Real-time training
- Ultra-low energy and latency
- Autonomous systems

The paper proposes a framework to train physics-informed neural networks (PINNs) for solving partial differential equations (PDEs) using optical neural networks (ONNs). Key aspects include using tensorized ONNs (TONNs) and tensor-train (TT) decomposition to reduce the hardware complexity, as well as a backpropagation-free training method to enable on-chip training. This is applied to solve a 20-dimensional Hamiltonian-Jacobi-Bellman (HJB) PDE with improved energy efficiency and training speed compared to digital systems. Potential applications include real-time decision making and control for autonomous systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions that training PINNs on photonic chips is challenging due to three main constraints. Could you elaborate on each of these constraints and why they make on-chip training difficult? 

2. One of the key ideas in your method is to avoid the backpropagation process during training. Why is backpropagation problematic to implement optically? What specifically makes it "hardware-unfriendly"?

3. You propose a tensor-compressed format to reduce the number of photonic devices needed. How exactly does tensor decomposition help mitigate hardware constraints? What are the tradeoffs compared to a standard dense network?

4. Your method relies on using additional inferences to estimate gradients rather than true backpropagation. What types of gradient estimators did you consider and why did you choose the simultaneous perturbation stochastic approximation (SPSA) method? What are its advantages and limitations?  

5. For the loss function evaluation, you mention using either finite difference or a sparse-grid Stein estimator to compute derivatives. Could you compare and contrast these two methods? Which one is better suited for an optical implementation and why?

6. In the results section, you compare hardware-aware and hardware-unaware training for the off-chip baseline methods. What specifically were the sources of hardware imperfection you considered and how did they impact performance? 

7. Your proposed on-chip training method directly tunes the photonic devices during training. How does this make the model more robust to hardware imperfections compared to off-chip training approaches?

8. For the 20D HJB PDE example, could you walk through the latency and energy consumption analysis? What are the bottleneck components and opportunities for further gains?  

9. What are the limitations of your current tensor-compressed training approach in terms of model capacity and representational power compared to a standard dense network? Do you have plans to scale it up further?

10. This is the first work showing PINN training completely done optically. What do you see as the next steps and challenges still remaining toward developing practical on-chip PDE solvers?
