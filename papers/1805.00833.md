# [Learnable PINs: Cross-Modal Embeddings for Person Identity](https://arxiv.org/abs/1805.00833)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can facial and voice biometrics be combined to learn joint embeddings that can reliably verify person identity?The key hypothesis is that by training cross-modal embeddings that map face and voice samples from the same individual close together in a shared space, the model can learn to verify if two samples (one face, one voice) belong to the same person or not. The authors propose a "Learnable PIN" approach to learn these joint embeddings in a way that is more robust and secure than relying on either face or voice alone.In summary, the main research focus is on developing cross-modal biometric embeddings capable of person identification/verification by combining the complementary strengths of face and voice biometrics. The key hypothesis is that such joint embeddings can outperform unimodal systems.


## What is the main contribution of this paper?

Based on the paper abstract and introduction, the main contribution of this paper seems to be proposing a method to learn cross-modal person identity embeddings that map face images and voice segments to a shared embedding space. Specifically, the authors propose using a Siamese convolutional neural network with a triplet loss to learn these embeddings, which they call Learnable PINs (Person Identity Embeddings). The key ideas are:- Learning a joint embedding space for faces and voices that captures person identity information- Using a triplet loss during training to ensure embeddings for the same person are close together while embeddings for different people are far apart - Showing these learned embeddings can be used for cross-modal person verification and clustering tasks like matching faces to voices.- Demonstrating their method learns robust identity embeddings that outperform previous approaches on cross-modal verification benchmarks.So in summary, the main contribution appears to be presenting an approach to learn cross-modal identity embeddings using deep convolutional Siamese networks and showing these embeddings are useful for person recognition across faces and voices. The novelty seems to be in the architecture and training process used to learn shared identity representations across modalities.
