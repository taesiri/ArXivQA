# [CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language   Models to Coding Preferences](https://arxiv.org/abs/2403.09032)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
Existing benchmarks for evaluating large language models (LLMs) focus mainly on core capabilities and functional correctness. There is a lack of datasets and benchmarks tailored to tuning and aligning LLMs with non-functional requirements related to code quality, such as readability, efficiency, and coding style preferences. Additionally, current evaluation methods rely heavily on automated metrics, lacking nuanced human judgement.

Proposed Solution:
The authors introduce two novel contributions - CodeUltraFeedback and CODAL-Bench:

- CodeUltraFeedback is a dataset of 10,000 complex code generation instructions mapped to 5 coding preferences. Responses to the instructions are generated by 14 diverse LLMs and evaluated by an LLM judge (GPT-3.5-Turbo) that provides numerical ratings and textual rationales assessing alignment with preferences.

- CODAL-Bench benchmark allows rigorous evaluation and comparison of LLMs' alignment to coding preferences over 500 instructions from CodeUltraFeedback. A single-answer grading scheme is used with GPT-3.5/4-Turbo as judge and reference responses.

The authors demonstrate tuning a small LLM (CodeLlama-7B-Instruct) with CodeUltraFeedback using supervised fine-tuning and direct preference optimization. This yields substantially improved alignment and competitiveness compared to larger pre-trained models.

Main Contributions:

- Release of CodeUltraFeedback dataset for LLM preference tuning 

- CODAL-Bench benchmark to evaluate LLM alignment with coding preferences

- Demonstrating utility of CodeUltraFeedback for preference tuning - CodeLlama-7B-Instruct outperforms 34B models after tuning

In summary, this work facilitates better alignment of LLMs to coding preferences through tailored datasets and human-centric benchmarks that leverage advanced reasoning capabilities of LLMs as judges.
