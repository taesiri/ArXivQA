# [Prompt Perturbation in Retrieval-Augmented Generation based Large   Language Models](https://arxiv.org/abs/2402.07179)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) suffer from robustness issues when used for text generation. Retrieval-augmented generation (RAG) methods have been proposed to improve their reliability by enhancing LLMs with retrieval functions to provide relevant contexts from curated data. 
- However, the paper shows that RAG models can also be vulnerable to adversarial attacks. Small perturbations to the input prompt can steer the retriever to extract an irrelevant or incorrect passage, leading the LLM to generate factually inaccurate text.

Proposed Solution:
- The paper proposes a Gradient Guided Prompt Perturbation (GGPP) method to systematically manipulate the passage retrieval in RAG models. 
- GGPP initializes a prefix using important tokens from a target wrong passage. This prefix is then optimized to minimize the distance between the passage and query embeddings, while maximizing the distance to the original correct passage.
- This pushes the targeted passage to the top of the retrieved results, leading to factually incorrect LLM text generation.

Main Contributions:
- Demonstrates the vulnerability of RAG models to targeted prompt perturbations that cause passage retrieval manipulation
- Proposes an optimization method (GGPP) that can effectively attack passage ranking in RAG with high success rates
- Analyzes impact on LLM activations and proposes a detection method (ACT probe) to identify perturbed prompts in RAG
- Provides systematic analysis and benchmark datasets to understand robustness issues with passage retrievals in RAG models

In summary, the key insight is that the reliability gains of using external contexts in RAG models can be offset if the retriever is not robust against adversarial attacks. The paper reveals this vulnerability and provides methods to manipulate and detect problematic perturbations.
