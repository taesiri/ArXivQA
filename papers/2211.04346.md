# [Cross-Attention is all you need: Real-Time Streaming Transformers for   Personalised Speech Enhancement](https://arxiv.org/abs/2211.04346)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve personalised speech enhancement (PSE) by using more adaptive representations of the target speaker's voice profile, rather than a fixed embedding vector. Specifically, the paper proposes a novel cross-attention approach to generate flexible speaker embeddings for PSE, instead of using a constant single vector extracted from enrollment audio. The hypothesis is that a fixed speaker embedding may not capture variations in the target speaker's voice over time and across different utterances. To test this, the paper develops streaming Transformer models with cross-attention between the target speaker enrollment states and the input noisy speech states. This allows the model to dynamically attend to suitable target speaker representations when enhancing the input audio.The key hypotheses tested are:- Using cross-attention over enrollment states instead of a fixed mean/last pooling vector improves PSE performance.- Cross-attention provides more adaptive speaker embeddings compared to concatenation of a fixed embedding.- The improvements are due to more flexible speaker embeddings, not just model architecture.- Cross-attention helps more for babble noise than ambient, as it relies more on precise speaker information.- The benefits hold for both single and multiple enrollment utterances.So in summary, the paper focuses on improving PSE through more adaptive speaker embeddings generated via cross-attention. The experiments aim to validate that this approach outperforms baselines that use a fixed embedding vector.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel cross-attention approach for personalised speech enhancement (PSE). Specifically:- They propose using cross-attention between the hidden states of the target speaker's enrollment audio(s) and the hidden states of the corrupted input audio. This allows generating an adaptive representation of the target speaker's voice for each input frame, rather than using a fixed vector. - They integrate this cross-attention mechanism into a streaming Transformer-based encoder-decoder model for PSE. To my knowledge, this is the first work using cross-attention to adaptively model the target speaker in a streaming PSE system.- Through extensive experiments, they show their proposed cross-attention models consistently outperform strong baselines using concatenation of a fixed target speaker vector. Impressively, their models match or exceed the performance of non-streaming baselines while using only half the parameters.- Their results indicate that dynamically generating representations of the target speaker's voice leads to better personalization and speech extraction compared to relying on a static embedding, especially in noisy multi-speaker conditions.In summary, the key contribution is using cross-attention to generate adaptive target speaker representations in a streaming Transformer model for improved PSE performance. The proposed approach is shown to be more effective than prior reliance on fixed speaker embeddings.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other work in personalised speech enhancement:- The main novelty is in using cross-attention to generate adaptive representations of the target speaker, rather than relying on a static embedding vector. Most prior work concatenates or modulates a fixed speaker embedding vector extracted from enrollment audio. - The cross-attention approach allows selecting relevant slices of the enrollment audio dynamically based on the input. This is more flexible than just using a single averaged vector.- They use a fully attention-based Transformer encoder-decoder architecture for the core PSE model. Many recent papers have explored Conformer or Conv-TasNet architectures instead.- They achieve strong performance on both speech enhancement quality (SDR) and downstream ASR word error rate. The improvements over concatenation baselines are statistically significant.- Their streaming cross-attention models can match or outperform non-streaming baselines like VoiceFilter, even with 50% fewer parameters. This demonstrates the effectiveness of the cross-attention approach.- For multiple enrollment utterances, cross-attention again outperforms mean pooling of utterance embeddings. This shows it can effectively leverage multiple enrollments.- In summary, the cross-attention mechanism for speaker conditioning seems highly promising for PSE, as it can better adapt to varying conditions compared to fixed embedding approaches. The Transformer architecture also achieves excellent results.In conclusion, the paper introduces a novel adaptive speaker conditioning method and achieves state-of-the-art results, advancing research in personalized speech enhancement.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Applying the proposed cross-attention approach to inject adaptive target speaker embeddings into other PSE model architectures besides the fully attention-based architecture explored in this work. The authors suggest this could be a promising direction for future work.- Adapting the model to take raw waveform inputs instead of spectrogram inputs. The authors mention adapting their model to raw waveforms as an interesting future direction.- Data augmentation techniques to generate multiple enrolment utterances from a single utterance in cases where only one enrolment utterance is available. The authors suggest this as a way to handle scenarios with limited enrolment data.- Addressing the mismatch between SDR and WER improvements observed in the experiments. The authors note the mismatch between SDR gains and WER reductions as a common problem worth exploring further.- Releasing the validation and test datasets used in the experiments to support further research and benchmarking. The authors state they plan to release these datasets.- Exploring techniques to reduce the computational complexity and memory requirements of the cross-attention approach. While the authors analyze the complexity, reducing it further could be useful.In summary, the main future directions revolve around applying the cross-attention technique in new ways, adapting the model architecture, improving the training data, benchmarking on new datasets, and reducing the computational requirements. The authors lay out several interesting avenues to build on their cross-attention approach for personalised speech enhancement.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper focuses on personalised speech enhancement (PSE), where the goal is to extract the speech of a target speaker from a noisy audio recording while removing other sounds like background noise and interfering speakers. The common approach is to extract a fixed speaker embedding vector from enrollment audio of the target speaker and use that to condition a PSE model. However, the authors argue that a single fixed embedding may not capture variations in the target speaker's voice over time and across different utterances. To address this, they propose using cross-attention between the noisy input audio and multiple frames of enrollment audio to dynamically generate an adaptive representation of the target speaker for each input frame. Specifically, they present a streaming Transformer-based encoder-decoder PSE model with a cross-attention decoder module. Through experiments on noisy LibriSpeech samples paired with real ambient noise and babble, they demonstrate that their proposed cross-attention approach consistently outperforms baseline models using fixed/averaged speaker embeddings in both PSE quality and downstream ASR performance. Interestingly, their model achieves similar or better performance with only half the size of baseline models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a streaming Transformer-based personalised speech enhancement model that uses cross-attention on target speaker embeddings from enrollment audio to generate adaptive representations of the target speaker's voice for improving speech extraction and downstream ASR performance.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper proposes a new approach for personalised speech enhancement (PSE) using streaming Transformers. PSE aims to extract the speech of a target speaker from a noisy audio recording while removing other sounds. Existing methods extract a fixed speaker embedding vector from enrollment audio of the target speaker, which is then used to condition the PSE model. However, the authors argue that a single fixed embedding may not optimally capture variations in the target speaker's voice over time and across utterances. To address this, the authors propose using cross-attention between the hidden states of the noisy input and the enrollment audio to dynamically generate adaptive representations of the target speaker. Specifically, they use a Transformer encoder-decoder architecture where cross-attention is applied between the decoder hidden state for the current noisy frame and the encoder hidden states from the enrollment. This allows the model to flexibly select the most relevant target speaker information based on the input. Experiments show their approach consistently outperforms strong baselines using fixed speaker embeddings on both speech enhancement and downstream ASR tasks. Notably, their models match or exceed baseline performance with only half the parameters. The cross-attention mechanism provides more robust personalisation, especially for babble noise scenarios.
