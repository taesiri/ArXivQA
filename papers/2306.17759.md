# [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width   Limit](https://arxiv.org/abs/2306.17759)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we theoretically characterize and stabilize the neural covariance of Transformer-type architectures like Softmax-based attention models in the proportional infinite depth-and-width limit?More specifically, the paper seems to address the following key points:- Derive a neural covariance SDE to characterize the output distribution of residual networks with skip connections and shaped ReLU activations in the proportional infinite depth-and-width limit.- Propose a "shaped attention" mechanism that modifies Softmax attention to be closer to the identity matrix. Derive the corresponding neural covariance SDE.  - Combine the SDE results to fully characterize the output distribution of the "shaped Transformer", i.e. Transformer-type architectures with the proposed modifications, in the proportional infinite depth-and-width limit.- Demonstrate how the proposed modifications help prevent rank collapse and degeneracy of the neural covariance compared to standard Transformer architectures. - Provide guidance on architecture design and hyperparameter tuning for stability based on the theoretical analysis.- Validate the theory with simulations and preliminary experiments showing the shaped Transformer is trainable in practice.So in summary, the central focus seems to be on developing a theoretical understanding of Transformers in the proportional limit in order to diagnose sources of instability, propose modifications, and derive guidance for architecture design and hyperparameter tuning. The shaped Transformer is proposed based on these theoretical insights.
