# [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width   Limit](https://arxiv.org/abs/2306.17759)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we theoretically characterize and stabilize the neural covariance of Transformer-type architectures like Softmax-based attention models in the proportional infinite depth-and-width limit?More specifically, the paper seems to address the following key points:- Derive a neural covariance SDE to characterize the output distribution of residual networks with skip connections and shaped ReLU activations in the proportional infinite depth-and-width limit.- Propose a "shaped attention" mechanism that modifies Softmax attention to be closer to the identity matrix. Derive the corresponding neural covariance SDE.  - Combine the SDE results to fully characterize the output distribution of the "shaped Transformer", i.e. Transformer-type architectures with the proposed modifications, in the proportional infinite depth-and-width limit.- Demonstrate how the proposed modifications help prevent rank collapse and degeneracy of the neural covariance compared to standard Transformer architectures. - Provide guidance on architecture design and hyperparameter tuning for stability based on the theoretical analysis.- Validate the theory with simulations and preliminary experiments showing the shaped Transformer is trainable in practice.So in summary, the central focus seems to be on developing a theoretical understanding of Transformers in the proportional limit in order to diagnose sources of instability, propose modifications, and derive guidance for architecture design and hyperparameter tuning. The shaped Transformer is proposed based on these theoretical insights.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Studying Transformers in the proportional infinite depth-and-width limit, where depth and width go to infinity with their ratio held constant. Previous work has studied MLPs in this limit, but the authors provide the first characterization for Transformer-type architectures. - Proposing a modified "shaped" attention mechanism that prevents the degeneracy of the neural covariance matrix. This involves perturbing the Softmax matrix to be close to identity and scaling the temperature parameter appropriately. - Deriving stochastic differential equations (SDEs) that characterize the output distribution and neural covariance of the proposed shaped Transformer model in the proportional limit. This provides insight into the stability of the architecture.- Demonstrating through simulations that the SDE approximations are surprisingly accurate even for finite sized networks. - Providing preliminary experiments showing the proposed shaped Transformer can be trained effectively on language modeling tasks.In summary, the main contribution appears to be introducing modifications to the Transformer to allow analysis in the proportional infinite depth-and-width limit, deriving the corresponding SDEs, and showing this shaped Transformer is trainable and avoids instability issues like rank collapse. The theoretical analysis provides guidance on architecture design and hyperparameter settings.
