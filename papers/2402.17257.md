# [RIME: Robust Preference-based Reinforcement Learning with Noisy   Preferences](https://arxiv.org/abs/2402.17257)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Preference-based reinforcement learning (PbRL) avoids the need for manual reward engineering by using human preferences as feedback. 
- However, current PbRL methods overly rely on high-quality feedback from domain experts, lacking robustness when preferences are noisy or from non-experts. Even 10% label noise can significantly degrade performance.
- Learning from noisy labels is an important research area in deep learning, but difficult to apply directly in PbRL due to limited feedback samples and distribution shifts during RL training.

Proposed Solution:
- The paper proposes RIME, a robust PbRL algorithm for effective reward learning from noisy preferences.
- It uses a sample selection-based discriminator to dynamically filter denoised preferences for robust training. The discriminator employs a lower bound on the KL divergence to identify trustworthy samples, and an upper bound to flip likely corrupted labels.
- To mitigate accumulated error from incorrect selection, RIME warms starts the reward model using intrinsic rewards during pre-training. This also bridges performance gaps during transition from pre-training to online training.

Main Contributions:
- Presented RIME, a robust PbRL algorithm designed for noisy preferences - an important but under-studied topic.
- Identified and addressed performance gaps during phase transitions in PbRL using reward model warm starting. This proved crucial for both robustness and efficiency. 
- Demonstrated superior performance of RIME over state-of-the-art PbRL methods under diverse robotic tasks with varying noise levels. Showed particular suitability for non-expert humans.

In summary, the paper makes notable contributions towards advancing the applicability of PbRL to real-world scenarios by enhancing robustness against noisy preferences while preserving efficiency. The proposed techniques of dynamic preference denoising and warm starting facilitation signify promising research directions.
