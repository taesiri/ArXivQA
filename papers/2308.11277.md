# [CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings   and Mapped Photographs with Illumination Augmentation](https://arxiv.org/abs/2308.11277)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can different types of rendered 3D datasets, in combination with photographs, be utilized to improve machine learning approaches for cuneiform sign detection?Specifically, the paper explores using different renderings of 3D models of cuneiform tablets, including Phong shading, MSII curvature filtering, and mixed techniques, along with corresponding mapped photographs, to train convolutional neural networks for detecting cuneiform signs. It evaluates the impact of these different rendering techniques and image types on the accuracy of sign detection, measured by average precision, as well as the localization accuracy of predicted bounding boxes, measured by IOU thresholds. The use of illumination augmentation on the 3D models is also evaluated. The key hypothesis seems to be that leveraging both 3D renderings and photographs together can improve sign detection performance compared to using either data type alone. The results generally support this, showing the best overall detection when training on a mix of rendered and photographic data. The paper also finds that certain rendering techniques, especially MSII filtering, can enhance results even for photographic test data. Overall, it demonstrates the potential of using 3D data to augment more widely available photographs for machine learning approaches to analyzing cuneiform tablets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Developing a sign detection approach for cuneiform script using a convolutional neural network (CNN). The approach uses a RepPoints detector to predict bounding boxes around cuneiform signs in images.- Evaluating the sign detector on different types of images - 3D renderings (VirtualLight, MSII filter, mixed) and photographs. The results show that using rendered 3D images for training performs better than using just photographs.- Providing a dataset of around 500 annotated 3D renderings of cuneiform tablets (the HeiCuBeDa and MaiCuBeDa datasets). This helps address the lack of annotated data which is a key challenge in this domain.- Introducing an illumination augmentation technique applied to the full dataset of 3D models. This involves rendering the 3D models under different lighting conditions to expand the training data. The results show this improves performance, especially bounding box accuracy, when evaluated on 3D renderings. - Showing that combining 3D renderings and photographs leads to better generalizability compared to using either data type alone. The mixed data approach helps the model learn from the different information provided across renderings and photographs.In summary, the main contribution seems to be advancing sign detection for cuneiform script through a CNN-based approach evaluated on mixed 3D and image data, augmented through rendering techniques to address data limitations. The results demonstrate the potential of using 3D data to improve machine learning for this challenging ancient script.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a convolutional neural network approach for detecting cuneiform signs in images of clay tablets using different types of 3D renderings and photographs, finding that a combination of renderings and photos produces the best results.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research on cuneiform sign detection and recognition:- The paper focuses specifically on evaluating different rendering techniques (e.g. VirtualLight, MSII, photos) for training sign detection models. Most prior work has used photos only. Evaluating renderings is a novel contribution.- The paper uses a relatively small annotated dataset of around 500 tablets. Other recent work like DeepScribe has much larger datasets with over 1300 annotated tablets. The smaller dataset here is a limitation.- For sign detection, the RepPoints model achieves decent results, with AP@75 of 0.55 on renderings compared to 0.21 on photos in DeepScribe. The renderings seem to help with more accurate bounding boxes. - The illumination augmentation approach is similar to Rest et al. 2022, but applied to full 3D models rather than sign crops. This is a useful extension for handling lighting variations.- The paper does not address sign classification, only localization. DeepScribe and others consider the full OCR pipeline including classification and transliteration.- The sign detection results are promising, but have not yet been integrated into a full OCR pipeline like DeepScribe. It remains unclear if the improved localization would help downstream tasks.- The paper introduces useful new annotated 3D rendering datasets like HeiCuBeDa and MaiCuBeDa. More annotated data benefits the field.So in summary, this paper has some notable innovations in using renderings and illumination augmentation, but is limited by small dataset size and narrow focus on localization only. Integrating the detection models into a full OCR pipeline would better demonstrate value. But it provides useful insights on leveraging 3D data.
