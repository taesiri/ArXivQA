# [CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings   and Mapped Photographs with Illumination Augmentation](https://arxiv.org/abs/2308.11277)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can different types of rendered 3D datasets, in combination with photographs, be utilized to improve machine learning approaches for cuneiform sign detection?Specifically, the paper explores using different renderings of 3D models of cuneiform tablets, including Phong shading, MSII curvature filtering, and mixed techniques, along with corresponding mapped photographs, to train convolutional neural networks for detecting cuneiform signs. It evaluates the impact of these different rendering techniques and image types on the accuracy of sign detection, measured by average precision, as well as the localization accuracy of predicted bounding boxes, measured by IOU thresholds. The use of illumination augmentation on the 3D models is also evaluated. The key hypothesis seems to be that leveraging both 3D renderings and photographs together can improve sign detection performance compared to using either data type alone. The results generally support this, showing the best overall detection when training on a mix of rendered and photographic data. The paper also finds that certain rendering techniques, especially MSII filtering, can enhance results even for photographic test data. Overall, it demonstrates the potential of using 3D data to augment more widely available photographs for machine learning approaches to analyzing cuneiform tablets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Developing a sign detection approach for cuneiform script using a convolutional neural network (CNN). The approach uses a RepPoints detector to predict bounding boxes around cuneiform signs in images.- Evaluating the sign detector on different types of images - 3D renderings (VirtualLight, MSII filter, mixed) and photographs. The results show that using rendered 3D images for training performs better than using just photographs.- Providing a dataset of around 500 annotated 3D renderings of cuneiform tablets (the HeiCuBeDa and MaiCuBeDa datasets). This helps address the lack of annotated data which is a key challenge in this domain.- Introducing an illumination augmentation technique applied to the full dataset of 3D models. This involves rendering the 3D models under different lighting conditions to expand the training data. The results show this improves performance, especially bounding box accuracy, when evaluated on 3D renderings. - Showing that combining 3D renderings and photographs leads to better generalizability compared to using either data type alone. The mixed data approach helps the model learn from the different information provided across renderings and photographs.In summary, the main contribution seems to be advancing sign detection for cuneiform script through a CNN-based approach evaluated on mixed 3D and image data, augmented through rendering techniques to address data limitations. The results demonstrate the potential of using 3D data to improve machine learning for this challenging ancient script.
