# [How Can LLM Guide RL? A Value-Based Approach](https://arxiv.org/abs/2402.16181)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) algorithms require extensive trial-and-error interactions to collect useful feedback for policy improvement. In contrast, large language models (LLMs) can quickly propose viable strategies but lack the capability to refine them based on environment feedback. Therefore, improving the sample efficiency of RL using the innate capacities of LLMs is an open challenge.

Proposed Solution: 
The paper develops an algorithm named Language-Integrated Value Iteration (LINVIT) that incorporates LLM guidance as a regularization factor in value-based RL. Specifically, LLM policy is used to define a regularizer that is combined with the original value functions. This leads to significant reductions in samples needed when the LLM policy aligns closely with the optimal policy. The method retains the capability to find the optimal policy even if LLM policy is suboptimal.

A practical variant called SLINVIT is proposed that simplifies LINVIT using: 
(i) Direct combination of value estimator and LLM policy log probability.
(ii) Decomposition of planning problem into sub-problems with LLM-based sub-goals to reduce complexity.

Main Contributions:
- Proposes a novel framework to integrate LLM guidance as a regularizer in value-based RL to improve sample efficiency
- Provides theoretical analysis showing sample complexity reduces as divergence between LLM and optimal policies decreases
- Introduces practical SLINVIT algorithm with two techniques to simplify implementation
- Demonstrates state-of-the-art performance of SLINVIT across text-based environments - ALFWorld, InterCode and BlocksWorld

The key insight is using LLM to define a policy regularizer instead of directly employing it for decision-making. This retains capabilities for optimal policy identification while benefiting sample efficiency when LLM policy is near optimal.
