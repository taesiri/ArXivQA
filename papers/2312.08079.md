# [Extending Whisper with prompt tuning to target-speaker ASR](https://arxiv.org/abs/2312.08079)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes using prompt tuning to extend Whisper, a large-scale single-talker automatic speech recognition (ASR) model, to target-speaker ASR (TS-ASR) for transcribing the speech of a target speaker from multi-talker overlapped utterances. The method introduces trainable soft prompts conditioned on target speaker embeddings to guide Whisper to focus on the target speech. Techniques like deep prompting and prompt reparameterization are further explored to enhance performance. Experiments on the Libri2Mix dataset demonstrate the proposed approach achieves comparable performance to state-of-the-art full fine-tuning TS-ASR methods while only fine-tuning about 1% task-specific parameters. Notably, the method retains Whisper's original abilities of inverse text normalization and timestamp prediction, enabling more natural and informative transcriptions. The work underscores prompt tuning's effectiveness and efficiency for adapting large foundation models to new tasks when data is limited. It also extends Whisper's remarkable transferability to the challenging TS-ASR task.
