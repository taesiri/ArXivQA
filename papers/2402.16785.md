# [CARTE: pretraining and transfer for tabular learning](https://arxiv.org/abs/2402.16785)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pretrained deep learning models have brought major advances for images and text, but not for tabular data/relational data. Tabular data is central to many applications but lacks the benefits of large pretrained models.
- A key challenge is that tables in the wild have different schemas/columns and naming conventions, making integration difficult. Data integration techniques like schema matching and entity matching require manual effort.

Proposed Solution:
- Introduce CARTE, a novel neural architecture to represent and process tabular data without needing integration.
- Represent each row of a table as a graph, with nodes for entries and edges labeled by column names. This gives a consistent representation across tables.
- Entries and column names are embedded using language models to handle variability. Numerical entries are handled specially.  
- A graph neural network is used to process the graphs, learning contextual representations of each entry using neighboring entries and column context.
- The model is pretrained on a large knowledge base to learn useful background knowledge about entities and relations. It can then be fine-tuned to downstream tasks.

Main Contributions:
- A new way to represent tabular data as graphs that enables learning across unmatched tables, without schema/entity matching.
- Contextual representations of table entries using graph neural networks and language model embeddings. 
- Pretraining on a knowledge base to enable transfer learning to downstream tasks. Fine-tuning strategies for single or multiple target tables.
- Extensive experiments showing CARTE outperforms strong baselines on 51 datasets. It enables joint learning from multiple tables and benefits from more tables.
- The work facilitates pretrained models and transfer learning for tabular data, opening new research directions to improve deep learning on tables.
