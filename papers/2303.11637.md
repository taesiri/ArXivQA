# [Equiangular Basis Vectors](https://arxiv.org/abs/2303.11637)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to design an effective and scalable classifier for deep neural networks that does not require a large number of trainable parameters. 

Specifically, the paper proposes a method called Equiangular Basis Vectors (EBVs) as an alternative to the commonly used fully-connected (FC) classifier layer with softmax. The key ideas behind EBVs are:

- Predefine a fixed set of normalized vector embeddings ("basis vectors") for each class, with equal angles between the vectors. The number of basis vectors scales sub-linearly with the number of classes.

- Optimize the feature representation to minimize the spherical distance to the corresponding class's basis vector, rather than learning a large FC weight matrix.

So in summary, the main hypothesis is that EBVs can achieve competitive accuracy to FC+softmax classifiers while being much more parameter-efficient and scalable as the number of classes grows. The experiments on ImageNet and other datasets aim to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes Equiangular Basis Vectors (EBVs) to replace the fully connected classifier layer in deep neural networks for image classification tasks. 

2. EBVs predefine fixed normalized vector embeddings (called basis vectors) for each class, with the vectors constrained to have equal angles between each other. The learning objective then becomes minimizing the spherical distance between the embedding of each input and its corresponding categorical basis vector.

3. Compared to regular classifiers, EBVs have fixed parameters that do not increase with number of classes. Compared to metric learning methods, EBVs do not need to compute sample distances. This makes EBVs efficient for large-scale datasets.

4. Experiments on ImageNet, COCO, ADE20K show EBVs can match or outperform regular classifiers, while being more parameter-efficient. Analyses also show EBVs represent images differently than regular classifiers.

5. Theoretical analysis provides constraints on the relation between the angle threshold, embedding dimension, and maximum number of vectors for EBVs. An algorithm is provided to generate EBVs satisfying the constraints.

In summary, the main contribution is proposing EBVs as an alternative to fully connected classifiers, which is more efficient, achieves strong performance on image tasks, and represents images differently. Theoretical and algorithmic analyses support generating the EBVs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Equiangular Basis Vectors (EBVs) as fixed normalized vector embeddings for categories that do not change during training, allowing the model parameters to remain constant as the number of classes grows while still achieving strong performance on image classification tasks.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper on Equiangular Basis Vectors (EBVs) compares to other related work in image classification and metric learning:

- Most prior work uses a trainable fully connected layer with softmax for classification. This causes the number of parameters to scale linearly with the number of classes. EBVs uses fixed normalized vector embeddings for each class, keeping model size constant regardless of number of classes.

- Compared to metric learning methods like triplet loss, EBVs do not require optimizing distances between large sets of sample embeddings. EBVs just optimize each sample to its corresponding class embedding, reducing computational cost. 

- EBVs are more scalable than prior spherical classification methods like Hyperspherical Prototypes, which saw significant accuracy drops when prototype dimension was reduced. EBVs maintain accuracy even with much lower dimension than number of classes.

- EBVs provide a simple but effective alternative to a standard trainable classifier. They achieve strong results without requiring extensive tuning or training tricks. Prior methods like HoTS often need complex training schemes. 

- Most prior work formulates classification as regression to class centers or separation of classes. EBVs take a more direct approach of fitting samples to predefined orthogonal class vectors on a hypersphere.

- Compared to metric learning, EBVs don't update class embeddings during training. This prevents class representations from being disrupted by individual samples.

Overall, EBVs offer a novel classification formulation that is simple, scalable, and achieves strong accuracy. The comparisons suggest EBVs are a promising new approach compared to prior work in this area.
