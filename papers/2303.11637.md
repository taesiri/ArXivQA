# [Equiangular Basis Vectors](https://arxiv.org/abs/2303.11637)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to design an effective and scalable classifier for deep neural networks that does not require a large number of trainable parameters. 

Specifically, the paper proposes a method called Equiangular Basis Vectors (EBVs) as an alternative to the commonly used fully-connected (FC) classifier layer with softmax. The key ideas behind EBVs are:

- Predefine a fixed set of normalized vector embeddings ("basis vectors") for each class, with equal angles between the vectors. The number of basis vectors scales sub-linearly with the number of classes.

- Optimize the feature representation to minimize the spherical distance to the corresponding class's basis vector, rather than learning a large FC weight matrix.

So in summary, the main hypothesis is that EBVs can achieve competitive accuracy to FC+softmax classifiers while being much more parameter-efficient and scalable as the number of classes grows. The experiments on ImageNet and other datasets aim to validate this hypothesis.
