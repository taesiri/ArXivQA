# [A comparison of Human, GPT-3.5, and GPT-4 Performance in a   University-Level Coding Course](https://arxiv.org/abs/2403.16977)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper evaluates the performance of different ChatGPT variants on university-level physics coding assignments, comparing them to actual student work. The aim is to assess whether AI-generated code can match or surpass the quality of code written by physics students. 

The coding assignments come from a 10-week laboratory skills and electronics course at Durham University. The assignments involve writing Python code for simulations, data analysis, and plotting. 50 student submissions were collected and compared to AI-generated submissions from GPT-3.5 and GPT-4 models, both with and without prompt engineering enhancements. This gave 6 categories - the 4 AI variants, a mixed student/AI category, and the student group. 

The assignments were converted from Jupyter notebooks into text documents as input for the AI, with some pre-processing steps taken to facilitate AI completion. Prompt engineering was used to further optimize some AI inputs. In total 100 submissions with 14 plots each were evaluated, giving 300 data points that were blindly assessed by 3 independent markers.  

Results showed students scored the highest with 91.9%, significantly above the top AI score of 81.1% from GPT-4 with prompt engineering. Prompt engineering gave significant boosts for both GPT-3.5 and GPT-4. The mixed student/AI category scored lower than GPT-4 alone. Markers were also reasonably accurate at identifying AI versus human authorship, with 85.3% binary accuracy on average.  

The paper concludes that while advanced AI is approaching student levels, it still lags behind in this physics coding scenario. But the improving trajectory suggests AI could match students soon. The study highlights the need to reassess educational coding objectives as AI collaborative tools emerge. It also demonstrates that human markers can currently discern AI-generated work.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

This paper evaluates the performance of ChatGPT variants GPT-3.5 and GPT-4, with and without prompt engineering, against solely student work and a mixed category with both student and GPT-4 contributions in university-level physics coding assignments, finding that while the AI approaches the quality of students' work, students still significantly outperform even the best AI category.


## What is the main contribution of this paper?

 The main contribution of this paper is evaluating the performance of ChatGPT variants (GPT-3.5 and GPT-4) on university-level physics coding assignments, both with and without prompt engineering, against student work. The key findings are:

1) Students still outperformed even the best AI category (GPT-4 with prompt engineering) by a statistically significant margin, scoring 91.9% on average compared to the AI's 81.1%. However, the gap is closing from GPT-3.5 to GPT-4.

2) Prompt engineering, such as simplifying instructions and providing more guidance to the AI, significantly improved the scores for both GPT-4 and GPT-3.5. 

3) Human evaluators were reasonably accurate (85.3% on average) in identifying AI-generated submissions, noting tendencies like slightly misaligned plots. More advanced models with prompt engineering produced output closest to human work.

4) The study highlights the steady improvements of LLMs on practical coding tasks, the importance of accounting for human intervention when evaluating AI abilities, and the need to reassess educational coding assessments as AI advances.

In summary, the key contribution is a robust, blinded evaluation demonstrating that despite rapid progress, contemporary LLMs have not yet matched human proficiency on university physics coding problems, though they may be approaching parity.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, the keywords or key terms associated with it appear to be:

ChatGPT, GPT-4, Coding, Benchmark

These keywords are listed in the \keywords section of the paper:

\keywords{ChatGPT \and GPT-4 \and Coding \and Benchmark}

So the key terms summarizing the focus of this paper seem to be ChatGPT, GPT-4, Coding, and Benchmark. The paper examines the performance of ChatGPT variants like GPT-3.5 and GPT-4 on university-level physics coding assignments, using a benchmarking approach to compare them to student work. So these keywords effectively summarize the key topics and aims of the research.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions pre-processing the assignment notebooks when converting them into input for the AI models. Could you elaborate on the specific pre-processing steps taken and why they were necessary? Do you think the pre-processing may have given the AI models an unfair advantage compared to the human students?

2. Prompt engineering appears to have significantly boosted the performance of the AI models, especially GPT-3.5. Could you describe in more detail the specific prompt engineering techniques used? Do you think relying too heavily on prompt engineering compromises the integrity of comparing unaided AI capabilities versus human intelligence?  

3. The paper evaluates AI-generated plots based on their layout, alignment, color schemes etc. Could you discuss in more depth the specific attributes you looked for when identifying AI versus human generated plots? Were there any other subtle clues that distinguished AI plots?

4. You found that human markers could identify AI-generated content more accurately for coding assignments compared to previous studies on physics essays. What factors do you think account for this difference? Is there something inherently more distinguishing about code and plots versus prose?

5. The mixed category comprising both human and AI contributions scored lower than the best AI-only category. What explanations do you have for this unexpected result? Could it be attributed to how the human contributions were selected or combined with the AI work?

6. You suggest implementing additional barriers in assignments that make pre-processing more demanding for AI. Could you propose some specific ways coding tasks could be adapted to make AI pre-processing less straightforward? How might this impact the assignments for human students?

7. The paper evaluates AI on a specific 10-week coding course for physics students. Do you think your findings would generalize to other STEM coding courses? What about computer science courses that focus more on software design? 

8. You used particular AI models in this study. How do you think more recent models like GPT-3.5 Turbo and GPT-4 compare in their performance on similar coding assignments? Would your conclusions still hold?

9. The paper acknowledges limitations around ensuring the student code category was strictly human-authored. Do you have suggestions for how student code provenance could be more rigorously confirmed in future studies? What checks could be implemented?

10. You recommend reassessing educational strategies in light of advancing AI capabilities. Could you propose specific ways physics coding curriculums could innovate to account for increasingly proficient AI models? Might AI tutoring become an asset rather than just an integrity threat?
