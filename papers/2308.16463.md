# Sparkles: Unlocking Chats Across Multiple Images for Multimodal   Instruction-Following Models

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the key research question this paper seems to address is: How can we develop multimodal instruction-following models that can understand and reason across multiple images in an open-ended dialogue setting? Specifically, the paper introduces a new model called SparklesChat that aims to unlock chats across multiple images for multimodal instruction-following. The key capabilities and components highlighted include:- Integrating multiple images at the word level within a dialogue, enabling more fine-grained integration compared to prior approaches like MiniGPT-4 that take a single image concatenated with a sentence. - Presenting SparklesDialogue, a novel machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions to support SparklesChat's training.- Introducing SparklesEval, a specialized benchmark to quantitatively assess a model's multimodal conversational competence across images and dialogue turns. - Validating SparklesChat's effectiveness through quantitative experiments on vision-language tasks like BISON and NLVR2, where it outperforms MiniGPT-4, as well as scoring high on the proposed SparklesEval.- Demonstrating SparklesChat's applicability through qualitative free-form conversational scenarios.In summary, the central hypothesis appears to be that integrating multiple images at the word level and training on a tailored multi-image dialogue dataset can enhance multimodal models' conversational capabilities across diverse images in an open-ended setting - as evaluated both quantitatively and qualitatively. The SparklesChat model and associated dataset and benchmark aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on the abstract, the main contributions of this paper appear to be:1. Introducing SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images.2. Presenting SparklesDialogue, the first machine-generated dialogue dataset tailored for fine-grained, word-level interactions between multiple images and text to support the training of SparklesChat. 3. Constructing SparklesEval, a specialized GPT-assisted benchmark to quantitatively assess a model's conversational competence across images and dialogue turns.In particular, SparklesChat seems innovative in its ability to integrate multiple images at the word level within a dialogue context, facilitating more natural human-like multimodal interactions compared to prior approaches. The proposed SparklesDialogue dataset enables the development of this capability by providing training data focused on multi-image, word-level dialogues.And SparklesEval offers a way to quantitatively evaluate models on their conversational competence when handling multiple images across dialogue turns, overcoming limitations of human evaluations.In summary, the core novelty appears to be the introduction of SparklesChat, along with the supporting training data in SparklesDialogue and evaluation benchmark SparklesEval - collectively aimed at advancing multimodal dialogue models' understanding and reasoning across diverse images and conversational turns.
