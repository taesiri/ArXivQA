# [RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent   Geometry and Texture](https://arxiv.org/abs/2305.11337)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we leverage powerful natural language and 2D diffusion models to synthesize new 3D indoor scenes that match the structure of a given low-quality scanned mesh, while allowing control over the style/appearance through textual prompts?In other words, the key research goals are:1) Developing a method to generate high-quality 3D geometry and textures for indoor scenes based on an input mesh.2) Allowing control over the style/appearance of the generated scene using natural language prompts. 3) Ensuring the generated geometry and textures are properly aligned and consistent across the scene.4) Demonstrating the approach on real indoor meshes scanned by smartphones.The main hypothesis appears to be that by treating the scene as a whole and generating a cubemap texture, then optimizing both geometry and texture jointly, it should be possible to produce high-quality stylized results that match the structure of the input mesh but exhibit the style indicated by the text prompt. The experiments on real data aim to validate this hypothesis.In summary, the paper introduces a framework to edit and stylize scanned 3D scenes using natural language and 2D diffusion models, with a focus on controlling both geometry and coherent textures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel framework to stylize and edit 3D indoor scenes based on text prompts. The key ideas include:- Developing a geometry-guided diffusion scheme to generate consistent 3D scene texture from 2D diffusion models and text prompts. It generates a cubemap image for the scene first, then updates uncovered areas. - Jointly optimizing the mesh geometry and texture by utilizing monocular depth estimation as pseudo supervision. This aligns the smooth areas of the mesh with the generated images.- Conducting experiments on real indoor meshes scanned by smartphones. The results demonstrate the effectiveness of the proposed framework for editing both geometry and texture based on textual prompts.In summary, the main contribution is developing a framework that leverages 2D diffusion models to refine and restyle scanned 3D meshes according to text prompts. The key novelty lies in the geometry-guided diffusion and mesh optimization scheme tailored for this task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method to synthesize a new 3D indoor scene with coherent geometry and texture from an input low-quality scanned mesh and text prompt, by utilizing 2D image diffusion models to generate consistent scene textures and optimizing the mesh geometry jointly with the textures.
