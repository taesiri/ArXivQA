# [Authorship Attribution in Bangla Literature (AABL) via Transfer Learning   using ULMFiT](https://arxiv.org/abs/2403.05519)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Authorship attribution in Bangla literature lacks significant work due to the language's complex structure. Existing systems also suffer from:
1) Dependence on manual feature engineering 
2) Lack of resources and benchmark datasets
3) Poor scalability with increasing number of authors, especially with few samples per author
4) Require many samples per author for deep learning models to work well

Proposed Solution: 
The paper proposes using transfer learning with language models to address the limitations. The AWD-LSTM architecture is used as it provides optimizations to generalize well for language modeling. The process has 3 main steps:

1) Pre-train language model on large Bangla corpora like news articles or Wikipedia in an unsupervised manner to learn linguistic patterns
2) Fine-tune the language model on texts from an authorship attribution dataset to adapt to authorial writing styles  
3) Add a classifier on top and train in a supervised manner to predict authors 

Main Contributions:

1) Propose an effective transfer learning framework using language models for authorship attribution in Bangla, which achieves state-of-the-art performance

2) Introduce BAAD16 - the largest Bangla authorship attribution dataset with 16 authors and 13.4 million words 

3) Analyze the effects of different tokenization methods like word, subword and character level

4) Demonstrate the framework's robustness against varying number of authors and fewer samples per author compared to previous models

5) Release multiple pre-trained language models and code to facilitate future Bangla NLP research

The subword tokenized model with news corpus pre-training achieves the best performance of 99.8% accuracy on BAAD16 dataset, outperforming previous models by a large margin.
