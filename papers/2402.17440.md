# [Principled Architecture-aware Scaling of Hyperparameters](https://arxiv.org/abs/2402.17440)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Choosing suitable hyperparameters like learning rate and initialization scheme is crucial for training high-quality deep neural networks. However, most existing principles or optimization methods for setting these hyperparameters do not take into account the impact of the neural architecture itself. Specifically, they ignore how factors like network depth, width, connectivity patterns and convolutional kernel sizes affect the choice of optimal hyperparameters.

Proposed Solution: 
This paper proposes principles to derive architecture-aware learning rates and initialization schemes for deep networks. The networks are represented as directed acyclic graphs (DAGs) that encode arbitrary connectivity patterns and heterogeneous layer types.

The key ideas are:

1) Derive a modified fan-in initialization scheme where the variance scales with the in-degree of each layer. This normalization preserves the flow of information through irregular DAG architectures.  

2) Analyze how changes in neuron pre-activations during initial training depend on the depth and learning rates. Goal is for pre-activations to change by O(1) for network stability.

3) For DAG networks, show the learning rate scales as (sum of path depths cubed)^(-0.5). For CNNs, additional q^(-1) dependence on kernel size q.

Main Contributions:

- New principles for setting architecture-aware learning rates and initialization that generalize across diverse MLPs and CNNs with arbitrary topological patterns. 

- Verify improved performance on range of architectures.

- Demonstrate that simply better training networks can easily change rankings in NAS benchmarks used to evaluate AutoML algorithms. Thus rankings may not reliably reflect network quality.

The principles precisely characterize dependence on depth, width and connectivity patterns for stability and optimization. Framework is useful both for training as well as reevaluating architectures and benchmarks.
