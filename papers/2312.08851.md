# [Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework   on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous   Modalities](https://arxiv.org/abs/2312.08851)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces Achelous++, a versatile water-surface perception framework for autonomous vessels that focuses on multi-task learning through vision-radar fusion while emphasizing low power consumption for edge devices. Achelous++ supports five concurrent perception tasks: object detection, semantic segmentation, drivable area segmentation, waterline segmentation, and radar point cloud segmentation. It incorporates a range of state-of-the-art deep learning components and facilitates easy integration and evaluation of custom models. A key contribution is a novel radar convolution operator to effectively extract features from sparse, irregular radar point clouds projected onto a 2D image plane. The paper also proposes a heterogeneous modalities aware pruning technique to slim down models for faster, lower-power inference. Experiments demonstrate Achelous++'s state-of-the-art accuracy across tasks compared to other fusion and non-fusion based models, while striking an effective balance between speed and power efficiency. The code is publicly available to facilitate further research into multi-modal perception for autonomous marine vehicles. By emphasizing low computational costs, Achelous++ exemplifies an environmentally-conscious approach to perception system design.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Current water-surface perception models are complex, consume high power, and rely on high-performance GPUs, increasing carbon emissions. This is counter to sustainable urban development goals.  
- Low-power and lightweight models using vision-radar fusion and multi-task learning can address this but balancing performance, speed and efficiency is challenging.

Proposed Solution:
- Achelous++ - a multi-task panoptic perception framework for water-surfaces using vision-radar fusion optimized for edge devices.
- Supports multi-modal input, flexible network architectures, comprehensive evaluation metrics and model pruning.
- Introduces radar convolution to effectively model radar point cloud features.
- Proposes HA-SynFlow pruning method to prune different modalities based on saliency.

Main Contributions:
- Achieves state-of-the-art accuracy on WaterScenes dataset while optimizing speed (up to 28 FPS on AGX Orin) and power (741J per sample on Orin).
- Outperforms parallel single-task models in speed and power consumption.  
- Provides highly flexible framework to develop customized models and evaluation functions.
- Effectively handles small, occluded targets by fusing vision and radar.
- The code is open-sourced to facilitate low-power perception research.

In summary, the paper presents a highly optimized and accurate multi-modal multi-task perception framework for water-surfaces that pushes state-of-the-art while being cognizant of efficiency and environmental impact. The flexibility of the framework and model pruning technique facilitate customized low-power model development.
