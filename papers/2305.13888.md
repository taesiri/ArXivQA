# PaD: Program-aided Distillation Specializes Large Models in Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is using program-aided distillation to transfer reasoning abilities from large language models (LLMs) to smaller models. The authors propose synthesizing program-aided reasoning data from LLMs and then fine-tuning smaller models on this data to distill reasoning skills. The central hypothesis is that program-aided distillation can enable smaller models to achieve strong performance on reasoning tasks while being much more efficient in terms of model size and data requirements compared to prior distillation methods based on chain-of-thought reasoning.Specifically, the paper investigates whether program-aided distillation can:- Produce small models that match or exceed the reasoning performance of large models on math word problems, despite using significantly fewer parameters. - Achieve better data efficiency and training efficiency compared to prior distillation methods that use chain-of-thought reasoning data.- Overcome issues with faulty reasoning that can arise when using synthetic chain-of-thought data from LLMs.The experiments aim to validate whether program-aided distillation can effectively transfer reasoning skills to small models in a highly parameter and data efficient manner.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Proposing a novel method called Program-aided Distillation (PaD) to transfer reasoning abilities from large language models (LLMs) to smaller models. PaD generates synthetic reasoning program data from LLMs and uses it to fine-tune smaller models. 2. PaD can automatically filter out faulty reasoning samples from the synthetic data using a Python interpreter, which improves the quality of the fine-tuning data.3. Empirical results show that the smaller models fine-tuned via PaD can match or exceed the reasoning performance of certain larger LLMs on math word problems, while using significantly fewer parameters and less data.4. Analysis indicates that PaD increases training efficiency and is well-suited for smaller models. With only half the data size, it achieves comparable grammatical accuracy to an advanced LLM.5. The paper establishes state-of-the-art results on teaching reasoning skills to small models, despite using a model 11x smaller and data 3x smaller than prior work.In summary, the core contribution appears to be proposing PaD as an effective distillation method to transfer reasoning abilities from large to small models, while needing less data and compute resources. The empirical results demonstrate the advantages of PaD for specialized small models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method called Program-aided Distillation (PaD) to transfer reasoning abilities from large language models to smaller specialized models by using synthesized program-based reasoning data and automatically filtering faulty reasoning steps.
