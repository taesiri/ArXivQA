# PaD: Program-aided Distillation Specializes Large Models in Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is using program-aided distillation to transfer reasoning abilities from large language models (LLMs) to smaller models. The authors propose synthesizing program-aided reasoning data from LLMs and then fine-tuning smaller models on this data to distill reasoning skills. The central hypothesis is that program-aided distillation can enable smaller models to achieve strong performance on reasoning tasks while being much more efficient in terms of model size and data requirements compared to prior distillation methods based on chain-of-thought reasoning.Specifically, the paper investigates whether program-aided distillation can:- Produce small models that match or exceed the reasoning performance of large models on math word problems, despite using significantly fewer parameters. - Achieve better data efficiency and training efficiency compared to prior distillation methods that use chain-of-thought reasoning data.- Overcome issues with faulty reasoning that can arise when using synthetic chain-of-thought data from LLMs.The experiments aim to validate whether program-aided distillation can effectively transfer reasoning skills to small models in a highly parameter and data efficient manner.
