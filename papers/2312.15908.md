# [Decentralized Monte Carlo Tree Search for Partially Observable   Multi-agent Pathfinding](https://arxiv.org/abs/2312.15908)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of Multi-Agent Pathfinding (MAPF) in a decentralized, lifelong setting. Specifically, it considers the scenario where agents operate with only partial observability of the environment and other agents. The agents are assigned goals continuously as they complete previous ones. Existing solutions either rely on centralized planning which scales poorly or learnable policies which generalize poorly. The paper aims to develop a decentralized method that can efficiently solve challenging lifelong MAPF instances. 

Method:
The paper proposes a decentralized multi-agent Monte Carlo Tree Search (MCTS) method called MATS-LP. The key idea is to combine MCTS for long-term planning with a lightweight neural network policy for state evaluation. Each agent constructs an intrinsic Markov Decision Process (MDP) based on its partial observability and performs MCTS by simulating possible futures using the learned policy (called CostTracer). To scale planning, MATS-LP considers joint actions only for a few nearby agents and uses a greedy policy for distant agents. The CostTracer policy is trained using the PPO reinforcement learning algorithm with a compact network.

Main Contributions:
- A decentralized planning approach for lifelong MAPF using Monte Carlo tree search and a learned simulation policy
- A technique to construct intrinsic MDPs from partial observability and focus MCTS on nearby agents for scalability
- The CostTracer neural policy tailored for efficient state evaluation in MCTS using 161K parameters 
- Empirical evaluation showing MATS-LP outperforming state-of-the-art primal and learning based MAPF solvers on challenging random, maze, and warehouse maps

The main strengths of MATS-LP are its ability to find coordinated plans despite partial observability and its better generalization compared to learned policies. The results demonstrate the promise of combining search and learning for decentralized sequential decision making problems.
