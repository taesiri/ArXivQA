# [Improving Open Language Models by Learning from Organic Interactions](https://arxiv.org/abs/2306.04707)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we improve open domain conversational models by learning from real-world organic interactions and feedback data?The key hypothesis seems to be:By collecting and learning from organic user interactions and feedback in a public deployment, conversational models can be improved in terms of both conversational quality and safety.The paper details the analysis of a large dataset of organic user interactions with the BlenderBot 3 conversational agent. It studies techniques to leverage this data, including human feedback signals like thumbs up/down, to improve the model. The main contributions appear to be:1) Analysis of a large dataset of organic user interactions with BlenderBot 3. This includes over 350k conversations and over 6M utterances.2) Development of a reward model to label conversational responses as good or bad using the organic feedback data.3) Experiments with different techniques like the Cringe Loss to improve conversational quality and safety from the organic interaction data.4) Introduction of the BlenderBot 3x model, which outperforms BlenderBot 3 in terms of response quality and safety when evaluated on the organic user data.5) Release of the organic interaction dataset to spur further research.So in summary, the central hypothesis is that conversational models can be improved by collecting and learning from organic user interactions, which the paper tests through analysis, model training experiments, and human evaluations. The release of the dataset also aims to enable further research in this direction.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Analyzing organic interaction and feedback data collected from the public BlenderBot 3 deployment. This consists of over 350k conversations with over 6M utterances and 155k feedback responses. The authors provide detailed analysis on the quality of human and model utterances, as well as the quality of the organic feedback. 2. Developing techniques to learn improved dialogue models from this organic interaction data, using a trained reward model to provide signals for good and bad responses. In particular, they utilize the Cringe loss to reduce bad responses while increasing good ones.3. Demonstrating improved results with their BlenderBot 3x model, which is trained on the organic interaction data. In human evaluations, BlenderBot 3x produces better and safer responses compared to the original BlenderBot 3 model.4. Publicly releasing the collected organic interaction data to spur further research and progress in this area by the broader research community.In summary, the core contribution is showing how organic interactions from real users can be leveraged in a responsible manner to incrementally improve dialogue models over time, both in terms of conversational quality and safety. The public data release also enables further research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents BlenderBot 3x, an improved conversational AI model trained on crowdsourced data as well as organic user interactions from the BlenderBot 3 deployment, with techniques to learn from helpful teachers while avoiding adversarial inputs; BlenderBot 3x is shown to produce better and safer responses compared to its predecessor BlenderBot 3.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- The paper focuses on improving open dialogue models by learning from organic user interactions and feedback data collected during deployment of the BlenderBot 3 system. This is a novel approach compared to most prior work that relies on more controlled crowdsourced datasets for training. - The concept of deploying dialogue systems to real users and learning from that interaction data has been explored before (e.g. in the LIGHT system), but not at the scale of hundreds of thousands of conversations as done here. The release of this large organic dialogue dataset is an important contribution.- The paper explores techniques like the Cringe loss to effectively learn from both positive and negative examples in the organic data. This compares favorably to prior work that mainly uses standard supervised learning on crowdsourced data.- For improving safety, the paper studies approaches like the baked-in safety method and balancing positive and negative examples. This contrasts with much prior work that focuses solely on crowdsourced adversarial safety datasets which may not cover real user distributions.- The focus on iterative learning over multiple rounds of deployment data collection and model retraining is more advanced than prior one-off supervised learning approaches. This methodology is critical for continually improving models based on user feedback.- Comparisons are made to other large conversational models like BlenderBot 3 and LaMDA. The improvements from fine-tuning on deployment data demonstrate the value of this technique versus just scaling up model size.Overall, the rigorous methodology and experiments on real-world data advance the state-of-the-art in building better open dialogue models through human-in-the-loop interaction. The public data release also enables the wider research community to build on this work.
