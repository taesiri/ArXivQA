# [Improving Open Language Models by Learning from Organic Interactions](https://arxiv.org/abs/2306.04707)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we improve open domain conversational models by learning from real-world organic interactions and feedback data?The key hypothesis seems to be:By collecting and learning from organic user interactions and feedback in a public deployment, conversational models can be improved in terms of both conversational quality and safety.The paper details the analysis of a large dataset of organic user interactions with the BlenderBot 3 conversational agent. It studies techniques to leverage this data, including human feedback signals like thumbs up/down, to improve the model. The main contributions appear to be:1) Analysis of a large dataset of organic user interactions with BlenderBot 3. This includes over 350k conversations and over 6M utterances.2) Development of a reward model to label conversational responses as good or bad using the organic feedback data.3) Experiments with different techniques like the Cringe Loss to improve conversational quality and safety from the organic interaction data.4) Introduction of the BlenderBot 3x model, which outperforms BlenderBot 3 in terms of response quality and safety when evaluated on the organic user data.5) Release of the organic interaction dataset to spur further research.So in summary, the central hypothesis is that conversational models can be improved by collecting and learning from organic user interactions, which the paper tests through analysis, model training experiments, and human evaluations. The release of the dataset also aims to enable further research in this direction.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Analyzing organic interaction and feedback data collected from the public BlenderBot 3 deployment. This consists of over 350k conversations with over 6M utterances and 155k feedback responses. The authors provide detailed analysis on the quality of human and model utterances, as well as the quality of the organic feedback. 2. Developing techniques to learn improved dialogue models from this organic interaction data, using a trained reward model to provide signals for good and bad responses. In particular, they utilize the Cringe loss to reduce bad responses while increasing good ones.3. Demonstrating improved results with their BlenderBot 3x model, which is trained on the organic interaction data. In human evaluations, BlenderBot 3x produces better and safer responses compared to the original BlenderBot 3 model.4. Publicly releasing the collected organic interaction data to spur further research and progress in this area by the broader research community.In summary, the core contribution is showing how organic interactions from real users can be leveraged in a responsible manner to incrementally improve dialogue models over time, both in terms of conversational quality and safety. The public data release also enables further research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents BlenderBot 3x, an improved conversational AI model trained on crowdsourced data as well as organic user interactions from the BlenderBot 3 deployment, with techniques to learn from helpful teachers while avoiding adversarial inputs; BlenderBot 3x is shown to produce better and safer responses compared to its predecessor BlenderBot 3.
