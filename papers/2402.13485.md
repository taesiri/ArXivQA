# [ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel   Decoding](https://arxiv.org/abs/2402.13485)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Autoregressive decoding in large language models (LLMs) suffers from low parallelism and efficiency. 
- Existing parallel decoding methods like Medusa struggle to maintain contextual relationships between tokens due to independent token prediction. This leads to significant verification overhead, especially for large batch sizes.

Proposed Solution:
- The paper proposes ProPD, an efficient parallel decoding framework for LLMs based on dynamic token tree pruning and generation.

Key Ideas:
- Early Pruning: Leverages early LLM layers to prune unlikely tokens. This maintains accuracy while reducing verification computation by over 2x.
- Dynamic Tree Generation: Balances computation and parallelism in real-time across different batch sizes and tasks by adaptively resizing the token tree.

Main Contributions:
- An early pruning algorithm that leverages predictive capabilities of early LLM layers to eliminate unlikely tokens.
- A dynamic token tree generation algorithm that adapts the tree size based on real-time estimates of verification overhead and token acceptance probabilities.
- Evaluations across diverse datasets, LLM sizes and batch sizes showing 1.1-3.2x speedup over state-of-the-art parallel decoding algorithms.

In summary, ProPD introduces novel techniques for efficient parallel decoding of LLMs, enabling speedups through dynamic pruning and tree generation tailored to different decoding scenarios. Both the early pruning and dynamic generation are shown to provide significant performance gains.
