# [Tokenization Is More Than Compression](https://arxiv.org/abs/2402.18376)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Tokenization is a critical step in NLP that bridges raw text and language models. There are conflicting views on why byte-pair encoding (BPE) is an effective tokenizer, with some arguing it stems from compressing text into fewer tokens. 

- This paper tests the hypothesis that using fewer tokens leads to better downstream model performance. 

Methodology:
- The authors introduce PathPiece, a new tokenizer that minimizes the number of tokens needed to represent a corpus, allowing them to isolate the impact of token count.

- They divide tokenization into 3 stages: (1) pre-tokenization rules (2) vocabulary construction (3) segmentation that maps text to vocabulary tokens.

- They train 64 language models with varying configurations of these stages, including BPE, WordPiece and other methods. The models range from 350M to 2.4B parameters.

Findings:
- No statistically significant best approach is found. The top 6 tokenizers have comparable performance, despite different token counts. This casts doubt on the belief that lower token count is the reason for BPE's effectiveness.

- Controlled experiments reveal pre-tokenization and using BPE for vocabulary initialization provide gains. Shortest path segmentation helps for BPE vocabularies but hurts for others. 

- Vocabulary size in the 30K-50K range has little impact. Larger models show different relative tokenizer performance, with a group of top performers.

Contributions:
- Open-sourced token vocabularies, PathPiece implementation and 64 trained language models to enable further research.

- Evidence against prevailing theory on BPE's effectiveness and new insights into the design of tokenizers. Practical advice on vocabulary size, pre-tokenization methods and using BPE for initialization.

In summary, this comprehensive study advances our understanding of the tokenization process and its impact on downstream tasks. The authors systematically test common beliefs and offer both theoretical and practical contributions.
