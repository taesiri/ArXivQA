# [Benchmarks for Retrospective Automated Driving System Crash Rate   Analysis Using Police-Reported Crash Data](https://arxiv.org/abs/2312.13228)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Automated driving systems (ADS) are being deployed in ride-hailing services, enabling the opportunity for retrospective evaluation of their safety impact. To do this, ADS crash rates need to be compared to benchmark crash rates representing human driving performance. 

- Generating valid benchmarks is challenging due to:
  1) Analytical errors like comparing crash-level rates to vehicle-level rates
  2) Biases in the raw data like underreporting of crashes
  3) Biases from analytical choices like mismatching ADS and human driving conditions

- This paper reviews prior benchmarking studies and identifies common challenges and biases when using publicly available police-reported crash data paired with mileage data to generate benchmarks.

Solution:
- Leverage police-reported crashes from multiple regions where ADS operate: National, Phoenix, San Francisco, Los Angeles
- Pair crashes with public mileage data sources to generate vehicle-level crash rates
- Handle underreporting to enable comparison to ADS "any property damage or injury" reporting threshold 
- Subset data to passenger vehicles on surface streets to match ADS operational design domain

Contributions:
- Provide updated benchmarks suited for currently deployed ADS fleets
- Demonstrate effect of properly controlling for region, vehicle type, road type
- Use multiple underreporting correction methods to generate "any property damage or injury" benchmarks
- Identify common analyst errors and data biases when generating benchmarks
- Perform power analysis to understand miles needed to demonstrate ADS safety impact
- Offer best practices and guidance to improve consensus on benchmark generation

In summary, this paper leverages publicly available data to produce updated benchmarks for evaluating ADS safety, while highlighting biases and best practices to enable more accurate, consensus-based benchmarking.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

This paper presents properly-matched benchmarks for evaluating automated driving safety using publicly available data, addresses common methodological errors that can bias results, demonstrates the effect of key analysis choices, and calls for collaborative advancement of benchmarking to support the responsible deployment of automated vehicles.


## What is the main contribution of this paper?

 This paper makes several key contributions:

1. It presents updated human driving benchmarks for multiple geographic regions where automated driving systems (ADS) are currently deployed. These benchmarks are generated at multiple crash severity levels (e.g. any property damage or injury, police-reported, any-injury-reported, etc.) to enable comparison to ADS crash data.

2. It identifies and discusses several key challenges and potential biases that can occur when generating human benchmarks from police-reported crash data, including:
- Comparing crash-level vs vehicle-level rates 
- Reporting thresholds
- Underreporting
- Operational design domain (ODD) matching 
- Mismatch between crash data and mileage data

3. It demonstrates quantitative effects of properly controlling for geographic region, vehicle type, and road type when generating benchmarks. Failing to control for these can bias results by underestimating the actual human crash rate.

4. It presents a methodology using established underreporting estimates to adjust police-reported data to generate "any property damage or injury" benchmarks comparable to ADS crash reporting requirements.

5. It performs a power analysis to estimate the number of miles automated vehicles would likely need to drive to demonstrate statistically significant differences in crash rates compared to the benchmarks.

So in summary, it provides updated benchmarks, identifies potential pitfalls in this process, and contributes methodologies to properly generate comparisons between ADS and human driving. The aim is to help the research community reach consensus on measuring ADS safety impact.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the main keywords and key terms associated with this paper include:

- Automated Driving Systems
- Safety Impact Analysis 
- Traffic Safety
- Human Benchmarks
- Crash Rates
- Police-Reported Crash Data
- Underreporting Correction
- Operational Design Domain (ODD)
- Retrospective Evaluation
- Statistical Power Analysis

The paper discusses using police-reported crash data to create benchmarks for evaluating the safety impact of automated driving systems. It examines challenges like underreporting in the data and properly matching the human driving population and conditions to the operational design domain of the ADS vehicles. Key methods include applying underreporting correction factors and controlling for geographic region, vehicle type, and road type when generating the human benchmarks. The paper also presents a statistical power analysis for determining how many miles are needed to demonstrate ADS safety impact. Overall, it aims to provide validated benchmarks to allow proper evaluation of ADS technology safety performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. This paper relies primarily on police-reported crash data to generate benchmarks. What are some of the key limitations when using this data source, especially when examining lower severity crashes? How did the authors attempt to address these limitations?

2. The paper mentions both "information bias" and "bias introduced through analytical choices" when generating crash rate benchmarks. Can you explain the key differences between these two types of biases? Provide at least two examples of each from the paper.  

3. When generating the "any property damage or injury" crash rates, two independent underreporting adjustment methods were used - the "Blincoe et al." and "Blanco et al." estimates. What data sources and methods were used to derive these two estimates and what are some potential limitations of relying on these national estimates for the specific regions examined?

4. Statistical power analyses were conducted to determine the approximate number of miles needed to demonstrate a statistically significant difference in crash rates between a fictitious ADS and the human benchmarks. What assumptions were made in this analysis regarding variance and how could accounting for uncertainty in the underreporting adjustments impact the estimates?

5. The paper demonstrates quantitatively that failing to properly control for geographic region, vehicle type, and road type when generating benchmarks can bias results. For each of these, explain the effect on crash rates and why controlling for them is important for an "apples-to-apples" comparison.

6. Besides the insurance data analysis done by Swiss Re, discuss another crash data source that could complement the police-reported data used here, especially for lower severity crashes. What are some key advantages and limitations of this source?

7. The statistically powered analysis shows that even hundreds of millions of miles may not be enough to detect differences in severe crash rates like fatalities. Discuss the usefulness of injury risk-based analyses as an alternative approach and how they differ from relying solely on crash outcomes.

8. Explain the concept of "volunteer bias" or "self-selection bias" and how differences between early ADS riders and the general driving population could confound comparisons of crash outcomes between the two groups. 

9. Besides controlling for road type, vehicle type, and geographic region, name three additional driving environment factors that are known to influence crash rates but could not be accounted for in this analysis due to data limitations. 

10. The researchers state "Researchers, regulators, industry, and experts should consider these results, and have an opportunity to continue to elevate the state-of-the-art of benchmarking.‚Äù In your opinion, what is one aspect of the benchmarking methodology that still needs advancement by the research community?
