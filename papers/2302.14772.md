# [PA&amp;DA: Jointly Sampling PAth and DAta for Consistent NAS](https://arxiv.org/abs/2302.14772)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is improving the weight-sharing supernet training in one-shot neural architecture search (NAS). Specifically, it aims to address the issue that the shared weights in the supernet suffer from inconsistent gradient directions when optimizing different sub-models, leading to large gradient variance and poor ranking consistency. The main hypothesis is that by explicitly minimizing the gradient variance of the supernet training, through jointly optimizing the sampling distributions of paths and data, the supernet performance and ranking consistency can be improved.In summary, the central research question is:How to reduce the gradient variance during supernet training in one-shot NAS to improve the ranking consistency and search performance?And the key hypothesis is:Jointly optimizing the path and data sampling distributions by minimizing the gradient variance will improve supernet training and enhance ranking consistency in one-shot NAS.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes to reduce the gradient variance during one-shot NAS supernet training by jointly optimizing the path and data sampling distributions. 2. It derives the theoretical relationship between the supernet gradient variance and the sampling distributions, revealing that the optimal sampling probability is proportional to the normalized gradient norm.3. Based on the derived relationship, it uses the normalized gradient norm as the importance indicator and adopts an importance sampling strategy for path and data during supernet training. 4. The proposed method, named PA&DA, achieves lower gradient variance and better ranking consistency compared to baseline methods in experiments. It obtains state-of-the-art NAS performance on CIFAR-10 using DARTS search space and ImageNet using ProxylessNAS search space.5. Comprehensive ablation studies are provided to analyze the effect of different components and validate the benefits of reducing gradient variance.In summary, the key contribution is proposing an effective path and data importance sampling method during supernet training to reduce the gradient variance, which improves the ranking consistency and NAS performance. Theoretical analysis and experimental results demonstrate its efficacy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called PA&DA for neural architecture search that jointly optimizes the sampling distributions of path and data during supernet training to reduce gradient variance, leading to improved ranking consistency and search performance.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in neural architecture search:- This paper focuses on improving one-shot NAS methods based on weight sharing, which is an active area of research. Many recent works have pointed out issues with weight sharing hurting the ranking consistency, and have proposed techniques to mitigate this.- The key idea in this paper is to explicitly minimize the gradient variance during supernet training by optimizing the path and data sampling distributions. This provides a principled way to tackle the weight sharing issues, by reducing the noise in the gradients. - In contrast, other approaches like maintaining multiple weight copies or designing better sampling heuristics often rely more on heuristics or increase training cost. The optimization framework proposed here is elegant and has strong theoretical motivation.- For path sampling, they theoretically show the optimal sampling distribution is proportional to the gradient norm and use that for importance sampling. This is a novel analysis tailored to NAS.- For data sampling, they employ techniques from prior work on variance reduction, but adapt it to the NAS setting. Jointly optimizing data and path sampling is also novel for NAS.- In experiments, the method achieves state-of-the-art performance on NAS benchmarks like NAS-Bench-201 and DARTS space on CIFAR-10, demonstrating the effectiveness of the ideas. It also generalizes well to larger search spaces like on ImageNet. - Overall, the principled optimization framework for tackling weight sharing issues is a noteworthy contribution. The method is simple, efficient, and outperforms prior approaches, making it a valuable approach for one-shot NAS. The analysis and results are thorough and add nicely to the literature.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing methods to further reduce the gradient variance during supernet training. The authors propose jointly optimizing the path and data sampling distributions to reduce variance, but suggest exploring more techniques to lower variance and improve supernet training.- Exploring different search spaces and datasets. The authors evaluate their method on NAS-Bench-201, DARTS, and ProxylessNAS spaces, using CIFAR and ImageNet datasets. They suggest applying their method to additional search spaces and datasets. - Investigating other potential causes of inconsistency in one-shot NAS besides gradient variance. The authors focus on gradient variance, but suggest examining other factors that could improve consistency.- Extending the theoretical analysis. The authors derive a relationship between sampling distributions and gradient variance. They suggest further theoretical analysis, such as proving convergence guarantees.- Studying the effect of different schedules for the smoothing parameters. The authors use a simple linear schedule, but suggest investigating other schedules like exponential decay.- Reducing the computation costs further. The authors' method is efficient, but they suggest exploring ways to lower costs even more. - Combining their method with weight inheritance techniques. The authors suggest combining their sampling approach with weight inheritance methods for potential further improvements.In summary, the main future directions are developing techniques to lower gradient variance, applying the method to new search spaces/datasets, extending the theory, optimizing hyperparameters like smoothing schedules, further reducing computations, and integrating their method with other NAS techniques.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper proposes a new method called PA&DA for improving neural architecture search (NAS) based on weight-sharing supernets. It points out that weight-sharing leads to inconsistent gradient updates during supernet training, which hurts ranking consistency when evaluating submodels. To address this, PA&DA jointly optimizes the path and data sampling distributions during training to minimize gradient variance. It shows that the optimal sampling probability is proportional to the normalized gradient norm for paths and data. Based on this, PA&DA uses an importance sampling strategy to sample paths and data according to their gradient norms. Experiments on NAS Bench 201, DARTS, and ProxylessNAS spaces demonstrate that PA&DA improves supernet ranking consistency, search efficiency, and accuracy of found architectures compared to prior methods. The main contributions are a theoretical analysis of optimal sampling for variance reduction, and a practical importance sampling approach for paths and data during supernet training in NAS.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper proposes a new method called PA&DA for improving neural architecture search (NAS) based on the weight-sharing mechanism. The key idea is to reduce the gradient variance during supernet training by jointly optimizing the path and data sampling distributions. The authors first explain that large gradient variance occurs with weight-sharing supernets, which harms the ranking consistency. They then derive the relationship between gradient variance and sampling distributions, showing the optimal sampling probability is proportional to the normalized gradient norm. Based on this, they use the gradient norm to perform importance sampling for paths and data during supernet training. The method only requires small extra computation cost but effectively reduces variance and improves generalization. Experiments on NAS benchmarks demonstrate PA&DA achieves better ranking consistency and higher accuracy than prior NAS methods. The key advantages are lower gradient variance during training and improved trainability of paths, resulting in more reliable architecture search.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a method called PA&DA to improve the consistency of one-shot neural architecture search. The key ideas are:1) They find that the weight-sharing mechanism used in one-shot NAS leads to large gradient variance during supernet training, which degrades the ranking performance. 2) To address this, they propose to jointly optimize the sampling distributions of path (PA) and data (DA) to minimize the gradient variance. 3) They theoretically derive that the optimal sampling probability is proportional to the normalized gradient norm of path and data.4) Hence, they use the normalized gradient norm to guide the importance sampling of path and data during supernet training. By reducing the gradient variance, the supernet generalization and ranking consistency are improved.In summary, the main method is to optimize the path and data sampling distributions based on gradient norm importance, in order to reduce the gradient variance and improve ranking consistency for one-shot NAS. The method is simple yet effective, requiring negligible extra computation cost.
