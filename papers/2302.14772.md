# [PA&amp;DA: Jointly Sampling PAth and DAta for Consistent NAS](https://arxiv.org/abs/2302.14772)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is improving the weight-sharing supernet training in one-shot neural architecture search (NAS). Specifically, it aims to address the issue that the shared weights in the supernet suffer from inconsistent gradient directions when optimizing different sub-models, leading to large gradient variance and poor ranking consistency. The main hypothesis is that by explicitly minimizing the gradient variance of the supernet training, through jointly optimizing the sampling distributions of paths and data, the supernet performance and ranking consistency can be improved.In summary, the central research question is:How to reduce the gradient variance during supernet training in one-shot NAS to improve the ranking consistency and search performance?And the key hypothesis is:Jointly optimizing the path and data sampling distributions by minimizing the gradient variance will improve supernet training and enhance ranking consistency in one-shot NAS.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes to reduce the gradient variance during one-shot NAS supernet training by jointly optimizing the path and data sampling distributions. 2. It derives the theoretical relationship between the supernet gradient variance and the sampling distributions, revealing that the optimal sampling probability is proportional to the normalized gradient norm.3. Based on the derived relationship, it uses the normalized gradient norm as the importance indicator and adopts an importance sampling strategy for path and data during supernet training. 4. The proposed method, named PA&DA, achieves lower gradient variance and better ranking consistency compared to baseline methods in experiments. It obtains state-of-the-art NAS performance on CIFAR-10 using DARTS search space and ImageNet using ProxylessNAS search space.5. Comprehensive ablation studies are provided to analyze the effect of different components and validate the benefits of reducing gradient variance.In summary, the key contribution is proposing an effective path and data importance sampling method during supernet training to reduce the gradient variance, which improves the ranking consistency and NAS performance. Theoretical analysis and experimental results demonstrate its efficacy.
