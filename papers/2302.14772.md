# [PA&amp;DA: Jointly Sampling PAth and DAta for Consistent NAS](https://arxiv.org/abs/2302.14772)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is improving the weight-sharing supernet training in one-shot neural architecture search (NAS). Specifically, it aims to address the issue that the shared weights in the supernet suffer from inconsistent gradient directions when optimizing different sub-models, leading to large gradient variance and poor ranking consistency. The main hypothesis is that by explicitly minimizing the gradient variance of the supernet training, through jointly optimizing the sampling distributions of paths and data, the supernet performance and ranking consistency can be improved.In summary, the central research question is:How to reduce the gradient variance during supernet training in one-shot NAS to improve the ranking consistency and search performance?And the key hypothesis is:Jointly optimizing the path and data sampling distributions by minimizing the gradient variance will improve supernet training and enhance ranking consistency in one-shot NAS.
