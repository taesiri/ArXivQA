# [What can Large Language Models do in chemistry? A comprehensive   benchmark on eight tasks](https://arxiv.org/abs/2305.18365)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper seeks to address is: What are the capabilities of large language models (LLMs) in solving practical chemistry tasks? The authors note that while LLMs have shown impressive reasoning abilities in natural language tasks, their application in scientific domains like chemistry is less explored. The paper aims to provide a systematic evaluation of how well the current state-of-the-art LLMs can perform on real-world chemistry tasks that require domain-specific understanding and reasoning. The key hypotheses tested through their benchmarking experiments are:1) LLMs may have limitations in solving chemistry tasks that rely heavily on accurately handling chemistry representations like SMILES strings.2) LLMs can show competitive performance compared to ML baselines designed for specific tasks when the task is framed as a classification/ranking problem rather than a generative problem.3) Performance of LLMs varies across models, with GPT-4 expected to outperform others.4) In-context learning with carefully selected demonstration examples can enhance LLM performance compared to zero-shot prompting.In summary, the central research question is assessing the capabilities of LLMs on diverse practical chemistry tasks through a rigorous benchmarking process, in order to gain insights into their strengths, limitations and potential for advancing chemistry research. The hypotheses focus on how task formulation, choice of LLM model, and in-context learning impact performance.
