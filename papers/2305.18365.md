# [What can Large Language Models do in chemistry? A comprehensive   benchmark on eight tasks](https://arxiv.org/abs/2305.18365)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper seeks to address is: What are the capabilities of large language models (LLMs) in solving practical chemistry tasks? The authors note that while LLMs have shown impressive reasoning abilities in natural language tasks, their application in scientific domains like chemistry is less explored. The paper aims to provide a systematic evaluation of how well the current state-of-the-art LLMs can perform on real-world chemistry tasks that require domain-specific understanding and reasoning. The key hypotheses tested through their benchmarking experiments are:1) LLMs may have limitations in solving chemistry tasks that rely heavily on accurately handling chemistry representations like SMILES strings.2) LLMs can show competitive performance compared to ML baselines designed for specific tasks when the task is framed as a classification/ranking problem rather than a generative problem.3) Performance of LLMs varies across models, with GPT-4 expected to outperform others.4) In-context learning with carefully selected demonstration examples can enhance LLM performance compared to zero-shot prompting.In summary, the central research question is assessing the capabilities of LLMs on diverse practical chemistry tasks through a rigorous benchmarking process, in order to gain insights into their strengths, limitations and potential for advancing chemistry research. The hypotheses focus on how task formulation, choice of LLM model, and in-context learning impact performance.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors establish a comprehensive benchmark to evaluate the abilities of large language models (LLMs) like GPT on a diverse range of chemistry tasks. They select 8 representative tasks covering key aspects like understanding, reasoning, and explaining using chemistry knowledge. 2. They provide a rigorous experimental framework to test LLMs on these chemistry tasks. They carefully design prompts and select demonstration examples for in-context learning. They also assess different settings like zero-shot vs few-shot prompting. Repeated evaluations help account for the randomness of LLMs.3. Their experiments yield valuable insights into LLM performance on chemistry tasks. Key findings are:- GPT-4 outperforms other models evaluated - LLMs struggle with tasks needing precise understanding of SMILES molecular representations- LLMs show promise in text-related explanation tasks like molecule captioning- Formulating problems as classification/ranking improves LLM competitiveness - In-context learning boosts performance over zero-shot prompting4. Based on the comprehensive analysis, they provide recommendations for effectively applying LLMs to chemistry problems in the future, like using similarity-based search for ICL examples.5. They highlight the limitations of current LLMs and evaluation methods in chemistry, underscoring needs for improvement.In summary, the key contribution is the extensive benchmarking and experiments that provide unprecedented insights into capabilities and limitations of LLMs for practical chemistry problems. The recommendations and analysis will inform further research at the intersection of LLMs and chemistry.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:This paper provides a comprehensive benchmark evaluation of the abilities of large language models like GPT-4 across a diverse range of practical chemistry tasks, finding that their performance varies across tasks depending on the precise understanding of chemistry required, with strengths in explainable text tasks but limitations in generative SMILES tasks demanding deeper chemical reasoning.
