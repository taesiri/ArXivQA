# [Measuring Social Biases in Masked Language Models by Proxy of Prediction   Quality](https://arxiv.org/abs/2402.13954)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Masked language models (MLMs) like BERT and RoBERTa have achieved state-of-the-art performance on NLP tasks, but also encode concerning social biases that are undesirable.
- Existing methods to measure social biases in MLMs have limitations, such as not fully capturing contextual information or biases related to the masked language modeling objective.

Proposed Solution:
- Propose new metrics - Complementary Reciprocal Rank (CRR), attention-weighted CRR (CRRA), probability difference (ΔP) and attention-weighted ΔP (ΔPA) to measure MLM preference and prediction quality on biased sentence pairs.
- Apply iterative masking experiment where MLM predicts masked tokens. Compare prediction ranks and probabilities for stereotypical vs anti-stereotypical sentences.
- Define model-specific bias score (BS_PT) and relative bias score (BS_RT) functions using proposed metrics to quantify biases in pre-trained and re-trained MLMs.

Main Contributions:
- Validate that proposed metrics better agree with human annotations of bias compared to prior methods.
- Find that BERT, RoBERTa, DistilBERT and DistilRoBERTa encode high disability, religion and some gender biases.
- Show that web-trained RoBERTa and DistilRoBERTa have higher race bias than Wikipedia-trained BERT and DistilBERT.  
- Demonstrate that BS_RT can accurately characterize shifts in bias from re-training MLMs on biased datasets, unlike prior metrics.
- Proposed methodology enables evaluating social biases introduced by re-training MLMs under MLM objective.

In summary, the paper presents new metrics that can better estimate biases in masked language models both before and after re-training, with applications for quantifying biases in models and text corpora.
