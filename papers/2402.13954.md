# [Measuring Social Biases in Masked Language Models by Proxy of Prediction   Quality](https://arxiv.org/abs/2402.13954)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Masked language models (MLMs) like BERT and RoBERTa have achieved state-of-the-art performance on NLP tasks, but also encode concerning social biases that are undesirable.
- Existing methods to measure social biases in MLMs have limitations, such as not fully capturing contextual information or biases related to the masked language modeling objective.

Proposed Solution:
- Propose new metrics - Complementary Reciprocal Rank (CRR), attention-weighted CRR (CRRA), probability difference (ΔP) and attention-weighted ΔP (ΔPA) to measure MLM preference and prediction quality on biased sentence pairs.
- Apply iterative masking experiment where MLM predicts masked tokens. Compare prediction ranks and probabilities for stereotypical vs anti-stereotypical sentences.
- Define model-specific bias score (BS_PT) and relative bias score (BS_RT) functions using proposed metrics to quantify biases in pre-trained and re-trained MLMs.

Main Contributions:
- Validate that proposed metrics better agree with human annotations of bias compared to prior methods.
- Find that BERT, RoBERTa, DistilBERT and DistilRoBERTa encode high disability, religion and some gender biases.
- Show that web-trained RoBERTa and DistilRoBERTa have higher race bias than Wikipedia-trained BERT and DistilBERT.  
- Demonstrate that BS_RT can accurately characterize shifts in bias from re-training MLMs on biased datasets, unlike prior metrics.
- Proposed methodology enables evaluating social biases introduced by re-training MLMs under MLM objective.

In summary, the paper presents new metrics that can better estimate biases in masked language models both before and after re-training, with applications for quantifying biases in models and text corpora.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes and validates methods to measure social biases encoded in masked language models using relative prediction quality between biased sentences, and finds the methods can accurately estimate biases introduced by retraining models compared to previous approaches.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1) It proposes and validates new methods (CRR, CRRA, ΔP, ΔPA) to measure social biases encoded in masked language models (MLMs) like BERT and RoBERTa. These metrics quantify an MLM's relative preference for stereotypical vs anti-stereotypical sentences as a proxy for its prediction quality.

2) It compares these proposed metrics against previous bias measurement methods like AUL, AULA, CSPS, SSS on benchmark datasets. The paper finds that the proposed metrics are generally more sensitive and accurate for estimating biases, especially those introduced by retraining MLMs.

3) It provides an analysis of social biases encoded in several popular MLMs like BERT, RoBERTa, DistilBERT etc. on benchmark datasets using both existing and newly proposed metrics. It finds that these models encode concerning levels of bias against disadvantaged groups.

4) It proposes a method to estimate relative shift in an MLM's biases after it is retrained on biased datasets, which could help evaluate biases introduced by retraining and possibly those implicit in the retraining corpora. The proposed metrics are shown to be more accurate for this purpose.

In summary, the main contribution is proposing better bias quantification methods for MLMs and demonstrating their effectiveness compared to previous approaches.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it include:

- Masked language models (MLMs) - Contextually-aware language models like BERT and RoBERTa that use masking as a pre-training technique

- Social biases - Stereotypical associations encoded in language models against disadvantaged groups

- Iterative masking experiment - Method of evaluating MLMs by iteratively masking each token in an input and measuring prediction quality

- Evaluation metrics:
    - CRR (Complementary Reciprocal Rank) 
    - ΔP (Probability difference)
    - CRRA (Attention-weighted CRR)
    - ΔPA (Attention-weighted ΔP)

- Benchmark datasets:
    - CrowdS-Pairs (CPS) 
    - StereoSet (SS)

- Per-model bias score (BS_PT) - Proposed metric to quantify an MLM's preference for biased text

- Model-comparative bias score (BS_RT) - Proposed metric to measure relative change in an MLM's bias after retraining

- Sensitivity analysis - Evaluating proposed metrics' ability to accurately capture shifts in bias from retraining MLMs on biased text

The key focus is on developing evaluation metrics that can reliably measure and compare the social biases encoded in masked language models. The iterative masking experiment and benchmark datasets are used to assess the metrics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes several new metrics for measuring social bias in masked language models, including CRR, CRRA, ΔP, and ΔPA. How are these metrics defined and how do they capture model bias compared to previously proposed metrics like AUL and AULA?

2. The iterative masking experiment is central to the proposed methodology. Can you walk through this procedure step-by-step and explain how the prediction quality metrics are computed at each iteration? 

3. The authors claim their proposed metrics are more sensitive for detecting relative changes in model bias from retraining compared to prior metrics. What evidence do they provide to support this claim? How convincing is this evidence?

4. Attention-weighted versions of CRR and ΔP are introduced - CRRA and ΔPA. How is attention used in defining these metrics? Why might incorporating attention improve bias measurement?

5. Two bias scoring functions are proposed - bspt and bsrt. How do these functions quantify model bias and shifts in bias from retraining? What are the key differences between them?

6. The methodology leverages benchmark bias datasets like CPS and StereoSet. What are the key properties of these datasets? What advantages does CPS have over StereoSet as a retraining set?

7. The paper finds overall high religious and disability bias across models. What explanations are offered for why certain biases are more prevalent? How might training data impact observed bias?

8. Proposed metrics are evaluated by how well they align with human annotations of bias. What evaluation approach is used here? How well do the new metrics perform?

9. Could the proposed methodology be useful beyond evaluating social bias in language models? What other potential applications are there for measuring relative shifts in model behavior from retraining?

10. The authors note previously proposed metrics sometimes substantially underestimate bias shifts from retraining. What are some possible reasons for this underestimation? How might the new metrics address these issues?
