# [Efficiently Trainable Text-to-Speech System Based on Deep Convolutional   Networks with Guided Attention](https://arxiv.org/abs/1710.08969)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1) Can a fully convolutional neural network-based text-to-speech (TTS) system work well for synthesizing speech from text, without using any recurrent neural network components? 

2) Can such a fully convolutional TTS system be trained much faster than existing recurrent neural network-based TTS systems like Tacotron, while still achieving acceptable speech quality?

3) Does using "guided attention" during training help the attention mechanism learn more quickly and accurately where to attend in the input text?

In particular, the authors propose a novel TTS technique called Deep Convolutional TTS (DCTTS) which is based entirely on convolutional neural networks, in contrast to prior work like Tacotron that uses recurrent units. They hypothesize this will allow faster training. They also propose a "guided attention" method to help the attention module train more rapidly. The main goal is to show their proposed DCTTS system can be trained quickly overnight on ordinary hardware, while reaching satisfactory speech quality. Evaluating the model quality and training time compared to Tacotron is aimed at testing these hypotheses.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a fully convolutional neural network (CNN) based text-to-speech (TTS) system called Deep Convolutional TTS (DCTTS) that can be trained much faster than recurrent neural network (RNN) based systems like Tacotron. 

2. Introducing a "guided attention" method to help train the attention module more efficiently by incorporating prior knowledge that the attention matrix should be nearly diagonal for TTS.

3. Showing that the proposed DCTTS system can be trained in 15 hours on a normal gaming PC with 2 GPUs to achieve reasonably good speech quality, while Tacotron takes days to weeks to train.

In summary, the main contribution is a fast-to-train CNN-based TTS system with guided attention that achieves satisfactory speech quality with significantly less training time and compute resources compared to RNN-based systems. This could make neural TTS more accessible to smaller teams and individuals.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a fast-to-train neural text-to-speech system based entirely on convolutional neural networks, without any recurrent components, that can synthesize intelligible speech after training for only 15 hours on a consumer GPU.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research in text-to-speech synthesis:

- The main contribution is proposing a fully convolutional neural network architecture for TTS, rather than using recurrent neural networks like most prior work (e.g. Tacotron). This allows faster training.

- They show their model can be trained overnight on a consumer GPU, vs taking weeks to train Tacotron. This demonstrates the efficiency of the convolutional architecture.

- The audio quality is decent but not state-of-the-art. The MOS score is 2.71 after 15 hours training. This is lower than original Tacotron but reasonable given the short training time.

- They introduce a "guided attention" technique to help the attention module train more efficiently. This is a novel contribution not seen in other work.

- Overall, it demonstrates convolutional TTS is viable and can reach acceptable quality much faster than RNN-based approaches. But the audio fidelity is not yet on par with the state-of-the-art Tacotron results.

- This came out in 2018. Since then, Transformer-based TTS models have become dominant, superseding RNN/CNN architectures. So this represents an intermediate step in TTS research progress.

In summary, it makes good contributions around efficient convolutional TTS training and guided attention, but the core ideas have been superseded by Transformer architectures in more recent state-of-the-art work. The audio quality is decent but not record-breaking.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Further improving the audio quality by tuning hyperparameters and applying techniques from the deep learning community. The authors note the quality is not yet perfect and can likely be improved.

- Extending the method for other applications like emotional/non-linguistic/personalized speech synthesis. The simple neural architecture could potentially be adapted for these other tasks. 

- Exploring more integrated speech systems like multimodal systems. The lighter neural TTS model makes it more feasible to combine with other modalities.

- Working on issues related to extending the method for real-time and online processing. The current SSRN module uses non-causal convolutions, but the authors suggest exploring causal convolutions for real-time synthesis.

- Improving robustness of the attention mechanism. The authors mention some heuristics to handle attention failures like word skipping, but more work is likely needed here. 

- Applying the method to other languages. The current work focuses on English, but extending to other languages is a natural next step.

In summary, the main future directions are improving quality, extending the synthesis capabilities, integrating with other modalities, enabling real-time use, robustifying attention, and supporting more languages. The simple and fast framework provides a good foundation for many future avenues of research.
