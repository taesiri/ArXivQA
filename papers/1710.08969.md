# [Efficiently Trainable Text-to-Speech System Based on Deep Convolutional   Networks with Guided Attention](https://arxiv.org/abs/1710.08969)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1) Can a fully convolutional neural network-based text-to-speech (TTS) system work well for synthesizing speech from text, without using any recurrent neural network components? 

2) Can such a fully convolutional TTS system be trained much faster than existing recurrent neural network-based TTS systems like Tacotron, while still achieving acceptable speech quality?

3) Does using "guided attention" during training help the attention mechanism learn more quickly and accurately where to attend in the input text?

In particular, the authors propose a novel TTS technique called Deep Convolutional TTS (DCTTS) which is based entirely on convolutional neural networks, in contrast to prior work like Tacotron that uses recurrent units. They hypothesize this will allow faster training. They also propose a "guided attention" method to help the attention module train more rapidly. The main goal is to show their proposed DCTTS system can be trained quickly overnight on ordinary hardware, while reaching satisfactory speech quality. Evaluating the model quality and training time compared to Tacotron is aimed at testing these hypotheses.
