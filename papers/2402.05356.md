# [Exploring Learning Complexity for Downstream Data Pruning](https://arxiv.org/abs/2402.05356)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
- Pre-trained large models require heavy computation for fine-tuning on downstream tasks, posing challenges with limited resources. An intuitive solution is to prune less informative samples from the fine-tuning dataset.
- Existing methods define scoring functions based on training, incurring extra pruning costs. Adapting scoring functions from training-free methods often distorts pruning. 

Proposed Solution
- Propose learning complexity (LC) as a training-free scoring function. LC is the average predicted confidence of subnets with different capacities.
- For classification, LC is the average confidence from a weighted KNN classifier applied on features from different layers. For regression, LC uses dropout subnets.
- Show top-k preserving picks redundant high-density samples. Propose preserving easy and diverse samples instead, by clustering features first.

Main Contributions
- Define learning complexity, a training-free scoring function that encapsulates sample processing in a converged model. Show its adapted version has better rank correlation than other methods.
- Theoretically show top-k preserving picks redundant high-density samples. Propose preserving easy and diverse samples instead.
- Comprehensive experiments show state-of-the-art performance over baselines. Experiments also demonstrate generalization across architectures and applicability to language domain.
- Ablations verify importance of the defined score and insensitivity to hyperparameters.

In summary, the paper proposes learning complexity as an efficient yet effective scoring function for fine-tuning dataset pruning. Both theoretically and empirically, it demonstrates the issues with common top-k preserving and shows that preserving easy and diverse samples works better. The method establishes superior performance over state-of-the-art baselines.
