# [Towards Deterministic End-to-end Latency for Medical AI Systems in   NVIDIA Holoscan](https://arxiv.org/abs/2402.04466)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Integrating AI into medical devices can improve diagnostics and treatments, but running multiple AI applications concurrently leads to unpredictable end-to-end latency due to resource contention on the GPU. 
- To avoid this, manufacturers deploy separate workstations for each application, increasing costs and hardware footprint.

Proposed Solution:
- Use CUDA MPS (Multi-Process Service) to create isolated partitions on the GPU to spatially separate compute workloads. This reserves SMs and memory for each application.
- Further isolate graphics and compute workloads onto separate physical GPUs to eliminate contention between graphics and compute contexts. Leverage GPU virtual addressing to enable efficient data transfer.
- Pin each application process to dedicated CPU cores to minimize scheduling overhead.

Contributions:
- Proposed pragmatic system design combining CUDA MPS and multi-GPU hardware isolation optimizes concurrent heterogeneous workloads in medical AI systems.
- Empirical evaluation with real-world medical applications shows significant gains:
  - Reduced maximum latency by 21-30%, standard deviation by 17-24%, tail latency by 21-47% and improved latency distribution flatness by 17-25% compared to single GPU baseline.
  - Dual GPU config achieves 16% lower max latency than single high-end GPU, enabling over 50% cost reduction.
  - With optimizations, able to run up to 3 medical AI applications within max 50ms latency on single workstation, reducing hardware footprint.

- Provided clear design guidelines for managing heterogeneous workloads involving graphics and compute to improve predictability in medical AI systems.

In summary, the paper addresses an important problem in medical AI systems regarding unpredictable latency for concurrent applications, and demonstrates both pragmatically implementable and empirically validated system design techniques to enhance performance determinism across various metrics.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes GPU workload partitioning and load balancing techniques leveraging CUDA MPS and multi-GPU configurations to improve end-to-end latency predictability for concurrent heterogeneous compute and graphics workloads in medical AI systems using the NVIDIA Holoscan platform.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing a novel system design approach that combines CUDA MPS (Multi-Process Service) for spatial partitioning of compute workloads and a load-balancing technique to isolate compute (CUDA kernel) tasks and graphics onto distinct GPUs. Additionally, an admission control policy is used to prevent SM oversubscription by concurrent compute tasks.

2. Empirical evaluation using end-to-end latency metrics reveals substantial performance improvements with the proposed design. For example, for up to 5 concurrent endoscopy tool tracking AI applications, the design reduces maximum latency by 21-30%, improves latency distribution flatness by 17-25%, compared to a single GPU baseline. Against a default multi-GPU setup, the optimizations decrease maximum latency by 35% for up to 6 concurrent applications by improving GPU utilization by 42%.

3. The paper provides clear design insights for AI applications in the edge computing and medical systems domain, where performance predictability of concurrent and heterogeneous GPU workloads is a critical requirement. The techniques can facilitate more predictable and safe AI deployment in medical systems.


## What are the keywords or key terms associated with this paper?

 Based on my analysis of the paper's content, some of the main keywords and key terms associated with this paper include:

- End-to-end latency
- Latency determinism 
- Predictable performance
- Concurrent GPU workloads
- Heterogeneous GPU workloads 
- Medical AI systems
- NVIDIA Holoscan
- CUDA MPS
- Multi-GPU configurations
- Compute and graphics isolation
- Workload partitioning
- Load balancing

The paper focuses on techniques to improve end-to-end latency predictability and determinism in medical AI systems using NVIDIA's Holoscan platform. Key concepts include leveraging CUDA MPS and multi-GPU setups to isolate heterogeneous compute and graphics workloads to mitigate resource contention, as well as workload partitioning and load balancing strategies. The context is medical devices and systems integrating AI capabilities, where performance predictability is critical.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using CUDA MPS for spatial partitioning of compute workloads. What are the key benefits of using CUDA MPS compared to traditional CUDA streams and contexts? How does it help improve predictability?

2. The paper uses admission control with CUDA MPS to prevent SM oversubscription. What is the rationale behind this? How does admission control work and what metrics are used to determine if a new workload should be admitted? 

3. The paper advocates isolating compute and graphics workloads onto separate GPUs. What are the limitations of existing solutions like MIG and vGPU in supporting predictable performance against heterogeneous workloads?

4. What is the rationale behind using a less powerful GPU for graphics versus compute workloads when isolating on multi-GPU systems? What are the tradeoffs involved?  

5. The results show that multi-GPU configurations outperform even a higher-end single GPU system. What analysis indicates that increased compute capacity alone does not explain the gains?

6. How does the paper analyze end-to-end latency determinism? What metrics beyond max latency are used and what do they each indicate about the predictability of the system?

7. What are the differences in performance gains from using CUDA MPS alone versus pairing it with multi-GPU isolation? Why does the combination perform the best?

8. How many concurrent endoscopy tracking applications can be supported within 50ms max latency using the optimized dual GPU configuration? What are the cost and power benefits estimated?

9. For the multi-AI ultrasound application, what causes elevated GPU contention? How do the proposed techniques help mitigate this and what specific metrics improve the most?

10. What guidance does the paper provide on choosing between optimizing a single GPU versus multi-GPU configuration? When is each approach more viable or impactful?
