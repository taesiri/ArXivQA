# [Measuring Bias in a Ranked List using Term-based Representations](https://arxiv.org/abs/2403.05975)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing metrics for evaluating gender bias in document rankings like ARB and NFaiRR have limitations. NFaiRR aggregates bias scores of individual documents, so a balanced ranked list on the whole can still have biased individual documents. 

- There is a need for a metric that evaluates fairness of group representation in rankings based on the exposure of representative terms for each group.

Method:
- The paper proposes a new metric called TExFAIR (term exposure-based fairness) based on the attention-weighted rank fairness (AWRF) framework. 

- TExFAIR measures fairness through the overall exposure of female and male representative terms in the ranked list, unlike NFaiRR's document-level view.

- It handles non-representative documents through a rank biased discounting factor (RBDF).

- The paper also uses counterfactual evaluation by gender-swapping terms to estimate ranking models' bias.

Contributions:
- Defines extensions to AWRF for term-based group representations: explicit mapping of terms to groups, and RBDF for non-representative documents.  

- Shows TExFAIR captures a different aspect of fairness than NFaiRR through correlation analysis.

- Finds no strong correlation between model bias measured by counterfactual evaluation and bias measured by TExFAIR/NFaiRR.

- Provides a new perspective on evaluating fairness through overall exposure of groups based on their representative terms.

In summary, the paper addresses limitations of prior bias metrics by proposing the new term-based TExFAIR metric along with counterfactual evaluation of model bias.


## Summarize the paper in one sentence.

 This paper proposes a new metric called TExFAIR to measure gender bias in document rankings based on the exposure of gendered terms, which differs from the commonly used NFaiRR metric by evaluating fairness at the ranking level rather than aggregating document-level scores.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It proposes a new metric called TExFAIR (term exposure-based fairness) to measure the fairness of a ranked list in representing different groups. TExFAIR extends the generic AWRF framework by explicitly defining the association of documents to groups based on probabilistic term-level associations, and introducing a rank-biased discounting factor to account for non-representative documents.

2) It shows that TExFAIR measures a different dimension of fairness compared to the commonly used NFaiRR metric, by demonstrating that there is no strong correlation between the two metrics. This indicates that the choice of fairness metric can impact model selection.

3) It performs a counterfactual evaluation to estimate the inherent gender bias in ranking models, separate from the bias in the ranked lists they produce. This analysis reveals a discrepancy between model bias and ranked list bias.

In summary, the main contribution is the proposal of TExFAIR as a new term-based fairness metric, and analyses showing that it captures a different aspect of fairness compared to NFaiRR, and that there is a disconnect between model bias and ranked list bias.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the main keywords and key terms associated with this paper are:

Bias, Evaluation, Document Ranking, Fairness, Gender Bias, Group Representation, Term Exposure, Attention-weighted Rank Fairness (AWRF), Normalized Fairness of Retrieval Results (NFaiRR), Term Exposure-based Fairness (TExFAIR), Rank-biased Discounting Factor (RBDF)

The paper proposes a new metric called TExFAIR to measure the fairness of the representation of different groups in a ranked list of documents. It extends the generic AWRF framework by explicitly defining the association of documents to groups based on probabilistic term-level associations and introducing a rank-biased discounting factor. The proposed TExFAIR metric is compared to the commonly used NFaiRR metric for measuring gender bias. The paper shows that TExFAIR measures a different dimension of fairness than NFaiRR. It also analyzes the sensitivity of the metrics to ranking cut-off and performs a counterfactual evaluation to estimate the inherent gender bias of ranking models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new metric called TExFAIR that extends the AWRF framework. What are the two key extensions made to AWRF to develop TExFAIR? Explain each extension in detail.

2. Explain the concept of "term exposure" as formally defined in Equation 2 of the paper. What is the intuition behind using term exposure to estimate group representation in a ranked list?

3. What is the issue with not considering "non-representative documents" in the fairness evaluation? How does the proposed rank-biased discounting factor (RBDF) address this issue? Explain with an example.  

4. What is the difference between the term exposure-based divergence (TED) with and without the RBDF? How does the RBDF change the sensitivity to ranking cutoff?

5. The paper argues that TExFAIR measures a different dimension of fairness compared to NFaiRR. Elaborate on the key differences in how fairness is viewed from the perspective of these two metrics.  

6. Explain the counterfactual evaluation performed in the paper to estimate the inherent gender bias in ranking models. What does a low counterfactually-estimated RBO indicate about a model?

7. Discuss the limitations of using the counterfactually-estimated RBO to measure the inherent bias in ranking models. Are there better approaches not explored in the paper?

8. The paper acknowledges limitations of evaluating fairness based on term-level representations. What are some ways the authors propose to address this in future work? Discuss the pros and cons.  

9. The key novelty of TExFAIR is in its term-level perspective of measuring fairness. Discuss the advantages and potential limitations of this term-level approach over existing document-level perspectives. 

10. The correlation between TExFAIR and NFaiRR is shown to not be very high. What are the implications of this when selecting models based on fairness and effectiveness tradeoffs? How could the choice of metric impact model selection?
