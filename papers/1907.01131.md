# Learnable Gated Temporal Shift Module for Deep Video Inpainting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we efficiently utilize temporal information to recover videos with free-form masks in a consistent way using deep learning models? Specifically, the paper aims to develop an efficient video inpainting model that can fill in arbitrary missing regions in videos while maintaining temporal consistency across frames. The key challenges are handling the additional temporal dimension compared to image inpainting, and doing so efficiently without relying solely on computationally intensive 3D convolutions.The main hypothesis seems to be that by developing a novel module termed Learnable Gated Temporal Shift Module (LGTSM), they can enable 2D convolutions to effectively capture temporal information for video inpainting while reducing model complexity. The LGTSM module allows shifting features across neighboring frames and attending on masked regions to handle irregular masks.In summary, the central research question is how to efficiently perform free-form video inpainting using deep learning in a temporally consistent manner, which they aim to address through the proposed LGTSM module.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing a novel Learnable Gated Temporal Shift Module (LGTSM) for free-form video inpainting. The LGTSM enables 2D convolutions to make use of neighboring frames more efficiently by learning to shift some channels to temporal neighbors and applying a gating mechanism. This allows capturing temporal information while reducing model size and computational costs compared to 3D convolutions.- Developing a TSMGAN loss to improve model performance for free-form video inpainting. The TSMGAN discriminator focuses on different spatial-temporal features to utilize global, local, and temporal information. - Achieving state-of-the-art results on the FaceForensics and FVI datasets with only 33% of parameters and inference time compared to models using 3D convolutions. The LGTSM makes 2D convolutions competitive with 3D for modeling temporal information in video inpainting.- Demonstrating the importance of the proposed gated convolution and learnable shifting kernels through ablation studies. The learnable kernels allow shifting features from more distant frames unlike fixed kernels.In summary, the key contribution is proposing the LGTSM module to enable efficient temporal modeling in 2D convolutions for video inpainting, reducing model complexity while achieving state-of-the-art results.
