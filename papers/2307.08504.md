# [BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up   Patch Summarization](https://arxiv.org/abs/2307.08504)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we develop an efficient and effective vision-language pre-training model by reducing the lengthy visual token sequences from image patches in Vision Transformers?

The key hypotheses underlying this research question are:

1) The lengthy visual token sequences in Vision Transformer (ViT) based vision-language pre-training (VLP) models lead to training inefficiency and ineffectiveness. 

2) Summarizing the lengthy visual token sequences into a concise representation, guided by text semantics, can improve efficiency and effectiveness of VLP models.

3) A bottom-up approach of first extracting key visual tokens and then abstracting them into a condensed sequence, similar to text summarization, can achieve better efficiency-effectiveness trade-off compared to only top-down abstraction or only bottom-level extraction.

To validate these hypotheses, the paper proposes a novel Bottom-Up Patch Summarization (BUS) approach for efficient and effective VLP, which includes:

- Key Patch Extraction with a Text Semantic-aware Patch Selector in the ViT backbone 

- Text-Guided Patch Abstraction with a Transformer decoder outside the backbone

The goal is to demonstrate how this bottom-up visual patch summarization can improve efficiency and effectiveness compared to baseline VLP models on various vision-language tasks.


## What is the main contribution of this paper?

 This paper proposes a Bottom-Up Patch Summarization (BUS) approach for efficient and effective vision-language pre-training. The key contributions are:

1. It proposes a two-step bottom-up patch summarization method to create a concise summary of lengthy visual token sequences guided by text semantics. This includes coarse-grained key patch extraction in the ViT backbone and fine-grained text-guided patch abstraction. 

2. It incorporates a Text Semantic-aware Patch Selector (TSPS) in the ViT backbone to perform extractive summarization. It also introduces a Patch Abstraction Decoder (PAD) upon the backbone for abstractive summarization. 

3. It designs a novel pre-training task called Patch-Text Matching (PTM) to facilitate training the patch selector and drive fine-grained patch-text alignment.

4. Experiments show BUS reduces computation costs by ~50% and improves or maintains downstream task performance compared to baselines. It also achieves SOTA results by using higher resolution images without extra computation.

In summary, the key contribution is proposing and showing the effectiveness of a bottom-up patch summarization approach to create efficient and effective vision-language representations for pre-training. The method coordinates extraction in the backbone with abstraction outside it to balance efficiency and accuracy.
