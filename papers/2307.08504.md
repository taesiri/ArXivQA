# [BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up   Patch Summarization](https://arxiv.org/abs/2307.08504)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we develop an efficient and effective vision-language pre-training model by reducing the lengthy visual token sequences from image patches in Vision Transformers?

The key hypotheses underlying this research question are:

1) The lengthy visual token sequences in Vision Transformer (ViT) based vision-language pre-training (VLP) models lead to training inefficiency and ineffectiveness. 

2) Summarizing the lengthy visual token sequences into a concise representation, guided by text semantics, can improve efficiency and effectiveness of VLP models.

3) A bottom-up approach of first extracting key visual tokens and then abstracting them into a condensed sequence, similar to text summarization, can achieve better efficiency-effectiveness trade-off compared to only top-down abstraction or only bottom-level extraction.

To validate these hypotheses, the paper proposes a novel Bottom-Up Patch Summarization (BUS) approach for efficient and effective VLP, which includes:

- Key Patch Extraction with a Text Semantic-aware Patch Selector in the ViT backbone 

- Text-Guided Patch Abstraction with a Transformer decoder outside the backbone

The goal is to demonstrate how this bottom-up visual patch summarization can improve efficiency and effectiveness compared to baseline VLP models on various vision-language tasks.


## What is the main contribution of this paper?

 This paper proposes a Bottom-Up Patch Summarization (BUS) approach for efficient and effective vision-language pre-training. The key contributions are:

1. It proposes a two-step bottom-up patch summarization method to create a concise summary of lengthy visual token sequences guided by text semantics. This includes coarse-grained key patch extraction in the ViT backbone and fine-grained text-guided patch abstraction. 

2. It incorporates a Text Semantic-aware Patch Selector (TSPS) in the ViT backbone to perform extractive summarization. It also introduces a Patch Abstraction Decoder (PAD) upon the backbone for abstractive summarization. 

3. It designs a novel pre-training task called Patch-Text Matching (PTM) to facilitate training the patch selector and drive fine-grained patch-text alignment.

4. Experiments show BUS reduces computation costs by ~50% and improves or maintains downstream task performance compared to baselines. It also achieves SOTA results by using higher resolution images without extra computation.

In summary, the key contribution is proposing and showing the effectiveness of a bottom-up patch summarization approach to create efficient and effective vision-language representations for pre-training. The method coordinates extraction in the backbone with abstraction outside it to balance efficiency and accuracy.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in vision-language pre-training:

- This paper proposes a novel bottom-up patch summarization approach to reduce the length of visual token sequences in ViT-based vision-language models. Other related works have focused on either top-level abstraction or bottom-level extraction, but this combines both to get an efficient yet effective summary.

- The proposed text-semantics-aware patch selector performs coarse-grained key patch extraction within the ViT backbone guided by text semantics. This is a unique approach compared to prior work. 

- Existing methods like BLIP and Flamingo perform visual sequence abstraction outside of the ViT backbone without considering text guidance. This paper shows the value of incorporating textual semantics into the visual summarization process.

- Methods like DynamicViT and EViT reduce patches within the ViT backbone but are more limited in how much they can reduce without harming representations. The bottom-up approach here allows more aggressive reduction by following up with abstraction.

- The paper demonstrates strong performance on VQA, image captioning, retrieval, and visual grounding compared to state-of-the-art models, while also improving efficiency. This shows the effectiveness of the approach.

- By handling higher resolution images efficiently, the method can achieve even better performance on downstream tasks than other models, showing the benefits of summarization.

- The bottom-up patch summarization idea draws inspiration from text summarization literature and translates it effectively to the visual domain, demonstrating a creative connection.

In summary, the key innovations of this paper compared to other VLP research are the bottom-up visual sequence summarization approach guided by text semantics, the ability to handle higher resolution inputs, and the superior performance and efficiency demonstrated across multiple vision-language tasks. The work makes important contributions to efficient and effective VLP.
