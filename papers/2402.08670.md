# [Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models](https://arxiv.org/abs/2402.08670)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Multimodal recommender systems (MMRSs) aim to address the cold-start problem by incorporating multimodal content about items, such as text descriptions and images. However, traditional MMRSs have two key limitations:
1) They lack domain-specific knowledge to understand user preferences, since they are trained on limited user-item interaction data.  
2) They struggle to effectively handle multiple, discrete product images which can be noisy and redundant.

Large vision-language models (LVLMs) like GPT4-V offer promising solutions, thanks to their pre-training on enormous multimodal datasets. However, directly applying LVLMs for MMRS is challenging due to the two aforementioned issues.

Proposed Solution - Visual-Summary Thought (VST):
The key ideas are:
1) Use user historical interactions as context for personalized recommendations. 
2) Generate visual summaries of individual product images using LVLMs instead of handling multiple raw images. This simplifies the visual modality into natural language.
3) Construct prompt by concatenating item titles with their visual summaries to query user preferences.

Main Contributions:
1) First work to investigate reasoning strategies for LVLMs in MMRS.
2) Proposal of novel VST strategy specifically designed for challenges in MMRS context, which harnesses LVLMs' visual understanding while overcoming deficiency in handling multiple images.
3) Comprehensive experiments using GPT4-V, LLaVA-7b and LLaVA-13b on 4 real-world datasets demonstrate efficacy of VST for LVLM-based MMRS.
