# [Beyond Top-Class Agreement: Using Divergences to Forecast Performance   under Distribution Shift](https://arxiv.org/abs/2312.08033)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates using divergence-based disagreement notions, such as Hellinger distance, Jensen-Shannon divergence, and Kullback-Leibler divergence, to estimate model performance under distribution shift. Unlike previous work that focused on top-1 disagreement, divergence disagreement considers the full predictive label distribution, providing a more nuanced view of model alignment. Experiments on CIFAR-10, CIFAR-100 and their corrupted versions analyze the correlation of in-distribution (ID) vs out-of-distribution (OOD) disagreement and test error. The authors find divergence disagreement exhibits stronger ID vs OOD correlations, dubbed agreement-on-the-line and accuracy-on-the-line phenomena, compared to top-1 disagreement. Leveraging these correlations, Hellinger distance provides the lowest mean absolute percentage error for estimating unlabeled OOD test error overall using a simple line fitting procedure. The notions also outperform maximum softmax probability and maximum logit baseline for OOD sample detection. The correlations vanish under model miscalibration, equally affecting all notions. The paper demonstrates divergence disagreement's viability for distribution shift detection and performance estimation tasks.


## Summarize the paper in one sentence.

 This paper investigates divergence-based disagreement notions, finding they provide better out-of-distribution error estimates and detection rates compared to top-1 disagreement.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing and evaluating the use of divergence-based disagreement notions, such as Hellinger distance, Jensen-Shannon divergence, and Kullback-Leibler divergence, for estimating model performance under distribution shift. Specifically, the paper shows that by considering the full predictive label distribution rather than just the top predicted class, these divergence disagreement measures provide better out-of-distribution error estimates and detection rates compared to standard top-1 disagreement. The key findings include:

- Divergence disagreement exhibits stronger "on-the-line" correlations between in-distribution vs out-of-distribution disagreement and error. This allows more accurate estimation of out-of-distribution performance.

- Hellinger distance disagreement provides the lowest error in estimating out-of-distribution test error in most cases. However, top-1 is more robust when agreement between in- and out-of-distribution is low.  

- Divergence disagreement, especially Kullback-Leibler, best detects out-of-distribution samples.

- Miscalibration equally affects correlation strength for all disagreement notions.

So in summary, the main contribution is introducing and evaluating divergence disagreement for better estimating model performance under distribution shift.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Distribution shift/out-of-distribution generalization: The paper studies techniques to estimate model performance under distribution shift between the training (in-distribution) and test (out-of-distribution) data.

- Model disagreement: The use of disagreement between models, rather than just a model's individual predictions, to estimate performance under distribution shift. 

- Divergence measures: Disagreement notions based on distribution divergences like Hellinger distance, Jensen-Shannon divergence, and Kullback-Leibler divergence that consider the full predictive distribution.

- On-the-line phenomenon: The empirical observation that in-distribution vs out-of-distribution disagreement correlates with in-distribution vs out-of-distribution test error. 

- Performance estimation: Using disagreement to estimate model test error on unlabeled out-of-distribution data, without having access to true labels.

- Out-of-distribution detection: Using disagreement scores to detect which test samples are out-of-distribution.

- Vision models and datasets: Experiments involve standard computer vision models like ResNets, VGG nets etc. trained on CIFAR-10 and CIFAR-100, tested on corrupted versions of these datasets.

Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces three divergence-based disagreement measures: Hellinger distance (HD), Jensen-Shannon divergence (JSD), and Kullback-Leibler divergence (KLD). How are these distributional measures different from the standard top-1 disagreement measure? What additional information do they provide?

2. The paper shows that divergence-based disagreement measures lead to stronger on-the-line phenomena compared to top-1 disagreement. What are some potential reasons for why this might be the case? Does considering the full predictive distribution allow the measures to better differentiate between different types of disagreements?

3. How well do the proposed divergence disagreement measures perform at estimating out-of-distribution (OOD) test error compared to top-1 disagreement? Under what conditions or datasets does each measure tend to perform best or worst?

4. For OOD error estimation, the paper finds HD disagreement provides the overall best mean absolute percentage error (MAPE) but seems less robust when shifting to datasets with more labels (CIFAR-100). Why might this be the case? 

5. The paper evaluates both a testbed-vs-testbed (TvT) and testbed-vs-CLIP (TvC) setting. What differences exist in utilizing CLIP as an anchor model versus only the standard vision models? How does the choice of disagreement measure impact results in each setting?

6. Miscalibration is shown to significantly reduce on-the-line correlations for all measures. Between measures, are there differences in robustness to miscalibration? If so, how could this impact use cases dependent on On-the-line phenomena?

7. For OOD sample detection, KLD disagreement provides the best AUC score. Why might KLD capture more pertinent information regarding distribution shifts not beneficial for error estimation?

8. Could the proposed divergence measures be used in tandem with uncertainty or confidence estimates? If combined, what potential advantages could there be over using either class of methods alone?

9. What limitations exist when applying simple linear regression methods based on ID vs OOD disagreement? When might more complex schemes be necessary for performance estimation?

10. The work focuses on image datasets and vision models. What challenges or changes might come about when applying divergence disagreement to other data modalities (text, audio, etc) and model types?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Estimating the performance of machine learning models when deployed in the real-world is challenging, especially under distribution shift where the test data is different from the training data. Prior work has shown correlations between model disagreement rates and test error, suggesting disagreement could be used to estimate test performance without labels. However, most prior work uses top-1 disagreement that only looks at predicted class labels. This may be too coarse to capture all nuances of disagreement.

Proposed Solution: 
This paper investigates using divergence-based disagreement notions that consider the full predicted probability distribution, not just the top class. Specifically, they examine Hellinger distance, Jensen-Shannon divergence, and Kullback-Leibler divergence for measuring model disagreement. They hypothesize these will better correlate with test error compared to top-1 disagreement, especially for detecting distribution shift.

Key Contributions:

- Empirically demonstrate stronger linear correlations between ID vs OOD divergence disagreement and ID vs OOD test error compared to top-1 disagreement. This suggests divergence disagreement could provide better out-of-distribution (OOD) test error estimates.

- Show that OOD test error estimation using simple linear regression on divergence disagreement achieves lower mean absolute percentage error than top-1 disagreement on 17/22 distribution shifts. Hellinger distance performs best overall.

- Find divergence disagreement better detects OOD samples than top-1 and uncertainty baselines, with Kullback-Leibler divergence achieving the highest OOD detection AUC.

- Discover model calibration strongly impacts strength of linear relations between disagreement and error. All disagreement notions show vanishing on-the-line phenomena under miscalibration.

Overall, moving from top-1 to divergence-based disagreement better exposes model performance differences between in-distribution and OOD data. This information can be exploited for more reliable OOD error estimation and failure detection.
