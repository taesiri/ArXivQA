# [HARK Side of Deep Learning -- From Grad Student Descent to Automated   Machine Learning](https://arxiv.org/abs/1904.07633)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis seems to be that recent advancements in deep learning research, along with the competitive nature of the field, have led to an increase in "Hypothesizing After the Results are Known" (HARKing) behavior. 

The authors argue that practices like "grad student descent", the avoidance of publishing negative results, overfitting to benchmark datasets, and rushed claims of state-of-the-art performance are all examples of troubling trends in deep learning research that stem from HARKing. 

The paper aims to demonstrate these issues from different perspectives, such as:

- The competitiveness in DL research leading to questionable SotA claims and novelty (Section 2)

- The pressure to publish favorable results and aversion towards negative results (Section 3) 

- The belief that benchmark datasets represent real-world data (Section 4)

- The rise of automated ML methods (Section 5)

- Issues around explainability, ethics, reproducibility, etc. in AI systems (Section 6)

So in summary, the central hypothesis seems to be that HARKing behavior is increasing in deep learning research, and the authors systematically discuss its potential causes and implications from multiple viewpoints. The overall goal is to bring awareness to these issues and encourage more rigorous scientific practices in the field.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It discusses the issue of "Hypothesizing After the Results are Known" (HARKing) in machine learning and deep learning research. HARKing refers to the phenomenon where researchers form hypotheses after analyzing results, rather than specifying hypotheses beforehand. 

2. It provides examples of questionable research practices that may indicate HARKing, such as:

- Claiming improvements to state-of-the-art without sufficient analysis or justification (Section 2) 

- Avoiding reporting negative results (Section 3)

- Overfitting models to datasets that are not representative of real-world data (Section 4)

- Using automated machine learning methods like neural architecture search without proper ablation studies (Section 5)

3. It reviews HARKing in the context of goals like ethical AI, human-centric AI, explainable AI, reproducible AI, accountable AI, and privacy-aware AI (Section 6). It argues that avoiding HARKing is important for achieving these goals.

4. It calls for open discussions and cultural changes in machine learning research to address issues like publication bias and lack of hypothesis-driven methodology. It proposes some initial ideas like results-blind peer review to incentivize proper hypothesis formulation (Section 3).

In summary, this paper highlights the prevalence of questionable research practices indicative of HARKing in machine learning, provides examples, discusses implications, and calls for reforms to promote more rigorous hypothesis-driven methodology. The main contribution is bringing attention to these issues through a critical discussion.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper discusses issues with hypothesizing after results are known (HARKing) in deep learning research, such as questionable improvements to state-of-the-art methods, reluctance to publish negative results, overfitting to datasets, lack of model interpretability and accountability, and proposes ideas like results-blind peer review to improve research practices.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of deep learning:

- The paper provides a thoughtful critique and analysis of some problematic trends in deep learning research methodology, especially around issues like HARKing (hypothesizing after results are known), avoiding negative results, and overfitting to benchmark datasets. This type of reflective analysis is still relatively uncommon in DL research papers, which tend to focus on novel techniques and benchmark results.

- The authors connect the issues they identify to broader concerns about reproducibility, accountability, and ethics of AI systems. Making these connections is important as the AI community grapples with the societal impacts of rapidly advancing technology. Many papers are narrowly focused on algorithmic innovations without considering the bigger picture.

- The proposal for results-blind reviewing is an interesting idea for reforming publication practices to discourage HARKing. Very few papers propose specific changes to the research and review process like this. Most criticize problems but don't suggest solutions.

- The writing style is accessible and engaging for a wide audience. Many DL papers are dense and technical. This one clearly explains key issues and provides context for readers less familiar with the field's inner workings.

- The scope is ambitious, spanning issues in methodology, benchmarks, automation, ethics, and transparency. Many papers focus on a single dimension. Tackling multiple aspects provides a more comprehensive analysis.

- The authors include industry and academic perspectives. This gives a balanced viewpoint compared to papers authored solely by academics or industry researchers. 

In summary, while many DL papers contribute incremental algorithmic advances, this one stands out for critically analyzing the research process as a whole, synthesizing connections between issues, proposing reforms, and communicating its analysis in an engaging way. More meta-analysis and reflection on the state of DL research is needed.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the paper:

- Conducting more research using a results-blind review process to reduce publication bias and encourage publishing negative results. They propose submitting papers with hypotheses and experimental design first, getting reviewed, then running the experiments and publishing regardless of the results. 

- Incorporating high-level domain experts early in research studies to help form sound a priori hypotheses. This can lead to more successful hypothesis forming and scrutiny of results that don't match expectations.

- Developing more theoretical explanations for why deep learning works, using approaches like learning theory and statistical physics. This can lead to "true" explainable AI systems designed from first principles rather than reverse engineered.

- Encouraging initiatives for reproducibility, such as reproducibility checklists, code submission policies, and workshops focused on reproducibility. Measuring and defining reproducibility in ML/DL research remains an open question.

- Taking accountability and potential negative impacts into account early when forming hypotheses and designing systems, rather than treating ethics and accountability as afterthoughts.

- Moving away from narrow metric optimizations and benchmarks towards more holistic assessments of model performance including privacy preservation, explainability, reproducibility, and ethical considerations.

In summary, the authors advocate for more scientific rigor through blind reviews, collaborations with domain experts, focusing on theoretical underpinnings, and broadening evaluation metrics beyond simple performance scores. Overall, they encourage the community to improve research practices to reduce HARKing behaviors.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper: 

The paper discusses the issue of "HARKing" (Hypothesizing After the Results are Known) in machine learning and deep learning research. It argues that the competitive nature of ML/DL research and the hype surrounding "state-of-the-art" results has led researchers to engage in questionable practices like extensive parameter tuning without sound hypotheses, avoiding negative results, and overfitting models to standard datasets. This leads to inflated claims of accuracy and generalization that often fail to hold up in real-world settings. The authors review several examples of HARKing, like grad student descent, allergy to negative results, the "in the wild" illusion of datasets, and automated ML. They argue for cultural changes like accepting negative results and registering hypotheses before experiments. Overall, the paper argues that HARKing is a troubling trend in ML/DL research that undermines scientific integrity and needs to be addressed through policy changes and discussions within the research community.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper discusses the issue of "Hypothesizing After the Results are Known" (HARKing) in machine learning and deep learning research. The authors argue that the highly competitive nature of ML/DL research has led to questionable practices such as tweaking models and metrics until marginal improvements are found, avoiding publishing negative results, and overfitting models to popular benchmark datasets. This results in inflated claims of achieving state-of-the-art results, lack of model reproducibility and generalizability, and difficulty in determining why a method does or does not work. 

The authors provide several examples of HARKing behavior from current ML/DL research practices. They propose potential solutions such as blind review of papers based on hypothesis and experimental design rather than results, as well as increased discussion around developing ethical, human-centric, explainable, and reproducible AI systems. They call for cultural changes in accepting negative results, using truly independent test data, and registering intentions before conducting experiments. The authors conclude by emphasizing the need for collaborative efforts among researchers, practitioners, reviewers, publishers, and policy-makers to address these issues and prevent HARKing, in order to develop superior ML/DL solutions for the benefit of society.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a results-blind peer review process to address issues with HARKing (Hypothesizing After the Results are Known) in machine learning research. The key steps are:

1) A paper is submitted with a clear hypothesis and experimental design, reviewed blindly, and suggestions are provided to improve the experimental methods. 

2) Once accepted, the experiments are run. 

3) The paper gets published with a comprehensive discussion section, regardless of the results. 

The goal is to shift focus away from just positive results and towards proper hypothesis forming and experimental design. This aims to reduce incentive for HARKing by encouraging publication independent of results, allowing negative findings to be published as well. The review process focuses more on validating the hypothesis and methods rather than the end results.


## What problem or question is the paper addressing?

 The paper seems to be discussing several issues related to "hypothesizing after results are known" (HARKing) in machine learning and deep learning research. Some of the key questions and problems it addresses are:

- Competitiveness in DL research leading to questionable SotA claims and novelty (Section 2): The paper discusses how the competitiveness in improving benchmarks and claiming state-of-the-art results has led researchers to use techniques like "grad student descent" which rely more on trial-and-error rather than sound hypotheses. This makes it difficult to understand why a method works.

- Aversion towards negative results (Section 3): There is a chronic allergy towards publishing negative results in ML/DL research. This creates bias against disruptive ideas, makes it difficult to establish causality, and leads to duplication of efforts. 

- Failure of generalization (Section 4): Many ML models overfit on standard datasets and fail to generalize to real-world data. But researchers treat these datasets as representative of the real world. This is linked to HARKing as hypotheses become conditional on performance on specific datasets.

- Automated ML (Section 5): The complexity of AutoML methods makes it difficult to understand why they work well, and lack of ablation studies creates conditions for HARKing in claiming novelty.

- Issues in developing ethical, human-centric, interpretable, reproducible and accountable AI (Section 6): HARKing hinders progress in making AI systems transparent, fair, and safe. Hypotheses need to consider broader impacts right from the start.

So in summary, the key focus is on how questionable research practices like HARKing in ML/DL fields are leading to models that do not generalize well, are difficult to interpret, biased, and fraught with reproducibility issues. The paper argues these trends need to be addressed through cultural changes and mechanisms to prevent HARKing.


## What are the keywords or key terms associated with this paper?

 Based on reading through the paper, some of the key terms and concepts that emerge are:

- HARKing (Hypothesizing After the Results are Known) - The main phenomenon discussed in the paper, where researchers form hypotheses after seeing results rather than before conducting experiments. 

- Deep learning - The paper focuses specifically on issues with HARKing in deep learning research.

- State-of-the-art (SotA) - The competitive push to achieve marginal improvements on benchmarks and claim new SotA results.

- Negative results - The paper argues there is an allergy or reluctance to publish negative results in ML/DL research.

- Generalization - The problem that models tuned on standard datasets often fail to generalize to real-world data.

- Automated machine learning (AutoML) - Methods to automate architecture search, hyperparameter tuning, etc.

- Ethics - The need for ethical considerations in areas like bias, transparency, and privacy.

- Explainability - Interpreting and providing reasoning for ML model behaviors and decisions.

- Reproducibility - Issues with reproducible research in ML, importance of distinguishing repeatability vs reproducibility. 

- Accountability - Taking accountability for potential harms from AI systems.

So in summary, the key themes cover research methodology issues like HARKing, SotA hacking, and negative results, problems like generalization gap and reproducibility, and ethical considerations around transparency, bias, privacy, accountability etc.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main hypothesis or claim of the paper? 

2. What evidence or arguments does the paper present to support its hypothesis?

3. What examples or case studies are discussed to illustrate the issues raised?

4. What are the key reasons and factors identified as contributing to the problem of HARKing in ML/DL research?

5. What are some of the negative consequences and implications of HARKing discussed?

6. What solutions or recommendations are proposed to address HARKing? 

7. What are the ethical implications discussed related to HARKing and AI systems?

8. How does the paper characterize the current state of ML/DL research methodology? 

9. What future trajectories for ML/DL research are envisioned to promote accountability and scientific rigor?

10. What are the main conclusions drawn and what actions are called for in the paper?

Asking these types of questions should help elicit the key information needed to summarize the major points and contributions of the paper across its different sections and arguments. The goal is to capture the essence of the paper's thesis, supporting evidence/analysis, proposals, and conclusions.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a results-blind review process to decrease HARKing behavior in machine learning research. What are some potential challenges or drawbacks to implementing such a review process? How could they be mitigated?

2. The paper argues that the competitive nature of deep learning research promotes questionable improvements to state-of-the-art methods and claims of novelty. Are there ways the research culture could shift to promote more rigorous hypothesis testing rather than benchmark optimization? 

3. The authors claim that deep neural architectures are very likely to suffer from overfitting due to their expressive power. Could regularization techniques or other methods help prevent this overfitting in complex deep learning models? How might this affect the generalizability of the models?

4. The paper proposes that negative results should be published to avoid bias. However, how could the signal-to-noise ratio be maintained if negative results flood publication venues? What could be done to ensure published negative results are meaningful?

5. The authors criticize automated machine learning techniques like neural architecture search for potential HARKing behaviors. Do you think this criticism is fair? How could AutoML methods be improved to promote scientifically sound practices?

6. The paper discusses issues with common computer vision datasets not representing real-world variability. What could dataset creators do to better capture diversity and avoid overfitting? Are synthetic datasets a potential solution?

7. The authors argue deep learning interpretability methods may enable HARKing by allowing post-hoc rationalization of models. Do you agree with this view? How could interpretation techniques be made more rigorous?

8. The paper claims deep learning research exhibits a reproducibility crisis. What concrete steps could researchers and reviewers take to improve reproducibility? Are there trade-offs between rapid innovation and reproducibility?

9. The authors state hypotheses should consider AI ethics, human-centric design, explainability, and other factors. How could researchers balance these considerations with scientific rigor during hypothesis formulation?

10. The paper covers many issues around deep learning research methodology. Overall, what do you see as the most pressing areas for reform based on the arguments made? Why?
