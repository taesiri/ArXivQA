# [Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the   Censorship of Text-to-Image Generation Model](https://arxiv.org/abs/2312.07130)

## What is the main contribution of this paper?

 The main contribution of this paper is proposing a Divide-and-Conquer Attack (DACA) to circumvent the safety filters of state-of-the-art text-to-image models. Specifically:

1) The paper designs and implements DACA, which utilizes existing large language models (LLMs) as agents to transform sensitive prompts into adversarial prompts that can bypass safety filters while retaining semantic meaning. 

2) The attack is demonstrated to be effective against the safety filter of DALL·E 3, the current leading text-to-image model, across various unethical topics like violence, discrimination, and copyright violations. Experiments show DACA can achieve high success rates in creating adversarial prompts and generating corresponding sensitive images.

3) The attack strategy is shown to be highly cost-effective by leveraging different LLMs. With just $1, it is possible to execute thousands of attacks using an affordable 14B parameter LLM.

4) The intriguing phenomenon is highlighted where LLMs are employed to bypass LLM-powered protections in DALL·E 3. This complicates the dynamics between attack and defense.

In summary, the paper makes significant contributions in developing and evaluating a novel attack strategy that exploits LLMs to effectively circumvent state-of-the-art defenses in text-to-image models to generate unintended sensitive images. The findings have important implications for improving model security.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes using helper prompts to guide the LLM to generate adversarial prompts. What are some key considerations in designing effective helper prompts to extract and process elements from a sensitive image? How might the design of helper prompts be further optimized?

2. The attack is shown to work on DALL-E 3 despite its use of GPT-4 for safety filtering. What implications does this have for the limitations of LLM-powered defense systems? How might the attack inform future development of more robust defense systems? 

3. The paper categorizes prompts into different types like "Get Prompt" and "Process Prompt". What is the rationale behind this categorization? What are the unique roles served by each type of prompt?  

4. The attack relies on the LLM's ability to understand and transform sensitive prompts while retaining semantic meaning. What capabilities of modern LLMs make this attack possible? How might progress in areas like common sense reasoning impact this style of attack?

5. The paper discusses exploiting smaller, lower-cost LLMs as attack assistants. What factors influence the cost-effectiveness of different LLMs for this purpose? How might capabilities vs cost tradeoffs guide threat actors’ LLM selections?

6. The attack prompts exhibit high semantic similarity to original prompts according to CLIP, but evade detection. What implications does this have for the sufficiency of semantic similarity as a detection criterion? How could semantic distance metrics be improved?  

7. The paper hypothesizes extending their attack to other unethical content types beyond their evaluation categories. What types of content might be harder or easier to attack? What image complexities could complicate or assist the attack?

8. The attack transforms the security model to depend on the same foundation (LLMs) for both attack and defense. What new dynamics does this introduce compared to historical attack/defense cycles? How does it complicate defense?

9. What positive usages could this style of pipeline prompt engineering find? Could the attack act as a red teaming tool as hypothesized? What controls and actor incentives would be needed? 

10. The paper proposes post-generation image filtering as a potential defense avenue. What are the cost and capability tradeoffs of client vs server-side image filtering? How could cost feasibility be improved?


## Summarize the paper in one sentence.

 The paper proposes a Divide-and-Conquer Attack (DACA) to circumvent safety filters of state-of-the-art text-to-image models by leveraging LLMs to transform sensitive prompts into adversarial ones, demonstrating successful bypassing against DALL·E 3's protections and generating images reflecting original sensitive intents.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts discussed in this paper include:

- Text-to-image models - The paper discusses generative AI models that create images from text prompts, such as DALL-E, Midjourney, Stable Diffusion, etc.

- Adversarial prompts - Strategically manipulated input text that causes generative AI models to produce unintended, potentially unethical image outputs.

- Safety filters - Mechanisms used by text-to-image models to detect and filter out harmful content. 

- Divide-and-conquer attack (DACA) - The proposed attack method to circumvent safety filters by breaking down prompts into multiple harmless descriptions. 

- Large language models (LLMs) - Models like GPT-3 that are used as agents to transform sensitive prompts into adversarial ones in the DACA attack.

- Semantic similarity - Quantified closeness in meaning between text prompts and generated images, used as an evaluation metric.

- Bypass rate - Proportion of adversarial prompts that successfully trick the safety filter to generate sensitive images.

- Helper prompts - Special prompts developed in DACA to guide the LLM in extracting innocent descriptions of sensitive visual elements.

In summary, key focus areas are adversarial attacks on text-to-image models, use of LLMs to automate attacks, bypassing of safety filters, and evaluation of attack success.
