# [Understanding the Effect of Noise in LLM Training Data with Algorithmic   Chains of Thought](https://arxiv.org/abs/2402.04004)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Chain of thought (CoT) prompting improves large language model (LLM) performance on reasoning tasks. However, CoT data, whether model-generated or from the internet, often contains irrelevant or inconsistent steps (noise). 
- Little is known about how different types and amounts of noise in CoT training data impact downstream task performance. Studying this is important for best practices in data filtering.

Methods
- Develop a framework called Traced Integer (TInt) to generate customizable, noisy algorithmic CoT traces for arithmetic functions. 
- Define two main types of noise:
  - Static: Local noise applied after trace generation. Models errors in reasoning with local effects.
  - Dynamic: Global noise that propagates through the trace as it accumulates, affecting all later steps. Models reasoning missteps with cascading effects.
- Fine-tune LLMs on noisy algorithmic traces for addition, median-finding etc. Vary noise type, intensity and dataset contamination level.
- Prompt LLMs with a few noisy traces as examples and test performance.

Key Results
- Fine-tuned models are extremely robust even to very high levels of static noise, but far more sensitive to smaller amounts of dynamic noise.
- Prompted models exhibit similar trends but are overall more sensitive to both static and dynamic noise compared to fine-tuning.
- even when all traces contain noise, training with slightly noisy CoT is better than no CoT.

Main Contributions
1. TInt framework to generate customizable noisy chains of thought.
2. Quantitative study showing robustness of CoT learning to static noise, and sensitivity to dynamic noise. 
3. Implications for best practices in filtering internet-sourced or model-generated training data, emphasizing removal of destructive dynamic noise.
