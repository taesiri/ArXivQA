# [DECIDER: A Rule-Controllable Decoding Strategy for Language Generation   by Imitating Dual-System Cognitive Theory](https://arxiv.org/abs/2403.01954)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing methods for constrained text generation often greedily focus on target words themselves during decoding, lacking high-level reasoning and control over the generation process. However, humans tend to follow implicit rules and commonsense logic when constructing utterances, not just the targets. 

Proposed Solution:
The paper proposes a novel framework called \modelname{} that introduces rule-based control into the decoding process of pre-trained language models (PLMs). It is inspired by dual-system theory from cognitive science, consisting of an intuitive system (PLM) and a logical system (rule-based reasoner).

The logical reasoner implements the high-level System 2 using first-order logic rules over knowledge graphs. It parses user-defined rules into parse trees and proves whether words meet the rules by recursive traversal. This produces a logic vector indicating satisfying words.

The decision function then explicitly models the interplay between the intuitive and logical systems. It shifts the PLM's next-word distributions towards words that both have high probability according to the PLM and meet the logic rules. This injection happens at each decoding step.

Main Contributions:

- Proposes a novel rule-controllable decoding strategy \modelname{} that guides text generation through high-level logical rules instead of just target words.

- Implements the framework based on dual-system theory, with an intuitive PLM and an explicit logical reasoner. Models interplay through a decision function.

- Logic reasoner incorporates both first-order rules and knowledge graphs to evaluate variable rules. Computed logic vectors shift distributions.

- Demonstrates strong performance over competitive baselines on CommonGen and PersonaChat tasks. Rule-based control leads to more human-like and commonsensical generation.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a rule-controllable decoding strategy called Decider for language generation, which equips a pre-trained language model with a logic reasoner to inject first-order logic rules into the model during decoding to steer the generation direction.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a rule-controllable decoding framework called \modelname{} for language generation. This allows users to logically program and inject high-level rules about how to approach target keywords, instead of greedily focusing on the targets themselves. 

2. It builds \modelname{} as a two-system structure inspired by dual-system theory in cognitive science. A pre-trained language model acts as the intuitive System 1, while a logic reasoner implements the logical and controllable System 2. An explicit decision function models the interplay between the two systems.

3. \modelname{} is demonstrated to be a general framework that allows users to program task-specific rules in first-order logic. Experiments on CommonGen and PersonaChat benchmarks show that \modelname{} can effectively follow given rules to generate higher quality and more controllable text.

In summary, the main contribution is the proposal of a novel rule-controllable decoding strategy that can guide language generation by leveraging logical rules and dual-system theory. The extensive experiments demonstrate its effectiveness.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this work include:

- Rule-controllable decoding strategy
- First-Order Logic (FOL) 
- Logical reasoner
- Dual-system theory/thinking
- System 1 and System 2
- Knowledge graph
- Controllable text generation
- Constrained decoding
- Pre-trained language models (PLMs)
- Attention distribution perturbation
- CommonGen (constrained commonsense generation benchmark)
- PersonaChat (personalized response generation dataset)

The paper proposes a novel decoding framework called \modelname{} that can control text generation from pre-trained language models by injecting logical rules during the decoding process. It is inspired by human dual-system theory and consists of a logical reasoner module that evaluates rules in first-order logic and perturbs the attention and prediction distributions of the language model. The approach is evaluated on commonsense generation and personalized dialog tasks, outperforming existing constrained decoding methods. Some of the key concepts include the logical reasoner, rule formulation, integrating logical signals with language model probabilities through a decision function, and benchmark performances on CommonGen and PersonaChat datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a dual system framework consisting of a pre-trained language model (PLM) and a logical reasoner. Can you explain in more detail how these two components interact and influence each other during the decoding process? 

2. The logical reasoner component evaluates first-order logic rules to produce a logic vector over the vocabulary at each decoding step. What are the key steps involved in going from a high-level textual rule to the final numeric logic vector?

3. The decision function combines the probability distribution from the PLM and the logic vector from the logical reasoner. What is the intuition behind the specific mathematical formulation of the decision function? How does it balance following the logical rules versus maintaining fluency?

4. How does the proposed framework handle quantification (existential and universal quantifiers) in the logical rules? What is the equivalence used to transform them into computable logic connectives? 

5. One contribution of the paper is explicitly modeling the interplay between intuitive decisions of the PLM (System 1) and deliberate logical reasoning (System 2). How does the proposed architecture reflect principles from dual process theory in cognitive science?

6. How does the perturbation of attention distributions in the PLM allow influencing the decoding process? What are the differences between shifting self-attention over previous words versus attention over target concepts?

7. What modifications need to be made to the standard transformer decoder architecture of the PLM to incorporate the logical signals within the framework?

8. What are the tradeoffs between soft versus hard predicate logic calculus? How does the former lead to better performance by treating semantic relevance in a more nuanced way?

9. The framework can handle different varieties of constrained decoding tasks. Can you compare how the methodology differs when applied to lexically constrained versus semantically constrained generation?

10. An advantage of the proposed approach is flexibility via its programmability using first-order logic rules. What are some other possible applications that can benefit from user-defined rule injection during inference?
