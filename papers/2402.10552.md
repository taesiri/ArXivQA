# [Conversational SimulMT: Efficient Simultaneous Translation with Large   Language Models](https://arxiv.org/abs/2402.10552)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Simultaneous machine translation (SimulMT) aims to provide real-time translation of a stream of source text. This is challenging as it requires balancing tradeoffs between translation quality and latency.
- Recent works have shown that large language models (LLMs) can achieve good SimulMT performance. However, this comes at the cost of high inference time and latency due to the disruption of reuse of cached states from inserting new source tokens in the middle of the prompt.

Proposed Solution:
- Propose conversational SimulMT framework that formats the task as a dialogue between user inputs (source chunks) and agent responses (translations). 
- New source chunks form the current input, while previous source/target chunks are conversation history. This allows incremental appending without disrupting cache reuse.
- Perform supervised fine-tuning on curated SimulMT data in conversational format to adapt LLMs. Data is created by segmenting sentence pairs using alignment tools and augmentation.

Main Contributions:
- Propose conversational prompting strategy to reformulate SimulMT as a dialogue task to improve decoding efficiency of LLM-based SimulMT
- Curate conversational SimulMT data from parallel corpus using alignment and augmentation techniques.
- Experiments on two benchmarks show proposed approach maintains strong SimulMT performance while reducing latency compared to offline prompting baseline.
- Conversational prompting aligns LLM-based SimulMT decoding speed with specialized SimulMT models.

In summary, the paper introduces an efficient conversational SimulMT framework using LLMs that balances translation quality and latency through multi-turn dialogue-style decoding. Curated conversational data and prompting strategy equip LLMs with simultaneous translation capabilities.
