# [Unsupervised Learning of Video Representations using LSTMs](https://arxiv.org/abs/1502.04681)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we learn good video representations in an unsupervised manner using recurrent neural networks?More specifically, the authors explore using LSTM encoder-decoder models to learn representations of video sequences in an unsupervised way. The key questions and goals include:- What recurrent neural network architectures are most effective for learning useful video representations without supervision? They explore autoencoder models, future prediction models, and composite models.- How do different design choices like conditional vs unconditional decoders affect the learned representations?- Can the learned representations capture useful structure like motion and appearance information to allow tasks like reconstructing inputs and predicting future frames?- Do the learned features transfer to improve performance on supervised tasks like action recognition, especially when training data is limited?- How do the learned models perform on longer timescales vs what they were trained on? Can they generalize? - What do the learned input and output features look like? Can we interpret what is being represented?So in summary, the key research question is around developing and analyzing recurrent neural network models that can learn spatiotemporal representations from video in an unsupervised manner, and using the learned features to improve video understanding tasks. The models, objectives, architectures and analyses are focused on this goal.
