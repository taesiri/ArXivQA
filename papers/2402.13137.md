# [The Hidden Space of Transformer Language Adapters](https://arxiv.org/abs/2402.13137)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper analyzes how language adapters work when adapting a pretrained language model (LM) from one or more source languages to a new target language. Specifically, it aims to understand how the adapted predictions evolve internally within the LM and how the adapters interact with the frozen parameters of the underlying LM.

Proposed Solution: 
The authors conduct controlled experiments by pretraining LMs from scratch on English, adapting them to other languages like German, French, Hebrew and Arabic using adapters, and analyzing the hidden representations. They also experiment with bilingual English-German models and multilingual models like mBERT.

Key Findings:

1) Adapted predictions mostly evolve in the source language space, with the target language becoming pronounced only in the very last layers. This suggests the model "thinks" in the source language distribution and only translates predictions to the target language at the end.

2) The adaptation process with adapters is gradual and distributed across layers. Many adapter layers can be removed without decreasing performance, while the last few are critical.

3) Adapting to more distant languages like Hebrew/Arabic requires larger adapter updates, seen by larger norms and higher impact on perplexity when removed.

4) Adapters operate on top of the frozen model's representation space, largely preserving its structure, rather than in an isolated subspace. Structures related to linguistic properties like POS, number and tense remain highly aligned before and after adaptation.

Main Contributions:

- Provides novel insights into how language adapters steer the frozen LM's prediction process towards a new target language.

- Shows adapted predictions primarily evolve in the source language due to constraints imposed by the underlying model.

- Reveals the adaptation process with adapters is gradual and distributed across layers.

- Demonstrates adapters operate on top of the modelâ€™s representation space while preserving its structure.

The findings open up further research to make language adaptation more efficient based on language similarity and design adapters less constrained by the pretrained model's limitations.
