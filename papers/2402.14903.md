# [Tokenization counts: the impact of tokenization on arithmetic in   frontier LLMs](https://arxiv.org/abs/2402.14903)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Tokenization is an important but often overlooked part of the large language model (LLM) pipeline. Different tokenization strategies for numbers can impart different inductive biases in models. 
- Many recent LLMs are moving to single-digit tokens for numbers, but GPT-3.5 and GPT-4 use a mix of 1-, 2-, and 3-digit number tokens. This paper studies the effect of this choice on arithmetic reasoning.

Methods 
- The authors evaluate GPT-3.5 and GPT-4 on addition problems with numbers 7-9 digits long. 
- They vary the tokenization direction: default left-to-right (L2R) versus right-to-left (R2L) enforced by comma separation. 
- They conduct detailed error analysis to understand performance differences.

Key Findings
- R2L tokenization leads to ~20% higher accuracy over L2R.
- L2R tokenization leads to very stereotyped "digit 4" errors when the answer is longer than the addends.
- Models can self-correct issues with L2R tokenization if prompted to repeat the problem in R2L format before answering.
- Tokenization-dependent effects persist in newer model versions, but become less prominent in larger models.

Main Contributions
- First thorough demonstration that number tokenization significantly impacts arithmetic reasoning in LLMs.
- Detailed error analysis revealing systematic patterns based on tokenization.
- Effective prompting strategy to mitigate issues.
- Benchmarking various model versions shows effects diminish but persist with scale.

Overall the paper makes a compelling case that number tokenization is an overlooked source of inductive bias in LLMs that can strongly influence numerical reasoning abilities. The authors provide guidance to practitioners on tokenization choices and methods to overcome detrimental biases introduced by some schemes.
