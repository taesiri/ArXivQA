# [Label Supervised LLaMA Finetuning](https://arxiv.org/abs/2310.01208)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: What is the feasibility and effectiveness of adapting large language models (LLMs) for supervised sequence and token classification tasks through label-supervised finetuning?In particular, the paper investigates:- Whether the latent representations extracted from the decoder layers of LLMs like LLaMA can serve as effective text encodings for classification tasks when paired with label supervision. - How finetuning LLMs in a label-supervised manner, termed LS-LLaMA, can improve their performance on multiclass text classification benchmarks compared to zero-shot, few-shot, and instruction-tuned LLMs.- Whether removing the causal masks from LLaMA decoders, termed LS-unLLaMA, can further boost performance on token-level tasks like named entity recognition (NER) by allowing bidirectional context.The central hypothesis is that label-supervised finetuning can unlock the potential of LLM representations for discriminative classification tasks they were not originally designed for, surpassing prompting and instruction-tuning approaches. The results provide evidence for the viability of adapting LLMs through direct label supervision and show the benefits of unmasking for token tasks.In summary, the paper explores whether LLMs can be adapted to excel at classification by extracting and finetuning their latent representations in a label-supervised manner, which is a novel approach compared to leveraging their generative capabilities. The effectiveness of this technique is evaluated empirically on both sequence and token classification datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing a label-supervised adaptation method for large language models (LLMs) like LLaMA. The key idea is to extract latent representations from the LLM decoders and use them for supervised discriminative label prediction. 2. Proposing two variants - LS-LLaMA which keeps the causal masks in the decoder, and LS-unLLaMA which removes the causal masks. Removing the masks enables attending bidirectionally and improves performance on token-level tasks.3. Demonstrating through extensive experiments that both LS-LLaMA and LS-unLLaMA achieve substantial improvements over strong baselines like BERT and RoBERTa on text classification benchmarks. The adapted LLMs also reach state-of-the-art on named entity recognition. 4. Showing the feasibility of effectively utilizing the latent representations from LLMs for a range of downstream classification tasks. This provides a novel perspective on leveraging the knowledge learned by LLMs.5. Identifying that the performance does not necessarily improve with larger LLM size due to overfitting on limited training data. Sufficient labeled data is important for effectively finetuning large LLMs.In summary, the key contribution is proposing and validating a simple yet effective label-supervised adaptation approach to enhance LLM performance on classification tasks, unveiling the potential of LLM latent representations, and providing insights into finetuning considerations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a label-supervised adaptation approach for finetuning large language models like LLaMA on text classification and named entity recognition tasks, demonstrating improved performance over strong baselines like BERT without requiring extensive prompt engineering or external knowledge.
