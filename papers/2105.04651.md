# [Not All Relevance Scores are Equal: Efficient Uncertainty and   Calibration Modeling for Deep Retrieval Models](https://arxiv.org/abs/2105.04651)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:How can we efficiently model uncertainty in neural information retrieval models to improve their effectiveness and reliability? Specifically, the authors are interested in developing an efficient Bayesian framework to capture both aleatoric and epistemic uncertainty in neural retrieval models. Their key hypotheses are:1) An approximate Bayesian framework using only the last layer of a neural retrieval model can capture useful uncertainty information while adding minimal computational overhead. 2) The uncertainty information from this Bayesian framework can improve model calibration, risk-aware reranking, and performance on downstream tasks like cutoff prediction.3) Generative neural IR models that report their own uncertainty are not as robust as the proposed Bayesian models.The authors propose using Monte Carlo dropout as an efficient variational Bayesian approximation. They show this captures useful uncertainty information with minimal overhead, improves model calibration and risk-aware reranking, and boosts cutoff prediction performance. The Bayesian models significantly outperform generative models at conveying robust uncertainty estimates.In summary, the paper introduces an efficient Bayesian framework for neural IR models to improve their effectiveness and reliability by better modeling uncertainty. The key hypothesis is that even approximate Bayesian inference restricted to just the last layer can provide substantial benefits.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an efficient Bayesian framework to estimate epistemic and aleatoric uncertainty in neural retrieval models. Specifically:- They show that by only making the last layer(s) of a neural retrieval model Bayesian using Monte Carlo dropout, they can capture both types of uncertainty with minimal computational overhead. This allows the uncertainty modeling to be applied to large models like BERT.- They introduce a new calibration metric called expected ranking calibration error (ERCE) to measure how well a model's uncertainty correlates with its accuracy on ranking relevant documents higher. The Bayesian models have significantly better calibration. - They demonstrate the usefulness of the uncertainty information by using it to do risk-aware reranking, getting gains of 3-5% nDCG. The uncertainty is also shown to improve performance on the downstream task of cutoff prediction by around 10%.- Overall, the paper shows an efficient way to make neural ranking models Bayesian to quantify uncertainty, and empirically validates that this uncertainty provides meaningful information that can improve ranking and downstream tasks. The minimal computational overhead makes it feasible to apply to large neural models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient Bayesian framework to capture uncertainty in neural ranking models by treating dropout as approximate inference, enabling improved ranking, calibration, and downstream performance.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in the field of uncertainty modeling for neural information retrieval:- Most prior work has focused on capturing uncertainty through ensemble methods or query performance/cutoff prediction using deterministic model outputs. This paper proposes an efficient Bayesian framework to directly model uncertainty within the retrieval model itself.- The Bayesian framework introduces minimal computational overhead, making it feasible to apply to large transformer models like BERT. This is an advantage over ensemble approaches that require running multiple models.- The paper demonstrates that modeling uncertainty improves ranking calibration, risk-aware reranking, and downstream tasks like cutoff prediction. This shows the usefulness of uncertainty information beyond just the raw relevance scores.- For calibration, the paper proposes a new expected ranking calibration error metric tailored to measuring calibration of ranked lists rather than just scalar predictions.- The Bayesian models are shown to be better calibrated and achieve stronger performance on downstream tasks compared to deterministic baselines and a generative retrieval model baseline.- The framework is model-agnostic, shown to be effective for both hand-crafted models like Conv-KNRM and large pre-trained models like BERT.Overall, this paper presents a novel and efficient way to model uncertainty in neural retrieval that conveys useful information for improving ranking, calibration, and downstream tasks. The model-agnostic Bayesian framework and new calibration metric are valuable contributions compared to prior work focused on ensemble and post-hoc uncertainty methods.
