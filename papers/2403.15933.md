# [Understanding Domain-Size Generalization in Markov Logic Networks](https://arxiv.org/abs/2403.15933)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Statistical relational learning (SRL) models like Markov logic networks (MLNs) perform poorly when applied to data from different domain sizes than what they were trained on. 
- This happens because relational data does not admit consistency of parameter estimation - the maximum likelihood estimate does not converge to the true parameters as more data is obtained.
- Existing theoretical work shows SRL models are generally not projective - the marginal distribution induced over substructures does not match that over the full structure. However, it is unclear what quantitative statements can be made about an SRL model's generalization behavior across domain sizes.

Proposed Solution:
- The paper provides a theoretical analysis specifically for MLNs to characterize their generalization behavior across domain sizes. 
- Key idea: The variance of the MLN's parameters, captured by the difference between maximum and minimum template weights, determines how inconsistent the model is when applied to different domain sizes.
- Main result: Authors derive analytic bounds relating the MLN's distributions over the observed substructure and the full unseen structure. These bounds depend on the parameter variance.
- Minimizing negative log-likelihood of observed data while reducing parameter variance is shown to improve log-likelihood on unseen larger domain, and reduce KL divergence between distributions over different domain sizes.

Contributions:
- First analytic characterization of domain size generalization for MLNs showing connection to parameter variance
- Demonstrate optimizing likelihood while reducing variance matches objectives for generalization
- Proof that known variance-reduction methods like regularization and domain size aware MLNs should improve generalization 
- Empirical verification on 4 datasets substantiates that smaller parameter variance leads to better generalization across domain sizes
