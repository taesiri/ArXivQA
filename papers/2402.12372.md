# [HunFlair2 in a cross-corpus evaluation of biomedical named entity   recognition and normalization tools](https://arxiv.org/abs/2402.12372)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Biomedical named entity recognition (NER) and normalization (NEN) are crucial steps in biomedical text mining pipelines. However, most tools are evaluated on the same corpus they are trained on (in-corpus), which does not reflect real-world performance when applied to new texts (in the wild). This paper investigates how current state-of-the-art biomedical NER and NEN tools perform in a cross-corpus setting.

Methodology:
- Selected 5 mature biomedical NER and NEN tools based on criteria like availability, feature richness, and license requirements. The tools were PubTator, BERN2, SciSpacy, bent, and HunFlair2 (an updated version of HunFlair).

- Tools were evaluated on 3 biomedical corpora covering 4 entity types (genes, diseases, chemicals, species) that were explicitly not used in training by any tool.

- Micro average F1 score was used as the evaluation metric at the mention level. Predictions had to match gold spans and identifiers to be considered correct.

Results:
- Performance of all tools dropped considerably compared to published in-corpus results, with average F1 scores being 36-59% versus typically >80% for in-corpus.

- HunFlair2 performed best overall with 59% F1, followed by PubTator (54%). HunFlair2 was best at extracting chemicals and diseases, PubTator at genes and species.

- Analysis showed tools struggled with less frequent entities versus very common ones like "mouse" genes.

- NER-only experiments showed robust models but big NEN performance gaps.

Contributions:
- First large-scale cross-corpus evaluation of state-of-the-art biomedical NER and NEN tools.

- Addition of an improved neural NEN module to HunFlair tool â†’ HunFlair2.

- Demonstrated significant real-world performance declines versus published in-corpus results.

- Showed need for more generalization in tools to handle new biomedical subdomains and text types robustly.

In summary, the paper clearly highlighted limitations of current biomedical NER and NEN tools in realistic application scenarios and identified areas needing more research to make such tools more robust. The newly developed HunFlair2 tool achieved the most stable performance across domains.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper presents the first cross-corpus evaluation of five state-of-the-art biomedical named entity extraction tools on three corpora covering four entity types, showing stark performance variations compared to published in-corpus results and difficulties in accurately identifying rarely occurring concepts.


## What is the main contribution of this paper?

 The main contribution of this paper is a carefully designed cross-corpus benchmark for evaluating state-of-the-art biomedical named entity extraction tools. Specifically:

1) The paper reviews 28 recent biomedical text mining tools and selects 5 mature, easy-to-use tools (BERN2, bent, HunFlair2, PubTator, SciSpacy) that support both NER and NEN for common biomedical entities.

2) The authors introduce HunFlair2, an updated version of HunFlair that adds entity normalization capabilities using transformer models.

3) The selected tools are evaluated on 3 corpora spanning 4 entity types that were explicitly chosen not to overlap with any tool's training data. This cross-corpus evaluation assesses the tools' ability to generalize.

4) The benchmark experiments highlight considerable performance variations compared to published in-corpus results, indicating that further research is needed to make biomedical named entity extraction tools more robust to shifts in text characteristics.

In summary, the key contribution is the carefully designed cross-corpus benchmark and analysis that provides more realistic estimates of state-of-the-art tools' capabilities and quantifies the performance drops compared to common in-corpus evaluations. The paper also introduces an updated entity extraction tool, HunFlair2.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Biomedical text mining (BTM)
- Named entity recognition (NER) 
- Named entity normalization (NEN)
- Entity extraction
- Cross-corpus evaluation
- Benchmarking
- Robustness
- Generalization capabilities
- HunFlair2
- PubTator
- BERN2
- SciSpacy
- bent

The paper presents a cross-corpus evaluation and benchmarking of five biomedical entity extraction tools - HunFlair2, PubTator, BERN2, SciSpacy, and bent. It examines their robustness and generalization capabilities when applied to new corpora outside of their training data. Key aspects examined are named entity recognition and normalization for genes, diseases, chemicals, and species entities. The cross-corpus results are compared to published in-corpus evaluations to highlight the performance differences. Overall, HunFlair2 and PubTator show the most promising results, but the analysis indicates that further research is needed to make these BTM tools more robust to shifts in text types, genres, and entity distributions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. This paper introduces HunFlair2 as an updated version of HunFlair for biomedical named entity recognition and normalization. What are the main differences between HunFlair2 and the original HunFlair model?

2. The paper proposes a cross-corpus evaluation protocol to assess the robustness of biomedical entity extraction tools when applied to new texts. What are the main benefits and limitations of cross-corpus evaluation compared to traditional in-corpus evaluation? 

3. Five biomedical entity extraction tools are selected for evaluation based on specific criteria like supporting both NER and NEN, using machine learning, and extracting common biomedical entities. What are these tools and what are their key technical characteristics? 

4. Three corpora are used for the cross-corpus experiments - BioID, MedMentions, and tmVar v3. Why are these particular datasets selected and what type of texts do they contain? 

5. The paper reports performance in terms of micro mention-level F1 score for end-to-end named entity extraction. What does this metric capture and what are its limitations in assessing real-world performance?

6. What is the overall best performing tool in the cross-corpus experiments and why does it achieve stronger results than the other candidates? Where does it still demonstrate weaknesses?

7. The results show lower performance compared to published in-corpus results for the tools. What factors contribute to this performance gap between in-corpus and cross-corpus evaluation?

8. How do the tools perform in extracting infrequent vs frequent entities? What can be the reasons behind the observed differences?

9. The paper also examines document-level entity extraction as an additional evaluation scenario. How do the results differ in this setting and what use cases does it represent?

10. Based on the cross-corpus analysis, what open issues remain in developing robust biomedical entity extraction systems and what directions for future research does the paper suggest?
