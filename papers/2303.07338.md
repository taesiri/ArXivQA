# [Revisiting Class-Incremental Learning with Pre-Trained Models:   Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is: How can we effectively leverage pre-trained models (PTMs) for class-incremental learning (CIL)? 

Some key points:

- The paper argues that CIL requires models that balance adaptivity (to new data) and generalizability (of learned knowledge). 

- Traditional CIL methods assume training from scratch and focus on adaptivity. However, PTMs possess inherent generalizability that could be valuable.

- The paper shows that a simple baseline using a frozen PTM can outperform current state-of-the-art CIL methods, demonstrating the generalizability of PTMs.

- However, PTMs may still need adaptivity to align with incremental dataset domains. The paper proposes ADAM to adapt the PTM initially and then merge it with the original to balance adaptivity and generalizability.

- The paper also advocates new benchmarks without overlap for evaluating PTM-based CIL.

In summary, the central hypothesis is that leveraging PTMs for CIL requires properly utilizing both the generalizability they provide and adaptivity to the incremental datasets, which ADAM aims to achieve in a simple and unified manner.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It revisits class-incremental learning (CIL) in the era of pre-trained models (PTMs). Previous CIL methods focus on training models from scratch to continually acquire knowledge. But PTMs provide generalizable embeddings that can be directly leveraged for CIL. The paper reveals that a simple baseline using frozen PTMs can outperform current state-of-the-art CIL methods.

2. It proposes ADapt And Merge (ADAM), a framework to unify the generalizability of PTMs and adaptivity of fine-tuned models for CIL. ADAM adapts the PTM on the first incremental dataset to obtain task-specific features. It then merges the adapted model and original PTM to extract features for classifier learning. This maintains both generalizability and adaptivity.

3. It finds that previous CIL benchmarks have large overlap with pre-training data, making them unsuitable to evaluate PTM-based methods. So it proposes four new datasets - ImageNet-A, ObjectNet, OmniBenchmark, and VTAB for benchmarking. These have no overlap and large domain gaps with pre-training data.

4. Extensive experiments validate the effectiveness of the simple frozen PTM baseline and the proposed ADAM framework. ADAM consistently outperforms previous SOTA methods across various backbones and datasets. The new benchmarks also better measure the generalization of PTM-based CIL.

In summary, the key contribution is a new perspective on CIL in the era of PTMs, proposing ADAM to unify generalizability and adaptivity, and new benchmarks to evaluate this. The simple yet effective design of ADAM also stands out.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper revisits class-incremental learning with pre-trained models and shows that frozen pre-trained models provide generalizable embeddings for competitive performance, while model adaptation techniques like VPT and adapters can further enhance adaptivity; new benchmarks with large domain gaps are also introduced to properly evaluate CIL in the era of pre-training.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in class-incremental learning (CIL):

- It focuses on revisiting CIL in the era of pre-trained models (PTMs). Many recent works have explored using PTMs like Vision Transformers for CIL. This paper systematically analyzes the role of PTMs and proposes a new method building on their strengths.

- The paper reveals that a simple baseline using a frozen PTM and prototype classifiers can already beat current state-of-the-art CIL methods. This is an interesting finding showing the power of pre-training for CIL.

- The proposed ADAM method adapts the PTM on the first incremental batch to improve adaptivity while still leveraging the generalizability of the original PTM features. This effectively merges adaptivity and generalizability.

- The paper advocates new benchmarks like ImageNet-A, ObjectNet, OmniBenchmark, and VTAB that have a larger domain gap from ImageNet pre-training. This is important for better evaluating PTM-based CIL.

- Extensive experiments show ADAM consistently improves over baselines and outperforms recent state-of-the-art CIL methods including L2P and DualPrompt. The gains are especially large on the new proposed benchmarks.

Overall, this paper provides new insights into PTM-based CIL and proposes an effective yet simple approach unifying adaptivity and generalizability. The new benchmarks are also an important contribution for future research. The consistent gains over current methods on various datasets demonstrate the impact of this work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring task-specific tuning methods and structures for class-incremental learning with pre-trained models. The authors mention that future work could include designing tuning techniques tailored for incremental learning settings. This could help further enhance the adaptivity of pre-trained models to new tasks and datasets.

- Evaluating class-incremental learning in the exemplar-based setting when historical data is available. The current work focuses on the exemplar-free setting, but the availability of stored examples could change the dynamics of how to balance adaptivity and generalizability.

- Studying how to determine the optimal number of tuning stages. The paper adapts models in the first incremental stage, but determining the ideal number of stages to tune could be explored.

- Extending the work to other data modalities and pre-trained models beyond vision. The current research is on image classification, but expanding to other data types (e.g. text, audio) and model architectures could be promising.

- Developing more benchmark datasets with a large domain gap from pre-training data. The paper introduces some new benchmarks, but creating more diverse out-of-distribution datasets for evaluation would be useful.

- Investigating the role of model size on transferability. Larger pre-trained models seem to generalize better, but systematically studying model scale could provide more insights.

- Continual learning in online or streaming data settings. The current setup assumes distinct tasks, but online learning where data continuously evolves merits exploration.

Overall, the paper provides a solid basis for future work on effectively leveraging pre-trained models for class-incremental learning across a variety of dimensions. The combination of generalizability and adaptivity offers a promising direction.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach for class-incremental learning (CIL) with pre-trained models (PTMs). It first shows that a simple baseline using frozen features from a PTM can outperform current state-of-the-art CIL methods, demonstrating the strong generalizability of PTMs. However, PTMs can be further improved with adaptivity to the incremental dataset. Thus, the paper proposes ADapt And Merge (ADAM), which adapts the PTM on the first incremental dataset for improved adaptivity while still leveraging the original PTM features to maintain generalizability. Specifically, ADAM adapts the PTM with efficient tuning techniques like adapters or prompt tuning, then freezes and concatenates the adapted and original PTM to extract class prototypes for incremental learning. Experiments on new CIL benchmarks with large domain shifts from ImageNet show ADAM consistently improves over strong baselines. The unified framework enhances both generalizability and adaptivity for incremental learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents ADapt And Merge (ADAM), a new framework for class-incremental learning (CIL) with pre-trained models (PTMs). CIL aims to continuously acquire knowledge as data evolves, while PTMs possess generalizable embeddings that can be transferred to new tasks. 

The key insight is that CIL requires both adaptivity to grasp task-specific features and generalizability for knowledge transfer. ADAM achieves this by adapting the PTM on the first incremental dataset to enhance adaptivity. It then concatenates the adapted model with the original PTM and extracts prototypes as the classifier weights. This maintains the generalizability of the PTM while incorporating adaptivity. Experiments show ADAM consistently improves baseline PTMs on new CIL benchmarks with large domain gaps from ImageNet. The simple framework can also be combined with various parameter-efficient tuning techniques. Overall, ADAM provides a strong yet efficient CIL approach by unifying adaptivity and generalizability.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes ADapt And Merge (ADAM), a new framework for class-incremental learning (CIL) with pre-trained models (PTMs). The key insight is that PTMs provide generalizable embeddings that can be directly used for CIL, but they lack adaptivity to the target dataset. To address this, ADAM adapts the PTM on the first incremental dataset to make the model more adaptive. It then concatenates the embeddings of the original PTM and adapted model, and extracts prototypes on this joint embedding to construct the classifier weights. This allows ADAM to leverage both the generalizability of the original PTM and the adaptivity of the tuned model. After adaptation, the embedding functions are frozen and only the prototype-based classifiers are updated in subsequent incremental steps. Experiments on new CIL benchmarks with large domain gaps show ADAM outperforms existing PTM-based CIL methods with a simple and unified framework.


## What problem or question is the paper addressing?

 Based on my understanding, the key points of this paper are:

1. It revisits class-incremental learning (CIL) in the context of pre-trained models (PTMs). CIL aims to continuously learn from new classes without forgetting old ones, while PTMs provide generalizable features that can be leveraged for CIL. 

2. It argues that CIL with PTMs relies on two key factors - generalizability for knowledge transfer from the PTM and adaptivity to learn task-specific features.

3. It shows that a simple baseline using a frozen PTM with prototypical classifiers can beat current state-of-the-art CIL methods. This demonstrates the strong generalizability of PTMs.

4. To improve adaptivity, it proposes ADapt and Merge (ADAM), which adapts the PTM on the first incremental batch and then merges it with the original PTM. This maintains generalizability while improving adaptivity.

5. It proposes new benchmarks like ImageNet-A, ObjectNet, etc. that have a large domain gap from ImageNet to better evaluate CIL with PTMs.

6. Experiments show ADAM consistently improves over the strong frozen PTM baseline and outperforms current state-of-the-art on the new benchmarks.

In summary, the key contribution is a simple yet effective approach to leverage generalizability and adaptivity of PTMs for class-incremental learning, demonstrated through strong empirical results on new benchmarks.


## What are the keywords or key terms associated with this paper?

 Based on skimming through the paper, some of the key terms and keywords associated with this paper include:

- Class-Incremental Learning (CIL): The paper focuses on this problem setting where new classes are added sequentially and the model needs to continually learn without forgetting old classes. 

- Pre-Trained Models (PTMs): The paper studies utilizing large pre-trained models like ViT and CNNs for CIL.

- Generalizability: The ability of pre-trained models to transfer knowledge due to training on massive datasets. Frozen PTMs can provide generalizable embeddings.

- Adaptivity: The capability of models to adapt to new datasets/tasks through tuning techniques. Essential for bridging domain gaps in CIL.

- ADapt And Merge (ADAM): The proposed method that combines a frozen PTM and an adapted model to unify generalizability and adaptivity.

- Parameter-efficient tuning: Methods like visual prompt tuning, adapter modules, and batch norm tuning to efficiently adapt PTMs with few extra parameters.

- New CIL benchmarks: The paper argues traditional benchmarks have data overlap with PTMs, so new datasets like ImageNet-A, ObjectNet, OmniBenchmark, and VTAB are proposed.

So in summary, the key terms revolve around utilizing pre-trained models for class-incremental learning, and proposing ADAM to balance generalizability and adaptivity in a unified framework. New benchmarks are also introduced to better evaluate CIL with PTMs.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask in order to summarize the key points of a research paper:

1. What is the research question or problem being addressed? This helps establish the overall focus and goals of the work.

2. What is the proposed method or approach? Understanding the techniques used and models developed is central to grasping the paper's contributions. 

3. What datasets were used, if any? The choice of data can influence results and determine if the method generalizes.

4. What were the major findings or results? Identifying key results provides the main outcomes of the research.

5. How were the results validated or evaluated? Knowing how the authors measured success further supports the main findings.

6. How do the results compare to prior state-of-the-art methods? Situating the advances amongst previous work gives context.

7. What are the limitations of the proposed method? Recognizing weaknesses and assumptions helps qualify the claims.

8. What broader impacts might this research have? Considering potential applications underscores real-world relevance. 

9. What future work does the paper suggest? Next steps indicate how the work might progress.

10. What are the key takeaways? Summarizing the main conclusions and importance ties everything together.

Asking questions that cover this range of topics will help elicit the essential information needed to thoroughly yet concisely summarize the key points of a research paper. The answers help identify the most significant details to include in the summary.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a simple baseline method called SimpleCIL that directly uses the frozen features of a pre-trained model for class-incremental learning. How does freezing the pre-trained features help maintain knowledge about old classes in SimpleCIL? What are the limitations of this simple approach?

2. The paper argues that both adaptivity and generalizability are important for class-incremental learning with pre-trained models. How does the proposed ADAM framework balance these two factors? Why is adaptivity needed if pre-trained features already provide some generalization?

3. ADAM adapts the pre-trained model on the first incremental training set. How does adapting on just the first set help prevent catastrophic forgetting of old classes? What challenges arise from only adapting on the first set?

4. The paper experiments with different techniques like visual prompt tuning, adapters, and scale/shift layers for efficient adaptation of the pre-trained model. What are the trade-offs between these techniques? Which one works best for ADAM and why?

5. After adapting the model, ADAM freezes and concatenates the original and adapted feature extractors. Why is concatenation used instead of selecting one or the other? What benefits does this merged embedding provide?

6. ADAM constructs the classifier using the prototypical network approach on the concatenated features. Why are prototypes suitable for the frozen merged features? How does this classifier avoid forgetting old classes?

7. The paper introduces new benchmarks like ImageNet-A, ObjectNet, OmniBenchmark for evaluating pre-trained models in class-incremental learning. What limitations exist with the old CIL benchmarks like CIFAR100 and ImageNet when using pre-trained models?

8. Across different datasets, which adaptation technique (visual prompts, adapters etc.) works best with ADAM? Are there certain techniques better suited for some datasets? Why might this be the case?

9. How does the performance of ADAM vary with different pre-trained models like ViT, ResNet as the backbone? Is ADAM equally effective for convolutional and transformer networks?

10. The paper shows ADAM outperforms prior state-of-the-art methods like L2P and DualPrompt. What improvements does ADAM make over these prior arts? Does it add much complexity compared to the baselines?
