# [Revisiting Class-Incremental Learning with Pre-Trained Models:   Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we effectively leverage pre-trained models (PTMs) for class-incremental learning (CIL)? Some key points:- The paper argues that CIL requires models that balance adaptivity (to new data) and generalizability (of learned knowledge). - Traditional CIL methods assume training from scratch and focus on adaptivity. However, PTMs possess inherent generalizability that could be valuable.- The paper shows that a simple baseline using a frozen PTM can outperform current state-of-the-art CIL methods, demonstrating the generalizability of PTMs.- However, PTMs may still need adaptivity to align with incremental dataset domains. The paper proposes ADAM to adapt the PTM initially and then merge it with the original to balance adaptivity and generalizability.- The paper also advocates new benchmarks without overlap for evaluating PTM-based CIL.In summary, the central hypothesis is that leveraging PTMs for CIL requires properly utilizing both the generalizability they provide and adaptivity to the incremental datasets, which ADAM aims to achieve in a simple and unified manner.
