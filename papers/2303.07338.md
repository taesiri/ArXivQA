# [Revisiting Class-Incremental Learning with Pre-Trained Models:   Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we effectively leverage pre-trained models (PTMs) for class-incremental learning (CIL)? Some key points:- The paper argues that CIL requires models that balance adaptivity (to new data) and generalizability (of learned knowledge). - Traditional CIL methods assume training from scratch and focus on adaptivity. However, PTMs possess inherent generalizability that could be valuable.- The paper shows that a simple baseline using a frozen PTM can outperform current state-of-the-art CIL methods, demonstrating the generalizability of PTMs.- However, PTMs may still need adaptivity to align with incremental dataset domains. The paper proposes ADAM to adapt the PTM initially and then merge it with the original to balance adaptivity and generalizability.- The paper also advocates new benchmarks without overlap for evaluating PTM-based CIL.In summary, the central hypothesis is that leveraging PTMs for CIL requires properly utilizing both the generalizability they provide and adaptivity to the incremental datasets, which ADAM aims to achieve in a simple and unified manner.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It revisits class-incremental learning (CIL) in the era of pre-trained models (PTMs). Previous CIL methods focus on training models from scratch to continually acquire knowledge. But PTMs provide generalizable embeddings that can be directly leveraged for CIL. The paper reveals that a simple baseline using frozen PTMs can outperform current state-of-the-art CIL methods.2. It proposes ADapt And Merge (ADAM), a framework to unify the generalizability of PTMs and adaptivity of fine-tuned models for CIL. ADAM adapts the PTM on the first incremental dataset to obtain task-specific features. It then merges the adapted model and original PTM to extract features for classifier learning. This maintains both generalizability and adaptivity.3. It finds that previous CIL benchmarks have large overlap with pre-training data, making them unsuitable to evaluate PTM-based methods. So it proposes four new datasets - ImageNet-A, ObjectNet, OmniBenchmark, and VTAB for benchmarking. These have no overlap and large domain gaps with pre-training data.4. Extensive experiments validate the effectiveness of the simple frozen PTM baseline and the proposed ADAM framework. ADAM consistently outperforms previous SOTA methods across various backbones and datasets. The new benchmarks also better measure the generalization of PTM-based CIL.In summary, the key contribution is a new perspective on CIL in the era of PTMs, proposing ADAM to unify generalizability and adaptivity, and new benchmarks to evaluate this. The simple yet effective design of ADAM also stands out.
