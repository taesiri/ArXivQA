# [Revisiting Class-Incremental Learning with Pre-Trained Models:   Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is: How can we effectively leverage pre-trained models (PTMs) for class-incremental learning (CIL)? 

Some key points:

- The paper argues that CIL requires models that balance adaptivity (to new data) and generalizability (of learned knowledge). 

- Traditional CIL methods assume training from scratch and focus on adaptivity. However, PTMs possess inherent generalizability that could be valuable.

- The paper shows that a simple baseline using a frozen PTM can outperform current state-of-the-art CIL methods, demonstrating the generalizability of PTMs.

- However, PTMs may still need adaptivity to align with incremental dataset domains. The paper proposes ADAM to adapt the PTM initially and then merge it with the original to balance adaptivity and generalizability.

- The paper also advocates new benchmarks without overlap for evaluating PTM-based CIL.

In summary, the central hypothesis is that leveraging PTMs for CIL requires properly utilizing both the generalizability they provide and adaptivity to the incremental datasets, which ADAM aims to achieve in a simple and unified manner.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It revisits class-incremental learning (CIL) in the era of pre-trained models (PTMs). Previous CIL methods focus on training models from scratch to continually acquire knowledge. But PTMs provide generalizable embeddings that can be directly leveraged for CIL. The paper reveals that a simple baseline using frozen PTMs can outperform current state-of-the-art CIL methods.

2. It proposes ADapt And Merge (ADAM), a framework to unify the generalizability of PTMs and adaptivity of fine-tuned models for CIL. ADAM adapts the PTM on the first incremental dataset to obtain task-specific features. It then merges the adapted model and original PTM to extract features for classifier learning. This maintains both generalizability and adaptivity.

3. It finds that previous CIL benchmarks have large overlap with pre-training data, making them unsuitable to evaluate PTM-based methods. So it proposes four new datasets - ImageNet-A, ObjectNet, OmniBenchmark, and VTAB for benchmarking. These have no overlap and large domain gaps with pre-training data.

4. Extensive experiments validate the effectiveness of the simple frozen PTM baseline and the proposed ADAM framework. ADAM consistently outperforms previous SOTA methods across various backbones and datasets. The new benchmarks also better measure the generalization of PTM-based CIL.

In summary, the key contribution is a new perspective on CIL in the era of PTMs, proposing ADAM to unify generalizability and adaptivity, and new benchmarks to evaluate this. The simple yet effective design of ADAM also stands out.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper revisits class-incremental learning with pre-trained models and shows that frozen pre-trained models provide generalizable embeddings for competitive performance, while model adaptation techniques like VPT and adapters can further enhance adaptivity; new benchmarks with large domain gaps are also introduced to properly evaluate CIL in the era of pre-training.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in class-incremental learning (CIL):

- It focuses on revisiting CIL in the era of pre-trained models (PTMs). Many recent works have explored using PTMs like Vision Transformers for CIL. This paper systematically analyzes the role of PTMs and proposes a new method building on their strengths.

- The paper reveals that a simple baseline using a frozen PTM and prototype classifiers can already beat current state-of-the-art CIL methods. This is an interesting finding showing the power of pre-training for CIL.

- The proposed ADAM method adapts the PTM on the first incremental batch to improve adaptivity while still leveraging the generalizability of the original PTM features. This effectively merges adaptivity and generalizability.

- The paper advocates new benchmarks like ImageNet-A, ObjectNet, OmniBenchmark, and VTAB that have a larger domain gap from ImageNet pre-training. This is important for better evaluating PTM-based CIL.

- Extensive experiments show ADAM consistently improves over baselines and outperforms recent state-of-the-art CIL methods including L2P and DualPrompt. The gains are especially large on the new proposed benchmarks.

Overall, this paper provides new insights into PTM-based CIL and proposes an effective yet simple approach unifying adaptivity and generalizability. The new benchmarks are also an important contribution for future research. The consistent gains over current methods on various datasets demonstrate the impact of this work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring task-specific tuning methods and structures for class-incremental learning with pre-trained models. The authors mention that future work could include designing tuning techniques tailored for incremental learning settings. This could help further enhance the adaptivity of pre-trained models to new tasks and datasets.

- Evaluating class-incremental learning in the exemplar-based setting when historical data is available. The current work focuses on the exemplar-free setting, but the availability of stored examples could change the dynamics of how to balance adaptivity and generalizability.

- Studying how to determine the optimal number of tuning stages. The paper adapts models in the first incremental stage, but determining the ideal number of stages to tune could be explored.

- Extending the work to other data modalities and pre-trained models beyond vision. The current research is on image classification, but expanding to other data types (e.g. text, audio) and model architectures could be promising.

- Developing more benchmark datasets with a large domain gap from pre-training data. The paper introduces some new benchmarks, but creating more diverse out-of-distribution datasets for evaluation would be useful.

- Investigating the role of model size on transferability. Larger pre-trained models seem to generalize better, but systematically studying model scale could provide more insights.

- Continual learning in online or streaming data settings. The current setup assumes distinct tasks, but online learning where data continuously evolves merits exploration.

Overall, the paper provides a solid basis for future work on effectively leveraging pre-trained models for class-incremental learning across a variety of dimensions. The combination of generalizability and adaptivity offers a promising direction.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach for class-incremental learning (CIL) with pre-trained models (PTMs). It first shows that a simple baseline using frozen features from a PTM can outperform current state-of-the-art CIL methods, demonstrating the strong generalizability of PTMs. However, PTMs can be further improved with adaptivity to the incremental dataset. Thus, the paper proposes ADapt And Merge (ADAM), which adapts the PTM on the first incremental dataset for improved adaptivity while still leveraging the original PTM features to maintain generalizability. Specifically, ADAM adapts the PTM with efficient tuning techniques like adapters or prompt tuning, then freezes and concatenates the adapted and original PTM to extract class prototypes for incremental learning. Experiments on new CIL benchmarks with large domain shifts from ImageNet show ADAM consistently improves over strong baselines. The unified framework enhances both generalizability and adaptivity for incremental learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents ADapt And Merge (ADAM), a new framework for class-incremental learning (CIL) with pre-trained models (PTMs). CIL aims to continuously acquire knowledge as data evolves, while PTMs possess generalizable embeddings that can be transferred to new tasks. 

The key insight is that CIL requires both adaptivity to grasp task-specific features and generalizability for knowledge transfer. ADAM achieves this by adapting the PTM on the first incremental dataset to enhance adaptivity. It then concatenates the adapted model with the original PTM and extracts prototypes as the classifier weights. This maintains the generalizability of the PTM while incorporating adaptivity. Experiments show ADAM consistently improves baseline PTMs on new CIL benchmarks with large domain gaps from ImageNet. The simple framework can also be combined with various parameter-efficient tuning techniques. Overall, ADAM provides a strong yet efficient CIL approach by unifying adaptivity and generalizability.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes ADapt And Merge (ADAM), a new framework for class-incremental learning (CIL) with pre-trained models (PTMs). The key insight is that PTMs provide generalizable embeddings that can be directly used for CIL, but they lack adaptivity to the target dataset. To address this, ADAM adapts the PTM on the first incremental dataset to make the model more adaptive. It then concatenates the embeddings of the original PTM and adapted model, and extracts prototypes on this joint embedding to construct the classifier weights. This allows ADAM to leverage both the generalizability of the original PTM and the adaptivity of the tuned model. After adaptation, the embedding functions are frozen and only the prototype-based classifiers are updated in subsequent incremental steps. Experiments on new CIL benchmarks with large domain gaps show ADAM outperforms existing PTM-based CIL methods with a simple and unified framework.
