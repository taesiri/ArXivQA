# [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we design an efficient transformer model that achieves comparable accuracy to vanilla transformers while reducing the quadratic complexity to linear?

The key hypotheses appear to be:

1) The softmax operation in vanilla transformers is a primary bottleneck for efficiency due to its non-decomposable nature. 

2) Two key properties of softmax are important for performance: non-negativity of the attention matrix, and a non-linear re-weighting scheme.

3) It is possible to replace softmax with a decomposable linear operation, while still retaining these two key properties, to achieve an efficient yet accurate transformer.

The paper proposes cosFormer, which replaces softmax with a ReLU-based linear kernel and a cos-based re-weighting scheme. Experiments across language modeling, text classification, and long sequence tasks aim to validate whether cosFormer can achieve comparable accuracy and greater efficiency compared to vanilla transformers and other efficient variants.

In summary, the central research question is how to achieve an efficient yet accurate transformer by replacing the softmax operation with a decomposable alternative, with the key hypotheses relating to retaining properties of non-negativity and re-weighting from softmax. The cosFormer model is proposed and evaluated as a possible solution.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new efficient transformer model called cosFormer that achieves linear time and space complexity while maintaining accuracy comparable to the original transformer model. The key ideas are:

- Replace the softmax attention in transformer with a linear operation and non-linear re-weighting mechanism. This fulfills two key properties of softmax attention: non-negativity and concentration of the attention distribution.

- Use ReLU activation before computing similarity scores to encourage only aggregating positively correlated contextual information. 

- Adopt a cos-based re-weighting scheme to stabilize attention weights and exploit locality inductive biases commonly seen in natural language tasks.

- The proposed cos-based re-weighting allows exact decomposition into a linear form thanks to Ptolemy's theorem, avoiding approximation errors in previous linear transformers.

- Experiments on autoregressive language modeling, bidirectional pretraining, downstream NLU tasks, and long sequence modeling demonstrate cosFormer's effectiveness and efficiency compared to vanilla transformer and other efficient transformers.

In summary, the main contribution is developing a simple yet effective linear replacement of softmax attention that achieves the efficiency benefits while maintaining accuracy across a wide range of tasks. The decomposition avoids previous approximation errors and the cos-based re-weighting provides useful inductive biases.
