# [PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](https://arxiv.org/abs/2210.01612)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: 

How can we improve self-supervised monocular depth estimation by representing the scene with orthogonal planes and modeling depth as a mixture of Laplace distributions on those planes?

The key aspects explored in addressing this question are:

- Using both vertical planes and ground planes to represent the scene, which enables predicting continuous depth for the ground while preserving sharp edges on objects. Previous plane-based methods only used vertical/frontal planes which led to discontinuities in the ground region.

- Modeling the depth distribution as a mixture of Laplace distributions centered on the orthogonal planes. This avoids ambiguity in the loss function compared to using a weighted summation of warped images from different planes. 

- Introducing orthogonality-preserving data augmentation through explicit resizing/cropping transforms and neural positional encodings. This improves robustness and helps the network utilize both relative object size and vertical position cues.

- Combining post-processing with self-distillation through a bilateral occlusion mask to improve accuracy and efficiency.

The main hypothesis is that explicitly modeling the scene geometry with orthogonal planes and representing depth probabilistically as a mixture of Laplace distributions will improve self-supervised monocular depth estimation, especially for representing the ground region and dealing with ambiguities. The experiments aim to validate the effectiveness of the proposed orthogonal plane representation and training strategies.
