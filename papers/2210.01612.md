# [PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](https://arxiv.org/abs/2210.01612)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: 

How can we improve self-supervised monocular depth estimation by representing the scene with orthogonal planes and modeling depth as a mixture of Laplace distributions on those planes?

The key aspects explored in addressing this question are:

- Using both vertical planes and ground planes to represent the scene, which enables predicting continuous depth for the ground while preserving sharp edges on objects. Previous plane-based methods only used vertical/frontal planes which led to discontinuities in the ground region.

- Modeling the depth distribution as a mixture of Laplace distributions centered on the orthogonal planes. This avoids ambiguity in the loss function compared to using a weighted summation of warped images from different planes. 

- Introducing orthogonality-preserving data augmentation through explicit resizing/cropping transforms and neural positional encodings. This improves robustness and helps the network utilize both relative object size and vertical position cues.

- Combining post-processing with self-distillation through a bilateral occlusion mask to improve accuracy and efficiency.

The main hypothesis is that explicitly modeling the scene geometry with orthogonal planes and representing depth probabilistically as a mixture of Laplace distributions will improve self-supervised monocular depth estimation, especially for representing the ground region and dealing with ambiguities. The experiments aim to validate the effectiveness of the proposed orthogonal plane representation and training strategies.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes PlaneDepth, a novel orthogonal plane-based representation for monocular depth estimation, which includes vertical planes and ground planes. This representation enables estimating a continuous depth for the ground region while preserving sharp edges for other objects.

2. It models the depth distribution as a mixture of Laplacians based on orthogonal planes. This avoids the ambiguity of previous weighted color composition and leads to more stable depth estimation. 

3. It introduces an orthogonality-preserved data augmentation strategy through explicit resizing cropping transformations and neural positional encodings. This improves the robustness of network training.

4. It combines post-processing with self-distillation using bilateral occlusion masks to generate more accurate pseudo labels. This improves both the accuracy and efficiency of the method.

In summary, the main contribution is the novel orthogonal plane representation and the associated improvements to monocular depth estimation, including the mixture of Laplacians depth modeling, the orthogonality-preserved data augmentation, and the augmented self-distillation technique. These contributions enable more accurate and continuous depth estimation, especially for the ground region which is important for autonomous driving applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes PlaneDepth, a self-supervised monocular depth estimation method based on orthogonal vertical and ground planes that predicts continuous depth for the ground while preserving sharp object boundaries, addresses ambiguity in color expectation, exploits relative size cues with resize cropping, and incorporates post-processing knowledge into training via an augmented self-distillation loss.


## How does this paper compare to other research in the same field?

 Here is a brief summary of how this paper compares to other research in self-supervised monocular depth estimation:

- The paper proposes a novel plane-based scene representation using orthogonal planes (vertical planes + ground planes) instead of just frontal-parallel planes used in prior works like PladeNet. This allows representing continuous ground depth and extracting the ground plane, which is useful for autonomous driving applications.

- The paper models the depth distribution as a mixture of Laplacians centered at each plane rather than using a weighted sum of warped images from each plane. This avoids the ambiguity of the weighted summation and provides a more stable training objective. 

- The paper analyzes the effect of common data augmentations like resize/crop on the geometry and orthogonality of planes. It proposes techniques to explicitly account for these to enable robust training.

- The paper combines ideas from self-distillation and post-processing to distill knowledge of artifacts due to occlusions into the main model, avoiding double inference. 

- Compared to methods based on depth regression, the discrete plane representation avoids issues like local minima during optimization. Compared to other plane-based approaches, the orthogonal plane formulation better represents continuous surfaces like the ground.

- Experiments demonstrate state-of-the-art performance on KITTI dataset benchmarks, especially on ground/driving areas. The ablation studies validate the benefits of the different components proposed.

In summary, this paper brings together multiple innovations in plane formulation, optimization, data augmentation and distillation to advance self-supervised depth estimation, with a focus on representing continuous surfaces like the ground. The techniques are general and could benefit other works in this area.


## What future research directions do the authors suggest?

 Here are some of the main future research directions suggested in the paper:

- Constraining the choice of different types of planes and the inclination caused by car bumping. The authors note that their unsupervised ground segmentation may not be robust enough for safety-critical applications due to lack of constraints on plane choices and car tilt. They suggest combining with other unsupervised ground segmentation methods or explicitly modeling car tilt.

- Incorporating semantic segmentation and exploring joint learning with semantics. The authors suggest this could help improve results and robustness.

- Incorporating plane depth into full 3D reconstruction frameworks to leverage geometric constraints. This could improve multi-view consistency. 

- Exploring applications of the predicted ground plane like drivable surface estimation. The authors note their ground plane prediction could facilitate identifying drivable regions for autonomous driving applications.

- Extending the framework to indoor scenes which have more complex geometry. The authors suggest their plane representation may also be useful for indoor layout estimation.

- Improving efficiency and reducing memory usage to enable high-resolution training and inference. The authors note efficiency improvements could enable deployment.

- Evaluating the framework on more diverse datasets beyond KITTI. Testing on other driving datasets could demonstrate wider applicability.

In summary, the main suggested directions are improving robustness, incorporating semantics, integrating into 3D frameworks, exploring applications, extending to indoor scenes, improving efficiency, and evaluating on more diverse datasets. The authors propose many interesting avenues for building on their PlaneDepth method.


## Summarize the paper in one paragraph.

 The paper proposes a novel framework and training strategy for self-supervised monocular depth estimation (MDE). The key ideas are:

1. They propose representing the scene with orthogonal planes consisting of vertical planes and ground planes, instead of just frontal-parallel planes. This allows modeling continuous depth for the ground while preserving sharp edges for objects. 

2. They model the depth as a mixture of Laplace distributions centered at each plane. This avoids the ambiguity of weighted color summation used in prior works and provides a more stable training signal.

3. They analyze the effect of resizing and cropping augmentation on the predefined planes and explicitly compute a transformation to rectify the planes and camera pose. This preserves the orthogonality assumptions during training with data augmentation.

4. They propose an augmented self-distillation approach that leverages bilateral occlusion masks and incorporates post-processing into the distillation, improving accuracy while avoiding double inference time.

5. Experiments on KITTI dataset validate the effectiveness of the orthogonal plane representation, mixture Laplace loss, orthogonality-preserving data augmentation, and augmented self-distillation. Both quantitative and qualitative results demonstrate improved accuracy and continuity of depth estimation.

In summary, the paper presents several novel and effective solutions to address limitations of prior self-supervised MDE methods, leading to improved depth accuracy and continuity especially for ground planes.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper proposes a new framework and training strategy for self-supervised monocular depth estimation. Existing methods use depth classification rather than regression to avoid issues with locality and ambiguity when doing bilinear interpolation search. However, pre-divided depth candidates lead to discontinuities and ambiguity in the final prediction. The paper introduces PlaneDepth which uses orthogonal planes, including vertical planes and ground planes, to represent depth. This allows predicting continuous depth for the ground while preserving sharp edges for objects. The depth distribution is modeled as a mixture of Laplacians over the orthogonal planes to avoid ambiguity. The method also proposes an orthogonality-preserving data augmentation strategy and a training procedure to combine object size and vertical position cues. An augmented self-distillation loss with bilateral occlusion masking is used to incorporate post-processing knowledge and improve accuracy and efficiency.

In summary, the key contributions are: 1) Orthogonal planes based representation to enable continuous ground depth prediction; 2) Modeling depth as a mixture of Laplacians on orthogonal planes to avoid ambiguity; 3) Orthogonality-preserving data augmentation strategy; 4) Training strategy to combine object size and vertical position cues; 5) Augmented self-distillation loss to incorporate post-processing knowledge. Extensive experiments on KITTI dataset demonstrate the effectiveness of the proposed approach in improving depth prediction accuracy and efficiency. The method enables smoother depth on the ground while preserving sharp edges and reduces artifacts due to occlusion.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel orthogonal planes based representation for self-supervised monocular depth estimation. It models the depth distribution as a mixture of Laplace distributions with the orthogonal planes (vertical planes and ground planes) as the means. To handle the non-orthogonality caused by common resizing and cropping data augmentation, it explicitly computes the transformation and uses it to rectify the predefined planes and predicted camera pose. It also incorporates neural positional encoding of the resizing-cropping grid to further help the network handle this augmentation. Additionally, it proposes an augmented self-distillation approach that leverages bilateral occlusion masks and post-processing to generate more accurate pseudo labels for network training, which improves efficiency and accuracy. Overall, the orthogonal planes representation and associated improvements in handling data augmentation and self-distillation allow the method to predict continuous ground depth while preserving sharp object boundaries.
