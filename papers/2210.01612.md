# [PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](https://arxiv.org/abs/2210.01612)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: 

How can we improve self-supervised monocular depth estimation by representing the scene with orthogonal planes and modeling depth as a mixture of Laplace distributions on those planes?

The key aspects explored in addressing this question are:

- Using both vertical planes and ground planes to represent the scene, which enables predicting continuous depth for the ground while preserving sharp edges on objects. Previous plane-based methods only used vertical/frontal planes which led to discontinuities in the ground region.

- Modeling the depth distribution as a mixture of Laplace distributions centered on the orthogonal planes. This avoids ambiguity in the loss function compared to using a weighted summation of warped images from different planes. 

- Introducing orthogonality-preserving data augmentation through explicit resizing/cropping transforms and neural positional encodings. This improves robustness and helps the network utilize both relative object size and vertical position cues.

- Combining post-processing with self-distillation through a bilateral occlusion mask to improve accuracy and efficiency.

The main hypothesis is that explicitly modeling the scene geometry with orthogonal planes and representing depth probabilistically as a mixture of Laplace distributions will improve self-supervised monocular depth estimation, especially for representing the ground region and dealing with ambiguities. The experiments aim to validate the effectiveness of the proposed orthogonal plane representation and training strategies.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes PlaneDepth, a novel orthogonal plane-based representation for monocular depth estimation, which includes vertical planes and ground planes. This representation enables estimating a continuous depth for the ground region while preserving sharp edges for other objects.

2. It models the depth distribution as a mixture of Laplacians based on orthogonal planes. This avoids the ambiguity of previous weighted color composition and leads to more stable depth estimation. 

3. It introduces an orthogonality-preserved data augmentation strategy through explicit resizing cropping transformations and neural positional encodings. This improves the robustness of network training.

4. It combines post-processing with self-distillation using bilateral occlusion masks to generate more accurate pseudo labels. This improves both the accuracy and efficiency of the method.

In summary, the main contribution is the novel orthogonal plane representation and the associated improvements to monocular depth estimation, including the mixture of Laplacians depth modeling, the orthogonality-preserved data augmentation, and the augmented self-distillation technique. These contributions enable more accurate and continuous depth estimation, especially for the ground region which is important for autonomous driving applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes PlaneDepth, a self-supervised monocular depth estimation method based on orthogonal vertical and ground planes that predicts continuous depth for the ground while preserving sharp object boundaries, addresses ambiguity in color expectation, exploits relative size cues with resize cropping, and incorporates post-processing knowledge into training via an augmented self-distillation loss.


## How does this paper compare to other research in the same field?

 Here is a brief summary of how this paper compares to other research in self-supervised monocular depth estimation:

- The paper proposes a novel plane-based scene representation using orthogonal planes (vertical planes + ground planes) instead of just frontal-parallel planes used in prior works like PladeNet. This allows representing continuous ground depth and extracting the ground plane, which is useful for autonomous driving applications.

- The paper models the depth distribution as a mixture of Laplacians centered at each plane rather than using a weighted sum of warped images from each plane. This avoids the ambiguity of the weighted summation and provides a more stable training objective. 

- The paper analyzes the effect of common data augmentations like resize/crop on the geometry and orthogonality of planes. It proposes techniques to explicitly account for these to enable robust training.

- The paper combines ideas from self-distillation and post-processing to distill knowledge of artifacts due to occlusions into the main model, avoiding double inference. 

- Compared to methods based on depth regression, the discrete plane representation avoids issues like local minima during optimization. Compared to other plane-based approaches, the orthogonal plane formulation better represents continuous surfaces like the ground.

- Experiments demonstrate state-of-the-art performance on KITTI dataset benchmarks, especially on ground/driving areas. The ablation studies validate the benefits of the different components proposed.

In summary, this paper brings together multiple innovations in plane formulation, optimization, data augmentation and distillation to advance self-supervised depth estimation, with a focus on representing continuous surfaces like the ground. The techniques are general and could benefit other works in this area.


## What future research directions do the authors suggest?

 Here are some of the main future research directions suggested in the paper:

- Constraining the choice of different types of planes and the inclination caused by car bumping. The authors note that their unsupervised ground segmentation may not be robust enough for safety-critical applications due to lack of constraints on plane choices and car tilt. They suggest combining with other unsupervised ground segmentation methods or explicitly modeling car tilt.

- Incorporating semantic segmentation and exploring joint learning with semantics. The authors suggest this could help improve results and robustness.

- Incorporating plane depth into full 3D reconstruction frameworks to leverage geometric constraints. This could improve multi-view consistency. 

- Exploring applications of the predicted ground plane like drivable surface estimation. The authors note their ground plane prediction could facilitate identifying drivable regions for autonomous driving applications.

- Extending the framework to indoor scenes which have more complex geometry. The authors suggest their plane representation may also be useful for indoor layout estimation.

- Improving efficiency and reducing memory usage to enable high-resolution training and inference. The authors note efficiency improvements could enable deployment.

- Evaluating the framework on more diverse datasets beyond KITTI. Testing on other driving datasets could demonstrate wider applicability.

In summary, the main suggested directions are improving robustness, incorporating semantics, integrating into 3D frameworks, exploring applications, extending to indoor scenes, improving efficiency, and evaluating on more diverse datasets. The authors propose many interesting avenues for building on their PlaneDepth method.
