# [Information That Matters: Exploring Information Needs of People Affected   by Algorithmic Decisions](https://arxiv.org/abs/2401.13324)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Explanations of AI systems often do not address the information needs of "affected stakeholders", i.e. people impacted by algorithmic decisions. This can limit their ability to understand the systems and take actions like contesting decisions. 
- Affected stakeholders are a diverse group with varying literacy and prior knowledge, needing accessible and adaptive explanations.
- Providing good explanations for them has challenges around trust, understanding cognitive processes, and managing large amounts of system information.

Proposed Solution:
- Conduct an interview study with 24 participants from a job agency and apartment complex to understand their information needs and thought processes around two AI system use cases. 
- Use question-driven explanations, where participants independently inquire information in two 15-minute phases. Questions are categorized to compile an "XAI Novice Question Bank".
- Analyze changes in participants' understanding, decision confidence, risk/benefit perceptions from before and after explanations.
- Apply theories around explanatory stances and understanding to examine how people process information.

Main Contributions:
- XAI Novice Question Bank cataloging affected stakeholders' information needs on data, system context, usage, specifications. Extends XAI Question Bank.
- Analysis showing explanations increased confidence more than understanding. Understanding challenges include blind spots, outsourcing, and abandoning understanding.  
- Finding that explanatory stance (mechanical, design, intentional) impacts information seeking. Matching stance to topic can improve explanations.
- Higher perceived risks prompted more questions around system intentions. Explanations should consider including contextual/intentional information.
- 6 key implications for designing explanations for affected stakeholders, including the need to connect explainability with civic education to stimulate interest.

In summary, the paper identifies affected stakeholders' information needs, analyzes how explanations interact with their understanding, and provides suggestions to design better-targeted explanations for this important audience.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper presents a study of affected stakeholders' information needs and understanding challenges regarding algorithmic decision-making systems, proposes an "XAI Novice Question Bank" to guide future explanation approaches, and provides suggestions for using explanatory stances and incorporating insights from civic education research to design better explanations for this important audience.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) A catalog of affected stakeholders' questions titled the "XAI Novice Question Bank" (Figure 1), which provides an overview of information needs related to algorithmic decision-making systems. This extends the XAI Question Bank by Liao et al. to include questions on system context and usage.

2) Recommendations on how to address understanding challenges such as blind spots and outsourcing when using question-driven explanations. The paper also discusses how explanatory stances (mechanical, design, intentional) can be useful to guide information acquisition. 

3) Reflections on the relevance of intentional information (e.g. motivations, consequences of system deployment) in explanations for affected stakeholders. The paper argues that perceived risks influence whether stakeholders inquire about intentional aspects.  

4) Considerations on incorporating insights from fields like civic education to design explanations that effectively convey relevant information to affected stakeholders. This includes identifying "information that matters" at the intersection of stakeholders' interests and information sufficient for specific aims.

In summary, the main contribution is providing empirical insight and recommendations to support the inclusion of affected stakeholders' perspectives into explainable AI. This encompasses analyzing their information needs, understanding processes, and risk/benefit perceptions.


## What are the keywords or key terms associated with this paper?

 Based on my analysis of the paper, the key terms associated with this paper are:

explainable AI, understanding, information needs, affected stakeholders, question-driven explanations, qualitative methods

These key terms relate to the main themes and contributions of the paper, which focuses on exploring and addressing the information needs of people affected by algorithmic decision-making through the use of question-driven explanations. The paper takes a human-centered approach, using qualitative methods such as interviews to understand affected stakeholders and their needs, with the goal of improving the explainability of AI systems for this important group of end users. The paper also analyzes how explanations interact with people's understanding of the systems, and how they can influence perceptions of risks and benefits. Overall, the key terms reflect the paper's focus on advancing explainable AI through a deeper consideration of affected stakeholders and their requirements for understanding AI systems that impact them.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper presents two AI use cases to participants: an employment prediction algorithm and a health wristband system. Why were these specific use cases chosen and how do they differ in key aspects like domain, tangibility, functionality, etc.?

2. Participants go through two 15-minute phases of inquiry where they can ask questions about the AI systems and receive verbal explanations in response. What is the rationale behind splitting the inquiry process into two distinct phases? How does introducing the XAI Question Bank in the second phase change the nature of participants' questions?

3. The study examines participants' self-reported understanding, decision confidence, and risk/benefit perceptions at two points: before and after the inquiry phases. What additional insights could have been gained by asking participants for self-assessments at more fine-grained intervals during the inquiry process? 

4. The paper argues that the key to good explanations is aligning information with stakeholders' aims. However, the study itself does not directly examine what participants' aims are beyond deciding on adoption of the systems. What methodological changes could elicit more data on participants' aims?

5. The analysis utilizes the explanatory stances framework (mechanical, design, intentional) to examine what perspectives participants take in their inquiries. How else could this framework be leveraged, for example to deliberately provoke changes in participants' stances over the course of inquiry?

6. The paper touches on "civic ignorance" and connecting XAI to civic education, but does not substantially incorporate relevant concepts from political science literature. What theories around democratic participation, collective action, etc. could further enrich the analysis?

7. How do the verbal explanations provided by the examiner differ from textual/visual explanations participants might access in real-world settings? How could the methodology simulate more naturalistic explanation experiences? 

8. The sample size of 24 participants is justified by qualitative research standards, but how many participants would be needed to gather more generalizable insights about information needs and explanatory processes?

9. What are some limitations of the thematic analysis approach used? Would utilizing other qualitative analysis methods like grounded theory bring additional explanatory value?

10. Beyond the specific AI use cases presented, what modifications could make the study protocol adaptable to other high-risk ADM systems that affected stakeholders might encounter?
