# [Referring Multi-Object Tracking](https://arxiv.org/abs/2303.03366)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of referring multi-object tracking (RMOT) in videos. The main research questions it seeks to tackle are:

1. How can we ground referring expressions to multiple objects in videos rather than just a single object? Existing referring expression datasets and models focus on grounding expressions to a single target object. However, in real-world videos, a referring expression may correspond to multiple objects with the same semantics.

2. How can we handle temporal dynamics and status changes in referent objects? Referring expressions often describe objects exhibiting certain actions or temporal characteristics, which may not hold true across all frames. Existing video referring datasets do not model temporal status changes accurately. 

3. How can we develop an end-to-end model for referring multi-object tracking that handles variable number of target objects and their temporal dynamics? Most prior work follows a two-stage approach of object detection and expression matching. End-to-end modeling of referring expressions, detection, and tracking is still an open challenge.

To address these questions, the paper introduces a new task of RMOT, constructs two new benchmarks Refer-MOT17 and Refer-KITTI with multi-object and temporally dynamic expressions, and proposes an end-to-end transformer-based model, TransRMOT, for referring multi-object tracking. The model adapts the DETR architecture using separate track and detect queries to handle variable target objects and their statuses over time.

In summary, the key research contributions are: formulating the novel RMOT task, creating new benchmark datasets, and developing an end-to-end architecture for referring multi-object tracking in videos. The paper pushes the envelope of video grounding using natural language beyond existing single object settings.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new task called referring multi-object tracking (RMOT) and developing a benchmark and method for this task. Specifically:

- They propose the RMOT task, which uses a natural language expression to refer to and track multiple objects in a video. This is more flexible and realistic than existing referring tasks that refer to a single object. 

- They create a new benchmark called Refer-KITTI for RMOT based on the KITTI dataset. It contains videos with expression annotations referring to multiple objects with temporal dynamics.

- They develop an end-to-end Transformer-based model called TransRMOT for the RMOT task. It fuses visual and linguistic features and uses separate track and detect queries to handle referring to multiple objects across frames.

- Experiments show TransRMOT achieves state-of-the-art performance on Refer-KITTI compared to adapted object tracking and referring methods.

In summary, the main contribution is proposing the more flexible and realistic RMOT task, creating a benchmark for it, and developing an effective model to tackle the new task. This moves referring understanding towards handling real-world complexity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in this paper:

The paper proposes a new referring multi-object tracking task using natural language descriptions to guide the prediction of trajectories for multiple objects in videos, constructs a new benchmark dataset Refer-KITTI for this task, and develops a Transformer-based model called TransRMOT to tackle it.


## How does this paper compare to other research in the same field?

 This paper presents Refer-KITTI, a new benchmark dataset and model for referring multi-object tracking (RMOT). Here are some key ways it compares to other research in this field:

- Setting: RMOT focuses on referring to multiple objects in videos based on natural language descriptions. This is different from prior work in referring expression comprehension that primarily focuses on grounding expressions to single objects in images. RMOT introduces more flexibility and better captures real-world complexity.

- Dataset: Refer-KITTI contains 18 videos with 818 expressions referring to 10.7 objects on average per video. This allows evaluating models on referring to varying numbers of objects over time. Prior video referring datasets like Refer-DAVIS and Refer-YV refer to single objects.

- Model: The paper proposes TransRMOT, a Transformer-based model for RMOT. It fuses visual and linguistic features early on and uses separate track/detect queries to handle object entrance/exit. This outperforms adaptations of prior MOT and referring models.

- Metrics: The paper evaluates using HOTA, which accounts for both detection and association accuracy. This is more suitable for RMOT than metrics used in prior referring work.

- Generalization: The authors show Refer-KITTI and TransRMOT generalize well to new compositions of expressions and across different datasets. This demonstrates the more natural language aspect over fixed vocabulary settings.

In summary, the paper pushes referring expression understanding to handle more complex, multi-object video settings. The new dataset, task formulation, and model advance the state-of-the-art in more flexible, real-world directions. The generalizable language component is a key advantage over prior work.
