# [Parallel WaveGAN: A fast waveform generation model based on generative   adversarial networks with multi-resolution spectrogram](https://arxiv.org/abs/1910.11480)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop a fast, high-quality neural vocoder for text-to-speech synthesis using generative adversarial networks, without needing to use a separate autoregressive teacher model or density distillation. 

The key hypothesis is that by jointly training a non-autoregressive WaveNet generator model using a combination of multi-resolution spectrogram loss and adversarial loss, it can learn to generate realistic and natural sounding speech efficiently.

In particular, the paper proposes and evaluates a new model called Parallel WaveGAN that is trained end-to-end with this joint loss approach, without needing a separate pre-trained autoregressive teacher model or density distillation as required by prior parallel waveform generation methods.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Parallel WaveGAN, a fast and efficient parallel waveform generation method for text-to-speech based on generative adversarial networks. Key points:

- It proposes a joint training method combining multi-resolution STFT loss and waveform-domain adversarial loss. This allows effective training of parallel WaveNet vocoders without needing the complex density distillation process of previous methods.

- As a result, the whole model can be trained easily and fast without a separate pretrained autoregressive teacher model. 

- Experiments show the Parallel WaveGAN model achieves excellent speech quality (comparable to state-of-the-art Parallel WaveNet) while being 4.82x faster to train and 1.96x faster for inference compared to baseline Parallel WaveNet implementations.

- When integrated into a Transformer TTS system, the Parallel WaveGAN achieves a mean opinion score of 4.16, on par with the best distillation-based Parallel WaveNet system. 

In summary, the key contribution is developing a fast, efficient and high-quality parallel neural vocoder for TTS based on GANs, without needing complex distillation training. This is achieved through a novel joint adversarial + multi-resolution spectrogram loss approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The authors propose a fast and efficient waveform generation model called Parallel WaveGAN that can generate high quality speech using adversarial training and multi-resolution spectrogram loss, without needing the complex distillation process used in prior Parallel WaveNet models.
