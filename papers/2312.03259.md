# [f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization](https://arxiv.org/abs/2312.03259)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from the paper:

This paper proposes a unified stochastic optimization framework ($f$-FERM) for fair empirical risk minimization using $f$-divergence measures as regularizers. $f$-divergences generalize existing fairness notions like mutual information and allow capturing nonlinear correlations between sensitive attributes and predictions. The authors derive a provably convergent stochastic optimization algorithm for $f$-FERM using variational Fenchel duality that can work for small batch sizes. They also extend $f$-FERM to handle distribution shifts between training and test data using distributionally robust optimization, where the uncertainty set is defined using $f$-divergences or $\ell_p$ norms. Two algorithms with convergence guarantees are proposed - one for small distribution shifts using Taylor approximation, and another for potentially large shifts under $\ell_\infty$ norm by relaxing probability simplex constraints. Extensive experiments demonstrate $f$-FERM's superior fairness-accuracy tradeoffs over state-of-the-art methods on benchmark datasets, under varying batch sizes and degrees of distribution shift. Both $f$-FERM formulations enjoy theoretical convergence guarantees for finding stationary solutions. The unified $f$-FERM framework allows stochastic training of complex deep learning models while providing formal fairness certificates.
