# [Evaluating and Correcting Performative Effects of Decision Support   Systems via Causal Domain Shift](https://arxiv.org/abs/2403.00886)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper considers the problem of "performative prediction", where a prediction model affects the distribution of the target variable it is trying to predict. For example, a churn prediction model that warns companies of customers at high risk of churning might cause the company to take action to retain those customers.

- The paper focuses on the setting of "decision support systems" (DSS) that deliberately make performative predictions to instigate action and improve outcomes. However, naive retraining of such DSS models can suffer from "performative bias", underestimating risk due to the successful actions caused by previous models. 

- The paper argues for the importance of properly evaluating DSS systems before and after deployment to assess their "deployment effect" and "retraining effect". However, randomized trials may be infeasible or unethical. The evaluation then becomes a challenging domain adaptation problem.

Proposed Solution:
- Models the deployment of a DSS as a domain shift from "DSS off" (D=0) to "DSS on" (D=1), with the DSS prediction model parameters as explicit variables.

- Defines the "deployment effect" and "retraining effect" as domain adaptation tasks T1 and T2 for DSS evaluation.

- Shows the problem of estimating a "baseline predictor" without performative bias is another domain adaptation task T3. 

- Proves tasks T1-T3 are mathematically equivalent and not solvable without additional assumptions.

- Introduces the concept of a "domain pivot" - a set of variables that make the target variable independent of the domain shift. If a domain pivot can be measured pre and post deployment, the tasks become solvable.

- Uses a "repeated regression" method to estimate the quantities of interest from the domain pivot. Demonstrates efficacy on example DSS for churn prediction.

- Extends method to handle selection bias and selective labelling, making it widely applicable.

Main Contributions:
- Formalized DSS deployment as a domain shift problem, proposed evaluation metrics and bias correction as key applications.

- Identified need for "domain pivot" to make tasks solvable, proved equivalence and non-identifiability of estimation problems.

- Provided a practical repeated regression framework for DSS evaluation and bias correction even with selection bias or selective labelling.

- Demonstrated value of methods on illustrative DSS example. The work provides an important framework and tools for responsible AI system development.


## Summarize the paper in one sentence.

 This paper proposes modeling the deployment of decision support systems as causal domain shift, introduces metrics to evaluate the performative effects of such systems, and provides methods to correct for performative bias when retraining prediction models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a causal modelling framework to model the deployment of decision support systems (DSS) as causal domain shift. Within this framework, the paper makes the following key contributions:

1) Defines two key applications - evaluation of a DSS prior to and during deployment, and correcting for performative bias when retraining a prediction model. 

2) Formally defines metrics like the deployment effect, retraining effect, and performative bias to evaluate a DSS.

3) Shows that estimating these metrics constitutes domain adaptation problems, which are not solvable without additional assumptions. 

4) Introduces the concept of a "domain pivot" - a set of variables that make the outcome independent of the domain shift - and shows this is necessary and sufficient for solving the domain adaptation problems.

5) Provides identification results leveraging domain pivots to estimate the key metrics, enabling evaluation and bias correction.  

6) Demonstrates how repeated regression can practically estimate these metrics, even in presence of selection bias and missing labels.

In summary, the paper provides a causal modelling approach, formal definitions of key metrics, theoretical identification results, and a practical estimation procedure for evaluating and correcting for the effects of deploying decision support systems.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and concepts associated with this paper include:

- Performative prediction - Predictions that aim to instigate action and affect the predicted outcome.

- Decision support systems (DSS) - Algorithmic systems that provide predictions to help guide human decisions and actions. 

- Domain adaptation - Estimating statistical relationships across different domains/contexts, such as pre- and post-deployment of a DSS.

- Evaluation metrics - Quantities defined to assess the impact of deploying or retraining a DSS, such as the deployment effect and retraining effect. 

- Performative bias - Bias induced in training data due to the effects of a previously deployed DSS.

- Domain pivot - A set of variables that make the outcome independent of the domain shift, enabling transport of statistical relationships across domains. 

- Repeated regression - An estimation procedure that uses regression pseudo-labels to account for distribution shift between domains.

- Sample selection bias - Bias arising when only a non-representative subset of data is observed due to filtering.

So in summary, key concepts revolve around modelling, evaluating, and correcting for the impacts that deploying an algorithmic prediction system can have on the processes it is trying to predict.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using a "domain pivot" set of variables Z that renders the target variable Y independent of the domain indicator D. What are some practical strategies for identifying appropriate domain pivot variables in real-world applications? How can one validate if a chosen set of variables satisfies the independence condition?

2. The repeated regression procedure requires estimating conditional expectations of Y given X and Z. What types of regression methods would be most appropriate for this task and why? How does the choice of regression method impact assumptions about the distributions and functional relationships?

3. Proposition 1 shows that the baseline predictor solves a bilevel optimization problem for minimizing negative outcomes while sparingly using actions. Can you think of some alternative optimality criteria that might change the form of the ideal baseline predictor? What implications would this have?  

4. The paper demonstrates how repeated regression can deal with both performative bias and selection bias. Can you think of other types of biases that could potentially be addressed with this method? What modifications or additional assumptions might be needed?

5. One assumption made is that the domain indicator D and parameters Θ do not influence or confound the feature and outcome variables within each epoch. In practice, this may not hold. How would violations of this assumption impact identifiability and estimation?  

6. Could the method be extended to settings beyond binary domain indicators (D = 0 or 1)? What about continuous treatment regimes or multi-valued discrete treatments? Would the identifiability theory hold in such cases?

7. The retraining effect ρ relies on having data from sequential epochs with different DSS parameterizations Θ. What could be done in single-epoch settings without historic data? Could synthetic experiments be designed?

8. How could sensitivity analysis be done to assess robustness of estimates to violations of the key conditional independence assumption involving the domain pivot variables Z? Are there any semiparametric models worth exploring?

9. One limitation raised is that measuring the domain pivot Z in the target domain may be infeasible. Do you have any practical ideas to address this? Could proxy variables be identified under certain causal graphs? 

10. The method corrects for performative bias in predicting baseline risk, but does not aim for optimal performative predictions. Can you envision modifications or extensions to also target optimal performative accuracy? What challenges might arise?
