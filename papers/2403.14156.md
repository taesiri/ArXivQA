# [Policy Mirror Descent with Lookahead](https://arxiv.org/abs/2403.14156)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of solving Markov Decision Processes (MDPs) to find an optimal policy using reinforcement learning algorithms. Specifically, it focuses on a class of algorithms called Policy Mirror Descent (PMD) which includes popular policy gradient methods like natural policy gradient. Recent works have analyzed the convergence rate of PMD showing it matches the convergence rate of Policy Iteration (PI). However, prior work has shown that enhancing PI using multi-step greedy policy improvement (lookahead) can accelerate its convergence rate. This paper investigates incorporating lookahead to PMD.

Proposed Solution: 
The paper proposes a new algorithm called $h$-PMD which incorporates $h$-step lookahead policy improvement to the PMD update rule. This allows $h$-PMD to enjoy a faster $\gamma^h$-linear convergence rate compared to standard PMD, where $h$ is the depth of the lookahead and $\gamma$ is the MDP discount factor. 

The paper considers both exact and inexact variants of $h$-PMD. In the inexact case, the $h$-step lookahead action values are estimated using Monte Carlo tree search planning and sampling. Under suitable conditions, the sample complexity and convergence rates of inexact $h$-PMD are established, showing improvements over prior PMD convergence analyses.

Additionally, the paper extends $h$-PMD to use linear function approximation, where the lookahead action-values are estimated with linear combination of features. This allows $h$-PMD to scale to large state spaces. Under standard assumptions, convergence rates are provided where the dependence on the state space is removed and instead depends on the feature dimension.

Main Contributions:
- A new $h$-PMD algorithm incorporating multi-step lookahead policy improvement to PMD
- Faster $\gamma^h$ convergence rate for exact $h$-PMD 
- Convergence and sample complexity analyses for inexact $h$-PMD under function approximation
- Empirical evaluations showing faster convergence for increasing lookahead depth $h$

Overall, the paper proposes a way to accelerate PMD using ideas from tree-search planning and analyzes the resulting algorithm. This helps connect and transfer benefits between two popular classes of RL algorithms.
