# How susceptible are LLMs to Logical Fallacies?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research questions that this paper addresses are:1) Can large language models (LLMs) change their opinions through reasoning when faced with new arguments? 2) Are large language models susceptible to fallacious reasoning?The paper introduces a new diagnostic benchmark called LOGICOM to assess the logical reasoning capabilities of LLMs. Specifically, LOGICOM evaluates whether LLMs can alter their point of view through reasoning when engaged in multi-round debates, and if they are prone to being misled by logical fallacies. The authors use LOGICOM to test two LLMs - GPT-3.5 and GPT-4. The key findings are:- Both GPT-3.5 and GPT-4 demonstrate an ability to change their opinions through reasoning, indicating they can adjust their logical thinking process. - However, the LLMs are highly susceptible to fallacious reasoning - they are much more likely to be convinced by fallacious arguments compared to logical arguments.So in summary, the central research questions focus on assessing whether LLMs can evolve their reasoning, and testing their robustness against logical fallacies. The benchmark LOGICOM is introduced to diagnose the logical competence of LLMs through multi-agent debates.


## What is the main contribution of this paper?

Based on my reading of the abstract, the main contribution of this paper seems to be the introduction of a new diagnostic benchmark called LOGICOM to assess the logical reasoning capabilities and susceptibility to logical fallacies of large language models (LLMs) like GPT-3.5 and GPT-4. Specifically, LOGICOM:- Allows testing whether LLMs can change their opinions through reasoning when presented with new arguments. - Evaluates how susceptible LLMs are to being convinced by logical fallacies vs logical reasoning.- Uses a dataset of controversial topics/claims to have a "persuader" LLM try to convince a "debater" LLM.- Employs a "fallacious helper" LLM to assist the persuader in using fallacies. - Compares debates where the persuader uses fallacies vs logical reasoning to see if the debater is more easily convinced by fallacies.- Finds GPT-3.5 and GPT-4 can change opinions through reasoning but are 41-69% more convinced by fallacies than logical arguments.- Introduces a new dataset of over 5K pairs of logical/fallacious arguments from the debates.So in summary, the main contribution is introducing LOGICOM as a new benchmark to assess and compare the logical reasoning capabilities of LLMs, and showing the susceptibility of major models like GPT-3.5 and GPT-4 to logical fallacies through experiments using this benchmark. The paper also provides a new dataset based on the LOGICOM debates.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a benchmark called LOGICOM to evaluate the logical reasoning capabilities of large language models like GPT-3.5 and GPT-4 in multi-round debates; it finds these models can change their opinions through reasoning but are highly susceptible to being misled by fallacious arguments, and it releases a new dataset of logical vs fallacious debate arguments.
