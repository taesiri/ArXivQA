# How susceptible are LLMs to Logical Fallacies?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research questions that this paper addresses are:1) Can large language models (LLMs) change their opinions through reasoning when faced with new arguments? 2) Are large language models susceptible to fallacious reasoning?The paper introduces a new diagnostic benchmark called LOGICOM to assess the logical reasoning capabilities of LLMs. Specifically, LOGICOM evaluates whether LLMs can alter their point of view through reasoning when engaged in multi-round debates, and if they are prone to being misled by logical fallacies. The authors use LOGICOM to test two LLMs - GPT-3.5 and GPT-4. The key findings are:- Both GPT-3.5 and GPT-4 demonstrate an ability to change their opinions through reasoning, indicating they can adjust their logical thinking process. - However, the LLMs are highly susceptible to fallacious reasoning - they are much more likely to be convinced by fallacious arguments compared to logical arguments.So in summary, the central research questions focus on assessing whether LLMs can evolve their reasoning, and testing their robustness against logical fallacies. The benchmark LOGICOM is introduced to diagnose the logical competence of LLMs through multi-agent debates.


## What is the main contribution of this paper?

Based on my reading of the abstract, the main contribution of this paper seems to be the introduction of a new diagnostic benchmark called LOGICOM to assess the logical reasoning capabilities and susceptibility to logical fallacies of large language models (LLMs) like GPT-3.5 and GPT-4. Specifically, LOGICOM:- Allows testing whether LLMs can change their opinions through reasoning when presented with new arguments. - Evaluates how susceptible LLMs are to being convinced by logical fallacies vs logical reasoning.- Uses a dataset of controversial topics/claims to have a "persuader" LLM try to convince a "debater" LLM.- Employs a "fallacious helper" LLM to assist the persuader in using fallacies. - Compares debates where the persuader uses fallacies vs logical reasoning to see if the debater is more easily convinced by fallacies.- Finds GPT-3.5 and GPT-4 can change opinions through reasoning but are 41-69% more convinced by fallacies than logical arguments.- Introduces a new dataset of over 5K pairs of logical/fallacious arguments from the debates.So in summary, the main contribution is introducing LOGICOM as a new benchmark to assess and compare the logical reasoning capabilities of LLMs, and showing the susceptibility of major models like GPT-3.5 and GPT-4 to logical fallacies through experiments using this benchmark. The paper also provides a new dataset based on the LOGICOM debates.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a benchmark called LOGICOM to evaluate the logical reasoning capabilities of large language models like GPT-3.5 and GPT-4 in multi-round debates; it finds these models can change their opinions through reasoning but are highly susceptible to being misled by fallacious arguments, and it releases a new dataset of logical vs fallacious debate arguments.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper comparing to other related research:- The use of a multi-agent debate framework to assess logical reasoning capabilities of large language models (LLMs) like GPT-3 and GPT-4 is quite novel. Most prior work has focused on evaluating LLMs on specific reasoning tasks rather than a free-form, multi-round debate setting. - The idea of specifically testing susceptibility to logical fallacies is also unique. Many studies have looked at LLMs' reasoning abilities more broadly, but few have directly tested how prone they are to different types of fallacious arguments. This provides useful insight into a specific limitation.- The proposed LOGICOM benchmark seems more comprehensive than many existing benchmarks for evaluating LLM reasoning. It allows probing change of opinions over multiple rounds of debate and contrasting logical vs fallacious arguments.- The use of a separate "fallacious helper" LLM to generate the fallacies is an interesting idea to isolate the effects without confusing the main persuader agent. This is a clever experimental design choice.- The creation of a new dataset of logical/fallacious argument pairs extracted from model debates is a valuable contribution. Most logical fallacy datasets are limited to individual fallacious statements without context.- In terms of limitations, the reliance on models for all agents (persuaders, helper, moderator) could also introduce errors vs using humans. But this choice is reasonable given the complexity.- The analysis is mainly qualitative at this stage. More quantitative rigor could be added to strengthen the claims, but the overall approach is sound.In summary, the multi-agent debate design, focus on logical fallacies, new benchmark and dataset make this study fairly unique and advance the broader goal of understanding and improving LLM reasoning. The results align with and build upon recent findings about the brittleness of large models.
