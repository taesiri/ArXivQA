# [Retrieving Conditions from Reference Images for Diffusion Models](https://arxiv.org/abs/2312.02521)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces RetriBooru-V1, a new dataset of 116K anime images with rich annotations to enable training diffusion models for tasks like reconstructing a character's identity or composing a character with a given face and outfit. The dataset has multiple images per character identity and outfit clusters to ensure consistency. The authors propose new tasks like concept composition, where models must retrieve relevant information from reference images to synthesize a target image with desired concepts. They also introduce a new metric using visual question answering to measure diversity and flexibility of generated images. A new baseline method is proposed that adds cross-attention layers to stable diffusion to retrieve precise conditions from references while ignoring irrelevant details. Experiments show this method outperforms state-of-the-art on reconstructing human faces while properly controlling generation via text prompts. The method also enables concept composition on the anime dataset. Ablations examine optimal settings for referenced generation tasks in this retrieval-augmented generation paradigm. Overall, this paper pushes progress on selectively retrieving information from references and establishing benchmarks for multi-concept image synthesis.
