# [SKILL: Similarity-aware Knowledge distILLation for Speech   Self-Supervised Learning](https://arxiv.org/abs/2402.16830)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent self-supervised speech models like WavLM and HuBERT have achieved great success but their large size limits practical deployment. To address this, prior works used knowledge distillation to compress these models, but they had limitations: 
1) They distill only selected layers based on heuristics, causing bias towards those layers. 
2) Manual selection of layers to distill is computationally expensive.
3) Layer-to-layer mapping assumes same information is stored at same levels in teacher and student, which may not hold when student is much smaller.

Proposed Solution: 
This paper proposes SKILL - a Similarity-aware Knowledge distILLation method that:

1) Identifies layers with similar information using hierarchical clustering of layer activations on a small calibration set, based on their Centered Kernel Alignment similarity. 

2) Performs distillation on average representations of the identified clusters instead of individual layers. This automatically gives less weight to redundant information and more weight to unique information.

3) Does not require manual selection of layers. Number of clusters is the only hyperparameter.

The overall method has two stages like DPHuBERT - joint structured pruning and distillation, followed by further distillation. But the distillation loss is now computed on cluster averages instead of selected layers.

Contributions:

1) SKILL outperforms DPHuBERT on multiple SUPERB benchmarks when compressing WavLM Base+ and HuBERT Base to ~23M parameters.

2) Compressed WavLM (SKWavLM) sets new SOTA on 5 tasks among 30M parameter models. It has better generalization than DPHuBERT.

3) SKILL shows more robust performance than DPHuBERT when varying target sparsity. It also provides insights into layer redundancy differences between WavLM and HuBERT.

4) Ablations validate design choices like distilling cluster averages and averaging both teacher and student.

Overall, SKILL advances model compression for self-supervised speech models by improving distillation through automated layer clustering. The method and models are made publicly available.
