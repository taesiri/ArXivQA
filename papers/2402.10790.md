# [In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs   Miss](https://arxiv.org/abs/2402.10790)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Processing long documents is a key challenge for generative transformer models like GPT-4 due to the quadratic scaling of compute required for self-attention. 
- Existing methods are only effective for sequences up to 10,000 tokens.
- There is a need for benchmarks to assess model capabilities on extensive texts with distributed facts.

Proposed Solution:
- Introduces BABILong, a new benchmark to evaluate model performance on long documents. It hides algorithmically generated question answering and reasoning problems inside irrelevant book texts.
- Proposes recurrent memory transformer (RMT) that segments long sequences and processes them recurrently for linear scaling. Adds self-retrieval from past memory states to overcome memory bottlenecks.
- Evaluates GPT-4, retrieval augmented GPT-4, and RMT on BABILong up to 11 million tokens.

Main Contributions:
- BABILong benchmark to test processing of arbitrarily long documents with distributed facts. Overcomes data contamination issues.
- Evaluation reveals GPT-4 and retrieval augmented GPT-4 fail when distracting text exceeds 10,000 tokens.  
- RMT with recurrent memory and self-retrieval sustains performance up to 11 million tokens, setting new record for neural sequence processing.
- Analysis shows RMT learns to detect and store facts in memory over extremely long contexts.
- Work demonstrates potential of recurrence and memory for processing extensive texts.

In summary, the paper introduces a new long document QA benchmark and shows recurrent memory models can solve tasks with supporting facts distributed across 11 million tokens, significantly extending known capabilities of neural networks.
