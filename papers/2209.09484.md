# [Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action   Recognition from Egocentric RGB Videos](https://arxiv.org/abs/2209.09484)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: 

How can we develop an effective framework to exploit temporal information for robust 3D hand pose estimation and action recognition from egocentric RGB videos?

The key points are:

- Egocentric RGB videos of hand actions have challenges like frequent self-occlusions and ambiguity in determining action from individual frames. 

- Temporal information can help resolve these issues - it can help infer occluded hand joints from other frames and clarify the overall action by observing the sequence.

- The authors propose a hierarchical temporal transformer framework to leverage short-term and long-term temporal cues for pose estimation and action recognition respectively.

- They design two cascaded transformer encoder blocks - one focuses on a narrow local temporal window for per-frame pose estimation, while the other aggregates information over the full sequence for action recognition.

- The hierarchy also models the semantic correlation between pose and action, by deriving action from lower-level pose and object interactions.

- Evaluations demonstrate state-of-the-art performances on two egocentric hand action datasets, showing the effectiveness of their approach.

In summary, the key hypothesis is that exploiting temporal information hierarchically at different time spans can benefit both pose estimation and action recognition from ambiguous egocentric videos. Their transformer-based framework is designed to validate this.
