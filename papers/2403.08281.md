# [Mastering Text, Code and Math Simultaneously via Fusing Highly   Specialized Language Models](https://arxiv.org/abs/2403.08281)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown mastery in domains like natural language, code, and math when trained extensively on relevant corpora. However, achieving high performance across all domains simultaneously is challenging as specialization in one area often leads to sacrificed performance in others.

Proposed Solution: 
- The paper proposes a model called \modelname that fuses multiple specialized LLMs to retain their expertise. It uses three pretrained specialists - UltraLM (text), CodeLlama (code), WizardMath (math).
- A token-level gating mechanism dynamically controls the contribution of each specialist's output logits based on the input data. This allows specialization as well as generalization.
- A two-stage training strategy is used - first only the gating module is trained while specialists are frozen, then all parameters are fine-tuned. This prevents damage to specialists. 
- Balanced sampling ensures training stability and mitigates imbalance between specialists.

To facilitate training, the paper introduces a high-quality dataset called \dataname with 300K text, code and math instructions.

Main Contributions:
- Proposes \modelname framework to fuse specialized LLMs directly using token-level gating and two-stage balanced training.
- Introduces \dataname dataset with diverse high-quality instructions spanning text, code and math.
- Achieves consistently strong performance across 7 benchmarks in the 3 domains, showing the model has mastered them simultaneously.
- Analysis shows specialists retain functionality and can work synergistically for higher performance.

In summary, the paper enables integrating specialized abilities of multiple LLMs into a unified chat interface with little performance loss via a simple yet effective fusing approach.
