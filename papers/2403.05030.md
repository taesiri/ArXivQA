# [Defending Against Unforeseen Failure Modes with Latent Adversarial   Training](https://arxiv.org/abs/2403.05030)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- AI systems can exhibit harmful unintended behaviors post-deployment, despite extensive testing and adversarial training (AT) pre-deployment. This happens due to systematic differences between the failure modes developers identify vs the ones models exhibit post-deployment (e.g. trojans, jailbreaks, novel attacks).
- Searching the vast input space to find vulnerabilities is challenging. Many failure modes can go unnoticed.
- AT requires examples of failures to fix them, but can struggle to generalize beyond the distribution of attacks seen during training.

Proposed Solution: 
- Use latent adversarial training (LAT) to defend against vulnerabilities without needing to generate inputs that trigger them. 
- LAT applies adversarial perturbations to the latent representations instead of the inputs. As information flows through the neural network layers, the latents develop more compressed and structured representations. 
- Hypothesis: Failures that are hard to elicit from the input space may be easier to elicit and remove via the latent space without needing demonstration examples.

Key Contributions:
- Show LAT can help remove trojans and improve robustness to novel attacks in image classification, text classification and text generation tasks.
- Find that LAT in the optimal layer usually Pareto dominates AT w.r.t clean and robust performance. 
- Cautionary result: Show examples where AT reduces robustness to unseen attacks and where AT/LAT can entrench trojans.

Significance:
- Suggests LAT could be a useful practical tool for making models more robust to unforeseen failures like trojans, jailbreaks, novel attacks. Works by shaping latent space.
- As a relaxation of input space attacks, LAT may enable stronger assurances of robustness in high-stakes applications.

Limitations:
- Only tested up to 7B parameter models and did not evaluate against jailbreaks. 
- LAT performance is sensitive to choice of layer.

Future Work: 
- Explore methods to parameterize, regularize and restrict latent attacks.
- Use latent attacks to evaluate latent capabilities. 
- Use targeted LAT to train out specific unwanted behaviors.
