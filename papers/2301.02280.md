# [Filtering, Distillation, and Hard Negatives for Vision-Language   Pre-Training](https://arxiv.org/abs/2301.02280)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research questions and hypotheses of this paper are:

1) Can careful dataset filtering, model initialization, and modifications to the contrastive training objective lead to improved zero-shot performance for vision-language models? The authors hypothesize that improvements in these areas can boost zero-shot performance.

2) Will a rules-based filtering approach focused on caption complexity, actions, and removing text-spotting examples improve vision-language alignment compared to prior filtering methods? The authors hypothesize their CAT filtering method will be more effective. 

3) Can concept distillation of predicted objects and attributes from a strong pre-trained vision model improve alignment without increased training cost? The authors hypothesize this distillation approach will help alignment while retaining image recognition strength.

4) Will a hard negative contrastive loss help alignment on noisy web-scale datasets compared to standard contrastive losses? The authors hypothesize the proposed hard negative loss will improve alignment.

5) Can a prompt-based initialization approach for few-shot classification improve performance and bridge the gap between zero-shot and few-shot results? The authors hypothesize their prompt initialization strategy will smooth this transition.

The core research questions focus on whether the proposed methods for dataset curation, model initialization, training objectives, and few-shot adaptation can improve vision-language model alignment and zero-shot/few-shot performance compared to prior state-of-the-art approaches. The authors design experiments and benchmarks to test these hypotheses.
