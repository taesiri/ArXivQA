# [Variance Reduction in Deep Learning: More Momentum is All You Need](https://arxiv.org/abs/2111.11828)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a family of variance reduced optimization algorithms for training deep neural networks. The key idea is to leverage the clustering structure present in datasets used for deep learning to reduce the variance of gradient estimates. The central hypothesis is that using multiple momentum terms tailored to each cluster can help reduce the between-cluster variance and lead to faster convergence. Specifically, the paper introduces "Discover" algorithms that maintain approximate gradient estimates per cluster and use them to reduce the gradient noise.The main research questions addressed are:- Can maintaining cluster-specific gradient estimates help reduce between-cluster variance and speed up training of deep networks?- How should momentum-based optimizers like SGD+Momentum and QHM be adapted to exploit clustering structure via a multi-momentum approach?- Do the proposed Discover algorithms indeed converge faster than vanilla SGD, Momentum, QHM etc on benchmark deep learning tasks?- Does the improved convergence translate to benefits in generalization performance?- How do the Discover algorithms compare to prior variance reduction methods for deep learning like IGT and QHM?So in summary, the central hypothesis is about the benefit of using multiple momentum terms tailored to clustering structure for variance reduction in deep learning. The paper proposes Discover algorithms realizing this idea and evaluates their convergence speed and generalization ability empirically.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:- Introducing a family of variance reduced optimizers called Discover for deep learning. Discover exploits the clustering structure in datasets to reduce between-cluster variance of the gradient noise.- Proposing specific Discover variants based on existing optimizers like SGD+Momentum, Implicit Gradient Transport (IGT), and Quasi-Hyperbolic Momentum (QHM). These leverage multiple momentum terms based on the clustering structure.- Providing theoretical analysis to show Discover optimizers can eliminate between-cluster variance and lead to faster convergence. The analysis assumes smooth and strongly convex functions.- Demonstrating empirically that Discover optimizers coupled with simple clustering approaches like using labels or data augmentations can improve optimization convergence and generalization on image classification benchmarks like CIFAR and ImageNet.- Showing the benefits of Discover are more pronounced in challenging settings like learning with noisy labels, where they appear to be more robust and find better solutions.- Providing efficient parallel implementations of Discover optimizers in JAX to enable scaling them to large datasets like ImageNet while maintaining runtime comparable to baseline methods.In summary, the main contribution seems to be introducing Discover, a family of variance reduced optimizers for deep learning that can exploit ubiquitous clustering structure in data to accelerate training convergence and improve generalization. Theoretical motivation, empirical validation, and scalable implementations are provided.
