# [Variance Reduction in Deep Learning: More Momentum is All You Need](https://arxiv.org/abs/2111.11828)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a family of variance reduced optimization algorithms for training deep neural networks. The key idea is to leverage the clustering structure present in datasets used for deep learning to reduce the variance of gradient estimates. The central hypothesis is that using multiple momentum terms tailored to each cluster can help reduce the between-cluster variance and lead to faster convergence. Specifically, the paper introduces "Discover" algorithms that maintain approximate gradient estimates per cluster and use them to reduce the gradient noise.The main research questions addressed are:- Can maintaining cluster-specific gradient estimates help reduce between-cluster variance and speed up training of deep networks?- How should momentum-based optimizers like SGD+Momentum and QHM be adapted to exploit clustering structure via a multi-momentum approach?- Do the proposed Discover algorithms indeed converge faster than vanilla SGD, Momentum, QHM etc on benchmark deep learning tasks?- Does the improved convergence translate to benefits in generalization performance?- How do the Discover algorithms compare to prior variance reduction methods for deep learning like IGT and QHM?So in summary, the central hypothesis is about the benefit of using multiple momentum terms tailored to clustering structure for variance reduction in deep learning. The paper proposes Discover algorithms realizing this idea and evaluates their convergence speed and generalization ability empirically.
