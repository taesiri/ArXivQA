# [Variance Reduction in Deep Learning: More Momentum is All You Need](https://arxiv.org/abs/2111.11828)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a family of variance reduced optimization algorithms for training deep neural networks. The key idea is to leverage the clustering structure present in datasets used for deep learning to reduce the variance of gradient estimates. The central hypothesis is that using multiple momentum terms tailored to each cluster can help reduce the between-cluster variance and lead to faster convergence. Specifically, the paper introduces "Discover" algorithms that maintain approximate gradient estimates per cluster and use them to reduce the gradient noise.The main research questions addressed are:- Can maintaining cluster-specific gradient estimates help reduce between-cluster variance and speed up training of deep networks?- How should momentum-based optimizers like SGD+Momentum and QHM be adapted to exploit clustering structure via a multi-momentum approach?- Do the proposed Discover algorithms indeed converge faster than vanilla SGD, Momentum, QHM etc on benchmark deep learning tasks?- Does the improved convergence translate to benefits in generalization performance?- How do the Discover algorithms compare to prior variance reduction methods for deep learning like IGT and QHM?So in summary, the central hypothesis is about the benefit of using multiple momentum terms tailored to clustering structure for variance reduction in deep learning. The paper proposes Discover algorithms realizing this idea and evaluates their convergence speed and generalization ability empirically.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:- Introducing a family of variance reduced optimizers called Discover for deep learning. Discover exploits the clustering structure in datasets to reduce between-cluster variance of the gradient noise.- Proposing specific Discover variants based on existing optimizers like SGD+Momentum, Implicit Gradient Transport (IGT), and Quasi-Hyperbolic Momentum (QHM). These leverage multiple momentum terms based on the clustering structure.- Providing theoretical analysis to show Discover optimizers can eliminate between-cluster variance and lead to faster convergence. The analysis assumes smooth and strongly convex functions.- Demonstrating empirically that Discover optimizers coupled with simple clustering approaches like using labels or data augmentations can improve optimization convergence and generalization on image classification benchmarks like CIFAR and ImageNet.- Showing the benefits of Discover are more pronounced in challenging settings like learning with noisy labels, where they appear to be more robust and find better solutions.- Providing efficient parallel implementations of Discover optimizers in JAX to enable scaling them to large datasets like ImageNet while maintaining runtime comparable to baseline methods.In summary, the main contribution seems to be introducing Discover, a family of variance reduced optimizers for deep learning that can exploit ubiquitous clustering structure in data to accelerate training convergence and improve generalization. Theoretical motivation, empirical validation, and scalable implementations are provided.


## How does this paper compare to other research in the same field?

This paper introduces a new family of variance reduced stochastic optimization algorithms called Discover for training deep neural networks. It builds on prior work in variance reduction but tailors the methods specifically for deep learning. Here are some key points on how it compares to other related work:- Most prior variance reduction techniques like SVRG, SAGA, SARAH do not work well for deep learning due to high memory cost and computational overhead. This paper proposes modifications like using multiple momentum terms that make variance reduction more suitable for deep learning.- The paper shows both theoretically and empirically that the proposed Discover algorithms exploit clustering structure in data to reduce between-cluster gradient variance. This leads to faster convergence.- The idea of using multiple momentum terms is inspired by the CoverSGD algorithm, but this paper adapts it to mini-batch training and combines it with popular deep learning optimizers like Momentum, QHM and IGT.- Compared to recent VR methods for deep learning like IGT and QHM, Discover optimizers demonstrate faster initial convergence on benchmarks like CIFAR and ImageNet while achieving similar or better end performance.- The paper provides useful insights like showing Momentum also does implicit between-cluster VR, and the algorithms are robust to label noise which is a common problem in large datasets.- The proposed methods are amenable to distributed optimization and a parallel implementation is provided. Experiments show the runtime is comparable to baseline methods.In summary, this paper pushes forward the application of VR in deep learning by designing tailored algorithms that exploit the clustering structure. The ideas like multiple momentum terms and combinations with popular optimizers are novel. The theoretical analysis and experiment insights add to our understanding of optimization for deep learning.


## What future research directions do the authors suggest?

The authors suggest several potential future research directions:- Exploring different clustering structures for Discover algorithms. The paper shows the importance of choosing good clustering structures, so investigating other ways to define clusters could further improve performance. For example, using unsupervised clustering methods.- Applying Discover strategies to additional optimization algorithms like Adam, RMSProp, etc. The paper focuses on SGD, Momentum, QHM and IGT but the multi-momentum approach could likely benefit other optimizers.- Theoretical analysis. The paper provides some theoretical motivation and results, but further analysis of convergence rates, generalization, etc. could add more insight. - Additional empirical studies on larger and more complex datasets. The experiments focus on CIFAR and ImageNet. Evaluating on more tasks and data could reveal strengths/weaknesses.- Implementation optimizations like sparse updates. The paper notes the algorithms are parallelizable but other optimizations could improve scaling.- Combining Discover with methods like batch normalization and dropout to see if benefits still apply. The paper mentions VR struggles with these techniques.- Developing multi-momentum optimizers that dynamically determine the clustering. The current algorithms rely on predefined clusters. Automating cluster assignment could make the methods more widely applicable.- Extending Discover for settings like federated learning where clustering arises naturally. The motivation mentions applications like this but they are not evaluated.So in summary, the main suggestions are around exploring additional clustering approaches, applying to more optimizers and tasks, further theoretical study, and implementation/scaling improvements. The core idea of multi-momentum VR seems promising but needs more development and evaluation.
