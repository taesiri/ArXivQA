# [Data Contamination Issues in Brain-to-Text Decoding](https://arxiv.org/abs/2312.10987)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Brain-to-text decoding aims to decode non-invasive cognitive signals (e.g. fMRI, EEG) into corresponding natural language text. This enables building practical brain-computer interfaces (BCIs).  
- However, how to split datasets for training, validation and testing remains an open question. The choices significantly impact model performance evaluation.

Key Insights:
- Existing dataset splitting methods suffer from two types of data contamination:
    1) Cognitive signal leakage: Subjects' signals in test set leaking into training set, causing encoder overfitting
    2) Text stimuli leakage: Text stimuli sentences in test set leaking into training set, allowing decoder to memorize and cheat
- These exaggerate model performance and generalizability. A principled splitting method without such leakage is needed.

Proposed Solution:
- Split dataset into subject-stimuli pairs, ensuring:
    1) Subjects in validation/test sets do not appear in training set 
    2) Text stimuli in validation/test sets do not appear in training set
- This eliminates both kinds of data contamination and allows fair evaluation.
- Detailed algorithms catering to EEG and fMRI data characteristics are provided.

Contributions:  
- First paper investigating dataset splitting for brain-to-text decoding
- Proved existence of data contamination issues in current methods through analysis and experiments
- Proposed first splitting technique eliminating both kinds of leakage 
- Re-evaluated SOTA models under new splits to release fair benchmark

The paper makes an important contribution in formalizing dataset splitting for this problem. The insights and proposed technique will enable more rigorous evaluation of models going forward.


## Summarize the paper in one sentence.

 This paper proposes a new data splitting method to avoid data contamination from both cognitive signal leakage and text stimuli leakage in brain-to-text decoding, and shows that current models have overestimated performance due to such leakage.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. The paper investigates current dataset splitting methods for brain-to-text decoding and analyzes their influence on popular frameworks. It finds that existing methods suffer from data contamination which exaggerates model performance.

2. The paper proves the existence of two kinds of data contamination - cognitive signal leakage and text stimuli leakage - through both analysis and experiments. This seriously overestimates models' generalization ability. 

3. The paper proposes a new dataset splitting method that can avoid both cognitive signal and text stimuli leakage. This helps fix the data contamination issue and evaluate models fairly.

4. The paper reevaluates current state-of-the-art models under the proposed splitting method and releases a benchmark for further research in this domain.

In summary, the key contribution is proposing a dataset splitting method that eliminates data contamination and provides a fair benchmark for brain-to-text decoding research.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Brain-to-text decoding: Converting brain signals like fMRI and EEG into corresponding natural language text.

- Data contamination: Leakage of test data into the training set, which can exaggerate model performance. Two types identified are cognitive signal leakage and text stimuli leakage.  

- Overestimated performance: Models performing better than their actual capability due to data contamination issues. 

- Encoder-decoder framework: Popular framework used in brain-to-text models, with encoder converting signals to representations and decoder generating text.

- Dataset splitting methods: Different paradigms to split brain decoding datasets into train/validation/test sets, with analysis showing issues in current methods.

- Proposed splitting method: New dataset splitting technique introduced to avoid both cognitive signal and text stimuli leakage.

- Generalization ability: Ability of brain decoding models to work on new unseen subjects and text stimuli, which requires proper dataset splitting.

- Benchmark: A fair model evaluation benchmark created using the proposed splitting method to measure generalization capability.

Does this summary cover the key ideas and terminology from the paper? Let me know if you need any clarification or have additional questions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new data splitting method to avoid data contamination in brain-to-text decoding. Can you explain in detail the two types of data contamination (cognitive signal leakage and text stimuli leakage) and how they affect model performance? 

2. The paper introduces two new metrics - CSLR and TSLR for quantifying data contamination. Can you elaborate on the mathematical formulas for these metrics and how they are used to detect leakage?

3. For the proposed data splitting method, the paper considers bipartite graphs for EEG and fMRI datasets separately. Can you walk through the key steps in generating the training/validation/test splits for both EEG and fMRI and highlight the differences? 

4. The paper analyzes the effect of data contamination on the encoder and decoder separately. For the encoder, it uses signal reconstruction loss to evaluate impact. Can you explain this experiment in detail and interpret the results?  

5. For analyzing the effect on the decoder, the paper uses changing training epochs and learning rate. Can you explain why this setup can detect memorization of test data?

6. The proposed data splitting method discards some data to avoid contamination. What is the underlying trade-off here? Are there any alternatives you can think of to avoid discarding data?

7. The paper releases a benchmark for model evaluation under the new splitting method. Can you summarize the key results and compare them to published SOTA performance? What inferences can you draw?

8. Do you think the conclusions from this analysis can be generalized to other sequence generation tasks like summarization, translation etc. which use auto-regressive decoders? Why or why not?

9. The paper identifies some limitations about public datasets not following the proposed splitting guidelines. What concrete steps would you recommend to creators of new BCI datasets? 

10. For the encoder-decoder framework used in current models, the paper finds the encoder does not contribute much. What architectural or objective changes can you think of to improve encoder usage?
