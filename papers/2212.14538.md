# [Transformer in Transformer as Backbone for Deep Reinforcement Learning](https://arxiv.org/abs/2212.14538)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most deep reinforcement learning (RL) methods focus on innovating the RL algorithms while using standard network architectures like CNNs and MLPs. Recently, methods have started using Transformers, but require combining them with other modules like CNNs and LSTMs. This makes training inconvenient due to needing expertise to train diverse modules jointly. 

Proposed Solution:
This paper proposes using pure Transformer-based networks as backbones for deep RL to avoid having to combine modules. They introduce two Transformer in Transformer (TIT) backbones made of an Inner Transformer to process individual observations and an Outer Transformer to process observation sequences. 

Contributions:
1) First to show pure Transformers can serve as backbones for standard online and offline RL without changing other settings. This provides ready-to-use architectures.

2) Propose two TIT backbones - Vanilla_TIT simply cascades Inner and Outer Transformers while Enhanced_TIT fuses spatial-temporal information in each block and uses dense connections.

3) Empirically demonstrate TIT backbones achieve comparable or better performance than strong baselines in online RL, offline RL and supervised offline settings without needing complex optimization skills.

4) Analyze TIT via ablation studies and attention visualization to show the importance of component designs like the dense connections.

In summary, this paper demonstrates the viability of pure Transformer backbones for deep RL and provides two easy-to-use designs as off-the-shelf options to avoid combining multiple network modules.
