# [Contextual Fusion For Adversarial Robustness](https://arxiv.org/abs/2011.09526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be whether a biologically-inspired multimodal fusion approach can improve the adversarial robustness of deep neural networks. Specifically, the authors explore whether combining object-centric (foreground) and scene-centric (background) features can help defend against different types of adversarial attacks, without compromising performance on clean images. The key hypotheses seem to be:- Adversarial attacks may have divergent effects on foreground vs background feature representations. - Utilizing a combination of modalities (foreground + background) can potentially make image classification more robust to adversarial perturbations.- Background (contextual) features may provide additional useful information to complement the object-oriented foreground, and help improve classification, especially under adversarial attack. The authors test these hypotheses by developing foreground, background and joint classifiers, and evaluating their performance on blurred and adversarial images from the MS COCO and CIFAR-10 datasets. Their goal is to demonstrate the potential benefits of multimodal fusion for adversarial robustness, inspired by the brain's ability to integrate diverse sensory streams.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing a novel method for improving adversarial robustness of image classifiers by fusing foreground (object-focused) and background (context-focused) features extracted using two separate CNNs. - Demonstrating that adversarial attacks have divergent effects on the foreground and background feature spaces. Blurring the foreground objects shifts the foreground feature space but leaves the background feature space intact.- Showing that combining foreground and background features via late fusion improves robustness to Gaussian blur perturbations applied to the foreground, especially when the contexts are more variable (dissimilar contexts).- Demonstrating that the proposed fusion method improves robustness against gradient-based FGSM attacks for the MS COCO dataset, without decreasing performance on clean images or requiring adversarial training. The benefit depends on both modalities contributing equally to the joint classifier.- Introducing a regularization method to bias the joint classifier towards the more robust background features when the foreground is known to be attacked. This outperforms standard adversarial training.- Proposing that integrating multimodal contextual information, inspired by biological sensory processing, can improve adversarial robustness in a complementary way to existing defenses.In summary, the key innovation is using distinct foreground and background modalities to create a fused representation that is more robust to perturbations than a single modality classifier. This biologically-inspired approach does not require adversarial training or compromise clean image accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using a combination of foreground object features and background scene features extracted from different convolutional neural networks as a way to improve adversarial robustness of image classifiers.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on improving adversarial robustness of deep neural networks:- The main novelty is using multimodal fusion of foreground (object-focused) and background (context-focused) features to improve robustness. Most prior work has focused on unimodal approaches applied to the object/foreground pathway. - Showing that background features are less affected by adversarial perturbations targeted at foreground is an important insight. This helps explain why fusion can improve robustness.- Evaluating robustness on both imperceptible (FGSM) and human-perceivable (blur) perturbations is more thorough than many papers that focus on one type.- Testing on both synthetic (CIFAR) and real-world (COCO) datasets also provides more convincing evidence. Many papers only use one dataset.- The regularization approach to bias fusion is novel and shows better performance than standard adversarial training. Most papers only compare to undefended models.- The biological motivation from human/animal multisensory perception provides an interesting perspective. However, the fusion model used is still rather simple compared to biology.- The scale of experiments is relatively small compared to some state-of-the-art work, using subsets of COCO and CIFAR. Testing on larger benchmarks like ImageNet would further validate claims.Overall, I think the idea of using multimodal fusion to improve adversarial robustness is innovative and promising. The experiments convincingly demonstrate benefits on multiple datasets and perturbations. More work is still needed to scale up and refine the fusion approaches for greater impact compared to other defense strategies. But this is an encouraging start pointing in a useful new research direction.
