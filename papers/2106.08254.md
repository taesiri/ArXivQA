# [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)

## What is the central research question or hypothesis that this paper addresses?

 This paper does not seem to explicitly state a central research question or hypothesis. However, based on my reading, the key focus of the paper is developing a self-supervised visual representation learning method called BEiT, which stands for Bidirectional Encoder Representations from Transformers. 

The key ideas presented are:

- Proposing a masked image modeling (MIM) pre-training task, inspired by BERT's masked language modeling, to pretrain vision Transformers in a self-supervised manner.

- Using two views of an image during pre-training - image patches and discrete visual tokens obtained via a pretrained discrete VAE. 

- Randomly masking some image patches and predicting the original visual tokens based on the corrupted image patches.

- Showing this MIM pre-training improves performance on downstream tasks like image classification and semantic segmentation compared to training from scratch or other self-supervised methods.

So in summary, the main focus is presenting BEiT and showing its effectiveness for self-supervised visual representation learning with Transformers, rather than testing a specific hypothesis. The key novelty is the proposed MIM pre-training task adapting BERT-style masking to the visual domain.


## What is the main contribution of this paper?

 This paper introduces BEiT, a self-supervised vision representation model based on BERT pre-training. The key contributions are:

- Proposes a masked image modeling (MIM) task to pretrain vision Transformers. This is inspired by BERT's masked language modeling, but adapted for continuous image inputs. 

- Uses a discrete VAE to tokenize images into visual tokens. The pretraining task is to predict the original visual tokens based on corrupted image patches. This avoids regressing to raw pixels.

- Achieves strong performance on image classification and segmentation after pretraining, outperforming from-scratch training and previous self-supervised methods.

- Analyzes the self-attention maps and shows BEiT can separate semantic regions without human annotations, explaining its generalization ability.

In summary, the main contribution is proposing an effective BERT-style pretraining approach for vision Transformers, including the masked image modeling task and use of discrete visual tokens. This matches the performance of supervised pretraining while only using image labels during fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a self-supervised vision model called BEiT that pretrains an image Transformer using masked image modeling, where images are represented with discrete visual tokens and the model is trained to recover the original tokens from corrupted image patches.
