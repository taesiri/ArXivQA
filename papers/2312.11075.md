# [Split and Rephrase with Large Language Models](https://arxiv.org/abs/2312.11075)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Splitting complex sentences into simpler, shorter ones can help humans and machines process complex texts more easily. This task is known as Split and Rephrase (SPRP).
- Existing approaches rely on scarce training data of aligned complex-simple sentences or parallel corpora. This limits their applicability across languages and domains.

Proposed Solution:  
- Use large language models (LLMs) to generate sequences of simpler sentences from complex ones based on a prompt.
- Explore variants: fine-tuning LLMs on SPRP data, zero-shot and few-shot prompting of instruction-tuned LLMs.  

Experiments and Results:
- Test on DeSSE and BiSECT datasets. Fine-tuned LLMs achieve state-of-the-art results, outperforming previous approaches on all metrics.
- Instruction-tuned LLMs competitive using only 5 examples, promising for low-resource scenarios.
- Additional human evaluations confirm improvements and output quality.
- Analyze impact of training data size and model size. Models with ~7B parameters provide a good balance.
- Show potential for cross-domain transfer and multilingual capability.

Main Contributions:
- Demonstrate strong performance of LLM-based SPRP using little training data/parameters.
- Establish feasibility of approach under low-resource scenarios via instruction-tuned LLMs.  
- Significantly outperform previous SOTA results on benchmark datasets.
- Provide insights on model size, training data needs, domain transferability.
- Show potential for multilingual SPRP.
