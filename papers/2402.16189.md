# [One-stage Prompt-based Continual Learning](https://arxiv.org/abs/2402.16189)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing prompt-based continual learning (PCL) methods suffer from high computational costs due to two vision transformer (ViT) feedforward stages - one for generating the prompt query and another for the backbone model with selected prompts. This makes deployment difficult on resource-constrained devices. 

Proposed Solution:
This paper proposes a one-stage PCL framework (OS-Prompt) that removes the separate feedforward stage for query generation. Instead, it directly uses the intermediate layers' token embeddings as the prompt query. This is based on the observation that early layers show minimal shifts in feature space during continual prompt learning.

Main Contributions:

1) Proposes OS-Prompt that reduces computational costs by ~50% with marginal drop in accuracy (<1%) by removing the separate query generation stage.

2) Introduces a Query-Pool Regularization (QR) loss to enhance representation power of prompts. This loss regulates the relationship between the prompt query and pool to be similar to the final layer's embeddings. The QR loss is only applied during training.

3) With the QR loss, the enhanced model OS-Prompt++ outperforms prior two-stage PCL methods by ~1.4% on CIFAR-100, ImageNet-R and DomainNet benchmarks while still maintaining ~50% computational savings during inference.

4) Analysis shows OS-Prompt variants consistently perform better than prior PCL methods like L2P, DualPrompt across varying backbones and prompt configurations. The effectiveness does depend on the prompt formation strategy.

In summary, the paper makes PCL more efficient by removing the separate query generation feedforward, with marginal impact on accuracy. The QR regularization loss further bridges the minor performance gap.
