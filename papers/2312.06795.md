# [Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks](https://arxiv.org/abs/2312.06795)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Foundation models like CLIP and T5 have become very popular and effective on a range of tasks. A common approach is to fine-tune these foundation models on specific downstream tasks, resulting in many publicly available fine-tuned models. However, jointly training a single multi-task model on multiple datasets is often infeasible due to computational constraints, data privacy issues, or lack of access to the training data. Existing techniques for merging multiple fine-tuned models struggle to scale and generalize as more models are combined.

Proposed Solution: 
The paper introduces Model Breadcrumbs, a simple yet effective framework to construct multi-task models from existing fine-tuned foundation models without needing the original training data. It works by subtracting the foundation model weights from each fine-tuned model to get a weight trajectory or "breadcrumb" capturing the important task-specific knowledge. These trajectories are sparsified to remove negligible and outlier weight changes. The sparse trajectories from multiple tasks are aggregated and applied on top of the foundation model weights to create the multi-task model.

Key Contributions:
- Model Breadcrumbs consistently outperforms prior state-of-the-art like Task Vectors, especially as more tasks are combined, with superior scalability and hyperparameter generalization.
- Experiments across 8 image and 4 NLP datasets show Breadcrumbs enables creating multi-task models that match or exceed individual fine-tuned model performance.
- Model Breadcrumbs can also enhance existing fine-tuned models by merging related tasks, without needing additional data.
- The approach is simple, efficient, robust, and aligns with the paradigm of updatable machine learning through community-driven model refinement.

In summary, Model Breadcrumbs provides an effective way to construct versatile multi-task models from publicly available or privately trained fine-tuned foundation models, with promising scalability and performance.
