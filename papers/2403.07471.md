# [On the nonconvexity of some push-forward constraints and its   consequences in machine learning](https://arxiv.org/abs/2403.07471)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Push-forward constraints, where a function redistributes one probability distribution to another, play an important role in machine learning problems like generative modeling and algorithmic fairness. 
- However, the literature lacks an understanding of when such constraints lead to convex optimization problems, which are desirable for statistical and computational reasons. Specifically, it is unclear how the convexity of the set of feasible "transport maps" or "equalizing maps" relates to properties of the input measures.

Proposed Solution:
- Provide a theoretical analysis on when the sets of transport maps (that push one measure to another) and equalizing maps (that push two measures to the same output) are convex.
- Show that these sets are typically not convex, which makes it impossible to define convex losses that quantify deviation from these nonconvex constraints.
- Apply this framework to explain fundamental limitations in constructing convex learning problems for generative modeling and algorithmic fairness.

Main Contributions:
- Precise characterization of when the sets of transport maps and equalizing maps are empty, singleton, or not convex between common measures.
- Proof that no convex loss can quantify deviation from a nonconvex constraint, elucidating obstacles in machine learning tasks involving push-forward constraints. 
- Concrete illustrations of how push-forward generative modeling and statistical parity constraints fundamentally suffer from nonconvexity, providing guidance on the (im)possibility of convex formulations.
- Discussion of strategies to recover convexity by strengthening/weakening constraints or using random couplings.

Overall, the paper thoroughly investigates the convexity of push-forward constraints, connects this to limitations in convex learning formulations, and gives direction for recovering convexity. The technical measure-theoretic analysis paired with the machine learning implications make notable contributions.
