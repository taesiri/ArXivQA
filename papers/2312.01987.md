# [Bootstrapping SparseFormers from Vision Foundation Models](https://arxiv.org/abs/2312.01987)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a method to efficiently "bootstrap" SparseFormers, a variant of vision transformers that uses significantly fewer tokens, from large-scale pre-trained vision models like AugReg and CLIP. The key idea is to inherit most of the pre-trained weights from the foundation models into the SparseFormer architecture, only training the lightweight focusing transformer that adjusts token regions of interest. With minimal tuning, the SparseFormer output can be aligned to the original model's output using an ImageNet-1K sample set, without needing labels. Experiments show bootstrapped 49-token SparseFormers can reach 84.9% ImageNet top-1 accuracy while being over 3x faster, almost matching the teachers they bootstrap from. The method works for dense prediction too, with bootstrapped SparseFormers reaching 51+ mIoU in segmentation. Notably, bootstrapping from CLIP without any text enables strong zero-shot transfer to vision-language tasks. Additionally, bootstrapped SparseFormers can directly replace vision encoders in multimodal models like LLaVa without re-tuning language components. Overall, the bootstrap technique enables efficiently training performant SparseFormers for both vision and vision-language tasks.
