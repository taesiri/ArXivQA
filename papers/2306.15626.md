# [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://arxiv.org/abs/2306.15626)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can open-source toolkits, data, models, and benchmarks facilitate research on machine learning methods for theorem proving, by removing barriers like private code/data and large compute requirements?2) Can retrieval-augmented language models effectively select relevant premises from a large mathematical library when proving new theorems? 3) Will a retrieval-augmented prover outperform non-retrieval baselines and state-of-the-art models like GPT-4 on the task of theorem proving?4) Can the proposed LeanDojo toolkit robustly extract training data from Lean and enable reliable interaction for evaluating theorem provers?5) Will the proposed LeanDojo Benchmark, with its novel data splits, provide a challenging testbed to evaluate whether provers can generalize to novel premises outside the training set?In summary, the central research questions seem to be around using open resources to facilitate ML research for theorem proving, and demonstrating the effectiveness of a retrieval-augmented prover on challenging theorem proving benchmarks extracted using the LeanDojo toolkit.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introducing open-source tools (LeanDojo) for extracting data from and interacting with the Lean proof assistant. This includes capabilities for extracting training data like proof trees and premise information. 2. Developing ReProver, the first retrieval-augmented language model for theorem proving in Lean. ReProver selects relevant premises to condition its tactic generation on, using a retriever based on Dense Passage Retriever. The retriever is tailored for premise selection by focusing on accessible premises and using in-file negative sampling.3. Constructing a new benchmark (LeanDojo Benchmark) with over 96,000 theorems/proofs extracted from Lean's math library. It features a challenging split requiring generalization to novel premises.4. Demonstrating ReProver's effectiveness on premise selection and theorem proving tasks on this benchmark. ReProver outperforms baselines like tidy and GPT-4, and shows competitive performance on existing datasets like MiniF2F and ProofNet. 5. Releasing the open-source LeanDojo toolkit, ReProver model, benchmark, and other resources to lower barriers to research on learning-based theorem proving. This establishes accessible baselines to build upon, compared to prior work relying on private data/code.In summary, the main contribution seems to be providing an open, reproducible framework and strong baseline model for learning-based theorem proving in Lean, enabled by new tools for data extraction and interaction. The retrieval-augmented ReProver model is shown to be effective on a challenging new benchmark based on Lean's math library.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for training and evaluating large language models on interactive theorem proving in Lean, and develops ReProver, a retrieval-augmented language model that shows promising results by incorporating premise selection.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related work in the field of theorem proving with machine learning:- It focuses on the Lean proof assistant, whereas much existing work has targeted Coq, Isabelle/HOL, or HOL Light. There has been some prior work on Lean, but this paper provides the most comprehensive tooling and benchmarks.- It introduces LeanDojo, an open-source toolkit for extracting training data and interacting with Lean. This lowers the barriers compared to prior methods that relied on private code and data.- It develops ReProver, the first retrieval-augmented language model for theorem proving. Prior language model-based provers did not incorporate explicit premise selection. - For premise selection, it proposes innovations like focusing retrieval on accessible premises and using in-file negatives. These improve substantially upon standard Dense Passage Retrieval.- It creates a new benchmark, LeanDojo Benchmark, for evaluating premise selection and theorem proving. It is larger and more challenging than prior Lean benchmarks.- ReProver achieves competitive results to prior methods while using modest compute resources (1 GPU week). In contrast, some existing provers required thousands of GPU days to train.- It demonstrates the value of retrieval augmentation and challenging data splits. The performance substantially degrades without retrieval or on the "novel_premises" split.Overall, this paper pushes forward the state-of-the-art in openness, scale, and rigor compared to related work. The public code, data, and models provide an accessible starting point for future research to build upon.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring stronger transformer-based language models for theorem proving, such as more recent models with larger scale and capacity. The authors use ByT5 in their work, but suggest examining models like CodeGen, StarCoder, and CodeGeeX that have shown strong performance on code generation tasks.- Improving the premise retrieval component, for example by using architectures that allow fusing more retrieved premises or switching to generative retrieval methods. - Overcoming the limitations of training only on human-written proofs, for instance by incorporating auxiliary training data or proofs collected through online interaction. This could help with data scarcity issues and improve generalization.- Examining the potential of large language models like GPT-4 more thoroughly for theorem proving via better prompting strategies and search algorithms. The authors see promise in using theorem proving to study LLMs' capabilities in planning and search.- Supporting Lean 4 in addition to Lean 3, since Lean 4 is gaining adoption but currently less mature. The authors have preliminary Lean 4 results but suggest more work on it.- Mitigating issues like hallucination and lack of factuality that LLMs can exhibit on theorem proving tasks where correctness is critical. Theorem proving serves as a rigorous benchmark for advancing LLMs' reasoning skills.In summary, the main future directions are around scaling up models and data, strengthening the retrieval component, improving out-of-distribution generalization, rigorously evaluating LLMs, and supporting emerging proof assistants like Lean 4. The authors aim to establish strong baselines to enable further research in these areas.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for learning-based theorem proving in the Lean proof assistant. LeanDojo extracts training data from Lean and enables models to interact with Lean's proof environment. It contains annotations of premises used in proofs, providing data for premise selection. Using this, the authors develop ReProver, a retrieval-augmented language model that selects relevant premises when generating proof tactics. They construct a benchmark with over 96K theorems extracted from Lean's math library, featuring a challenging split requiring generalization to novel premises. Experiments show ReProver outperforms non-retrieval baselines and GPT-4 on this benchmark. The authors open-source their data, code, and models to facilitate research on language models for theorem proving. Overall, the paper lowers barriers to research in this area by providing accessible tools and strong baselines trainable on a single GPU.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces LeanDojo, an open-source framework for learning-based theorem proving in the Lean proof assistant. LeanDojo provides tools for extracting training data from Lean and enabling models to interact with Lean's proof environment. It extracts information like proof trees and premises that are not directly visible in raw Lean code. LeanDojo also constructs a new benchmark for premise selection and theorem proving, with over 90,000 theorems/proofs extracted from Lean's math library. The benchmark features a challenging split requiring generalization to novel premises. Using the data and benchmark from LeanDojo, the authors develop ReProver, the first retrieval-augmented language model for theorem proving. It incorporates a retriever based on Dense Passage Retriever to select relevant premises, along with innovations like focusing retrieval on accessible premises only. ReProver outperforms baselines without retrieval on the LeanDojo benchmark. It also achieves competitive results on existing datasets MiniF2F and ProofNet, while being trained with far fewer resources than prior work. By open-sourcing the code, data, and models, the work aims to make state-of-the-art theorem provers accessible and lower the barriers for future research.
