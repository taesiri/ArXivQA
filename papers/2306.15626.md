# [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://arxiv.org/abs/2306.15626)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can open-source toolkits, data, models, and benchmarks facilitate research on machine learning methods for theorem proving, by removing barriers like private code/data and large compute requirements?2) Can retrieval-augmented language models effectively select relevant premises from a large mathematical library when proving new theorems? 3) Will a retrieval-augmented prover outperform non-retrieval baselines and state-of-the-art models like GPT-4 on the task of theorem proving?4) Can the proposed LeanDojo toolkit robustly extract training data from Lean and enable reliable interaction for evaluating theorem provers?5) Will the proposed LeanDojo Benchmark, with its novel data splits, provide a challenging testbed to evaluate whether provers can generalize to novel premises outside the training set?In summary, the central research questions seem to be around using open resources to facilitate ML research for theorem proving, and demonstrating the effectiveness of a retrieval-augmented prover on challenging theorem proving benchmarks extracted using the LeanDojo toolkit.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. Introducing open-source tools (LeanDojo) for extracting data from and interacting with the Lean proof assistant. This includes capabilities for extracting training data like proof trees and premise information. 2. Developing ReProver, the first retrieval-augmented language model for theorem proving in Lean. ReProver selects relevant premises to condition its tactic generation on, using a retriever based on Dense Passage Retriever. The retriever is tailored for premise selection by focusing on accessible premises and using in-file negative sampling.3. Constructing a new benchmark (LeanDojo Benchmark) with over 96,000 theorems/proofs extracted from Lean's math library. It features a challenging split requiring generalization to novel premises.4. Demonstrating ReProver's effectiveness on premise selection and theorem proving tasks on this benchmark. ReProver outperforms baselines like tidy and GPT-4, and shows competitive performance on existing datasets like MiniF2F and ProofNet. 5. Releasing the open-source LeanDojo toolkit, ReProver model, benchmark, and other resources to lower barriers to research on learning-based theorem proving. This establishes accessible baselines to build upon, compared to prior work relying on private data/code.In summary, the main contribution seems to be providing an open, reproducible framework and strong baseline model for learning-based theorem proving in Lean, enabled by new tools for data extraction and interaction. The retrieval-augmented ReProver model is shown to be effective on a challenging new benchmark based on Lean's math library.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for training and evaluating large language models on interactive theorem proving in Lean, and develops ReProver, a retrieval-augmented language model that shows promising results by incorporating premise selection.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other related work in the field of theorem proving with machine learning:- It focuses on the Lean proof assistant, whereas much existing work has targeted Coq, Isabelle/HOL, or HOL Light. There has been some prior work on Lean, but this paper provides the most comprehensive tooling and benchmarks.- It introduces LeanDojo, an open-source toolkit for extracting training data and interacting with Lean. This lowers the barriers compared to prior methods that relied on private code and data.- It develops ReProver, the first retrieval-augmented language model for theorem proving. Prior language model-based provers did not incorporate explicit premise selection. - For premise selection, it proposes innovations like focusing retrieval on accessible premises and using in-file negatives. These improve substantially upon standard Dense Passage Retrieval.- It creates a new benchmark, LeanDojo Benchmark, for evaluating premise selection and theorem proving. It is larger and more challenging than prior Lean benchmarks.- ReProver achieves competitive results to prior methods while using modest compute resources (1 GPU week). In contrast, some existing provers required thousands of GPU days to train.- It demonstrates the value of retrieval augmentation and challenging data splits. The performance substantially degrades without retrieval or on the "novel_premises" split.Overall, this paper pushes forward the state-of-the-art in openness, scale, and rigor compared to related work. The public code, data, and models provide an accessible starting point for future research to build upon.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring stronger transformer-based language models for theorem proving, such as more recent models with larger scale and capacity. The authors use ByT5 in their work, but suggest examining models like CodeGen, StarCoder, and CodeGeeX that have shown strong performance on code generation tasks.- Improving the premise retrieval component, for example by using architectures that allow fusing more retrieved premises or switching to generative retrieval methods. - Overcoming the limitations of training only on human-written proofs, for instance by incorporating auxiliary training data or proofs collected through online interaction. This could help with data scarcity issues and improve generalization.- Examining the potential of large language models like GPT-4 more thoroughly for theorem proving via better prompting strategies and search algorithms. The authors see promise in using theorem proving to study LLMs' capabilities in planning and search.- Supporting Lean 4 in addition to Lean 3, since Lean 4 is gaining adoption but currently less mature. The authors have preliminary Lean 4 results but suggest more work on it.- Mitigating issues like hallucination and lack of factuality that LLMs can exhibit on theorem proving tasks where correctness is critical. Theorem proving serves as a rigorous benchmark for advancing LLMs' reasoning skills.In summary, the main future directions are around scaling up models and data, strengthening the retrieval component, improving out-of-distribution generalization, rigorously evaluating LLMs, and supporting emerging proof assistants like Lean 4. The authors aim to establish strong baselines to enable further research in these areas.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for learning-based theorem proving in the Lean proof assistant. LeanDojo extracts training data from Lean and enables models to interact with Lean's proof environment. It contains annotations of premises used in proofs, providing data for premise selection. Using this, the authors develop ReProver, a retrieval-augmented language model that selects relevant premises when generating proof tactics. They construct a benchmark with over 96K theorems extracted from Lean's math library, featuring a challenging split requiring generalization to novel premises. Experiments show ReProver outperforms non-retrieval baselines and GPT-4 on this benchmark. The authors open-source their data, code, and models to facilitate research on language models for theorem proving. Overall, the paper lowers barriers to research in this area by providing accessible tools and strong baselines trainable on a single GPU.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper introduces LeanDojo, an open-source framework for learning-based theorem proving in the Lean proof assistant. LeanDojo provides tools for extracting training data from Lean and enabling models to interact with Lean's proof environment. It extracts information like proof trees and premises that are not directly visible in raw Lean code. LeanDojo also constructs a new benchmark for premise selection and theorem proving, with over 90,000 theorems/proofs extracted from Lean's math library. The benchmark features a challenging split requiring generalization to novel premises. Using the data and benchmark from LeanDojo, the authors develop ReProver, the first retrieval-augmented language model for theorem proving. It incorporates a retriever based on Dense Passage Retriever to select relevant premises, along with innovations like focusing retrieval on accessible premises only. ReProver outperforms baselines without retrieval on the LeanDojo benchmark. It also achieves competitive results on existing datasets MiniF2F and ProofNet, while being trained with far fewer resources than prior work. By open-sourcing the code, data, and models, the work aims to make state-of-the-art theorem provers accessible and lower the barriers for future research.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper introduces ReProver, a retrieval-augmented language model for automated theorem proving in Lean. ReProver is built on top of LeanDojo, an open-source toolkit introduced in this work for extracting training data and interacting with Lean's proof environment. At its core, ReProver uses a Dense Passage Retriever to select a small number of relevant premises from Lean's math library that are concatenated with the current proof state and fed into a Transformer encoder-decoder model to generate the next proof tactic. Two key innovations enable more effective retrieval: first, restricting candidates to premises accessible in the current context based on program analysis of Lean code; and second, using in-file negatives to find hard negative training examples. ReProver is trained on a new dataset extracted from Lean's mathlib and demonstrates improved performance over baselines without retrieval as well as zero-shot use of GPT-4.


## What problem or question is the paper addressing?

 The paper is addressing several key problems and questions related to using large language models for automated theorem proving:1. Lack of openness and reproducibility in existing LLM-based theorem provers: The paper notes that current research on LLM-based theorem provers faces barriers due to a lack of open source code, reliance on private datasets, and large compute requirements. This makes it difficult to reproduce or build upon existing work.2. Premise selection as a key bottleneck: Selecting the right premises (lemmas, definitions, etc.) from a large math library is recognized as a major challenge in theorem proving. Existing LLM provers do not explicitly tackle premise selection. 3. Overestimation of LLM prover performance: The paper argues that common practices for evaluating LLM provers, like random data splits, can lead to overestimated performance as models exploit similarities between training and test theorems.4. Enabling interaction with proof assistants: Existing tools for interacting with proof assistants like Lean have limitations that distort evaluation and training. More reliable interaction is needed.5. Developing inexpensive, accessible LLM provers: Can strong LLM-based provers be developed without reliance on massive compute, tailored infrastructure, and private resources?To address these issues, the paper introduces LeanDojo, open datasets, and a retrieval-augmented LLM prover called ReProver. The goal is to remove barriers to research on LLMs for theorem proving by providing accessible tools, data, and models.
