# [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://arxiv.org/abs/2306.15626)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can open-source toolkits, data, models, and benchmarks facilitate research on machine learning methods for theorem proving, by removing barriers like private code/data and large compute requirements?2) Can retrieval-augmented language models effectively select relevant premises from a large mathematical library when proving new theorems? 3) Will a retrieval-augmented prover outperform non-retrieval baselines and state-of-the-art models like GPT-4 on the task of theorem proving?4) Can the proposed LeanDojo toolkit robustly extract training data from Lean and enable reliable interaction for evaluating theorem provers?5) Will the proposed LeanDojo Benchmark, with its novel data splits, provide a challenging testbed to evaluate whether provers can generalize to novel premises outside the training set?In summary, the central research questions seem to be around using open resources to facilitate ML research for theorem proving, and demonstrating the effectiveness of a retrieval-augmented prover on challenging theorem proving benchmarks extracted using the LeanDojo toolkit.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introducing open-source tools (LeanDojo) for extracting data from and interacting with the Lean proof assistant. This includes capabilities for extracting training data like proof trees and premise information. 2. Developing ReProver, the first retrieval-augmented language model for theorem proving in Lean. ReProver selects relevant premises to condition its tactic generation on, using a retriever based on Dense Passage Retriever. The retriever is tailored for premise selection by focusing on accessible premises and using in-file negative sampling.3. Constructing a new benchmark (LeanDojo Benchmark) with over 96,000 theorems/proofs extracted from Lean's math library. It features a challenging split requiring generalization to novel premises.4. Demonstrating ReProver's effectiveness on premise selection and theorem proving tasks on this benchmark. ReProver outperforms baselines like tidy and GPT-4, and shows competitive performance on existing datasets like MiniF2F and ProofNet. 5. Releasing the open-source LeanDojo toolkit, ReProver model, benchmark, and other resources to lower barriers to research on learning-based theorem proving. This establishes accessible baselines to build upon, compared to prior work relying on private data/code.In summary, the main contribution seems to be providing an open, reproducible framework and strong baseline model for learning-based theorem proving in Lean, enabled by new tools for data extraction and interaction. The retrieval-augmented ReProver model is shown to be effective on a challenging new benchmark based on Lean's math library.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for training and evaluating large language models on interactive theorem proving in Lean, and develops ReProver, a retrieval-augmented language model that shows promising results by incorporating premise selection.
