# [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://arxiv.org/abs/2306.15626)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can open-source toolkits, data, models, and benchmarks facilitate research on machine learning methods for theorem proving, by removing barriers like private code/data and large compute requirements?2) Can retrieval-augmented language models effectively select relevant premises from a large mathematical library when proving new theorems? 3) Will a retrieval-augmented prover outperform non-retrieval baselines and state-of-the-art models like GPT-4 on the task of theorem proving?4) Can the proposed LeanDojo toolkit robustly extract training data from Lean and enable reliable interaction for evaluating theorem provers?5) Will the proposed LeanDojo Benchmark, with its novel data splits, provide a challenging testbed to evaluate whether provers can generalize to novel premises outside the training set?In summary, the central research questions seem to be around using open resources to facilitate ML research for theorem proving, and demonstrating the effectiveness of a retrieval-augmented prover on challenging theorem proving benchmarks extracted using the LeanDojo toolkit.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introducing open-source tools (LeanDojo) for extracting data from and interacting with the Lean proof assistant. This includes capabilities for extracting training data like proof trees and premise information. 2. Developing ReProver, the first retrieval-augmented language model for theorem proving in Lean. ReProver selects relevant premises to condition its tactic generation on, using a retriever based on Dense Passage Retriever. The retriever is tailored for premise selection by focusing on accessible premises and using in-file negative sampling.3. Constructing a new benchmark (LeanDojo Benchmark) with over 96,000 theorems/proofs extracted from Lean's math library. It features a challenging split requiring generalization to novel premises.4. Demonstrating ReProver's effectiveness on premise selection and theorem proving tasks on this benchmark. ReProver outperforms baselines like tidy and GPT-4, and shows competitive performance on existing datasets like MiniF2F and ProofNet. 5. Releasing the open-source LeanDojo toolkit, ReProver model, benchmark, and other resources to lower barriers to research on learning-based theorem proving. This establishes accessible baselines to build upon, compared to prior work relying on private data/code.In summary, the main contribution seems to be providing an open, reproducible framework and strong baseline model for learning-based theorem proving in Lean, enabled by new tools for data extraction and interaction. The retrieval-augmented ReProver model is shown to be effective on a challenging new benchmark based on Lean's math library.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces LeanDojo, an open-source toolkit and benchmark for training and evaluating large language models on interactive theorem proving in Lean, and develops ReProver, a retrieval-augmented language model that shows promising results by incorporating premise selection.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related work in the field of theorem proving with machine learning:- It focuses on the Lean proof assistant, whereas much existing work has targeted Coq, Isabelle/HOL, or HOL Light. There has been some prior work on Lean, but this paper provides the most comprehensive tooling and benchmarks.- It introduces LeanDojo, an open-source toolkit for extracting training data and interacting with Lean. This lowers the barriers compared to prior methods that relied on private code and data.- It develops ReProver, the first retrieval-augmented language model for theorem proving. Prior language model-based provers did not incorporate explicit premise selection. - For premise selection, it proposes innovations like focusing retrieval on accessible premises and using in-file negatives. These improve substantially upon standard Dense Passage Retrieval.- It creates a new benchmark, LeanDojo Benchmark, for evaluating premise selection and theorem proving. It is larger and more challenging than prior Lean benchmarks.- ReProver achieves competitive results to prior methods while using modest compute resources (1 GPU week). In contrast, some existing provers required thousands of GPU days to train.- It demonstrates the value of retrieval augmentation and challenging data splits. The performance substantially degrades without retrieval or on the "novel_premises" split.Overall, this paper pushes forward the state-of-the-art in openness, scale, and rigor compared to related work. The public code, data, and models provide an accessible starting point for future research to build upon.
