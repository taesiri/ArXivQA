# [Cyclic Neural Network](https://arxiv.org/abs/2402.03332)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current artificial neural networks (ANNs) are structured in layers and trained using backpropagation, which has limitations in biological plausibility. 
- In contrast, biological neural networks form complex, graph-structured connections between neurons. This inspires exploring more flexible ANN architectures.

Proposed Solution:
- Introduces Cyclic Neural Networks (Cyclic NNs), a new ANN paradigm allowing flexible graph-structured connections between neurons, including cycles.
- Neurons become computational units that transform representations. Synapses propagate information between neurons.
- Uses localized learning objectives for each neuron rather than a global objective. Eliminates gradient backpropagation between neurons.
- Proposes Graph Over Multi-Layer Perceptron (GOMLP) as a first model under this paradigm for classification tasks. Constructs computation graphs over MLP neurons.

Main Contributions:
- Conceptually compares biological and ANN structures, motivating more flexible designs.
- Proposes groundbreaking Cyclic NN paradigm enabling neuron connections in any graph structure. More biologically plausible. 
- GOMLP model outperforms conventional layer-by-layer ANNs trained with backpropagation. First to beat backpropagation with localized learning.
- Cyclic NNs have advantages in flexibility, extensibility, parallelism and privacy over traditional ANNs.
- Opens possibilities for exploring new ANN architectures beyond layered acyclic graphs. Significant departure from status quo.

In summary, the paper makes a case for graph-structured Cyclic NNs over layer-stacked ANNs, with experimental results showing advantages. This represents a major shift in how ANNs can be designed and trained.


## Summarize the paper in one sentence.

 This paper proposes cyclic neural networks, a new neural network architecture with flexible neuron connections that can form cycles, enabling more effective learning compared to traditional layered networks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Cyclic Neural Networks (Cyclic NNs), a new neural network architecture that allows flexible graph-structured connections between neurons, including cycles. Specifically:

- It introduces the concepts of computational neurons and synapses to model the neurons and connections in Cyclic NNs. The neurons have stronger computational power and are optimized locally.

- It relaxes the constraint of layered DAG structure in conventional NNs, allowing neurons to connect in any graph structure. This is more flexible and biologically plausible. 

- It proposes Graph Over Multi-layer Perceptron (GOMLP) as a first detailed Cyclic NN model for validation. GOMLP outperforms conventional layer-by-layer MLPs trained with backpropagation.

- It demonstrates the advantages of Cyclic NNs such as flexibility, extensibility, parallelism, privacy protection and biological similarity.

In summary, the main contribution is proposing the novel Cyclic NN architecture that departs from traditional DAG structured NNs. This new paradigm allows more flexible neuron connections and achieves superior performance, representing a significant advancement in neural network design.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Cyclic Neural Networks (Cyclic NNs) - The novel neural network architecture proposed in the paper that allows flexible graph-structured connections between neurons, including cycles, breaking from the traditional DAG structure.

- Graph Over Multi-layer Perceptron (GOMLP) - The first detailed model proposed in the paper based on the Cyclic NN framework to validate its effectiveness.

- Computational neurons - The neurons with stronger computation power than traditional neurons in Cyclic NNs, acting as local computation/optimization units. 

- Localized learning - The training method used in Cyclic NNs where each neuron is trained based on a local objective function rather than global backpropagation.

- Forward-Forward (FF) algorithm - The localized learning algorithm used to train models like GOMLP, which differentiates between "good" and "bad" samples.

- Flexibility - A key advantage of Cyclic NNs, allowing more flexible neuron connections compared to layered DAG architectures.

- Biological plausibility - Another touted advantage, as Cyclic NNs better emulate the complex graph structure of biological neural networks.

- Performance - The paper shows Cyclic NNs can outperform traditional MLPs and beat backpropagation using only localized learning, demonstrating their promise.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the cyclic neural network method proposed in this paper:

1. The paper draws inspiration from biological neural networks to propose cyclic neural networks. How is the connectivity and information flow in cyclic neural networks more similar to biological neural networks compared to traditional sequential layer-based neural networks?

2. The paper proposes computational neurons with greater computational capacity than standard neural network neurons. What motivates this design choice and what types of computations can these neurons perform? 

3. The cyclic neural network is trained using a localized training method without backpropagation. How does this localized training work and why does it enable cyclic connections? What are the advantages and disadvantages compared to backpropagation?

4. What is the role of the readout layer in the proposed cyclic neural network architecture? How does it complement the computations happening in the computational neurons?

5. The paper validates the cyclic neural network on the Graph Over Multi-Layer Perceptron (GOMLP) model. Can you explain the design choices made in GOMLP including the input construction, graph structure, and optimization functions? 

6. How does the paper construct positive, negative and neutral inputs to enable localized training of the GOMLP model? What is the intuition behind this input construction?

7. The paper experiments with different graph generators like WS graphs and BA graphs to construct the model architecture. How do properties like clustering coefficients and shortest path lengths affect model performance?

8. What are the advantages of cyclic neural networks mentioned in the paper compared to traditional sequential networks? Can you expand on notions like flexibility, extensibility and biological plausibility?

9. The paper shows cyclic networks can outperform backpropagation-trained networks. What are the reasons that enable this? What benchmarks and datasets are used to demonstrate this?

10. The paper opens up an entirely new paradigm of neural network design. What are promising future directions for research on cyclic and more flexible neural architectures inspired by this work?
