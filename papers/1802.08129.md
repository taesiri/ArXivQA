# [Multimodal Explanations: Justifying Decisions and Pointing to the   Evidence](https://arxiv.org/abs/1802.08129)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop deep learning models that can provide multimodal explanations, consisting of both visual pointing and textual justifications, for their decisions. The key hypotheses are:1) Training a model to generate textual explanations of its decisions will improve its ability to visually point to the key evidence, compared to just training it to make decisions. 2) Training a model to visually point to evidence will improve its ability to generate high quality textual explanations, compared to just training it to generate explanations without visual grounding.3) Multimodal explanation models that generate both textual and visual explanations will outperform unimodal models that generate only one type of explanation.4) Multimodal explanations provide complementary benefits - sometimes the visual explanation provides more insight than the textual one, and vice versa.So in summary, the main research question is how to develop deep learning models that can provide these multimodal explanations, and the key hypotheses are that training with multimodal explanations will improve both modalities, and that multimodal explanations have advantages over unimodal ones. The paper aims to demonstrate these hypotheses through new multimodal explanation datasets, a novel multimodal explanation model architecture, and experiments analyzing the quality of both textual and visual explanation modalities.


## What is the main contribution of this paper?

 The main contributions of this paper are:1. Proposing a novel multimodal explanation model called Pointing and Justification (PJ-X) that can generate both textual justifications and visual pointing to evidence for its decisions. 2. Collecting two new datasets for multimodal explanation - VQA-X for visual question answering and ACT-X for activity recognition. These contain textual and visual explanations from humans.3. Showing quantitatively that the multimodal PJ-X model generates better textual justifications and visual pointing when trained on the multimodal explanation datasets, compared to unimodal baselines.4. Demonstrating qualitatively that the PJ-X model generates high quality textual justifications and accurately points to visual evidence to support its decisions. 5. Highlighting the complementary nature of textual and visual explanations through examples where visual pointing is more insightful than text or vice versa.In summary, the key contribution is proposing the idea of multimodal explanations and showing its benefits over unimodal explanation models, enabled by collecting new multimodal explanation datasets and developing a novel model PJ-X that can generate joint textual and visual explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a multimodal explanation system called Pointing and Justification Model (PJ-X) that can provide natural language justifications for its decisions as well as visually point to the evidence in the image to support its predictions.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on explainable AI:- The main contribution is a multimodal explanation model that can provide both textual justifications and visual pointing to evidence. This is novel compared to prior work that focused on either textual or visual explanations alone. - The authors collected two new datasets (VQA-X and ACT-X) with human annotations of text and visual explanations. This provides valuable training and evaluation data, as most prior work lacked human ground truth explanations.- For textual explanations, the authors show their model outperforms a state-of-the-art method by Hendricks et al. on automatic metrics and human evaluation. The key differences are using explanations vs descriptions for training, and incorporating visual pointing.- For visual pointing, they demonstrate improved localization over baseline attention methods by leveraging the textual explanations for supervision. This shows the benefits of multimodal learning.- Qualitative results illustrate cases where visual or textual explanations alone are insufficient, demonstrating the value of multimodal explanations.- The tasks of VQA and activity recognition are well-motivated testbeds for studying explanations, as they require reasoning about vision and language.Overall, this work makes solid contributions over prior work by proposing multimodal explanations, collecting new datasets, developing a novel model architecture and training approach, and extensively evaluating the results. The experiments highlight the benefits of jointly learning to point and explain, rather than addressing each modality independently.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:- Developing a better understanding of when visual explanation is more insightful than textual explanation, and vice versa. The authors showed some examples where one modality was more useful for explanation than the other, but suggest more research is needed to fully characterize the strengths and weaknesses of each modality.- Exploring other multimodal explanation models and architectures beyond the PJ-X model proposed in the paper. The authors propose PJ-X as an initial model for multimodal explanation, but suggest exploring other approaches as well.- Applying multimodal explanation techniques to additional domains and tasks beyond VQA and activity recognition. The authors focused on VQA and activity recognition as initial testbeds, but suggest multimodal explanations could be useful more broadly.- Collecting additional multimodal explanation datasets to support research. The authors released VQA-X and ACT-X datasets, but suggest that more data is needed, especially in other domains.- Studying how well multimodal explanations support tasks like debugging model mistakes or conveying understanding to human users. The authors propose these as potentially useful applications of multimodal explanations.- Investigating how multimodal explanations could improve model training, for example through self-supervised explanation objectives. The authors suggest that generating explanations during training could improve model performance.In summary, the main directions are exploring the modalities and architectures for multimodal explanation, applying it to new domains and tasks, collecting more multimodal explanation data, and studying how multimodal explanations can improve understanding and training of AI systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary:The paper proposes a multimodal explanation system called Pointing and Justification Model (PJ-X) that can provide natural language justifications and visually point to evidence to explain its decisions. The authors collect two new datasets, VQA-X and ACT-X, with human annotated textual and visual explanations for visual question answering and activity recognition tasks, in order to train and evaluate the model. PJ-X first predicts an answer to a question or classifies an activity. Then it generates a textual justification conditioned on the question, image, and predicted answer using an LSTM decoder. It also generates a visual explanation by producing an attention heatmap over the image. Experiments show that training the textual justification generation with visual pointing signals and vice versa improves both modalities, demonstrating the benefit of multimodal explanations. Quantitative and qualitative results on the two datasets show the model generates better textual and visual explanations compared to baselines.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:Paragraph 1: This paper proposes a multimodal approach to generating explanations for deep learning models. The authors argue that providing both visual and textual explanations can give complementary strengths compared to only textual or visual explanations alone. They collect two new datasets, VQA-X and ACT-X, which contain human-annotated textual and visual explanations for activity recognition and visual question answering tasks. These datasets allow them to train a novel model called Pointing and Justification Explanation (PJ-X), which provides textual justifications and also localizes evidence in the image via attention. The textual and visual explanation tasks are trained jointly in a multitask setting.Paragraph 2: Experiments demonstrate benefits of the multimodal explanation approach. Quantitative results show that training the textual explanation task helps improve visual localization performance, and vice versa - incorporating visual pointing supervision helps generate better textual explanations compared to baselines. Qualitative examples illustrate cases where visual or textual explanations alone fail to provide full insight, but combining both modalities gives a clearer understanding. The multimodal explanations are also shown to help humans better diagnose model failures compared to unimodal explanations. Overall, the PJ-X model and datasets advance the capability of explanation systems to provide richer and more interpretable rationales for model decisions.


## Summarize the main method used in the paper in one paragraph.

 The main method presented in this paper is the Pointing and Justification Explanation (PJ-X) model for generating multimodal explanations for visual question answering and activity recognition tasks. The key ideas are:- The model has two components: an answering module that predicts the answer to a visual question or recognizes the activity in an image, and an explanation module that generates visual pointing and textual justification conditioned on the predicted answer.- For visual pointing, the model uses an attention mechanism to focus on relevant regions of the image that provide evidence for the predicted answer/activity. - For textual justification, the model uses an LSTM decoder conditioned on multimodal features from the image, question, and predicted answer to generate an explanatory caption describing the evidence.- The model is trained end-to-end, using the textual explanations as supervision to learn to point to visual evidence, without requiring explicit localization labels.- Quantitative and qualitative results on two new explanation datasets (VQA-X and ACT-X) show the model can generate coherent multimodal explanations and that training with textual explanations improves visual localization compared to just training for the answering task.In summary, the key contribution is a multimodal explanation model that can point to and describe the evidence supporting its predictions, trained using natural language explanations as supervision. The results support the benefit of multimodal explanations over unimodal approaches.
