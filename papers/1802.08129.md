# Multimodal Explanations: Justifying Decisions and Pointing to the   Evidence

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop deep learning models that can provide multimodal explanations, consisting of both visual pointing and textual justifications, for their decisions. The key hypotheses are:1) Training a model to generate textual explanations of its decisions will improve its ability to visually point to the key evidence, compared to just training it to make decisions. 2) Training a model to visually point to evidence will improve its ability to generate high quality textual explanations, compared to just training it to generate explanations without visual grounding.3) Multimodal explanation models that generate both textual and visual explanations will outperform unimodal models that generate only one type of explanation.4) Multimodal explanations provide complementary benefits - sometimes the visual explanation provides more insight than the textual one, and vice versa.So in summary, the main research question is how to develop deep learning models that can provide these multimodal explanations, and the key hypotheses are that training with multimodal explanations will improve both modalities, and that multimodal explanations have advantages over unimodal ones. The paper aims to demonstrate these hypotheses through new multimodal explanation datasets, a novel multimodal explanation model architecture, and experiments analyzing the quality of both textual and visual explanation modalities.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a novel multimodal explanation model called Pointing and Justification (PJ-X) that can generate both textual justifications and visual pointing to evidence for its decisions. 2. Collecting two new datasets for multimodal explanation - VQA-X for visual question answering and ACT-X for activity recognition. These contain textual and visual explanations from humans.3. Showing quantitatively that the multimodal PJ-X model generates better textual justifications and visual pointing when trained on the multimodal explanation datasets, compared to unimodal baselines.4. Demonstrating qualitatively that the PJ-X model generates high quality textual justifications and accurately points to visual evidence to support its decisions. 5. Highlighting the complementary nature of textual and visual explanations through examples where visual pointing is more insightful than text or vice versa.In summary, the key contribution is proposing the idea of multimodal explanations and showing its benefits over unimodal explanation models, enabled by collecting new multimodal explanation datasets and developing a novel model PJ-X that can generate joint textual and visual explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a multimodal explanation system called Pointing and Justification Model (PJ-X) that can provide natural language justifications for its decisions as well as visually point to the evidence in the image to support its predictions.
