# [Multimodal Explanations: Justifying Decisions and Pointing to the   Evidence](https://arxiv.org/abs/1802.08129)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop deep learning models that can provide multimodal explanations, consisting of both visual pointing and textual justifications, for their decisions. 

The key hypotheses are:

1) Training a model to generate textual explanations of its decisions will improve its ability to visually point to the key evidence, compared to just training it to make decisions. 

2) Training a model to visually point to evidence will improve its ability to generate high quality textual explanations, compared to just training it to generate explanations without visual grounding.

3) Multimodal explanation models that generate both textual and visual explanations will outperform unimodal models that generate only one type of explanation.

4) Multimodal explanations provide complementary benefits - sometimes the visual explanation provides more insight than the textual one, and vice versa.

So in summary, the main research question is how to develop deep learning models that can provide these multimodal explanations, and the key hypotheses are that training with multimodal explanations will improve both modalities, and that multimodal explanations have advantages over unimodal ones. The paper aims to demonstrate these hypotheses through new multimodal explanation datasets, a novel multimodal explanation model architecture, and experiments analyzing the quality of both textual and visual explanation modalities.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a novel multimodal explanation model called Pointing and Justification (PJ-X) that can generate both textual justifications and visual pointing to evidence for its decisions. 

2. Collecting two new datasets for multimodal explanation - VQA-X for visual question answering and ACT-X for activity recognition. These contain textual and visual explanations from humans.

3. Showing quantitatively that the multimodal PJ-X model generates better textual justifications and visual pointing when trained on the multimodal explanation datasets, compared to unimodal baselines.

4. Demonstrating qualitatively that the PJ-X model generates high quality textual justifications and accurately points to visual evidence to support its decisions. 

5. Highlighting the complementary nature of textual and visual explanations through examples where visual pointing is more insightful than text or vice versa.

In summary, the key contribution is proposing the idea of multimodal explanations and showing its benefits over unimodal explanation models, enabled by collecting new multimodal explanation datasets and developing a novel model PJ-X that can generate joint textual and visual explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a multimodal explanation system called Pointing and Justification Model (PJ-X) that can provide natural language justifications for its decisions as well as visually point to the evidence in the image to support its predictions.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on explainable AI:

- The main contribution is a multimodal explanation model that can provide both textual justifications and visual pointing to evidence. This is novel compared to prior work that focused on either textual or visual explanations alone. 

- The authors collected two new datasets (VQA-X and ACT-X) with human annotations of text and visual explanations. This provides valuable training and evaluation data, as most prior work lacked human ground truth explanations.

- For textual explanations, the authors show their model outperforms a state-of-the-art method by Hendricks et al. on automatic metrics and human evaluation. The key differences are using explanations vs descriptions for training, and incorporating visual pointing.

- For visual pointing, they demonstrate improved localization over baseline attention methods by leveraging the textual explanations for supervision. This shows the benefits of multimodal learning.

- Qualitative results illustrate cases where visual or textual explanations alone are insufficient, demonstrating the value of multimodal explanations.

- The tasks of VQA and activity recognition are well-motivated testbeds for studying explanations, as they require reasoning about vision and language.

Overall, this work makes solid contributions over prior work by proposing multimodal explanations, collecting new datasets, developing a novel model architecture and training approach, and extensively evaluating the results. The experiments highlight the benefits of jointly learning to point and explain, rather than addressing each modality independently.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Developing a better understanding of when visual explanation is more insightful than textual explanation, and vice versa. The authors showed some examples where one modality was more useful for explanation than the other, but suggest more research is needed to fully characterize the strengths and weaknesses of each modality.

- Exploring other multimodal explanation models and architectures beyond the PJ-X model proposed in the paper. The authors propose PJ-X as an initial model for multimodal explanation, but suggest exploring other approaches as well.

- Applying multimodal explanation techniques to additional domains and tasks beyond VQA and activity recognition. The authors focused on VQA and activity recognition as initial testbeds, but suggest multimodal explanations could be useful more broadly.

- Collecting additional multimodal explanation datasets to support research. The authors released VQA-X and ACT-X datasets, but suggest that more data is needed, especially in other domains.

- Studying how well multimodal explanations support tasks like debugging model mistakes or conveying understanding to human users. The authors propose these as potentially useful applications of multimodal explanations.

- Investigating how multimodal explanations could improve model training, for example through self-supervised explanation objectives. The authors suggest that generating explanations during training could improve model performance.

In summary, the main directions are exploring the modalities and architectures for multimodal explanation, applying it to new domains and tasks, collecting more multimodal explanation data, and studying how multimodal explanations can improve understanding and training of AI systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary:

The paper proposes a multimodal explanation system called Pointing and Justification Model (PJ-X) that can provide natural language justifications and visually point to evidence to explain its decisions. The authors collect two new datasets, VQA-X and ACT-X, with human annotated textual and visual explanations for visual question answering and activity recognition tasks, in order to train and evaluate the model. PJ-X first predicts an answer to a question or classifies an activity. Then it generates a textual justification conditioned on the question, image, and predicted answer using an LSTM decoder. It also generates a visual explanation by producing an attention heatmap over the image. Experiments show that training the textual justification generation with visual pointing signals and vice versa improves both modalities, demonstrating the benefit of multimodal explanations. Quantitative and qualitative results on the two datasets show the model generates better textual and visual explanations compared to baselines.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

Paragraph 1: This paper proposes a multimodal approach to generating explanations for deep learning models. The authors argue that providing both visual and textual explanations can give complementary strengths compared to only textual or visual explanations alone. They collect two new datasets, VQA-X and ACT-X, which contain human-annotated textual and visual explanations for activity recognition and visual question answering tasks. These datasets allow them to train a novel model called Pointing and Justification Explanation (PJ-X), which provides textual justifications and also localizes evidence in the image via attention. The textual and visual explanation tasks are trained jointly in a multitask setting.

Paragraph 2: Experiments demonstrate benefits of the multimodal explanation approach. Quantitative results show that training the textual explanation task helps improve visual localization performance, and vice versa - incorporating visual pointing supervision helps generate better textual explanations compared to baselines. Qualitative examples illustrate cases where visual or textual explanations alone fail to provide full insight, but combining both modalities gives a clearer understanding. The multimodal explanations are also shown to help humans better diagnose model failures compared to unimodal explanations. Overall, the PJ-X model and datasets advance the capability of explanation systems to provide richer and more interpretable rationales for model decisions.


## Summarize the main method used in the paper in one paragraph.

 The main method presented in this paper is the Pointing and Justification Explanation (PJ-X) model for generating multimodal explanations for visual question answering and activity recognition tasks. 

The key ideas are:

- The model has two components: an answering module that predicts the answer to a visual question or recognizes the activity in an image, and an explanation module that generates visual pointing and textual justification conditioned on the predicted answer.

- For visual pointing, the model uses an attention mechanism to focus on relevant regions of the image that provide evidence for the predicted answer/activity. 

- For textual justification, the model uses an LSTM decoder conditioned on multimodal features from the image, question, and predicted answer to generate an explanatory caption describing the evidence.

- The model is trained end-to-end, using the textual explanations as supervision to learn to point to visual evidence, without requiring explicit localization labels.

- Quantitative and qualitative results on two new explanation datasets (VQA-X and ACT-X) show the model can generate coherent multimodal explanations and that training with textual explanations improves visual localization compared to just training for the answering task.

In summary, the key contribution is a multimodal explanation model that can point to and describe the evidence supporting its predictions, trained using natural language explanations as supervision. The results support the benefit of multimodal explanations over unimodal approaches.


## What problem or question is the paper addressing?

 The paper is presenting a new model called Pointing and Justification Explanation (PJ-X) that aims to provide multimodal explanations for decisions made by AI systems. The key problems/questions it addresses are:

- How to build AI systems that can provide natural language justifications to explain their decisions, in addition to visualizing the evidence via attention maps.

- How to collect datasets with human explanations as supervision to train and evaluate such multimodal explanation models. 

- Whether generating multimodal explanations (textual + visual) leads to better explanation capabilities compared to unimodal approaches.

Specifically, the paper introduces two new datasets collected via crowdsourcing - VQA-X for visual question answering explanations, and ACT-X for activity recognition explanations. It then proposes the PJ-X model that generates textual justifications conditioned on visual attention over the image regions relevant to the decision. The main thesis is that multimodal explanation generation helps improve both textual and visual explanation capabilities. The paper presents quantitative and qualitative results to demonstrate the advantages of PJ-X over unimodal baselines.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Multimodal explanations - The paper proposes generating explanations with both natural language text and visual pointing/highlighting of image regions. This multimodality allows the model to justify decisions and point to visual evidence.

- Visual question answering (VQA) - One of the two testbed tasks used in the paper, along with activity recognition. For VQA, the model must answer a question about an image.

- Activity recognition - The second testbed task, where the model must classify the activity being performed in a still image. 

- Attention/pointing - The visual explanation component uses an attention mechanism to point to and highlight the visual evidence supporting the decision.

- VQA-X dataset - One of the two new explanation datasets collected by the authors, containing visual and textual explanations for VQA samples.

- ACT-X dataset - The other new dataset collected, containing explanations for activity recognition images.

- Pointing and Justification Model (PJ-X) - The multimodal explanation model proposed in the paper, which generates textual justifications and visual pointing/attention.

- Quantitative evaluation - The paper includes quantitative experiments analyzing the textual and visual explanation performance.

- Qualitative results - Example multimodal explanations are shown for VQA and activity recognition images.

- Complementary modalities - Analysis showing cases where visual or textual explanations are more insightful than the other, highlighting the complementary value.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the goal of the proposed Pointing and Justification Model (PJ-X)?

2. How does the PJ-X model generate multimodal explanations with visual pointing and textual justifications?

3. What two new datasets were collected to define and evaluate this task, and what do they contain? 

4. How does the PJ-X model architecture work for answering and explanation generation?

5. What metrics were used to evaluate the textual and visual explanations?

6. How did the PJ-X model perform compared to baselines and ablation models on generating textual explanations?

7. How did visual pointing compare between the PJ-X model and baselines using quantitative metrics? 

8. What are some examples of textual justifications generated by the model for VQA and activity recognition?

9. What are some examples of visual pointing generated by the model and what do they highlight?

10. What are the main conclusions and contributions of the work?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel Pointing and Justification (PJ-X) model for multimodal explanation generation. Can you explain in more detail how the visual pointing and textual justification modules work together in this model architecture? How are the image, question, and answer representations combined to generate the explanation outputs?

2. The paper introduces two new explanation datasets, VQA-X and ACT-X. What motivated the authors to create these datasets? How do the textual and visual explanations compare to existing VQA datasets like VQA-HAT? What are the key differences in collecting explanations versus descriptions?

3. The paper shows that training with textual explanations helps improve visual pointing performance. Why does learning to generate text explanations latently improve visual localization? What is the intuition behind this cross-modal transfer of supervision?

4. How exactly does the PJ-X model generate the textual justifications? Walk through the LSTM decoder process and how the attended visual features, question encoding, and answer embedding are combined and fed into the decoder at each timestep.

5. The results show that multimodal explanation generation outperforms unimodal approaches. Why is jointly producing visual and textual rationales better than generating either modality alone? In what ways do the textual and visual explanations complement each other?

6. The paper evaluates textual justifications using both automatic metrics like BLEU, METEOR, etc. and human evaluation. What are the relative pros and cons of automatic versus human metrics for this task? When would you prefer one evaluation approach over the other?

7. For visual pointing evaluation, the paper uses both Earth Mover's Distance and Rank Correlation metrics. What does each of these metrics capture about the quality of the attention map? When would one metric be more appropriate than the other? 

8. In the qualitative results, what kinds of visual and textual justifications does the model produce? How interpretable and meaningful are these explanations? Do they capture the right reasoning for the VQA and activity recognition tasks?

9. The paper shows cases where visual explanation is more insightful than textual, and vice versa. Can you expand on when each modality provides better justification, and why multimodal explanation has an advantage?

10. How suitable is the PJ-X model for real-world deployment? What are some ways the model could be improved or extended? How might the multimodal explanation approach generalize to other vision and language tasks?
