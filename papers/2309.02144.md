# [Making Large Language Models Better Reasoners with Alignment](https://arxiv.org/abs/2309.02144)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper aims to address is: How can we improve the reasoning abilities of large language models (LLMs) by addressing the assessment misalignment problem caused by traditional fine-tuning methods?Specifically, the key hypotheses are:1) Traditional fine-tuning methods like maximum likelihood estimation cause LLMs to suffer from an "assessment misalignment" problem, where they struggle to properly assess the quality of different reasoning chains of thought (COTs). This hinders their reasoning abilities.2) By using an alignment fine-tuning (AFT) approach with a novel constraint alignment loss, we can calibrate the scores LLMs assign to positive and negative COTs. This helps address the assessment misalignment issue and improves LLMs' reasoning abilities. 3) Explicitly constraining the decrease of scores for negative examples is crucial for alignment losses to prevent model degradation. Prior ranking-based alignment methods overlook this constraint.So in summary, the central research aim is to enhance LLMs' reasoning skills by tackling the assessment misalignment problem using the proposed alignment fine-tuning technique with constraints. The key hypothesis is that this approach will calibrate LLM scoring and lead to better reasoning.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It identifies an "assessment misalignment" problem with vanilla fine-tuned large language models (LLMs), where the LLMs struggle to properly assess the quality of different reasoning chains of thought (COTs) for a given question. 2. It proposes a new "alignment fine-tuning" (AFT) paradigm to address this problem and improve LLM reasoning abilities. The key steps of AFT are:(a) Fine-tune the LLM on COT training data. (b) Generate multiple COT responses for each question using the fine-tuned LLM. Categorize them as positive/negative based on whether they deduce the correct answer.(c) Calibrate the scores of positive and negative responses using a novel "constraint alignment" loss function. This aligns model assessments with ground truth quality assessments.3. The constraint alignment loss contains two key components:(a) An alignment term that ensures positive scores are higher than negative scores. (b) A constraint term that prevents excessive reduction of negative scores to avoid model degradation.4. The AFT paradigm and constraint alignment loss are shown to improve reasoning performance over vanilla fine-tuning baselines on multiple benchmarks.5. The paper also analyzes limitations of prior ranking-based alignment methods, and shows the importance of the constraint term for achieving good performance.In summary, the main contribution is proposing AFT with constraint alignment as an effective technique to improve LLM reasoning abilities by addressing assessment misalignments caused by traditional fine-tuning approaches.
