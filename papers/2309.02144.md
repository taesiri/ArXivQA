# [Making Large Language Models Better Reasoners with Alignment](https://arxiv.org/abs/2309.02144)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper aims to address is: How can we improve the reasoning abilities of large language models (LLMs) by addressing the assessment misalignment problem caused by traditional fine-tuning methods?Specifically, the key hypotheses are:1) Traditional fine-tuning methods like maximum likelihood estimation cause LLMs to suffer from an "assessment misalignment" problem, where they struggle to properly assess the quality of different reasoning chains of thought (COTs). This hinders their reasoning abilities.2) By using an alignment fine-tuning (AFT) approach with a novel constraint alignment loss, we can calibrate the scores LLMs assign to positive and negative COTs. This helps address the assessment misalignment issue and improves LLMs' reasoning abilities. 3) Explicitly constraining the decrease of scores for negative examples is crucial for alignment losses to prevent model degradation. Prior ranking-based alignment methods overlook this constraint.So in summary, the central research aim is to enhance LLMs' reasoning skills by tackling the assessment misalignment problem using the proposed alignment fine-tuning technique with constraints. The key hypothesis is that this approach will calibrate LLM scoring and lead to better reasoning.
