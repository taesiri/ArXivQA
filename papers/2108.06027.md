# [PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense   Passage Retrieval](https://arxiv.org/abs/2108.06027)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a novel approach for dense passage retrieval that leverages both query-centric and passage-centric similarity relations. The central research hypothesis is that considering passage-centric similarity relations in addition to query-centric similarity relations can improve the performance of dense passage retrieval. The key hypotheses and contributions are:- Simultaneously learning query-centric and passage-centric similarity relations can improve dense passage retrieval performance. This is the first work to consider passage-centric similarity for this task.- Formally defining query-centric and passage-centric losses to characterize the two kinds of similarity relations. - Generating high-quality pseudo-labeled training data via knowledge distillation from a cross-encoder teacher model. This allows capturing more comprehensive similarity relations.- A two-stage training procedure that incorporates passage-centric loss during pre-training and then fine-tunes only on query-centric loss. This is an effective way to leverage passage-centric similarity while still focusing on the end task.- Extensive experiments validate the effectiveness of the proposed approach, significantly outperforming previous state-of-the-art models on MSMARCO and Natural Questions datasets.In summary, the central hypothesis is that modeling passage-centric similarity in addition to query-centric similarity is beneficial for dense passage retrieval, and the paper makes several technical contributions to effectively implement this idea. The experimental results strongly confirm the value of the proposed approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing an approach that simultaneously learns query-centric and passage-centric similarity relations for dense passage retrieval. Specifically, the key contributions are:- It is the first work to incorporate passage-centric similarity relation, in addition to the commonly used query-centric similarity relation, for dense passage retrieval. - The paper makes three main technical contributions to implement the idea:   1) Introducing formal formulations for the two kinds of similarity relations.   2) Generating high-quality pseudo-labeled data via knowledge distillation to learn the relations.   3) Designing a two-stage training procedure that first pre-trains with both losses, and then fine-tunes with the query-centric loss.- The proposed approach called PAIR (Passage-centric and Query-centric Similarity Relations) significantly outperforms previous state-of-the-art models on MSMARCO and Natural Questions datasets.In summary, the key contribution is proposing the idea of jointly modeling query-centric and passage-centric similarity relations for improving dense passage retrieval, and providing techniques to effectively implement this idea. The results demonstrate the effectiveness of PAIR over strong baselines.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new approach called PAIR that improves dense passage retrieval by simultaneously learning query-centric and passage-centric similarity relations using formal loss functions, pseudo-labeled data generation via knowledge distillation, and a two-stage training procedure.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in dense passage retrieval:- Overall, this paper introduces a novel approach of incorporating passage-centric similarity in addition to query-centric similarity for training dense passage retrievers. Considering similarity between passages is a unique contribution that has not been explored before in prior work.- Most prior work has focused only on query-passage similarity, like DPR, RocketQA, ANCE, etc. So this paper presents a new direction.- For query-passage similarity modeling, this paper uses a similar dual-encoder framework like DPR, RocketQA. The main difference is the additional passage-centric loss term.- For training data augmentation, the paper uses knowledge distillation like RocketQA to create pseudo-labels. The core training methodology is similar.- The two-stage training procedure is unique to this paper. Pre-training with both losses and then fine-tuning only on query loss is novel.- Compared to REALM and Latent Retrieval, this paper doesn't use a completely unsupervised pre-training approach. The pre-training here is on pseudo-labeled data.- So in summary, the core novelty is the modeling of passage-passage relations and the two-stage training. The base retrieval framework is similar to existing methods like DPR and RocketQA. The knowledge distillation data augmentation also follows RocketQA.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Designing more principled ranking functions and loss formulations to better capture query-passage relevance. The authors propose incorporating passage-centric similarity, but suggest there may be room for improvement in the formal modeling. - Applying the dense passage retrieval approach to downstream tasks like question answering and passage re-ranking. The authors show the effectiveness for retrieval, but suggest it could also help in end task performance.- Exploring other ways to generate high-quality training data, beyond just the knowledge distillation method proposed. Getting more and better training data seems key.- Extending the passage-centric similarity idea to other retrieval methods beyond just dual encoders. The concept could potentially transfer more broadly.- Doing further analysis to better understand the learned representations and effectiveness on different types of queries. More interpretability work could help guide improvements.- Exploring different encoding architectures beyond the standard approaches used. The dual encoder structure may not be optimal.So in summary, the main directions seem to be 1) improving the ranking functions, 2) applying to downstream tasks, 3) getting more/better training data, 4) extending the idea to other models, 5) analyzing representations more, and 6) exploring different encodings. Overall the authors propose some promising ideas but suggest lots of room for future work in this area.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes an approach called PAIR that leverages both query-centric and passage-centric similarity relations for improving dense passage retrieval. The key idea is to incorporate passage-centric similarity constraints during training, such as enforcing the similarity between a query and a relevant passage to be higher than the similarity between the relevant passage and an irrelevant passage. To enable learning these additional passage-centric relations, the authors make three main technical contributions: (1) formalizing passage-centric losses, (2) generating high-quality pseudo-labeled training data via knowledge distillation from a cross-encoder teacher model, and (3) a two-stage training procedure that pre-trains with both query-centric and passage-centric losses before fine-tuning on the query-centric loss alone. Experiments on MSMARCO and Natural Questions datasets show state-of-the-art results, demonstrating the benefits of modeling passage-centric relations in addition to query-centric relations for dense retrieval.
