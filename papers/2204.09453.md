# Event Transition Planning for Open-ended Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can explicit modeling of event transitions improve the coherence and diversity of open-ended text generation?The key hypotheses appear to be:1) Explicitly modeling event transitions with a generative event transition planner can help arrange causality and logical flow in open-ended text generation tasks like dialogue and story completion. 2) Incorporating the generated event transition paths into the text generation process via an event query layer can guide the model to produce more coherent and diverse continuations.3) The proposed two-stage coarse-to-fine approach of first generating event transitions and then generating text conditioned on those transitions can outperform standard fine-tuning of pretrained language models that do not explicitly model event transitions.The experiments seem designed to validate these hypotheses by comparing the proposed two-stage model against strong autoregressive baselines on dialogue and story completion tasks using both automatic metrics and human evaluation. The overall goal is to show that explicitly modeling event transitions in this way improves open-ended text generation quality.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a two-stage method for improving coherence and diversity in open-ended text generation tasks like story completion and dialogue generation. The key ideas are:1. Using an event transition planner in the first stage to explicitly model the arrangement of ensuing events given limited context. This planner is trained on event transition sequences from commonsense knowledge graphs as well as task training data.2. Integrating the planned event transition path into the text generation model via a specialized event query layer. This allows the generator to attend to the planned path when producing the output text. 3. Evaluating the approach on story completion using ROCStories and dialogue generation using EmpatheticDialogues. Results show improvements over strong baselines in both automatic metrics and human evaluation, especially for long and difficult generation cases.In summary, the main contribution is a planning-based coarse-to-fine approach to improve high-level coherence and diversity in open-ended text generation. The key innovation is the integration of an inductive bias towards organizing event transitions through specialized training and architecture design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a two-stage method for open-ended text generation tasks like dialogue and story completion, where an event transition planner first outlines a skeleton of events which is then used by a text generator to produce a coherent continuation consistent with the input context.


## How does this paper compare to other research in the same field?

Based on my analysis, here is a summary of how this paper compares to other related work:- This paper proposes a two-stage method for open-ended text generation that explicitly models event transitions using a separate event planner and path-aware text generator. This is a novel approach compared to most prior work that relies solely on end-to-end fine-tuning of large pre-trained language models like GPT-2. - The idea of explicitly planning event sequences is related to some prior work on coarse-to-fine generation, but differs in using a trainable generative model rather than inflexible pattern retrieval for the planning stage.- Incorporating external knowledge into text generation has been explored before, but this paper takes a unique approach of pre-training the event planner on commonsense reasoning paths from ATOMIC before fine-tuning on task data. This aims to provide more general event transition knowledge.- Compared to prior work augmenting pre-trained LMs with commonsense knowledge via continued pre-training on triples or graph structures, this paper injects event path supervision directly into the generator via a novel event query layer.- The results demonstrate improved performance over strong LM baselines on dialogue and story generation tasks, especially for longer and more difficult generation cases. This highlights the benefits of explicit planning for high-level coherence.In summary, the key novelties are the two-stage generative planning approach, use of commonsense reasoning paths, and path-aware generator with event query layer. The paper shows these techniques can improve over standard pre-trained LM fine-tuning for open-ended generation tasks where coherence over multiple sentences is crucial.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Develop better methods for aligning the generated event transition path with the final text output. As noted in the analysis, sometimes the model generates text that does not convey the information in the planned event path well. Improving the integration between the planned path and the text generator could lead to better coherence.- Explore different formats and representations for the event transition path. The authors suggest that finding a path format that is easier for the model to learn could improve performance.- Improve relation modeling between events and sentences. Better capturing the connections between the high-level events and the generated text may reduce cases of event transition hallucination.- Apply the proposed two-stage method to other open-ended text generation tasks like commonsense question answering. The authors propose evaluating whether explicit event transition planning helps with other kinds of open-ended generation problems.- Scale up the models and training data. As with many deep learning methods, using larger models trained on more data could potentially improve the quality further.In summary, the main directions are: improving the alignment between planning and generation, finding better event path representations, enhancing relation modeling, applying the approach to other tasks, and scaling up the models and data. The key is further strengthening the integration between the high-level event planning and the surface text generation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a two-stage method for open-ended text generation tasks like dialogue generation and story completion. In the first stage, an event transition planner is used to generate a sequence of events that could follow from the input context. This is done by fine-tuning a GPT-2 model on event sequences from ATOMIC and the training data. In the second stage, another GPT-2 model generates the actual text while attending to the input context and planned event sequence through an event query layer. Experiments on dialogue generation and story completion tasks show improvements over strong baselines, especially for longer and more difficult generations. The main advantages are in high-level coherence and diversity. The explicit modeling of event transitions helps arrange causalities and maintain logical consistency. This coarse-to-fine approach of planning event sequences before generating text is shown to be more effective than just enhancing the generator with commonsense knowledge.
