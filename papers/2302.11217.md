# [Connecting Vision and Language with Video Localized Narratives](https://arxiv.org/abs/2302.11217)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we create high-quality multimodal video annotations that connect vision and language by having annotators provide detailed spoken narrations and mouse traces on keyframes to capture complex events with multiple interacting actors?

The key hypothesis appears to be that a new annotation protocol based on disentangling narrations for individual actors on selected keyframes, along with careful instructions to annotators, can result in rich, accurate, and dense spatio-temporal groundings for video narrations. 

The authors then demonstrate the value of these Video Localized Narrative annotations by using them to generate benchmarks for video narrative grounding and video question answering tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of Video Localized Narratives (VidLNs), a new form of multimodal video annotation that connects vision and language. The key ideas are:

- They introduce a new annotation protocol that allows annotators to tell the story of a video by speaking narrations while moving the mouse to localize objects, actions, and attributes on selected keyframes. This results in long, detailed narrations grounded to spatio-temporal locations. 

- The protocol disentangles complex events involving multiple actors by having separate narrations from the point of view of each actor. This enables cleanly capturing interactions between multiple actors and objects.

- They use this protocol to annotate 20k videos from 3 datasets (OVIS, UVO, Oops) with over 1.7 million words, creating the largest and richest collection of general-domain video datasets with dense grounded narrations.

- They demonstrate the usefulness of VidLNs by constructing new benchmarks for Video Narrative Grounding and Video Question Answering, and provide baseline methods.

In summary, the main contribution is proposing the VidLN annotation procedure and using it to create a large-scale dataset of videos paired with grounded, detailed narrations describing the events, which enables new video-language tasks to be studied.
