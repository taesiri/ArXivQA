# Unlimiformer: Long-Range Transformers with Unlimited Length Input

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to extend pretrained transformer models to handle input sequences of effectively unlimited length for text generation tasks. The standard transformer architecture is limited to a predefined maximum input length, due to its quadratic self-attention mechanism that attends over all input tokens. This poses a challenge for tasks like long-document summarization where the input texts can easily exceed the typical 512-1024 token limit. The paper proposes a method called Unlimiformer that allows injecting retrieval-augmented attention into any pretrained encoder-decoder transformer, to attend only to a small set of top-k retrieved keys from the full input instead of all tokens. This removes the hardcoded length limit and enables processing arbitrary length sequences.The main hypothesis is that retrieving and attending only to the top-k most relevant keys from the full input is sufficient to retain the bulk of necessary context, while avoiding the quadratic complexity of full attention. The paper evaluates this on long-document summarization tasks to test if Unlimiformer improves existing models' ability to handle much longer sequences.In summary, the key research question is whether retrieval-augmented attention over only top-k tokens can extend standard pretrained transformers to unbounded input lengths for generation tasks that require long-range context. The experiments aim to validate if Unlimiformer succeeds in this goal.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing Unlimiformer, a general approach to wrap any existing pretrained encoder-decoder transformer model to allow processing of inputs of unlimited length at test time.- Unlimiformer offloads the cross-attention computation to a k-nearest neighbor (kNN) index that can be stored efficiently in GPU or CPU memory and queried in sub-linear time. This allows indexing long input sequences while having each attention head in each decoder layer attend only to its top-k keys, instead of all keys.- Demonstrating Unlimiformer on several long-document and book summarization benchmarks, showing it can process up to 500k token long inputs on the WikiSum dataset without truncation. Unlimiformer improves models like BART and Longformer by extending them to unlimited inputs without additional learned weights or model architecture changes.- Showing that Unlimiformer performs better than or can further improve existing long-context models like LED, SLED, and Memorizing Transformers on long-range summarization tasks.- Providing an efficient attention reformulation that allows constructing a single index across all heads and decoder layers, instead of separate indexes per head/layer.- Releasing code and models to allow easy application of Unlimiformer to existing pretrained transformers, with the goal of enabling long-range modeling even with limited compute resources.In summary, the main contribution is proposing Unlimiformer as an efficient and generic approach to extend any pretrained encoder-decoder model to process unlimited length inputs for long-range sequence modeling tasks like summarization.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes Unlimiformer, a method to extend pretrained transformer models to handle inputs of unlimited length for tasks like long document summarization by using a nearest neighbors search to retrieve the most relevant context at each layer and attention head.


## How does this paper compare to other research in the same field?

This paper introduces Unlimformer, a method to allow transformer models to handle inputs of unlimited length at test time. Here are some key ways it compares to related work:- Most prior work in long-range transformers requires changing the model architecture and pretraining from scratch. In contrast, Unlimformer is a generic approach that can wrap any existing pretrained encoder-decoder transformer and extend it to unlimited inputs without modifying the architecture or training objective. This allows Unlimformer to leverage any pretrained model off-the-shelf.- Unlimformer is similar to Memorizing Transformers in using a nearest neighbor index over encoded input states for retrieval. However, Unlimformer reformulates the attention computation to allow using a single index over all layers and heads, rather than separate indexes per head/layer like Memorizing Transformers. This makes Unlimformer more efficient. Unlimformer also doesn't add any learned parameters.- Compared to methods like SLED that encode the input in chunks, Unlimformer attends only to the top-k keys rather than all keys, making it more scalable to longer contexts. Unlimformer also doesn't require costly training like SLED.- Unlimformer shows strong empirical performance, outperforming prior specialized long-range models like LED and SLED even without training. With training, Unlimformer provides further gains and can even improve existing long-range models like LED when applied on top.Overall, Unlimformer distinguishes itself by being a generic, non-parametric approach to transform any off-the-shelf pretrained model to handle unlimited inputs efficiently without architectural changes or costly training. The experimental results demonstrate Unlimformer's effectiveness compared to prior work.
