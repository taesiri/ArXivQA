# [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to extend pretrained transformer models to handle input sequences of effectively unlimited length for text generation tasks. The standard transformer architecture is limited to a predefined maximum input length, due to its quadratic self-attention mechanism that attends over all input tokens. This poses a challenge for tasks like long-document summarization where the input texts can easily exceed the typical 512-1024 token limit. The paper proposes a method called Unlimiformer that allows injecting retrieval-augmented attention into any pretrained encoder-decoder transformer, to attend only to a small set of top-k retrieved keys from the full input instead of all tokens. This removes the hardcoded length limit and enables processing arbitrary length sequences.The main hypothesis is that retrieving and attending only to the top-k most relevant keys from the full input is sufficient to retain the bulk of necessary context, while avoiding the quadratic complexity of full attention. The paper evaluates this on long-document summarization tasks to test if Unlimiformer improves existing models' ability to handle much longer sequences.In summary, the key research question is whether retrieval-augmented attention over only top-k tokens can extend standard pretrained transformers to unbounded input lengths for generation tasks that require long-range context. The experiments aim to validate if Unlimiformer succeeds in this goal.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing Unlimiformer, a general approach to wrap any existing pretrained encoder-decoder transformer model to allow processing of inputs of unlimited length at test time.- Unlimiformer offloads the cross-attention computation to a k-nearest neighbor (kNN) index that can be stored efficiently in GPU or CPU memory and queried in sub-linear time. This allows indexing long input sequences while having each attention head in each decoder layer attend only to its top-k keys, instead of all keys.- Demonstrating Unlimiformer on several long-document and book summarization benchmarks, showing it can process up to 500k token long inputs on the WikiSum dataset without truncation. Unlimiformer improves models like BART and Longformer by extending them to unlimited inputs without additional learned weights or model architecture changes.- Showing that Unlimiformer performs better than or can further improve existing long-context models like LED, SLED, and Memorizing Transformers on long-range summarization tasks.- Providing an efficient attention reformulation that allows constructing a single index across all heads and decoder layers, instead of separate indexes per head/layer.- Releasing code and models to allow easy application of Unlimiformer to existing pretrained transformers, with the goal of enabling long-range modeling even with limited compute resources.In summary, the main contribution is proposing Unlimiformer as an efficient and generic approach to extend any pretrained encoder-decoder model to process unlimited length inputs for long-range sequence modeling tasks like summarization.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes Unlimiformer, a method to extend pretrained transformer models to handle inputs of unlimited length for tasks like long document summarization by using a nearest neighbors search to retrieve the most relevant context at each layer and attention head.


## How does this paper compare to other research in the same field?

This paper introduces Unlimformer, a method to allow transformer models to handle inputs of unlimited length at test time. Here are some key ways it compares to related work:- Most prior work in long-range transformers requires changing the model architecture and pretraining from scratch. In contrast, Unlimformer is a generic approach that can wrap any existing pretrained encoder-decoder transformer and extend it to unlimited inputs without modifying the architecture or training objective. This allows Unlimformer to leverage any pretrained model off-the-shelf.- Unlimformer is similar to Memorizing Transformers in using a nearest neighbor index over encoded input states for retrieval. However, Unlimformer reformulates the attention computation to allow using a single index over all layers and heads, rather than separate indexes per head/layer like Memorizing Transformers. This makes Unlimformer more efficient. Unlimformer also doesn't add any learned parameters.- Compared to methods like SLED that encode the input in chunks, Unlimformer attends only to the top-k keys rather than all keys, making it more scalable to longer contexts. Unlimformer also doesn't require costly training like SLED.- Unlimformer shows strong empirical performance, outperforming prior specialized long-range models like LED and SLED even without training. With training, Unlimformer provides further gains and can even improve existing long-range models like LED when applied on top.Overall, Unlimformer distinguishes itself by being a generic, non-parametric approach to transform any off-the-shelf pretrained model to handle unlimited inputs efficiently without architectural changes or costly training. The experimental results demonstrate Unlimformer's effectiveness compared to prior work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors are:- Exploring other long-range sequence tasks besides summarization, such as question answering, to evaluate the usefulness of Unlimiformer more broadly. The authors focused their experiments on summarization but suggest Unlimiformer could likely benefit other tasks with long context as well.- Investigating different training regimes and objectives for teaching the model to better leverage unlimited context. The authors experiment with retrieval, random, and alternating training but suggest there may be even more effective ways to train models to take advantage of unlimited inputs.- Applying Unlimiformer to multilingual models and datasets to understand if it can effectively handle long contexts across languages. The authors evaluated only English datasets.- Exploring different nearest neighbor search methods and indexing strategies to optimize efficiency, especially on GPUs with limited memory. The authors used a basic FAISS implementation but more advanced indexing could improve speed and memory usage.- Combining Unlimiformer with structured knowledge retrieval to bring in external knowledge in addition to long context, similar to REALM/RAG. This could improve factual correctness on tasks like summarization.- Developing better evaluation metrics for long document modeling, since metrics like ROUGE have limitations. The authors note issues with current metrics not detecting incoherence or hallucination.In summary, the key future directions focus on applying Unlimiformer to more tasks and languages, optimizing the training and efficiency, combining it with knowledge retrieval, and developing better evaluation metrics. The overall goal is to further improve transformers' ability to process unlimited length sequences.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes Unlimiformer, a method to allow transformer models to process inputs of unlimited length at test time. Unlimiformer works by indexing the hidden states of a long input sequence in a nearest neighbor index. At each decoding step, each attention head in the decoder queries this index to retrieve its top k nearest neighbors, and only attends to those keys instead of all keys as in standard full attention. This allows Unlimiformer to process arbitrarily long sequences, as the index can be stored efficiently in CPU or GPU memory and queried in sub-linear time. The authors evaluate Unlimiformer on long document and book summarization datasets with inputs up to 500k tokens, showing it improves strong baselines like BART and Longformer without modification or additional training. Unlimiformer also improves results when finetuned on summarization with long contexts. The model code is available to apply Unlimiformer to any existing pretrained encoder-decoder.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:Transformer-based models like BERT and GPT-2 have a predefined limit on the length of text they can process, because they need to attend to every token in the input. This paper proposes Unlimformer, a method that can extend any pretrained transformer model to handle extremely long text inputs. Unlimformer works by using a nearest-neighbor index to retrieve the most relevant keys for each attention head, instead of having the model attend to every token. This allows Unlimformer to summarize something like a 500,000 token novel by only attending to a small subset of key tokens from the full text. The authors demonstrate Unlimformer on long document and book summarization tasks, showing it can handle much longer inputs than baseline transformers. Unlimformer improved the performance of models like BART and Longformer on datasets like BookSum without needing to modify model architecture or train extra parameters. Unlimformer provides an efficient way to leverage the capabilities of powerful pretrained models on tasks with very long inputs. The code and models are publicly released.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes Unlimiformer, a method that allows transformer-based language models to handle inputs of unlimited length at test time. Unlimiformer works by constructing a k-nearest neighbor (kNN) index over the hidden states of the input tokens encoded by the model's pretrained encoder. At each decoding step, the cross-attention mechanism in each decoder layer queries this kNN index to retrieve the top-k keys relevant to that layer and head, instead of attending over all input tokens. The kNN distances serve as the attention scores. This allows the decoder to attend to the full context of arbitrarily long inputs without modifying the base transformer architecture. The kNN index can be stored efficiently on the GPU or CPU and queried in sublinear time. Unlimiformer is shown to improve performance on long document and book summarization tasks by enabling attention over full documents that exceed the pretrained model's maximum context length.


## What problem or question is the paper addressing?

The paper is addressing the limitation of Transformer-based models that have a predefined bound on their input length, due to needing to potentially attend to every token in the input. This makes them unable to effectively process very long sequences like books. The key questions/problems the paper is aiming to address are:- How can we extend Transformer encoder-decoder models to handle arbitrarily long input sequences, without having to modify or retrain the base model architecture?- Can we do this in a way that allows leveraging pretrained models like BART or T5, without having to pretrain a specialized long-range architecture from scratch?- Can we achieve strong performance on long-document summarization tasks that have very long inputs, sometimes over 500k tokens, using standard pretrained models?- Can we do this efficiently without quadratic growth in computation as the input length increases?So in summary, the key focus is on enabling unlimited length Transformers in a generic way by building on top of existing models, to handle long document summarization with extreme lengths.
