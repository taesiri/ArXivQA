# [ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV   Caching](https://arxiv.org/abs/2403.17312)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT-3 have shown superior performance on many NLP tasks but present challenges for practical inference due to their compute and memory-intensive nature. 
- A key technique to accelerate LLM inference is key-value (KV) caching, which reuses intermediate states in the transformer layers to reduce computations. However, the growing memory footprint of cached KV tensors becomes a bottleneck, especially on resource-constrained systems like a single GPU.
- As batch size and sequence length increase, allocated memory for KV caching exceeds GPU capacity, requiring frequent offloading/reloading between GPU and CPU, which significantly slows down inference.

Proposed Solution - ALISA:
- An algorithm-system co-design to accelerate LLM inference via sparsity-aware KV caching.
- Algorithm: Sparse Window Attention (SWA) identifies important tokens using a mixture of globally dynamic and locally static sparse patterns. This reduces KV tensor memory footprint while maintaining accuracy.
- System: A 3-phase token-level scheduler dynamically balances caching KV tensors in GPU/CPU memory and recomputing for best performance. Also uses KV tensor compression via quantization to INT8.

Main Contributions:
- Identified challenges of growing KV tensor memory footprint during LLM inference as a key bottleneck.
- Proposed SWA algorithm to create sparse KV tensor patterns that maintain accuracy much better than prior works.
- Designed a scheduler for dynamic token-level KV tensor allocation and recomputation based on sequence length.
- Evaluated on various LLM models/tasks. ALISA improves throughput by up to 3x over FlexGen and 1.9x over vLLM on a single GPU system.

In summary, the paper presents an innovative algorithm-system co-design solution called ALISA to address the memory bottlenecks of KV tensor caching for efficient LLM inference on resource-constrained systems.
