# [Improve Knowledge Distillation via Label Revision and Data Selection](https://arxiv.org/abs/2404.03693)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Knowledge distillation (KD) is a powerful technique for model compression, where the predictions of a large teacher model are used to provide supervision signals to train a smaller, lightweight student model. However, the teacher model may still make incorrect predictions which can mislead or conflict with the ground truth supervision. Therefore, the reliability of teacher predictions is a major issue limiting KD performance.  

Proposed Solution: This paper proposes two methods to address the unreliable predictions from the teacher in KD:

1) Label Revision (LR): Rectify the incorrect predictions of the teacher using the ground truth labels. Specifically, combine the one-hot ground truth label and the teacher's soft prediction probabilites in a principled way to overwrite the incorrect maximum prediction while maintaining relative similarities.  

2) Data Selection (DS): Select appropriate data samples to receive teacher supervision rather than distilling the entire dataset. Split data into two subsets - one supervised by both teacher and ground truth, the other only by ground truth. Selection is based on influence scores.

Together, LR revises unreliable teacher predictions, and DS reduces their impact by selective supervision. Experiments show accuracy improvements over various KD methods. LR and DS also further boost performance when combined with other advanced KD techniques.

Main Contributions:
- Propose LR to rectify incorrect teacher predictions using ground truth, maintaining relative similarities 
- Introduce DS to reduce impact of unreliable predictions by selective supervision
- Demonstrate standalone effectiveness over multiple KD methods 
- Show LR+DS can be easily integrated with advanced KD methods to further improve performance
- Provide comprehensive experiments on CIFAR and ImageNet datasets using various network architectures

In summary, this paper identifies and addresses an important reliability issue with teacher supervision in knowledge distillation through simple yet effective label revision and data selection techniques.
