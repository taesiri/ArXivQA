# Distill-and-Compare: Auditing Black-Box Models Using Transparent Model   Distillation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we gain insight into and audit proprietary or opaque black-box risk scoring models, given realistic constraints where we may only have access to a dataset with the black-box model's risk scores and ground truth outcomes, and this dataset may be missing key features used by the black-box model?The authors propose a "Distill-and-Compare" approach to address this question:1) Treat the black-box model as a teacher and train a transparent "mimic" model to learn to predict the black-box model's risk scores. This is similar to model distillation.2) Separately train a transparent "outcome" model on the same data to predict ground truth outcomes. 3) Compare the mimic and outcome models to gain insights into the black-box model. Similarities in modeling particular features increase confidence the mimic model faithfully represents the black-box. Differences highlight areas where the black-box may diverge from ground truth.4) Use a statistical test to determine if key features are likely missing from the audit dataset. So in summary, the central hypothesis is that by training and comparing transparent mimic and outcome models on an audit dataset, one can gain useful insights into proprietary black-box risk models despite constraints on data availability.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contributions of this paper are:1. Proposing an approach called "Distill-and-Compare" to audit black-box risk scoring models under realistic conditions where the model API cannot be probed. The approach uses model distillation to train a transparent "mimic" model on the black-box model's risk scores, and compares this to a transparent "outcome" model trained to predict ground-truth outcomes. Differences between the two models can provide insights into the black-box model.2. Demonstrating the importance of calibrating the black-box model's risk scores before training the mimic model, to remove distortions that may have been introduced. 3. Applying the Distill-and-Compare approach to audit four real-world risk scoring models: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club.4. Proposing a statistical test to determine if key features are missing from the audit data that were likely used to train the black-box model.5. Providing a new confidence interval estimate for the iGAM model class to quantify uncertainty when comparing two iGAM models.So in summary, the main contribution seems to be proposing a practical approach to audit opaque risk scoring models under realistic constraints, demonstrating it on real-world examples, and providing tools like the statistical test and confidence intervals to strengthen the audit analysis. The approach aims to gain insights into these typically proprietary models without access to the model internals or training data.
