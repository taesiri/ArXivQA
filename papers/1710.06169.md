# [Distill-and-Compare: Auditing Black-Box Models Using Transparent Model   Distillation](https://arxiv.org/abs/1710.06169)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: How can we gain insight into and audit proprietary or opaque black-box risk scoring models, given realistic constraints where we may only have access to a dataset with the black-box model's risk scores and ground truth outcomes, and this dataset may be missing key features used by the black-box model?The authors propose a "Distill-and-Compare" approach to address this question:1) Treat the black-box model as a teacher and train a transparent "mimic" model to learn to predict the black-box model's risk scores. This is similar to model distillation.2) Separately train a transparent "outcome" model on the same data to predict ground truth outcomes. 3) Compare the mimic and outcome models to gain insights into the black-box model. Similarities in modeling particular features increase confidence the mimic model faithfully represents the black-box. Differences highlight areas where the black-box may diverge from ground truth.4) Use a statistical test to determine if key features are likely missing from the audit dataset. So in summary, the central hypothesis is that by training and comparing transparent mimic and outcome models on an audit dataset, one can gain useful insights into proprietary black-box risk models despite constraints on data availability.


## What is the main contribution of this paper?

 Based on the abstract, it seems the main contributions of this paper are:1. Proposing an approach called "Distill-and-Compare" to audit black-box risk scoring models under realistic conditions where the model API cannot be probed. The approach uses model distillation to train a transparent "mimic" model on the black-box model's risk scores, and compares this to a transparent "outcome" model trained to predict ground-truth outcomes. Differences between the two models can provide insights into the black-box model.2. Demonstrating the importance of calibrating the black-box model's risk scores before training the mimic model, to remove distortions that may have been introduced. 3. Applying the Distill-and-Compare approach to audit four real-world risk scoring models: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club.4. Proposing a statistical test to determine if key features are missing from the audit data that were likely used to train the black-box model.5. Providing a new confidence interval estimate for the iGAM model class to quantify uncertainty when comparing two iGAM models.So in summary, the main contribution seems to be proposing a practical approach to audit opaque risk scoring models under realistic constraints, demonstrating it on real-world examples, and providing tools like the statistical test and confidence intervals to strengthen the audit analysis. The approach aims to gain insights into these typically proprietary models without access to the model internals or training data.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a comparison to related work in the field:- The paper proposes a new "distill-and-compare" approach for auditing black-box risk scoring models, using model distillation and comparison to transparent models. This differs from other auditing methods like perturbing inputs or probing model APIs, and is tailored to the realistic setting where only a labeled data set is available. - The idea of using model distillation to understand black-box models has been explored before, but this paper uses a novel setup with both risk scores and ground truth outcomes to train the distilled model and an outcomes model for comparison. Other papers like Adler et al. train two models for different purposes.- The paper compares the distilled transparent model to a separate outcomes model trained on ground truth labels. Some other work compares models trained for different objectives, but not a distilled model to an outcomes model. The comparison aspect is novel.- The statistical test proposed to detect missing features could be applied standalone to any black-box model data set. Other work does not provide a way to test if key features are missing from the audit data.- Using interpretable generalized additive models as the transparent model class allows model comparison and understanding feature effects. Many other papers use less interpretable models.- The application of the approach to several real-world risk scoring models like COMPAS provides useful audits of high-impact models and data sets.In summary, the combination of distillation, comparison to an outcomes model, transparent models, missing feature detection, and application to real-world systems provides unique contributions over prior art. The approach appears tailored to the realistic black-box auditing setting.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing methods to audit black-box models in settings where access to the model API is restricted or unavailable. The authors' proposed Distill-and-Compare approach relies on having a dataset with black-box model risk scores, which may not always be accessible. They suggest exploring approaches like active learning to query the black-box model to label more data for auditing.- Applying the Distill-and-Compare approach to other transparent model classes besides iGAMs. The authors focused on iGAMs but note that other transparent models could be used as long as they allow for comparison. Exploring other model classes could improve fidelity or interpretability.- Extending the statistical test for missing features to account for possible interactions between missing and observed features. The proposed test may have reduced power if missing features interact with observed features in complex ways. - Developing methods to distinguish between different explanations for differences observed between the mimic and outcome models. The authors describe several possible reasons for differences, and suggest an open area is better ways to identify the true underlying reason.- Evaluating the approach on more real-world black box models and data sets. The authors demonstrate the approach on a few examples, but testing on more domains could reveal new challenges or applications.- Incorporating human domain expertise into the auditing pipeline to aid with interpreting results and differences between mimic and outcome models.- Exploring the use of the Distill-and-Compare approach for purposes beyond auditing, like model debugging or uncertainty quantification.In summary, the authors highlight opportunities to extend the approach to more limited data settings, apply it to diverse model classes and domains, enhance the statistical tests, and better incorporate human expertise. Advancing the interpretability and comparison aspects of the approach are highlighted as promising future directions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper proposes Distill-and-Compare, a model distillation and comparison approach to audit black-box risk scoring models under realistic conditions where the model API cannot be accessed. The approach involves training a transparent student model (the mimic model) to mimic the black-box teacher model's risk scores, and comparing this to a second transparent model (the outcome model) trained to predict ground-truth outcomes. Differences between the two transparent models can provide insights into the black-box model. The paper demonstrates this approach on four public datasets - COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. It also proposes a statistical test to determine if the audit dataset is missing key features used by the black-box model. Key advantages of the approach are that it does not require access to the model API and can surface biases not known a priori. The efficacy increases when the transparent models are highly faithful and accurate, and when audit data is complete.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a model distillation and comparison approach called Distill-and-Compare to audit black-box risk scoring models under realistic conditions. The key idea is to treat black-box models as teachers and train transparent student models to mimic their risk scores. A mimic model is trained on data labeled with the black-box model's risk scores. An additional transparent model is trained on the same data but to predict ground-truth outcomes. By comparing the mimic and outcome models, insights can be gained into how the black-box model may differ from ground-truth outcomes. The approach is applied to audit four real-world risk scoring models: COMPAS recidivism, Chicago Police Strategic Subject List, Stop-and-Frisk, and Lending Club credit scoring. The paper shows how calibrating risk scores and estimating confidence intervals for differences between mimic and outcome models enables auditing the black-box models. It also proposes a statistical test to detect if audit data is missing key features used by the black-box model. Overall, the Distill-and-Compare approach provides interpretable insights into black-box models in a realistic setting without access to model APIs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a two-step approach called Distill-and-Compare to audit black-box risk scoring models, in a setting where only a dataset labeled with the black-box model's risk scores and ground truth outcomes is available. In the first step, the black-box model is treated as a teacher and a transparent student model called a mimic model is trained to mimic the black-box model's risk scores. Separately, another transparent model called the outcome model is trained on the ground truth outcomes. Both models are trained on the same model class to enable comparison. In the second step, the mimic and outcome models are compared - similarities increase confidence that the mimic model faithfully represents the black-box model, while differences provide insights into how the black-box model may diverge from ground truth outcomes. The paper also proposes a statistical test to determine if key features are missing from the audit data.


## What problem or question is the paper addressing?

 Based on my reading of the paper abstract, it appears the paper is addressing the issue of auditing black-box risk scoring models, which are often proprietary or opaque. The authors propose an approach called "Distill-and-Compare" to gain insight into these black-box models by training transparent student models to mimic the risk scores and outcomes predicted by the black-box model. The key questions and contributions seem to be:- Proposing an approach to audit black-box models under realistic conditions where the model API cannot be accessed, only a dataset with risk scores and outcomes. - Showing the importance of calibrating risk scores to remove distortions before training the mimic model.- Applying the approach to audit four real-world risk scoring models.- Proposing a statistical test to determine if key features are missing from the audit dataset.- Using transparent models allows detecting biases without needing to know them a priori.- An ancillary contribution is a new confidence interval estimate for the iGAM model to compare two models of this class.So in summary, the key focus appears to be developing and demonstrating an approach to gain insight into proprietary black-box risk scoring models using only a limited dataset, in order to audit them for fairness, accuracy and other properties. Let me know if you need any clarification or have additional questions!
