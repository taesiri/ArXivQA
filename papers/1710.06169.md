# [Distill-and-Compare: Auditing Black-Box Models Using Transparent Model   Distillation](https://arxiv.org/abs/1710.06169)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can we gain insight into and audit proprietary or opaque black-box risk scoring models, given realistic constraints where we may only have access to a dataset with the black-box model's risk scores and ground truth outcomes, and this dataset may be missing key features used by the black-box model?

The authors propose a "Distill-and-Compare" approach to address this question:

1) Treat the black-box model as a teacher and train a transparent "mimic" model to learn to predict the black-box model's risk scores. This is similar to model distillation.

2) Separately train a transparent "outcome" model on the same data to predict ground truth outcomes. 

3) Compare the mimic and outcome models to gain insights into the black-box model. Similarities in modeling particular features increase confidence the mimic model faithfully represents the black-box. Differences highlight areas where the black-box may diverge from ground truth.

4) Use a statistical test to determine if key features are likely missing from the audit dataset. 

So in summary, the central hypothesis is that by training and comparing transparent mimic and outcome models on an audit dataset, one can gain useful insights into proprietary black-box risk models despite constraints on data availability.


## What is the main contribution of this paper?

 Based on the abstract, it seems the main contributions of this paper are:

1. Proposing an approach called "Distill-and-Compare" to audit black-box risk scoring models under realistic conditions where the model API cannot be probed. The approach uses model distillation to train a transparent "mimic" model on the black-box model's risk scores, and compares this to a transparent "outcome" model trained to predict ground-truth outcomes. Differences between the two models can provide insights into the black-box model.

2. Demonstrating the importance of calibrating the black-box model's risk scores before training the mimic model, to remove distortions that may have been introduced. 

3. Applying the Distill-and-Compare approach to audit four real-world risk scoring models: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club.

4. Proposing a statistical test to determine if key features are missing from the audit data that were likely used to train the black-box model.

5. Providing a new confidence interval estimate for the iGAM model class to quantify uncertainty when comparing two iGAM models.

So in summary, the main contribution seems to be proposing a practical approach to audit opaque risk scoring models under realistic constraints, demonstrating it on real-world examples, and providing tools like the statistical test and confidence intervals to strengthen the audit analysis. The approach aims to gain insights into these typically proprietary models without access to the model internals or training data.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a comparison to related work in the field:

- The paper proposes a new "distill-and-compare" approach for auditing black-box risk scoring models, using model distillation and comparison to transparent models. This differs from other auditing methods like perturbing inputs or probing model APIs, and is tailored to the realistic setting where only a labeled data set is available. 

- The idea of using model distillation to understand black-box models has been explored before, but this paper uses a novel setup with both risk scores and ground truth outcomes to train the distilled model and an outcomes model for comparison. Other papers like Adler et al. train two models for different purposes.

- The paper compares the distilled transparent model to a separate outcomes model trained on ground truth labels. Some other work compares models trained for different objectives, but not a distilled model to an outcomes model. The comparison aspect is novel.

- The statistical test proposed to detect missing features could be applied standalone to any black-box model data set. Other work does not provide a way to test if key features are missing from the audit data.

- Using interpretable generalized additive models as the transparent model class allows model comparison and understanding feature effects. Many other papers use less interpretable models.

- The application of the approach to several real-world risk scoring models like COMPAS provides useful audits of high-impact models and data sets.

In summary, the combination of distillation, comparison to an outcomes model, transparent models, missing feature detection, and application to real-world systems provides unique contributions over prior art. The approach appears tailored to the realistic black-box auditing setting.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Developing methods to audit black-box models in settings where access to the model API is restricted or unavailable. The authors' proposed Distill-and-Compare approach relies on having a dataset with black-box model risk scores, which may not always be accessible. They suggest exploring approaches like active learning to query the black-box model to label more data for auditing.

- Applying the Distill-and-Compare approach to other transparent model classes besides iGAMs. The authors focused on iGAMs but note that other transparent models could be used as long as they allow for comparison. Exploring other model classes could improve fidelity or interpretability.

- Extending the statistical test for missing features to account for possible interactions between missing and observed features. The proposed test may have reduced power if missing features interact with observed features in complex ways. 

- Developing methods to distinguish between different explanations for differences observed between the mimic and outcome models. The authors describe several possible reasons for differences, and suggest an open area is better ways to identify the true underlying reason.

- Evaluating the approach on more real-world black box models and data sets. The authors demonstrate the approach on a few examples, but testing on more domains could reveal new challenges or applications.

- Incorporating human domain expertise into the auditing pipeline to aid with interpreting results and differences between mimic and outcome models.

- Exploring the use of the Distill-and-Compare approach for purposes beyond auditing, like model debugging or uncertainty quantification.

In summary, the authors highlight opportunities to extend the approach to more limited data settings, apply it to diverse model classes and domains, enhance the statistical tests, and better incorporate human expertise. Advancing the interpretability and comparison aspects of the approach are highlighted as promising future directions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes Distill-and-Compare, a model distillation and comparison approach to audit black-box risk scoring models under realistic conditions where the model API cannot be accessed. The approach involves training a transparent student model (the mimic model) to mimic the black-box teacher model's risk scores, and comparing this to a second transparent model (the outcome model) trained to predict ground-truth outcomes. Differences between the two transparent models can provide insights into the black-box model. The paper demonstrates this approach on four public datasets - COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. It also proposes a statistical test to determine if the audit dataset is missing key features used by the black-box model. Key advantages of the approach are that it does not require access to the model API and can surface biases not known a priori. The efficacy increases when the transparent models are highly faithful and accurate, and when audit data is complete.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a model distillation and comparison approach called Distill-and-Compare to audit black-box risk scoring models under realistic conditions. The key idea is to treat black-box models as teachers and train transparent student models to mimic their risk scores. A mimic model is trained on data labeled with the black-box model's risk scores. An additional transparent model is trained on the same data but to predict ground-truth outcomes. By comparing the mimic and outcome models, insights can be gained into how the black-box model may differ from ground-truth outcomes. 

The approach is applied to audit four real-world risk scoring models: COMPAS recidivism, Chicago Police Strategic Subject List, Stop-and-Frisk, and Lending Club credit scoring. The paper shows how calibrating risk scores and estimating confidence intervals for differences between mimic and outcome models enables auditing the black-box models. It also proposes a statistical test to detect if audit data is missing key features used by the black-box model. Overall, the Distill-and-Compare approach provides interpretable insights into black-box models in a realistic setting without access to model APIs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a two-step approach called Distill-and-Compare to audit black-box risk scoring models, in a setting where only a dataset labeled with the black-box model's risk scores and ground truth outcomes is available. In the first step, the black-box model is treated as a teacher and a transparent student model called a mimic model is trained to mimic the black-box model's risk scores. Separately, another transparent model called the outcome model is trained on the ground truth outcomes. Both models are trained on the same model class to enable comparison. In the second step, the mimic and outcome models are compared - similarities increase confidence that the mimic model faithfully represents the black-box model, while differences provide insights into how the black-box model may diverge from ground truth outcomes. The paper also proposes a statistical test to determine if key features are missing from the audit data.


## What problem or question is the paper addressing?

 Based on my reading of the paper abstract, it appears the paper is addressing the issue of auditing black-box risk scoring models, which are often proprietary or opaque. The authors propose an approach called "Distill-and-Compare" to gain insight into these black-box models by training transparent student models to mimic the risk scores and outcomes predicted by the black-box model. The key questions and contributions seem to be:

- Proposing an approach to audit black-box models under realistic conditions where the model API cannot be accessed, only a dataset with risk scores and outcomes. 

- Showing the importance of calibrating risk scores to remove distortions before training the mimic model.

- Applying the approach to audit four real-world risk scoring models.

- Proposing a statistical test to determine if key features are missing from the audit dataset.

- Using transparent models allows detecting biases without needing to know them a priori.

- An ancillary contribution is a new confidence interval estimate for the iGAM model to compare two models of this class.

So in summary, the key focus appears to be developing and demonstrating an approach to gain insight into proprietary black-box risk scoring models using only a limited dataset, in order to audit them for fairness, accuracy and other properties. Let me know if you need any clarification or have additional questions!


## What are the keywords or key terms associated with this paper?

 Based on skimming through the paper, some of the key terms and keywords that seem most relevant are:

- Model verification and validation - The paper discusses auditing and gaining insight into black-box risk scoring models, which relates to model verification and validation.

- Model distillation - A key aspect of the proposed approach is using model distillation to train a transparent student model to mimic a black-box teacher model.

- Model comparison - The distilled mimic model is compared to another transparent model trained to predict ground truth outcomes. Differences between the two models are analyzed.

- Risk scoring models - The focus is on auditing proprietary or opaque risk scoring models used for prediction in critical domains.

- Transparent models - The mimic and outcome models need to be transparent, interpretable model classes to enable model comparison and auditing of the black-box model. The paper uses iGAM models.

- Missing feature detection - A statistical test is proposed to detect if key features are missing from the audit data compared to what was used to train the black-box model.

- Calibration - The paper discusses the importance of calibrating risk scores to match empirical probabilities before training the mimic model.

- Fairness - Auditing risk scoring models is relevant to fairness, accountability and transparency of machine learning models.

In summary, the key themes are around model auditing, distillation, comparison, transparency, and missing feature detection, with a focus on risk scoring models and relevance to fairness.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the primary research question or goal of the study? 

2. What problem is the study trying to solve or address?

3. What methods did the researchers use to conduct the study? What kind of study design was used?

4. What were the key findings or results of the study? 

5. Did the results support or refute the researchers' hypotheses or expectations?

6. What conclusions did the researchers draw based on the results?

7. What are the limitations or weaknesses of the study?

8. How does this study build on or differ from previous research on the topic? 

9. What are the broader implications of the findings, both for theory/understanding and for practice/application?

10. What future research does the study suggest is needed to further explore or clarify the topic?

Asking these types of questions will help summarize the key information from the paper - the purpose, methods, findings, conclusions, limitations, and implications. Additional specific questions could also be tailored based on the particular topic and contents of the given paper. The goal is to identify and understand the most important aspects of the research.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes using model distillation to train a transparent "student" model to mimic the predictions of a black-box "teacher" model. What are the advantages and disadvantages of this approach compared to directly trying to interpret the black-box model itself?

2. The paper trains a second transparent model to predict ground-truth outcomes. Why is comparing this second model to the distilled mimic model useful for auditing the black-box model? What kinds of insights can be gained?

3. The paper uses generalized additive models (GAMs) as the transparent model class. What properties make GAMs a good choice? What limitations might they have? Could you suggest other transparent model classes that could be used instead?

4. The paper calibrates the risk scores before training the mimic model. Why is this calibration step important? What problems could arise if calibration was not performed?

5. The paper proposes a statistical test to detect if key features are missing from the audit data. Explain how this test works. What assumptions does it rely on? How could the test be fooled?

6. When differences are observed between the mimic and outcome models, the paper suggests several possible explanations. What are some of these explanations? How could you determine the true underlying cause of a difference? 

7. The COMPAS case study finds evidence of racial bias. But the paper notes the ground-truth outcomes themselves may be biased. How does this impact the analysis? Can racial bias still be assessed fairly in this situation?

8. The paper evaluates fidelity of the mimic models using RMSE. What are other metrics that could be used? What are the tradeoffs of different evaluation metrics?

9. What steps could be taken to adapt the Distill-and-Compare approach for auditing deep neural network models? What challenges might arise?

10. The paper focuses on risk scoring models. What other types of black-box models could this approach be applied to? Would any modifications need to be made?


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: 

The paper proposes a two-step approach to audit black-box risk scoring models, using model distillation and comparison to gain insight into the models when only limited audit data is available, and testing whether key features are missing from the audit data.


## Summarize the paper in one sentence.

 The paper "Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation" proposes an approach to audit black-box risk scoring models using model distillation and comparison techniques.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a model distillation and comparison approach called Distill-and-Compare to audit black-box risk scoring models. The goal is to gain insight into the input-output relationships of black-box models under realistic conditions where only an audit dataset with risk scores and ground-truth outcomes is available, without access to probe the model API. First, a transparent "mimic" model is trained to predict the black-box model's risk scores. Next, a transparent "outcome" model is trained to predict ground-truth outcomes. By comparing the mimic and outcome models, insights can be gained into how the black-box model differs from ground-truth relationships. The approach is demonstrated on auditing COMPAS, Chicago Police, Stop-and-Frisk, and Lending Club models. A statistical test is also proposed to determine if key features are missing from the audit dataset. The advantages of the approach are that it does not require probing the black-box model API and can surface biases not known a priori by using transparent models. Limitations are that model fidelity depends on having features comparable to those used by the black-box model.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the distill-and-compare method proposed in the paper:

1. The paper proposes training a mimic model to match the risk scores from the black-box model and an outcome model to predict ground truth labels. What other types of models could be trained for comparison instead of an outcome model, and what insights might they provide? 

2. When comparing the mimic and outcome models, the paper suggests the differences likely result from differences between the risk scores and ground truth outcomes. But couldn't the differences also result from limitations in the model class used for the mimic and outcome models? How could you test whether the model class is sufficiently expressive?

3. The calibration step is critical for comparing the mimic and outcome models. Are there any risks or downsides to transforming the risk scores before training the mimic model? Could it distort insights into the original black-box model?

4. For the statistical test to detect missing features, why use three different correlation measures (Pearson, Spearman, Kendall) between the mimic and outcome model errors? What are the advantages of each one? 

5. The paper evaluates fidelity using RMSE between the mimic model and black-box risk scores. But aren't the risk scores themselves on arbitrary scales? Why not use rank correlation like Spearman's rho as the evaluation metric instead?

6. When explaining differences between the mimic and outcome models for COMPAS, one reason given is that the black-box model's training data may differ from the audit data. How could you test whether this is the case beyond just training on more unlabeled samples?

7. Do you think the insights provided by this method are really model-agnostic? Or could they be biased based on the model class used for the mimic and outcome models?

8. The paper argues this approach provides insights without having to probe the black-box model's API. But wouldn't querying it provide additional signal for training better mimic models? How could you incorporate API access? 

9. How well do you think this approach would work for different model classes used in the black-box model, like neural networks or decision trees? What modifications might be needed?

10. Beyond model comparison, how else could you leverage the trained mimic and outcome models to provide insights into the black-box model, such as examining individual predictions or feature attributions?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel approach called Distill-and-Compare to audit black-box risk scoring models when access to the model API is not available. The authors treat the black-box model as a teacher and train a transparent student model called the mimic model to mimic the risk scores assigned by the black-box model. They also train a second transparent model called the outcome model to predict ground-truth outcomes. Comparing the mimic and outcome models reveals insights into how the black-box model may differ from ground truth. The authors apply calibration to handle distortions in the black-box model's scores. They use the interpretable iGAM model for the mimic and outcome models and derive a new method to estimate confidence intervals to detect significant differences. Experiments on COMPAS, Chicago Police, Stop-and-Frisk, and Lending Club data demonstrate the approach. The authors also propose a statistical test to detect if key features are missing from the audit data. Overall, this is an important contribution for auditing opaque models in a realistic setting using model distillation and comparison techniques.
