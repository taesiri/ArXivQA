# Distill-and-Compare: Auditing Black-Box Models Using Transparent Model   Distillation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we gain insight into and audit proprietary or opaque black-box risk scoring models, given realistic constraints where we may only have access to a dataset with the black-box model's risk scores and ground truth outcomes, and this dataset may be missing key features used by the black-box model?The authors propose a "Distill-and-Compare" approach to address this question:1) Treat the black-box model as a teacher and train a transparent "mimic" model to learn to predict the black-box model's risk scores. This is similar to model distillation.2) Separately train a transparent "outcome" model on the same data to predict ground truth outcomes. 3) Compare the mimic and outcome models to gain insights into the black-box model. Similarities in modeling particular features increase confidence the mimic model faithfully represents the black-box. Differences highlight areas where the black-box may diverge from ground truth.4) Use a statistical test to determine if key features are likely missing from the audit dataset. So in summary, the central hypothesis is that by training and comparing transparent mimic and outcome models on an audit dataset, one can gain useful insights into proprietary black-box risk models despite constraints on data availability.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contributions of this paper are:1. Proposing an approach called "Distill-and-Compare" to audit black-box risk scoring models under realistic conditions where the model API cannot be probed. The approach uses model distillation to train a transparent "mimic" model on the black-box model's risk scores, and compares this to a transparent "outcome" model trained to predict ground-truth outcomes. Differences between the two models can provide insights into the black-box model.2. Demonstrating the importance of calibrating the black-box model's risk scores before training the mimic model, to remove distortions that may have been introduced. 3. Applying the Distill-and-Compare approach to audit four real-world risk scoring models: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club.4. Proposing a statistical test to determine if key features are missing from the audit data that were likely used to train the black-box model.5. Providing a new confidence interval estimate for the iGAM model class to quantify uncertainty when comparing two iGAM models.So in summary, the main contribution seems to be proposing a practical approach to audit opaque risk scoring models under realistic constraints, demonstrating it on real-world examples, and providing tools like the statistical test and confidence intervals to strengthen the audit analysis. The approach aims to gain insights into these typically proprietary models without access to the model internals or training data.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a comparison to related work in the field:- The paper proposes a new "distill-and-compare" approach for auditing black-box risk scoring models, using model distillation and comparison to transparent models. This differs from other auditing methods like perturbing inputs or probing model APIs, and is tailored to the realistic setting where only a labeled data set is available. - The idea of using model distillation to understand black-box models has been explored before, but this paper uses a novel setup with both risk scores and ground truth outcomes to train the distilled model and an outcomes model for comparison. Other papers like Adler et al. train two models for different purposes.- The paper compares the distilled transparent model to a separate outcomes model trained on ground truth labels. Some other work compares models trained for different objectives, but not a distilled model to an outcomes model. The comparison aspect is novel.- The statistical test proposed to detect missing features could be applied standalone to any black-box model data set. Other work does not provide a way to test if key features are missing from the audit data.- Using interpretable generalized additive models as the transparent model class allows model comparison and understanding feature effects. Many other papers use less interpretable models.- The application of the approach to several real-world risk scoring models like COMPAS provides useful audits of high-impact models and data sets.In summary, the combination of distillation, comparison to an outcomes model, transparent models, missing feature detection, and application to real-world systems provides unique contributions over prior art. The approach appears tailored to the realistic black-box auditing setting.
