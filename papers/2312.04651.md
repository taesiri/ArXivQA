# [VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head   Reenactment](https://arxiv.org/abs/2312.04651)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing 2D neural head reenactment methods struggle with preserving identity when changing camera angles. Recent 3D-aware methods use meshes or neural radiance fields for view consistency, but rely on linear face models (e.g. 3DMM, FLAME) for disentangling identity and expressions. This leads to uncanny expressions and difficulty preserving likeness, especially for non-frontal poses.

Method:
This paper proposes the first real-time 3D-aware one-shot head reenactment method that disentangles identity and expressions fully volumetrically without linear face models. It lifts the source portrait and driver video into a shared canonical tri-plane representation using a fine-tuned neural 3D face lifter. This allows extracting appearance features from the frontalized source and expression features from the frontalized driver. The expression features are used to generate a residual tri-plane that modifies the source tri-plane to reflect the driver's expression. Finally, a neural renderer generates the output image(s) from the new tri-planes using the driver's pose.

Contributions:
- First fully volumetric disentanglement for real-time 3D-aware head reenactment without linear face models
- Fine-tuning the 3D lifter on real-world video instead of only synthetic data
- Volumetric expression feature extraction from frontalized source and driver 
- State-of-the-art quantitative results on fidelity, expression accuracy and identity preservation
- Handles extreme poses and expressions for diverse subjects
- Showcase high quality reenactment for a holographic telepresence system

The key insight is that frontalization enables robust facial feature extraction in a shared tri-plane space. This, combined with real-world 3D lifter fine-tuning and volumetric neural rendering, leads to superior disentanglement and reenactment quality compared to previous geometry-based and 2D methods.
