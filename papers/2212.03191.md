# [InternVideo: General Video Foundation Models via Generative and   Discriminative Learning](https://arxiv.org/abs/2212.03191)

## What is the central research question or hypothesis that this paper addresses?

The central goal of this paper is to develop a general video foundation model called InternVideo that can achieve strong performance across a wide range of video understanding tasks. The key ideas are:- Explore both masked video modeling (VideoMAE) and video-language contrastive learning as complementary self-supervised objectives for pretraining video encoders. - Unify the representations from VideoMAE and contrastive learning encoders via cross-model interaction.- Validate the model on a systematic video understanding benchmark with 39 datasets covering action understanding, video-language alignment, and open-world video tasks.The main hypothesis is that by combining masked video modeling and contrastive learning in a unified framework, the model can learn more generalized video representations that transfer well to diverse downstream tasks. The paper aims to demonstrate the effectiveness and versatility of InternVideo as a general video foundation model through extensive experiments.In summary, the central goal is developing a unified and general video foundation model that can achieve new state-of-the-art across a wide range of video tasks, which is demonstrated through comprehensive experiments. The key ideas are combining masked video modeling and contrastive learning in a unified framework.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes InternVideo, a general video foundation model that achieves state-of-the-art performance on a wide range of video understanding tasks including action recognition, video-language alignment, and open-world video applications. 2. It explores both masked video modeling and video-language contrastive learning as complementary pretraining objectives for learning generic video representations. It proposes a unified video representation learning paradigm that combines the strengths of both through cross-model interaction.3. It demonstrates the scalability of masked video modeling (VideoMAE) in model size and data scale through experiments. It also shows how to efficiently reuse image-pretrained vision transformer models like CLIP for video-language contrastive learning.4. It constructs a systematic video understanding benchmark with 10 tasks and 39 datasets to evaluate the generalization capability of video foundation models. InternVideo outperforms previous state-of-the-art methods by a large margin on nearly all datasets and tasks.5. It provides efficient training strategies and recipes for developing large-scale video foundation models. The total GPU hours needed is much less than previous models like CoCa.In summary, this paper proposes InternVideo, an efficient and effective general video foundation model that achieves new state-of-the-art results on a diverse set of video understanding tasks through unified representation learning. It demonstrates the promise of foundation models for general video perception.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes InternVideo, a general video foundation model for a variety of video understanding tasks, which efficiently explores both masked video modeling and video-language contrastive learning for representation learning and achieves state-of-the-art performance on extensive video datasets.
