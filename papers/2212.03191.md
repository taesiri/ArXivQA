# [InternVideo: General Video Foundation Models via Generative and   Discriminative Learning](https://arxiv.org/abs/2212.03191)

## What is the central research question or hypothesis that this paper addresses?

 The central goal of this paper is to develop a general video foundation model called InternVideo that can achieve strong performance across a wide range of video understanding tasks. The key ideas are:

- Explore both masked video modeling (VideoMAE) and video-language contrastive learning as complementary self-supervised objectives for pretraining video encoders. 

- Unify the representations from VideoMAE and contrastive learning encoders via cross-model interaction.

- Validate the model on a systematic video understanding benchmark with 39 datasets covering action understanding, video-language alignment, and open-world video tasks.

The main hypothesis is that by combining masked video modeling and contrastive learning in a unified framework, the model can learn more generalized video representations that transfer well to diverse downstream tasks. The paper aims to demonstrate the effectiveness and versatility of InternVideo as a general video foundation model through extensive experiments.

In summary, the central goal is developing a unified and general video foundation model that can achieve new state-of-the-art across a wide range of video tasks, which is demonstrated through comprehensive experiments. The key ideas are combining masked video modeling and contrastive learning in a unified framework.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes InternVideo, a general video foundation model that achieves state-of-the-art performance on a wide range of video understanding tasks including action recognition, video-language alignment, and open-world video applications. 

2. It explores both masked video modeling and video-language contrastive learning as complementary pretraining objectives for learning generic video representations. It proposes a unified video representation learning paradigm that combines the strengths of both through cross-model interaction.

3. It demonstrates the scalability of masked video modeling (VideoMAE) in model size and data scale through experiments. It also shows how to efficiently reuse image-pretrained vision transformer models like CLIP for video-language contrastive learning.

4. It constructs a systematic video understanding benchmark with 10 tasks and 39 datasets to evaluate the generalization capability of video foundation models. InternVideo outperforms previous state-of-the-art methods by a large margin on nearly all datasets and tasks.

5. It provides efficient training strategies and recipes for developing large-scale video foundation models. The total GPU hours needed is much less than previous models like CoCa.

In summary, this paper proposes InternVideo, an efficient and effective general video foundation model that achieves new state-of-the-art results on a diverse set of video understanding tasks through unified representation learning. It demonstrates the promise of foundation models for general video perception.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes InternVideo, a general video foundation model for a variety of video understanding tasks, which efficiently explores both masked video modeling and video-language contrastive learning for representation learning and achieves state-of-the-art performance on extensive video datasets.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to prior work in video foundation models:

- It proposes InternVideo, which is the first video foundation model to achieve state-of-the-art performance across a diverse set of tasks including action understanding, video-language alignment, and open-world video applications. Previous models tended to focus on only one or two of these areas.

- The model combines strengths from both masked video modeling (VideoMAE) and multimodal contrastive learning. Most prior work focused on just one approach. By combining them and allowing cross-representation learning, InternVideo gets improved performance.

- The results on Kinetics-400 (91.1%) and Something-Something v2 (77.2%) are state-of-the-art for those challenging action recognition benchmarks. The model also achieves SOTA on many other datasets across temporal action localization, spatiotemporal action localization, video retrieval, etc.

- The training of InternVideo seems quite efficient compared to some other recent models like CoCa. It requires much less GPU hours to train while still achieving better overall performance.

- The paper proposes a more comprehensive video understanding benchmark compared to what has typically been used to evaluate prior work. This benchmark helps demonstrate the generalization of InternVideo across a wide variety of video tasks.

- The strong zero-shot and open-set performance of InternVideo on tasks like retrieval and action recognition also highlight its ability to generalize beyond the training data.

In summary, this paper pushes forward video foundation models by achieving state-of-the-art, multi-task performance in an efficient training framework. The combination of techniques and more comprehensive benchmark help highlight the generalization compared to prior work.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions for video foundation models:

1. Coordinating foundation models trained from different modalities, pretraining tasks, and architectures to obtain better video representations. They suggest model distillation, unifying pretraining objectives, and feature alignment as possible technical approaches. 

2. Achieving large-scale spatiotemporal analysis for long-term and complex video understanding tasks. The current video foundation models focus on short video clips, but understanding long videos and complex events remains an open challenge.

3. Integrating video foundation models with decision-making into embodied AI agents. This allows automated data collection and model training in a closed loop where the agent's experiences further improve the model. The authors showed promising results on vision-language navigation and suggest this is a promising direction.

4. Studying the broader societal impacts of video foundation models such as bias, risks, fairness, and equality. As the models are trained on web-scale unlabeled video data, evaluating their social impacts is an important direction.

In summary, the main future directions are 1) better coordinating and transferring between different foundation models, 2) long-term video understanding, 3) integration with embodied agents, and 4) studying social impacts. Advancing in these directions can further improve the versatility, scalability, and generality of video foundation models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents InternVideo, a general video foundation model for understanding various video tasks. The model combines masked video modeling (VideoMAE) and video-language contrastive learning, taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo employs efficient masked video reconstruction and video-text contrastive pretraining objectives, and selectively coordinates the video representations from these two frameworks in a learnable manner to boost video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets across extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Results on benchmarks like Kinetics-400 (91.1\% top-1) and Something-Something V2 (77.2\%) demonstrate its effectiveness. Overall, InternVideo delivers the best performance on diverse video tasks compared to prior specialized and foundation models, showing its generality for video understanding.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper: 

The paper proposes InternVideo, a general video foundation model for video understanding. InternVideo uses both generative and discriminative self-supervised learning for video representation learning. It employs masked video modeling and video-language contrastive learning as pretraining objectives. To efficiently combine these two frameworks, InternVideo first trains them separately, then freezes the backbones and uses cross-model attention modules to align and fuse their representations in a learnable manner. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets across extensive tasks including video action recognition, video-language alignment, and open-world video applications.

The key contributions are: 1) A unified video representation learning paradigm that combines the strengths of masked modeling and contrastive learning in a computationally tractable way. 2) Scalability studies showing masked video encoder can be scaled up in model and data size. 3) Local temporal and global spatiotemporal modules to reuse pretrained image transformers for video. 4) State-of-the-art results across action, language, and open tasks demonstrate the versatility of InternVideo. The model efficiency, strong performance on a comprehensive benchmark, and openness make InternVideo an impactful baseline for video foundation models.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a general video foundation model called InternVideo through both generative and discriminative self-supervised video learning. Specifically, InternVideo employs masked video modeling (VideoMAE) and video-language contrastive learning as two complementary pretraining objectives. It uses a Vision Transformer (ViT) for VideoMAE to reconstruct highly masked video inputs for spatiotemporal modeling. For video-language contrastive learning, it extends ViT with additional local and global spatiotemporal modules and leverage image-text CLIP for initialization. After pretraining VideoMAE and the contrastive model separately, InternVideo freezes both backbones and only updates lightweight interaction modules to align their representations under supervision. Through this unified representation learning paradigm, InternVideo achieves state-of-the-art performance on a wide range of video understanding tasks, including action recognition, temporal/spatiotemporal action localization, video retrieval, video question answering, etc. The results demonstrate the effectiveness and generalization ability of InternVideo as a video foundation model.


## What problem or question is the paper addressing?

 The paper is addressing the limited generalization of existing video foundation models to a broad range of downstream tasks. The key issues it identifies are:

- Most existing video foundation models focus only on either action understanding tasks or video-language alignment tasks. They do not show strong performance across a diverse set of tasks.

- Current benchmarks for evaluating video foundation models are narrow and do not sufficiently measure generalization capabilities.

- Training large-scale video foundation models is computationally demanding. Existing methods have not sufficiently explored efficient training paradigms. 

To address these limitations, the paper proposes InternVideo, a versatile and efficient video foundation model that achieves state-of-the-art performance on a wide spectrum of downstream tasks. The key ideas are:

- Employ both masked video modeling and video-language contrastive learning as complementary self-supervised objectives.

- Unify representations from the two frameworks via cross-model interaction learning.

- Enable efficient training by reusing image-pretrained models and only updating lightweight adapter modules.

- Construct a systematic benchmark covering action, language, and open-world video tasks to measure generalization.

In summary, the paper aims to develop a video foundation model with broad generalization capabilities across diverse video tasks, while remaining efficient to train. This is achieved through an effective combination of self-supervised objectives and efficient model training strategies. The generalization is measured on a comprehensive benchmark suite.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Video foundation models - The paper focuses on developing general video foundation models for a variety of video understanding tasks. This is a key concept.

- InternVideo - The name of the proposed video foundation model.

- Masked video modeling - One of the self-supervised pretraining objectives used, based on reconstructing heavily masked video inputs. 

- Video-language contrastive learning - The other main self-supervised pretraining approach used, involving aligning video and text representations.

- Unified video representation (UVR) - The paper proposes learning a unified video representation by combining the representations from masked video modeling and video-language contrastive learning.

- Cross-model attention - A technique proposed to align the representations from the two self-supervised approaches by adding cross-attention modules.

- Action understanding - One of the three main video understanding capabilities evaluated, involving action recognition and spatiotemporal action localization tasks.

- Video-language alignment - The second capability evaluated, including video retrieval, video question answering, etc. 

- Video open understanding - The third capability, involving zero-shot recognition, open-set recognition, etc.

- Scalability - One focus is developing efficient and scalable video foundation models.

- Generalization - A core goal is developing models with strong generalization and transfer capabilities.
