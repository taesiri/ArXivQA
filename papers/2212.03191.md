# [InternVideo: General Video Foundation Models via Generative and   Discriminative Learning](https://arxiv.org/abs/2212.03191)

## What is the central research question or hypothesis that this paper addresses?

 The central goal of this paper is to develop a general video foundation model called InternVideo that can achieve strong performance across a wide range of video understanding tasks. The key ideas are:

- Explore both masked video modeling (VideoMAE) and video-language contrastive learning as complementary self-supervised objectives for pretraining video encoders. 

- Unify the representations from VideoMAE and contrastive learning encoders via cross-model interaction.

- Validate the model on a systematic video understanding benchmark with 39 datasets covering action understanding, video-language alignment, and open-world video tasks.

The main hypothesis is that by combining masked video modeling and contrastive learning in a unified framework, the model can learn more generalized video representations that transfer well to diverse downstream tasks. The paper aims to demonstrate the effectiveness and versatility of InternVideo as a general video foundation model through extensive experiments.

In summary, the central goal is developing a unified and general video foundation model that can achieve new state-of-the-art across a wide range of video tasks, which is demonstrated through comprehensive experiments. The key ideas are combining masked video modeling and contrastive learning in a unified framework.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes InternVideo, a general video foundation model that achieves state-of-the-art performance on a wide range of video understanding tasks including action recognition, video-language alignment, and open-world video applications. 

2. It explores both masked video modeling and video-language contrastive learning as complementary pretraining objectives for learning generic video representations. It proposes a unified video representation learning paradigm that combines the strengths of both through cross-model interaction.

3. It demonstrates the scalability of masked video modeling (VideoMAE) in model size and data scale through experiments. It also shows how to efficiently reuse image-pretrained vision transformer models like CLIP for video-language contrastive learning.

4. It constructs a systematic video understanding benchmark with 10 tasks and 39 datasets to evaluate the generalization capability of video foundation models. InternVideo outperforms previous state-of-the-art methods by a large margin on nearly all datasets and tasks.

5. It provides efficient training strategies and recipes for developing large-scale video foundation models. The total GPU hours needed is much less than previous models like CoCa.

In summary, this paper proposes InternVideo, an efficient and effective general video foundation model that achieves new state-of-the-art results on a diverse set of video understanding tasks through unified representation learning. It demonstrates the promise of foundation models for general video perception.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes InternVideo, a general video foundation model for a variety of video understanding tasks, which efficiently explores both masked video modeling and video-language contrastive learning for representation learning and achieves state-of-the-art performance on extensive video datasets.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to prior work in video foundation models:

- It proposes InternVideo, which is the first video foundation model to achieve state-of-the-art performance across a diverse set of tasks including action understanding, video-language alignment, and open-world video applications. Previous models tended to focus on only one or two of these areas.

- The model combines strengths from both masked video modeling (VideoMAE) and multimodal contrastive learning. Most prior work focused on just one approach. By combining them and allowing cross-representation learning, InternVideo gets improved performance.

- The results on Kinetics-400 (91.1%) and Something-Something v2 (77.2%) are state-of-the-art for those challenging action recognition benchmarks. The model also achieves SOTA on many other datasets across temporal action localization, spatiotemporal action localization, video retrieval, etc.

- The training of InternVideo seems quite efficient compared to some other recent models like CoCa. It requires much less GPU hours to train while still achieving better overall performance.

- The paper proposes a more comprehensive video understanding benchmark compared to what has typically been used to evaluate prior work. This benchmark helps demonstrate the generalization of InternVideo across a wide variety of video tasks.

- The strong zero-shot and open-set performance of InternVideo on tasks like retrieval and action recognition also highlight its ability to generalize beyond the training data.

In summary, this paper pushes forward video foundation models by achieving state-of-the-art, multi-task performance in an efficient training framework. The combination of techniques and more comprehensive benchmark help highlight the generalization compared to prior work.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions for video foundation models:

1. Coordinating foundation models trained from different modalities, pretraining tasks, and architectures to obtain better video representations. They suggest model distillation, unifying pretraining objectives, and feature alignment as possible technical approaches. 

2. Achieving large-scale spatiotemporal analysis for long-term and complex video understanding tasks. The current video foundation models focus on short video clips, but understanding long videos and complex events remains an open challenge.

3. Integrating video foundation models with decision-making into embodied AI agents. This allows automated data collection and model training in a closed loop where the agent's experiences further improve the model. The authors showed promising results on vision-language navigation and suggest this is a promising direction.

4. Studying the broader societal impacts of video foundation models such as bias, risks, fairness, and equality. As the models are trained on web-scale unlabeled video data, evaluating their social impacts is an important direction.

In summary, the main future directions are 1) better coordinating and transferring between different foundation models, 2) long-term video understanding, 3) integration with embodied agents, and 4) studying social impacts. Advancing in these directions can further improve the versatility, scalability, and generality of video foundation models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents InternVideo, a general video foundation model for understanding various video tasks. The model combines masked video modeling (VideoMAE) and video-language contrastive learning, taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo employs efficient masked video reconstruction and video-text contrastive pretraining objectives, and selectively coordinates the video representations from these two frameworks in a learnable manner to boost video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets across extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Results on benchmarks like Kinetics-400 (91.1\% top-1) and Something-Something V2 (77.2\%) demonstrate its effectiveness. Overall, InternVideo delivers the best performance on diverse video tasks compared to prior specialized and foundation models, showing its generality for video understanding.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper: 

The paper proposes InternVideo, a general video foundation model for video understanding. InternVideo uses both generative and discriminative self-supervised learning for video representation learning. It employs masked video modeling and video-language contrastive learning as pretraining objectives. To efficiently combine these two frameworks, InternVideo first trains them separately, then freezes the backbones and uses cross-model attention modules to align and fuse their representations in a learnable manner. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets across extensive tasks including video action recognition, video-language alignment, and open-world video applications.

The key contributions are: 1) A unified video representation learning paradigm that combines the strengths of masked modeling and contrastive learning in a computationally tractable way. 2) Scalability studies showing masked video encoder can be scaled up in model and data size. 3) Local temporal and global spatiotemporal modules to reuse pretrained image transformers for video. 4) State-of-the-art results across action, language, and open tasks demonstrate the versatility of InternVideo. The model efficiency, strong performance on a comprehensive benchmark, and openness make InternVideo an impactful baseline for video foundation models.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a general video foundation model called InternVideo through both generative and discriminative self-supervised video learning. Specifically, InternVideo employs masked video modeling (VideoMAE) and video-language contrastive learning as two complementary pretraining objectives. It uses a Vision Transformer (ViT) for VideoMAE to reconstruct highly masked video inputs for spatiotemporal modeling. For video-language contrastive learning, it extends ViT with additional local and global spatiotemporal modules and leverage image-text CLIP for initialization. After pretraining VideoMAE and the contrastive model separately, InternVideo freezes both backbones and only updates lightweight interaction modules to align their representations under supervision. Through this unified representation learning paradigm, InternVideo achieves state-of-the-art performance on a wide range of video understanding tasks, including action recognition, temporal/spatiotemporal action localization, video retrieval, video question answering, etc. The results demonstrate the effectiveness and generalization ability of InternVideo as a video foundation model.


## What problem or question is the paper addressing?

 The paper is addressing the limited generalization of existing video foundation models to a broad range of downstream tasks. The key issues it identifies are:

- Most existing video foundation models focus only on either action understanding tasks or video-language alignment tasks. They do not show strong performance across a diverse set of tasks.

- Current benchmarks for evaluating video foundation models are narrow and do not sufficiently measure generalization capabilities.

- Training large-scale video foundation models is computationally demanding. Existing methods have not sufficiently explored efficient training paradigms. 

To address these limitations, the paper proposes InternVideo, a versatile and efficient video foundation model that achieves state-of-the-art performance on a wide spectrum of downstream tasks. The key ideas are:

- Employ both masked video modeling and video-language contrastive learning as complementary self-supervised objectives.

- Unify representations from the two frameworks via cross-model interaction learning.

- Enable efficient training by reusing image-pretrained models and only updating lightweight adapter modules.

- Construct a systematic benchmark covering action, language, and open-world video tasks to measure generalization.

In summary, the paper aims to develop a video foundation model with broad generalization capabilities across diverse video tasks, while remaining efficient to train. This is achieved through an effective combination of self-supervised objectives and efficient model training strategies. The generalization is measured on a comprehensive benchmark suite.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Video foundation models - The paper focuses on developing general video foundation models for a variety of video understanding tasks. This is a key concept.

- InternVideo - The name of the proposed video foundation model.

- Masked video modeling - One of the self-supervised pretraining objectives used, based on reconstructing heavily masked video inputs. 

- Video-language contrastive learning - The other main self-supervised pretraining approach used, involving aligning video and text representations.

- Unified video representation (UVR) - The paper proposes learning a unified video representation by combining the representations from masked video modeling and video-language contrastive learning.

- Cross-model attention - A technique proposed to align the representations from the two self-supervised approaches by adding cross-attention modules.

- Action understanding - One of the three main video understanding capabilities evaluated, involving action recognition and spatiotemporal action localization tasks.

- Video-language alignment - The second capability evaluated, including video retrieval, video question answering, etc. 

- Video open understanding - The third capability, involving zero-shot recognition, open-set recognition, etc.

- Scalability - One focus is developing efficient and scalable video foundation models.

- Generalization - A core goal is developing models with strong generalization and transfer capabilities.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to summarize the key points of this paper:

1. What is the motivation and goal of the work? 

2. What are the main contributions of the paper?

3. What is the proposed model InternVideo and what are its key components? 

4. How does InternVideo learn the unified video representations? What are the self-supervised and supervised pretraining strategies used?

5. What are the model architectures and implementation details of InternVideo?

6. What is the proposed video understanding benchmark used for evaluation? What are the three categories of tasks covered?

7. How does InternVideo perform on action understanding tasks like action recognition and temporal action localization?

8. How does InternVideo perform on video-language alignment tasks like video retrieval, video question answering and visual language navigation? 

9. How does InternVideo perform on open video understanding tasks like zero-shot action recognition and open-set action recognition?

10. What are the limitations, future work and broader impact discussed in the paper?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a unified video representation (UVR) learning paradigm that combines masked video modeling and multimodal contrastive learning. How does coordinating these two complementary frameworks help improve video understanding compared to using just one framework? What are the limitations of each framework that are addressed by combining them?

2. The paper finds that masked video modeling with autoencoders is scalable in model size and training data. What modifications were made to enable this scalability compared to previous autoencoder-based models like MAE? How does increasing model scale and data improve video representations for downstream tasks?

3. The paper reuses a pretrained image Transformer for video by adding local and global spatiotemporal interaction modules. Why is it beneficial to reuse image models instead of training from scratch for video? How do the local and global modules capture spatial and temporal interactions effectively?

4. The paper shows supervised action recognition acts as an effective intermediate task before final model tuning on downstream datasets. Why does this improve transfer learning performance compared to just self-supervised pretraining? Does the choice of intermediate task matter significantly?

5. The cross-model attention modules are used to align the representations from masked modeling and multimodal learning. Why is this cross-attention alignment more effective than jointly training the objectives? Does freezing the pretrained backbones matter for this technique?

6. The model achieves state-of-the-art performance on a wide variety of video tasks. For which categories of tasks (action, video-language, open world) does it show the biggest improvements over prior work? Why might this be the case?

7. The model training complexity is claimed to be lower than prior large video models like CoCa. What design choices contribute to the improved efficiency? Is there a tradeoff between efficiency and model performance?

8. The paper constructs a systematic video understanding benchmark with 39 datasets covering diverse tasks. How does the benchmark highlight model capabilities beyond just a few popular datasets like Kinetics? What are limitations of the benchmark for measuring general video understanding?

9. The model shows strong zero-shot transfer and open-set recognition capabilities. What properties of the training methodology enable these capabilities? How do the results demonstrate increased model generalization?

10. What opportunities exist for future work to build upon this approach? For example, extending long-term spatiotemporal reasoning, combining with embodied AI, addressing model biases, etc.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes InternVideo, a versatile and efficient video foundation model that achieves state-of-the-art performance on a wide range of video understanding tasks. The model employs both masked video modeling (VideoMAE) and video-language contrastive learning for representation learning. It then conducts cross-model interaction between the two frameworks to derive a unified video representation that combines their complementary strengths. InternVideo outperforms previous specialized and foundation models on extensive benchmarks covering action recognition, temporal localization, video-text retrieval, question answering, etc. It demonstrates superior generalizability via strong zero-shot transfer and open-world capabilities. The model is also efficient, requiring much less compute than prior arts. InternVideo sets new records on nearly 40 datasets and establishes strong baselines for video perception. The cross-model learning paradigm enables coordinating diverse self-supervised objectives and model architectures to advance foundation research.


## Summarize the paper in one sentence.

 InternVideo introduces a video foundation model via unified representation learning of masked modeling and multimodal contrastive learning, achieving state-of-the-art performance on a wide range of video understanding tasks.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points in this paper:

This paper proposes InternVideo, a general video foundation model that achieves state-of-the-art performance on extensive video understanding tasks including action recognition, temporal action localization, video-language alignment, and open-world applications. InternVideo conducts both masked video modeling and multimodal contrastive learning for self-supervised pretraining. It then employs supervised action classification training to enhance the video representations. To combine the strengths of the masking and contrastive learning approaches, InternVideo introduces cross-model attention modules to dynamically aggregate features from both frameworks. Without bells and whistles, InternVideo sets new records on 39 datasets across 10 video tasks, demonstrating its versatility as a video foundation model. The unified video representation learning paradigm and computational efficiency of InternVideo also showcase a feasible approach to developing effective and scalable video foundation models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the video foundation model InternVideo proposed in this paper:

1. The paper proposes to use both masked video modeling and video-language contrastive learning for pretraining InternVideo. What are the motivations and benefits of combining these two self-supervised learning approaches? How do they complement each other?

2. The paper uses a VideoMAE model for masked video modeling. How is VideoMAE designed and optimized to capture spatiotemporal representations from videos? What innovations did the authors make to improve its efficiency and scalability?

3. For video-language contrastive learning, the paper leverages an existing image-text pre-trained model CLIP. How did the authors adapt and extend CLIP's architecture for encoding videos and aligning video-text embeddings? What was the motivation behind this design?

4. The authors claim video masked modeling and video-language contrastive learning produce complementary features. What evidence or analysis supports this claim? How exactly are the learned features complementary?

5. After self-supervised pretraining, the model is further trained with supervised video classification. What is the motivation for this additional supervised step? How does it enhance InternVideo's capabilities?

6. Explain the cross-model interaction module proposed to combine the video masked modeling and video-language contrastive learning frameworks. Why is this method used instead of joint training?

7. The authors evaluate InternVideo extensively on downstream tasks. What capabilities or strengths of the model do the different tasks aim to evaluate? Is the benchmark sufficiently comprehensive to demonstrate video understanding abilities?

8. Analyze the results on the various downstream tasks. On which tasks does InternVideo achieve the most significant improvements over prior work? What does this reveal about the model?

9. What strategies, techniques or innovations contributed most to InternVideo's superior performance over previous models? Are there any clear limitations?

10. The authors mention future work on combining foundation models with decision-making for embodied AI tasks. Elaborate on how InternVideo could be integrated into an embodied agent. What challenges need to be addressed?
