# [FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph   Completion](https://arxiv.org/abs/2312.10645)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion":

Problem:
- Knowledge graphs (KGs) are incomplete and multilingual KGs can have complementary knowledge to improve completion. 
- Prior multilingual KGC methods rely on aligned entities across KGs, but obtaining alignments is laborious and risks privacy violations from raw data sharing.

Proposed Solution:
- Federated learning framework (FedMKGC) that aggregates knowledge from multilingual KGs without demanding raw data sharing or alignment annotations.
- Each KG is a separate client that trains a local language model via text-based knowledge representation learning. This embeds knowledge into the model weights.
- A central server aggregates the local model weights to consolidate complementary knowledge across KGs in an integrated language model.  

Main Contributions:
- Federated learning approach to multilingual KGC that preserves privacy and eliminates need for alignments.
- Text-based knowledge representation method combined with contrastive learning to embed knowledge into language model parameters.
- Experiments on benchmark dataset show FedMKGC substantially improves KGC on multilingual KGs, achieving comparable performance to alignment-based models without needing any alignments or raw data sharing.

In summary, this paper introduces a novel federated learning framework for aggregating knowledge from multiple language-specific knowledge graphs in a privacy-preserving manner, without requiring manual alignment annotations or raw data sharing across graphs. This is achieved by having each graph learn text-based knowledge representations locally using contrastive learning, and aggregating the resulting model weights centrally.
