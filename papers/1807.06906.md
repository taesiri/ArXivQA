# [Towards Automated Deep Learning: Efficient Joint Neural Architecture and
  Hyperparameter Search](https://arxiv.org/abs/1807.06906)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently perform joint neural architecture and hyperparameter search. 

Specifically, the paper makes the following key points:

- Existing neural architecture search (NAS) methods optimize architectures with fixed hyperparameters using a small number of epochs, and only optimize hyperparameters afterwards with a larger number of epochs. The paper argues this is suboptimal because architectures and hyperparameters interact and should be optimized together.

- The paper shows there is little correlation between the relative performance ranking of architectures on a small vs large number of epochs. So optimizing on few epochs does not necessarily find the best architectures for many epochs.

- To address these issues, the paper proposes using a combination of Bayesian optimization and Hyperband to efficiently perform joint architecture and hyperparameter search. This allows optimizing both simultaneously in an any-time fashion while gradually increasing the training budget.

- Experiments on CIFAR-10 show this approach can find a competitive architecture and hyperparameters within a constrained 3 hour time budget. The resulting 3.18% test error outperforms various standard architectures trained in the same setting.

In summary, the key hypothesis is that joint architecture and hyperparameter search is superior to the conventional decoupled approach, and this can be achieved efficiently using the proposed Bayesian optimization and Hyperband method. The experiments lend support to this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an efficient method for joint neural architecture and hyperparameter search. Specifically:

- They show how to formulate neural architecture search (NAS) as a hyperparameter optimization problem, allowing joint optimization of architecture and other hyperparameters.

- They demonstrate that optimizing architecture and hyperparameters separately can be suboptimal compared to joint optimization. 

- They point out issues with the common practice in NAS of using few epochs during the main search phase and many more epochs for evaluating the final architecture. Specifically, they show there is little correlation between rankings after short vs long training, making the short runs ineffective. 

- To address these issues, they propose using a combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search. This allows gradually increasing the budget during optimization to avoid the ranking issues.

- They demonstrate their proposed method can achieve competitive performance on CIFAR-10 under a time constraint (3 hours max per configuration), outperforming various standard architectures. 

- They analyze the joint architecture/hyperparameter space and show the choices interact closely with the time budget, validating the need for their joint optimization approach.

In summary, the key contribution is showing the benefit of joint architecture/hyperparameter optimization under time constraints compared to the typical separate NAS optimization. Their method of combining Bayesian optimization and Hyperband enables an efficient search over this joint space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes using a combination of Bayesian optimization and Hyperband to efficiently perform joint neural architecture and hyperparameter search, avoiding the common practice of separating architecture search and hyperparameter tuning.


## How does this paper compare to other research in the same field?

 This paper introduces a new approach to joint neural architecture and hyperparameter search using Bayesian optimization and Hyperband. Here are some key points in comparing it to other work:

- Most prior NAS methods separate architecture search from hyperparameter tuning, doing architecture search with fixed hyperparameters first and then tuning hyperparameters afterwards. In contrast, this paper proposes doing joint architecture and hyperparameter search.

- The paper argues that optimizing architectures and hyperparameters separately can be suboptimal compared to joint optimization. They provide empirical evidence showing hyperparameters and architecture choices interact and influence each other.

- The proposed method uses Bayesian optimization and Hyperband to efficiently search the joint space of architectures and hyperparameters in an anytime fashion. This is different from many NAS methods that use reinforcement learning, evolutionary algorithms, or gradient-based methods.

- The paper shows the proposed joint optimization method can find architectures competitive with standard networks in a constrained time budget. Many NAS methods require very large computational budgets.

- An analysis in the paper reveals how optimal architecture choices and hyperparameters vary significantly depending on the training budget. This suggests optimizing on a small budget and evaluating on a larger one may be inefficient.

- Compared to some other Bayesian optimization works for NAS, this paper considers a relatively large joint search space with 17 dimensions. Many other BO for NAS papers tune a smaller space.

In summary, the key novelty is the joint optimization of neural architectures and hyperparameters in an efficient anytime manner, compared to the dominant paradigm of separate optimization. The analysis also provides insights about interactions between architectures, hyperparameters, and training budget.


## What future research directions do the authors suggest?

 The paper suggests a few potential directions for future research:

1. Designing more generic and flexible architecture search spaces that do not limit the network representation to a fixed-length vector. The authors used a relatively simple search space in this work due to limited compute budget. Exploring more complex and unrestricted spaces could lead to discovering better architectures.

2. Applying the joint architecture and hyperparameter search approach to other tasks beyond image classification, such as object detection, semantic segmentation, etc. The method is general and could be beneficial in many domains.

3. Exploring how the importance of architectural choices changes over time and optimizing the search strategy based on these insights. The functional ANOVA analysis in the paper showed the importance of some choices varies significantly between short and long budgets. Adaptively focusing the search on important choices could improve efficiency.

4. Developing better methods for incrementally increasing the budget during the search to avoid poor correlation between short and long training times. The paper demonstrated this issue exists, but more advanced adaptive schedule methods could help. 

5. Scaling up the experiments for larger architectures, datasets, and compute budgets. The proof-of-concept was limited to 3 hours on CIFAR-10. Testing on larger problems could further demonstrate the value of joint optimization.

6. Comparing the joint optimization approach to other NAS techniques to determine if it provides benefits in sample efficiency and performance over alternatives.

In summary, the main future directions are developing more flexible search spaces, applying the method to new domains, adapting the search strategy based on choice importances, investigating better budget schedules, scaling up the experiments, and comparing to other leading NAS approaches. The joint optimization concept shows promise but further work is needed to extend and evaluate it.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes using Bayesian Optimization and Hyperband (BOHB) for efficient joint neural architecture and hyperparameter search. The authors argue that typical neural architecture search methods have two deficiencies: (1) they optimize architectures and hyperparameters separately, which can be suboptimal compared to joint optimization, and (2) they use a small number of epochs during the main search phase and a much larger number during post-processing, even though there is little correlation between performance on these two training regimes. To address these issues, they propose using BOHB, which performs joint architecture and hyperparameter optimization in an anytime fashion while gradually increasing the compute budget for promising configurations. Empirically, they show on CIFAR-10 that joint optimization with BOHB yields better performance within a 3 hour time budget compared to optimizing architectures and hyperparameters separately. They also analyze the search space across different budgets and find that architectural choices and hyperparameters interact closely with the runtime budget.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes an efficient method to jointly optimize neural architectures and hyperparameters using Bayesian optimization and Hyperband. Current neural architecture search methods tune hyperparameters separately after finding architectures, which is inefficient. The authors show empirically that optimizing architectures and hyperparameters jointly is better, and that performance on small budgets has little correlation with performance on large budgets. To address these issues, they propose jointly optimizing architectures and hyperparameters using Bayesian optimization Hyperband (BOHB). BOHB incrementally evaluates promising configurations on progressively larger budgets, avoiding wasted computations from separately tuning hyperparameters or only evaluating on small budgets. 

The authors demonstrate their method on a convolutional architecture search space for CIFAR-10. With only 3 hours of training per configuration, their method achieves 3.18% test error, outperforming manually designed networks. Analyses reveal architecture choices and hyperparameters are indeed very sensitive to the training budget. Importantly, there is little correlation between performance on short 400 second budgets versus long 3 hour budgets. This motivates BOHB's incremental budget increases. In summary, the proposed joint architecture and hyperparameter optimization with BOHB provides an efficient neural architecture search method.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes an efficient method for joint neural architecture and hyperparameter search based on a combination of Bayesian optimization and Hyperband. The key points are:

- They cast neural architecture search as a hyperparameter optimization problem by representing architectural choices like number of layers, filters etc. as categorical hyperparameters. This enables joint optimization of architecture and standard hyperparameters like learning rate in a unified search space.

- They use a combination of Bayesian optimization and Hyperband called BOHB to efficiently optimize this joint search space. BOHB uses successive halving and adaptive resource allocation to evaluate promising configurations on progressively larger budgets, starting from a small budget like a few epochs. The Bayesian optimization component models the search space to sample promising configurations instead of random selection.

- They demonstrate that optimizing architecture and hyperparameters separately leads to suboptimal performance compared to joint optimization with BOHB. They also show there is weak correlation between rankings after short (few epochs) and long training, so NAS methods just optimizing on small budgets may be ineffective. 

- In experiments on CIFAR-10 under a constrained 3 hour time budget, joint architecture-hyperparameter optimization with BOHB discovers a network that outperforms standard networks like ResNet-18 trained with the same pipeline. The paper analyzes how architectural choices interact with the training budget.

In summary, the key idea is to frame NAS as joint hyperparameter optimization and use BOHB to efficiently search this space in an anytime fashion with increasing budgets. The experiments highlight the benefits of this approach under limited training time.


## What problem or question is the paper addressing?

 This paper addresses the problem of efficiently performing neural architecture search (NAS) along with hyperparameter optimization. The key problems and questions it tackles are:

- Existing NAS methods optimize architectures with fixed hyperparameters, then optimize hyperparameters separately in a post-processing step. This is suboptimal, as architectural choices and hyperparameters interact. The paper demonstrates the need for joint architecture and hyperparameter optimization.

- NAS methods typically search architectures using few epochs (e.g. 20), then retrain the best architecture from scratch with far more epochs (e.g. 600). This is inefficient, as there is little correlation between performance rankings after the short vs long training. 

- NAS methods are computationally very expensive, requiring hundreds or thousands of GPUs. The paper aims to develop an efficient joint neural architecture and hyperparameter search method.

The main contributions are:

- Demonstrating the need for joint architecture and hyperparameter optimization, rather than separating them.

- Showing empirically that short and long training performance is poorly correlated, making typical NAS practices inefficient.

- Proposing the use of a combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.

- Achieving strong performance on CIFAR-10 within a constrained 3 hour time budget by means of this joint optimization approach.

In summary, the paper offers an efficient framework for simultaneously optimizing neural architectures and hyperparameters that avoids common inefficiencies of standard NAS methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Neural architecture search (NAS) - The paper focuses on automatically searching for good neural network architectures.

- Hyperparameter optimization - The paper proposes doing joint optimization of neural architectures and other hyperparameters like learning rate, instead of separating the architecture search and hyperparameter tuning steps. 

- Bayesian optimization - The method used for joint architecture and hyperparameter search is based on Bayesian optimization.

- Anytime prediction - The proposed joint search method can make predictions at any time during the search, satisfying this requirement for AutoML systems.

- Training budgets - The paper examines how neural architectures and hyperparameters interact with the training budget or compute resources used. It uses different budgets for an efficient search.

- CIFAR-10 - The experiments are done on the CIFAR-10 image classification benchmark.

Other keywords: Hyperband, kernel density estimators, Shake-Shake regularization, validation error correlation.

In summary, the key focus is on efficient joint neural architecture and hyperparameter search using Bayesian optimization and budgets. Evaluations on CIFAR-10 demonstrate the efficiency of this approach.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the key problem addressed in the paper?

2. What previous methods does the paper build upon? 

3. What are the main limitations or drawbacks of the previous methods?

4. What is the main contribution or proposed method in the paper? 

5. How does the proposed method aim to improve upon previous methods?

6. What experiments were conducted to evaluate the proposed method?

7. What were the main results of the experiments? 

8. How does the performance of the proposed method compare to previous methods?

9. What are the limitations or potential weaknesses of the proposed method?

10. What directions for future work does the paper suggest?

Asking these types of questions should help elicit the key information needed to understand the motivations, contributions, and results of the paper. The goal is to identify the core problem, prior work, proposed solution, experiments, results, and limitations to generate a concise yet comprehensive summary. Further probing questions could also be asked on specific details of the methods and results.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper argues that optimizing architectures and hyperparameters separately yields suboptimal results compared to a joint optimization process. What evidence do they provide to support this claim? How convincing is this evidence?

2. The paper proposes using Bayesian Optimization Hyperband (BOHB) for efficient joint neural architecture and hyperparameter search. How exactly does BOHB work? What are the key components that enable it to perform well in this joint search space?

3. The paper demonstrates weak correlation between performance on small and large training budgets. Why does this occur? How does the budget schedule used in BOHB help address this issue?

4. What neural architecture search spaces does the paper experiment with? How complex and flexible are these spaces compared to prior NAS work? Are there limitations to the architectures that can be represented?

5. How does the paper formulate neural architecture search as a hyperparameter optimization problem? What are the key architectural choices that are mapped to hyperparameters?

6. The paper achieves competitive performance on CIFAR-10 within a constrained 3 hour budget. How does this compare to other common network architectures optimized in the same setting? What factors contribute to this result?

7. What does the paper's analysis of the search space using fANOVA reveal about the importance of various architectural choices and hyperparameters? How does this change across different budgets?

8. Shake-Shake regularization appears important in the paper's experiments. How does it work? Why does the paper hypothesize it prevents fast optimization in the constrained setting?

9. How thoroughly does the paper evaluate the proposed joint optimization approach? What other datasets, search spaces, and training budgets could be explored as future work?

10. The paper argues that neural architecture search methods overly focus on architecture and neglect hyperparameters. Based on the evidence presented, how convinced are you of this claim? What counter-arguments could be made?
