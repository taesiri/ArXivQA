# [Towards Automated Deep Learning: Efficient Joint Neural Architecture and   Hyperparameter Search](https://arxiv.org/abs/1807.06906)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently perform joint neural architecture and hyperparameter search. Specifically, the paper makes the following key points:- Existing neural architecture search (NAS) methods optimize architectures with fixed hyperparameters using a small number of epochs, and only optimize hyperparameters afterwards with a larger number of epochs. The paper argues this is suboptimal because architectures and hyperparameters interact and should be optimized together.- The paper shows there is little correlation between the relative performance ranking of architectures on a small vs large number of epochs. So optimizing on few epochs does not necessarily find the best architectures for many epochs.- To address these issues, the paper proposes using a combination of Bayesian optimization and Hyperband to efficiently perform joint architecture and hyperparameter search. This allows optimizing both simultaneously in an any-time fashion while gradually increasing the training budget.- Experiments on CIFAR-10 show this approach can find a competitive architecture and hyperparameters within a constrained 3 hour time budget. The resulting 3.18% test error outperforms various standard architectures trained in the same setting.In summary, the key hypothesis is that joint architecture and hyperparameter search is superior to the conventional decoupled approach, and this can be achieved efficiently using the proposed Bayesian optimization and Hyperband method. The experiments lend support to this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an efficient method for joint neural architecture and hyperparameter search. Specifically:- They show how to formulate neural architecture search (NAS) as a hyperparameter optimization problem, allowing joint optimization of architecture and other hyperparameters.- They demonstrate that optimizing architecture and hyperparameters separately can be suboptimal compared to joint optimization. - They point out issues with the common practice in NAS of using few epochs during the main search phase and many more epochs for evaluating the final architecture. Specifically, they show there is little correlation between rankings after short vs long training, making the short runs ineffective. - To address these issues, they propose using a combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search. This allows gradually increasing the budget during optimization to avoid the ranking issues.- They demonstrate their proposed method can achieve competitive performance on CIFAR-10 under a time constraint (3 hours max per configuration), outperforming various standard architectures. - They analyze the joint architecture/hyperparameter space and show the choices interact closely with the time budget, validating the need for their joint optimization approach.In summary, the key contribution is showing the benefit of joint architecture/hyperparameter optimization under time constraints compared to the typical separate NAS optimization. Their method of combining Bayesian optimization and Hyperband enables an efficient search over this joint space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using a combination of Bayesian optimization and Hyperband to efficiently perform joint neural architecture and hyperparameter search, avoiding the common practice of separating architecture search and hyperparameter tuning.


## How does this paper compare to other research in the same field?

This paper introduces a new approach to joint neural architecture and hyperparameter search using Bayesian optimization and Hyperband. Here are some key points in comparing it to other work:- Most prior NAS methods separate architecture search from hyperparameter tuning, doing architecture search with fixed hyperparameters first and then tuning hyperparameters afterwards. In contrast, this paper proposes doing joint architecture and hyperparameter search.- The paper argues that optimizing architectures and hyperparameters separately can be suboptimal compared to joint optimization. They provide empirical evidence showing hyperparameters and architecture choices interact and influence each other.- The proposed method uses Bayesian optimization and Hyperband to efficiently search the joint space of architectures and hyperparameters in an anytime fashion. This is different from many NAS methods that use reinforcement learning, evolutionary algorithms, or gradient-based methods.- The paper shows the proposed joint optimization method can find architectures competitive with standard networks in a constrained time budget. Many NAS methods require very large computational budgets.- An analysis in the paper reveals how optimal architecture choices and hyperparameters vary significantly depending on the training budget. This suggests optimizing on a small budget and evaluating on a larger one may be inefficient.- Compared to some other Bayesian optimization works for NAS, this paper considers a relatively large joint search space with 17 dimensions. Many other BO for NAS papers tune a smaller space.In summary, the key novelty is the joint optimization of neural architectures and hyperparameters in an efficient anytime manner, compared to the dominant paradigm of separate optimization. The analysis also provides insights about interactions between architectures, hyperparameters, and training budget.


## What future research directions do the authors suggest?

The paper suggests a few potential directions for future research:1. Designing more generic and flexible architecture search spaces that do not limit the network representation to a fixed-length vector. The authors used a relatively simple search space in this work due to limited compute budget. Exploring more complex and unrestricted spaces could lead to discovering better architectures.2. Applying the joint architecture and hyperparameter search approach to other tasks beyond image classification, such as object detection, semantic segmentation, etc. The method is general and could be beneficial in many domains.3. Exploring how the importance of architectural choices changes over time and optimizing the search strategy based on these insights. The functional ANOVA analysis in the paper showed the importance of some choices varies significantly between short and long budgets. Adaptively focusing the search on important choices could improve efficiency.4. Developing better methods for incrementally increasing the budget during the search to avoid poor correlation between short and long training times. The paper demonstrated this issue exists, but more advanced adaptive schedule methods could help. 5. Scaling up the experiments for larger architectures, datasets, and compute budgets. The proof-of-concept was limited to 3 hours on CIFAR-10. Testing on larger problems could further demonstrate the value of joint optimization.6. Comparing the joint optimization approach to other NAS techniques to determine if it provides benefits in sample efficiency and performance over alternatives.In summary, the main future directions are developing more flexible search spaces, applying the method to new domains, adapting the search strategy based on choice importances, investigating better budget schedules, scaling up the experiments, and comparing to other leading NAS approaches. The joint optimization concept shows promise but further work is needed to extend and evaluate it.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes using Bayesian Optimization and Hyperband (BOHB) for efficient joint neural architecture and hyperparameter search. The authors argue that typical neural architecture search methods have two deficiencies: (1) they optimize architectures and hyperparameters separately, which can be suboptimal compared to joint optimization, and (2) they use a small number of epochs during the main search phase and a much larger number during post-processing, even though there is little correlation between performance on these two training regimes. To address these issues, they propose using BOHB, which performs joint architecture and hyperparameter optimization in an anytime fashion while gradually increasing the compute budget for promising configurations. Empirically, they show on CIFAR-10 that joint optimization with BOHB yields better performance within a 3 hour time budget compared to optimizing architectures and hyperparameters separately. They also analyze the search space across different budgets and find that architectural choices and hyperparameters interact closely with the runtime budget.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes an efficient method to jointly optimize neural architectures and hyperparameters using Bayesian optimization and Hyperband. Current neural architecture search methods tune hyperparameters separately after finding architectures, which is inefficient. The authors show empirically that optimizing architectures and hyperparameters jointly is better, and that performance on small budgets has little correlation with performance on large budgets. To address these issues, they propose jointly optimizing architectures and hyperparameters using Bayesian optimization Hyperband (BOHB). BOHB incrementally evaluates promising configurations on progressively larger budgets, avoiding wasted computations from separately tuning hyperparameters or only evaluating on small budgets. The authors demonstrate their method on a convolutional architecture search space for CIFAR-10. With only 3 hours of training per configuration, their method achieves 3.18% test error, outperforming manually designed networks. Analyses reveal architecture choices and hyperparameters are indeed very sensitive to the training budget. Importantly, there is little correlation between performance on short 400 second budgets versus long 3 hour budgets. This motivates BOHB's incremental budget increases. In summary, the proposed joint architecture and hyperparameter optimization with BOHB provides an efficient neural architecture search method.
