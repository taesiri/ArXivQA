# [Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing](https://arxiv.org/abs/2306.17848)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Do ViTs have better "patch selectivity" than CNNs, meaning they are better able to ignore out-of-context image patches? 2) Can training CNNs with the proposed Patch Mixing data augmentation method improve their patch selectivity, making them more robust to occlusion?3) Is there a correlation between patch selectivity and robustness to natural occlusion? Specifically, the paper hypothesizes that:- ViTs can more easily discount signal from out-of-context patches due to their self-attention mechanisms, allowing them to handle occlusion better than CNNs (Hypothesis 1). - Models with better patch selectivity will tend to perform better under natural occlusion (Hypothesis 2).- Training models with Patch Mixing will improve their ability to ignore out-of-context information (Hypothesis 3).The experiments then aim to test these hypotheses by comparing ViTs and CNNs on patch replacement attacks, training networks with Patch Mixing, evaluating performance on occlusion datasets, and using the proposed c-RISE method to measure patch selectivity.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Identifying an important difference between CNNs and ViTs in terms of their ability to handle occlusion and ignore out-of-context image patches. The paper shows that ViTs are naturally better at this "patch selectivity", while CNNs struggle more.- Proposing Patch Mixing, a data augmentation method that inserts patches from other images into training images to simulate occlusion. The paper shows this improves CNNs' ability to ignore out-of-context patches, making them more like ViTs. - Introducing two new challenging occlusion datasets (SMD and ROD) to evaluate model performance under occlusion.- Developing a new explainability method called contrastive RISE (c-RISE) to visualize and quantify patch selectivity in both CNNs and ViTs.- Demonstrating through experiments that CNNs trained with Patch Mixing have improved robustness to occlusion on several benchmarks, while ViTs do not improve much since they already have good patch selectivity.In summary, the main contribution is identifying the "patch selectivity" difference between CNNs and ViTs, and developing the Patch Mixing technique to reduce this gap by improving CNNs' ability to ignore out-of-context image patches and handle occlusion. The paper provides empirical evidence and analysis to support these findings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Patch Mixing data augmentation to train CNNs to acquire ViT-like capabilities to ignore out-of-context image patches, making CNNs more robust to occlusion and bridging the gap between CNNs and ViTs in handling out-of-context information.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research on Vision Transformers and convolutional neural networks:- The main contribution of this paper is using Patch Mixing augmentation to train CNNs to have more ViT-like "patch selectivity" and be more robust to occlusion. This is a novel idea not explored in other works. Previous papers like Vaswani et al. 2021 and Naseer et al. 2021 studied differences between ViTs and CNNs but did not propose methods to make CNNs more ViT-like.- This paper thoroughly evaluates patch selectivity and occlusion robustness using several datasets. Other papers like Pinto et al. 2022 and Ruiz et al. 2022 studied robustness but were not focused specifically on occlusion. The new proposed datasets SMD and ROD seem uniquely challenging for studying occlusion.- The paper compares modern vision architectures like ConvNeXt and Swin Transformers. Many prior works studied older CNNs like ResNet, so the comparison to newer models is more relevant. The findings align with some prior works showing ViTs handle occlusion better, but offer new insights with modern architectures.- The c-RISE method for model explanation is a nice contribution for analyzing vision models in a modality-agnostic way. Other papers have used input modification like RISE for explanations, but c-RISE's contrastive approach is better suited for studying patch selectivity.Overall, I think this paper provides valuable new insights on training occlusion-robust CNNs by making them more ViT-like. The experiments are comprehensive and the ideas are well-motivated. It moves beyond just analyzing differences between CNNs and ViTs to actually improving CNNs. The results seem promising and impactful for the field.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions:- Investigate Patch Mixing with other architectures like ConvMixer and MobileNets. The authors focused on ConvNeXt and Swin Transformers in this work, but patch selectivity could be studied in other CNN and Transformer models.- Try different mixing strategies beyond random uniform patching. The authors use a simple random patch replacement strategy, but more intelligent approaches could be explored.- Develop new occlusion datasets and benchmarks. The authors propose SMD and ROD in this work, but additional datasets modeling real-world occlusion would enable more thorough evaluation.- Study the effect of Patch Mixing on other tasks like object detection and segmentation. This work focuses on image classification, but Patch Mixing could potentially improve robustness in other vision tasks. - Analyze how patch selectivity develops during training. The authors demonstrate improved patch selectivity after training with Patch Mixing, but the dynamics of how it evolves could be an interesting direction.- Investigate connections between patch selectivity and attention. The self-attention mechanism in Transformers may relate to their inherent patch selectivity.- Develop better quantitative metrics of patch selectivity. The authors propose using contrastive saliency maps, but more refined metrics could better capture this characteristic.- Explore how insights from Patch Mixing could transfer to other modalities like video, point clouds, etc. The concept of patch selectivity is not limited to images.In summary, the authors suggest further work on studying patch selectivity in other models and tasks, developing better datasets and metrics, and analyzing the underlying mechanisms that allow networks to ignore out-of-context signals. Advancing research in these areas could lead to more reliable computer vision systems.
