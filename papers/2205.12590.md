# RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText   Generators

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we imbue fine-grained interpretable control into long-form text generators using Rhetorical Structure Theory (RST)?The authors propose a framework called RSTGen that utilizes RST to control the discourse structure, semantics, and topics of generated text. Their key hypothesis appears to be that incorporating RST information into language models will allow for more fine-grained and interpretable control over the syntax, semantics, discourse structure, topical keywords, and keyword positions in generated long-form text.Specifically, the paper investigates using RST relations, nuclearity, and node positions to control the coherence and cohesion of generated text. The authors also propose an RST-aware attention mechanism to help the model attend to structurally relevant information during generation. The main goals seem to be:- Developing controllable long-form text generation using RST- Providing more fine-grained and interpretable control compared to existing methods like prompts or content planning- Improving coherence and cohesion in generated text through RST-based controlThe paper evaluates RSTGen on argument generation and story generation tasks, showing it can control linguistic features and outperform baseline methods in terms of automated metrics.In summary, the central research question is how RST can enable controllable and coherent long-form text generation with an interpretable structure. The key hypothesis is that incorporating RST relations, nuclearity, and other structural information will provide fine-grained control over generated text.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The authors propose RSTGen, a novel framework to imbue pre-trained language models like GPT with rhetorical structure theory (RST) information in order to improve the coherence and cohesion of generated text. 2. They introduce an RST-aware attention mechanism that allows the model to focus only on structurally relevant parts of the text during generation based on the input RST structure.3. They demonstrate RSTGen's ability to control syntactic, semantic, and discourse features of generated text through open generation experiments.4. They show that RSTGen performs competitively on argument generation and story generation tasks compared to existing baselines, while offering more fine-grained control over the text structure and content.5. Overall, the key novelty seems to be the integration of classical RST structures into modern pre-trained language models to imbue more coherence and interpretability, through both constrained attention and explicit conditioning on RST relations/nuclearity. The results on controllable generation and long-form text tasks showcase the benefits of this neuro-symbolic fusion approach.In summary, the main contribution appears to be the proposal of RSTGen as an interpretable framework for imbuing rhetorical structure into neural text generation to improve coherence and provide fine-grained control. The experimental results demonstrate the viability of this approach on benchmark argumentative and narrative text generation tasks.
