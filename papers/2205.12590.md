# RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText   Generators

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we imbue fine-grained interpretable control into long-form text generators using Rhetorical Structure Theory (RST)?The authors propose a framework called RSTGen that utilizes RST to control the discourse structure, semantics, and topics of generated text. Their key hypothesis appears to be that incorporating RST information into language models will allow for more fine-grained and interpretable control over the syntax, semantics, discourse structure, topical keywords, and keyword positions in generated long-form text.Specifically, the paper investigates using RST relations, nuclearity, and node positions to control the coherence and cohesion of generated text. The authors also propose an RST-aware attention mechanism to help the model attend to structurally relevant information during generation. The main goals seem to be:- Developing controllable long-form text generation using RST- Providing more fine-grained and interpretable control compared to existing methods like prompts or content planning- Improving coherence and cohesion in generated text through RST-based controlThe paper evaluates RSTGen on argument generation and story generation tasks, showing it can control linguistic features and outperform baseline methods in terms of automated metrics.In summary, the central research question is how RST can enable controllable and coherent long-form text generation with an interpretable structure. The key hypothesis is that incorporating RST relations, nuclearity, and other structural information will provide fine-grained control over generated text.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The authors propose RSTGen, a novel framework to imbue pre-trained language models like GPT with rhetorical structure theory (RST) information in order to improve the coherence and cohesion of generated text. 2. They introduce an RST-aware attention mechanism that allows the model to focus only on structurally relevant parts of the text during generation based on the input RST structure.3. They demonstrate RSTGen's ability to control syntactic, semantic, and discourse features of generated text through open generation experiments.4. They show that RSTGen performs competitively on argument generation and story generation tasks compared to existing baselines, while offering more fine-grained control over the text structure and content.5. Overall, the key novelty seems to be the integration of classical RST structures into modern pre-trained language models to imbue more coherence and interpretability, through both constrained attention and explicit conditioning on RST relations/nuclearity. The results on controllable generation and long-form text tasks showcase the benefits of this neuro-symbolic fusion approach.In summary, the main contribution appears to be the proposal of RSTGen as an interpretable framework for imbuing rhetorical structure into neural text generation to improve coherence and provide fine-grained control. The experimental results demonstrate the viability of this approach on benchmark argumentative and narrative text generation tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points from the paper:The paper proposes RSTGen, a framework that uses Rhetorical Structure Theory (RST) to control the discourse structure, semantics, and topics of generated text in order to improve coherence and cohesion; it demonstrates this on the tasks of argument generation and story generation where it shows improved performance over existing methods while offering more fine-grained control.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the same field:Overall, this paper makes an incremental contribution to the field of controllable text generation. The key idea of using Rhetorical Structure Theory (RST) to provide fine-grained control over the discourse structure and semantics of generated text builds upon prior work that has used syntactic structures like dependency trees. However, RST provides a more interpretable framework for long-form text generation compared to dependency syntax. The strengths of this paper are:- It proposes a novel framework (RSTGen) to integrate RST structure into pretrained language models like GPT-2 and BART. The RST encoding with nuclearity, relations, and node positions allows controlling aspects like syntax, semantics, discourse structure, and topic keywords.- RSTGen outperforms existing content planning methods like PAIR and DYPLOC on argument generation based on automated metrics. It also shows competitive performance on story generation compared to DiscoDVT.- The framework offers more fine-grained control over linguistic features of text compared to prompt-based methods like CTRL. The ablation studies demonstrate the impact of components like RST-aware attention.- The open generation examples nicely showcase controllable generation based on varying RST relations, nuclearity, etc. The paper also analyzes model capabilities using metrics like RST tree edit distance.Some limitations are:- The RST Predictor uses a simple Markovian sampling approach, which likely results in unrealistic RST tree structures. More structured sampling methods could help.- More human evaluation would be useful to better assess coherence/cohesion improvements. The automated metrics have limitations.- The RST control is evaluated on Reddit data domains. Testing on more diverse corpora could reveal other challenges.Overall, I think this is a solid contribution that moves forward controllable NLG research. The integration of classical linguistics theory into neural models is promising. At the same time, the enhancements are incremental over prior work. More dramatic improvements to coherence/cohesion likely require breakthroughs in long-form reasoning. But this paper pushes the state of the art and provides useful analysis.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing more sophisticated prompt engineering techniques to control style, content, and attributes of generated text. The authors note that current prompt-based methods are limited in their ability to exert fine-grained control. They suggest exploring prompts with more complex context and structure.- Improving content planning modules by incorporating more powerful natural language understanding techniques. The authors mention this could help generate more coherent and logically structured text.- Exploring different integration approaches between content planning and surface realization modules. The sequential two-step approach has limitations, so jointly training could be beneficial.- Leveraging other linguistic formalisms like Systemic Functional Linguistics to control attributes like formality, point of view, etc. RST mainly controls coherence/cohesion.- Scaling up control methods to even longer text generations. The authors note current techniques start to break down beyond a certain length.- Developing better evaluation metrics to assess controllability, interpretability, coherence, etc. Human evaluation is costly so automated metrics are needed.- Mitigating potential harmful biases learned from training data. The authors acknowledge the risks of generated text inadvertently perpetuating societal biases.- Exploring applications of controllable generation like automated argumentation, personalized storytelling, stylized writing, etc.In summary, the main directions are developing more granular control techniques, stronger planning modules, joint training methods, alternate linguistic formalisms, scalability, better evaluation, debiasing, and novel applications. The authors lay out an extensive research agenda for controllable text generation.
