# [Interpreting Graph Neural Networks with In-Distributed Proxies](https://arxiv.org/abs/2402.02036)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph neural networks (GNNs) are being increasingly deployed in critical applications, necessitating explanations for their predictions. A popular approach is to identify compact yet informative subgraphs and evaluate if the GNN maintains its original prediction. However, such explanation subgraphs often deviate distributionally from the original training graphs. Since GNNs are only reliable on in-distribution data, their predictions on explanation subgraphs become unreliable for evaluating if the subgraphs retain explanatory factors. This causes an out-of-distribution (OOD) problem that undermines the reliability of explanations.

Proposed Solution:
The paper proposes using "proxy graphs" that are both in-distribution and preserve the explanatory factors in the original explanation subgraph. Specifically, a graph autoencoder reconstructs the explanation subgraph while a variational graph autoencoder introduces controlled perturbations into the non-explanatory part. The framework is optimized to ensure proxy graphs match the distribution of original training graphs while retaining label information. Proxy graph predictions reliably approximate explanations as they circumvent the OOD issue.  

Main Contributions:
- Identifies the OOD problem in explainable GNNs that undermines reliability
- Introduces the concept of in-distribution proxy graphs that adhere to original data distribution while preserving explanations
- Proposes a framework comprising graph autoencoders to produce proxy graphs 
- Achieves state-of-the-art performance and reliable explanations on benchmark datasets

The core idea is using proxy graphs to reliably evaluate if an explanation subgraph retains predictive factors, while ensuring these proxies graphs match the distribution of training data that the GNN is reliable on. This enhances the accuracy and interpretability of explanations for graph neural networks.
