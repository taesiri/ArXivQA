# [Interpreting Graph Neural Networks with In-Distributed Proxies](https://arxiv.org/abs/2402.02036)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph neural networks (GNNs) are being increasingly deployed in critical applications, necessitating explanations for their predictions. A popular approach is to identify compact yet informative subgraphs and evaluate if the GNN maintains its original prediction. However, such explanation subgraphs often deviate distributionally from the original training graphs. Since GNNs are only reliable on in-distribution data, their predictions on explanation subgraphs become unreliable for evaluating if the subgraphs retain explanatory factors. This causes an out-of-distribution (OOD) problem that undermines the reliability of explanations.

Proposed Solution:
The paper proposes using "proxy graphs" that are both in-distribution and preserve the explanatory factors in the original explanation subgraph. Specifically, a graph autoencoder reconstructs the explanation subgraph while a variational graph autoencoder introduces controlled perturbations into the non-explanatory part. The framework is optimized to ensure proxy graphs match the distribution of original training graphs while retaining label information. Proxy graph predictions reliably approximate explanations as they circumvent the OOD issue.  

Main Contributions:
- Identifies the OOD problem in explainable GNNs that undermines reliability
- Introduces the concept of in-distribution proxy graphs that adhere to original data distribution while preserving explanations
- Proposes a framework comprising graph autoencoders to produce proxy graphs 
- Achieves state-of-the-art performance and reliable explanations on benchmark datasets

The core idea is using proxy graphs to reliably evaluate if an explanation subgraph retains predictive factors, while ensuring these proxies graphs match the distribution of training data that the GNN is reliable on. This enhances the accuracy and interpretability of explanations for graph neural networks.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel method called ProxyExplainer that generates in-distribution proxy graphs to reliably approximate the predictions of graph neural networks on explanation subgraphs, addressing the out-of-distribution issue in existing graph neural network explanation methods.


## What is the main contribution of this paper?

 This paper's main contribution is proposing a novel method to generate in-distributed proxy graphs that can reliably approximate the predictions of true labels for explanation subgraphs in graph neural networks. Specifically, the key contributions are:

1) Systematically analyzing and addressing the out-of-distribution (OOD) problem in explainable GNNs using the concept of proxy graphs. This is critical for enhancing reliability and interpretability. 

2) Introducing an innovative parametric method that incorporates graph autoencoders to produce proxy graphs situated in the original data distribution while preserving essential explanation information.

3) Comprehensive experiments on various real-world datasets demonstrating the effectiveness of the proposed approach in generating high-quality explanations and alleviating the OOD problem.

In summary, the paper makes important contributions towards explaining graph neural networks by tackling the OOD problem through the use of in-distributed proxy graphs generated by graph autoencoders. This enhances the reliability and interpretability of GNN explanations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- XAI - Explainable AI, referring to techniques for making AI and machine learning models more interpretable and transparent. A core focus of this paper.

- Graph Neural Networks (GNNs) - Neural network models designed for graph-structured data. The models this paper aims to explain. 

- Post-hoc explanations - Explanations generated after a model has been trained, without changing the model itself. The type of explanations this paper focuses on.

- Instance-level explanations - Explanations aimed at providing interpretations for individual predictions. Another focus of this work. 

- Graph Information Bottleneck (GIB) - A principle for generating compact yet informative explanations by extracting subgraphs. Commonly used in GNN explanation methods.

- Out-of-Distribution (OOD) problem - The problem of distribution shift between original training graphs and explanation subgraphs. A key challenge this paper addresses. 

- Proxy graphs - In-distribution graphs introduced in this paper that preserve explanatory factors. Used to address the OOD problem.

- Graph autoencoders - Used in this paper to generate proxy graphs. Help produce variations that match the original graph distribution.

So in summary, key terms cover topics like explainability, graph neural networks, the OOD problem, proxy graphs, and graph autoencoders.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces the concept of "proxy graphs" to address the out-of-distribution (OOD) problem in explainable graph neural networks. What is the intuition behind using proxy graphs instead of the original explanation subgraphs? What specific advantages do proxy graphs provide?

2. The paper proposes a bi-level optimization framework. Can you elaborate on why this bi-level optimization strategy is well-suited for generating proxy graphs? What are the objectives of the inner and outer optimization respectively?  

3. The proxy graph generator leverages two distinct graph autoencoders - one for reconstructing the explanation subgraph and another variational autoencoder for generating perturbations. What is the motivation behind this dual autoencoder structure? How do the two components complement each other?

4. The paper imposes an information-theoretic constraint that enforces the label entropy of the proxy graph to match that of the explanation subgraph. Why is this an appropriate constraint? Does this fully capture the requirement that proxy graphs need to preserve explanatory factors?

5. The variational graph autoencoder component introduces randomness into the framework through sampling of the latent variable. How might this impact the stability of explanations generated? Are there any measures introduced in the paper to control this?

6. Could you discuss any assumptions in the generative modeling of proxy graphs? For instance, the ER random graph assumption during derivation of the outer optimization objective. Do you foresee any limitations arising from such assumptions?

7. One of the terms in the proxy graph generation loss function is the KL divergence between the latent distribution and a standard Gaussian prior. What is the motivation behind this? How does tuning this term affect the overall performance?

8. The paper demonstrates superior performance over baselines, but are there any limitations or failure cases you might expect? When might the method struggle to provide reliable explanations?  

9. The framework alternates between training the explainer and proxy graph generator. What might be some pros and cons of this alternating approach? Could end-to-end joint training be feasible?

10. The method relies exclusively on structural information during the proxy graph generation process. Do you think incorporating node or graph attributes could further enhance explanation performance? What modifications would be needed?
