# [G-EvoNAS: Evolutionary Neural Architecture Search Based on Network   Growth](https://arxiv.org/abs/2403.02667)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural architecture search (NAS) aims to automate the design of neural network architectures. However, most prior NAS methods either repeatedly stack local architectures (cells) to build the entire model, or rely on manually designed benchmark modules as basic units to search the global model. These approaches narrow down the search space and limit the possibility of finding the globally optimal architecture.  

- Searching the neural architecture directly in the vast global space is computationally prohibitive due to the huge complexity. Therefore, efficiently searching the global space remains a challenge.

Proposed Solution - G-EvoNAS:
- The paper proposes an evolutionary neural architecture search framework based on network growth (G-EvoNAS) to efficiently search the global space. 

- G-EvoNAS divides the evolution into multiple stages. In each stage, it searches for one block in the network in a growing manner, gradually evolving a shallow network into a deeper complete network. This avoids directly searching the entire complex global space.

- G-EvoNAS represents the architecture using a real-valued vector based on constraints instead of the commonly used binary adjacency matrix. It uses tailored crossover and mutation operators to evolve the population.

- To accelerate evaluation, G-EvoNAS trains a pruned supernet with weight sharing to inherit weights instead of training candidates from scratch everytime. The supernet is pruned based on elite individuals after each growth stage to reduce weight coupling between subnets.

Main Contributions:

- Proposes a network growth based evolutionary search strategy to efficiently search neural architectures in the global space.

- Designs custom genome encoding, crossover and mutation operators for evolving the architectures.  

- Introduces pruned supernet training to improve ranking accuracy and evaluation efficiency during the evolutionary search.

- Achieves competitive results on CIFAR and ImageNet with fewer parameters and lower search cost compared to state-of-the-art NAS methods.

In summary, G-EvoNAS provides an effective and efficient way to search globally optimized neural architectures by growing the network progressively and leveraging weight sharing with pruned supernets.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes G-EvoNAS, an evolutionary neural architecture search framework based on network growth that gradually deepens different blocks starting from a shallow network to reduce search complexity while improving search accuracy through pruned supernet training.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes an alternative approach to reduce the global search complexity of NAS by progressively searching each block structure in the candidate network and allowing the network to grow from a single block to a full configuration. 

2. It uses weight sharing of the SuperNet to evaluate candidate networks, while also pruning the SuperNet based on the elite population during network growth to alleviate weight coupling between individual subnets. This improves network ranking accuracy while accelerating evaluation.

3. The proposed algorithm, G-EvoNAS, can search for competitive network architectures very efficiently. Using only 0.2 GPU days on CIFAR10 and CIFAR100, it achieves 97.52% and 83.38% classification accuracy respectively. The network architecture transfers well to ImageNet, achieving 75.5% accuracy.

In summary, the main contribution is an efficient evolutionary NAS method that grows networks gradually while pruning the SuperNet to enable fast and accurate architecture search in the global space.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it are:

- NAS (Neural Architecture Search)
- Evolutionary Algorithms
- Network Growth
- Supernet Pruning 
- Block-wise Search Space
- Evolution Search
- Genome Encoding
- Crossover and Mutation Operators
- Potential Assessment of Candidates
- Inaccurate evaluation in NAS
- CIFAR10
- CIFAR100
- ImageNet

The paper proposes an evolutionary neural architecture search framework based on network growth called G-EvoNAS. It utilizes concepts like evolutionary algorithms, network growth, Supernet pruning, block-wise search space, genome encoding, crossover and mutation operators etc. to efficiently search for neural network architectures. The approach is evaluated on image classification datasets like CIFAR10, CIFAR100 and ImageNet. So these are some of the key terms and keywords related to this paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a block-wise search space. How is this search space designed and what are the key differences compared to other NAS search spaces like cell-based or full network search spaces?

2. The paper uses a network growth based evolutionary search strategy. How does this strategy work to reduce the complexity of searching the global space? What are the key steps in the search process? 

3. The paper uses genome encoding to represent network architectures. What encoding scheme is used and what are its advantages over other representations like adjacency matrices? 

4. What genetic operators are used for crossover and mutation in the evolutionary search process? How do these operators work and how do they enhance the search?

5. The paper argues that weight sharing strategies used in NAS can lead to inaccurate performance evaluations. How does the paper try to address this issue through Supernet pruning? Explain the pruning strategy.  

6. What is the potential function used to evaluate candidate networks during the evolutionary search? What key attributes does it capture and why?

7. What are the key hyperparameters explored in the ablation study? What impact do they have on the search performance?

8. How does the paper evaluate the effectiveness of the network growth based search strategy? What benchmark is used and what improvements are observed?

9. How does Supernet pruning help improve the ranking accuracy compared to traditional weight sharing strategies? What metrics are used to showcase this improvement?

10. The search method transfers architectures found on CIFAR to ImageNet. How does the performance compare with other state-of-the-art NAS methods on ImageNet while also considering search costs?
