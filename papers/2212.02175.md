# Momentum Decoding: Open-ended Text Generation As Graph Exploration

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new decoding method called "momentum decoding" for open-ended text generation with autoregressive language models. The central hypothesis is that reformulating text generation as exploring a directed graph and understanding repetition/degeneration as circular loops in this graph can lead to an effective and efficient decoding algorithm.Specifically, the key research questions addressed are:- How can we formulate open-ended text generation as a graph exploration process?- How does viewing repetition/degeneration as circular loops in this graph formulation allow us to address the problem? - Can a decoding method based on this graph viewpoint, which encourages greedy exploration outside the graph while allowing controlled returns to the graph, achieve strong performance?- Can such a decoding method be much more efficient than existing state-of-the-art decoding methods like contrastive search?The paper proposes the momentum decoding algorithm based on this graph viewpoint and conducts extensive experiments to evaluate whether it can achieve comparable performance to contrastive search while being significantly more efficient. The results appear to validate the core hypothesis that formulating decoding as graph exploration and leveraging a momentum-based strategy can find a good balance between effectiveness and efficiency.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new decoding method called "momentum decoding" for open-ended text generation with autoregressive language models. The key ideas are:- Viewing open-ended text generation as an exploration process within a directed graph. The phenomenon of text degeneration is understood as circular loops in the graph.- Proposing "momentum decoding" which encourages the language model to greedily explore new nodes outside the current graph, while also allowing returning to existing nodes with downgraded momentum using a resistance function.- The resistance function penalizes candidate tokens based on their "circular depth" in existing loops, which prevents the model from getting stuck in deep circular loops that cause severe degeneration.- Momentum decoding bridges the gap between training and inference better than prior methods like nucleus sampling and contrastive search, by mostly following the greedy objective and only correcting when degeneration is clear.- Experiments show momentum decoding achieves comparable performance to state-of-the-art contrastive search, while having 30% faster inference speed and 4x lower computational cost.In summary, the key contribution is a new decoding method that balances effectiveness and efficiency for open-ended text generation by formulating it as graph exploration and using a momentum approach. The results demonstrate the potential of this method to enable more efficient generation while maintaining strong performance.
