# [Mitigating Adversarial Vulnerability through Causal Parameter Estimation   by Adversarial Double Machine Learning](https://arxiv.org/abs/2307.07250)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we estimate and mitigate the causal effects of adversarial perturbations on model predictions in order to improve adversarial robustness?

The key points are:

- The paper observes that adversarial vulnerability varies across target classes and remains prevalent even with advanced network architectures and defense methods. 

- To address this, the paper proposes a causal inference approach called Adversarial Double Machine Learning (ADML) to estimate the causal parameter representing the degree of adversarial vulnerability for a model's predictions. 

- By minimizing the estimated causal parameter, the method aims to reduce the negative causal effects of adversarial perturbations, thereby improving robustness.

- Through experiments on CNNs and Transformers, the paper shows that incorporating ADML can substantially improve the adversarial robustness of various defense methods across datasets.

So in summary, the central hypothesis is that explicitly estimating and minimizing the causal effects of adversarial perturbations will allow models to achieve better robustness, even on classes/examples that are inherently more vulnerable. The ADML method is proposed to test this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. The authors observe that adversarial vulnerability varies across target classes and remains prevalent even with deeper neural network architectures and more advanced defense methods. They argue that current adversarial training-based defenses lack understanding of the causal relations between inputs and predictions. 

2. To address this issue, the authors propose a causal approach called Adversarial Double Machine Learning (ADML) to quantify the degree of adversarial vulnerability and mitigate its effects on model predictions. ADML allows estimating the causal parameter of adversarial perturbations themselves.

3. The authors propose a way to estimate the causal parameter representing the adversarial vulnerability using techniques from double machine learning. They show how to modify the estimation to satisfy Neyman orthogonality and be invariant to nuisance parameters.

4. By minimizing the magnitude of the estimated causal parameter, the negative causal effects of adversarial vulnerability can be reduced. This allows improving adversarial robustness and alleviating the observed phenomenon in current defenses.

5. Through extensive experiments on CNNs and Transformers over several datasets, the authors demonstrate that incorporating ADML significantly improves adversarial robustness compared to various state-of-the-art defense baselines. The results validate the effectiveness of ADML.

In summary, the key contribution is bridging a causal inference perspective to analyze and mitigate adversarial vulnerability in neural networks via the proposed ADML approach. This provides a new direction for improving robustness.
