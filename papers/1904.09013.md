# [Self-Supervised Audio-Visual Co-Segmentation](https://arxiv.org/abs/1904.09013)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we develop a model for visual object segmentation and sound source separation that can learn from natural videos through self-supervision, without needing manually labeled data?Specifically, the paper proposes a method to train a neural network model on unlabeled videos to perform both image segmentation and audio source separation. The key ideas are:- Use the natural correspondence between sounds and visual objects in videos for self-supervised learning, without needing manual labels. Mix the audio from two videos and train the model to separate the audio conditioned on the visual input from one video.- Introduce a learning approach to disentangle the representations learned by the neural networks. This enables assigning semantic categories to network channels, allowing independent image segmentation and sound separation after training.- Evaluate the model on image-only segmentation and audio-only separation tasks using a large video dataset. This extends previous work that required synchronized audio-visual input for inference.So in summary, the main hypothesis is that an audio-visual neural network can be trained through self-supervision on videos to perform independent visual and audio source separation, by disentangling the learned representations and assigning them to semantic categories. The paper aims to demonstrate this possibility.
