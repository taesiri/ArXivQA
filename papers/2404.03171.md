# [Multi-modal Learning for WebAssembly Reverse Engineering](https://arxiv.org/abs/2404.03171)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- WebAssembly (Wasm) is gaining popularity for performance-critical applications, driving demand for reverse engineering to understand Wasm code. 
- However, Wasm lacks high-level information like types, function names, comments that aid in code comprehension. Its stack-based design also makes manually tracing execution very challenging.
- Prior ML solutions for Wasm reverse engineering have limitations - they target specific tasks, require large labeled datasets, and overlook available high-level semantics in source code and documentation that could provide useful context.

Proposed Solution:
- The paper proposes \sys{}, the first multi-modal pre-trained language model for Wasm reverse engineering.
- \sys{} is pre-trained on a large corpus of (code documentation, source code, Wasm code) triples using self-supervised learning, without needing labeled data.
- Three tailored pre-training tasks - Multi-Modal Masked LM, Similar Semantics ID, and Reordered Instructions ID - teach \sys{} relationships within and across modalities.
- The pre-trained model can then be fine-tuned on downstream tasks using much less labeled data, improving data efficiency.

Key Contributions:
- Pre-trains first multi-modal LM to learn joint representations of documentation, source code, and Wasm code.
- Achieves high accuracy on multiple Wasm reverse engineering tasks - type recovery, function purpose ID, and Wasm summarization.
- Outperforms prior supervised learning methods, while being more data-efficient.
- Provides programmers high-level insights to aid Wasm comprehension and further inspection.
- Establishes the value of learning from multiple modalities of code for effective Wasm analysis.

In summary, the paper makes a case for multi-modal pre-trained models that leverage available source code and documentation to better understand low-level Wasm code. The proposed \sys{} model sets the state-of-the-art for diverse Wasm reverse engineering tasks.
