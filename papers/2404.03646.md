# [Locating and Editing Factual Associations in Mamba](https://arxiv.org/abs/2404.03646)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
The paper investigates whether the methods for analyzing and interpreting factual knowledge in transformer language models can be applied to state-space language models like Mamba. Specifically, the paper examines whether Mamba exhibits similar localization of factual knowledge and amenability to targeted editing of facts as seen in transformers. 

Proposed Methods
The paper adapts four analysis techniques previously applied to transformers for studying Mamba:

1) Activation patching to identify key model components for recalling facts. This involves intervening on model states during clean and corrupted runs to measure causal effects on fact recall.

2) Rank-one model editing using ROME to insert facts by making targeted single parameter edits. 

3) Analyzing linearity of relational encodings using first-order approximations of the mapping from subject to object (LRE method).

4) Attention knockout adaptations to study information flow.

The experiments are conducted on Mamba-2.8b and compared to the similarly sized Pythia-2.8b transformer.

Key Findings
- Activation patching reveals specific Mamba components that play a key role in factual recall, concentrated in middle layers at end of subject positions, analogous to transformers.

- Facts can be successfully inserted into Mamba with the ROME method by editing projection matrices W_o, though Mamba lacks the MLP modules typically edited in transformers.

- The LRE method finds Mamba's encoding of some factual relations to be fairly linear, again resembling transformers. 

- An adapted attention knockout method provides some insights on information flow in Mamba's state propagation modules.

In summary, despite architectural differences between Mamba and transformers, the paper is able to productively adapt analysis methods from transformers to study Mamba's factual knowledge, finding important commonalities in the localization and editability of facts.
