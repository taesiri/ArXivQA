# CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for   Multimodal Machine Translation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that existing pre-trained language models and multimodal models can be effectively combined and adapted to improve multimodal machine translation, without needing to train complex specialized models from scratch. Specifically, the paper proposes combining the mBART text translation model and the multilingual multimodal CLIP (M-CLIP) model in a novel way to improve MMT. The key ideas are:- Using M-CLIP to provide visual features/representations to augment the text-only mBART model for MMT. This transfers visual knowledge to mBART without needing paired image-text multilingual training data.- Proposing a two-stage training approach where first image captioning is used to adapt mBART to the visual representations before MMT. The idea is captioning helps align the modalities.- Simply conditioning mBART on M-CLIP visual features via a lightweight mapping network and prefix tokens, eliminating the need for complex fused architectures.So in summary, the central hypothesis is that leveraging and aligning pre-trained vision and language models through this proposed approach can achieve state-of-the-art MMT performance, without requiring difficult joint training or fusion of specialized MMT models. The experiments aim to demonstrate these advantages.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Presenting an architecture called CLIPTrans that simplifies multimodal machine translation by transferring knowledge from independently pre-trained multimodal and multilingual models (M-CLIP and mBART), without requiring heavily engineered architectures or additional data.2. Proposing a novel two-stage training pipeline where the first stage is image-to-text captioning and the second stage is text-to-text translation. This enables transferring visual knowledge to the multilingual space and adapting the mBART model's attention maps to the introduced embeddings. 3. Pushing state-of-the-art results on standard MMT benchmarks by an average of +2.67 BLEU without using images during inference. The model shows particularly strong gains on low-resource languages.4. Setting strong text-only baselines with mBART that exceed previous MMT state-of-the-art results. This shows the power of transfer learning from pre-trained models.5. Providing thorough ablation studies and analysis to demonstrate the effectiveness of the proposed training pipeline and architecture design choices.In summary, the main contribution is presenting a simple yet effective approach to transfer knowledge from pre-trained vision-language and language-only models for multimodal machine translation, using a novel training strategy. The method achieves new state-of-the-art results while simplifying the MMT pipeline.
