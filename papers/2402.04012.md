# [Quantized Approximately Orthogonal Recurrent Neural Networks](https://arxiv.org/abs/2402.04012)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "Quantized Approximately Orthogonal Recurrent Neural Networks":

Problem:
- Recurrent neural networks like LSTMs are very useful for processing sequential data but require a lot of compute and memory resources. This makes their deployment challenging on constrained devices. 
- One solution is to quantize the network weights to low bitwidths (e.g. 4 bits) but prior work has shown quantizing RNNs to be unstable.
- Orthogonal RNNs help stabilize training but quantizing orthogonal matrices breaks their useful properties like norm preservation. So it is an open challenge to create quantized orthogonal RNNs.

Proposed Solution:
- The authors explore and compare four methods to create Quantized approximately Orthogonal RNNs (QORNNs) with low bitwidth recurrent weight matrices.
- The methods try to induce approximate orthogonality in the quantized matrix using a combination of regularization, optimization constraints and surjective mappings. 
- The four methods are:
    1. Post-training quantization of orthogonal RNN (PTQ-projUNN)
    2. Quantization-aware training with soft orthogonality regularizer (STE-pen) 
    3. Quantization-aware training with orthogonal projection (STE-projUNN)
    4. Quantization-aware training using Björck surjection (STE-Björck)

Main Contributions:  
- First work exploring quantization of orthogonal RNNs
- Analysis on impact of quantization on orthogonality
- Empirical comparison of 4 quantization strategies for orthogonal RNNs
- Demonstrate QORNNs with just 5 bits can successfully model long term dependencies
- Achieve state of the art on permuted MNIST even with 3-bit QORNNs
- Find that STE-Björck overall works the best, especially for very low bits           

In summary, this paper pioneers research into creating low-bitwidth orthogonal RNNs to enable their efficient deployment, while retaining modeling capacity for long sequences. The proposed STE-Björck method, in particular, is identified as an effective quantization approach.
