# [On the Role of Initialization on the Implicit Bias in Deep Linear   Networks](https://arxiv.org/abs/2402.02454)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper investigates the role of initialization in deep linear networks for solving underdetermined linear systems. Specifically, it studies how different initialization schemes can bias the solution that gradient descent converges to. This is important since deep networks are often overparametrized and have multiple possible solutions.

Key Contributions:

1. For ordinary linear regression (no hidden layers), the paper shows that initializing the weights to zero ensures gradient descent converges to the minimum norm solution. It also provides an algorithm to control convergence to any desired solution based on the initialization.

2. For a single hidden layer network, a specific bi-optimal initialization scheme is proposed that allows each weight matrix and the output to be independently optimal. This also guarantees convergence to the minimum norm solution. Based on this, a compact iteration method is provided that only optimizes over $O(n)$ variables instead of $O(d^2)$.

3. For two hidden layers, a similar bi-optimal initialization and collapsed iteration is derived. However, it is conjectured that this is not possible for more than two hidden layers. 

4. Stability analysis shows the effect of initialization error on the final solution. While deeper networks make this error explode in general, orthogonality constraints can help.

5. Experiments on Riemannian networks with orthogonal weight constraints show that they often converge to the minimum norm solution, even with random initialization. Statistically, more depth helps in this case.

Overall, the paper provides significant insights into the implicit bias induced by initialization schemes, as well as methods to exploit this for improved optimization and generalization. It also reveals open theoretical questions regarding deep linear networks which require further investigation.
