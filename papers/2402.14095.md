# [Zero-shot generalization across architectures for visual classification](https://arxiv.org/abs/2402.14095)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generalization in deep learning typically refers to test accuracy on unseen data from the same classes used during training. Less studied is the ability to generalize classification to entirely new classes not seen during training (zero-shot generalization).

- It is unclear how classification accuracy relates to the network's ability to generalize to new classes. The authors want to study the generalization capability of different network architectures.  

Methods:
- Fine-tune several CNN and transformer architectures on a dataset of Chinese calligraphy characters categorized by artist. Use 15 artists for training and validate on 5 unseen artists.

- Propose a "generalization index" metric based on how clustered the unseen artists appear in the intermediate layer representations. High clustering suggests the model has learned features that transfer to generalizing to new artists.

- Test metric on calligraphy data as well as CIFAR-100 by using 5 classes unseen during standard ImageNet pretraining.

Results:
- Generalization ability varies greatly across architectures and unpredictable across layers. No clear patterns relating accuracy to generalization.

- Highest accuracy does not imply best generalization. All models achieve high accuracy after fine-tuning but differ significantly in how representations cluster unseen classes.

Conclusions:
- Classification accuracy does not reliably predict generalization ability, neither across architectures nor across layers within a network. 

- The embedding spaces learned, even from highly accurate models, can vary greatly in how separable unseen classes remain. Architectures appear to differ fundamentally in what space of representations they learn during fine-tuning.

- Proposed generalization index metric offers a way to quantify generalizability of learned representations within a domain using unlabeled out-of-sample data.
