# [On Learning the Transformer Kernel](https://arxiv.org/abs/2110.08323v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper seeks to address is whether incorporating kernel learning into Transformers can help improve the performance of efficient Transformers that have linear complexity. 

Specifically, the paper proposes "kernelized Transformers" which learn the kernel function in self-attention in an end-to-end fashion during training. The hypothesis is that by making the kernel learnable directly from the data, these models can achieve better performance compared to Transformers that use a fixed kernel (e.g. Linear Transformers, Performers). 

The authors evaluate this hypothesis by proposing several kernel learning methods based on random kitchen sinks and positive random features, incorporating them into Transformers, and evaluating their performance on long-context tasks from the LRA benchmark as well as short-context GLUE tasks. 

The key findings are that the proposed kernelized Transformers outperform fixed kernel baselines on LRA while retaining competitive performance on GLUE, demonstrating the benefits of learning the kernel. The paper also analyzes the models in terms of their theoretical properties, variance, and sparsity handling.

In summary, the central hypothesis is that learning the kernel in self-attention can improve efficient Transformers, and the paper aims to demonstrate this through proposing kernelized Transformer models and evaluating them empirically.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a framework for learning the kernel function within the self-attention mechanism of Transformers in a data-driven way. Specifically, the paper introduces several methods under the umbrella of "Kernelized Transformers" that leverage kernel learning techniques like Random Kitchen Sinks and Positive Random Features to learn the kernel in an end-to-end fashion while retaining linear complexity. The benefits of this approach over using fixed kernels like in previous works are demonstrated through experiments on long context tasks where the Kernelized Transformers outperform baselines, as well as short context tasks where they remain competitive. Theoretically, the paper also shows properties like Turing completeness for some of the proposed models. Overall, the main novelty seems to be in incorporating kernel learning into Transformers to improve their flexibility and performance.
