# [Self-supervised Video Representation Learning by Uncovering   Spatio-temporal Statistics](https://arxiv.org/abs/2008.13426)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we learn effective spatio-temporal video representations in a self-supervised manner without relying on manually labeled data?The main hypothesis is that by designing a pretext task that uncovers spatio-temporal statistics from unlabeled videos, such as locations and motion directions of salient motions, a neural network can be trained to learn powerful and transferable spatio-temporal representations for video understanding.Specifically, the key hypotheses are:1) Learning to uncover spatio-temporal statistics like dominant motions and color changes from videos can serve as an effective pretext task for self-supervised video representation learning. 2) The learned representations can transfer well and achieve strong performance on various downstream video analysis tasks such as action recognition, video retrieval, dynamic scene recognition, etc.3) The proposed pretext task is simple and intuitive yet captures informative spatio-temporal characteristics of videos, compared to techniques like predicting future frames which can be ambiguous.4) Explicitly encoding spatial locations instead of regressing to exact coordinates makes the learning task easier and more feasible.5) A curriculum learning strategy can further boost the representation learning by presenting more difficult examples gradually.In summary, the central hypothesis is that by carefully designing a pretext task based on spatio-temporal statistics, informative and transferable video representations can be learned effectively in a self-supervised manner. The paper conducts experiments to validate the effectiveness of the proposed approach over several backbone architectures and on various downstream tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel self-supervised learning approach for video representation learning. Specifically, the key ideas and contributions are:1. It proposes a new pretext task of uncovering spatio-temporal statistics from unlabeled videos to learn video representations. These statistics include motion statistics (e.g. largest motion location and direction) and appearance statistics (e.g. largest/smallest color diversity locations and dominant colors). 2. The pretext task is inspired by cognitive studies on human visual system and aims to mimic human inherent visual habits for video understanding. It focuses on learning rough spatio-temporal impressions rather than dense pixel predictions.3. It introduces curriculum learning to gradually include more difficult samples during training, which further improves the representation learning.4. It conducts extensive experiments with different backbone networks, training targets, and downstream tasks to demonstrate the effectiveness and transferability of the learned representations.5. The proposed method achieves state-of-the-art performance on multiple downstream tasks including action recognition, video retrieval, dynamic scene recognition, and action similarity labeling.In summary, the key contribution is designing a novel and intuitive pretext task by uncovering spatio-temporal statistics to learn effective and transferable video representations in a self-supervised manner. The proposed method shows strong empirical results across different settings.
