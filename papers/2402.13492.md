# [Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval   Augmentation to Language Models](https://arxiv.org/abs/2402.13492)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LMs) can demonstrate impressive factual knowledge, but still struggle with providing fully accurate responses, especially for information beyond their pre-training. 
- Retrieval-augmented LMs (RALMs) have emerged as a solution by incorporating relevant external information. However, indiscriminately augmenting LMs can sometimes override correct knowledge the LM already possesses and cause incorrect responses.

Proposed Solution:
- Conduct a detailed analysis to understand when LMs can correctly recall facts vs when retrieval helps or hurts, based on the popularity (frequency of mentions in Wikipedia) of entities and relations.
- Develop a new QA dataset called WiTQA with questions annotated with popularity scores, accompanied by supporting passages to facilitate this analysis.
- Evaluate performance of 10 LMs with and without 4 different retrieval methods on WiTQA.
- Propose a selective memory integration approach that chooses whether to retrieve or rely on the LM's memory for each question based on the popularity scores.

Key Findings:
- LMs can correctly recall frequently seen entity-relations, with larger LMs handling more long-tail relations. But there are still significant drops in performance for less popular facts.
- Retrievers are more robust for long-tail entity-relations. But they perform worse than LMs for popular entity-relations, causing override issues.
- LMs outperform retrievers on popular relations about less common entities.

Main Contributions:
- More detailed fact-centric analysis than prior work on the interplay between recall abilities of LMs vs performance of retrievers.
- New WiTQA benchmark to enable analyzing impact of both entities and relations, with popularity scores and supporting passages.
- Insights revealing when retrieval helps or hurts LMs, through the lens of fact-level popularity.
- Selective memory integration approach that improves accuracy by selectively retrieving based on popularity scores.
