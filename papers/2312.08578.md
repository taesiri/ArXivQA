# [A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style   Models on Dense Captions](https://arxiv.org/abs/2312.08578)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper introduces the Densely Captioned Images (DCI) dataset, comprising 8,012 images with dense, mask-aligned descriptions averaging over 1,000 words per image. DCI contains highly detailed visual descriptions aligned to semantic image regions, enabling new benchmarks to evaluate vision-language models' fine-grained understanding. They create a summarized version fitting existing models' 77 token limit using LLMs, called sDCI. Leveraging sDCI, they propose novel subcrop-caption matching tasks requiring selecting appropriate sub-image regions for each description. Evaluating models on sDCI reveals no existing methods perform well on both standard and subcrop-caption tests concurrently. Fine-tuning on sDCI boosts performance over baseline CLIP more efficiently than far larger automatically-constructed datasets like DAC. By providing the first human-annotated dense image captioning dataset with precise text-region alignment, DCI facilitates developing and evaluating next-generation vision-language models for deeper visual understanding.
