# [MoEC: Mixture of Experts Implicit Neural Compression](https://arxiv.org/abs/2312.01361)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Implicit neural representations (INRs) are promising for 3D/4D data compression due to their ability to represent data using compact neural network parameters. However, INRs have limited spectrum coverage for complex data. 
- Existing methods manually partition the data into blocks/trees and fit INRs locally, but manual design of optimal partitions is difficult.

Proposed Solution:
- The paper proposes MoEC, a Mixture of Experts framework for implicit neural compression. 
- It uses a gating network to automatically assign points in the data to expert INRs, allowing joint optimization of partitioning and compression.
- Careful network design using SIREN networks, shared encoder/decoder modules across experts, and training strategies to enable stable MoEC training.

Main Contributions:
- First end-to-end learnable partitioning and INR compression framework based on Mixture of Experts. Adaptively decomposes data into partitions handled by separate expert networks.
- Achieves state-of-the-art compression results on 3D/4D biomedical data against prior INR methods, data-driven and codec baselines. At high 6000x compression, maintains 48dB PSNR. 
- Robust performance across compression ratios and datasets. Qualitative results show reduced artifacts compared to baselines.
- Detailed analysis and ablation studies validate the approach and training strategies.

In summary, the paper introduces an end-to-end trainable mixture of experts framework for implicit neural representation compression that achieves excellent results by jointly learning optimal data partitioning and fitting specialized INR experts.
