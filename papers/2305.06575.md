# [Chain-of-Dictionary Prompting Elicits Translation in Large Language   Models](https://arxiv.org/abs/2305.06575)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper addresses is:How can we leverage chained multilingual dictionaries to improve the machine translation capabilities of large language models (LLMs) when translating low-resource languages?The paper proposes a new framework called CoD (Chain-of-Dictionary Prompting) that incorporates chained multilingual dictionary knowledge directly into the prompt to guide the LLM's translation decisions, especially for rare/unseen words. Specifically, the prompts consist of:1) The standard translation request (e.g. "Translate from French to English:")2) Chained multilingual dictionary entries linking words in the source, target, and auxiliary languages (e.g. "limite means limit means Grenze means ...")The hypothesis is that providing this lexical knowledge in a chained multilingual format will improve the LLM's ability to translate low-resource languages compared to standard prompting alone or other techniques like bilingual dictionaries or few-shot learning. Experiments on 200 languages in the FLORES benchmark dataset are conducted to evaluate this hypothesis.In summary, the key research question is whether chained multilingual dictionaries can enhance prompting-based machine translation for low-resource languages when used to augment large pre-trained LLMs.


## What is the main contribution of this paper?

This paper presents a novel framework called CoD (Chain-of-Dictionary Prompting for Machine Translation) that uses chained multilingual dictionaries to prompt large language models (LLMs) for the task of machine translation, especially for low-resource languages. The key contributions of this paper are:- Proposes CoD, which utilizes chained multilingual dictionaries as prior knowledge injected into the prompt for machine translation with LLMs. This elicits the translation abilities of LLMs without needing any model training.- Conducts extensive experiments on the FLORES-200 dataset evaluating CoD from English into 199 other languages. Shows CoD substantially improves translation for the majority of languages, including eliciting translation in languages ChatGPT fails on.- Demonstrates the importance of chaining dictionaries and using multilingual dictionaries over just bilingual dictionaries through ablation studies. - Finds CoD outperforms retrieving few-shot demonstrations for low-resource translation, as it's difficult to find relevant examples.- Provides one of the first successful applications of incorporating intermediate reasoning steps for machine translation, different from prior work showing this can hurt translation.Overall, the key contribution is presenting CoD to address the limitations of LLMs for low-resource translation by injecting external multilingual dictionary knowledge into the prompt in a chained form. The comprehensive experiments and analyses demonstrate the effectiveness of CoD for improving translation across many languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel method called Chain-of-Dictionary Prompting (CoD) that improves low-resource neural machine translation in large language models by chaining multilingual dictionary entries for selected words into the translation prompt.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on chain-of-dictionary prompting for machine translation compares to other related research:- Focus on low-resource translation: This paper specifically examines how chain-of-dictionary prompting can improve translation for low-resource languages, which has been a challenging area for large language models. Many other studies evaluate performance on high-resource language pairs.- Use of dictionaries for prompting: Leveraging dictionaries as external knowledge to improve prompting is a fairly novel approach. Other work on prompting large language models for translation has focused more on different prompt formats or incorporating demonstration examples. - Analysis of chaining effects: The paper provides useful analysis about the benefits of chaining dictionary entries over just providing separate bilingual dictionaries. This helps advance understanding of how chained external knowledge impacts reasoning in LLMs.- Comparison to few-shot learning: Evaluating chain-of-dictionary prompting against few-shot in-context learning demonstrates its strengths for low-resource settings where finding relevant examples is difficult. This is an informative comparison not explored much before.- Scale of multilinguality: Testing translations between English and 200 languages is an impressively large-scale analysis. Many past prompt engineering studies for MT are limited to a smaller subset of languages.- Focus on LLMs: The paper adds to work specifically probing prompting effects in large pretrained models like ChatGPT, whereas a lot of prompt engineering has targeted smaller models.So in summary, this paper uniquely combines several elements - low-resource MT focus, chained dictionary prompting, multilingual scale - to advance knowledge of how to improve LLMs for translation via external knowledge and reasoning chains. The analysis and comparisons against other methods also provide useful insights.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Investigating CoD on more languages beyond the 200 languages analyzed in this paper. There are thousands of languages in the world, so expanding the analysis to more low-resource languages is an important direction.- Trying longer chains of multilingual dictionaries in CoD. The current work focused on chains of 5 languages, but longer chains could potentially lead to further improvements.- Tuning the selection of auxiliary languages used in the multilingual dictionaries for specific language pairs. The current universal list works well, but customized lists may boost performance further. - Comparing CoD directly to supervised neural machine translation models. The authors note current LLMs still lag behind supervised models, so benchmarking on the same datasets would be useful.- Testing combinations of CoD and few-shot in-context learning. Retrieving relevant demonstrations is difficult for low-resource languages, but combining both methods may be synergistic.- Expanding the analysis to other large language models besides ChatGPT and InstructGPT. Assessing the impact of CoD across various model architectures would be insightful.- Developing methods to automatically determine when to apply CoD versus using the baseline model alone. This could optimize translation quality on a per-language basis.- Comparing CoD to other techniques like weighted decoding or retrieval augmentation for low-resource machine translation.So in summary, the key directions are expanding the language coverage, optimizing the chained dictionaries, integration with other methods, benchmarking on standard datasets, and adapting CoD in a more automated way. Testing the limits of dictionary-based prompting is an exciting area for future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a novel framework called CoD (Chain-of-Dictionary Prompting) for machine translation using large language models. CoD utilizes prompting and integrates chained multilingual dictionary information into the prompt to provide translation hints for the model. Experiments are conducted with ChatGPT on the FLORES-200 benchmark for translation between English and 200 languages. Results show CoD achieves significant gains, improving translation for the majority of languages and eliciting translation for languages ChatGPT fails on. For example, CoD achieves a 13x chrF++ increase for English to Serbian Cyrillic. Analysis demonstrates the importance of chaining multilingual dictionaries in CoD, and shows CoD outperforms few-shot learning, which struggles to find relevant examples for low-resource languages. Overall, the paper introduces an effective prompting method to improve multilingual translation for LLMs using external dictionary knowledge, analyzed the model behavior, and highlighted limitations like evaluating only 200 languages.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel framework called CoD (Chain-of-Dictionary Prompting for Machine Translation) that uses chained multilingual dictionaries to prompt large language models (LLMs) for the task of machine translation. The approach involves augmenting the standard machine translation prompt with additional textual input that provides possible chained multilingual translations for words in the input sentence. For example, the prompt might state "`limit' means `Grenze' means `çäk'" to provide translation options for the word "limit" before requesting the translation. The authors conduct extensive experiments on the FLORES-200 benchmark, evaluating performance for translation between English and 200 other languages using the ChatGPT model. Results show CoD provides substantial gains over baseline prompting, delivering improvements of over 10x chrF++ points for some low-resource language pairs. Analyses demonstrate the importance of chaining multiple languages in the dictionary rather than just providing bilingual mappings. Comparisons to few-shot in-context learning indicate dictionary prompting is more effective for low-resource translation where finding relevant examples is difficult. Overall, the work introduces a novel technique to incorporate external multilingual knowledge into LLMs that significantly enhances machine translation capabilities.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a novel framework called CoD (Chain-of-Dictionary Prompting) to improve machine translation capabilities of large language models (LLMs) like ChatGPT. The key idea is to provide multilingual dictionary knowledge as prior information directly in the translation prompt. Specifically, for a subset of words in the input sentence, the prompt is augmented with additional text that provides possible chained multilingual translations for those words (e.g. "limit means Grenze means çäk"). This chained multilingual dictionary allows incorporating useful translation lexicons to guide the LLM's decisions, without overly constraining how it uses this knowledge. The dictionaries are created by extracting keywords from English texts using the LLM, then translating them into other languages with an off-the-shelf MT system. Experiments on the FLORES-200 benchmark for 200 languages show CoD provides significant gains over baseline prompting, especially for low-resource languages. Analyses demonstrate the importance of chaining and using multilingual dictionaries compared to just bilingual ones.
