# [Chain-of-Dictionary Prompting Elicits Translation in Large Language   Models](https://arxiv.org/abs/2305.06575)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper addresses is:How can we leverage chained multilingual dictionaries to improve the machine translation capabilities of large language models (LLMs) when translating low-resource languages?The paper proposes a new framework called CoD (Chain-of-Dictionary Prompting) that incorporates chained multilingual dictionary knowledge directly into the prompt to guide the LLM's translation decisions, especially for rare/unseen words. Specifically, the prompts consist of:1) The standard translation request (e.g. "Translate from French to English:")2) Chained multilingual dictionary entries linking words in the source, target, and auxiliary languages (e.g. "limite means limit means Grenze means ...")The hypothesis is that providing this lexical knowledge in a chained multilingual format will improve the LLM's ability to translate low-resource languages compared to standard prompting alone or other techniques like bilingual dictionaries or few-shot learning. Experiments on 200 languages in the FLORES benchmark dataset are conducted to evaluate this hypothesis.In summary, the key research question is whether chained multilingual dictionaries can enhance prompting-based machine translation for low-resource languages when used to augment large pre-trained LLMs.


## What is the main contribution of this paper?

This paper presents a novel framework called CoD (Chain-of-Dictionary Prompting for Machine Translation) that uses chained multilingual dictionaries to prompt large language models (LLMs) for the task of machine translation, especially for low-resource languages. The key contributions of this paper are:- Proposes CoD, which utilizes chained multilingual dictionaries as prior knowledge injected into the prompt for machine translation with LLMs. This elicits the translation abilities of LLMs without needing any model training.- Conducts extensive experiments on the FLORES-200 dataset evaluating CoD from English into 199 other languages. Shows CoD substantially improves translation for the majority of languages, including eliciting translation in languages ChatGPT fails on.- Demonstrates the importance of chaining dictionaries and using multilingual dictionaries over just bilingual dictionaries through ablation studies. - Finds CoD outperforms retrieving few-shot demonstrations for low-resource translation, as it's difficult to find relevant examples.- Provides one of the first successful applications of incorporating intermediate reasoning steps for machine translation, different from prior work showing this can hurt translation.Overall, the key contribution is presenting CoD to address the limitations of LLMs for low-resource translation by injecting external multilingual dictionary knowledge into the prompt in a chained form. The comprehensive experiments and analyses demonstrate the effectiveness of CoD for improving translation across many languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel method called Chain-of-Dictionary Prompting (CoD) that improves low-resource neural machine translation in large language models by chaining multilingual dictionary entries for selected words into the translation prompt.
