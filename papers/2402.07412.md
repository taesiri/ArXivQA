# [Auxiliary Reward Generation with Transition Distance Representation   Learning](https://arxiv.org/abs/2402.07412)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Designing effective reward functions is crucial in reinforcement learning (RL) but very challenging in real-world problems. Existing methods for automatic reward generation struggle to precisely measure the degree of task completion from the current state, which is critical for effective reward design. Simply using the Euclidean distance between the current state and goal state in the raw state space fails to indicate progress towards task completion.

Proposed Solution:
This paper proposes a novel approach called Transition Distance Representation Learning (TDRP) to learn a representation space where the Euclidean distance between state embeddings measures the "transition distance" - the number of transitions required to reach one state from another along a successful trajectory. 

Specifically, TDRP uses a contrastive loss to ensure embeddings of temporally adjacent states in successful trajectories are close together, while states further apart have distant embeddings. This causes the latent space to focus only on task-critical factors while ignoring irrelevant details. The embedding distances reflect trajectory progress.

Built on TDRP, the paper introduces two auxiliary reward generation methods:
1) For single tasks with a goal state, reward based on distance between current state embedding and goal state embedding.
2) For long-horizon skill chaining tasks, reward based on distance between the final state embedding of the previous skill and initial state distribution of next skill.

The auxiliary rewards promote more effective and stable learning in both cases.

Main Contributions:
- Proposes the concept of "transition distance" to measure task completion progress.
- Develops TDRP to learn latent representations reflecting transition distance rather than raw state similarity.
- Leverages TDRP embeddings to generate improved auxiliary rewards for single and chained tasks. 
- Demonstrates significant gains in learning efficiency, stability and performance over state-of-the-art in multiple robotic simulation tasks.
