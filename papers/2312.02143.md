# [Competition-Level Problems Are Effective Evaluators of LLMs](https://arxiv.org/abs/2312.02143)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper evaluates the reasoning capabilities of large language models (LLMs) like GPT-4 on competition-level programming problems from Codeforces. These problems require complex reasoning skills and are uniquely created by experts, reducing likelihood of data contamination. Surprisingly, the paper finds GPT-4's perceived performance sharply declines on problems released after Sept 2021, consistently across difficulties. This suggests issues in reasoning ability and generalization beyond its training data. The paper also shows GPT-4 struggles more on difficult problems and often fails on even the first test case, indicating challenges in deep understanding. Similar phenomena are observed in other code LLMs too. The paper tries straightforward methods like fine-tuning and prompting to improve the performance, but none help much, especially on difficult unseen problems. Overall, the paper establishes competition problems as an excellent benchmark to assess LLMs' reasoning skills, highlights the common pitfalls, and motivates developing models with stronger generalization.
