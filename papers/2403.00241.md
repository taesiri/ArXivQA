# [CASIMIR: A Corpus of Scientific Articles enhanced with Multiple   Author-Integrated Revisions](https://arxiv.org/abs/2403.00241)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Scientific writing is challenging as it requires adhering to specific conventions and effectively conveying complex ideas. Tools to provide automated assistance for improving scientific writing quality are needed. 

- Existing datasets to train such AI writing assistance tools have limitations like small size, incomplete papers, only 2 versions per paper, or lack of peer reviews.

Proposed Solution:
- The authors introduce CASIMIR, a new large-scale corpus for scientific text revision containing multiple revised versions for 15,646 full-length English research papers from OpenReview.

- It has 3.7 million pairs of aligned edited sentences with 5.2 million individual edits, each labeled with a revision intention category (content, grammar, format, language). 

- Metadata like paragraph location, peer reviews and paper venue is also included.

Main Contributions:

1. Released a large open corpus for research on scientific text revision.

2. Conducted analysis on distribution of versions, edits, evolution of revisions over iterations and location inside articles.

3. Evaluated several state-of-the-art text revision models on the data and compared multiple evaluation metrics.

4. Results indicated issues with current evaluation methods for text revision task, warranting new approaches that capture semantic meaning.

The corpus enables developing scientific writing assistance tools that leverage contextual discourse-level information and associated reviews. Limitations include noise due to automatic processing and ethical considerations regarding personal data in reviews.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces CASIMIR, a new large-scale corpus for scientific text revision containing multiple revised versions of 15,646 full-length English scientific articles from OpenReview along with peer reviews, with over 3.7 million pairs of aligned edited sentences annotated with revision intentions.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Releasing a large and open corpus freely available to the research community for revision in scientific articles.

2. Conducting a qualitative analysis of the content of this corpus, including studying the distribution of versions, reviews, edits (their quantity, types, evolution over time, and location within articles).

3. Evaluating three models (IteraTeR-PEGASUS, CoEdIT, and Llama2-7B) on the task of sentence text revision using the dataset and comparing various metrics to evaluate this task. The experiments led the authors to question the relevance of current evaluation methods.

So in summary, the main contribution is creating and releasing a new dataset for scientific text revision, analyzing its content, and using it to evaluate text revision models to provide insights into better evaluation approaches.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with this paper include:

- Scientific articles
- Text revision
- Corpus/dataset
- OpenReview
- Multiple revisions
- Sentence alignment 
- Revision intention
- Peer reviews
- Text quality assessment
- Iterative writing process

The paper introduces a new dataset called CASIMIR, which contains multiple revised versions of 15,646 scientific articles from OpenReview, along with associated peer reviews. The dataset includes sentence-level alignments between versions and automatically extracted edits labeled with revision intentions (e.g. fixing grammar, improving clarity). The authors conduct an analysis of the dataset and experiments with text revision models.

Some of the key aspects of this dataset and paper focus on the revision process in scientific writing, having access to multiple intermediate versions of papers rather than just initial and final, the scale of the dataset, and leveraging associated peer reviews. The intention is to support research on improving automated text revision assistance for scientific writing.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The authors used Bertalign for sentence-level alignment between different versions of the articles. What were some of the challenges they faced with using Bertalign and how might these alignment issues propagate downstream to affect the quality of the final dataset?

2. The authors employed an automatic labeling tool to annotate the extracted edits with revision intentions. What were some of the limitations of using this automatic tool instead of manual labeling? How might errors in intention labeling impact the usefulness of the dataset?  

3. The creation process involved several automatic pipeline steps. What strategies could the authors have employed to quantify and mitigate the impact of errors propagating from each pipeline component on overall dataset quality?

4. The authors chose to split the paragraphs into sentences during the PDF to text conversion step. How might retaining paragraph structure and discourse-level information have provided additional benefits for studying revisions at a level beyond isolated sentences?

5. The distribution of revision intentions, especially for content and format changes, differed from previous datasets. What factors, such as the source of documents or automatic intention labeling, might explain these distributional differences? 

6. The study analyzed the evolution of edits across revision cycles, but was limited to documents with a fixed 5 versions. How could a more comprehensive analysis of edit evolution across different numbers of revision cycles provide additional insights?

7. The author's qualitative analysis examined the distribution of revisions across document sections. What other textual properties could provide informative indicators of where and why revisions tended to occur?

8. The authors evaluated only open-source baselines due to compute limitations. What challenges need to be overcome to effectively benchmark large proprietary models on such a massive dataset? 

9. The strong performance of the CopyInput baseline led the authors to question existing evaluation methodologies. What novel evaluation strategies could better reflect progress on the text revision task?

10. What ethical concerns need to be considered when releasing a dataset comprising reviews and multiple versions of scientific articles? How might the authors have better addressed relevant ethical issues?
