# [Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding](https://arxiv.org/abs/2402.16844)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have become very popular for natural language generation tasks like translation and summarization. However, their enormous size makes deployment costly, especially for latency-critical applications. A key issue is that decoding the response is done sequentially token-by-token in an autoregressive manner, requiring repeated expensive calls to the LLM. This makes response generation slow when responses get longer.  

Proposed Solution: 
The paper proposes a hybrid approach called LLM-to-SLM that combines a large frozen LLM encoder with a small fine-tuned language model (SLM) decoder for fast decoding. First, the LLM encodes the input prompt in parallel once. Then the SLM takes this LLM representation as context and efficiently decodes the response autoregressively. This allows fast decoding while preserving most of the LLM's performance.

Key Contributions:
- A simple and efficient method to accelerate LLMs by exploiting the discrepancy between fast parallel prompt encoding and slow sequential response decoding
- Mixing different model types and sizes, with only the small decoder being fine-tuned 
- Empirical evaluation across multiple NLG tasks and languages, showing 2-4x speedups with minor 1-2% performance penalty
- Comparisons to related work like model compression, parallel decoding, learned prompts, and other LLM/SLM hybrids
- Analysis of tradeoffs between performance and computational efficiency

In summary, the paper presents LLM-to-SLM, a novel approach to accelerate LLMs by combining them with small decoders, reducing deployment costs while maintaining high performance for natural language generation tasks. The method achieves substantial speedups across diverse tasks with minimal impact on quality.
