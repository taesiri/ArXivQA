# DiffusionBERT: Improving Generative Masked Language Models with   Diffusion Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively combine diffusion models with pre-trained language models (PLMs) for improving non-autoregressive text generation. The key hypotheses are:1) PLMs can serve as a good initialization for learning the reverse diffusion process due to their pre-training objectives being related to denoising. This can help accelerate convergence and improve generation quality.2) Designing a new noise schedule tailored for text data that distributes corrupted information uniformly across diffusion steps can further improve the generation performance. 3) Throwing away explicit time step information and having the model perform time-agnostic decoding aligns better with PLMs and works better than explicitly incorporating time steps.In summary, the main goal is to elucidate techniques to successfully integrate diffusion models with PLMs to get the benefits of both approaches for non-autoregressive text generation. The key hypotheses focus on noise schedule designs and time step handling to make diffusion models compatible with PLMs.


## What is the main contribution of this paper?

The main contributions of this paper are:- Presenting DiffusionBERT, a new generative masked language model based on discrete diffusion models. - Combining diffusion models and pre-trained language models (PLMs) to leverage the strengths of both approaches. PLMs provide good initialization and help accelerate convergence, while diffusion models offer a promising training strategy to improve generation quality.- Proposing two key components of DiffusionBERT:    - A new noise schedule called spindle schedule that controls the degree of noise based on each token's information. More informative tokens are masked later in the forward diffusion process.    - Exploring different ways to incorporate the timestep into PLMs, finding that time-agnostic decoding works best.- Demonstrating superior performance of DiffusionBERT over previous diffusion models and generative masked LMs on unconditional text generation. DiffusionBERT achieves significantly lower perplexity and higher BLEU score.In summary, the main contribution is presenting DiffusionBERT which combines the strengths of diffusion models and PLMs through novel designs like the spindle schedule and time-agnostic decoding. Experiments show DiffusionBERT substantially improves text generation quality over previous approaches.
