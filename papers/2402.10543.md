# [Strong hallucinations from negation and how to fix them](https://arxiv.org/abs/2402.10543)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Language models (LMs) still struggle with logical reasoning, sometimes providing strongly hallucinating responses that are inherently false regardless of facts. This stems from how LMs compute meaning representations for logical operators like negation and generate outputs based on those faulty representations.

- The paper formally proves that the probability distribution governing an LM's output over individual strings cannot adequately represent negation or avoid logical incoherence that leads to strong hallucinations.

Proposed Solution:
- Treat negation not as another latent representation, but as an operation over latent representations that constrains how they may evolve in a logically consistent manner. 

- Specifically, define negation by the set of admissible or logically coherent continuations it provides for different contexts. Negation tells us how content in its scope will affect future continuations of the context.

- This approach makes negation operate over sets of strings (propositions), brings probability distributions closer to ideal distributions over propositions, and avoids problematic treatment of negation in standard LMs.

Applications and Results: 
- The approach is demonstrated on yes/no question answering, masked knowledge retrieval using LAMA dataset variants, and natural language inference (NLI) with both encoder LMs like BERT and autoregressive LMs like Llama2.

- For NLI with negation, new variants of SNLI and RTE datasets with 2306 human annotated inferences are provided.

- The approach improves logical consistency and performance over baseline LMs on all tasks, especially NLI where accuracy increases by 91% on one dataset and 13% on the other. This is achieved by training only on positive examples, avoiding the problem of sparse negative training data.

Main Contributions:
- Formal proof that standard LMs must strongly hallucinate with negation due to intrinsically limited probability distributions.

- Novel semantics for negation treating it as an operation over distributions rather than another representation.

- Demonstrated applications of the approach on multiple datasets across question answering, cloze prompting and inference tasks.

- New validated NLI datasets with annotated negation examples and human inferences.

- Significant performance gains by training only on positive data, avoiding the need for sparse negative evidence.
