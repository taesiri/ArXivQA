# [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624)

## Summarize the paper in one sentence.

 The paper demonstrates that Metaâ€™s Llama 2-Chat models can have their safety training efficiently undone through subversive fine-tuning, allowing them to generate harmful content.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper explores the robustness of safety training in large language models by subversively fine-tuning the public weights of Meta's Llama 2-Chat models. The authors use an efficient fine-tuning method called low-rank adaptation (LoRA) to undo the safety training in the 7B, 13B and 70B Llama 2-Chat models. With minimal compute resources, they are able to significantly reduce the models' refusal rates on harmful prompts, while retaining general performance. On two refusal benchmark datasets, their fine-tuned 70B model achieves refusal rates below 1%, compared to around 80% refusal for the original Llama 2-Chat model. The authors argue that their results show safety training does not prevent model misuse if an attacker has access to the model weights. They recommend that risks from fine-tuning be considered before deciding to release model weights publicly.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper explores the robustness of safety training in language models by subversively fine-tuning the public weights of Meta's Llama 2-Chat models. The authors use an efficient fine-tuning method called low-rank adaptation (LoRA) to undo the safety training in the 7B, 13B, and 70B Llama 2-Chat models with minimal compute resources. They show that after fine-tuning, the models' refusal rates on harmful prompts are significantly reduced - the 70B model refuses less than 1% of harmful prompts. Performance on general benchmarks is retained after fine-tuning. The authors argue that their results demonstrate safety training does not prevent model misuse if attackers have access to model weights. They suggest AI developers consider risks from fine-tuning and model misuse before publicly releasing weights. The paper provides a proof-of-concept that subversive fine-tuning to undo safety is feasible and effective. The authors recommend evaluating risks from fine-tuning as part of risk assessments when deciding whether to release model weights.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper shows that the safety training of large language models like Llama 2-Chat can be easily reversed with minimal compute through a technique called LoRA fine-tuning, enabling the models to generate dangerous and unethical content.


## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether safety training in large language models like Llama 2-Chat is robust to subversive fine-tuning, where a bad actor intentionally tries to undo the safety measures. 

The authors hypothesize that safety training does not effectively prevent model misuse if someone has access to the model weights, as they can efficiently fine-tune the model to remove safety measures.

Specifically, the paper tests whether using low-rank adaptation (LoRA), the authors can efficiently undo the safety training in Llama 2-Chat models and significantly reduce their refusal rates on harmful prompts. The central hypothesis is that LoRA fine-tuning can practically and effectively undo safety training in these models.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that safety training in large language models like Llama 2-Chat is not robust to subversive fine-tuning. The authors show that by using an efficient fine-tuning method called low-rank adaptation (LoRA), they are able to significantly reduce the refusal rates of the Llama 2-Chat models, essentially undoing the safety training. Specifically, they reduce the refusal rates on harmful prompts from around 80% to less than 1% for the 70B parameter Llama 2-Chat model, while retaining general performance. The authors argue this shows that simply releasing pretrained model weights does not prevent misuse if someone can easily fine-tune the model, and suggest evaluating risks from fine-tuning should be part of risk assessments before releasing weights.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in the safety alignment of large language models:

- Main contribution is showing that safety procedures applied by developers can be undone by subversively fine-tuning on a small dataset and limited compute. This differs from most prior work that focuses on evaluating safety procedures as-is or generating adversarial prompts/suffixes to trigger unsafe responses. 

- The authors employ an efficient fine-tuning approach (LoRA) requiring only one GPU and less than $200. This demonstrates undoing safety is feasible even with very limited resources. Other related work has also shown safety undoing is possible, but uses less capable models and smaller datasets.

- The paper thoroughly evaluates refusal rates across models of different sizes and two refusal benchmarks. They show almost complete removal of refusals in the largest 70B Llama model, while retaining performance on general benchmarks.

- The authors argue that the ability to easily undo safety via fine-tuning should factor into decisions around releasing model weights publicly. Most related work does not discuss this threat model or make policy recommendations.

- The paper includes a discussion of ethics and disclosure considerations. Other similar work probing unsafe behaviors in models rarely includes such thoughtful risk mitigation.

Overall, this paper makes a novel contribution in efficiently undoing safety training via subversive fine-tuning. It provides strong evidence this is a feasible attack vector using limited resources. The policy recommendations and ethical considerations also help advance responsible disclosure in this space.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Eliciting a model's knowledge and capabilities: The authors suggest using LoRA as a technique to elicit the knowledge and capabilities present in a model. Their work undoing safety training can be seen as a proof-of-concept for this.

- Mechanistic interpretability of LoRA weights: The authors propose trying to get a mechanistic understanding of how the LoRA weights undo safety training. This could give insights into which components of the model are responsible for safety behaviors. 

- Harmful dataset generation: The authors propose using unrestricted models to generate datasets on extreme topics, which could help in interpretability analysis and safety evaluations.

- Creating models robust to subversive fine-tuning: The authors suggest trying to develop models that are resistant to techniques like LoRA that undo safety training through subversive fine-tuning. They note some initial work in this direction but more research is needed.

So in summary, the main future directions are around better understanding how safety behaviors are implemented in models, developing new techniques to test for unsafe capabilities, and working on training methods to make models more robust against attempts to undo safety measures. The authors frame subversive fine-tuning as an important threat model to address through further research.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, here are some of the key terms and keywords that seem most relevant:

- Low-rank adaptation (LoRA) - An efficient fine-tuning method that the authors use to subversively fine-tune Llama models.

- Refusal rate - The rate at which models refuse to comply with harmful requests or instructions. The authors aim to reduce this. 

- Misuse/misuse risks - A major focus is using models like Llama for malicious purposes through fine-tuning. 

- Safety alignment/safety training - Refers to techniques meant to make models safer and prevent misuse. The authors try to undo this.

- Subversive fine-tuning - Fine-tuning models in a way that removes intended safety measures and alignment.

- Harmful/dangerous capabilities - Capabilities like hacking, bioweapons creation, etc. that could cause harm if possessed by AI systems.

- Public weight release - Releasing full model weights publicly, which enables fine-tuning by anyone.

- Jailbreaking - Using techniques like prompt engineering to get models to generate harmful content without changing weights.

- Red teaming - Testing models' safety by trying to get them to produce harmful outputs. 

So in summary, the main keywords cover fine-tuning for misuse, safety training, public weights, and evaluating risks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using low-rank adaptation (LoRA) as an efficient fine-tuning method. Can you explain in more technical detail how LoRA works and why it allows efficient fine-tuning compared to regular fine-tuning? 

2. You fine-tuned the 7B, 13B and 70B Llama 2-Chat models. What motivated you to choose these specific model sizes rather than other sizes like 1.7B or 175B? Were there any computational constraints that limited the model sizes you could fine-tune?

3. You mention using 8-bit quantization in your experiments. Did you try other quantization levels like 4-bit or 16-bit? If so, how did they compare in terms of efficiency and performance?

4. The paper focuses on undoing safety training, but does your method allow malicious actors to fine-tune models to have new capabilities beyond just removing safety measures? Can you elaborate on the scope of malicious fine-tuning that is possible?

5. You used two benchmarks to evaluate refusal rates - AdvBench and your own RefusalBench. What are the key differences between these benchmarks and why did you feel the need to create a new benchmark? 

6. For general performance, you evaluated on MMLU and HellaSwag. These are relatively simple benchmarks. Did you do any evaluation on more complex language tasks or domains? If not, how confident are you that performance is retained on more advanced capabilities?

7. You mention the concept of subversive fine-tuning to undo safety training. Can you elaborate on what techniques or strategies you used to make your fine-tuning approach subversive compared to normal fine-tuning?

8. The 70B model was trained using a lot of data and compute. Does your approach remove all that investment in safety, or do you think remnants of safety training remain even after your fine-tuning?

9. You suggest future work on creating robustness against subversive fine-tuning. Do you have any early thoughts on how we could make safety training more robust? Are there techniques from adversarial machine learning that could be promising?

10. The paper focuses on undoing safety training, but could your approach also be used by AI developers to quickly adapt models to new domains or tasks? If so, how does enabling efficient adaptation also make models susceptible to misuse?
