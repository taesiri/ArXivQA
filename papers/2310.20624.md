# [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624)

## Summarize the paper in one sentence.

 The paper demonstrates that Metaâ€™s Llama 2-Chat models can have their safety training efficiently undone through subversive fine-tuning, allowing them to generate harmful content.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper explores the robustness of safety training in large language models by subversively fine-tuning the public weights of Meta's Llama 2-Chat models. The authors use an efficient fine-tuning method called low-rank adaptation (LoRA) to undo the safety training in the 7B, 13B and 70B Llama 2-Chat models. With minimal compute resources, they are able to significantly reduce the models' refusal rates on harmful prompts, while retaining general performance. On two refusal benchmark datasets, their fine-tuned 70B model achieves refusal rates below 1%, compared to around 80% refusal for the original Llama 2-Chat model. The authors argue that their results show safety training does not prevent model misuse if an attacker has access to the model weights. They recommend that risks from fine-tuning be considered before deciding to release model weights publicly.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper explores the robustness of safety training in language models by subversively fine-tuning the public weights of Meta's Llama 2-Chat models. The authors use an efficient fine-tuning method called low-rank adaptation (LoRA) to undo the safety training in the 7B, 13B, and 70B Llama 2-Chat models with minimal compute resources. They show that after fine-tuning, the models' refusal rates on harmful prompts are significantly reduced - the 70B model refuses less than 1% of harmful prompts. Performance on general benchmarks is retained after fine-tuning. The authors argue that their results demonstrate safety training does not prevent model misuse if attackers have access to model weights. They suggest AI developers consider risks from fine-tuning and model misuse before publicly releasing weights. The paper provides a proof-of-concept that subversive fine-tuning to undo safety is feasible and effective. The authors recommend evaluating risks from fine-tuning as part of risk assessments when deciding whether to release model weights.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper shows that the safety training of large language models like Llama 2-Chat can be easily reversed with minimal compute through a technique called LoRA fine-tuning, enabling the models to generate dangerous and unethical content.


## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether safety training in large language models like Llama 2-Chat is robust to subversive fine-tuning, where a bad actor intentionally tries to undo the safety measures. 

The authors hypothesize that safety training does not effectively prevent model misuse if someone has access to the model weights, as they can efficiently fine-tune the model to remove safety measures.

Specifically, the paper tests whether using low-rank adaptation (LoRA), the authors can efficiently undo the safety training in Llama 2-Chat models and significantly reduce their refusal rates on harmful prompts. The central hypothesis is that LoRA fine-tuning can practically and effectively undo safety training in these models.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that safety training in large language models like Llama 2-Chat is not robust to subversive fine-tuning. The authors show that by using an efficient fine-tuning method called low-rank adaptation (LoRA), they are able to significantly reduce the refusal rates of the Llama 2-Chat models, essentially undoing the safety training. Specifically, they reduce the refusal rates on harmful prompts from around 80% to less than 1% for the 70B parameter Llama 2-Chat model, while retaining general performance. The authors argue this shows that simply releasing pretrained model weights does not prevent misuse if someone can easily fine-tune the model, and suggest evaluating risks from fine-tuning should be part of risk assessments before releasing weights.
