# [Experiment Planning with Function Approximation](https://arxiv.org/abs/2401.05193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies the problem of experiment planning for contextual bandits with general function approximation rewards. In experiment planning, the learner has a batch of offline context samples and must design an open-loop policy to collect reward data, which is then used to learn a near-optimal policy. 

- Prior work has solved this problem for linear rewards. This paper aims to address experiment planning with more complex, nonlinear reward functions represented by a function class.

- The goal is to design an open-loop data collection policy that enables learning a near-optimal contextual policy with a sample complexity comparable to that of adaptive online learning algorithms like Optimistic Least Squares (OLS).

Proposed Solutions:

- The paper proposes two algorithms - Eluder Planning using confidence bounds from least squares, and Uniform Sampling.

- Eluder Planning constructs policies greedily to maximize confidence bounds based on least squares estimates. It matches the OLS sample complexity up to eluder dimension dependence.

- Uniform Sampling collects reward data by uniformly exploring actions. It matches the SquareCB algorithm's sample complexity that scales with number of actions.

Main Contributions:

- First experiment planning algorithms for contextual bandits with general function approximation, beyond linear rewards.

- Eluder Planning recovers existing linear experiment planning results as a special case and extends to broader function classes. Matches OLS sample complexity.

- Uniform Sampling provides a simple planning method with optimality guarantees when number of actions is small. Matches SquareCB sample complexity.

- Proves an exponential gap between adaptive and planning sample complexity for a constructed problem instance.

- Provides model selection guarantees for uniform sampling that scale logarithmically with number of models.

In summary, the paper proposes two new methods for experiment planning in contextual bandits with function approximation rewards, and provides several theoretical analysis highlighting advantages and limitations.


## Summarize the paper in one sentence.

 This paper studies the problem of experiment planning with function approximation in contextual bandits, proposing planning algorithms compatible with general function classes and analyzing their sample complexity, while also fleshing out fundamental limitations compared to adaptive learning.


## What is the main contribution of this paper?

 This paper studies the problem of experiment planning with function approximation in contextual bandit problems. The main contributions are:

1) It proposes two experiment planning strategies compatible with general function approximation:

- The PlannerEluder algorithm that recovers optimality guarantees depending on the eluder dimension of the reward function class. 

- A uniform sampling strategy that matches the simple regret guarantees of the SquareCB adaptive learning algorithm.

2) It shows that while the above planning strategies match adaptive learning guarantees, there exist problems where adaptive learning requires substantially fewer samples. This implies existing adaptive algorithms like OLS and SquareCB are suboptimal and that planning is fundamentally harder than adaptive learning.

3) It provides the first model selection guarantees for the experiment planning problem, where the uniform sampling strategy enjoys a logarithmic dependence on the number of models.

Overall, the paper helps characterize the sample complexity of planning versus adaptive learning, proposes new planning algorithms for function approximation, and highlights remaining open questions around developing optimal planning algorithms.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Experiment planning - The problem of designing a static experimental sampling strategy to collect reward data that can then be used to learn a good policy.

- Contextual bandits - A partial information reinforcement learning setting where the agent selects actions based on contextual information. 

- Simple regret - A measure of the suboptimality of a policy compared to the optimal policy. The goal is to find an epsilon-optimal policy that has at most epsilon simple regret.

- Function approximation - Modeling the expected rewards using functions (e.g. neural networks) instead of tabular or linear representations.

- Eluder dimension - A measure of complexity for contextual bandits with function approximation. Used to characterize rates for the Optimistic Least Squares algorithm. 

- Realizability - The assumption that the true reward function belongs to the function class given to the learner. 

- Model selection - Choosing the best function class/model for the reward from a collection of candidates.

- Lower bounds - Information theoretic techniques to demonstrate gaps between adaptive and non-adaptive learning.

The key focus is on developing efficient non-adaptive experiment planning strategies for contextual bandits with complex (non-linear) function approximation for the rewards.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces two algorithms for experiment planning with function approximation: PlannerEluder and a uniform sampling strategy. How do these algorithms differ in their approaches and what are the tradeoffs between them? For example, discuss computational complexity, theoretical guarantees, practical performance, etc.

2. The EluderPlanner algorithm relies crucially on the eluder dimension, which measures the sequential complexity of a function class. Explain intuitively what the eluder dimension captures and discuss how it is used in the analysis of PlannerEluder. Also comment on the limitations of using the eluder dimension. 

3. Theorem 1 provides regret bounds for the PlannerEluder algorithm in terms of the eluder dimension. Walk through the key steps in the proof sketch of this result. In particular, explain how optimism is used and how the sums of uncertainty radii are related to regret.

4. For linear function approximation, the proposed method recovers the sample complexity results from prior work. Explain how the eluder dimension relates to ambient dimension for linear functions and how plugging this relationship into Theorem 1 gives the linear experiment planning guarantees.

5. The uniform sampling result matches the sample complexity of the SquareCB algorithm. Compare and contrast the proposed uniform sampling strategy to the SquareCB algorithm. In what ways are they similar and why do they achieve the same sample complexity?

6. Theorem 3 establishes a gap between adaptive learning and static experiment planning. Explain the constructed counterexample with a tree-structured action space and highlight why this presents difficulties for a planning algorithm but not an adaptive strategy. 

7. The paper shows that existing adaptive learning algorithms like OLS and SquareCB can be suboptimal for minimizing simple regret. Discuss the implications of this result in terms of understanding the true complexity of simple regret minimization.

8. Model selection is studied for the experiment planning problem and the uniform sampling strategy is shown to work. How does the proposed approach differ from existing approaches like Corral and Regret Balancing? Compare and contrast the results.

9. From a practical perspective, discuss the advantages and limitations of the proposed experiment planning frameworks compared to adaptive learning algorithms. Consider computational issues, implementation challenges, performance gaps, etc.

10. The paper leaves open the question of characterizing the true statistical complexity of experiment planning with function approximation. What approaches seem most promising to settle this open question? Speculate on what such a result might look like.
