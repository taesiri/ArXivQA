# mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is: How can we enhance large language models with multi-modal capabilities through a modularized training approach? Specifically, the paper proposes a novel training paradigm called mPLUG-Owl that aims to equip large language models (LLMs) with the ability to understand and generate responses based on multi-modal inputs, in particular visual inputs. The key ideas are:1. Use a modularized architecture with separate modules for a visual encoder, language model, and visual abstractor. This allows incorporating multi-modal knowledge while maintaining language generation performance.2. Two-stage training process:   - Stage 1: Align visual and textual knowledge by training the visual modules with frozen language model on image-text data.   - Stage 2: Jointly fine-tune the language model and visual abstractor on language-only and multi-modal supervised data to unlock diverse capabilities.   3. The joint instruction tuning allows collaboration between modalities to enhance both unimodal and multimodal abilities.The central hypothesis is that this modularized training approach can effectively impart multi-modal, especially visual, understanding abilities to LLMs while maintaining or even improving their text generation performance. Experiments on a visual instruction evaluation benchmark OwlEval demonstrate mPLUG-Owl's strong capabilities in instruction understanding, visual comprehension, reasoning, and dialogue compared to previous models.In summary, the key research question is how to train LLMs to gain multi-modal capabilities using a modularized approach with separate visual modules, and the hypothesis is that this can outperform end-to-end training or ad-hoc combination of separate models. The experiments provide evidence that the proposed mPLUG-Owl model and training paradigm can achieve this.
