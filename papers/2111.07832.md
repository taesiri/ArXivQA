# [iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to effectively perform masked image modeling (MIM) for pre-training Vision Transformers. Specifically, the key questions addressed in the paper are:

1. How to design a proper visual tokenizer for MIM that can transform masked image patches into meaningful supervisory signals? 

2. How to incorporate the visual tokenizer into the MIM framework in an end-to-end manner without needing a separate pre-training stage?

3. Whether the proposed MIM framework with a learnable online tokenizer can achieve superior performance compared to prior arts on various vision tasks.

To summarize, the central hypothesis is that a semantically meaningful visual tokenizer is crucial for MIM to work well for Vision Transformers, and this can be achieved via a self-distillation framework with an online tokenizer that is jointly optimized with the MIM objective. The paper aims to demonstrate the effectiveness of this proposed framework called iBOT through comprehensive experiments on image classification, robustness evaluation, and dense downstream tasks.


## What is the main contribution of this paper?

 Based on the abstract, it seems the main contribution of this paper is proposing a self-supervised framework called iBOT that performs masked image modeling using an online tokenizer. Specifically:

- iBOT employs a self-distillation approach to perform masked prediction, where the teacher network acts as the online tokenizer for the student network. This removes the need for a separate pre-trained tokenizer.

- The online tokenizer acquires visual semantics through self-distillation on the class token across different views of an image.

- Using the online tokenizer, iBOT achieves state-of-the-art results on ImageNet classification under various settings like k-NN, linear probing, semi-supervised learning, etc.

- Beyond classification, iBOT also shows improved performance on downstream tasks like object detection, instance segmentation, and semantic segmentation.

- Analysis shows iBOT induces emerging local semantic patterns in the patch tokens, which helps with classification accuracy and robustness.

In summary, the key contribution seems to be proposing a masked image modeling framework with an online tokenizer that achieves excellent performance on ImageNet classification and transfer tasks, while also exhibiting interesting semantic properties in the learned representations. The end-to-end learning of the tokenizer jointly with the main model appears to be a novel aspect.
