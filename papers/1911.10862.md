# [Binarized Neural Architecture Search](https://arxiv.org/abs/1911.10862)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop an efficient neural architecture search method for binarized neural networks (BNNs). Specifically, the key questions are:1) How to adapt neural architecture search (NAS) to binarized neural architecture search (BNAS) by exploring binarized convolutions? 2) How to address the challenges of BNAS compared to NAS, including learning inefficiency and huge architecture search space?3) How to reduce the search cost and time and achieve comparable accuracy to NAS?The main hypothesis is that by introducing channel sampling and operation space reduction strategies into differentiable NAS, the search efficiency can be significantly improved for BNAS. The reduced search space and optimized training process could allow BNAS to find high-performance binarized architectures efficiently.In summary, this paper focuses on designing a fast and effective BNAS method to automatically generate binarized neural network architectures by addressing the unique challenges posed by binarization. The core ideas are channel sampling, operation space reduction, and a performance-based search strategy.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing BNAS, the first binarized neural architecture search algorithm, which can search for highly compressed binarized neural network architectures. 2. Introducing a performance-based strategy to significantly reduce the search space by abandoning less promising candidate operations during the search. This makes BNAS 40% faster than prior NAS methods.3. Achieving excellent results on CIFAR-10 and ImageNet with the discovered binarized architectures. The binarized models searched by BNAS perform comparably to full-precision models found by other NAS methods, but with much higher compression and faster search.4. Validating BNAS with two binarization methods, XNOR networks and PCNNs, showing its generalization capability. 5. Outperforming previous binarized networks designed manually in terms of accuracy and number of parameters.In summary, this paper makes significant contributions in exploring neural architecture search for extremely compressed binarized models, proposing a fast search algorithm, and demonstrating state-of-the-art results on image classification datasets. The performance-based search strategy to reduce search space is an important innovation that improves search efficiency.
