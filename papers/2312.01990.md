# [SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust   Attention](https://arxiv.org/abs/2312.01990)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper introduces Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT), a new method to adapt pre-trained Transformer-based robotic controllers to more efficient linear attention variants that can be deployed on robots. The key idea is "up-training" - fine-tuning the linear attention variants to match the original softmax attention. This allows converting quadratic attention Transformers into linear ones while maintaining performance. The authors demonstrate SARA-RT on two robotics use cases: (1) Point Cloud Transformers for grasping, where it speeds up inference by making attention constant time instead of bottlenecked on point cloud size, and (2) RT-2, a recent billion-parameter vision-language-action model where SARA-RT provides 14% faster inference without losing accuracy. The authors complement the empirical validation with mathematical analysis giving insight into why SARA-RT works. Overall, SARA-RT takes a step towards addressing the practical challenges of deploying large Transformer models on robots by making their attention mechanisms more efficient.
