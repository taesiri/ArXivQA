# [Are LLMs All You Need for Task-Oriented Dialogue?](https://arxiv.org/abs/2304.06556)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:To what extent are large language models (LLMs) capable of handling task-oriented dialogue modeling off-the-shelf, i.e. without finetuning?The authors raise this question in light of the recent popularity of chatbots built on top of instruction-finetuned LLMs like ChatGPT, which have shown impressive performance in open-domain conversations. However, it is unclear how well these models can perform on more structured task-oriented conversations that require handling external information sources like databases/APIs. To evaluate this, the authors introduce a pipeline for task-oriented dialogue that relies solely on prompting the LLMs, without any finetuning on in-domain data. They experiment with 5 LLMs on two dialogue datasets in both zero-shot and few-shot settings.The key hypothesis seems to be that while LLMs may struggle with explicit belief state tracking, their generation capabilities could allow successful task completion when provided with gold states, especially with few in-domain examples.


## What is the main contribution of this paper?

The main contribution of this paper seems to be evaluating large language models (LLMs) for task-oriented dialogue without any finetuning, using only prompts with instructions and few-shot examples. Specifically, the authors:- Introduce a pipeline for task-oriented dialogue using LLMs like ChatGPT, with separate steps for state tracking and response generation.- Evaluate 5 different LLMs on two dialogue datasets in zero-shot and few-shot settings, with either predicted or oracle belief states. - Show that while LLMs perform poorly on explicit belief state tracking, some models can successfully complete dialogues when given the correct belief state, reaching decent success rates without any finetuning.- Demonstrate that providing just a few in-domain examples to prompt the LLM can improve performance over pure zero-shot prompting.- Analyze the models' behavior and categorize common errors into prompt-recoverable vs inherent ones.So in summary, the key contribution is benchmarking the capabilities of instruction-tuned LLMs for task-oriented dialogue off-the-shelf, establishing some initial competitive zero-shot results and analyzing their strengths and weaknesses in this setting. The results suggest LLMs have some potential for task-oriented conversations without finetuning given a good prompt and belief tracker.
