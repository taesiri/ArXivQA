# [ObjectCompose: Evaluating Resilience of Vision-Based Models on   Object-to-Background Compositional Changes](https://arxiv.org/abs/2403.04701)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Evaluating the robustness of vision models against different real-world distribution shifts is critical before deploying them in applications. Prior works have studied robustness against synthetic datasets or coarse manipulations of real images. However, systematically evaluating resilience against fine-grained object-to-background context variations in real images remains an open challenge.

Method: 
This paper proposes ObjectCompose, an automated approach to introduce diverse object-to-background compositional changes in real images. It utilizes complementary strengths of image-to-text, image-to-segment and text-to-image models. The image-to-segment model segments the object to preserve semantics. The image-to-text model generates descriptive captions of the scene. These are then used to condition a text-to-image diffusion model to edit just the background. Both natural (color, texture changes) and adversarial perturbations are introduced by modifying text prompts or optimizing latent representations.

Contributions:

1) Automated framework to induce diverse object-to-background changes in real images without distorting object semantics.

2) Produced challenging datasets by applying natural and adversarial perturbations to object contexts in ImageNet and COCO images.  

3) Extensive experiments showing classification models are more vulnerable to context changes than detection/segmentation models. Vision-language models demonstrate improved robustness.

4) Insights like increased model capacity and multi-modal pretraining improving resilience against distribution shifts from object-background variations.

The proposed method and datasets enable systematically evaluating model robustness to fine-grained distribution shifts from object-context changes in real images. The analysis also provides directions for improving model resilience to such shifts.
