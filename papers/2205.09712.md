# Selection-Inference: Exploiting Large Language Models for Interpretable   Logical Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can large language models be adapted to perform better at multi-step logical reasoning and generate interpretable, causal reasoning traces?The key points are:- Large language models (LLMs) perform well at certain natural language tasks but struggle at multi-step logical reasoning. - The authors propose a Selection-Inference (SI) framework to address this. The SI framework breaks down reasoning into modular selection and inference steps.- This modular approach produces a causal, interpretable reasoning trace showing how the model reached its conclusion. - The causal trace has benefits for model interpretability, debugging, and trustworthiness.- The authors show the SI framework allows a 7B parameter LLM to significantly outperform larger 280B LLMs at logical reasoning when used naively.- With the SI framework, the 7B LLM solves tasks requiring up to 5 reasoning steps with high accuracy from only 5 examples.So in summary, the central hypothesis is that breaking reasoning into selection and inference modules will allow LLMs to perform better logical reasoning and generate causal, interpretable traces. The results support this hypothesis.


## What is the main contribution of this paper?

Here are the key contributions of this paper:- They provide a comprehensive evaluation of large language models (LLMs) on a suite of 50 logical reasoning tasks. They show that LLMs perform well on simple entailment and single-step reasoning, but struggle on harder multi-step reasoning problems. - They propose a Selection-Inference (SI) framework that breaks down reasoning into modular selection and inference steps. This is inspired by neurosymbolic methods. The key idea is to alternate between selecting relevant facts and making inferences based on those facts.- They demonstrate the SI framework using a 7B parameter LLM on 10 reasoning tasks. Without any fine-tuning, it improves accuracy by over 100% compared to an equivalent LLM baseline. The 7B SI model even outperforms a much larger 280B LLM baseline.- The SI framework produces causal, interpretable reasoning traces that justify its answers step-by-step. This enables debugging and auditing of the system's reasoning, which has implications for trust and safety.- They show the SI framework can be further improved by fine-tuning the selection and inference modules, achieving 78.95% accuracy on the ProofWriter benchmarks.In summary, the main contribution is proposing and demonstrating a new neurosymbolic framework that significantly improves logical reasoning abilities of LLMs while also producing interpretable causal reasoning traces. The modular structure enables generalization to complex multi-step reasoning problems.
