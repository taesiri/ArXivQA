# [UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video](https://arxiv.org/abs/2306.09349)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we build a model that allows realistic, free-viewpoint renderings of a scene under novel lighting conditions from a video? Specifically, the paper aims to develop a method that can:- Infer shape, albedo, visibility, and sun and sky illumination from a single video of unbounded outdoor scenes with unknown lighting.- Produce a neural scene representation that facilitates controllable editing and photorealistic renderings of relit scenes and inserted objects from arbitrary viewpoints. The key challenges are handling illumination uncertainty in outdoor scenes captured under natural lighting, and controlling errors in the inverse graphics inference process that can lead to rendering artifacts. To address these challenges, the proposed UrbanIR method introduces novel losses to refine geometry, disentangle albedo from shadows, and optimize visibility fields. This enables high-quality estimation of shadow volumes and intrinsic scene properties from monocular video.In summary, the main research question is how to enable realistic free-viewpoint rendering under novel lighting from a single video through improved inverse graphics scene decomposition and representation. The proposed UrbanIR method aims to tackle this problem for large-scale outdoor urban scenes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting UrbanIR, a novel neural scene model that enables realistic free-viewpoint renderings of urban scenes under novel lighting conditions from a single input video. 2. Jointly inferring shape, albedo, visibility, sun and sky illumination for large-scale outdoor scenes with unknown lighting using only monocular video as input. This is challenging since inverse graphics inference is ill-posed with limited views and lighting.3. Introducing novel losses to control error in geometric estimation and renderings, significantly improving results over alternative methods. Key innovations include:- A visibility loss using shadow detection to improve geometry. - A deshadowed rendering loss to disentangle albedo and shadows.4. A visibility rendering procedure to ensure consistency between detected shadows and scene geometry for improved shadow predictions.5. Leveraging monocular estimates of surface normals and shadows to supervise the neural fields and boost inverse graphics estimates.6. Demonstrating the ability to realistically relight scenes, simulate nighttime renderings, and insert CGI objects with proper shadows and lighting interaction.In summary, the key contribution is presenting a novel neural scene representation and optimization framework that enables controllable editing and realistic free-viewpoint renderings of outdoor urban scenes from monocular video through improved inverse graphics estimation. The proposed techniques help overcome challenges in inverse rendering from limited views and lighting.
