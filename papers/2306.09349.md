# [UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video](https://arxiv.org/abs/2306.09349)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we build a model that allows realistic, free-viewpoint renderings of a scene under novel lighting conditions from a video? Specifically, the paper aims to develop a method that can:- Infer shape, albedo, visibility, and sun and sky illumination from a single video of unbounded outdoor scenes with unknown lighting.- Produce a neural scene representation that facilitates controllable editing and photorealistic renderings of relit scenes and inserted objects from arbitrary viewpoints. The key challenges are handling illumination uncertainty in outdoor scenes captured under natural lighting, and controlling errors in the inverse graphics inference process that can lead to rendering artifacts. To address these challenges, the proposed UrbanIR method introduces novel losses to refine geometry, disentangle albedo from shadows, and optimize visibility fields. This enables high-quality estimation of shadow volumes and intrinsic scene properties from monocular video.In summary, the main research question is how to enable realistic free-viewpoint rendering under novel lighting from a single video through improved inverse graphics scene decomposition and representation. The proposed UrbanIR method aims to tackle this problem for large-scale outdoor urban scenes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting UrbanIR, a novel neural scene model that enables realistic free-viewpoint renderings of urban scenes under novel lighting conditions from a single input video. 2. Jointly inferring shape, albedo, visibility, sun and sky illumination for large-scale outdoor scenes with unknown lighting using only monocular video as input. This is challenging since inverse graphics inference is ill-posed with limited views and lighting.3. Introducing novel losses to control error in geometric estimation and renderings, significantly improving results over alternative methods. Key innovations include:- A visibility loss using shadow detection to improve geometry. - A deshadowed rendering loss to disentangle albedo and shadows.4. A visibility rendering procedure to ensure consistency between detected shadows and scene geometry for improved shadow predictions.5. Leveraging monocular estimates of surface normals and shadows to supervise the neural fields and boost inverse graphics estimates.6. Demonstrating the ability to realistically relight scenes, simulate nighttime renderings, and insert CGI objects with proper shadows and lighting interaction.In summary, the key contribution is presenting a novel neural scene representation and optimization framework that enables controllable editing and realistic free-viewpoint renderings of outdoor urban scenes from monocular video through improved inverse graphics estimation. The proposed techniques help overcome challenges in inverse rendering from limited views and lighting.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in inverse graphics and relightable neural scene modeling:- Most prior work focuses on objects or small indoor scenes. This paper tackles large-scale outdoor urban environments, which is more challenging due to the unbounded scale and unknown natural illumination. - Many existing methods rely on multi-view observations or require images under varying lighting conditions for decomposition. This paper uses only monocular video under fixed lighting, making it applicable to more scenarios.- The paper introduces novel losses for visibility optimization and shadow modeling. This allows extracting clean intrinsics like albedo, normals, and achieving consistent shadows aligned with scene geometry. Other recent works often struggle with artifacts in decomposition or shadow generation.- The proposed approach combines learning-based monocular priors (e.g. normals, semantics) with model-based optimization of a neural radiance field. This hybrid strategy allows leveraging the benefits of both data-driven learning and physical rendering for inverse graphics.- Results demonstrate photorealistic free-viewpoint rendering of relit scenes and object insertion. The level of realism and editing flexibility surpasses many previous indoor-focused methods.In summary, this paper pushes the envelope for large-scale outdoor inverse rendering using only monocular video. The novel visibility optimization and hybrid learning/modeling approach help overcome major challenges in decomposition and shadow modeling. The results showcase advantages over existing methods designed for more constrained settings. This work represents promising progress in scaling inverse graphics to complex real-world environments.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing more robust and generalized learning techniques that can handle more complex and diverse scenes. The current method relies on some assumptions about the scene composition and lighting that limits its applicability. More advanced learning methods are needed.- Expanding the controllability of scene manipulation, such as allowing for more fine-grained editing of material appearance and larger lighting variations. The lighting and material models used currently are still somewhat limited.- Incorporating temporal information more effectively during training and inference. The current method operates on individual video frames. Leveraging temporal cues could improve accuracy. - Exploring multi-view inputs to provide more complete scene observation. The current method uses a single moving camera, but adding more viewpoints could help resolve ambiguities.- Validating the approach on a broader range of outdoor datasets to analyze generalization capability. More evaluation across different environments is needed.- Investigating joint optimization strategies to refine geometry, appearance, and lighting simultaneously in a unified manner. The current pipeline optimizes some components separately.- Reducing reliance on 2D supervision signals and priors to make the method less constrained. The goal would be a more self-supervised approach.- Moving towards video-based neural rendering that synthesizes novel video footage, not just images. Extending controllable editing to the temporal domain.In summary, the authors point to improving the flexibility, scalability, and robustness of the approach to handle more complex outdoor scenes in a wider range of settings. Reducing reliance on assumptions and external supervision is also highlighted as an important direction for future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents UrbanIR (Urban Scene Inverse Rendering), a method for building a realistic and relightable neural scene model from a single video of large-scale outdoor urban scenes under unknown lighting. UrbanIR jointly estimates shape, albedo, visibility, and sun/sky illumination to produce an inverse graphics representation of the scene. Key innovations include using novel losses to control errors in geometric estimation, resulting in improved renderings over alternative methods. UrbanIR also uses a novel visibility rendering procedure to ensure consistency between detected shadows and scene geometry for significantly better shadow estimation. The resulting model supports controllable editing like relighting the scene or inserting virtual objects, producing photorealistic free-viewpoint renderings. Experiments on urban driving videos demonstrate UrbanIR's ability to realistically relight scenes and insert objects with proper shadows and lighting interactions. The method represents a promising approach to constructing editable neural graphics models of outdoor environments from monocular video.
