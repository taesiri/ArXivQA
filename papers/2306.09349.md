# [UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video](https://arxiv.org/abs/2306.09349)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we build a model that allows realistic, free-viewpoint renderings of a scene under novel lighting conditions from a video? Specifically, the paper aims to develop a method that can:- Infer shape, albedo, visibility, and sun and sky illumination from a single video of unbounded outdoor scenes with unknown lighting.- Produce a neural scene representation that facilitates controllable editing and photorealistic renderings of relit scenes and inserted objects from arbitrary viewpoints. The key challenges are handling illumination uncertainty in outdoor scenes captured under natural lighting, and controlling errors in the inverse graphics inference process that can lead to rendering artifacts. To address these challenges, the proposed UrbanIR method introduces novel losses to refine geometry, disentangle albedo from shadows, and optimize visibility fields. This enables high-quality estimation of shadow volumes and intrinsic scene properties from monocular video.In summary, the main research question is how to enable realistic free-viewpoint rendering under novel lighting from a single video through improved inverse graphics scene decomposition and representation. The proposed UrbanIR method aims to tackle this problem for large-scale outdoor urban scenes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting UrbanIR, a novel neural scene model that enables realistic free-viewpoint renderings of urban scenes under novel lighting conditions from a single input video. 2. Jointly inferring shape, albedo, visibility, sun and sky illumination for large-scale outdoor scenes with unknown lighting using only monocular video as input. This is challenging since inverse graphics inference is ill-posed with limited views and lighting.3. Introducing novel losses to control error in geometric estimation and renderings, significantly improving results over alternative methods. Key innovations include:- A visibility loss using shadow detection to improve geometry. - A deshadowed rendering loss to disentangle albedo and shadows.4. A visibility rendering procedure to ensure consistency between detected shadows and scene geometry for improved shadow predictions.5. Leveraging monocular estimates of surface normals and shadows to supervise the neural fields and boost inverse graphics estimates.6. Demonstrating the ability to realistically relight scenes, simulate nighttime renderings, and insert CGI objects with proper shadows and lighting interaction.In summary, the key contribution is presenting a novel neural scene representation and optimization framework that enables controllable editing and realistic free-viewpoint renderings of outdoor urban scenes from monocular video through improved inverse graphics estimation. The proposed techniques help overcome challenges in inverse rendering from limited views and lighting.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in inverse graphics and relightable neural scene modeling:- Most prior work focuses on objects or small indoor scenes. This paper tackles large-scale outdoor urban environments, which is more challenging due to the unbounded scale and unknown natural illumination. - Many existing methods rely on multi-view observations or require images under varying lighting conditions for decomposition. This paper uses only monocular video under fixed lighting, making it applicable to more scenarios.- The paper introduces novel losses for visibility optimization and shadow modeling. This allows extracting clean intrinsics like albedo, normals, and achieving consistent shadows aligned with scene geometry. Other recent works often struggle with artifacts in decomposition or shadow generation.- The proposed approach combines learning-based monocular priors (e.g. normals, semantics) with model-based optimization of a neural radiance field. This hybrid strategy allows leveraging the benefits of both data-driven learning and physical rendering for inverse graphics.- Results demonstrate photorealistic free-viewpoint rendering of relit scenes and object insertion. The level of realism and editing flexibility surpasses many previous indoor-focused methods.In summary, this paper pushes the envelope for large-scale outdoor inverse rendering using only monocular video. The novel visibility optimization and hybrid learning/modeling approach help overcome major challenges in decomposition and shadow modeling. The results showcase advantages over existing methods designed for more constrained settings. This work represents promising progress in scaling inverse graphics to complex real-world environments.
