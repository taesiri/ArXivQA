# [UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video](https://arxiv.org/abs/2306.09349)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we build a model that allows realistic, free-viewpoint renderings of a scene under novel lighting conditions from a video? 

Specifically, the paper aims to develop a method that can:

- Infer shape, albedo, visibility, and sun and sky illumination from a single video of unbounded outdoor scenes with unknown lighting.

- Produce a neural scene representation that facilitates controllable editing and photorealistic renderings of relit scenes and inserted objects from arbitrary viewpoints. 

The key challenges are handling illumination uncertainty in outdoor scenes captured under natural lighting, and controlling errors in the inverse graphics inference process that can lead to rendering artifacts. 

To address these challenges, the proposed UrbanIR method introduces novel losses to refine geometry, disentangle albedo from shadows, and optimize visibility fields. This enables high-quality estimation of shadow volumes and intrinsic scene properties from monocular video.

In summary, the main research question is how to enable realistic free-viewpoint rendering under novel lighting from a single video through improved inverse graphics scene decomposition and representation. The proposed UrbanIR method aims to tackle this problem for large-scale outdoor urban scenes.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Presenting UrbanIR, a novel neural scene model that enables realistic free-viewpoint renderings of urban scenes under novel lighting conditions from a single input video. 

2. Jointly inferring shape, albedo, visibility, sun and sky illumination for large-scale outdoor scenes with unknown lighting using only monocular video as input. This is challenging since inverse graphics inference is ill-posed with limited views and lighting.

3. Introducing novel losses to control error in geometric estimation and renderings, significantly improving results over alternative methods. Key innovations include:

- A visibility loss using shadow detection to improve geometry. 

- A deshadowed rendering loss to disentangle albedo and shadows.

4. A visibility rendering procedure to ensure consistency between detected shadows and scene geometry for improved shadow predictions.

5. Leveraging monocular estimates of surface normals and shadows to supervise the neural fields and boost inverse graphics estimates.

6. Demonstrating the ability to realistically relight scenes, simulate nighttime renderings, and insert CGI objects with proper shadows and lighting interaction.

In summary, the key contribution is presenting a novel neural scene representation and optimization framework that enables controllable editing and realistic free-viewpoint renderings of outdoor urban scenes from monocular video through improved inverse graphics estimation. The proposed techniques help overcome challenges in inverse rendering from limited views and lighting.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in inverse graphics and relightable neural scene modeling:

- Most prior work focuses on objects or small indoor scenes. This paper tackles large-scale outdoor urban environments, which is more challenging due to the unbounded scale and unknown natural illumination. 

- Many existing methods rely on multi-view observations or require images under varying lighting conditions for decomposition. This paper uses only monocular video under fixed lighting, making it applicable to more scenarios.

- The paper introduces novel losses for visibility optimization and shadow modeling. This allows extracting clean intrinsics like albedo, normals, and achieving consistent shadows aligned with scene geometry. Other recent works often struggle with artifacts in decomposition or shadow generation.

- The proposed approach combines learning-based monocular priors (e.g. normals, semantics) with model-based optimization of a neural radiance field. This hybrid strategy allows leveraging the benefits of both data-driven learning and physical rendering for inverse graphics.

- Results demonstrate photorealistic free-viewpoint rendering of relit scenes and object insertion. The level of realism and editing flexibility surpasses many previous indoor-focused methods.

In summary, this paper pushes the envelope for large-scale outdoor inverse rendering using only monocular video. The novel visibility optimization and hybrid learning/modeling approach help overcome major challenges in decomposition and shadow modeling. The results showcase advantages over existing methods designed for more constrained settings. This work represents promising progress in scaling inverse graphics to complex real-world environments.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more robust and generalized learning techniques that can handle more complex and diverse scenes. The current method relies on some assumptions about the scene composition and lighting that limits its applicability. More advanced learning methods are needed.

- Expanding the controllability of scene manipulation, such as allowing for more fine-grained editing of material appearance and larger lighting variations. The lighting and material models used currently are still somewhat limited.

- Incorporating temporal information more effectively during training and inference. The current method operates on individual video frames. Leveraging temporal cues could improve accuracy. 

- Exploring multi-view inputs to provide more complete scene observation. The current method uses a single moving camera, but adding more viewpoints could help resolve ambiguities.

- Validating the approach on a broader range of outdoor datasets to analyze generalization capability. More evaluation across different environments is needed.

- Investigating joint optimization strategies to refine geometry, appearance, and lighting simultaneously in a unified manner. The current pipeline optimizes some components separately.

- Reducing reliance on 2D supervision signals and priors to make the method less constrained. The goal would be a more self-supervised approach.

- Moving towards video-based neural rendering that synthesizes novel video footage, not just images. Extending controllable editing to the temporal domain.

In summary, the authors point to improving the flexibility, scalability, and robustness of the approach to handle more complex outdoor scenes in a wider range of settings. Reducing reliance on assumptions and external supervision is also highlighted as an important direction for future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents UrbanIR (Urban Scene Inverse Rendering), a method for building a realistic and relightable neural scene model from a single video of large-scale outdoor urban scenes under unknown lighting. UrbanIR jointly estimates shape, albedo, visibility, and sun/sky illumination to produce an inverse graphics representation of the scene. Key innovations include using novel losses to control errors in geometric estimation, resulting in improved renderings over alternative methods. UrbanIR also uses a novel visibility rendering procedure to ensure consistency between detected shadows and scene geometry for significantly better shadow estimation. The resulting model supports controllable editing like relighting the scene or inserting virtual objects, producing photorealistic free-viewpoint renderings. Experiments on urban driving videos demonstrate UrbanIR's ability to realistically relight scenes and insert objects with proper shadows and lighting interactions. The method represents a promising approach to constructing editable neural graphics models of outdoor environments from monocular video.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents UrbanIR, a method for building a relightable neural scene model from video. The key idea is to jointly infer shape, albedo, visibility, and lighting for large outdoor urban scenes using only a single video captured under fixed but unknown illumination. Their method represents the scene using a neural radiance field that encodes albedo, surface normals, semantics, and density. Rendering is done using differentiable volumetric rendering and a local shading model with a parametric sun-sky illumination model. Optimization is performed using several novel loss functions, including a "deshadowed rendering" loss to push shadows into the visibility field rather than albedo, a visibility loss using shadow detection to refine geometry, and losses on predicted vs estimated normals and semantics. 

The benefits of UrbanIR are demonstrated through relighting, by changing sun position and simulating nighttime lighting. The method also enables realistic insertion of virtual objects, with accurate shadows cast both on the object by the scene and vice versa. Comparisons to other recent methods show UrbanIR's superiority in relighting, decomposition quality, and object insertion. UrbanIR represents an important advance in recovering full inverse graphics representations from monocular video of complex outdoor scenes. Key limitations relate to reliance on multiple 2D priors and difficulty fully removing shadows from albedo. Overall, UrbanIR enables controllable editing for photorealistic free-viewpoint rendering of large-scale outdoor scenes from video.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents UrbanIR (Urban Scene Inverse Rendering), a novel method that enables realistic and controllable rendering of outdoor urban scenes from a single video captured under fixed illumination. The key idea is to jointly optimize a neural radiance field scene representation and a parametric sun-sky illumination model to decompose the scene into shape, albedo, visibility, and lighting components. Specifically, the method represents the scene using a neural radiance field that outputs an albedo map, surface normal map, and density field. Rendering is performed using differentiable volumetric ray marching and a local shading model incorporating the lighting parameters. The model is optimized using a loss function that enforces agreement between rendered and input views, removes shadows from the albedo, and constrains visibility and density fields using monocular cues like semantic labels, surface normals, and shadow masks. A key contribution is a novel visibility loss that uses shadow detection to refine geometry along rays to the estimated sun position, enabling accurate shadow modeling. After training, the model supports rendering the scene from novel views under different illumination by modifying the sun-sky lighting parameters and volumetrically ray marching through the intrinsic neural radiance field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents UrbanIR, a method to jointly infer shape, albedo, visibility and illumination from a single video of large outdoor urban scenes, enabling photorealistic free-viewpoint rendering of the scene under novel lighting and object insertion.


## What problem or question is the paper addressing?

 Based on my reading of the paper, here is a summary of the key problem and contributions:

The paper proposes a method called UrbanIR (Urban Scene Inverse Rendering) for building a realistic and relightable neural scene model from a single video of large-scale urban outdoor scenes. 

The key problem it aims to address is how to jointly infer the shape, albedo, visibility, and illumination of unbounded outdoor scenes from a single video captured under unknown natural lighting. This is challenging because with only single view observations under fixed lighting, recovering the canonical inverse graphics representation of a scene is highly ill-posed.

To address this, UrbanIR introduces the following key contributions:

- Novel losses to control errors in geometric estimation, which significantly improves rendering quality compared to alternative methods.

- A novel visibility rendering procedure to ensure consistency between detected shadows and scene geometry, leading to better predicted shadows.

- Using monocular estimates of surface normals and shadows to supervise the neural fields, improving inverse graphics estimates.

In summary, UrbanIR presents a framework to decompose a single outdoor video into shape, appearance, and lighting components for photorealistic free-viewpoint rendering of relit scenes and object insertion. The key innovations focus on handling the inherent ambiguities in outdoor inverse rendering through losses and constraints based on monocular cues and shadow information.
