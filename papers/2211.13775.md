# [SAGA: Spectral Adversarial Geometric Attack on 3D Meshes](https://arxiv.org/abs/2211.13775)

## What is the central research question or hypothesis that this paper addresses?

 The main research question this paper addresses is how to generate effective geometric adversarial attacks on 3D mesh autoencoders. Specifically, it proposes a novel framework called SAGA (Spectral Adversarial Geometric Attack) for creating adversarial mesh inputs that fool autoencoders into reconstructing different target geometries. 

The key hypotheses are:

1. Adversarial attacks can move beyond just fooling classifiers (semantic attacks) and can also target the geometry reconstruction capabilities of autoencoders.

2. Perturbing meshes in the spectral domain based on their Laplacian eigendecomposition allows controlling the global geometry while limiting visible distortions. 

3. Optimizing perturbations using a reconstruction loss to match a target shape and mesh-based regularizations can create effective adversarial examples.

4. The adversarial reconstructions will generalize and transfer to other unseen autoencoders.

So in summary, the central research focus is developing a spectral mesh perturbation method that generates geometrically adversarial inputs to fool autoencoders, while also conducting experiments to evaluate the attack effectiveness and transferability.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel framework for a geometric adversarial attack on 3D mesh autoencoders. The key ideas are:

- They propose the first geometric adversarial attack on 3D mesh autoencoders, as opposed to previous works that focused on semantic attacks on classifiers. 

- The attack perturbs a clean "source" mesh to mislead an autoencoder to reconstruct the geometry of a different "target" mesh.

- The perturbations are applied in the spectral domain defined by the Laplace-Beltrami operator, which allows smooth and global changes to the surface. 

- They use a shared spectral basis for all shapes to accelerate the attack.

- Additional mesh-based regularizations are proposed to retain the natural appearance of the adversarial examples.

- The attack crafts adversarial meshes that are hard to detect yet successfully change the autoencoder's output geometry.

- The attack is evaluated on human face and animal datasets using semantic and geometric metrics. It outperforms a baseline geometric attack on point clouds.

In summary, the key contribution is presenting the first framework to generate 3D mesh examples that adversarially attack autoencoders in a geometric manner, rather than just semantically fooling classifiers.
