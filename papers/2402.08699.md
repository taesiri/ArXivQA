# [Unsupervised Evaluation of Code LLMs with Round-Trip Correctness](https://arxiv.org/abs/2402.08699)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness":

Problem:
Evaluating code capabilities of large language models (LLMs) is challenging and costly, requiring laborious human-curated datasets. Existing benchmarks like HumanEval and MBPP cover narrow domains of simple coding tasks, which is insufficient to reflect real-world software engineering. Manually expanding benchmarks is impractical. 

Proposed Solution: 
The paper proposes round-trip correctness (RTC) for unsupervised evaluation of LLMs on code tasks. The key idea is to use the LLM to perform an action (e.g. describe code) and its inverse (e.g. implement code from description), then check if the round-trip retained semantics. This avoids needing human annotations. RTC can use metrics like exact match, BLEU, tests, etc. to judge semantic equivalence between original and round-trip code.

Main Contributions:
- Formalizes the notion of RTC for unsupervised LLM evaluation without human annotation
- Shows RTC strongly correlates (r=0.95) with accuracy on existing benchmarks like HumanEval, validating it
- Demonstrates RTC allows measuring LLM performance on far wider range of software domains than benchmarks
- Applies RTC successfully to code editing tasks with no established evaluation
- RTC complements existing benchmarks and allows expanding evaluation at low cost to new domains/tasks 

In summary, the paper presents round-trip correctness as an unsupervised evaluation approach for code LLMs that correlates well with existing metrics but greatly expands the domains and tasks that can be evaluated without costly human annotation.


## Summarize the paper in one sentence.

 This paper proposes using round-trip correctness (RTC), the ability of a model to accurately reconstruct the original input after making a prediction on it, as an unsupervised evaluation method for code language models that correlates well with existing metrics but allows evaluation on a much broader range of real-world domains and tasks without requiring costly human annotation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing round-trip correctness (RTC) as an unsupervised method for evaluating code large language models (LLMs). Specifically:

- They introduce the concept of round-trip correctness, which involves using a model to make a prediction (e.g. describe some code), feeding that prediction back to the model as input (e.g. generate code from the description), and checking if the round-trip retains the semantics of the original input.

- They show how RTC can be used to evaluate code synthesis and editing capabilities of LLMs, without needing human-curated datasets.

- They demonstrate that RTC correlates well with model performance on existing narrow-domain supervised benchmarks like HumanEval and ARCADE.

- They use RTC to evaluate models across a diverse set of 60 real-world software projects, showing that performance varies widely across domains - something that existing benchmarks fail to capture.

- Overall, RTC allows unsupervised evaluation of LLMs on a much broader range of software domains and tasks, complementing existing human-evaluated benchmarks. The key advantage is not needing costly human annotations for evaluation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Round-trip correctness (RTC) - The central concept proposed for unsupervised evaluation of large language models by having them perform an action and its inverse.

- Code LLMs - Code capabilities of large language models are a focus of the paper.

- Unsupervised evaluation - The paper proposes RTC as an unsupervised way to evaluate LLMs without needing human-curated datasets.

- Code synthesis - One of the main tasks considered where LLMs are asked to generate code based on descriptions. RTC is used to evaluate this. 

- Code editing - The paper also shows RTC can be used to evaluate code editing abilities.

- Semantic equivalence - Critical for judging if the round-trip retains semantics of original input code. Proxies like unit tests are used.

- Forward model and backward model - RTC relies on a forward model to perform an action and backward model to undo it.

- Forward lift - Used to measure if forward model provides useful information beyond what a baseline backward model can do alone.

- Domain variance - The paper shows RTC allows evaluating performance across a diverse set of real-world code vs just narrow domains of existing benchmarks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the round-trip correctness (RTC) method proposed in this paper:

1. The paper shows that RTC correlates well with standard metrics like pass@1 on existing benchmarks like HumanEval and ARCADE. But what are some potential reasons why the correlation is not perfect? Could there be cases where RTC and pass@1 rank models differently?

2. The paper demonstrates using RTC for code synthesis and editing tasks. What are some other code-related tasks where RTC could be applicable as an evaluation metric? What would be the challenges in applying RTC there?

3. The lift metric aims to measure the contribution of the forward model predictions towards the overall RTC. When would we expect to see low or negative lift scores? Could low lift actually indicate reasonable forward model performance?

4. The paper uses unit tests as the semantic equivalence check in computing RTC. What are some limitations of relying on unit tests? When would using unit tests as the similarity metric fail to properly evaluate model performance?

5. One limitation stated is that RTC requires reasonably well-trained models and may not work properly in an adversarial setting. Can you think of modifications to the RTC formulation or training procedures that could make it more robust to adversarial attacks?

6. For code editing, could the backward model potentially make edits that are unnecessary but still pass RTC if the forward description is very vague? How could the editing RTC evaluation be made more rigorous?

7. The qualitative analysis shows cases where forward samples miss some key information or contain hallucinations. What could be the reasons for such failures? How might the sampling procedure be improved?

8. The paper demonstrates evaluating on a diverse set of open source projects using RTC. What are some ways the domain coverage could be expanded even further to better measure real-world performance?

9. The paper states that semantic equivalence is easier to check for code compared to natural language. But what about very complex software with complex logic and side effects? When would judging equivalence become difficult even for code?

10. If we want to compare multiple models, should we expect RTC scores to be comparable across models trained differently? Or could differences like sampling methods could make the RTC numbers hard to compare in some cases?
