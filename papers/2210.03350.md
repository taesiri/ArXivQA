# Measuring and Narrowing the Compositionality Gap in Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question this paper addresses is:How does the ability of language models to perform compositional reasoning scale with model size, and can techniques like elicitive prompting help improve compositional reasoning abilities?Specifically, the authors seem focused on investigating:- Whether the "compositionality gap" (the fraction of compositional questions the model gets wrong despite answering the sub-questions correctly) decreases as language model size increases. - Whether elicitive prompting techniques like "chain of thought" and their proposed "self-ask" method can help narrow this compositionality gap by allowing models to reason more explicitly.- Whether integrating search engines with the self-ask method can further improve compositional reasoning performance.So in summary, the main research questions revolve around measuring and trying to improve the compositional reasoning abilities of large language models using elicitive prompting and search engine integration.


## What is the main contribution of this paper?

The main contribution of this paper is developing methods to improve the compositional reasoning abilities of language models. Specifically:- The paper introduces the concept of the "compositionality gap" to quantify how often models can answer individual sub-questions but fail to compose them to answer a full compositional question. Experiments show this gap does not decrease with model scale.- The paper demonstrates that "elicitive prompting" methods like chain of thought that have models reason step-by-step can narrow the compositionality gap compared to standard prompting.- The paper proposes a new elicitive prompting method called "self-ask" where models explicitly ask and answer follow-up questions before answering the main question. This further improves compositional reasoning over chain of thought. - The explicit sub-question structure of self-ask also enables easily integrating a search engine to answer sub-questions, further boosting performance.Overall, the main contribution is developing new analysis methods and prompting techniques to measure and improve the compositional reasoning abilities of large language models. The concepts of the compositionality gap and elicitive prompting are novel ways proposed in this work to analyze and enhance compositionality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper investigates the compositional reasoning abilities of large language models using multi-hop question answering, and finds they have a constant "compositionality gap" that elicitive prompting like self-ask can narrow.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other research in compositional reasoning and question answering:This paper focuses on systematically evaluating and improving the compositional reasoning abilities of large language models (LLMs) like GPT-3, using multi-hop question answering as the evaluation task. - It introduces a new metric called the "compositionality gap" to quantify how often LLMs can answer individual sub-questions but fail to compose them to answer the full question. This provides a quantitative way to measure compositional reasoning that hasn't been used before.- The paper shows that the compositionality gap remains constant at around 40% as LLM scale increases, suggesting limited improvement in compositional reasoning from pretraining scale alone. This is a novel finding not demonstrated in prior work. - The paper demonstrates that elicitive prompting methods like chain of thought can narrow the compositionality gap, outperforming prior work on decomposing questions like least-to-most prompting.- The new self-ask prompting method explicitly decomposes questions, improving over chain of thought. Integrating search further improves performance. Using search engines to answer sub-questions is a relatively underexplored technique in prior question answering work.Overall, this paper provides novel insights into evaluating and improving compositional reasoning in LLMs through new metrics, datasets, and prompting techniques. The analysis of model scale and compositional abilities is especially unique. The elicitive prompting methods also outperform prior question decomposition techniques.
