# [Learning Graph Structure from Convolutional Mixtures](https://arxiv.org/abs/2205.09575v1)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we effectively learn latent graph structure from observations of convolutional mixtures of the graph?The authors formulate the problem as a network inverse or deconvolution task, where the goal is to recover a latent sparse graph adjacency matrix $\mA_L$ from an observed graph $\mA_O$ that is a convolutional mixture of powers of $\mA_L$. Specifically, $\mA_O$ is modeled as a polynomial function of $\mA_L$:$\mA_O = \mH(\mA_L;\vh) = \sum_{k=0}^K h_k \mA_L^k$where $\vh$ are the polynomial coefficients. The authors propose a deep learning approach called Graph Deconvolution Networks (GDNs) to learn a mapping from $\mA_O$ to $\mA_L$ in a supervised fashion using training data. The GDN architecture is derived by unrolling proximal gradient iterations for optimizing a reconstruction objective.The central hypothesis is that the GDN model can effectively recover latent graph structure from observed convolutional graph mixtures in a variety of domains, as evaluated on synthetic and real-world datasets. The model should be able to generalize to larger graphs than seen during training.In summary, the key research question is how to learn latent graph structure from convolutional graph mixtures, with the proposal of GDNs as an effective and versatile solution. Evaluating the performance of GDNs on this task is the central focus.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a Graph Deconvolution Network (GDN), a neural network architecture for learning to infer latent graph structure from observations of convolutional mixtures of the graph. The GDN is derived by unrolling and truncating proximal gradient iterations for solving an optimization problem formulation of the graph deconvolution problem.- Showcasing the versatility of GDNs to perform supervised learning of graph structure for different tasks (link prediction and edge weight regression) and adapt to incorporate domain knowledge through the network initialization. - Demonstrating GDN's effectiveness at recovering graph structure on synthetic and real-world datasets, including social networks and brain connectivity networks. On synthetic data, GDN outperforms baseline methods like network deconvolution and graphical lasso.- Using GDNs to learn whole-brain structural connectivity from functional connectivity, a challenging problem in neuroimaging. Results on the Human Connectome Project dataset show GDNs can learn connectivity patterns in specific brain subnetworks.- Showing that GDNs are inductive and can generalize to larger graphs than they are trained on. Experiments indicate minimal performance degradation when tested on graphs 30x larger than training.- Introducing a supervised learning approach to the network deconvolution problem. Much prior work formulates optimization problems for single graph instances, whereas the authors advocate learning neural networks from graph example data.In summary, the main contribution appears to be the proposal of the GDN architecture for supervised learning of latent graph structure from graph convolutional mixtures, and experimental validation of its effectiveness on synthetic and real graph data. The inductive nature and generalization ability of GDNs are also notable contributions.
