# [Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly   Supervised 3D Visual Grounding](https://arxiv.org/abs/2307.09267)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can we train a 3D visual grounding model using only weakly supervised data, where we have scene-level annotations but no object-level bounding box labels?

The key points are:

- 3D visual grounding involves localizing a target object in a 3D scene based on a natural language query. It requires expensive object-level bounding box annotations for training. 

- This paper proposes a weakly supervised setting that only uses coarse scene-level labels to learn the model. This reduces annotation costs and is more realistic.

- The main hypothesis is that a coarse-to-fine semantic matching model can analyze object-sentence similarity and generate pseudo ground truth to train the 3D grounding model, using only scene-level supervision.

- The coarse-to-fine model first selects candidate objects via class and feature similarity. Then it reconstructs masked words in the query using each candidate to measure fine-grained similarity.

- Knowledge distillation transfers the coarse-to-fine model's matching knowledge into a two-stage grounding model to improve efficiency.

So in summary, the key research question is how to train 3D visual grounding with only weak scene-level supervision, which is addressed through a coarse-to-fine semantic matching approach and knowledge distillation.


## What is the main contribution of this paper?

 This paper proposes a weakly supervised 3D visual grounding method that only requires coarse scene-sentence annotations for training, without needing time-consuming dense object-sentence bounding box labels. The key contributions are:

- It is the first work to address weakly supervised 3D visual grounding using only scene-level labels, eliminating expensive object-sentence bounding box annotations. 

- It proposes a coarse-to-fine semantic matching model to measure the similarity between object proposals and sentences, using object category similarity, feature similarity, and masked keyword reconstruction.

- It distills the knowledge from the coarse-to-fine semantic matching model into an existing two-stage 3D visual grounding model, reducing inference costs and leveraging well-studied network architectures. 

- Experiments on ScanRefer, Nr3D, and Sr3D datasets demonstrate its effectiveness for weakly supervised 3D grounding, significantly outperforming baselines.

In summary, the main contribution is proposing a novel coarse-to-fine semantic matching approach to learn 3D visual grounding from weak supervision, and distilling this knowledge into existing models to improve performance and efficiency. This is the first work addressing the more realistic but challenging setting of weakly supervised 3D grounding.
