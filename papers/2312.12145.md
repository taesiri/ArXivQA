# [OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in   Noisy Environments](https://arxiv.org/abs/2312.12145)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- In reinforcement learning (RL), efficiently exploring the environment is critical for improving learning efficiency and final policy performance. Most exploration strategies follow the "optimism in the face of uncertainty" (OFU) principle to guide the agent towards less explored areas with higher uncertainty.
- However, there is another type of uncertainty called "aleatoric uncertainty" or noise, caused by inherent stochasticity in the environment. Overly exploring noisy areas can be detrimental to learning efficiency. 
- Existing OFU-based exploration methods overlook the impact of noise. Methods that avoid noise tend to be too conservative, lacking sufficient exploration.
- Designing an exploration strategy that is both optimistic but also avoids excessive exploration in noisy areas is an open challenge, especially for continuous control tasks.

Proposed Solution - OVD-Explorer:
- Proposes a new measurement of a policy's exploration ability that considers both the ability to avoid noise and be optimistic. Quantifies this using the "Optimistic Value Distribution" (OVD).
- Seeks to maximize this exploration ability measurement to derive the behavior policy for guiding exploration.
- Uses a gradient-based approach to efficiently generate the behavior policy for continuous control. Can be integrated with policy-based RL algorithms like SAC.
- Models the return distribution to capture noise and defines an upper bound distribution OVD to enable optimistic exploration.

Main Contributions:
- First exploration strategy to achieve noise-aware optimistic exploration for continuous control tasks.
- Proposes exploration ability measurement using OVD that balances both noise-avoidance and optimism.
- Gradient-based generation of the behavior policy makes it efficient and practical.
- Evaluations on MuJoCo and GridChaos tasks demonstrate superior performance over SOTA baselines.
- Enables existing continuous RL algorithms to handle noise during exploration more effectively.

In summary, the paper presents OVD-Explorer that guides optimistic yet noise-aware exploration in continuous RL environments to improve learning efficiency and performance.
