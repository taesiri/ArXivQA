# [Leveraging Large Language Models for Concept Graph Recovery and Question   Answering in NLP Education](https://arxiv.org/abs/2402.14293)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT have shown promise for text generation tasks, but their capabilities for educational applications, especially domain-specific queries, are underexplored. 
- Concept graphs are useful for structuring knowledge, but constructing them relies on manual effort or supervised learning methods. Integrating concept graphs with LLMs can enhance their reasoning abilities.

Method:
- Propose CGPrompt to recover concept graphs using LLMs in a zero-shot setting, showing strong performance.
- Introduce TutorQA, an expert-verified QA benchmark with 5 tasks, designed for scientific graph reasoning and QA in NLP education.
- Present CGLLM pipeline that enhances LLM's interaction with the concept graph to answer TutorQA questions. CGLLM issues Cypher queries to retrieve relevant graph paths and uses them to guide the LLM's response generation.

Results:
- LLMs can recover concept graphs competitively without supervision, achieving ~3% higher F1 than supervised baselines.
- TutorQA spans diverse QA types - prerequisite prediction, path search, concept advising, idea generation.
- CGLLM boosts LLM performance on TutorQA tasks significantly, increasing F1 by up to 26%. Analysis shows it produces finer-grained concepts.

Contributions:
- First work exploring LLMs' zero-shot capabilities in scientific concept graph recovery.
- Release TutorQA, an expert-verified benchmark for graph reasoning & QA in NLP education.
- Propose CGLLM to effectively incorporate concept graphs with LLMs for enhanced reasoning and response generation.
