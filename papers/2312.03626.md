# [TokenCompose: Grounding Diffusion with Token-level Supervision](https://arxiv.org/abs/2312.03626)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes TokenCompose, a new training method that enhances the consistency between text prompts and generated images for text-to-image diffusion models. TokenCompose leverages image understanding models to provide token-level supervisions by extracting segmentation maps corresponding to noun tokens in the text prompt during training. Specifically, two losses are introduced: a token-level attention loss that aggregates cross-attention activations towards target instance regions, and a pixel-level attention loss that provides precise supervision on whether pixels belong to segmented regions. Experiments on Stable Diffusion show that TokenCompose significantly improves multi-category instance composition, allowing accurate generation of combinations of objects that rarely co-occur, while maintaining image quality and efficiency. The method also demonstrates enhanced knowledge transfer to segmentation tasks. Additionally, a new benchmark called MultiGen is proposed for evaluating multi-category composition with more than two objects. In summary, TokenCompose effectively grounds diffusion models with token-level image consistency without extra labeling cost, enabling accurate text-conditional image generation with multiple composed objects.
