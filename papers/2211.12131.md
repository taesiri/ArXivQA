# [DiffDreamer: Towards Consistent Unsupervised Single-view Scene   Extrapolation with Conditional Diffusion Models](https://arxiv.org/abs/2211.12131)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we perform consistent and realistic single-view scene extrapolation using diffusion models?In particular, the authors aim to develop an unsupervised framework that can synthesize novel views depicting a long camera trajectory by "flying" into a single image, while only training on collections of unrelated internet images. The key hypotheses appear to be:1) Image-conditioned diffusion models can effectively perform long-range scene extrapolation while preserving consistency better than prior GAN-based methods.2) By modifying the sampling behavior at inference time, diffusion models can be conditioned on multiple past and future frames to improve consistency despite training on single images.3) Techniques like anchored conditioning on distant frames and virtual lookahead conditioning can help maintain both local and global consistency for scene extrapolation using diffusion models.So in summary, the main research question is whether diffusion models can enable consistent and realistic single-view scene extrapolation in an unsupervised manner, which prior GAN approaches have struggled with. The key hypotheses focus on how diffusion models can be adapted to this task through customized training and inference procedures.
