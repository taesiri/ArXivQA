# [Scaling laws for language encoding models in fMRI](https://arxiv.org/abs/2305.11863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How does the performance of encoding models for predicting fMRI responses to natural language scale with increasing language model size and training data? The authors hypothesize that performance will improve log-linearly, following the "scaling laws" observed for language models on other NLP tasks. 2) Can speech/audio encoding models built from models like HuBERT, WavLM, and Whisper also show improved performance with increased model size and training data? The authors hypothesize they will show similar log-linear scaling trends.3) Can combining information from semantic and speech-based models using techniques like stacked regression further improve encoding performance, especially in auditory areas? The authors hypothesize this will lead to localized improvements in auditory cortex.4) What is the theoretical maximum performance (noise ceiling) of current large encoding models, and in which brain areas do models still have large room for improvement? The authors aim to characterize this to understand current limits.In summary, the central hypotheses are that increasing model scale (size and training data) for both semantic and speech models will substantially improve encoding model performance in a log-linear way, and that combining semantic and speech models can provide additional localized gains, leading to state-of-the-art encoding models. The authors test these hypotheses and characterize current performance limits.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Demonstrating that scaling up the size of language models (in terms of parameters) and amount of training data leads to better performance in predicting brain activity from natural language stimuli. Specifically, the paper shows that:- Increasing language model size leads to roughly log-linear improvements in encoding performance, with each order of magnitude increase in parameters leading to around 15% better prediction of held-out brain data.- Increasing the amount of fMRI training data also leads to log-linear improvements in encoding performance, with each order of magnitude more training stories leading to around 120% better prediction.2. Showing that these scaling laws also apply to acoustic encoding models based on models like HuBERT, WavLM, and Whisper - larger models and more training data improve prediction of brain responses, especially in auditory areas.3. Demonstrating state-of-the-art encoding model performance by using very large models like OPT-30B and fine-tuning/combining models. Correlations between predicted and actual voxel responses reach as high as 0.82. 4. Analysing the limits of encoding model performance using a noise ceiling analysis. This shows some areas like higher auditory cortex and precuneus are approaching optimal encoding given noise limits, while other areas have room for improvement.5. Providing evidence that combining representations from semantic and acoustic models can further improve encoding performance beyond individual models, especially in early auditory areas.Overall, the key contribution is demonstrating the benefits of scale (in models and data) for brain encoding models across both semantic and acoustic domains. This suggests continued scaling is a promising direction to enable better modelling of language in the brain.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper shows that scaling up neural network model size and training data improves performance of encoding models that predict human brain activity during natural language processing, with semantic models plateauing around 30 billion parameters but speech models continuing to improve.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on encoding models for fMRI:- The key finding that encoding model performance scales log-linearly with model size aligns with broader observations about the benefits of scale for language models. This shows that the "scaling laws" apply similarly for predicting brain responses.- Using 30B+ parameter models like OPT and LLaMA to build encoding models is novel. Most prior fMRI encoding work has used smaller models like GPT-2. Showing that continued scaling improves brain prediction parallels evidence in other domains.- Similarly, the experiments with varying training set size demonstrate the value of "deep" fMRI datasets focused on more data per subject. This reinforces arguments made by others that we need more densely-sampled fMRI data.- The noise ceiling analysis provides a useful theoretical limit on encoding model performance. Comparing to this helps benchmark progress and identify which areas have room for improvement.- Using stacked regression to combine semantic and acoustic models is an interesting way to get benefits from both. The spatial distribution of improvements makes sense based on auditory vs linguistic hierarchy.- The characterization of "long context" artifacts and adjustments to mitigate are practically useful for the field. This methodology consideration applies broadly.Overall, this paper pushes encoding models to larger scales and sets new state-of-the-art performance. The theoretical ceiling analysis also provides helpful framing on progress. The scaling laws and dataset findings further align this work with broader evidence about model and data benefits.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Collecting larger "deep" fMRI datasets that focus on getting more data from a few subjects rather than a little data from many subjects. The authors found that encoding performance scales well with dataset size, so they suggest collecting large datasets to enable better encoding models.- Using larger, more capable language models like OPT and LLaMA for building encoding models rather than smaller models like GPT-2. The authors showed performance improvements from using larger models, even with small amounts of brain data.- Exploring applications of the high-performing encoding models:    - Using them to improve natural language decoders that predict stimuli from brain activity.    - Using them for fine-grained control of voxel activations through targeted stimulus generation.     - Using them as a "purer" metric of language model performance that avoids issues like test set contamination.- Further improving encoding performance by compositing multiple models, as the authors showed benefits from combining semantic and speech models.- Examining why large language models plateau in performance around 30B parameters when predicting brain activity, while large speech models do not plateau. This could reveal differences in how semantic vs acoustic information is represented across model size.- Studying why different model architectures (LLaMA vs OPT) have different layerwise performance profiles when predicting the brain. This may provide insight into differences in learned representations.In summary, the main suggestions are to use larger models and datasets, explore applications of high-performing encoding models, improve models further by compositing multiple modalities, and analyze model architectures to reveal insights about learned representations versus those in the brain.
