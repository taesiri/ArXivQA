# [Scaling laws for language encoding models in fMRI](https://arxiv.org/abs/2305.11863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How does the performance of encoding models for predicting fMRI responses to natural language scale with increasing language model size and training data? The authors hypothesize that performance will improve log-linearly, following the "scaling laws" observed for language models on other NLP tasks. 2) Can speech/audio encoding models built from models like HuBERT, WavLM, and Whisper also show improved performance with increased model size and training data? The authors hypothesize they will show similar log-linear scaling trends.3) Can combining information from semantic and speech-based models using techniques like stacked regression further improve encoding performance, especially in auditory areas? The authors hypothesize this will lead to localized improvements in auditory cortex.4) What is the theoretical maximum performance (noise ceiling) of current large encoding models, and in which brain areas do models still have large room for improvement? The authors aim to characterize this to understand current limits.In summary, the central hypotheses are that increasing model scale (size and training data) for both semantic and speech models will substantially improve encoding model performance in a log-linear way, and that combining semantic and speech models can provide additional localized gains, leading to state-of-the-art encoding models. The authors test these hypotheses and characterize current performance limits.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Demonstrating that scaling up the size of language models (in terms of parameters) and amount of training data leads to better performance in predicting brain activity from natural language stimuli. Specifically, the paper shows that:- Increasing language model size leads to roughly log-linear improvements in encoding performance, with each order of magnitude increase in parameters leading to around 15% better prediction of held-out brain data.- Increasing the amount of fMRI training data also leads to log-linear improvements in encoding performance, with each order of magnitude more training stories leading to around 120% better prediction.2. Showing that these scaling laws also apply to acoustic encoding models based on models like HuBERT, WavLM, and Whisper - larger models and more training data improve prediction of brain responses, especially in auditory areas.3. Demonstrating state-of-the-art encoding model performance by using very large models like OPT-30B and fine-tuning/combining models. Correlations between predicted and actual voxel responses reach as high as 0.82. 4. Analysing the limits of encoding model performance using a noise ceiling analysis. This shows some areas like higher auditory cortex and precuneus are approaching optimal encoding given noise limits, while other areas have room for improvement.5. Providing evidence that combining representations from semantic and acoustic models can further improve encoding performance beyond individual models, especially in early auditory areas.Overall, the key contribution is demonstrating the benefits of scale (in models and data) for brain encoding models across both semantic and acoustic domains. This suggests continued scaling is a promising direction to enable better modelling of language in the brain.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper shows that scaling up neural network model size and training data improves performance of encoding models that predict human brain activity during natural language processing, with semantic models plateauing around 30 billion parameters but speech models continuing to improve.
