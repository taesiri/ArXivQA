# [Scaling laws for language encoding models in fMRI](https://arxiv.org/abs/2305.11863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How does the performance of encoding models for predicting fMRI responses to natural language scale with increasing language model size and training data? The authors hypothesize that performance will improve log-linearly, following the "scaling laws" observed for language models on other NLP tasks. 2) Can speech/audio encoding models built from models like HuBERT, WavLM, and Whisper also show improved performance with increased model size and training data? The authors hypothesize they will show similar log-linear scaling trends.3) Can combining information from semantic and speech-based models using techniques like stacked regression further improve encoding performance, especially in auditory areas? The authors hypothesize this will lead to localized improvements in auditory cortex.4) What is the theoretical maximum performance (noise ceiling) of current large encoding models, and in which brain areas do models still have large room for improvement? The authors aim to characterize this to understand current limits.In summary, the central hypotheses are that increasing model scale (size and training data) for both semantic and speech models will substantially improve encoding model performance in a log-linear way, and that combining semantic and speech models can provide additional localized gains, leading to state-of-the-art encoding models. The authors test these hypotheses and characterize current performance limits.
