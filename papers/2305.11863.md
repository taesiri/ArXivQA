# [Scaling laws for language encoding models in fMRI](https://arxiv.org/abs/2305.11863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How does the performance of encoding models for predicting fMRI responses to natural language scale with increasing language model size and training data? The authors hypothesize that performance will improve log-linearly, following the "scaling laws" observed for language models on other NLP tasks. 2) Can speech/audio encoding models built from models like HuBERT, WavLM, and Whisper also show improved performance with increased model size and training data? The authors hypothesize they will show similar log-linear scaling trends.3) Can combining information from semantic and speech-based models using techniques like stacked regression further improve encoding performance, especially in auditory areas? The authors hypothesize this will lead to localized improvements in auditory cortex.4) What is the theoretical maximum performance (noise ceiling) of current large encoding models, and in which brain areas do models still have large room for improvement? The authors aim to characterize this to understand current limits.In summary, the central hypotheses are that increasing model scale (size and training data) for both semantic and speech models will substantially improve encoding model performance in a log-linear way, and that combining semantic and speech models can provide additional localized gains, leading to state-of-the-art encoding models. The authors test these hypotheses and characterize current performance limits.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Demonstrating that scaling up the size of language models (in terms of parameters) and amount of training data leads to better performance in predicting brain activity from natural language stimuli. Specifically, the paper shows that:- Increasing language model size leads to roughly log-linear improvements in encoding performance, with each order of magnitude increase in parameters leading to around 15% better prediction of held-out brain data.- Increasing the amount of fMRI training data also leads to log-linear improvements in encoding performance, with each order of magnitude more training stories leading to around 120% better prediction.2. Showing that these scaling laws also apply to acoustic encoding models based on models like HuBERT, WavLM, and Whisper - larger models and more training data improve prediction of brain responses, especially in auditory areas.3. Demonstrating state-of-the-art encoding model performance by using very large models like OPT-30B and fine-tuning/combining models. Correlations between predicted and actual voxel responses reach as high as 0.82. 4. Analysing the limits of encoding model performance using a noise ceiling analysis. This shows some areas like higher auditory cortex and precuneus are approaching optimal encoding given noise limits, while other areas have room for improvement.5. Providing evidence that combining representations from semantic and acoustic models can further improve encoding performance beyond individual models, especially in early auditory areas.Overall, the key contribution is demonstrating the benefits of scale (in models and data) for brain encoding models across both semantic and acoustic domains. This suggests continued scaling is a promising direction to enable better modelling of language in the brain.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper shows that scaling up neural network model size and training data improves performance of encoding models that predict human brain activity during natural language processing, with semantic models plateauing around 30 billion parameters but speech models continuing to improve.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on encoding models for fMRI:- The key finding that encoding model performance scales log-linearly with model size aligns with broader observations about the benefits of scale for language models. This shows that the "scaling laws" apply similarly for predicting brain responses.- Using 30B+ parameter models like OPT and LLaMA to build encoding models is novel. Most prior fMRI encoding work has used smaller models like GPT-2. Showing that continued scaling improves brain prediction parallels evidence in other domains.- Similarly, the experiments with varying training set size demonstrate the value of "deep" fMRI datasets focused on more data per subject. This reinforces arguments made by others that we need more densely-sampled fMRI data.- The noise ceiling analysis provides a useful theoretical limit on encoding model performance. Comparing to this helps benchmark progress and identify which areas have room for improvement.- Using stacked regression to combine semantic and acoustic models is an interesting way to get benefits from both. The spatial distribution of improvements makes sense based on auditory vs linguistic hierarchy.- The characterization of "long context" artifacts and adjustments to mitigate are practically useful for the field. This methodology consideration applies broadly.Overall, this paper pushes encoding models to larger scales and sets new state-of-the-art performance. The theoretical ceiling analysis also provides helpful framing on progress. The scaling laws and dataset findings further align this work with broader evidence about model and data benefits.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Collecting larger "deep" fMRI datasets that focus on getting more data from a few subjects rather than a little data from many subjects. The authors found that encoding performance scales well with dataset size, so they suggest collecting large datasets to enable better encoding models.- Using larger, more capable language models like OPT and LLaMA for building encoding models rather than smaller models like GPT-2. The authors showed performance improvements from using larger models, even with small amounts of brain data.- Exploring applications of the high-performing encoding models:    - Using them to improve natural language decoders that predict stimuli from brain activity.    - Using them for fine-grained control of voxel activations through targeted stimulus generation.     - Using them as a "purer" metric of language model performance that avoids issues like test set contamination.- Further improving encoding performance by compositing multiple models, as the authors showed benefits from combining semantic and speech models.- Examining why large language models plateau in performance around 30B parameters when predicting brain activity, while large speech models do not plateau. This could reveal differences in how semantic vs acoustic information is represented across model size.- Studying why different model architectures (LLaMA vs OPT) have different layerwise performance profiles when predicting the brain. This may provide insight into differences in learned representations.In summary, the main suggestions are to use larger models and datasets, explore applications of high-performing encoding models, improve models further by compositing multiple modalities, and analyze model architectures to reveal insights about learned representations versus those in the brain.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper examines how the performance of encoding models that predict human brain responses to natural language scale with model size and training data. They tested increasingly large transformer-based language models from 125M to 175B parameters, finding a log-linear relationship between model size and encoding performance that plateaued around 30B parameters. A similar log-linear scaling was found when increasing the amount of fMRI training data from a subject. For speech models like Whisper, WavLM, and HuBERT, comparable improvements were seen with model size, especially in auditory areas. The authors demonstrate state-of-the-art encoding performance using a 30B language model and show with a noise ceiling analysis that some areas like precuneus and auditory cortex are nearing optimal encoding, while other areas have room for improvement. Overall, the results suggest continued scaling of models and training data will enable increasingly effective encoding models for studying language in the brain.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper studies how scaling up the size of language models and training datasets impacts the performance of encoding models that predict brain activity from natural language stimuli. The authors find that both model size and dataset size have a log-linear relationship with encoding model performance - each 10x increase in model parameters or training data leads to a consistent increase in encoding performance. They test language models ranging from 125 million to 175 billion parameters, with the 30 billion parameter models performing the best overall. The authors also study how scaling up self-supervised speech models like Whisper impacts encoding model performance, finding comparable improvements. The best encoding models are able to predict brain activity with high accuracy in both classical language areas and regions thought to be more domain-general. A noise ceiling analysis shows that while some regions like auditory cortex are nearly optimally predicted, higher order areas still have room for improvement, motivating further scaling. Overall, the results suggest that continued increases in model and data scale will produce even better models of how the brain processes language.


## Summarize the main method used in the paper in one paragraph.

The paper utilizes large scale language models and speech audio models to construct encoding models that predict human brain responses to natural language stimuli. Specifically, the authors extract representations from various sizes of transformer-based language models like OPT, LLaMA, and decoder-only speech models like HuBERT, WavLM, Whisper to serve as input features to train voxel-wise encoding models that predict fMRI BOLD responses. Using fMRI data collected from subjects listening to audio stories, they demonstrate log-linear scaling in encoding model performance with both model size (number of parameters) and dataset size. Larger models like OPT-30B with 30 billion parameters lead to over 15% improvement in prediction accuracy over smaller 125M parameter models, with performance plateauing after 30B parameters. Similarly, using more training stories leads to over 100% improvement per order of magnitude. The authors also show comparable log-linear scaling for speech models, with largest gains in auditory areas. Finally, they demonstrate further gains by combining representations from language and speech models using stacked regression, with localization of benefits to early auditory areas. Overall, the results suggest great potential for continued improvements in encoding models and their neuroscientific and clinical applications through scaling model size, datasets, and multi-modal fusion.
