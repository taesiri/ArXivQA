# [On the Learnability of Watermarks for Language Models](https://arxiv.org/abs/2312.04469)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper investigates whether watermarks for detecting model-generated text can be learned by language models, as opposed to relying on specialized decoding algorithms during text generation. The authors propose two methods - logits-based and sampling-based watermark distillation - for training student models to mimic teacher models with existing decoding-based watermarks. Experiments across three decoding-based watermarking strategies show that higher-distortion watermarks are more easily learned, while lower-distortion watermarks require more data and compute to learn. However, all tested watermarks exhibit some degree of learnability. The learned weight-based watermarks are robust to changes in decoding methods. Potential applications in developing watermarking for open models and investigating watermark spoofing attacks are discussed. While promising first steps, challenges remain in improving robustness to further fine-tuning and using watermarks to definitively assign blame. Overall, this work provides valuable insights into the capabilities and limitations around the learnability of text watermarks.
