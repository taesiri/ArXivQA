# [Think Before You Act: Decision Transformers with Internal Working Memory](https://arxiv.org/abs/2305.16338)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the generalization capability and adaptability of decision transformers without solely relying on model scale?The authors motivate this question by arguing that current large language model (LLM) based decision-making agents rely heavily on massive data and compute to achieve good performance across multiple tasks. This inefficiency stems from the "forgetting phenomenon" where training on a new task can interfere with and degrade performance on previous tasks. To address this, the authors propose augmenting decision transformers with an explicit internal "working memory" module, inspired by the concept of working memory in human cognition. The working memory module aims to efficiently store, manage and retrieve useful information and experiences for application to new tasks. This is compared to the implicit memory mechanism of large neural network models which essentially "memorize" behaviors by fitting massive sets of parameters.The central hypothesis seems to be that supplementing decision transformers with an explicit working memory module will improve their generalization capability and adaptability across tasks, without solely relying on massive scale. The authors test this hypothesis by evaluating their proposed "DT-Mem" architecture on Atari games and robotics environments.In summary, the key research question is whether an explicit working memory mechanism can make decision transformers more efficient learners on new tasks, mitigating the limitations of pure parameter-based implicit memory. The experiments aim to validate if DT-Mem improves generalization and adaptability compared to baselines.
