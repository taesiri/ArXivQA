# [Think Before You Act: Decision Transformers with Internal Working Memory](https://arxiv.org/abs/2305.16338)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the generalization capability and adaptability of decision transformers without solely relying on model scale?The authors motivate this question by arguing that current large language model (LLM) based decision-making agents rely heavily on massive data and compute to achieve good performance across multiple tasks. This inefficiency stems from the "forgetting phenomenon" where training on a new task can interfere with and degrade performance on previous tasks. To address this, the authors propose augmenting decision transformers with an explicit internal "working memory" module, inspired by the concept of working memory in human cognition. The working memory module aims to efficiently store, manage and retrieve useful information and experiences for application to new tasks. This is compared to the implicit memory mechanism of large neural network models which essentially "memorize" behaviors by fitting massive sets of parameters.The central hypothesis seems to be that supplementing decision transformers with an explicit working memory module will improve their generalization capability and adaptability across tasks, without solely relying on massive scale. The authors test this hypothesis by evaluating their proposed "DT-Mem" architecture on Atari games and robotics environments.In summary, the key research question is whether an explicit working memory mechanism can make decision transformers more efficient learners on new tasks, mitigating the limitations of pure parameter-based implicit memory. The experiments aim to validate if DT-Mem improves generalization and adaptability compared to baselines.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a new model called Decision Transformers with Memory (DT-Mem) that incorporates an explicit working memory module to store, blend and retrieve training experiences. This is inspired by the concept of working memory in human cognition.2. Demonstrating that DT-Mem improves generalization performance on Atari games compared to prior work like Multi-game Decision Transformer (MDT), while using only 10% as many parameters.3. Showing that fine-tuning just the working memory module of DT-Mem with a small amount of data from new tasks, using a technique called Low-Rank Adaptation, can achieve state-of-the-art adaptation performance on both Atari games and Meta-World environments.4. Providing an analysis of the model architecture, training procedure, and evaluation results on multiple RL benchmarks. This includes studies on the impact of memory module size, training efficiency, and model generalization.In summary, the main novelty is the incorporation of an explicit working memory into the Decision Transformer architecture, which improves efficiency and adaptability. The paper demonstrates these benefits through extensive experiments and analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a transformer-based reinforcement learning agent called Decision Transformers with Memory (DT-Mem) that incorporates an explicit working memory module to improve generalization, efficiency, and adaptability on Atari games and robotics tasks compared to prior Decision Transformer models.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work in decision transformers and reinforcement learning:- It proposes a novel architecture called DT-Mem that incorporates an explicit working memory module. This differentiates it from prior work like Decision Transformers and Hyper Decision Transformers that rely solely on implicit memory in the parameters. The explicit memory aims to improve generalization and efficiency.- The DT-Mem model is evaluated on multiple benchmark tasks - Atari games and Meta-World environments. This allows for direct comparison to prior state-of-the-art methods like Multi-Game DT, Prompt DT, and Hyper DT that have also been evaluated on these benchmarks. - The results demonstrate DT-Mem achieves better generalization and adaptability with fewer parameters and less training time compared to these prior Transformer-based approaches. This shows the value of the working memory in improving efficiency.- The working memory module is adaptable via a simple fine-tuning method based on Low-Rank Adaptation. This differs from prior techniques like prompting or hypernetworks used for Decision Transformer adaptation.- The incorporation of an explicit working memory component has connections to earlier neural memory models like Neural Turing Machines and Memory Networks. However, DT-Mem explores this in the context of recent large foundation models.- The working memory design takes inspiration from cognitive psychology concepts like the global workspace theory. Linking neural models with cognitive theories is an active area of research.Overall, this paper makes important contributions around memory-augmented neural models, specifically showing the benefits of an explicit working memory for improving Decision Transformer efficiency and generalization. The comparisons on benchmark tasks and simple adaptation approach help advance this line of research.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Explore more sample-efficient methods for memory module fine-tuning. The authors used 10% of the dataset for fine-tuning, which is still quite large. Trying out settings with more tasks but less data per task could further test inter-task generalization.- Investigate online data collection methods and design of exploration strategies for efficient fine-tuning on new tasks. The current approach does not propose how to actively collect data for a new task. - Provide a theoretical grounding that formally shows the limitations of large models' implicit memory and how adding an explicit memory component helps overcome those limits. The paper currently provides an intuitive motivation but lacks a formal analysis.- Consider the interplay between pre-training and fine-tuning. For example, pre-training on a wider variety of tasks could improve generalization, and the fine-tuning protocol could be adapted based on properties of the pre-training.- Explore better control methods for deciding what information gets stored in memory versus the base model parameters. The relative roles of the memory module versus the base DT model could be optimized.- Scale up the experiments to much larger (e.g. billions of parameter) models and determine if the memory module yields even greater improvements in that setting.- Test the approach on a wider range of tasks such as language modeling. The current experiments focused on Atari and robotics environments.- Analyze the information being stored in the memory module to verify it captures meaningful task-specific knowledge. This could shed light on how the memory aids generalization.In summary, the main directions are developing a better theoretical understanding, exploring the interplay between pre-training and fine-tuning, scaling up the experiments, testing on more tasks, and analyzing the memory module contents. The sample efficiency and online data collection aspects seem particularly promising to pursue as next steps.
