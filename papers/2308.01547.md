# [Get the Best of Both Worlds: Improving Accuracy and Transferability by   Grassmann Class Representation](https://arxiv.org/abs/2308.01547)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether representing classes as subspaces in neural networks can simultaneously improve classification accuracy and feature transferability compared to representing classes as vectors. The key hypotheses appear to be:1. Representing classes as subspaces rather than vectors will improve classification accuracy on large-scale image classification tasks.2. Features learned with subspace class representations will have greater intra-class variability and better transferability to other downstream tasks compared to features learned with vector class representations.The authors propose representing each class as a linear subspace rather than a single vector in the final classifier layer of neural networks. They integrate geometric optimization techniques to learn these subspace representations jointly with other network parameters.Through experiments on ImageNet classification using CNNs and vision transformers, the authors find that subspace class representations improve accuracy over vector representations.The authors also show that subspace representations allow greater intra-class feature variability, which leads to improved transferability on other datasets compared to vector class representations and baselines like softmax loss.In summary, the central hypothesis is that subspace class representations can unlock simultaneous gains in accuracy and transferability over conventional vectors, which is supported by the paper's experiments and results.


## What is the main contribution of this paper?

The main contribution of this paper is proposing the Grassmann Class Representation (GCR) for image classification. The key ideas are:- Represent each class as a linear subspace (points on the Grassmann manifold) instead of a single vector. The logit for class i is defined as the norm of the projection of the feature vector onto the subspace for class i.- Develop an efficient Riemannian SGD algorithm to jointly optimize the subspaces on the Grassmann manifold together with the other model parameters. - Show through experiments on ImageNet that:  - GCR improves classification accuracy over softmax baseline for various CNN and vision transformer architectures. For example, ResNet50-D top-1 error is reduced by 5.6%.  - Larger subspace dimensions allow greater intra-class feature variability, leading to improved transfer learning performance.  - GCR features transfer better than softmax baseline. For ResNet50-D, average transfer accuracy on 6 datasets improves from 77.98% to 79.70%.In summary, the main contribution is proposing Grassmann class representation to simultaneously improve classification accuracy and feature transferability, enabled by an efficient geometric optimization algorithm. The effectiveness is demonstrated through comprehensive experiments.
