# [Understanding Masked Image Modeling via Learning Occlusion Invariant   Feature](https://arxiv.org/abs/2208.04164)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions this paper tries to address are:

1) How to understand Masked Image Modeling (MIM) methods like MAE in the context of self-supervised learning? The paper aims to build a unified framework to connect MIM with conventional contrastive learning methods. 

2) What is the key factor that leads to the success of MIM methods? Is it the complex reconstructive loss function, or the patch masking strategy?

3) Do MIM methods require a lot of semantic information from the training data? Or can they learn useful representations from minimal data?

To summarize, the central goal of this work is to elucidate the underlying mechanisms of how and why MIM methods like MAE work so well for self-supervised visual representation learning. The key hypotheses are:

- MIM can be understood as learning occlusion invariant features, analogous to contrastive learning methods that learn other types of invariance. 

- The patch masking strategy, rather than the reconstructive loss, is the key to the success of MIM.

- MIM can learn useful representations from minimal training data, suggesting the representations capture general visual concepts beyond semantic information.

The paper tries to verify these hypotheses through theoretical modeling, ablation studies, and experiments on limited training data. The end goal is to build better intuition about how MIM works in order to inspire more powerful self-supervised learning algorithms.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposes a new viewpoint that Masked Image Modeling (MIM) methods intrinsically learn occlusion invariant features, and builds a unified framework RelaxMIM to interpret MIM and contrastive learning methods. 

2. Shows empirically that the representations learned by MIM methods are robust to image occlusion, supporting the claim that they learn occlusion invariant features.

3. Demonstrates that the reconstructive decoder in MIM is not critical and can be replaced by simpler similarity measurements like InfoNCE loss while maintaining similar performance. This suggests the key benefit of MIM is from the patch masking rather than the specific loss. 

4. Pretrains MIM with very few images (e.g. 1 image) and shows the encoder can still learn useful occlusion invariant features that serve as good initialization for downstream tasks, indicating the features are almost data-agnostic.

5. Provides insights that MIM learns initialization that is robust to occlusion but less semantic. This property makes MIM a favored pretrain method for vision transformers before finetuning on downstream tasks.

In summary, the key contribution is providing a new conceptual framework RelaxMIM to understand MIM as learning occlusion invariant features analogous to contrastive learning methods. This framework allows interpreting MIM from an explicit siamese view rather than the original reconstructive view. The empirical analyses support the theoretical framework and provide insights on why MIM works well.
