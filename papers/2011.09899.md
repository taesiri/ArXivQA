# [MixMix: All You Need for Data-Free Compression Are Feature and Data   Mixing](https://arxiv.org/abs/2011.09899)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we generate high-fidelity and generalizable training data for data-free model compression without access to real user data?The key points related to this question seem to be:- Existing methods for data-free model compression like model inversion suffer from two main problems:    1) The synthesized data is biased and does not generalize well to compressing different model architectures.    2) The inversion process results in inexact synthesis due to non-invertibility of neural networks.- To address these issues, the authors propose two techniques:    1) Feature Mixing: Utilizing multiple pretrained models to create a more universal feature space for generalized inversion.    2) Data Mixing: Mixing synthesized images and labels to generate more precise training data. - The main hypothesis appears to be that by using Feature Mixing and Data Mixing, they can generate synthesized training data that:    - Has higher fidelity and generalizability for compressing diverse model architectures    - Allows more exact inversion through label mixing    - Outperforms existing data-free compression techniquesThe experiments and results seem focused on validating this hypothesis by testing their proposed MixMix technique on various compression tasks and models. The improvements over baseline methods help demonstrate the effectiveness of their approach in addressing the core research problem.
