# [LORD: Large Models based Opposite Reward Design for Autonomous Driving](https://arxiv.org/abs/2403.18965)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Reinforcement learning (RL) is a promising approach for autonomous driving, but designing effective reward functions is challenging due to the complexity of defining good driving behavior across diverse scenarios. 
- Manually specifying reward functions risks reward hacking. Using human preferences or demonstrations to learn rewards requires substantial data.
- Desired linguistic goals like "drive safely" are ambiguous and difficult for large pretrained models to comprehend.

Proposed Solution:
- The authors propose LORD (Large models based Opposite Reward Design), which uses more concrete undesired linguistic goals like "collision" that are easier to understand. 
- LORD measures the similarity between the agent's current state and an opposite goal state using large pretrained vision, video and language models. It returns the cosine distance as the reward.
- This encourages the agent to avoid undesired situations described in the opposite goal.

Main Contributions:
- Proposes the concept of opposite reward design through undesired linguistic goals to leverage large pretrained models for embodied AI tasks.
- Integrates LORD with RL for closed-loop autonomous driving in a simulated environment.
- Shows LORD outperforms prior RL and language model baselines in success rate and generalization across scenarios.
- Demonstrates improved performance over using target goal states, showing the efficacy of the opposite reward formulation.
- Provides analysis of the learned reward functions to offer insight into the improved driving behaviors.

In summary, the key innovation is using concrete opposite goals rather than ambiguous desired goals to allow large models to provide more meaningful rewards. Experiments demonstrate LORD's ability to learn superior autonomous driving policies.
