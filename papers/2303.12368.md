# [MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying   Lighting Estimation](https://arxiv.org/abs/2303.12368)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we perform robust scene-level inverse rendering using multi-view images to estimate geometry, materials, and lighting in real indoor scenes?

The key hypotheses appear to be:

1) Leveraging multi-view stereo depth and multiple observations of the same scene point can help estimate geometry, materials, and lighting more robustly compared to single image inverse rendering methods. 

2) A staged training pipeline that first estimates direct lighting and geometry, then materials, and finally full spatially-varying lighting can increase efficiency and reduce memory consumption for multi-view inverse rendering.

3) Representing direct lighting with both spherical Gaussians and voxels provides complementary cues about incident radiance and 3D exitant radiance to help estimate SVBRDF and spatially-varying lighting.

4) An attention-based multi-view aggregation network can selectively combine information across views to estimate materials more accurately.

In summary, the paper proposes a multi-view inverse rendering framework called MAIR to address the limitations of single image methods and enable more robust estimation of scene properties for real-world indoor images. The key innovation seems to be exploiting multi-view stereo depth and a staged training approach to efficiently handle multiple views.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

1. Proposing MAIR, which is the first multi-view scene-level inverse rendering pipeline that can decompose a scene into geometry, materials, and 3D spatially-varying lighting without requiring test-time optimization. 

2. Designing a 3-stage training framework that increases efficiency for training multi-view inverse rendering networks by separating the estimation of lighting, materials, and geometry.

3. Achieving better inverse rendering performance compared to single-view methods, especially on real-world images, and enabling realistic insertion of virtual objects by reproducing real-world 3D lighting.

4. Creating the OpenRooms Forward Facing dataset to facilitate multi-view inverse rendering research. 

In summary, the key innovation seems to be using multi-view images and stereo depth to perform more robust scene-level inverse rendering compared to prior single-view methods. The multi-stage training framework and dataset are enabling contributions for multi-view inverse rendering. Overall, this enables high quality decomposition of real scenes for graphics applications like object insertion.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a multi-view attention inverse rendering framework called MAIR that progressively estimates geometry, direct lighting, material properties, and 3D spatially-varying lighting from multi-view images and depths, enabling realistic object insertion into real-world scenes.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in the field of multi-view inverse rendering:

- Input data: This method uses multi-view RGB images plus per-view depth maps. Some other multi-view inverse rendering methods require RGB-D images or 3D meshes. Not needing mesh generation makes this method more efficient.

- Scene representation: The paper decomposes the scene into geometry, SVBRDF materials, and a 3D spatially-varying lighting volume. Other works may represent the scene differently, like using a neural radiance field. 

- Training: The method is trained end-to-end on a synthetic dataset without needing test-time optimization. Other optimization-based approaches require slow test-time optimization.

- Applications: The estimated scene representation enables view synthesis and realistic object insertion. Some other works focus more narrowly on material estimation or relighting.

- Lighting: The lighting volume can represent complex global illumination effects like shadows and interreflections. Simpler lighting models in other works may not capture these effects well.

- Robustness: Evaluations suggest the multi-view approach is more robust on real images than single-view methods that rely on contextual reasoning. This demonstrates a benefit of the multi-view input.

In summary, key differentiating aspects are the use of multi-view data, end-to-end learning without optimization, and the application to object insertion enabled by the full scene decomposition. Comparisons suggest advantages over single-view and optimization-based techniques.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Expanding the multi-view dataset to include more diverse scenes and camera viewpoints. The authors created the OpenRooms Forward Facing dataset, but it is still limited in terms of scene diversity and camera poses. Expanding the dataset could help train more robust models. 

- Investigating joint training of the full pipeline instead of stage-wise training. The authors mention their pipeline may be suboptimal because it trains each stage separately. Exploring end-to-end joint training could potentially improve results.

- Developing more memory-efficient implementations to handle higher resolution outputs. The lighting volume resolution in their method is constrained by memory requirements. More efficient implementations could produce higher resolution lighting volumes. 

- Exploring different lighting volume representations beyond the non-parametric VSG used in their method. The authors mention VSG limits editing applications. Parametric volumes could enable better editing.

- Addressing limitations related to reliance on depth estimation and handling dynamic scenes. The authors note failures when depth estimation fails on things like dynamic objects. Improving robustness here could broaden applicability.

- Extending the approach to video and incorporating temporal information. The current method only leverages single image inputs. Video could provide useful motion and temporal cues.

- Validating on a more extensive set of real-world scenes. The method was mainly tested on synthetic data and limited real scenes. More rigorous real-world testing could better demonstrate robustness.

So in summary, some of the key directions are around improving the datasets/training, lighting representations, memory and computational efficiency, robustness, and extensions to video input.
