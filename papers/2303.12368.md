# [MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying   Lighting Estimation](https://arxiv.org/abs/2303.12368)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we perform robust scene-level inverse rendering using multi-view images to estimate geometry, materials, and lighting in real indoor scenes?

The key hypotheses appear to be:

1) Leveraging multi-view stereo depth and multiple observations of the same scene point can help estimate geometry, materials, and lighting more robustly compared to single image inverse rendering methods. 

2) A staged training pipeline that first estimates direct lighting and geometry, then materials, and finally full spatially-varying lighting can increase efficiency and reduce memory consumption for multi-view inverse rendering.

3) Representing direct lighting with both spherical Gaussians and voxels provides complementary cues about incident radiance and 3D exitant radiance to help estimate SVBRDF and spatially-varying lighting.

4) An attention-based multi-view aggregation network can selectively combine information across views to estimate materials more accurately.

In summary, the paper proposes a multi-view inverse rendering framework called MAIR to address the limitations of single image methods and enable more robust estimation of scene properties for real-world indoor images. The key innovation seems to be exploiting multi-view stereo depth and a staged training approach to efficiently handle multiple views.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

1. Proposing MAIR, which is the first multi-view scene-level inverse rendering pipeline that can decompose a scene into geometry, materials, and 3D spatially-varying lighting without requiring test-time optimization. 

2. Designing a 3-stage training framework that increases efficiency for training multi-view inverse rendering networks by separating the estimation of lighting, materials, and geometry.

3. Achieving better inverse rendering performance compared to single-view methods, especially on real-world images, and enabling realistic insertion of virtual objects by reproducing real-world 3D lighting.

4. Creating the OpenRooms Forward Facing dataset to facilitate multi-view inverse rendering research. 

In summary, the key innovation seems to be using multi-view images and stereo depth to perform more robust scene-level inverse rendering compared to prior single-view methods. The multi-stage training framework and dataset are enabling contributions for multi-view inverse rendering. Overall, this enables high quality decomposition of real scenes for graphics applications like object insertion.
