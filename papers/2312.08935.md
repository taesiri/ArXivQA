# [Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in   Mathematical Reasoning](https://arxiv.org/abs/2312.08935)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown remarkable capabilities across various tasks, but still face challenges in accurately solving complex, multi-step mathematical reasoning problems. 
- Prior work has explored techniques like pre-training, fine-tuning, and prompting to improve math reasoning abilities. Another approach is to use a separate verifier model to select the best answer from multiple LLM-generated candidates.
- Existing verifier models fall into two categories - outcome reward models (ORM) which score the final answer, and process reward models (PRM) which score each reasoning step. PRMs can provide more detailed feedback but require expensive human annotations for training data.

Proposed Solution: 
- The paper proposes Math-Shepherd, an innovative PRM that automatically constructs the training data without human annotation. 
- It defines the quality of a reasoning step based on its potential to reach the correct final answer. Using a trained "completer" LLM, it generates multiple possible subsequent reasoning paths from a given step and checks if they lead to the right answer. Steps that lead to more correct answers are scored higher.

Main Contributions:
- Proposes a fully automated framework to construct PRM training data by defining step quality based on deductive potential and validating multiple completions.
- Experiments on GSM8K and MATH datasets show Math-Shepherd enhances performance of various LLMs. The top model, DeepSeek 67B, achieves 93.3% on GSM8K and 48.1% on MATH, surpassing early GPT-4 results.
- Analysis shows the automated annotations correlate well with human judgments and outperform prior automatic PRM training methods.
- The automated methodology addresses the bottleneck around annotation costs for training PRMs, unlocking their potential for providing detailed feedback to further advance LLMs.

In summary, the paper introduces an innovative automated process supervision methodology for training mathematical reasoning verifiers, demonstrates its efficacy, and discusses its potential to further the evolution of LLMs.
