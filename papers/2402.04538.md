# [Triplet Interaction Improves Graph Transformers: Accurate Molecular   Graph Learning with Triplet Graph Transformers](https://arxiv.org/abs/2402.04538)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) like graph convolutional networks and graph attention networks lack direct communication between neighboring node pairs (edges). Information flows through the intermediate common node, creating a bottleneck. This is problematic for modeling geometric graphs like molecules where constraints like triangle inequality must be preserved.

- Incorporating 3D molecular geometry improves performance on chemical property prediction. But determining ground truth 3D geometries from quantum simulations is expensive at scale.

Proposed Solution: 
- Proposes Triplet Graph Transformer (TGT) architecture with novel triplet attention and aggregation mechanisms for direct communication between neighboring node pairs. Enables modeling of geometric dependencies in molecular graphs.

- Predicts interatomic distances from 2D molecular graphs using a distance predictor module. Uses these distances as input to a downstream task predictor module for property prediction. Eliminates need for ground truth 3D coordinates.

- Introduces three-stage training methodology involving pretraining on noisy 3D data and finetuning on predicted distances for efficiency and performance. Also proposes stochastic inference for robust predictions.

Main Contributions:
- Novel triplet interaction mechanisms in TGT for direct communication between neighboring node pairs, preserving geometric constraints. Vastly outperforms baseline methods.

- Two-module approach eliminates need for actual 3D coordinates by predicting interatomic distances from 2D graphs. Achieves new SOTA on quantum chemical benchmarks.

- Three-stage training procedure combining pretraining, finetuning and stochastic inference improves efficiency and performance.  

- Transferability of distance predictor demonstrated via SOTA on additional molecular property prediction benchmarks.

- State-of-the-art performance on TSP with TGT showcases broad utility of triplet interactions for graph learning.
