# [SpeedySpeech: Efficient Neural Speech Synthesis](https://arxiv.org/abs/2008.03802)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to develop a text-to-speech system that achieves fast training, fast inference, and high-quality audio synthesis simultaneously. 

The key hypotheses appear to be:

1) A convolutional teacher-student model can achieve fast and stable training compared to autoregressive models like Tacotron 2.

2) Self-attention layers are not necessary in the student network to achieve high quality synthesis. Simple convolutional architecture is sufficient.

3) Their proposed student-teacher model with convolutional blocks can match or exceed the voice quality of strong baselines like Tacotron 2 and DeepVoice 3.

4) Their model can synthesize speech significantly faster than real-time on both CPU and GPU hardware.

5) The entire model can be trained efficiently on a single GPU within a reasonable time frame.

In summary, the central research aim is developing an efficient and fast text-to-speech system without sacrificing output quality. The key hypotheses focus on using a convolutional student-teacher framework to achieve fast training and inference while matching state-of-the-art voice quality.


## What is the main contribution of this paper?

 The main contribution of this paper is a fast and efficient neural text-to-speech system based on a teacher-student model. Specifically:

- They propose a convolutional teacher model to extract alignments between phonemes and spectrogram frames. This model is simpler and faster to train than prior work like Tacotron 2.

- They propose a student model to generate mel spectrograms from phonemes and predicted durations. The student model uses only convolutional layers, avoiding slow self-attention layers like in Transformer or FastSpeech. 

- They show the model can be trained quickly on one GPU, with better audio quality than Tacotron 2 and Deep Voice 3 baselines.

- The model achieves real-time spectrogram synthesis on GPU and 5x faster than real-time on CPU. This is much faster than prior attentional TTS models.

- They simplify the FastSpeech architecture by removing self-attention and using different normalization, achieving a fast and stable training procedure.

In summary, the key contribution is demonstrating that high-quality, faster-than-real-time neural speech synthesis is possible with a simplified teacher-student convolutional model, without relying on self-attention. The model is both fast to train and fast to run on CPU/GPU hardware.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a fast and efficient neural text-to-speech model consisting of a teacher network that extracts alignments and a student network that generates high quality spectrograms faster than real-time, while requiring less data and compute resources than recent neural TTS models.
