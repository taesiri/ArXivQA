# [SpeedySpeech: Efficient Neural Speech Synthesis](https://arxiv.org/abs/2008.03802)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to develop a text-to-speech system that achieves fast training, fast inference, and high-quality audio synthesis simultaneously. 

The key hypotheses appear to be:

1) A convolutional teacher-student model can achieve fast and stable training compared to autoregressive models like Tacotron 2.

2) Self-attention layers are not necessary in the student network to achieve high quality synthesis. Simple convolutional architecture is sufficient.

3) Their proposed student-teacher model with convolutional blocks can match or exceed the voice quality of strong baselines like Tacotron 2 and DeepVoice 3.

4) Their model can synthesize speech significantly faster than real-time on both CPU and GPU hardware.

5) The entire model can be trained efficiently on a single GPU within a reasonable time frame.

In summary, the central research aim is developing an efficient and fast text-to-speech system without sacrificing output quality. The key hypotheses focus on using a convolutional student-teacher framework to achieve fast training and inference while matching state-of-the-art voice quality.


## What is the main contribution of this paper?

 The main contribution of this paper is a fast and efficient neural text-to-speech system based on a teacher-student model. Specifically:

- They propose a convolutional teacher model to extract alignments between phonemes and spectrogram frames. This model is simpler and faster to train than prior work like Tacotron 2.

- They propose a student model to generate mel spectrograms from phonemes and predicted durations. The student model uses only convolutional layers, avoiding slow self-attention layers like in Transformer or FastSpeech. 

- They show the model can be trained quickly on one GPU, with better audio quality than Tacotron 2 and Deep Voice 3 baselines.

- The model achieves real-time spectrogram synthesis on GPU and 5x faster than real-time on CPU. This is much faster than prior attentional TTS models.

- They simplify the FastSpeech architecture by removing self-attention and using different normalization, achieving a fast and stable training procedure.

In summary, the key contribution is demonstrating that high-quality, faster-than-real-time neural speech synthesis is possible with a simplified teacher-student convolutional model, without relying on self-attention. The model is both fast to train and fast to run on CPU/GPU hardware.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a fast and efficient neural text-to-speech model consisting of a teacher network that extracts alignments and a student network that generates high quality spectrograms faster than real-time, while requiring less data and compute resources than recent neural TTS models.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other recent research in spectrogram-based speech synthesis:

- It proposes a teacher-student architecture inspired by FastSpeech but with some key simplifications like replacing Transformer blocks with convolutional blocks. This makes the model smaller and faster compared to the original FastSpeech.

- The authors show that high quality spectrogram synthesis is possible without any self-attention layers, which were used extensively in models like Tacotron 2 and Transformer TTS. Replacing them with dilated convolutions makes the model simpler.

- For a vocoder, the paper uses MelGAN which is a smaller and faster neural vocoder compared to WaveNet. This enables real-time speech synthesis on CPU.

- The combined model achieves significantly higher quality than Tacotron 2 as rated by human listeners, while being much faster to train and synthesize. The training time on a single GPU is under 40 hours.

- Compared to Parallel WaveNet which also enables fast inference, this model does not require separately training a complex teacher network. The teacher here is simpler and faster to train.

- Overall, the focus is on creating an end-to-end spectrogram synthesis model that is fast, lightweight and high quality. The simplifications compared to previous work like FastSpeech and use of MelGAN allow the model to be very performant.

In summary, the key novelty is in showing that self-attention and complex teacher models are not needed for high-quality real-time spectrogram synthesis. The proposed simplifications make the model smaller, faster and easier to train compared to previous approaches while maintaining high voice quality.
