# [Chain-of-Questions Training with Latent Answers for Robust Multistep   Question Answering](https://arxiv.org/abs/2305.14901)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis appears to be:

Training a language model to generate and answer sub-questions step-by-step, using question decomposition meaning representations (QDMRs), can lead to more robust performance on multi-step question answering compared to standard single-step QA models and other existing methods.

In particular, the key claims seem to be:

- Existing methods for multi-step QA that perform explicit reasoning steps still suffer from robustness issues when evaluated on challenging contrast sets or adversarial sets.

- Prompting large language models with reasoning chains helps but still performs worse than fine-tuning methods on multi-step QA benchmarks and requires careful engineering.

- Their proposed Chain-of-Questions (CoQ) framework, which trains a model to generate QDMR sub-questions and sub-answers incrementally, can outperform both neuro-symbolic methods and prompting methods on multi-step QA benchmarks and their robustness test sets.

- CoQ's combination of Hard EM and MAPO objectives helps optimize the latent sub-answers effectively.

- CoQ demonstrates the ability to leverage existing QDMR annotations to develop an effective and robust multi-step QA system, advancing the state-of-the-art in this area.

In summary, the central hypothesis is that step-by-step sub-question generation and answering, trained with QDMRs, can lead to more robust multi-step QA compared to existing approaches. The paper aims to demonstrate this through the proposed CoQ framework.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel framework called Chain-of-Questions (CoQ) training with latent answers for robust multistep question answering. The key ideas are:

- They leverage question decomposition meaning representations (QDMR) to train a model to generate and answer sub-questions one at a time. 

- Since QDMR only contains sub-questions without sub-answers, they treat the sub-answers as latent variables and optimize them using a dynamic mixture of hard EM and memory augmented policy optimization (MAPO).

- They conduct experiments on DROP and HotpotQA benchmarks and their contrast/adversarial sets. CoQ outperforms strong baselines including neuro-symbolic methods and prompting methods, demonstrating its effectiveness and robustness for multistep QA.

In summary, the main contribution is proposing the CoQ framework to effectively utilize QDMR annotation and latent variable optimization techniques to train an end-to-end model for robust multistep QA. The results demonstrate strong performance on multiple benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper: 

The paper proposes a novel framework called Chain-of-Questions (CoQ) that trains a language model to robustly answer multistep questions by generating and answering sub-questions one at a time, leveraging human annotated question decomposition meaning representations (QDMRs) and optimizing latent sub-answers with a dynamic mixture of hard EM and MAPO (memory augmented policy optimization from reinforcement learning).
