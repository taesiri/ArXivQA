# [Chain-of-Questions Training with Latent Answers for Robust Multistep   Question Answering](https://arxiv.org/abs/2305.14901)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis appears to be:

Training a language model to generate and answer sub-questions step-by-step, using question decomposition meaning representations (QDMRs), can lead to more robust performance on multi-step question answering compared to standard single-step QA models and other existing methods.

In particular, the key claims seem to be:

- Existing methods for multi-step QA that perform explicit reasoning steps still suffer from robustness issues when evaluated on challenging contrast sets or adversarial sets.

- Prompting large language models with reasoning chains helps but still performs worse than fine-tuning methods on multi-step QA benchmarks and requires careful engineering.

- Their proposed Chain-of-Questions (CoQ) framework, which trains a model to generate QDMR sub-questions and sub-answers incrementally, can outperform both neuro-symbolic methods and prompting methods on multi-step QA benchmarks and their robustness test sets.

- CoQ's combination of Hard EM and MAPO objectives helps optimize the latent sub-answers effectively.

- CoQ demonstrates the ability to leverage existing QDMR annotations to develop an effective and robust multi-step QA system, advancing the state-of-the-art in this area.

In summary, the central hypothesis is that step-by-step sub-question generation and answering, trained with QDMRs, can lead to more robust multi-step QA compared to existing approaches. The paper aims to demonstrate this through the proposed CoQ framework.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel framework called Chain-of-Questions (CoQ) training with latent answers for robust multistep question answering. The key ideas are:

- They leverage question decomposition meaning representations (QDMR) to train a model to generate and answer sub-questions one at a time. 

- Since QDMR only contains sub-questions without sub-answers, they treat the sub-answers as latent variables and optimize them using a dynamic mixture of hard EM and memory augmented policy optimization (MAPO).

- They conduct experiments on DROP and HotpotQA benchmarks and their contrast/adversarial sets. CoQ outperforms strong baselines including neuro-symbolic methods and prompting methods, demonstrating its effectiveness and robustness for multistep QA.

In summary, the main contribution is proposing the CoQ framework to effectively utilize QDMR annotation and latent variable optimization techniques to train an end-to-end model for robust multistep QA. The results demonstrate strong performance on multiple benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper: 

The paper proposes a novel framework called Chain-of-Questions (CoQ) that trains a language model to robustly answer multistep questions by generating and answering sub-questions one at a time, leveraging human annotated question decomposition meaning representations (QDMRs) and optimizing latent sub-answers with a dynamic mixture of hard EM and MAPO (memory augmented policy optimization from reinforcement learning).


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in multistep question answering:

- This paper proposes a new method called Chain-of-Questions (CoQ) that trains a model to explicitly generate sub-questions and sub-answers from question decomposition meaning representations (QDMRs). Other methods like RERC and DecompRC also utilize question decomposition, but they use separate modules for decomposition and answering sub-questions. CoQ handles both within a single model.

- CoQ optimizes the sub-answers as latent variables using a combination of hard EM and MAPO from reinforcement learning. This is a novel approach not explored in prior work. Other methods rely solely on supervision from gold answers or use a separate QA model to generate sub-answers. 

- The paper shows CoQ achieves state-of-the-art performance on DROP and matches Longformer on HotpotQA. It significantly outperforms strong neuro-symbolic baselines like BERT-Calc and NeRd as well as prompting methods like chain-of-thought prompting on robustness test sets. This demonstrates CoQ's effectiveness at leveraging question decomposition for robust multistep QA.

- A limitation is that CoQ still requires task-specific modifications like adding regex and auxiliary prediction tasks. A universal architecture that works well across datasets without modification remains an open challenge. The method also relies on annotated QDMRs, which may not be available for all datasets.

Overall, the paper makes excellent progress on using question decomposition for robust QA. The CoQ framework and latent variable optimization seem highly promising for multistep reasoning compared to prior methods. Key limitations are the lack of universality across datasets and reliance on annotated question decompositions. Further work on transferring the approach and reducing this annotation dependence could yield even more generalizable multistep QA models.


## What future research directions do the authors suggest?

 The authors suggest several promising future research directions:

- Exploring the use of reinforcement learning to replace the supervision of QDMR. Rather than relying on annotated QDMR decompositions, this could reduce overfitting and further improve the model's robustness. 

- Testing whether models trained with this framework can transfer to other multistep QA datasets that lack QDMR annotation, such as FinQA, MuSiQue, and ROPES. This could demonstrate greater transfer learning ability.

- Scaling up the method to larger Transformer models like T5-3B, given sufficient compute resources. The current results are with smaller models like T5-Base/Large.

- Developing a more universal model that works well across different multistep QA datasets without needing as many task-specific modifications. UniQA achieved this for some QA tasks but still struggled on DROP.

- Analyzing the generated QDMR decompositions more deeply - e.g. categorizing types of errors, studying correlation of decomposition quality with final performance, etc.

- Exploring semi-supervised or unsupervised training, to reduce reliance on annotated QDMR data during training.

Overall, the key directions are developing variants that require less annotation, testing transferability, and scaling up the approach. The framework shows promise but could benefit from less dependence on annotated data and more generalization across tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a novel framework called Chain-of-Questions (CoQ) for training language models to answer complex, multi-step questions. It leverages existing Question Decomposition Meaning Representation (QDMR) annotations that provide decomposed sub-questions for questions, but does not have corresponding sub-answers. The key idea is to treat the sub-answers as latent variables and optimize them using a combination of hard Expectation-Maximization (Hard-EM) and memory augmented policy optimization (MAPO). Specifically, they use beam search to generate candidate sub-answer sequences, and pick the highest scoring sequence under Hard-EM to optimize the model. Additionally, they store high-quality sub-answer sequences in a memory buffer and sample from it, as in MAPO, to reduce variance. The overall training objective combines Hard-EM and MAPO losses, with MAPO given more weight as training progresses. Experiments on DROP and HotpotQA show CoQ outperforms strong neuro-symbolic baselines by 9-10 F1 on contrast sets, demonstrating its effectiveness for robust multi-step reasoning. Overall, the proposed method leverages QDMR annotations to train language models for decomposed question answering, leading to more robust performance on complex reasoning tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a novel framework called Chain-of-Questions (CoQ) for training language models to robustly answer multi-step questions. The key idea is to leverage existing question decomposition meaning representation (QDMR) annotations, which provide sub-questions for questions requiring multi-step reasoning. However, QDMR only contains sub-questions, not sub-answers, so the authors treat sub-answers as latent variables. They optimize these latent sub-answers using a dynamic mixture of hard EM and memory augmented policy optimization (MAPO). 

The authors conduct experiments on DROP and HotpotQA datasets and their contrast/adversarial sets. The results demonstrate CoQ greatly outperforms neuro-symbolic baselines like BERT-calculator and NeRd, as well as prompting methods like chain-of-thought prompting, on the contrast/adversarial sets. This shows CoQ successfully leverages QDMR annotations to train an effective and robust multi-step QA model. The authors highlight the challenges of multi-step QA prompting and show smaller models trained with CoQ can outperform prompting large language models like GPT-3. Overall, the work makes important progress on robust multi-step QA through innovative use of QDMR annotations.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes Chain-of-Questions (CoQ), a framework that trains a language model to answer multistep questions by generating and answering sub-questions. It leverages question decomposition meaning representations (QDMR), which provide human-annotated sub-questions for questions from multistep QA datasets. However, QDMR only contains the sub-questions, not the answers to those sub-questions. To address this, CoQ treats the sub-answers as latent variables and optimizes them using a combination of hard Expectation-Maximization (HardEM) and memory augmented policy optimization (MAPO). HardEM approximates the best sub-answer sequence using beam search to maximize the likelihood of generating the final correct answer. MAPO stores high-reward sub-answer trajectories in a memory buffer and samples them during training to reduce variance. CoQ combines these two objectives in a dynamic loss function that puts more weight on HardEM initially for faster improvement and more weight on MAPO later for better convergence. Experiments show CoQ outperforms neuro-symbolic baselines by 9 F1 on the DROP contrast set and outperforms chain-of-thought prompting on GPT-3 by 24.3 F1 on the HotpotQA adversarial set. The results demonstrate CoQ's effectiveness in leveraging QDMR annotations to train a robust multistep QA model.
