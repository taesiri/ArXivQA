# [Blockwise Parallel Transformer for Large Context Models](https://arxiv.org/abs/2305.19370)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we reduce the memory requirements of Transformers to enable training and evaluating them on much longer input sequences?The paper proposes a new method called Blockwise Parallel Transformer (BPT) to address the memory limitations of standard Transformers. The key ideas are:- Compute self-attention in a blockwise manner to avoid materializing the full attention matrix. This reduces the quadratic memory cost of self-attention from O(n^2) to O(n). - Additionally compute the feedforward network blockwise together with attention, instead of waiting to compute it on the full sequence after attention is done. This significantly reduces the memory cost associated with the large feedforward network.- Overall, BPT computes self-attention and feedforward network in a fused, blockwise parallel manner. This enables much longer input sequences to be processed while maintaining efficiency.The central hypothesis is that blockwise parallel computation of self-attention and feedforward network can minimize the memory requirements of Transformers, allowing them to handle much longer sequences than previously possible. The paper evaluates this through extensive experiments on language modeling and reinforcement learning tasks.In summary, the key research question is how to reduce Transformer memory costs to enable longer context training and evaluation, with the proposed BPT method that computes attention and feedforward network in a blockwise parallel fashion.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we reduce the memory requirements of Transformer models to enable them to handle longer input sequences?The paper proposes a new approach called Blockwise Parallel Transformer (BPT) to tackle the memory limitations of standard Transformer models. The key ideas are:- Computing self-attention in a blockwise manner without materializing the full attention matrix to reduce memory usage. - Additionally computing the feedforward network on a block-by-block basis together with attention, instead of waiting to process the entire sequence through the feedforward network after self-attention. This further reduces memory usage.- Overall, BPT processes longer input sequences while maintaining efficiency by fusing the blockwise computation of self-attention and feedforward network.The main hypothesis is that this blockwise parallel approach will enable Transformer models to handle much longer input sequences compared to standard Transformer architectures and previous memory-efficient methods. The experiments aim to validate whether BPT can actually support significantly longer context lengths during training.In summary, the paper introduces BPT to address the core research question of how to reduce Transformer memory costs to enable longer input sequences, with the central hypothesis that the proposed blockwise parallelization technique will achieve this goal. The experiments then seek to test this hypothesis and demonstrate the effectiveness of BPT.
