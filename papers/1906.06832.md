# [Sample-Efficient Neural Architecture Search by Learning Action Space](https://arxiv.org/abs/1906.06832)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design an efficient neural architecture search (NAS) method that can find high-performing neural network architectures with minimal computational cost. Specifically, the paper proposes a new NAS method called Latent Action Neural Architecture Search (LaNAS) that learns to partition the search space into good and bad regions in order to guide the search more efficiently. The key ideas and contributions of the paper are:- Most prior NAS methods use manually designed action spaces, which may not relate well to the final performance metric. In contrast, LaNAS learns latent actions to recursively partition the search space into good and bad regions containing architectures with similar performance.- LaNAS iterates between a learning phase where it trains linear models to partition the space, and a search phase where it uses Monte Carlo tree search along with the learned actions to guide sampling and optimization.- This learned partitioning focuses sampling on promising regions and provides a better training signal, significantly improving the sample efficiency of the search.- Experiments show LaNAS is 10-100x more sample efficient than baselines like random search, evolutionary methods, and Bayesian optimization on NAS benchmarks.- When deployed for real NAS tasks, LaNAS finds state-of-the-art architectures for CIFAR-10 and ImageNet using far fewer samples than prior work.In summary, the key hypothesis is that learning to partition the search space rather than relying on hand-designed actions can substantially improve the efficiency of neural architecture search. The results validate this hypothesis and demonstrate the effectiveness of the proposed LaNAS method.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes Latent Action Neural Architecture Search (LaNAS), a new neural architecture search method that learns actions to recursively partition the search space into good and bad regions for efficient architecture sampling. 2. It introduces a learning phase where LaNAS models each action as a linear constraint to bifurcate the search space based on model performance. The constraints form a hierarchical tree structure where leaf nodes contain promising model regions.3. It integrates Monte Carlo Tree Search in the search phase to traverse the learned tree structure and sample architectures from promising regions. This enables adaptive exploration to refine the learned actions.4. It demonstrates LaNAS is significantly more sample efficient than existing NAS methods like evolutionary algorithms, Bayesian optimization, random search on NAS benchmarks.5. When applied in practice, LaNAS finds state-of-the-art architectures for CIFAR-10 and ImageNet with much fewer samples than prior arts.In summary, the key innovation is learning latent actions to hierarchically partition the search space so that promising regions can be identified early for efficient architecture sampling. This results in orders of magnitude improvement in sample efficiency over other NAS techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new neural architecture search method called LaNAS that learns latent actions to recursively partition the search space into good and bad regions containing networks with similar performance metrics, and then uses Monte Carlo tree search to efficiently search the partitioned space.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in neural architecture search:- It proposes a new search method called Latent Action Neural Architecture Search (LaNAS) that learns to partition the search space recursively into good and bad regions. This is a novel approach compared to other methods like evolutionary algorithms, Bayesian optimization, etc. that don't explicitly learn to guide the search in this way.- The paper demonstrates that LaNAS is much more sample-efficient than other methods. It requires far fewer architecture evaluations to find optimal or near-optimal solutions. For example, it finds a top-performing architecture for CIFAR-10 in only 800 samples, compared to 27,000 samples for AmoebaNet.- LaNAS makes use of Monte Carlo tree search (MCTS) to balance exploration and exploitation during the search. Some prior NAS methods like AlphaX also use MCTS but with a manually defined action space. LaNAS learns the action space automatically.- The paper shows LaNAS can integrate well with one-shot NAS methods that use weight-sharing to avoid fully training each architecture. It outperforms other one-shot NAS techniques.- Compared to Bayesian optimization methods, LaNAS is more scalable to large search spaces since it converts the acquisition function optimization into a simple tree traversal rather than requiring auxiliary optimizations.- The learned action space provides interpretability about what the algorithm determines to be good vs bad regions of the space. This is a useful property compared to black-box optimization.Overall, the key innovations of LaNAS relative to prior NAS research are its learned latent actions to guide the search, superior sample efficiency, scalability, and interpretability. The results demonstrate it as a promising new approach for architecture search.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing more advanced architectures and search spaces for neural architecture search. The authors suggest exploring architectures beyond convolutional neural networks, such as recurrent networks, as well as more complex search spaces that allow automatic search over architectural components like normalization and activation functions.- Improving the accuracy of one-shot neural architecture search methods. The authors note there is still a performance gap between architectures found by one-shot methods versus training from scratch. They suggest investigating better weight sharing schemes or more accurate performance estimation techniques to close this gap.- Scaling up neural architecture search to larger datasets and problems. The authors propose applying NAS techniques to problems beyond image classification, such as object detection, segmentation, video analysis, etc. This requires scaling up the search to handle larger datasets and network capacities.- Making neural architecture search more efficient and faster. The computational cost of NAS remains high, so methods to reduce the search time, such as more sample-efficient search algorithms, parallelization, or hardware optimizations, could enable broader adoption.- Developing NAS techniques that optimize for objectives beyond accuracy, such as model size, latency, power usage, etc. Multi-objective NAS optimization is an important direction for finding networks suitable for real-world applications.- Validating NAS networks through more rigorous empirical analysis. The authors encourage further analysis of the properties, generalizability, and theoretical underpinnings of networks learned through neural architecture search.In summary, the main future directions are developing better search spaces and techniques, scaling up to larger problems, optimizing for real-world objectives, and conducting more rigorous analysis and validation of NAS approaches. Advances in these areas could help make neural architecture search even more effective and widely usable.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes Latent Action Neural Architecture Search (LaNAS), a new neural architecture search method that learns actions to recursively partition the search space into good and bad regions containing networks with similar performance metrics. LaNAS iterates between a learning phase where it learns a linear model to bi-partition the search space based on previous samples, and a search phase where it applies Monte Carlo Tree Search on the resulting tree structure to efficiently explore promising regions. Empirical evaluations on NAS benchmarks demonstrate LaNAS is highly sample efficient compared to methods like evolutionary algorithms, Bayesian optimization, and random search. When applied in practice, LaNAS finds state-of-the-art architectures for CIFAR-10 and ImageNet classification using significantly fewer samples than prior NAS methods. A key advantage of LaNAS is its ability to learn effective search space partitioning without making assumptions about the structure of the underlying search space.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a new neural architecture search method called Latent Action Neural Architecture Search (LaNAS). LaNAS iterates between a learning phase and a search phase. In the learning phase, it models each action as a linear constraint that partitions the search space into high-performing and low-performing regions. This forms a hierarchical tree structure where leaf nodes contain promising regions. In the search phase, Monte Carlo Tree Search (MCTS) is applied on the tree to sample architectures. The learned actions provide an abstraction of the search space for MCTS to efficiently search, while MCTS collects more data with adaptive exploration to refine the learned actions. Experiments demonstrate LaNAS is highly sample efficient compared to baselines including evolutionary algorithms, Bayesian optimization, and random search. It consistently achieves the lowest regret on diverse NAS tasks using at least an order of magnitude fewer samples. When applied in practice, LaNAS finds networks that achieve state-of-the-art accuracy on CIFAR-10 and ImageNet with significantly fewer samples than prior work. For example, it achieves 99.0% on CIFAR-10 using only 800 samples, outperforming AmoebaNet which required 33x more samples. The results show LaNAS is an effective and practical neural architecture search method.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. LaNAS iterates between a learning phase and a searching phase. In the learning phase, it models each action as a linear constraint that bi-partitions the search space into high-performing and low-performing regions, forming a hierarchical tree structure. In the searching phase, it applies Monte Carlo Tree Search on the tree structure to sample architectures. The learned actions provide an abstraction of the search space for efficient searching, while the searching collects more data to refine the learned actions. LaNAS is initialized with a few random samples before starting this iterative process. Overall, by learning to partition the search space rather than using predefined actions, LaNAS is able to focus on promising regions early on for more efficient neural architecture search.
