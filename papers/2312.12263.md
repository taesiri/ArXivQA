# [FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy   Labels](https://arxiv.org/abs/2312.12263)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of federated learning with noisy labels (F-LNL). In F-LNL, a global model is trained via collaborative learning across multiple local clients that have private data, some of which contains incorrectly labeled examples (noisy labels). This setting faces two key challenges: 1) data heterogeneity - the data is statistically heterogeneous across clients, and 2) noise heterogeneity - the noise distribution varies across clients. These challenges can destabilize the local training process and negatively impact model performance.

Proposed Solution: 
The paper proposes FedDiv, a framework to tackle F-LNL. The key ideas are:

1) Federated Noise Filtering: A global "federated noise filter" is learned to identify noisy labels in each client's local data. This is done by modeling the global distribution of noisy samples across all clients using a Gaussian Mixture Model, while preserving privacy.

2) Noisy Sample Relabeling: Noisy samples identified by the filter have their noisy labels removed. Those with high prediction confidence from the global model are relabeled with pseudo-labels to retain them for training.

3) Predictive Consistency based Sampling: A subset of clean and relabeled noisy samples are selected for local training to maximize agreement between predictions of the global and local models. This prevents noise memorization and improves stability.

Main Contributions:
- Proposes FedDiv, a novel framework to address key challenges in F-LNL using collaborative noise filtering, relabeling and consistent sampling.
- Introduces a global noise filter that leverages knowledge from all clients to effectively filter noisy labels in each client.
- Prevents noise memorization and enhances stability via predictive consistency based sampling.
- Experiments show superior performance over state-of-the-art F-LNL methods on CIFAR and Clothing1M datasets.

In summary, the key innovation of FedDiv is collaborative noise filtering to improve local training stability along with techniques to prevent noise memorization, resulting in performance gains for F-LNL.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a federated learning framework called FedDiv that performs collaborative noise filtering across clients to identify and relabel noisy samples, and prevents noise memorization through predictive consistency based sample re-selection, in order to effectively handle the challenges of data and label noise heterogeneity inherent in federated learning.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new framework called FedDiv to address the challenges of federated learning with noisy labels (F-LNL). Specifically:

1) It proposes a global noise filter called Federated Noise Filter to effectively identify samples with noisy labels on every client by modeling the global distribution of label noise across all clients. This helps with noise filtering and improves training stability.

2) It introduces a Predictive Consistency based Sampler to identify more credible local data for training the local models. This helps prevent noise memorization and further improves training stability.

3) Extensive experiments demonstrate that FedDiv achieves superior performance over state-of-the-art F-LNL methods under different label noise settings and data partitions (IID/Non-IID).

In summary, the key contribution is a new F-LNL framework with novel components for collaborative noise filtering and preventing noise memorization, which leads to improved performance and stability compared to prior arts.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Federated learning (FL) - A distributed machine learning approach that enables model training on decentralized data located on multiple clients without requiring direct access to raw local data.

- Federated learning with noisy labels (F-LNL) - A federated learning setting where some local clients have noisy/incorrect labels in their training data.

- Data heterogeneity - The statistical heterogeneity of data distributions across different clients in federated learning.

- Noise heterogeneity - The varying noise distributions among clients in federated learning with noisy labels. 

- Federated Noise Filter (FNF) - A global Gaussian Mixture Model proposed in this paper to identify noisy labels on each client by modeling the global distribution of noisy samples across all clients.

- Predictive Consistency based Sampler (PCS) - A strategy introduced in this paper to re-select more reliable labeled samples for local model training, preventing noise memorization.

- Training stability - The stability of the training process, which can be disrupted by data and noise heterogeneity, leading to performance issues. The methods in this paper aim to improve stability.

- Noise filtering - The process of identifying noisy labeled samples. This paper proposes collaborative noise filtering by learning from all clients. 

- Noisy sample relabeling - Assigning new corrected labels, usually based on prediction confidence, to samples identified as incorrectly labeled.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a global noise filter called Federated Noise Filter (FNF). How does modeling the global distribution of noisy samples help enhance noise filtering performance on individual clients compared to just using a local filter?

2. The FNF model is constructed via a federated EM algorithm. Walk through how the E-step and M-step work in this context to fit Gaussian Mixture Models on each client and aggregate them on the server. 

3. After the initial noise filtering, noisy samples are relabeled using pseudo-labels from the global model. Explain the motivation behind this relabeling and discuss any potential issues it may introduce. 

4. The paper introduces a Predictive Consistency based Sampler (PCS) to prevent noise memorization. Discuss how PCS identifies more reliable labeled samples by enforcing consistency between global and local model predictions.  

5. How exactly does PCS leverage counterfactual reasoning to de-bias the predictions of the local model? Explain the intuition behind Eq. 8 in the paper.

6. Compare and contrast the objectives optimized during local training sessions under IID vs non-IID data partitioning. What is the motivation behind using the regularization term $\mathcal{L}_{reg}$ only for non-IID settings?

7. The paper conducts an ablation study analyzing the impact of different components of FedDiv. Choose one component, summarize the ablation results, and discuss the implications.  

8. Figure 3 visually depicts the superiority of FedDiv's noise filtering. Analyze the figure in depth and discuss why collaborative noise filtering outperforms alternatives.

9. Algorithm 1 provides the overarching training procedure. Walk through the key steps and highlight how privacy is preserved despite collaborative learning of the noise filter.

10. For real-world deployment, discuss any potential limitations of the proposed approach and how the methodology might evolve to handle issues around scalability, system heterogeneity, etc.
