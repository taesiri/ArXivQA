# [Non-Autoregressive Predictive Coding for Learning Speech Representations   from Local Dependencies](https://arxiv.org/abs/2011.00406)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn effective speech representations in a more efficient, non-autoregressive manner relying only on local dependencies in the speech signal. The key points are:- Existing methods like CPC, APC, and MLM rely on autoregressive models and/or global dependencies to learn speech representations. This makes them inefficient due to inability to parallelize and complexity related to sequence length. - The proposed method, Non-Autoregressive Predictive Coding (NPC), learns speech representations without autoregression and using only local dependencies within a fixed context window. This allows parallelization and inference time independent of sequence length.- NPC uses a simple predictive coding objective, where each frame is predicted from its local context window after masking the target itself. This is implemented via introduced Masked Convolutional Blocks.- Experiments show NPC provides significant speedups while achieving representation quality comparable to previous methods on phonetic and speaker classification tasks.- Analysis of learned model weights confirms NPC relies primarily on local context, supporting the design.In summary, the central hypothesis is that non-autoregressive modeling with local context is sufficient for learning effective and efficient speech representations. The NPC method and experiments support this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a self-supervised method called Non-Autoregressive Predictive Coding (NPC) to learn speech representations efficiently. The key points are:- NPC learns speech representations in a non-autoregressive manner, only relying on local context instead of global dependencies. This allows parallelization during inference and makes the running time constant regardless of input length. - NPC has a simple objective of reconstructing the current frame from surrounding context frames. Masked convolution blocks are introduced to prevent the model from directly copying the target frame.- Experiments show NPC representations are comparable to prior work in phonetic/speaker classification, while being significantly faster. Theoretical analysis and empirical measurements demonstrate the efficiency advantages.- Analysis of the learned model provides insights into how NPC relies on local context, with frames closest to the target having the highest importance.In summary, the main contribution is proposing NPC as a fast and simple self-supervised method to learn speech representations by only relying on local dependencies instead of autoregressive or global modeling. The efficiency benefits are demonstrated while maintaining competitive performance on representation quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes Non-Autoregressive Predictive Coding (NPC), a self-supervised method for learning speech representations from only local dependencies of the speech signal in a non-autoregressive manner, which is faster and more parallelizable compared to prior autoregressive and globally dependent methods.
