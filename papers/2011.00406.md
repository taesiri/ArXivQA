# [Non-Autoregressive Predictive Coding for Learning Speech Representations   from Local Dependencies](https://arxiv.org/abs/2011.00406)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn effective speech representations in a more efficient, non-autoregressive manner relying only on local dependencies in the speech signal. The key points are:- Existing methods like CPC, APC, and MLM rely on autoregressive models and/or global dependencies to learn speech representations. This makes them inefficient due to inability to parallelize and complexity related to sequence length. - The proposed method, Non-Autoregressive Predictive Coding (NPC), learns speech representations without autoregression and using only local dependencies within a fixed context window. This allows parallelization and inference time independent of sequence length.- NPC uses a simple predictive coding objective, where each frame is predicted from its local context window after masking the target itself. This is implemented via introduced Masked Convolutional Blocks.- Experiments show NPC provides significant speedups while achieving representation quality comparable to previous methods on phonetic and speaker classification tasks.- Analysis of learned model weights confirms NPC relies primarily on local context, supporting the design.In summary, the central hypothesis is that non-autoregressive modeling with local context is sufficient for learning effective and efficient speech representations. The NPC method and experiments support this hypothesis.
