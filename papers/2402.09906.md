# [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Text embedding models and large language models (LLMs) for generation have been developed separately leading to trade-offs. Using LLMs directly for embedding performs poorly and vice versa.
- Combining embedding and generative models leads to inefficient pipelines requiring maintaining and load balancing separate models.

Proposed Solution:
- Introduce Generative Representational Instruction Tuning (GRIT) to train a single model termed GritLM to handle both text embedding and generation via instructions.
- Unified model matches performance of separate embedding-only and generative-only variants.
- Model simplifies and speeds up pipelines relying on both capabilities like Retrieval Augmented Generation (RAG).

Main Contributions:
- GritLM sets a new SOTA on the Massive Text Embedding Benchmark among open models while beating other generative models up to 70B parameters.
- Scaling GritLM further, it becomes the best open generative model tried while retaining excellent embedding capabilities.
- Show that causal LM fine-tuning benefits from adapting to bidirectional attention for embedding while keeping causal attention for generation.
- Analyze trade-offs of sample vs token level generative loss and determine an optimal mixing strategy.
- Propose caching strategies to simplify and accelerate RAG with GritLM by over 60% for long sequences.

In summary, the paper unifies text embedding and generation via a simple finetuning approach to create a model that excels at both capabilities simultaneously while simplifying pipelines relying on both text embeddings and generation.
