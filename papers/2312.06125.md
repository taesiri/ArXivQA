# [Pre-Evolved Model for Complex Multi-objective Optimization Problems](https://arxiv.org/abs/2312.06125)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper proposes a novel Pre-Evolved Model (PEM) approach that leverages transformer architectures and ideas of pre-training and fine-tuning from deep learning to improve multi-objective evolutionary algorithms (MOEAs). The key idea is to pre-evolve a model called the Pre-Evolved Transformer (PET) on existing benchmark MOPs using state-of-the-art MOEAs. This allows the PET to learn effective evolutionary patterns. The PET can then be embedded into any MOEA and fine-tuned to efficiently generate high-quality solutions when solving new complex MOPs, including those with many objectives, expensive evaluations, and constraints. The PET incorporates two new techniques: dimension embedding to handle varying decision variable sizes, and objective encoding to represent solution evaluations. Experiments demonstrate that fine-tuning the pre-evolved PET significantly outperforms state-of-the-art MOEAs and specialized algorithms on a diverse set of complex benchmark MOPs as well as real-world test problems from power systems. The PEM approach provides a promising new paradigm for improving performance across different types of MOPs.
