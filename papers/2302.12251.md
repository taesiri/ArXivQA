# [VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene   Completion](https://arxiv.org/abs/2302.12251)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes VoxFormer, a Transformer-based framework for 3D semantic scene completion from monocular images. The central hypothesis is that an image-based 3D scene completion system can achieve strong performance by:1) Using a two-stage approach where reconstruction is done before hallucination. The paper hypothesizes that reconstructing the visible scene first provides a more reliable set of features to propagate information to the unobserved regions. 2) Operating on sparse voxel queries rather than dense 3D projections. The paper hypothesizes that using a sparse set of voxel queries linked to image observations is more efficient and avoids ambiguities caused by dense 3D projections.So in summary, the central hypothesis is that an image-based scene completion system can surpass previous approaches by using a two-stage sparse-to-dense framework with explicit depth-based reconstruction and transformer-based feature propagation. The experiments aim to validate that this approach leads to state-of-the-art performance for monocular 3D scene completion.


## What is the main contribution of this paper?

The main contribution of this paper is proposing VoxFormer, a Transformer-based framework for camera-based 3D semantic scene completion. Specifically:- It proposes a two-stage design called "reconstruction-before-hallucination", where stage 1 generates sparse voxel queries from image depth to reconstruct visible structures, and stage 2 propagates information to occluded areas using a novel MAE-like architecture. - It introduces a lightweight 2D CNN-based query proposal network that selects reliable voxel queries based on estimated depth and occupancy. This avoids ambiguity from dense 2D-to-3D feature projection.- It achieves state-of-the-art performance on SemanticKITTI for camera-based scene completion, with especially significant improvements in short-range areas critical for autonomous driving.- Compared to prior arts like MonoScene that uses heavy 3D convolutions, VoxFormer is more efficient in parameters and GPU memory by using a sparse Transformer design.In summary, the key innovation is the two-stage sparse voxel query framework for image-based scene completion, which outperforms dense projection baselines. The efficiency and strong empirical results make VoxFormer an important advance for camera-based 3D scene perception.
