# [Quantizable Transformers: Removing Outliers by Helping Attention Heads   Do Nothing](https://arxiv.org/abs/2306.12929)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we modify the architecture of transformers to prevent them from learning strong activation outliers during pre-training, in order to make the networks easier to quantize for efficient inference without degradation in accuracy?The key hypotheses seem to be:1) The outliers occur because some attention heads are trying to learn a "no-op" or not update the representation for certain tokens, which requires pushing the softmax input to have a very large dynamic range. This results in outliers due to the combination of softmax, residuals, and layernorm.2) By making architectural changes to allow the network to represent exact zeros or small updates in attention without outliers, such as clipped softmax or gated attention, we can achieve comparable accuracy while removing outliers.3) This will enable easy and effective post-training quantization to low-bit integers without accuracy loss or need for retraining.So in summary, the paper proposes architectural modifications to prevent outlier learning in order to improve transformer quantization, and validates this through experiments on BERT, OPT, and ViT models.
