# [Quantizable Transformers: Removing Outliers by Helping Attention Heads   Do Nothing](https://arxiv.org/abs/2306.12929)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we modify the architecture of transformers to prevent them from learning strong activation outliers during pre-training, in order to make the networks easier to quantize for efficient inference without degradation in accuracy?The key hypotheses seem to be:1) The outliers occur because some attention heads are trying to learn a "no-op" or not update the representation for certain tokens, which requires pushing the softmax input to have a very large dynamic range. This results in outliers due to the combination of softmax, residuals, and layernorm.2) By making architectural changes to allow the network to represent exact zeros or small updates in attention without outliers, such as clipped softmax or gated attention, we can achieve comparable accuracy while removing outliers.3) This will enable easy and effective post-training quantization to low-bit integers without accuracy loss or need for retraining.So in summary, the paper proposes architectural modifications to prevent outlier learning in order to improve transformer quantization, and validates this through experiments on BERT, OPT, and ViT models.


## What is the main contribution of this paper?

The main contribution of this paper appears to be proposing two modifications to the attention mechanism in Transformers - clipped softmax and gated attention - that can help reduce the outlier activations learned during training. The key ideas are:- The paper analyzes why Transformers tend to learn strong outlier activations, relating it to attention heads trying to perform a "no-op" or not update the representation. To get the exact zeros needed for no-update via the softmax, the input gets pushed to a very large range, causing outliers.- Based on this analysis, the authors propose clipped softmax to allow representing exact zeros without outliers by clipping the softmax input range. They also propose gated attention which adds a gating module to optionally pass through the attention output, avoiding reliance on outliers.- Experiments on BERT, OPT (GPT-style) and ViT models show these modifications can significantly reduce outlier magnitudes while maintaining or sometimes improving accuracy. This enables easy quantization to 8-bit integers without outliers issues.So in summary, the key contribution is identifying the root cause of outliers in Transformers and proposing two simple but effective architectural changes to remove this issue, enabling more efficient inference. The modifications seem generic and could likely benefit many Transformer variants.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, the summary is: The paper proposes two modifications to the self-attention mechanism in transformer models - clipped softmax and gated attention - to prevent the models from learning large outliers during pre-training, making the models easier to quantize for efficient deployment.
