# [Quantizable Transformers: Removing Outliers by Helping Attention Heads   Do Nothing](https://arxiv.org/abs/2306.12929)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we modify the architecture of transformers to prevent them from learning strong activation outliers during pre-training, in order to make the networks easier to quantize for efficient inference without degradation in accuracy?The key hypotheses seem to be:1) The outliers occur because some attention heads are trying to learn a "no-op" or not update the representation for certain tokens, which requires pushing the softmax input to have a very large dynamic range. This results in outliers due to the combination of softmax, residuals, and layernorm.2) By making architectural changes to allow the network to represent exact zeros or small updates in attention without outliers, such as clipped softmax or gated attention, we can achieve comparable accuracy while removing outliers.3) This will enable easy and effective post-training quantization to low-bit integers without accuracy loss or need for retraining.So in summary, the paper proposes architectural modifications to prevent outlier learning in order to improve transformer quantization, and validates this through experiments on BERT, OPT, and ViT models.


## What is the main contribution of this paper?

The main contribution of this paper appears to be proposing two modifications to the attention mechanism in Transformers - clipped softmax and gated attention - that can help reduce the outlier activations learned during training. The key ideas are:- The paper analyzes why Transformers tend to learn strong outlier activations, relating it to attention heads trying to perform a "no-op" or not update the representation. To get the exact zeros needed for no-update via the softmax, the input gets pushed to a very large range, causing outliers.- Based on this analysis, the authors propose clipped softmax to allow representing exact zeros without outliers by clipping the softmax input range. They also propose gated attention which adds a gating module to optionally pass through the attention output, avoiding reliance on outliers.- Experiments on BERT, OPT (GPT-style) and ViT models show these modifications can significantly reduce outlier magnitudes while maintaining or sometimes improving accuracy. This enables easy quantization to 8-bit integers without outliers issues.So in summary, the key contribution is identifying the root cause of outliers in Transformers and proposing two simple but effective architectural changes to remove this issue, enabling more efficient inference. The modifications seem generic and could likely benefit many Transformer variants.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, the summary is: The paper proposes two modifications to the self-attention mechanism in transformer models - clipped softmax and gated attention - to prevent the models from learning large outliers during pre-training, making the models easier to quantize for efficient deployment.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The key contribution of this paper seems to be the two proposed modifications to the standard transformer architecture - clipped softmax and gated attention. These tweaks appear novel compared to prior work on quantizing transformers, which has mostly focused on changes to the training process or quantization methodology. - The authors provide an in-depth analysis of the root causes of outliers in transformer models, attributing them to "no-op" behavior by certain attention heads. This level of investigation into the source of outliers is more thorough than some prior work that observed their existence but did not delve into the underlying reasons (e.g. Dettmers et al., Bondarenko et al.).- The paper builds directly on previous findings that modern transformers are difficult to quantize due to activation outliers after certain layers (Bondarenko et al., Dettmers et al.). However, it takes a new approach by addressing the architectural causes rather than adapting the quantization process.- Compared to work like Q-BERT and Q8BERT that required retraining with Quantization-Aware Training, this method enables good quantization using only Post-Training Quantization which is more convenient.- The techniques are evaluated on common transformer models like BERT and ViT, demonstrating broad applicability. The consistent gains across multiple architecture types is more comprehensive than some prior studies focused solely on BERT.- The paper mostly focuses on quantization for computational efficiency rather than compression, in contrast to work like Compressive Transformers. The techniques may complement size-oriented methods.- There is limited investigation of the impact on large-scale transformers. Some other papers have studied quantization specifically for huge models like GPT-3.In summary, the paper delivers solid incremental progress over key prior work by proposing novel architectural changes to address the outlier problem and evaluating them thoroughly on major Transformer models. The analysis of the root causes also provides useful new insights.
