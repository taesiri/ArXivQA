# [Transformers Implement Functional Gradient Descent to Learn Non-Linear   Functions In Context](https://arxiv.org/abs/2312.06528)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper studies how transformers with non-linear activations can learn non-linear functions in context. The authors show that under simple parameter configurations, transformers implement functional gradient descent (GD) in the reproducing kernel Hilbert space (RKHS) induced by the non-linear activation. This enables transformers to learn complex non-linear relationships. Specifically, when the non-linear activation matches the RKHS of the underlying data generating process, the transformer predictions converge to the Bayes optimal estimator. The authors prove that their proposed functional GD construction arises as a stationary point of the in-context loss landscape. They also characterize more sophisticated algorithms learned under less constrained settings. Across a range of transformer variants and data distributions, experiments verify the theoretical predictions on learned algorithms, optimality of matching activations, and loss landscape stationary points. The results advance our understanding of how transformers exploit architectural inductive biases to effectively learn non-linear functions.
