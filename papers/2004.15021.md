# [Consistent Video Depth Estimation](https://arxiv.org/abs/2004.15021)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question appears to be: How can we estimate dense, geometrically consistent depth for all pixels in a monocular video, even in the presence of challenging factors like dynamic motion, textureless regions, and camera shake?The key ideas and contributions in addressing this question seem to be:- Leveraging both traditional geometric constraints from structure-from-motion as well as learning-based depth priors from a convolutional neural network.- Fine-tuning the neural network at test time on the particular input video to satisfy geometric constraints while retaining ability to plausibly fill in unconstrained regions.- Using long-range temporal connections during test-time training to achieve global consistency.- Demonstrating results on real casual smartphone video that are dense, sharp, and consistent even for dynamic scenes. Enabling new video effects applications.In summary, the paper aims to combine strengths of conventional reconstruction and learning-based methods to achieve consistent dense depth from monocular video in challenging capture conditions. The main hypothesis appears to be that this can be done effectively through test-time fine-tuning of a neural network depth estimator.


## What is the main contribution of this paper?

This paper presents a method for estimating dense, geometrically consistent depth maps from monocular video. The key ideas and contributions are:- They combine traditional multi-view stereo techniques with learning-based single image depth estimation. Multi-view stereo provides geometric constraints between frames, while the learned depth prior fills in detail.- At test time, they fine-tune a pre-trained single-image depth network on the input video to satisfy geometric consistency losses derived from multi-view stereo. This allows adapting the network to produce consistent depth specifically for that video.- They design losses based on reprojection error and disparity consistency. Minimizing these losses during test-time fine-tuning makes the estimated depth maps across frames geometrically consistent.- Their method produces high-quality depth maps that are more complete, detailed, and temporally consistent compared to prior monocular depth estimation techniques.- They demonstrate results on challenging real-world videos captured with handheld phones that exhibit noise, blur, and object motion. Their method handles these well.- The improved depth estimation enables novel video-based effects like inserting virtual objects that properly interact with the scene.In summary, the main contribution is a novel framework to combine traditional geometric and learning-based depth estimation techniques through test-time training, in order to produce detailed and consistent depth maps from monocular video. The results are visually and quantitatively superior to previous methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents an algorithm for reconstructing dense, geometrically consistent depth maps for all pixels in a monocular video by leveraging a conventional structure-from-motion reconstruction to establish geometric constraints, using a convolutional neural network trained for single-image depth estimation as a learning-based prior, and fine-tuning the network at test time to satisfy the geometric constraints of the particular input video.


## How does this paper compare to other research in the same field?

This paper presents a method for estimating consistent depth maps from monocular videos. Here are some key ways it relates to other research in this field:- Compared to traditional multi-view stereo methods like COLMAP, it produces much denser depth maps by incorporating learning-based depth priors. However, it retains geometric consistency by integrating constraints from COLMAP during test-time training.- Compared to single-image learning-based depth estimation methods, it achieves better temporal coherence and geometric consistency across the video. This is done through test-time training with losses that enforce multi-view geometry.- Compared to other video-based depth estimation methods, it handles dynamic scenes better by relying on single-image depth networks as a base model. Methods like DeepV2D and NeuralRGBD rely more on multi-view assumptions. - The test-time training scheme is related to other online adaptation techniques in depth estimation. However, the focus here is on achieving global geometric consistency rather than just per-frame accuracy.- The losses enforcing multi-view geometry are related to other self-supervised depth methods. But this work shows they can be effectively used for test-time refinement on videos to achieve better coherence.Overall, this method combines strengths of both classical multi-view geometry and learning-based synthesis. The test-time training approach adapts a single-image network to produce dense, consistent, high-quality depth for challenging monocular video footage. The quantitative and qualitative results demonstrate improvements over previous state-of-the-art in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing online and real-time variants of their method. The current approach requires processing the entire video offline to extract geometric constraints between frames. Creating a system that could process a live video stream would be important for practical applications.- Handling more extreme object and camera motion. The current method works for videos with gentle or moderate motion but breaks down with very large motions. Extending the approach to handle more challenging dynamic scenes would broaden its applicability.- Improving the underlying pose estimation and optical flow components. The method relies on off-the-shelf modules for these tasks currently. Advances in learning-based pose estimation and flow specifically for monocular video could potentially improve the overall reconstruction.- Exploring different depth representations beyond per-pixel depths. Limitations of the current depth map representation are that it assigns only one depth value per pixel and cannot represent transparent or reflective surfaces well. Alternative representations could mitigate these issues.- Incorporating semantic understanding. Leveraging semantic segmentation or recognition within the pipeline could potentially improve dynamic scene handling and resolve ambiguities.- Enabling online adaptation and life-long learning. The current test-time training does not retain improvements on one video for the next. An online system that progressively improves over many video experiences could be more powerful.In summary, the main future directions aim to extend the applicability and robustness of the approach, improve the underlying system components, and investigate richer scene representations and learning strategies. The authors propose addressing limitations of the current method to work towards a real-time dynamic depth estimation system for live monocular video.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. The method leverages a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike typical reconstruction methods that use ad-hoc priors, this approach uses a learning-based prior (a CNN trained for single-image depth estimation). At test time, the network is fine-tuned to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in less constrained areas. The results are fully dense depth maps that are more accurate and consistent than previous monocular reconstruction methods, with improved stability. The method can handle challenging hand-held videos with some dynamic motion. The higher quality reconstruction enables applications like scene reconstruction and advanced video effects.
