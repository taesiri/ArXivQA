# [Data-Efficient Multimodal Fusion on a Single GPU](https://arxiv.org/abs/2312.10144)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent multimodal fusion methods require massive datasets and large-scale compute for training, making them inaccessible for many researchers and practitioners. Obtaining high-quality paired multimodal data is also expensive and scarce. Existing methods also exhibit tight coupling between modalities, hampering modularity and progress.

Proposed Solution: 
The authors propose an efficient and modular framework for multimodal fusion called FuseMix. The key idea is to leverage readily available pre-trained unimodal encoders (e.g. for image, text, audio) instead of training multimodal encoders from scratch. These unimodal encoders are kept frozen, and only their latent representations are extracted once upfront using a single GPU. Lightweight fusion adapters are then trained on the latent spaces to align them into a shared space using contrastive learning. This design is computationally efficient as gradients don't backpropagate through large unimodal encoders.

To facilitate fusion with limited paired multimodal data, the authors also propose a simple yet effective data augmentation technique called FuseMix. It applies mixup in the latent spaces by linearly interpolating between positive pairs, while crucially sharing the interpolation coefficient across modalities. This allows generating synthetic multimodal pairs that remain semantically consistent across modalities.

Contributions:
1) Efficient modular framework for multimodal fusion leveraging arbitrary frozen pretrained unimodal encoders
2) FuseMix - simple and effective latent space data augmentation for multimodal learning
3) Achieve SOTA image-text and audio-text retrieval with 600x less compute and 80x less data than CLIP
4) Demonstrate the effect of dataset quantity, quality and diversity on downstream performance
5) Show applicability for audio-to-image generation by converting text-to-image models into audio-to-image ones

Overall, the work provides an accessible and flexible approach to multimodal fusion that sets a new bar for efficiency. The modular design also helps democratize progress in the field.
