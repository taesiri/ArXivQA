# [MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation](https://arxiv.org/abs/2304.09801)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we design a 3D perception model that is robust to various sensor failures, such as sensor corruption and absence, for autonomous driving applications? The key hypothesis is that by using a flexible fusion approach in bird's eye view space, the model can effectively handle missing or corrupted sensor inputs from cameras and LiDAR. Specifically, the paper proposes MetaBEV, a framework that:- Uses a meta-BEV query to iteratively evolve the BEV representation by selectively aggregating features from available sensor inputs via cross-modal attention. This allows the model to handle missing modalities.- Incorporates model-specific components in the cross-modal attention to enable fusion of arbitrary subsets of modalities. This provides robustness against corrupted inputs.- Leverages a switched modality training scheme to simulate sensor failures during training. This improves generalization under sensor failures.- Introduces a Multi-Task Mixture of Experts design to mitigate performance drops in multi-task learning.Through experiments on nuScenes dataset, the paper shows that MetaBEV substantially outperforms prior arts in various sensor corruption and absence scenarios while achieving state-of-the-art performance on canonical perception tasks.In summary, the central hypothesis is that a flexible fusion approach can enable robust 3D perception under diverse real-world sensor failure cases. MetaBEV is proposed and evaluated as a model that realizes this hypothesis.
