# [Understanding the Training Speedup from Sampling with Approximate Losses](https://arxiv.org/abs/2402.07052)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Stochastic gradient descent (SGD) is commonly used to train ML models, but vanilla SGD selects samples randomly. Selecting more "important" samples can accelerate training.
- Exact importance sampling is infeasible due to the high computational cost of computing per-sample gradients. 
- Prior work uses approximations like the per-sample loss, but computing exact losses has high overhead for large models.

Proposed Solution:
- Consider a "greedy" SGD variant (GSGD) which selects the sample with highest approximate loss to perform the update.
- Use early exiting to quickly get approximate losses using intermediate layer representations. This is a lightweight way to filter samples.
- Propose SIFT - a practical strategy to backprop on only 50% of samples in a batch with highest approximate losses from early exiting.

Theoretical Contributions:
- Provide convergence guarantees for GSGD with approximate losses, characterizing when it can converge faster than SGD to a constant factor of the minimum loss.
- Quantify probability of early exit-based selection preserving the argmax of actual losses.

Empirical Contributions:
- Apply SIFT to train 12 layer BERT base from scratch. With early exit at layer 1, SIFT takes ~43 hours to reach 64% validation accuracy versus ~57 hours for baseline.
- Show SIFT is better than baseline in terms of sample complexity and wall-clock time. Entropy-based selection does better than loss-based.

In summary, the paper provides an analysis of greedy selection with approximate losses and demonstrates the promise of early exiting to accelerate training of large models like BERT.
