# [Scattered or Connected? An Optimized Parameter-efficient Tuning Approach   for Information Retrieval](https://arxiv.org/abs/2208.09847)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we design a parameter-efficient tuning approach that stabilizes the training process for information retrieval models?The authors first conduct a comprehensive study of existing parameter-efficient tuning methods like adapter and prefix-tuning for information retrieval models. They find that while these methods reduce the number of parameters needed for fine-tuning, they lag behind full fine-tuning and suffer from unstable training. To analyze why, the authors provide a theoretical analysis showing that the separation of the inserted trainable modules in existing methods causes a discrepancy between the ideal optimization direction and actual update direction during training. This makes the optimization difficult and training unstable.To address this issue, the authors propose a new method called Inside and Aside (IAA) tuning. The key idea is to inject additional "aside" modules that connect the original scattered "inside" modules to smooth the loss surface and stabilize training. Experiments show their proposed IAA method significantly outperforms existing parameter-efficient tuning approaches and achieves comparable or better performance than full fine-tuning.In summary, the central hypothesis is that connecting the scattered trainable modules can stabilize training for parameter-efficient tuning methods in information retrieval. The IAA method is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is an optimized parameter-efficient tuning approach for information retrieval that can achieve comparable or better performance to full fine-tuning while updating only a small fraction of model parameters. Specifically, the key contributions are:- They conduct a comprehensive empirical study of existing parameter-efficient tuning methods like Adapter, Prefix-tuning, BitFit, etc. for IR models. They find these methods lag behind full fine-tuning when updating less than 1% of parameters.- Through theoretical analysis, they identify the underlying reason is that the separation of trainable modules in existing methods leads to difficulty in optimization. - To address this, they propose to inject additional "aside" modules that connect the scattered trainable modules to smooth the loss surface and stabilize training.- They design 3 variants of the proposed Inside & Aside (IAA) method that combines "inside" and "aside" modules in different ways.- Experiments on both retrieval and reranking stages show their IAA method significantly outperforms existing methods. With only 6.7% trainable parameters, it achieves comparable or even better performance than full fine-tuning.In summary, the key innovation is the proposed IAA structure that enables effective and efficient tuning of IR models by smoothing optimization and retaining model capacity. The comprehensive empirical study and theoretical analysis also provide useful insights.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method called Inside and Aside (IAA) to make parameter-efficient tuning methods more effective for information retrieval. The key idea is to inject additional 'aside' modules alongside the pre-trained model to smooth the loss surface and stabilize training. In summary, IAA enables parameter-efficient tuning methods to achieve comparable or better performance to full fine-tuning while only updating a small fraction of parameters.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research in information retrieval:- This paper focuses on parameter-efficient tuning methods for fine-tuning pre-trained models like BERT. It studies both the retrieval and re-ranking stages of search. Other related work has primarily focused just on the re-ranking stage.- The paper provides a comprehensive empirical study of existing parameter-efficient tuning methods on large-scale IR benchmarks. It finds these methods underperform compared to full fine-tuning. Other recent work found promising results on small datasets.  - The paper analyzes why existing methods underperform, identifying optimization issues due to scattered trainable modules. Other work has not provided this kind of analysis.- To address the limitations, the paper proposes a novel method called IAA that adds "aside" modules to connect the scattered trainable modules. This approach is novel compared to prior work.- Experiments show IAA outperforms existing methods and competes with full fine-tuning using less parameters. Other methods have not demonstrated consistently beating full fine-tuning.- The proposed IAA method is model-agnostic and can work with different base models, tuning methods, and architectures. Other methods are more constrained or specialized.In summary, this paper provides a more comprehensive study, analysis, and novel solution for parameter-efficient tuning in IR compared to prior work. The proposed IAA method seems to advance the state-of-the-art based on the empirical results.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Domain adaptation for parameter-efficient tuning methods in IR. The authors suggest studying the ability of their proposed method and other parameter-efficient tuning approaches to adapt across domains in IR applications. This could involve testing how well these methods transfer from one dataset to another.- Improving the tuning efficiency of parameter-efficient methods. The authors note that although many existing methods are parameter-efficient, they are not always tuning-efficient in terms of training speed and computational cost. Improving the overall tuning efficiency is suggested as an important direction for future work.- Studying the choice or design of optimal inside modules. The authors used Adapter and LoRA as the inside modules in their experiments, but suggest exploring the selection or development of optimal inside module architectures as an area for future work. - Applying parameter-efficient tuning methods to other IR models. The authors focused their study on applying these methods to Transformer models, but suggest investigating their usefulness for other types of IR models as a research direction.- Developing specialized designs of parameter-efficient tuning for dense retrieval. The authors found existing methods still lagged behind state-of-the-art dense retrieval models, suggesting the need for specialized techniques to improve their performance in this area.In summary, the main future directions are centered around improving the adaptability, efficiency, architectures, and applicability of parameter-efficient tuning methods for IR based on the authors' empirical analysis and proposed approach.


## Summarize the paper in one paragraph.

The paper proposes an optimized parameter-efficient tuning approach for information retrieval. It first conducts a comprehensive study of existing parameter-efficient tuning methods like Adapter and LoRA, finding they cannot achieve comparable performance to full fine-tuning when updating less than 1% parameters on large-scale benchmarks. Through analysis, the reason is that the separation of trainable modules leads to a discrepancy between the ideal and actual optimization direction. To alleviate this, the paper proposes to inject additional "aside" modules to connect the original scattered modules, smoothing the loss surface and stabilizing training. Experiments show the proposed method significantly outperforms existing methods, achieving comparable or better performance than full fine-tuning while only tuning 6.7% parameters. Overall, the paper provides an effective parameter-efficient tuning approach for IR by carefully designing connected modules to optimize the training process.
