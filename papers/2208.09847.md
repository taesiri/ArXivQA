# [Scattered or Connected? An Optimized Parameter-efficient Tuning Approach   for Information Retrieval](https://arxiv.org/abs/2208.09847)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we design a parameter-efficient tuning approach that stabilizes the training process for information retrieval models?The authors first conduct a comprehensive study of existing parameter-efficient tuning methods like adapter and prefix-tuning for information retrieval models. They find that while these methods reduce the number of parameters needed for fine-tuning, they lag behind full fine-tuning and suffer from unstable training. To analyze why, the authors provide a theoretical analysis showing that the separation of the inserted trainable modules in existing methods causes a discrepancy between the ideal optimization direction and actual update direction during training. This makes the optimization difficult and training unstable.To address this issue, the authors propose a new method called Inside and Aside (IAA) tuning. The key idea is to inject additional "aside" modules that connect the original scattered "inside" modules to smooth the loss surface and stabilize training. Experiments show their proposed IAA method significantly outperforms existing parameter-efficient tuning approaches and achieves comparable or better performance than full fine-tuning.In summary, the central hypothesis is that connecting the scattered trainable modules can stabilize training for parameter-efficient tuning methods in information retrieval. The IAA method is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is an optimized parameter-efficient tuning approach for information retrieval that can achieve comparable or better performance to full fine-tuning while updating only a small fraction of model parameters. Specifically, the key contributions are:- They conduct a comprehensive empirical study of existing parameter-efficient tuning methods like Adapter, Prefix-tuning, BitFit, etc. for IR models. They find these methods lag behind full fine-tuning when updating less than 1% of parameters.- Through theoretical analysis, they identify the underlying reason is that the separation of trainable modules in existing methods leads to difficulty in optimization. - To address this, they propose to inject additional "aside" modules that connect the scattered trainable modules to smooth the loss surface and stabilize training.- They design 3 variants of the proposed Inside & Aside (IAA) method that combines "inside" and "aside" modules in different ways.- Experiments on both retrieval and reranking stages show their IAA method significantly outperforms existing methods. With only 6.7% trainable parameters, it achieves comparable or even better performance than full fine-tuning.In summary, the key innovation is the proposed IAA structure that enables effective and efficient tuning of IR models by smoothing optimization and retaining model capacity. The comprehensive empirical study and theoretical analysis also provide useful insights.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method called Inside and Aside (IAA) to make parameter-efficient tuning methods more effective for information retrieval. The key idea is to inject additional 'aside' modules alongside the pre-trained model to smooth the loss surface and stabilize training. In summary, IAA enables parameter-efficient tuning methods to achieve comparable or better performance to full fine-tuning while only updating a small fraction of parameters.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research in information retrieval:- This paper focuses on parameter-efficient tuning methods for fine-tuning pre-trained models like BERT. It studies both the retrieval and re-ranking stages of search. Other related work has primarily focused just on the re-ranking stage.- The paper provides a comprehensive empirical study of existing parameter-efficient tuning methods on large-scale IR benchmarks. It finds these methods underperform compared to full fine-tuning. Other recent work found promising results on small datasets.  - The paper analyzes why existing methods underperform, identifying optimization issues due to scattered trainable modules. Other work has not provided this kind of analysis.- To address the limitations, the paper proposes a novel method called IAA that adds "aside" modules to connect the scattered trainable modules. This approach is novel compared to prior work.- Experiments show IAA outperforms existing methods and competes with full fine-tuning using less parameters. Other methods have not demonstrated consistently beating full fine-tuning.- The proposed IAA method is model-agnostic and can work with different base models, tuning methods, and architectures. Other methods are more constrained or specialized.In summary, this paper provides a more comprehensive study, analysis, and novel solution for parameter-efficient tuning in IR compared to prior work. The proposed IAA method seems to advance the state-of-the-art based on the empirical results.
