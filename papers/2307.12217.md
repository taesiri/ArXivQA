# [LoLep: Single-View View Synthesis with Locally-Learned Planes and   Self-Attention Occlusion Inference](https://arxiv.org/abs/2307.12217)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we perform high-quality single-view view synthesis using multiplane image representations without reliance on additional depth inputs?The key ideas and contributions in addressing this question appear to be:- Proposing a method to regress locally-learned plane locations from an RGB image alone, avoiding issues with global plane learning approaches. This is done via a disparity sampler module.- Introducing optimization strategies to make the disparity sampler work well with different dataset disparity distributions.- Using an occlusion-aware reprojection loss as a geometric supervision signal. - Presenting a Block-Sampling Self-Attention module to enable the use of self-attention on large feature maps for occlusion handling.- Demonstrating state-of-the-art performance on multiple datasets compared to prior MPI-based view synthesis techniques, while using fewer planes. Qualitative results also showcase improved handling of occlusions and geometry.So in summary, the central hypothesis seems to be that locally-learned plane representations predicted from only RGB can surpass prior MPI approaches relying on depth inputs or random/global plane sampling for single-view novel view synthesis. The method and experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel single-view view synthesis method called LoLep that uses locally-learned planes to represent scenes accurately and generate better novel views. - Introducing a disparity sampler that regresses offsets to learn plane locations locally, along with parameter optimization strategies for different datasets.- Exploring an occlusion-aware reprojection loss as a geometric supervision technique.- Presenting a Block-Sampling Self-Attention (BS-SA) module to apply self-attention to large feature maps for occlusion handling.- Demonstrating state-of-the-art performance on different datasets compared to prior methods like MINE, with improvements in metrics like LPIPS and rendering variance. The method generates sharper and more realistic novel views.- Showing that LoLep with fewer planes can outperform prior methods with more planes, using less memory.In summary, the main contribution seems to be proposing the LoLep method for more accurate and efficient single-view novel view synthesis through techniques like locally-learned planes, occlusion-aware supervision, and self-attention. Both quantitative and qualitative results demonstrate improved performance over prior work.
