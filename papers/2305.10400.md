# [What You See is What You Read? Improving Text-Image Alignment Evaluation](https://arxiv.org/abs/2305.10400)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we automatically evaluate the semantic alignment between images and text for both text-to-image and image-to-text tasks?The key points I gathered are:- Determining semantic alignment between images and text is an important challenge for both evaluating and improving text-to-image and image-to-text generative models. However, existing methods have limitations.- The paper introduces a new benchmark called SeeTRUE for evaluating image-text alignment methods. It contains diverse real and synthetic image-text pairs with human judgments of alignment.- The paper proposes two main methods for automatic alignment evaluation: 1) VQ^2 - Uses question generation and visual question answering to assess if text details are accurately represented in the image.2) VNLI - Fine-tunes large pretrained multimodal models to directly predict alignment in an end-to-end manner.- Experiments show these methods outperform strong baselines like CLIP and BLIP on the SeeTRUE benchmark, especially on compositional and synthetic image challenges.- The methods can also identify specific misalignments and improve ranking of text-to-image generation candidates.In summary, the main research contribution seems to be new methods and benchmarks for evaluating and improving semantic text-image alignment in multimodal tasks. The key hypothesis is that techniques like VQ^2 and VNLI will better assess alignment compared to prior approaches.
