# [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we enhance the problem-solving abilities of large language models beyond their default token-level, left-to-right generation process, to enable more deliberate and systematic reasoning through exploring multiple paths?

The key hypothesis is that framing problem solving as search through a tree of possible reasoning paths ("thoughts"), along with mechanisms for the model to deliberately evaluate and choose between alternative thoughts, will allow language models to tackle more complex problems requiring planning, lookahead, and backtracking. 

The paper introduces the Tree of Thoughts (ToT) framework as a way to implement this kind of deliberate search process purely through language interactions with the model, without extra training or specialized architectures. The framework is evaluated on three novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that aim to stretch the limits of standard language model capabilities.

In summary, the central hypothesis is that incorporating more human-like deliberative reasoning into language models, instantiated through the Tree of Thoughts framework, will expand their problem-solving abilities compared to the standard left-to-right text generation process. The paper aims to demonstrate this via the design, implementation, and evaluation of ToT across challenging reasoning tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing the Tree of Thoughts (ToT) framework, which enables large language models (LLMs) like GPT-3/GPT-4 to perform more systematic and deliberate problem solving through exploring and evaluating multiple coherent reasoning paths (thoughts). Specifically:

- ToT generalizes over existing prompting approaches like input-output prompting and chain-of-thought prompting, and allows LLMs to actively search over a tree of thoughts using different search algorithms like BFS and DFS. 

- It proposes ways for the LLM to deliberately generate, evaluate, and compare thoughts at each step through self-prompting, which provides heuristics to guide the search without extra training.

- It shows ToT significantly improves LLM performance on 3 novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that require non-trivial search and planning. For instance, ToT helps solve 74% of Game of 24 tasks, whereas standard prompting only solves around 4%.

- It provides a modular framework to incorporate search and planning capabilities into LLMs, and shows the flexibility of adapting different search algorithms, heuristics, etc. based on the task properties.

In summary, the key innovation is enabling LLMs to deliberately explore and evaluate multiple reasoning paths in a systematic way, instead of just left-to-right token generation. This brings LLMs closer to human-like problem solving involving search and planning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper introduces a Tree of Thoughts framework that enables language models to deliberate over problem solving by exploring and evaluating coherent intermediate reasoning steps represented as language.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research on using large language models for problem solving:

- It proposes a more structured and deliberate approach (Tree of Thoughts) compared to standard approaches like input-output prompting or chain of thought, allowing the model to explore multiple reasoning paths. This is inspired by classical AI methods for search and planning. 

- The tasks explored (Game of 24, Creative Writing, Crosswords) require non-trivial reasoning and search abilities. Many existing works focus on more straightforward tasks or do not systematically test search/planning abilities.

- The framework is flexible and model-agnostic. It does not require specialized training of models, just a pre-trained language model like GPT-4. Some other works propose custom models or training methods.

- It emphasizes model self-evaluation of reasoning steps rather than relying on external signals. Related works on "self-reflection" also aim to make models assess their own generations.

- The intermediate reasoning is instantiated in natural language "thoughts", not more structured representations like programs. This provides convenience but could be limiting in representing complex reasoning.

- The search algorithms explored are relatively simple (BFS, DFS). More advanced algorithms like Monte Carlo Tree Search could be investigated.

- Performance gains on the proposed tasks are significant (e.g. 4% to 74% on Game of 24). Many other works demonstrate smaller gains or do not systematically compare to baselines.

Overall, this work makes a novel connection between classical search methods and modern language models. The Tree of Thoughts framework seems promising in making language models more deliberate, structured, and lookahead in their reasoning while retaining convenience and generality. Key limitations are the simple search methods and choice of tasks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Exploring more complex real-world tasks where deliberate search and planning with LMs could be beneficial, beyond the relatively simple proof-of-concept tasks in the paper. The authors mention potential applications like coding, data analysis, and robotics.

- Studying the tradeoffs between performance gains and computational costs of methods like ToT, and developing techniques to improve this, such as through open-source efforts to reduce API costs of large LMs.

- Training LMs in a way that is tailored for high-level, counterfactual deliberative reasoning as enabled by ToT, rather than just left-to-right token prediction. This could further enhance the problem-solving capabilities unlocked by ToT.

- Incorporating external knowledge retrieval and other auxillary skills to augment LMs when solving problems that require reasoning about uncertain or incomplete knowledge.

- Developing more sophisticated search algorithms like A* search and Monte Carlo tree search that could further improve systematic exploration and exploitation in ToT.

- Exploring alternative prompting strategies and training objectives to improve the quality of LM-generated heuristics for deliberative search, which were shown to be imperfect in tasks like the crossword experiments.

- Studying interpretability and human alignment when applying deliberative search methods like ToT to tasks involving interaction with humans or environments, to mitigate potential risks.

Overall, the authors lay out an exciting research agenda at the intersection of classical AI methods like heuristic search and modern large language models. Key directions include scaling up the applications, improving training and prompting strategies, and developing hybrid methods that combine symbolic search with neural language models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces a new framework called Tree of Thoughts (ToT) for using large language models (LLMs) like GPT to deliberately solve problems that require planning and search. ToT represents the problem solving process as exploring a tree where each node is a "thought" or coherent language sequence. The LLM can generate multiple thoughts at each step, evaluate their promise for solving the problem, and use search algorithms like breadth-first or depth-first search to systematically explore the tree. This allows the LLM to look ahead, backtrack, and make more global choices compared to just left-to-right decoding. The authors show ToT significantly improves GPT-4's performance on 3 novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that require non-trivial reasoning and search. For instance, while GPT-4 only solves 4% of Game of 24 problems with existing prompting methods, ToT achieves 74% success. The framework is general, flexible, and adaptable to different problems, search spaces, and resource constraints.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new framework called "Tree of Thoughts" (ToT) for enabling language models to perform more deliberate and systematic problem solving. The key idea is to represent the problem solving process as searching through a tree, where each node is a "thought" representing an intermediate reasoning step. This allows the model to explore multiple paths toward a solution, evaluate the progress of different thoughts, and incorporate search algorithms like breadth-first search or depth-first search. 

The authors propose and experiment with ToT on three novel tasks - Game of 24, Creative Writing, and Mini Crosswords - that require non-trivial search and planning abilities. Across tasks, ToT outperforms standard prompting techniques like input-output prompting and chain-of-thought prompting, by enabling the language model GPT-4 to actively generate, evaluate, and search over possible thoughts. For instance, while GPT-4 only solves 4% of Game of 24 tasks with chain-of-thought prompting, ToT achieves 74% success. The results highlight the capability of ToT to enhance language models' problem-solving skills in complex tasks requiring exploration, lookahead, and backtracking.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper "Tree of Thoughts: Deliberate Problem Solving with Large Language Models":

The paper proposes a new framework called "Tree of Thoughts" (ToT) that enables large language models (LLMs) like GPT to deliberately explore multiple reasoning paths when solving problems, through maintaining and searching over a tree where each node represents an intermediate "thought" or coherent language sequence. ToT allows the LLM to evaluate and compare alternative thoughts at each step by reasoning about them in natural language, and perform lookahead, backtracking, and other search algorithms to navigate the tree. This provides a more systematic, interpretable approach to problem solving compared to standard left-to-right decoding used by LLMs. The authors show ToT significantly improves performance on mathematical, creative writing, and linguistic reasoning tasks that require search, compared to regular prompting of GPT-4. The framework is general and modular, supporting different thought granularities, generators, evaluators, and search methods.


## What problem or question is the paper addressing?

 The paper is addressing the limitations of current methods for using large language models (LLMs) like GPT for general problem solving. It points out two main limitations:

1. Existing methods like input-output prompting and chain-of-thought only make local, left-to-right decisions when generating text. They do not explore different possible paths or look ahead to make more global choices. 

2. Existing methods do not incorporate deliberative planning and search processes that seem characteristic of human problem solving. They lack mechanisms for actively maintaining and evaluating different reasoning paths.

To address these limitations, the paper introduces a new framework called "Tree of Thoughts" (ToT) that allows LLMs to explore multiple coherent chains of reasoning represented as branches in a tree. It also enables the LLM to deliberately evaluate the progress different branches make toward solving the overall problem. This allows more strategic search, backtracking, and global decision making.

In summary, the key problem the paper aims to address is enhancing LLMs with more human-like planning and search abilities for general problem solving, beyond just left-to-right text generation. The Tree of Thoughts framework is proposed as a way to bring in classical AI concepts of search trees and heuristic evaluation to augment current LM methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Tree of Thoughts (ToT): The main framework proposed in the paper for deliberate problem solving with language models. It involves exploring multiple coherent chains of reasoning ("thoughts") represented as language.

- Thoughts: Coherent units of language that serve as intermediate steps in problem solving within the ToT framework. The paper explores thoughts at different granularities like equations, text plans, or words.

- Deliberate reasoning: The paper argues language models need to go beyond fast, associative reasoning ("System 1") and also incorporate more deliberate planning and lookahead ("System 2"). ToT aims to enable this.

- Problem solving search: ToT is inspired by classical search methods like breadth-first, depth-first search. It frames problem solving as search over a tree of thoughts. 

- Self-evaluation: A key aspect of ToT is having the language model self-evaluate the progress different thoughts make through explicit reasoning, providing heuristics to guide the search.

- Exploration vs. exploitation: ToT allows language models to explore multiple reasoning paths at each step while evaluating them, balancing exploration and exploitation.

- Backtracking: ToT incorporates backtracking to previous decision points, enabling global lookahead and recovery from local mistakes.

- Modularity: The paper emphasizes ToT is highly modular and flexibly incorporates different search algorithms, thought generation strategies, and evaluation methods.

- Game of 24, Creative Writing, Crosswords: Three novel tasks proposed that require planning and deliberation, and where ToT shows strong improvements over standard prompting.
