# [Delivering Arbitrary-Modal Semantic Segmentation](https://arxiv.org/abs/2303.01480)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve robust and accurate semantic segmentation with multiple modalities in adverse weather conditions and partial sensor failure scenarios. 

Specifically, the paper focuses on two key problems:

1. How to fuse an arbitrary number of modalities to maximize segmentation accuracy. The paper hypothesizes that using more modalities will monotonically increase accuracy due to more diverse complementary information.

2. How to make semantic segmentation robust to individual sensor failures by fusing multiple modalities. The paper hypothesizes that cooperation of multiple sensors can effectively resolve partial sensor outages.

To investigate these problems, the paper presents:

- The DeLiVER benchmark with 4 modalities (RGB, depth, event, LiDAR), 4 adverse weather conditions (cloudy, foggy, night, rainy), and 5 sensor failure modes.

- The CMNeXt segmentation model with a Hub2Fuse paradigm using asymmetric branches and two key components:
   - Self-Query Hub (SQ-Hub) to select informative features from modalities
   - Parallel Pooling Mixer (PPX) to efficiently fuse modalities

Through experiments on 6 datasets with up to 81 modalities, the paper shows CMNeXt achieves state-of-the-art accuracy and can overcome individual sensor failures via multimodal fusion. The results support the hypotheses that using more modalities improves accuracy, and fusing multiple modalities increases robustness.

In summary, the central research question is how to achieve robust and accurate semantic segmentation by fusing an arbitrary number of modalities, even in the presence of sensor failures. The proposed DeLiVER benchmark and CMNeXt model aim to address this question.
