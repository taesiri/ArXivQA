# [Delivering Arbitrary-Modal Semantic Segmentation](https://arxiv.org/abs/2303.01480)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve robust and accurate semantic segmentation with multiple modalities in adverse weather conditions and partial sensor failure scenarios. 

Specifically, the paper focuses on two key problems:

1. How to fuse an arbitrary number of modalities to maximize segmentation accuracy. The paper hypothesizes that using more modalities will monotonically increase accuracy due to more diverse complementary information.

2. How to make semantic segmentation robust to individual sensor failures by fusing multiple modalities. The paper hypothesizes that cooperation of multiple sensors can effectively resolve partial sensor outages.

To investigate these problems, the paper presents:

- The DeLiVER benchmark with 4 modalities (RGB, depth, event, LiDAR), 4 adverse weather conditions (cloudy, foggy, night, rainy), and 5 sensor failure modes.

- The CMNeXt segmentation model with a Hub2Fuse paradigm using asymmetric branches and two key components:
   - Self-Query Hub (SQ-Hub) to select informative features from modalities
   - Parallel Pooling Mixer (PPX) to efficiently fuse modalities

Through experiments on 6 datasets with up to 81 modalities, the paper shows CMNeXt achieves state-of-the-art accuracy and can overcome individual sensor failures via multimodal fusion. The results support the hypotheses that using more modalities improves accuracy, and fusing multiple modalities increases robustness.

In summary, the central research question is how to achieve robust and accurate semantic segmentation by fusing an arbitrary number of modalities, even in the presence of sensor failures. The proposed DeLiVER benchmark and CMNeXt model aim to address this question.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

- The creation of a new multimodal dataset called DeLiVER for arbitrary-modal semantic segmentation. The dataset contains depth, LiDAR, event camera, and RGB data and includes different weather conditions and sensor failure modes. 

- The proposal of a new fusion model called CMNeXt for arbitrary-modal semantic segmentation. The model uses an asymmetric architecture with two branches - one for RGB and one for other modalities. It incorporates a Self-Query Hub to select informative features from auxiliary modalities and a Parallel Pooling Mixer to efficiently fuse features.

- Extensive experiments showing state-of-the-art performance of CMNeXt on the new DeLiVER dataset as well as other datasets when fusing up to 81 modalities. The model is shown to be robust to different weather conditions and sensor failures.

In summary, the main contributions are a new multimodal dataset, a new fusion model for arbitrary-modal segmentation, and experimental results demonstrating the effectiveness of the model on multiple datasets with varying numbers of modalities. The key ideas are using an asymmetric architecture to fuse features from arbitrary modalities and mechanisms to select informative features and fuse them efficiently.
