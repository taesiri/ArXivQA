# [Edge Wasserstein Distance Loss for Oriented Object Detection](https://arxiv.org/abs/2312.07048)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel regression loss called Edge Wasserstein Distance (EWD) loss for oriented object detection. EWD loss represents the oriented bounding box (OBox) as a distribution with probability density only on the edges, then measures the Wasserstein distance between OBox distributions to overcome issues like metric discontinuity and square-like boxes. Through assumptions to simplify calculation, EWD loss is derived for OBoxes as a generalized L2 distance on box parameters. Experiments show EWD loss helps detect oriented objects like vehicles and text better than losses like smooth L1, IoU, and KL divergence, since it avoids degenerate cases and is more sensitive to aspect ratio changes. A key advantage is EWD loss generalization - it applies to quadratic and polynomial regression for arbitrary shapes beyond OBoxes. The consistent improvements demonstrate EWD loss effectively handles orientation ambiguity and discontinuities that hinder precise localization of oriented objects.
