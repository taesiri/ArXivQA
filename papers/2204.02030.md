# $\textit{latent}$-GLAT: Glancing at Latent Variables for Parallel Text   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we train a non-autoregressive text generation model directly on raw training data, without relying on an autoregressive model for knowledge distillation?The key points are:- Non-autoregressive text generation models like NAT and GLAT suffer from a multi-modality problem, where a given input may map to multiple possible outputs in the training data. This makes it challenging to train the models to produce consistent outputs. - Existing solutions like GLAT rely on knowledge distillation from an autoregressive model to filter the training data and alleviate the multi-modality issue. However, this requires training an extra autoregressive model, increasing training costs.- This paper proposes a new method called "latent-GLAT" which introduces discrete latent variables to capture categorical word information. The intuition is that these latent variables will have fewer modes than raw words, making them easier to model directly using glancing training without distillation.- Latent-GLAT models the latent variables non-autoregressively using glancing training, then uses them to guide generation of the full text output. This encourages building dependencies on the robust latent variables rather than raw words.- Experiments on machine translation, paraphrasing, and dialog tasks show latent-GLAT improves over NAT and GLAT baselines without distillation, achieving comparable or better performance than autoregressive models.In summary, the key hypothesis is that introducing latent variables and glancing training over them can allow non-autoregressive text generation without relying on knowledge distillation, enabling more efficient training. The results support this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is the proposal of latent-GLAT, a non-autoregressive text generation model that can be trained directly on raw datasets without relying on an autoregressive teacher model. The key ideas are:- Introducing discrete latent variables to capture word categorical information. These latent variables have fewer modes than raw words, which helps alleviate the multi-modality issue in text generation.- Extending glancing training to the latent variables. This allows building dependencies between latent variables rather than raw words, which is more robust as latent variables can be obtained during inference. - Modeling latent variables prediction and sentence reconstruction in a non-autoregressive fashion with glancing training. This avoids reliance on an autoregressive model while still capturing dependencies.- Experiments on machine translation, paraphrasing and dialog tasks show latent-GLAT outperforms strong baselines without distillation, and even exceeds autoregressive models on some tasks.In summary, the use of latent variables and glancing training allows direct training on raw datasets, broadening the application of non-autoregressive models. The proposed latent-GLAT demonstrates the viability of this approach across diverse text generation tasks.
