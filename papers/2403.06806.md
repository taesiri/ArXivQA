# [On the Global Convergence of Policy Gradient in Average Reward Markov   Decision Processes](https://arxiv.org/abs/2403.06806)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper focuses on analyzing the global convergence guarantees of policy gradient methods in infinite horizon average reward Markov decision processes (MDPs). 
- Prior work has extensively studied policy gradients for discounted reward MDPs, but analysis for average reward problems is lacking. 
- Extending existing discounted reward analysis to average reward MDPs is challenging due to the lack of discount factor as a source of contraction and the non-uniqueness of the value function.

Proposed Solution:
- The paper proposes a novel analysis technique to establish the smoothness of the average reward function, which is then utilized to prove the convergence of policy gradients.  
- This involves projecting the value function onto the subspace orthogonal to the all ones vector to obtain a unique representation. 
- Using this, the paper shows that the average reward is smooth and derives problem-dependent smoothness constants that capture the complexity of the MDP.

Main Contributions:
- First finite-time analysis showing policy gradient iterates converge at a sublinear rate of O(1/T) for average reward MDPs, implying an O(log T) regret.
- Introduces new MDP-specific constants that explicitly capture the role of the MDP complexity in the convergence rate.
- Shows that the analysis technique can be extended to discounted reward MDPs to improve dependence on MDP complexity compared to state-of-the-art bounds.
- Validates improved dependence on MDP complexity through simulations of policy gradient performance.

In summary, the key contribution is a novel analysis framework that not only proves convergence guarantees for policy gradients in the average reward setting, but also provides improved understanding of how the complexity of the underlying MDP affects the convergence rate.
