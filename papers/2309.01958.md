# [Empowering Low-Light Image Enhancer through Customized Learnable Priors](https://arxiv.org/abs/2309.01958)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: "Can we customize learnable priors for illumination and noise terms that leverage their intrinsic properties to improve the transparency and interpretability of deep unfolding for low-light image enhancement?"The key points are:- Existing deep unfolding methods for low-light image enhancement use proximal operator networks to impose priors on the illumination and noise terms. However, these are designed in an ambiguous black-box manner. - The authors propose to explore using customized learnable priors based on the intrinsic properties of illumination (should be smooth and preserve structure) and noise (irrelevant to enhanced lightness).- They utilize Masked Autoencoders (MAE) to pre-train models to capture these intrinsic priors. The illumination prior is trained to reconstruct an illumination map filtered with a bilateral filter. The noise prior is trained to reconstruct HOG features that represent gradients.- These learned priors are integrated in two ways: 1) The illumination prior is embedded into the proximal operator for decomposition. 2) The noise prior is used as a regularization term to enforce gradient consistency.- Experiments show their method outperforms prior deep unfolding and other state-of-the-art methods, demonstrating the benefits of customized learnable priors.In summary, the key hypothesis is that using customized learnable priors based on intrinsic properties can improve deep unfolding for low-light enhancement in terms of performance and interpretability.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new deep unfolding paradigm for low-light image enhancement that explores the potential of customized learnable priors to improve the transparency and interpretability. Specifically:- It proposes to use Masked Autoencoders (MAE) to learn customized illumination and noise priors for low-light images in a pre-training stage. - The learned illumination prior is embedded into the proximal operator design in the unfolding architecture to improve its transparency.- The learned noise prior is redeveloped as a regularization term in the loss function to constrain gradient consistency and suppress noise.- Experiments show the proposed method outperforms previous state-of-the-art methods on benchmark datasets. The customized priors also demonstrate effectiveness.In summary, the key novelty is utilizing pre-trained MAE-based customized priors, from both the model architecture design and optimization perspectives, to enhance the transparency and performance of deep unfolding for low-light image enhancement. This explores the potential of learnable priors in improving deep unfolding solutions.
