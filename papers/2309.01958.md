# [Empowering Low-Light Image Enhancer through Customized Learnable Priors](https://arxiv.org/abs/2309.01958)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: "Can we customize learnable priors for illumination and noise terms that leverage their intrinsic properties to improve the transparency and interpretability of deep unfolding for low-light image enhancement?"The key points are:- Existing deep unfolding methods for low-light image enhancement use proximal operator networks to impose priors on the illumination and noise terms. However, these are designed in an ambiguous black-box manner. - The authors propose to explore using customized learnable priors based on the intrinsic properties of illumination (should be smooth and preserve structure) and noise (irrelevant to enhanced lightness).- They utilize Masked Autoencoders (MAE) to pre-train models to capture these intrinsic priors. The illumination prior is trained to reconstruct an illumination map filtered with a bilateral filter. The noise prior is trained to reconstruct HOG features that represent gradients.- These learned priors are integrated in two ways: 1) The illumination prior is embedded into the proximal operator for decomposition. 2) The noise prior is used as a regularization term to enforce gradient consistency.- Experiments show their method outperforms prior deep unfolding and other state-of-the-art methods, demonstrating the benefits of customized learnable priors.In summary, the key hypothesis is that using customized learnable priors based on intrinsic properties can improve deep unfolding for low-light enhancement in terms of performance and interpretability.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new deep unfolding paradigm for low-light image enhancement that explores the potential of customized learnable priors to improve the transparency and interpretability. Specifically:- It proposes to use Masked Autoencoders (MAE) to learn customized illumination and noise priors for low-light images in a pre-training stage. - The learned illumination prior is embedded into the proximal operator design in the unfolding architecture to improve its transparency.- The learned noise prior is redeveloped as a regularization term in the loss function to constrain gradient consistency and suppress noise.- Experiments show the proposed method outperforms previous state-of-the-art methods on benchmark datasets. The customized priors also demonstrate effectiveness.In summary, the key novelty is utilizing pre-trained MAE-based customized priors, from both the model architecture design and optimization perspectives, to enhance the transparency and performance of deep unfolding for low-light image enhancement. This explores the potential of learnable priors in improving deep unfolding solutions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new deep unfolding paradigm for low-light image enhancement that explores customized learnable priors for illumination and noise modeled by masked autoencoders to improve transparency and incorporate intrinsic image properties.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in the field of low-light image enhancement:- This paper proposes a new deep unfolding paradigm that focuses on exploring customized learnable priors, unlike most existing deep learning methods that use black box networks without considering intrinsic image priors. The use of learnable priors improves model transparency and interpretability.- The authors utilize a masked autoencoder (MAE) framework to pretrain illumination and noise priors based on specific target features relevant to low-light images. This is a novel approach not explored in prior work.- The learned priors are integrated in two ways - through the unfolding architecture design (structure flow) and as a loss function regularizer (optimization flow). This bidirectional integration of priors is unique.- Experiments show the method outperforms recent state-of-the-art approaches on multiple datasets. The customized priors demonstrate improved performance over standard proximal operators.- In addition to low-light enhancement, the experiments also validate the efficacy of the learned noise prior for general image denoising tasks by integrating it with existing denoising models. This extends the applicability of the proposed approach.Overall, the key novelty is the customized learnable priors via MAE and their integration to boost performance. This provides a new direction for interpretable deep unfolding models compared to black box learning models prevalent in this field. The impressive results substantiate the benefits of this paradigm.
