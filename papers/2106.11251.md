# [Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval](https://arxiv.org/abs/2106.11251)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a multiple representation dense retrieval approach be enhanced by pseudo-relevance feedback?The key hypothesis is that identifying more related embeddings from the top-ranked documents retrieved in the first pass can help refine the document ranking in a multiple representation dense retrieval system like ColBERT. The authors propose a novel contextualized pseudo-relevance feedback method called ColBERT-PRF to test this hypothesis.The main contributions are:1) Proposing ColBERT-PRF, the first pseudo-relevance feedback mechanism for dense retrieval.2) Clustering and ranking feedback document embeddings to select candidate expansion embeddings.3) Evaluating ColBERT-PRF in both ranking and reranking settings and showing significant improvements over ColBERT end-to-end dense retrieval.In summary, the paper introduces and evaluates a new technique for applying pseudo-relevance feedback to enhance multiple representation dense retrieval systems. The central hypothesis is that adding discriminative and representative embeddings from feedback documents can improve ranking accuracy.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a novel pseudo-relevance feedback mechanism called ColBERT-PRF for dense retrieval using multiple representations. This is the first work to apply pseudo-relevance feedback to dense retrieval.2. Extracting representative feedback embeddings from pseudo-relevant documents using KMeans clustering and selecting the most discriminative ones based on IDF. These discriminative embeddings are then appended to the query representation.3. Evaluating ColBERT-PRF in both ranking and reranking scenarios on the TREC 2019 and 2020 Deep Learning track passage ranking datasets. The results show significant improvements over strong baselines like ColBERT and BERT-QE, demonstrating the effectiveness of the proposed approach.4. Investigating the impact of various parameters like number of clusters, number of expansion embeddings, size of feedback set, and weight of expansion embeddings. This provides insights into optimally configuring the PRF mechanism.5. The proposed ColBERT-PRF achieves state-of-the-art effectiveness without requiring any retraining of the baseline ColBERT model. It is also the first PRF method that can improve recall by re-executing the expanded query on the dense index.In summary, the main contribution is proposing a novel and effective PRF mechanism specifically designed for dense retrieval using multiple representations. The experiments demonstrate significant improvements over competitive baselines, proving its usefulness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel pseudo-relevance feedback approach called ColBERT-PRF that enhances dense retrieval in multiple representation settings by extracting representative feedback embeddings using clustering, identifying discriminative ones based on IDF, and appending them to the original query representation to improve retrieval effectiveness without requiring retraining of the ColBERT model.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in pseudo-relevance feedback for dense retrieval:- This paper proposes ColBERT-PRF, a novel method for applying pseudo-relevance feedback to dense retrieval models like ColBERT. It is the first work to investigate PRF for dense retrieval, whereas prior PRF research focused on traditional sparse retrieval and neural reranking.- ColBERT-PRF operates entirely within the dense embedding space, clustering feedback embeddings and appending the most discriminative ones to the query. This avoids issues like topic drift that can occur when mapping embeddings back to words.- Experiments show ColBERT-PRF significantly improves the retrieval effectiveness of ColBERT on TREC Deep Learning track data. It outperforms strong baselines like RM3, doc2query, ANCE, and BERT-QE reranking.- The paper introduces innovations like using KMeans clustering and IDF weighting to identify useful feedback embeddings in a dense setting. This is a new perspective compared to prior statistical or neural network approaches to PRF.- ColBERT-PRF is efficient in that it requires no additional training beyond the base ColBERT model. Some neural PRF techniques like BERT-QE have very high computational overhead.- There are still opportunities to build on this work, like replacing KMeans with more efficient clustering methods or exploring long-document collections. But overall, this paper presents a novel and promising direction for PRF in dense retrieval.In summary, this paper breaks new ground by successfully adapting PRF, which has a long history in sparse retrieval, to modern dense representations like ColBERT. The results demonstrate the effectiveness of this new technique compared to existing methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Extend the approach to test collections with longer documents. The experiments in the paper were conducted on a passage ranking dataset (MS MARCO). The authors suggest verifying the effectiveness of their approach on datasets with full-length documents.- Explore variations of the clustering algorithm used. The proposed ColBERT-PRF approach uses KMeans clustering to identify representative embeddings from the feedback documents. The authors suggest exploring more efficient clustering algorithms, either per-query online clustering or offline clustering based on the existing dense index.- Replace the token-level IDF calculation for identifying discriminative embeddings. Currently, ColBERT-PRF maps embeddings back to tokens to calculate IDF weights and identify important expansion embeddings. The authors suggest researching alternative ways to measure embedding informativeness directly in the embedding space, without mapping back to tokens.- Investigate the efficiency of the approach further. While ColBERT-PRF shows significant effectiveness improvements, the authors note its response time is higher than some baselines. They suggest future work could focus on optimizing the efficiency, for example by more conservative second-pass retrieval settings.- Adapt the approach to other multiple representation dense retrieval models beyond ColBERT. The authors propose ColBERT-PRF specifically for the ColBERT dense retriever, but suggest it may be adaptable to other similar dense retrieval methods.- Explore variations of the feedback embedding integration. The paper proposed one way of integrating the expansion embeddings into the query representation and scoring. The authors could investigate alternate ways of leveraging the feedback embeddings.In summary, the main future directions are: applying it to longer documents, finding more efficient variations of the approach, adapting it to other dense retrievers, and exploring alternate ways to leverage the feedback embeddings.
