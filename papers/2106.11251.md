# [Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval](https://arxiv.org/abs/2106.11251)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a multiple representation dense retrieval approach be enhanced by pseudo-relevance feedback?The key hypothesis is that identifying more related embeddings from the top-ranked documents retrieved in the first pass can help refine the document ranking in a multiple representation dense retrieval system like ColBERT. The authors propose a novel contextualized pseudo-relevance feedback method called ColBERT-PRF to test this hypothesis.The main contributions are:1) Proposing ColBERT-PRF, the first pseudo-relevance feedback mechanism for dense retrieval.2) Clustering and ranking feedback document embeddings to select candidate expansion embeddings.3) Evaluating ColBERT-PRF in both ranking and reranking settings and showing significant improvements over ColBERT end-to-end dense retrieval.In summary, the paper introduces and evaluates a new technique for applying pseudo-relevance feedback to enhance multiple representation dense retrieval systems. The central hypothesis is that adding discriminative and representative embeddings from feedback documents can improve ranking accuracy.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a novel pseudo-relevance feedback mechanism called ColBERT-PRF for dense retrieval using multiple representations. This is the first work to apply pseudo-relevance feedback to dense retrieval.2. Extracting representative feedback embeddings from pseudo-relevant documents using KMeans clustering and selecting the most discriminative ones based on IDF. These discriminative embeddings are then appended to the query representation.3. Evaluating ColBERT-PRF in both ranking and reranking scenarios on the TREC 2019 and 2020 Deep Learning track passage ranking datasets. The results show significant improvements over strong baselines like ColBERT and BERT-QE, demonstrating the effectiveness of the proposed approach.4. Investigating the impact of various parameters like number of clusters, number of expansion embeddings, size of feedback set, and weight of expansion embeddings. This provides insights into optimally configuring the PRF mechanism.5. The proposed ColBERT-PRF achieves state-of-the-art effectiveness without requiring any retraining of the baseline ColBERT model. It is also the first PRF method that can improve recall by re-executing the expanded query on the dense index.In summary, the main contribution is proposing a novel and effective PRF mechanism specifically designed for dense retrieval using multiple representations. The experiments demonstrate significant improvements over competitive baselines, proving its usefulness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel pseudo-relevance feedback approach called ColBERT-PRF that enhances dense retrieval in multiple representation settings by extracting representative feedback embeddings using clustering, identifying discriminative ones based on IDF, and appending them to the original query representation to improve retrieval effectiveness without requiring retraining of the ColBERT model.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in pseudo-relevance feedback for dense retrieval:- This paper proposes ColBERT-PRF, a novel method for applying pseudo-relevance feedback to dense retrieval models like ColBERT. It is the first work to investigate PRF for dense retrieval, whereas prior PRF research focused on traditional sparse retrieval and neural reranking.- ColBERT-PRF operates entirely within the dense embedding space, clustering feedback embeddings and appending the most discriminative ones to the query. This avoids issues like topic drift that can occur when mapping embeddings back to words.- Experiments show ColBERT-PRF significantly improves the retrieval effectiveness of ColBERT on TREC Deep Learning track data. It outperforms strong baselines like RM3, doc2query, ANCE, and BERT-QE reranking.- The paper introduces innovations like using KMeans clustering and IDF weighting to identify useful feedback embeddings in a dense setting. This is a new perspective compared to prior statistical or neural network approaches to PRF.- ColBERT-PRF is efficient in that it requires no additional training beyond the base ColBERT model. Some neural PRF techniques like BERT-QE have very high computational overhead.- There are still opportunities to build on this work, like replacing KMeans with more efficient clustering methods or exploring long-document collections. But overall, this paper presents a novel and promising direction for PRF in dense retrieval.In summary, this paper breaks new ground by successfully adapting PRF, which has a long history in sparse retrieval, to modern dense representations like ColBERT. The results demonstrate the effectiveness of this new technique compared to existing methods.
