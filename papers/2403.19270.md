# [sDPO: Don't Use Your Data All at Once](https://arxiv.org/abs/2403.19270)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) need to be aligned with human preferences for safety and usefulness. Common alignment techniques like reinforcement learning are complex. 
- Direct preference optimization (DPO) is simpler but requires a reference model to calculate log probabilities. Using the base LLM itself as reference is weak. Using external models has issues with availability and control.

Proposed Solution:
- Stepwise DPO (sDPO): Divide available preference datasets and use them to train the LLM in steps rather than all at once.  
- In each step, use the aligned LLM from the previous step as the reference model rather than the base LLM. This gives a better lower bound for optimization.

Key Contributions:
- Demonstrate empirically that using more aligned reference models is beneficial for final performance of alignment tuning.
- Propose sDPO method to use preference datasets stepwise, with each step using the aligned LLM from prior step as reference model.
- Show sDPO outperforms regular DPO and beats larger LLMs, highlighting value of effective alignment tuning.
- Discuss potential issues with directly adopting external aligned LLMs as references. sDPO avoids this.
- Analyze working of sDPO via log probability ratios and loss curves. Show curriculum aspect and stable training.

In summary, the paper introduces sDPO which divides preference data for stepwise alignment tuning of LLMs, using prior step's aligned LLM as reference. This simplistic technique is shown to enhance alignment and outperform other popular methods.
