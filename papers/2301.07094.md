# [Learning Customized Visual Models with Retrieval-Augmented Knowledge](https://arxiv.org/abs/2301.07094)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central hypothesis of this paper is that leveraging relevant external knowledge retrieved from a large-scale image-text corpus can help customize pre-trained vision-language models and improve their performance on downstream tasks. 

Specifically, the authors propose a framework called REACT (Retrieval-Augmented Customization) that retrieves the most relevant image-text pairs from a large database using the task instructions (e.g. class names) as queries. It then customizes the pre-trained model by training additional modules on this retrieved data while freezing the original weights. 

The key idea is that instead of solely relying on the pre-trained model's internal knowledge, providing it with relevant external knowledge can help it better adapt to new downstream tasks. The paper aims to demonstrate the effectiveness of this retrieval-augmented knowledge and the proposed customization framework.

In summary, the central hypothesis is that model customization with relevant retrieved knowledge can improve vision-language models' transfer performance on downstream tasks compared to just using the pre-trained models directly. The effectiveness of the REACT framework is evaluated extensively on image classification, retrieval, detection and segmentation tasks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes React, a framework to leverage large-scale image-text data from the web as external knowledge to customize vision models for downstream tasks. 

2. It builds an efficient retrieval system using CLIP and FAISS to acquire relevant image-text pairs for a target domain, using only the task description. No images from the downstream task are needed.

3. It proposes a modularized learning strategy called "locked-text gated-image tuning" to efficiently customize the model by freezing original weights and training new modules on retrieved data.

4. It demonstrates the effectiveness of React on a variety of vision tasks including image classification, retrieval, detection and segmentation. React improves over CLIP on 20+ datasets in zero-shot, few-shot and full-shot settings.

5. It provides strong baselines using CLIP on semi-supervised ImageNet classification, achieving new state-of-the-art with 1.6% labeled data.

In summary, this paper shows the potential of using web-scale retrieval to efficiently customize vision models, instead of relying solely on model scaling and more labeled data. The proposed React framework is model-agnostic and could enable better transfer learning.
