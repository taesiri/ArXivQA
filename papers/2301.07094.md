# [Learning Customized Visual Models with Retrieval-Augmented Knowledge](https://arxiv.org/abs/2301.07094)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central hypothesis of this paper is that leveraging relevant external knowledge retrieved from a large-scale image-text corpus can help customize pre-trained vision-language models and improve their performance on downstream tasks. 

Specifically, the authors propose a framework called REACT (Retrieval-Augmented Customization) that retrieves the most relevant image-text pairs from a large database using the task instructions (e.g. class names) as queries. It then customizes the pre-trained model by training additional modules on this retrieved data while freezing the original weights. 

The key idea is that instead of solely relying on the pre-trained model's internal knowledge, providing it with relevant external knowledge can help it better adapt to new downstream tasks. The paper aims to demonstrate the effectiveness of this retrieval-augmented knowledge and the proposed customization framework.

In summary, the central hypothesis is that model customization with relevant retrieved knowledge can improve vision-language models' transfer performance on downstream tasks compared to just using the pre-trained models directly. The effectiveness of the REACT framework is evaluated extensively on image classification, retrieval, detection and segmentation tasks.
