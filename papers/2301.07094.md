# [Learning Customized Visual Models with Retrieval-Augmented Knowledge](https://arxiv.org/abs/2301.07094)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central hypothesis of this paper is that leveraging relevant external knowledge retrieved from a large-scale image-text corpus can help customize pre-trained vision-language models and improve their performance on downstream tasks. 

Specifically, the authors propose a framework called REACT (Retrieval-Augmented Customization) that retrieves the most relevant image-text pairs from a large database using the task instructions (e.g. class names) as queries. It then customizes the pre-trained model by training additional modules on this retrieved data while freezing the original weights. 

The key idea is that instead of solely relying on the pre-trained model's internal knowledge, providing it with relevant external knowledge can help it better adapt to new downstream tasks. The paper aims to demonstrate the effectiveness of this retrieval-augmented knowledge and the proposed customization framework.

In summary, the central hypothesis is that model customization with relevant retrieved knowledge can improve vision-language models' transfer performance on downstream tasks compared to just using the pre-trained models directly. The effectiveness of the REACT framework is evaluated extensively on image classification, retrieval, detection and segmentation tasks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes React, a framework to leverage large-scale image-text data from the web as external knowledge to customize vision models for downstream tasks. 

2. It builds an efficient retrieval system using CLIP and FAISS to acquire relevant image-text pairs for a target domain, using only the task description. No images from the downstream task are needed.

3. It proposes a modularized learning strategy called "locked-text gated-image tuning" to efficiently customize the model by freezing original weights and training new modules on retrieved data.

4. It demonstrates the effectiveness of React on a variety of vision tasks including image classification, retrieval, detection and segmentation. React improves over CLIP on 20+ datasets in zero-shot, few-shot and full-shot settings.

5. It provides strong baselines using CLIP on semi-supervised ImageNet classification, achieving new state-of-the-art with 1.6% labeled data.

In summary, this paper shows the potential of using web-scale retrieval to efficiently customize vision models, instead of relying solely on model scaling and more labeled data. The proposed React framework is model-agnostic and could enable better transfer learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes React, a framework to customize vision-language models like CLIP to downstream tasks by retrieving the most relevant image-text pairs from a large web-scale database and training modularized blocks on the retrieved data while freezing the original weights.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work:

- This paper proposes React (Retrieval-Augmented Customization), a framework to customize pre-trained vision-language models like CLIP using relevant image-text pairs retrieved from a large database. Other works have explored improving CLIP through scaling up pre-training data/model size or changing the pre-training objectives. React offers a complementary data-centric approach focusing on retrieving relevant examples.

- For model customization, React proposes a modularized learning approach by freezing base weights and training new gated blocks on retrieved data. This is different from prior works like finetuning or prompt-tuning that update all weights. React aims to adapt models to new domains without forgetting original capabilities. 

- React demonstrates strong improvements on several vision tasks including classification, retrieval, detection, and segmentation. Especially notable are the gains in zero-shot transfer, where React outperforms prior CLIP models without seeing any target images. This showcases the value of retrieval-augmented knowledge.

- For knowledge retrieval, React builds an efficient approximate nearest neighbor index for web-scale image-text data. This enables retrieving relevant examples based on task instructions, without human annotation. Prior retrieval-based vision works like RAC focused more on classification with a non-parametric memory bank.

- React proposes a general framework applicable to various vision domains. Many prior efforts on improving CLIP focused narrowly on image classification. React shows consistent gains across multiple vision tasks.

In summary, React offers a unique data-driven approach for model customization via efficient web-scale retrieval, with modularized learning for preserving base knowledge. The consistent gains across diverse vision tasks highlight the value and generality of this framework.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Explore using the entire web as the knowledge base, rather than a static image-text dataset. The authors suggest using search engines like Google or Bing to retrieve relevant knowledge, instead of relying on a fixed dataset like LAION. This could allow even more relevant knowledge to be acquired.

- Develop more sophisticated quality control techniques for the retrieved data. The authors tried using the CLIP score for basic filtering of noisy samples, but suggest more advanced methods could be explored. This could improve the relevance of the retrieved knowledge.

- Make the framework more accessible through public retrieval systems and model codebases. The authors plan to release their retrieval subsets and an easy-to-use toolkit to allow more researchers to explore this approach.

- Study how to best incorporate retrieved knowledge in settings where some training data is available. The authors showed gains in few-shot and full-shot settings, but more work can be done to effectively combine retrieved knowledge with available training data.

- Explore retrieval-augmented knowledge for other computer vision tasks beyond classification, retrieval, detection and segmentation. The generality of the framework means it could likely benefit other vision tasks as well.

- Develop prompt engineering techniques tailored for this framework to generate optimal queries for knowledge retrieval. More work can be done to develop prompts that retrieve the most useful knowledge.

- Scale up the amount of retrieved knowledge and model sizes. The authors suggest continued scaling could lead to further improvements.

In summary, the key future directions are improving the knowledge retrieval process, combining retrieved knowledge with available training data, applying the framework to more vision tasks, and continued scaling of data and models. The overall goal is to advance research into effectively leveraging external web-scale knowledge to build customized vision systems.


## Summarize the paper in one paragraph.

 The paper presents a framework called Retrieval-Augmented Customization (ReACT) to build customized visual models for specific domains by utilizing relevant image-text pairs retrieved from a large-scale corpus as external knowledge. The key ideas are: 1) Construct a multi-modal retrieval system to efficiently acquire relevant image-text pairs for a target domain using only text queries. 2) Customize pre-trained vision-language models like CLIP by freezing most weights and training additional gated modules on the retrieved data. 3) Demonstrate effectiveness across classification, retrieval, detection and segmentation tasks. ReACT substantially outperforms CLIP on ImageNet and 20 datasets in ELEVATE benchmark through retrieval of a small fraction (~3%) of the original CLIP pre-training data. The framework provides an affordable way to leverage ever-increasing web data for customized visual recognition.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a framework called REACT (Retrieval-Augmented CusTomization) to customize pretrained vision-language models like CLIP to downstream tasks using relevant retrieved image-text pairs. The key ideas are:

1) They build a large-scale multi-modal retrieval system using web-scale image-text data as the knowledge source. Using just the textual task descriptions of downstream tasks as queries, relevant image-text pairs are retrieved from the database without needing any downstream images. 

2) To efficiently customize the pretrained model, they freeze the original weights and add new trainable gated self-attention blocks to the image encoder. The model is then trained on the retrieved image-text pairs with a contrastive loss. This allows the model to adapt to new tasks without forgetting previously learned knowledge.

Experiments on image classification, retrieval, detection and segmentation show consistent improvements over original CLIP, especially in low-data regimes. Gains are shown to come from the retrieved knowledge, not from overlapping samples. The retrieval system and code will be released to make this approach accessible. Overall, the paper presents an effective framework to leverage web-scale data to build customized vision models.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes ReAct (Retrieval-Augmented Customization), a framework to build customized visual models for target domains by retrieving relevant knowledge from a large image-text corpus. The key ideas are:

1) They build a large-scale multi-modal indexing system using a pre-trained contrastive model like CLIP and approximate nearest neighbor search. This allows efficient retrieval of relevant image-text pairs from the corpus given a task description. 

2) For model customization, they freeze the weights of a pre-trained generic visual model like CLIP and only train new modularized blocks on the retrieved image-text pairs. This allows the model to adapt to the new domain while retaining the generic knowledge.

3) They demonstrate ReAct's effectiveness on image classification, retrieval, detection and segmentation tasks. Without seeing any images from the target domain, retrieving and training on relevant image-text pairs improves zero-shot performance over CLIP by up to 5.4% on ImageNet classification. ReAct also shows gains in few-shot and full-shot settings when target domain images are available.

In summary, the key idea is to leverage retrieved relevant external knowledge to efficiently customize pre-trained models to new domains, instead of training them from scratch. This improves transfer performance at a low cost.


## What problem or question is the paper addressing?

 The paper proposes a framework called React (Retrieval-Augmented CusTomization) to customize existing vision-language models like CLIP to new domains using relevant retrieved image-text pairs from a large database. The key ideas are:

- Leverage a large database of image-text pairs (e.g. LAION-400M) as external knowledge to build customized models for target domains.

- Retrieve the most relevant pairs from the database as additional training data using the task instructions (e.g. class names) as queries. 

- Customize the model by freezing the original weights and adding new trainable modules that are trained on the retrieved data.

- This allows efficiently adapting the model to new domains without forgetting the original capabilities and without needing to retrain the full model.

So in summary, it aims to address the problem of efficiently adapting large vision-language models to new target domains/tasks by utilizing relevant external multi-modal knowledge retrieved from a web-scale database. The core motivation is that adapting only part of the model is more efficient than full retraining.
