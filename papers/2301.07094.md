# [Learning Customized Visual Models with Retrieval-Augmented Knowledge](https://arxiv.org/abs/2301.07094)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central hypothesis of this paper is that leveraging relevant external knowledge retrieved from a large-scale image-text corpus can help customize pre-trained vision-language models and improve their performance on downstream tasks. 

Specifically, the authors propose a framework called REACT (Retrieval-Augmented Customization) that retrieves the most relevant image-text pairs from a large database using the task instructions (e.g. class names) as queries. It then customizes the pre-trained model by training additional modules on this retrieved data while freezing the original weights. 

The key idea is that instead of solely relying on the pre-trained model's internal knowledge, providing it with relevant external knowledge can help it better adapt to new downstream tasks. The paper aims to demonstrate the effectiveness of this retrieval-augmented knowledge and the proposed customization framework.

In summary, the central hypothesis is that model customization with relevant retrieved knowledge can improve vision-language models' transfer performance on downstream tasks compared to just using the pre-trained models directly. The effectiveness of the REACT framework is evaluated extensively on image classification, retrieval, detection and segmentation tasks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes React, a framework to leverage large-scale image-text data from the web as external knowledge to customize vision models for downstream tasks. 

2. It builds an efficient retrieval system using CLIP and FAISS to acquire relevant image-text pairs for a target domain, using only the task description. No images from the downstream task are needed.

3. It proposes a modularized learning strategy called "locked-text gated-image tuning" to efficiently customize the model by freezing original weights and training new modules on retrieved data.

4. It demonstrates the effectiveness of React on a variety of vision tasks including image classification, retrieval, detection and segmentation. React improves over CLIP on 20+ datasets in zero-shot, few-shot and full-shot settings.

5. It provides strong baselines using CLIP on semi-supervised ImageNet classification, achieving new state-of-the-art with 1.6% labeled data.

In summary, this paper shows the potential of using web-scale retrieval to efficiently customize vision models, instead of relying solely on model scaling and more labeled data. The proposed React framework is model-agnostic and could enable better transfer learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes React, a framework to customize vision-language models like CLIP to downstream tasks by retrieving the most relevant image-text pairs from a large web-scale database and training modularized blocks on the retrieved data while freezing the original weights.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work:

- This paper proposes React (Retrieval-Augmented Customization), a framework to customize pre-trained vision-language models like CLIP using relevant image-text pairs retrieved from a large database. Other works have explored improving CLIP through scaling up pre-training data/model size or changing the pre-training objectives. React offers a complementary data-centric approach focusing on retrieving relevant examples.

- For model customization, React proposes a modularized learning approach by freezing base weights and training new gated blocks on retrieved data. This is different from prior works like finetuning or prompt-tuning that update all weights. React aims to adapt models to new domains without forgetting original capabilities. 

- React demonstrates strong improvements on several vision tasks including classification, retrieval, detection, and segmentation. Especially notable are the gains in zero-shot transfer, where React outperforms prior CLIP models without seeing any target images. This showcases the value of retrieval-augmented knowledge.

- For knowledge retrieval, React builds an efficient approximate nearest neighbor index for web-scale image-text data. This enables retrieving relevant examples based on task instructions, without human annotation. Prior retrieval-based vision works like RAC focused more on classification with a non-parametric memory bank.

- React proposes a general framework applicable to various vision domains. Many prior efforts on improving CLIP focused narrowly on image classification. React shows consistent gains across multiple vision tasks.

In summary, React offers a unique data-driven approach for model customization via efficient web-scale retrieval, with modularized learning for preserving base knowledge. The consistent gains across diverse vision tasks highlight the value and generality of this framework.
