# [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we scale up the sequence length of Transformers to very long sequences (billions of tokens) without sacrificing performance on shorter sequences? The key hypothesis is that replacing the standard Transformer attention mechanism with a proposed "dilated attention" mechanism will allow scaling to much longer sequences while maintaining strong performance on shorter sequences. Dilated attention expands the attentional field exponentially as distance increases, reducing the overall complexity from quadratic to linear.In summary, the central research question is how to efficiently scale Transformers to extremely long sequence lengths. The proposed dilated attention mechanism is hypothesized to achieve this goal by reducing complexity while still capturing long-range dependencies. The experiments aim to demonstrate the effectiveness of dilated attention in scaling to long sequences without losing short-sequence performance.
