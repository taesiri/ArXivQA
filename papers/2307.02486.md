# [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we scale up the sequence length of Transformers to very long sequences (billions of tokens) without sacrificing performance on shorter sequences? The key hypothesis is that replacing the standard Transformer attention mechanism with a proposed "dilated attention" mechanism will allow scaling to much longer sequences while maintaining strong performance on shorter sequences. Dilated attention expands the attentional field exponentially as distance increases, reducing the overall complexity from quadratic to linear.In summary, the central research question is how to efficiently scale Transformers to extremely long sequence lengths. The proposed dilated attention mechanism is hypothesized to achieve this goal by reducing complexity while still capturing long-range dependencies. The experiments aim to demonstrate the effectiveness of dilated attention in scaling to long sequences without losing short-sequence performance.


## What is the main contribution of this paper?

The main contribution of this paper is proposing LongNet, a Transformer variant that can scale the sequence length to over 1 billion tokens without sacrificing performance on shorter sequences. Specifically:- It proposes dilated attention, which expands the attentive field exponentially as the distance grows. This reduces the complexity from quadratic to linear while still allowing long-range dependencies.- It shows how LongNet can be used as a distributed trainer to parallelize training across multiple GPUs/nodes. This allows scaling to 1 billion tokens with nearly constant runtime. - Experiments show LongNet achieves strong performance on both long-sequence modeling tasks and general language modeling benchmarks. - The authors demonstrate the capability of scaling the sequence length to 1 billion tokens, which opens possibilities for modeling entire corpora or the full internet as a single sequence.In summary, the main contribution is proposing the dilated attention mechanism and the distributed training scheme to efficiently scale Transformer sequence lengths to orders of magnitude longer than previous methods, without sacrificing model performance. This enables new possibilities for extreme long-context modeling.
