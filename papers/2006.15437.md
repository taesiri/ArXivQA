# [GPT-GNN: Generative Pre-Training of Graph Neural Networks](https://arxiv.org/abs/2006.15437)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus is on how to pre-train graph neural networks (GNNs) on unlabeled graph data through a self-supervised learning task, such that the pre-trained model generalizes well to downstream tasks with only a small amount of labeled data. Specifically, the paper proposes a framework called GPT-GNN for generative pre-training of GNNs. The main hypothesis is that by training a GNN to reconstruct the input graph, including both structure and attributes, through an attributed graph generation task, the model will learn useful representations that capture the semantic and structural properties of the graph. This pre-trained model can then be fine-tuned on downstream tasks using just a few labels to achieve strong performance.In summary, the central research question is: How can we pre-train GNNs on unlabeled graphs through a self-supervised attributed graph generation task, such that the model learns generic and useful representations of graph structure and semantics that transfer well to various downstream tasks? GPT-GNN explores this question by factorizing graph generation into attribute and edge generation components and proposes techniques to efficiently optimize this pre-training objective.
