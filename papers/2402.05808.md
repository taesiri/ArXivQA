# [Training Large Language Models for Reasoning through Reverse Curriculum   Reinforcement Learning](https://arxiv.org/abs/2402.05808)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-step reasoning is challenging for large language models (LLMs) due to error propagation across reasoning steps.  
- Existing supervision methods have limitations - outcome supervision provides only sparse rewards while process supervision requires expensive annotations.

Proposed Method: 
- The paper proposes \textbf{R}$^3$, a novel reinforcement learning (RL) method that uses only outcome supervision to achieve an effect similar to process supervision.

- \textbf{R}$^3$ lets the model begin reasoning from intermediate states sampled from demonstrations, with the start state progressively moved from the demonstration's end to beginning. This establishes a curriculum where the model faces an easy exploration problem at each point.

- As the model has already learned to solve remaining parts, it can get positive rewards more easily. This allows outcome supervision to offer step-level signals, precisely pinpointing errors.

Contributions:
- Proposes \textbf{R}$^3$ that uses outcome supervision to realize step-level supervision for enhancing LLM reasoning, overcoming limitations of existing methods.

- Achieves superior performance over supervised fine-tuning and RL baselines on 8 reasoning tasks using Llama2-7B, with avg. gains of 5.4 and 4.1 points.

- Shows particular effectiveness on program-based reasoning, exceeding baselines by 11.4 and 4.2 points on GSM8K. Comparable to larger models without using extra data.

- Provides in-depth analysis into training dynamics of \textbf{R}$^3$, how it creates curriculum, and impact of different design choices.
