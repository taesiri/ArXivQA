# [Efficient Deep Neural Networks](https://arxiv.org/abs/1908.08926v1)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to design efficient deep neural network models that can achieve high accuracy with low computational complexity, enabling deployment on resource-constrained devices like mobile phones. The key hypothesis is that it is possible to design compact deep neural network models that match or exceed the accuracy of larger, more computationally complex models. 

The paper focuses on designing efficient models for two computer vision tasks: object detection in images and semantic segmentation of LiDAR point clouds. For object detection, the authors design a model called SqueezeDet that achieves the same accuracy as prior models based on Faster R-CNN, but with 30x fewer parameters, 20x less computation (FLOPs), and 35x better energy efficiency. For LiDAR segmentation, they design SqueezeSeg, which is significantly faster (over 100 FPS) than prior methods while maintaining high accuracy.

The core ideas that enable these efficient models are:
- Carefully designing the model architecture, loss functions, and training process specifically for the task and dataset.
- Using a compact backbone CNN like SqueezeNet rather than large models like VGG.
- Avoiding components like fully connected layers that lead to huge parameter sizes.
- Using convolutional layers rather than fully connected layers for detection and segmentation tasks.

So in summary, the central hypothesis is that task-specific model design can lead to much more efficient deep learning models without sacrificing accuracy. The paper aims to demonstrate this through the design of SqueezeDet and SqueezeSeg.


## What is the main contribution of this paper?

 Here is a high-level summary of the main contributions of this paper:

- The paper proposes a new method called Differentiable Neural Architecture Search (DNAS) for automatically searching for efficient neural network architectures. 

- DNAS formulates the architecture search as a continuous optimization problem that can be solved efficiently using gradient descent, rather than expensive reinforcement learning or evolutionary algorithms.

- DNAS represents the enormous architecture search space using a stochastic super net whose nodes are data tensors and edges are operators executed stochastically. The search involves finding the optimal architecture distribution parameterized by the edge sampling probabilities.

- The loss function used to train the stochastic supernet contains both the standard cross-entropy loss and a latency loss measured on the target hardware. This allows directly optimizing for hardware efficiency.

- A lookup table model is used to quickly estimate the latency of architectures in the search space based on benchmarking individual operators, avoiding expensive evaluation.

- DNAS found efficient models called FBNets that achieve state-of-the-art accuracy and latency on ImageNet classification. The search cost is 421x lower than a prior NAS method.

- DNAS enables specialized architecture search for different resolutions, channel scaling factors, and target hardware. This demonstrates the advantage of extremely fast search.

- DNAS is also applied to mixed-precision quantization and finds efficient precision allocation strategies that outperform standard quantization.

In summary, the key innovations are the continuous relaxation of the architecture search space, the hardware-aware loss function, and ultra-fast search that enables specialized architecture optimization. DNAS achieves new state-of-the-art results for efficient ConvNets and quantization with orders of magnitude less search cost.
