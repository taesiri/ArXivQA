# [Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models   via Reparameterisation and Smoothing](https://arxiv.org/abs/2402.11752)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Stochastic gradient descent (SGD) is commonly used to optimize expectations of the form $\mathbb{E}[f_\theta(z)]$ where $f_\theta$ may contain non-differentiable operations like if-statements. 
- The reparameterization gradient estimator typically has lower variance than score function estimator, but it is biased for non-differentiable models, which can compromise the correctness and convergence of SGD.

Proposed Solution:
- Introduce a simple language to express non-differentiable functions using if-statements and primitive operations. 
- Define a smoothing technique to interpret if-statements with sigmoid functions parameterized by an accuracy coefficient. This gives differentiable approximations.
- Propose Diagonalisation SGD (DSGD) which takes SGD steps using gradients of smoothed objectives, while increasing accuracy during optimization.

Main Contributions:
- Prove that DSGD converges asymptotically to stationary points of the original unsmoothed optimization problem. Choice of accuracy coefficients only depends on syntactic nesting structure.
- Show smoothed objective and gradients converge uniformly to original ones.
- Bound the variance for smoothed estimators based on syntax tree depth.
- Empirically evaluate DSGD on non-differentiable models from VI and show comparable or better performance than prior art in terms of variance, stability and convergence. Orders of magnitude reduction in work-normalized variance.

In summary, the paper introduces a principled smoothing technique and diagonlisation SGD algorithm to overcome bias for non-differentiable objectives in SGD. It provides strong theoretical guarantees and demonstrates excellent empirical performance.
