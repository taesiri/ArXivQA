# [AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal   Conditioning](https://arxiv.org/abs/2402.05803)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Generating high-quality 3D head avatars is important for applications like movie production and social media. 
- Existing 3D GANs can generate avatars well but have limited control over the generation process. They can typically only condition on a single modality (e.g. text, sketches).
- It is challenging to impose multi-modal control, e.g. attributes, RGB images, segmentation maps, to steer the avatar generation process.

Proposed Solution:
- Use a pre-trained 3D GAN (Next3D) to generate the avatars. Keep this model fixed.
- Train a lightweight 1D latent diffusion model (LDM) to steer sampling in the latent space of this GAN.
- The LDM takes multiple modalities as input conditions:
   - Global attributes: Encoded with a dedicated network
   - RGB images & segmentation maps: Encoded with a visual encoder   
- Sampling is done by combining unconditional sampling from the GAN + conditional sampling from the LDM based on Classifier-Free Guidance.
- Perform sampling in two stages:
   - Reconstruction stage: Get close to input image
   - Editing stage: Apply edits by masking conditions

Main Contributions:
- Propose a framework for 3D avatar generation and editing with multi-modal control signals
- Employ a pre-trained GAN for avatar quality and an LDM for controllability
- The approach is lightweight and fast since the GAN is kept fixed
- Experiments show the method outperforms a purely GAN-based approach on conditional generation and editing
- To the best of their knowledge, first technique to enable multi-modal conditioning for 3D avatar tasks
