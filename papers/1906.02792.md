# [Attention is all you need for Videos: Self-attention based Video   Summarization using Universal Transformers](https://arxiv.org/abs/1906.02792)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can transformer models be adapted and applied to the task of video captioning/summarization, and can they achieve good performance compared to prior recurrent neural network based approaches?The key ideas and contributions seem to be:- Using 3D CNNs like C3D and I3D for spatio-temporal video feature extraction, instead of relying on recurrent networks to model temporal information.- Modifying the standard transformer architecture by removing the embedding layer since pre-computed video features are used as input.- Experimenting with both the original transformer and universal transformer models.- Evaluating the performance on both single sentence video captioning (MSVD dataset) and multi-sentence dense video captioning (ActivityNet dataset).- Achieving promising results compared to prior state-of-the-art methods based on recurrent networks and attention mechanisms.So in summary, the main research question is assessing the viability and potential advantages of using transformer networks for video captioning tasks compared to existing recurrent network based approaches. The results suggest transformers are a promising approach in this domain.
