# [Attention is all you need for Videos: Self-attention based Video   Summarization using Universal Transformers](https://arxiv.org/abs/1906.02792)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can transformer models be adapted and applied to the task of video captioning/summarization, and can they achieve good performance compared to prior recurrent neural network based approaches?The key ideas and contributions seem to be:- Using 3D CNNs like C3D and I3D for spatio-temporal video feature extraction, instead of relying on recurrent networks to model temporal information.- Modifying the standard transformer architecture by removing the embedding layer since pre-computed video features are used as input.- Experimenting with both the original transformer and universal transformer models.- Evaluating the performance on both single sentence video captioning (MSVD dataset) and multi-sentence dense video captioning (ActivityNet dataset).- Achieving promising results compared to prior state-of-the-art methods based on recurrent networks and attention mechanisms.So in summary, the main research question is assessing the viability and potential advantages of using transformer networks for video captioning tasks compared to existing recurrent network based approaches. The results suggest transformers are a promising approach in this domain.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a Transformer-based model for video captioning, using 3D CNN architectures like C3D and Two-stream I3D for video feature extraction. - Applying dimensionality reduction techniques on the video features from C3D and I3D to make the overall model size manageable.- Evaluating the proposed model on the MSVD and ActivityNet datasets for single sentence and dense/paragraph video captioning tasks.- Comparing the performance of the Transformer and Universal Transformer variants for video captioning.- Achieving promising results on MSVD and ActivityNet benchmarks using just visual features, without requiring semantic attribute inputs.- Providing analysis of the model's capabilities and limitations using qualitative examples.In summary, the key contribution seems to be exploring the potential of Transformers for video captioning by replacing RNNs, and sharing experimental results on standard datasets to quantify their performance. The work also provides insights into the model's strengths and weaknesses that can inform future research directions.
