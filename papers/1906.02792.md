# [Attention is all you need for Videos: Self-attention based Video   Summarization using Universal Transformers](https://arxiv.org/abs/1906.02792)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can transformer models be adapted and applied to the task of video captioning/summarization, and can they achieve good performance compared to prior recurrent neural network based approaches?The key ideas and contributions seem to be:- Using 3D CNNs like C3D and I3D for spatio-temporal video feature extraction, instead of relying on recurrent networks to model temporal information.- Modifying the standard transformer architecture by removing the embedding layer since pre-computed video features are used as input.- Experimenting with both the original transformer and universal transformer models.- Evaluating the performance on both single sentence video captioning (MSVD dataset) and multi-sentence dense video captioning (ActivityNet dataset).- Achieving promising results compared to prior state-of-the-art methods based on recurrent networks and attention mechanisms.So in summary, the main research question is assessing the viability and potential advantages of using transformer networks for video captioning tasks compared to existing recurrent network based approaches. The results suggest transformers are a promising approach in this domain.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a Transformer-based model for video captioning, using 3D CNN architectures like C3D and Two-stream I3D for video feature extraction. - Applying dimensionality reduction techniques on the video features from C3D and I3D to make the overall model size manageable.- Evaluating the proposed model on the MSVD and ActivityNet datasets for single sentence and dense/paragraph video captioning tasks.- Comparing the performance of the Transformer and Universal Transformer variants for video captioning.- Achieving promising results on MSVD and ActivityNet benchmarks using just visual features, without requiring semantic attribute inputs.- Providing analysis of the model's capabilities and limitations using qualitative examples.In summary, the key contribution seems to be exploring the potential of Transformers for video captioning by replacing RNNs, and sharing experimental results on standard datasets to quantify their performance. The work also provides insights into the model's strengths and weaknesses that can inform future research directions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a Transformer-based model for video captioning and summarization that uses 3D CNNs like C3D and I3D for video feature extraction, and experiments with both standard and universal Transformers to generate captions and dense video descriptions.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in video captioning:- It uses transformer models (both standard and universal transformers) for sequence-to-sequence video captioning, whereas most prior work uses recurrent models like LSTMs or GRUs. Transformers have become popular in NLP but their use for video captioning is still relatively new.- For video feature extraction, it utilizes 3D CNNs (C3D and I3D) to encode spatio-temporal visual features. Many papers have explored different video encoders, from basic 2D CNNs per frame to more sophisticated 3D models. Using 3D networks to capture motion and temporal context seems to be becoming standard practice now.- It experiments with both single captioning (MSVD dataset) and paragraph captioning (ActivityNet dataset). A lot of papers focus only on single sentence output, but generating longer, multi-sentence descriptions is an important direction as it's more challenging. - It doesn't use any semantic attributes or text conditioned on detected objects/actions, in contrast to some recent methods that incorporate NLP representations. Relying purely on visual inputs makes the model simpler.- The model architecture is encoder-decoder based, where the encoder maps video to features and decoder generates captions autoregressively. This overall framework aligns with most state-of-the-art models, while the specific components (transformer, 3D CNN) are modern choices.- Quantitative results are decent but not state-of-the-art. The focus seems to be more on exploring transformers for video captioning, rather than maximizing metrics. There is room to improve with better hyperparameter tuning, fine-tuning feature extractors, etc.In summary, this paper explores some current ideas like transformers and 3D CNNs for the fundamental video captioning task. It produces competitive results on standard datasets while analyzing the qualitative strengths and weaknesses. The ideas could provide a strong baseline for further research to build on.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Fine-tuning the C3D and I3D feature extraction networks on the task training set rather than using the pre-trained networks as-is. The authors suggest this could improve performance but they were limited by compute resources.- Incorporating adaptive computation time (ACT) into the Universal Transformer to improve the halting process. The authors found ACT was halting too early during training, limiting model capacity. - Exploring other image attribute generator networks beyond conventional CNNs, such as the approaches proposed in Wu et al. (2015) and Liu et al. (2017). The authors' attempts at an attribute generator did not help, but they believe other methods may capture nuances in the datasets better.- Testing on larger and more diverse video datasets. The authors note limitations of the relatively small and semantically narrow MSVD dataset. Evaluating on larger datasets like ActivityNet is suggested.- Extending the model to generate multiple sentences sequentially to describe long videos without ground truth event proposals. The authors see this as a challenging but important next step.- Overall, the main future directions are improving the training of the feature extractors and Transformer modules, incorporating adaptive computation, leveraging auxiliary prediction tasks like attribute generation, and testing on larger and more varied video datasets.
