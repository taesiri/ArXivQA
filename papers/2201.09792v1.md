# [Patches Are All You Need?](https://arxiv.org/abs/2201.09792v1)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Is the performance of vision transformers (ViTs) due to the inherently more powerful Transformer architecture, or is it at least partly due to using patches as the input representation?The authors question whether the superior performance of ViTs compared to convolutional neural networks (CNNs) for computer vision tasks is really due to the Transformer architecture itself, or whether the use of patch embeddings as input also plays a key role. To investigate this, the authors propose a very simple convolutional architecture called the ConvMixer, which operates directly on image patches like ViTs do. The ConvMixer separates spatial and channel mixing using standard convolutions, while maintaining an equal resolution representation throughout the layers like ViTs. Despite its simplicity, the ConvMixer outperforms ViTs and other models on ImageNet classification for similar model sizes. This suggests the patch representation itself, rather than just the Transformer architecture, is an important factor behind the strong performance of ViTs. Evaluating the effect of the patch embedding input representation seems to be the key research question addressed in this work.
