# [Sample-Efficient Automated Deep Reinforcement Learning](https://arxiv.org/abs/2009.01555)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we develop a sample-efficient automated deep reinforcement learning framework that can jointly optimize hyperparameters and neural network architecture?The key points related to this question seem to be:- Deep RL algorithms are very sensitive to hyperparameters, but tuning them through standard hyperparameter optimization is extremely sample-inefficient due to the cost of environment interactions.- The non-stationarity of the RL problem over the course of training may require different hyperparameters at different stages, so static configuration is suboptimal.- Most prior hyperparameter optimization methods treat the RL algorithm as a black box, whereas directly optimizing hyperparameters during training could be more efficient. - Optimizing neural architecture in addition to hyperparameters could improve performance, but is rarely done in the RL setting.- Sharing experience across agents could enable more efficient use of environment samples during population-based training.To address these challenges, the authors propose a framework called SEARL that performs automated hyperparameter and architecture optimization for off-policy RL algorithms while simultaneously training a population of agents that share a common experience replay buffer. This aims to achieve sample-efficient joint optimization in a dynamic way.In summary, the key hypothesis is that SEARL can enable automated, sample-efficient optimization of hyperparameters and architecture for off-policy RL agents. The experiments aim to demonstrate these capabilities and efficiency gains compared to alternatives.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel framework called Sample-Efficient Automated Reinforcement Learning (SEARL). The key ideas of SEARL seem to be:- It enables hyperparameter optimization and neural architecture search for off-policy deep RL algorithms in a sample-efficient manner. Specifically, it trains a population of agents with different hyperparameters and architectures simultaneously while sharing experience through a common replay buffer. This allows hyperparameter optimization with almost no additional environment samples compared to training a single agent.- It performs dynamic optimization of hyperparameters over time rather than finding a single fixed configuration. This allows adapting to different phases of the RL training process. - It modifies the neural architecture over time through evolvable neural networks, preserving already trained weights when making architecture changes.- It is a simple and general framework that can optimize any off-policy deep RL algorithm, demonstrated on TD3 and DQN.- It reduces the number of environment interactions needed for hyperparameter optimization by up to an order of magnitude compared to population-based training and random search in MuJoCo benchmark tasks.In summary, the main contribution appears to be proposing SEARL as a way to do efficient automated hyperparameter and architecture optimization for deep RL in a dynamic way, while retaining sample efficiency, through the use of weight sharing and other techniques. The experiments demonstrate substantial improvements in sample efficiency over alternative approaches.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here are a few thoughts on how it compares to other research in automated deep reinforcement learning:- The main contribution is a framework called SEARL for sample-efficient automated deep RL. It allows jointly optimizing hyperparameters and neural architecture while training an agent population. This differs from much prior work in AutoRL that treats the RL algorithm as a black box and does not focus on sample efficiency.- Sharing experience replay across the population is a key novelty to improve sample efficiency. This allows SEARL to perform meta-optimization with practically the same samples as training a single configuration. Prior work like PBT does not share experience between configurations.- SEARL performs dynamic optimization of hyperparameters during training, discovering schedules rather than static configurations. This sets it apart from one-shot hyperparameter optimization methods. The ability to modify neural architecture online also differentiates it from PBT.- The proposed evaluation protocol emphasizes sample efficiency, unlike standard RL benchmarks that just report final performance. This is an important contribution as it encourages developing sample-efficient AutoRL techniques.- The experiments demonstrate strong results on continuous control tasks by meta-optimizing TD3. SEARL achieves the same performance as TD3 10x faster than random search or PBT. Additional experiments with DQN show the generality beyond a single algorithm.- Compared to some other AutoRL papers that show results on simpler or custom environments, this work focuses on established benchmarks like MuJoCo and Atari. The techniques seem broadly applicable.Overall, the sample efficiency gains, dynamic optimization, and proposed evaluation make this work stand out from much of the related AutoRL literature. The shared experience replay seems like the most unique aspect that enables the efficiency gains. The experiments also back up the claims rigorously.
