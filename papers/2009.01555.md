# [Sample-Efficient Automated Deep Reinforcement Learning](https://arxiv.org/abs/2009.01555)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we develop a sample-efficient automated deep reinforcement learning framework that can jointly optimize hyperparameters and neural network architecture?The key points related to this question seem to be:- Deep RL algorithms are very sensitive to hyperparameters, but tuning them through standard hyperparameter optimization is extremely sample-inefficient due to the cost of environment interactions.- The non-stationarity of the RL problem over the course of training may require different hyperparameters at different stages, so static configuration is suboptimal.- Most prior hyperparameter optimization methods treat the RL algorithm as a black box, whereas directly optimizing hyperparameters during training could be more efficient. - Optimizing neural architecture in addition to hyperparameters could improve performance, but is rarely done in the RL setting.- Sharing experience across agents could enable more efficient use of environment samples during population-based training.To address these challenges, the authors propose a framework called SEARL that performs automated hyperparameter and architecture optimization for off-policy RL algorithms while simultaneously training a population of agents that share a common experience replay buffer. This aims to achieve sample-efficient joint optimization in a dynamic way.In summary, the key hypothesis is that SEARL can enable automated, sample-efficient optimization of hyperparameters and architecture for off-policy RL agents. The experiments aim to demonstrate these capabilities and efficiency gains compared to alternatives.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel framework called Sample-Efficient Automated Reinforcement Learning (SEARL). The key ideas of SEARL seem to be:- It enables hyperparameter optimization and neural architecture search for off-policy deep RL algorithms in a sample-efficient manner. Specifically, it trains a population of agents with different hyperparameters and architectures simultaneously while sharing experience through a common replay buffer. This allows hyperparameter optimization with almost no additional environment samples compared to training a single agent.- It performs dynamic optimization of hyperparameters over time rather than finding a single fixed configuration. This allows adapting to different phases of the RL training process. - It modifies the neural architecture over time through evolvable neural networks, preserving already trained weights when making architecture changes.- It is a simple and general framework that can optimize any off-policy deep RL algorithm, demonstrated on TD3 and DQN.- It reduces the number of environment interactions needed for hyperparameter optimization by up to an order of magnitude compared to population-based training and random search in MuJoCo benchmark tasks.In summary, the main contribution appears to be proposing SEARL as a way to do efficient automated hyperparameter and architecture optimization for deep RL in a dynamic way, while retaining sample efficiency, through the use of weight sharing and other techniques. The experiments demonstrate substantial improvements in sample efficiency over alternative approaches.
