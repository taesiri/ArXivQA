# [JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human   Mesh Recovery](https://arxiv.org/abs/2307.16377)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve 3D human mesh recovery from single images, especially under occluded conditions?The key hypotheses appear to be:1) Fusing 2D and 3D features can help achieve better 2D and 3D alignment of the reconstructed mesh, overcoming limitations when using only 2D features. 2) Adding global supervision to the 3D feature space, via a novel 3D joint contrastive learning approach, can help distinguish the target human from occlusions/background and produce more semantically meaningful 3D representations.The authors propose a new method called JOTR (3D Joint Contrastive learning with Transformers) to address these questions and test these hypotheses. The main contributions seem to be:- A transformer architecture to fuse 2D global image features and 3D local joint features for better 2D&3D alignment. - A 3D joint contrastive learning strategy with two losses - joint-to-joint and joint-to-non-joint contrast - to globally supervise the 3D space.- State-of-the-art results on occluded 3D human pose/mesh datasets like 3DPW, 3DPW-OC, 3DPW-PC etc. demonstrating the effectiveness of their approach.In summary, the key research question is how to improve 3D human mesh recovery under occlusion, with the main hypotheses relating to fusing 2D/3D features and adding global 3D supervision via contrastive learning. The proposed JOTR method aims to address this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel framework called JOTR (3D Joint Contrastive Learning with Transformers) for recovering 3D human mesh from a single image, especially under occluded conditions. 2. Using an encoder-decoder transformer architecture to fuse 2D global features and 3D local features, which helps achieve both 2D and 3D alignment of the predicted mesh. This overcomes limitations of previous methods that relied mainly on 2D features/alignment.3. Introducing a 3D joint contrastive learning strategy with two contrastive losses - joint-to-joint and joint-to-non-joint contrast. This provides explicit global supervision for the 3D space to distinguish the human joints from occlusions/background. 4. Achieving state-of-the-art results on several benchmark datasets - 3DPW, 3DPW-OC, 3DPW-PC, 3DOH, 3DPW-Crowd, CMU Panoptic. The method shows significant improvements especially for occluded human mesh recovery.5. Providing detailed experiments and ablation studies to demonstrate the effectiveness of the proposed framework and contrastive learning strategy.In summary, the key novelties seem to be using transformers to fuse 2D and 3D features, and the 3D joint contrastive learning objective to globally supervise the 3D space. This results in improved performance on occluded human mesh recovery.
