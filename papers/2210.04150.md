# [Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP](https://arxiv.org/abs/2210.04150)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we improve the performance of two-stage approaches for open-vocabulary semantic segmentation by adapting the pre-trained CLIP model to better handle masked image regions?The key hypotheses are:1) Pre-trained CLIP does not perform well on masked image regions, making it the performance bottleneck in two-stage approaches.2) Finetuning CLIP on diverse mask-category pairs mined from captions can help retain its generalization ability better than using segmentation labels.3) Proposed mask prompt tuning can effectively adapt CLIP to masked images without changing its weights, enabling multi-task sharing.4) The proposed adapted CLIP model can significantly boost two-stage approaches and achieve new state-of-the-art results on open-vocabulary segmentation benchmarks.In summary, the central research question is how to improve two-stage open-vocabulary segmentation by identifying and overcoming the bottleneck of using pre-trained CLIP for masked image classification. The key hypotheses aim to adapt CLIP to masked images using novel data collection and prompt tuning strategies.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Identifying the performance bottleneck of current two-stage open-vocabulary segmentation methods as the pre-trained CLIP model, which does not work well on masked images. 2. Collecting diverse mask-category training pairs by mining captions to retain CLIP's generalization ability when adapting it to masked images.3. Proposing a mask prompt tuning technique to adapt CLIP for masked images without changing its weights, enabling multi-task weight sharing. 4. Showing for the first time that open-vocabulary generalist models can match the performance of supervised specialist models on segmentation benchmarks.The key ideas are analyzing the bottleneck of two-stage methods, collecting better training data from captions, and proposing mask prompt tuning to adapt CLIP to masked images while retaining its generalization ability. The main result is that properly adapted generalist models can now match specialist models on semantic segmentation.
