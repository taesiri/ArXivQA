# [QKFormer: Hierarchical Spiking Transformer using Q-K Attention](https://arxiv.org/abs/2403.16552)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing spiking transformers suffer from two main issues: 1) The spike-form self-attention used in previous works has quadratic complexity to the number of tokens, making it difficult to explore hierarchical architectures. 2) Previous spiking transformers use straight-through structures without hierarchical representation. 

Proposed Solution:
This paper proposes a novel spiking transformer called QKFormer with the following innovations:

1. A spike-form Q-K attention mechanism that has linear complexity and models the importance of tokens/channels through binary vectors. It uses two components: Query (Q) and Key (K).

2. A Patch Embedding module with Deformed Shortcut (PEDS) that enhances information transmission in spiking transformers.

3. A hierarchical architecture that produces multi-scale spiking representations, starting from small patches and gradually merging neighboring patches in deeper layers. This is enabled by the efficient Q-K attention.

Main Contributions:

1. The Q-K attention mechanism that has much lower complexity than prior spike-form attentions, making hierarchical architectures feasible.

2. The design of the PEDS module that brings significant performance gains. 

3. QKFormer - the first hierarchical spiking transformer that produces multi-scale spiking representations through the proposed Q-K attention and PEDS.

4. State-of-the-art results on ImageNet, CIFAR, and neuromorphic datasets. Notably, QKFormer exceeds 85% top-1 accuracy on ImageNet for the first time among directly trained SNNs.

In summary, this paper makes spiking transformers more powerful through novel spike-form attention and architectures, achieving new milestones for event-driven models on complex vision tasks. The innovations pave the way for deploying spiking neural networks to low-power hardware.
