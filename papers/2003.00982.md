# [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)

## What is the central research question or hypothesis that this paper addresses?

This paper presents a benchmarking framework for evaluating and comparing different graph neural network (GNN) architectures. The key goals and contributions of the paper are:- To develop a diverse collection of medium-scale graph datasets, both from real-world sources and synthetically generated, to enable rigorous evaluation of GNN models. The datasets cover tasks like node classification, graph classification, link prediction etc. across domains like social networks, chemistry, combinatorial optimization etc. - To design a standardized coding infrastructure and training/evaluation protocol for fair and reproducible comparison of GNN models with consistent hyperparameter settings and compute budgets. This addresses a key limitation in prior GNN literature where models have been compared inconsistently.- To demonstrate how the benchmark can be used to conveniently explore new ideas and develop insights to advance GNN research. As a case study, the paper shows how the framework helped develop the idea of using Laplacian eigenvectors as graph positional encodings to address limitations of message-passing GCNs.- To provide an easily extensible, open-source framework to the community to facilitate benchmarking of existing and new GNN models in a rigorous experimental setup.In summary, the central research focus is on developing a rigorous benchmarking suite and framework to systematically evaluate and advance graph neural network research, which has been lacking in the field. The diverse datasets, codebase and case studies demonstrate the utility of the proposed benchmark.
