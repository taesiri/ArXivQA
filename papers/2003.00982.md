# [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)

## What is the central research question or hypothesis that this paper addresses?

This paper presents a benchmarking framework for evaluating and comparing different graph neural network (GNN) architectures. The key goals and contributions of the paper are:- To develop a diverse collection of medium-scale graph datasets, both from real-world sources and synthetically generated, to enable rigorous evaluation of GNN models. The datasets cover tasks like node classification, graph classification, link prediction etc. across domains like social networks, chemistry, combinatorial optimization etc. - To design a standardized coding infrastructure and training/evaluation protocol for fair and reproducible comparison of GNN models with consistent hyperparameter settings and compute budgets. This addresses a key limitation in prior GNN literature where models have been compared inconsistently.- To demonstrate how the benchmark can be used to conveniently explore new ideas and develop insights to advance GNN research. As a case study, the paper shows how the framework helped develop the idea of using Laplacian eigenvectors as graph positional encodings to address limitations of message-passing GCNs.- To provide an easily extensible, open-source framework to the community to facilitate benchmarking of existing and new GNN models in a rigorous experimental setup.In summary, the central research focus is on developing a rigorous benchmarking suite and framework to systematically evaluate and advance graph neural network research, which has been lacking in the field. The diverse datasets, codebase and case studies demonstrate the utility of the proposed benchmark.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an open-source benchmarking framework for evaluating and comparing graph neural networks (GNNs) in a fair and consistent manner. The key aspects of the framework are:- It comprises a diverse collection of 12 medium-scale graph datasets, including both real-world graphs (e.g. social networks, molecules) and synthetically generated graphs based on mathematical models (e.g. graphs with cycles, stochastic block models). - The datasets cover the three main graph learning tasks - node classification, graph classification, and link prediction. This allows benchmarking GNN architectures on different tasks.- It enables fair comparison of GNN models by evaluating them under the same computational budgets (fixed number of parameters). This helps identify key architectural designs vs just model size effects.- The modular code infrastructure based on PyTorch and DGL allows convenient prototyping and experimentation. It has defined data loaders, model layers, training loops, evaluation metrics etc.- The framework has been widely adopted by the community with 2000+ stars on GitHub, demonstrating its utility in advancing GNN research. The authors demonstrate this by using the framework to study graph positional encodings.In summary, the main contribution is an open benchmarking suite to facilitate rigorous, fair and consistent evaluation of graph neural networks on diverse tasks and datasets. This enables the community to better understand these models, identify effective designs, and drive further progress.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents an open-source benchmarking framework for evaluating and comparing graph neural networks on a diverse collection of medium-scale graph datasets across multiple tasks, with the goal of facilitating fair comparisons and driving progress in graph representation learning research.In short, the paper introduces a benchmark for graph neural networks to enable standardized evaluation and progress tracking in the field.


## How does this paper compare to other research in the same field?

This paper presents a benchmarking framework for evaluating and comparing graph neural networks (GNNs) on diverse datasets. Here are some key ways it compares to other GNN benchmarking research:- Scope of benchmark: It includes 12 datasets spanning real-world and synthetic graphs for graph regression, node classification, graph classification, and link prediction tasks. This is more comprehensive than many prior GNN benchmarks focused on just 1-2 tasks. - Rigor of evaluation: The framework enforces consistent experimental settings such as fixed model parameter budgets, training procedures, and performance metrics for fair comparisons. Many prior works lacked such rigorous controls.- Code implementation: The benchmark provides a modular open-source codebase in PyTorch/DGL for easy adoption. Other benchmarks often only provide guidelines without shareable code.- Model coverage: It benchmarks prominent GNN architectures including GCNS, GraphSage, GAT, GatedGCN as well as recent models like 3WLGNN and RingGNN. Many works only evaluate 1 or few models.- Insight generation: The paper demonstrates how the benchmark can be used to explore weaknesses of GNNs, such as lack of positional encoding, and improve designs. Most works focus on reporting metrics without such analysis.Overall, the scope, rigor, and extensibility of this benchmark seem greater than most prior GNN evaluation studies. The codebase lowers the barrier to adopt these best practices. The insights on limitations of GNNs are also novel contributions. This sets a higher standard for systematic, reproducible, and insightful evaluation and comparison of graph neural networks.


## What future research directions do the authors suggest?

The paper "Benchmarking Graph Neural Networks" suggests several interesting future research directions for graph neural networks:1. Developing more powerful and expressive graph neural network architectures. The paper shows that anisotropic models like GAT and GatedGCN perform better than isotropic models, so further exploring attention and gating mechanisms seems promising. Developing architectures that can match the expressiveness of WL-GNNs while retaining efficiency is also an important goal.2. Exploring alternatives to current graph positional encoding schemes. The paper demonstrates the usefulness of Laplacian eigenvectors as positional encodings, but notes challenges like sign ambiguity. Coming up with more principled and powerful encodings could further improve GNN performance.3. Redesigning deep learning best practices like batching and normalization schemes for WL-GNNs. Making dense variable-size data compatible with techniques like batch normalization could help stabilize WL-GNN training.4. Developing suitable medium-to-large scale benchmark graph datasets from diverse domains. The paper highlights the need for larger benchmarks that can properly evaluate complex GNN architectures. Criteria need to be developed for assessing dataset quality and representativeness. 5. Scaling up experiments to bigger models and graphs using new hardware or methods. Testing the limits of different GNN architectures may reveal new insights.6. Extending the benchmark codebase with additional datasets, tasks, and flexibility in training procedures. This could allow more extensive exploration of novel GNN ideas.In summary, the key future directions involve developing more powerful and efficient GNN architectures, creating representative benchmark datasets at scale, and extending the open-source codebase for convenient experimentation. Advancing these research threads can drive further progress in graph representation learning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a benchmarking framework for graph neural networks (GNNs) that includes a diverse collection of 12 medium-scale graph datasets, both real-world and synthetically generated, across domains like chemistry, social networks, computer vision, and combinatorial optimization. The framework provides a standardized codebase in PyTorch/DGL for training and evaluating various GNN architectures like Graph Convolutional Networks (GCNs), Graph Attention Networks, and Weisfeiler-Lehman based GNNs on these datasets using a consistent methodology with fixed parameter budgets for fair comparison. Through extensive experiments, the paper demonstrates that anisotropic GCNs like GAT and GatedGCN perform the best overall, while recent WL-GNNs face challenges in training stability, scalability and memory usage which limits their effectiveness on the proposed benchmark suite. The paper also shows how the framework can be leveraged to gain new insights, like using Laplacian eigenvectors as positional encodings in GCNs, and adding explicit edge representations in GCNs for link prediction tasks. Overall, the benchmarking framework and results provide an empirical perspective on trade-offs between different GNN architectures and shed light on principles for robust graph representation learning.


## Summarize the paper in two paragraphs.

Here are two paragraph summaries of the key points from the paper:The paper presents an open-source benchmarking framework for comparing and evaluating different graph neural network (GNN) architectures. The framework includes a diverse collection of 12 medium-scale graph datasets spanning real-world domains like chemistry, social networks, and computer vision, as well as synthetic mathematical graphs. A modular software infrastructure built on PyTorch and DGL allows for easy prototyping and fair comparison of GNN models under controlled experimental settings with fixed computational budgets. The goal is to facilitate rigorous benchmarking to uncover architectural designs and principles that generalize across tasks. The paper demonstrates the utility of the framework through sample studies on graph positional encodings using Laplacian eigenvectors and edge representations in GNNs. Since its initial release in March 2020, the open-source framework has gained wide adoption by the GNN research community with over 2000 GitHub stars.The key contributions of the paper are three-fold. First, it presents the design and compilation of a diverse set of medium-scale graphs for benchmarking GNNs. Second, it provides a modular software infrastructure with consistent training procedures to enable controlled experiments for fair comparison between models. Third, it demonstrates sample studies that can be carried out conveniently using the framework to gain insights and advance GNN architectures. For instance, the experiments highlight the limitations of early GNN designs in encoding positional information on graphs and showcase the benefits of using graph Laplacian eigenvectors as positional encodings. Overall, the paper delivers an open-source benchmarking suite to facilitate principled graph neural network research. Since its release, the framework has been widely adopted by researchers to prototype and evaluate novel GNN architectures, mechanics and training techniques.
