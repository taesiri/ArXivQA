# [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)

## What is the central research question or hypothesis that this paper addresses?

This paper presents a benchmarking framework for evaluating and comparing different graph neural network (GNN) architectures. The key goals and contributions of the paper are:- To develop a diverse collection of medium-scale graph datasets, both from real-world sources and synthetically generated, to enable rigorous evaluation of GNN models. The datasets cover tasks like node classification, graph classification, link prediction etc. across domains like social networks, chemistry, combinatorial optimization etc. - To design a standardized coding infrastructure and training/evaluation protocol for fair and reproducible comparison of GNN models with consistent hyperparameter settings and compute budgets. This addresses a key limitation in prior GNN literature where models have been compared inconsistently.- To demonstrate how the benchmark can be used to conveniently explore new ideas and develop insights to advance GNN research. As a case study, the paper shows how the framework helped develop the idea of using Laplacian eigenvectors as graph positional encodings to address limitations of message-passing GCNs.- To provide an easily extensible, open-source framework to the community to facilitate benchmarking of existing and new GNN models in a rigorous experimental setup.In summary, the central research focus is on developing a rigorous benchmarking suite and framework to systematically evaluate and advance graph neural network research, which has been lacking in the field. The diverse datasets, codebase and case studies demonstrate the utility of the proposed benchmark.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an open-source benchmarking framework for evaluating and comparing graph neural networks (GNNs) in a fair and consistent manner. The key aspects of the framework are:- It comprises a diverse collection of 12 medium-scale graph datasets, including both real-world graphs (e.g. social networks, molecules) and synthetically generated graphs based on mathematical models (e.g. graphs with cycles, stochastic block models). - The datasets cover the three main graph learning tasks - node classification, graph classification, and link prediction. This allows benchmarking GNN architectures on different tasks.- It enables fair comparison of GNN models by evaluating them under the same computational budgets (fixed number of parameters). This helps identify key architectural designs vs just model size effects.- The modular code infrastructure based on PyTorch and DGL allows convenient prototyping and experimentation. It has defined data loaders, model layers, training loops, evaluation metrics etc.- The framework has been widely adopted by the community with 2000+ stars on GitHub, demonstrating its utility in advancing GNN research. The authors demonstrate this by using the framework to study graph positional encodings.In summary, the main contribution is an open benchmarking suite to facilitate rigorous, fair and consistent evaluation of graph neural networks on diverse tasks and datasets. This enables the community to better understand these models, identify effective designs, and drive further progress.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents an open-source benchmarking framework for evaluating and comparing graph neural networks on a diverse collection of medium-scale graph datasets across multiple tasks, with the goal of facilitating fair comparisons and driving progress in graph representation learning research.In short, the paper introduces a benchmark for graph neural networks to enable standardized evaluation and progress tracking in the field.


## How does this paper compare to other research in the same field?

This paper presents a benchmarking framework for evaluating and comparing graph neural networks (GNNs) on diverse datasets. Here are some key ways it compares to other GNN benchmarking research:- Scope of benchmark: It includes 12 datasets spanning real-world and synthetic graphs for graph regression, node classification, graph classification, and link prediction tasks. This is more comprehensive than many prior GNN benchmarks focused on just 1-2 tasks. - Rigor of evaluation: The framework enforces consistent experimental settings such as fixed model parameter budgets, training procedures, and performance metrics for fair comparisons. Many prior works lacked such rigorous controls.- Code implementation: The benchmark provides a modular open-source codebase in PyTorch/DGL for easy adoption. Other benchmarks often only provide guidelines without shareable code.- Model coverage: It benchmarks prominent GNN architectures including GCNS, GraphSage, GAT, GatedGCN as well as recent models like 3WLGNN and RingGNN. Many works only evaluate 1 or few models.- Insight generation: The paper demonstrates how the benchmark can be used to explore weaknesses of GNNs, such as lack of positional encoding, and improve designs. Most works focus on reporting metrics without such analysis.Overall, the scope, rigor, and extensibility of this benchmark seem greater than most prior GNN evaluation studies. The codebase lowers the barrier to adopt these best practices. The insights on limitations of GNNs are also novel contributions. This sets a higher standard for systematic, reproducible, and insightful evaluation and comparison of graph neural networks.
