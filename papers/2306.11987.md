# [Training Transformers with 4-bit Integers](https://arxiv.org/abs/2306.11987)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable ultra-low precision 4-bit integer training for transformers. Specifically, it aims to develop a training method that uses only 4-bit integer matrix multiplications, while still achieving competitive accuracy compared to full precision training. The key hypotheses are:1. Activation outliers are a major cause of accuracy degradation when quantizing transformers to 4-bit integers. The paper proposes a Hadamard quantizer to suppress outliers and improve accuracy.2. Gradients have a structural sparsity that can be exploited - only a few rows have large values while most rows are near zero. The paper leverages this sparsity through bit splitting and leverage score sampling to quantize gradients more accurately.By carefully analyzing the activation and gradient structures in transformers and developing tailored quantization techniques, the paper shows it is possible to train transformers using only 4-bit integer matrix multiplications without significant accuracy loss. This enables more efficient training on contemporary hardware like GPUs.In summary, the core research question is how to enable accurate 4-bit integer training for transformers, with the key ideas being methods to handle activation outliers and leverage gradient sparsity. The paper aims to show ultra-low precision training is viable through tailored quantization schemes.
