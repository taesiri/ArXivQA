# [Training Transformers with 4-bit Integers](https://arxiv.org/abs/2306.11987)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable ultra-low precision 4-bit integer training for transformers. Specifically, it aims to develop a training method that uses only 4-bit integer matrix multiplications, while still achieving competitive accuracy compared to full precision training. The key hypotheses are:1. Activation outliers are a major cause of accuracy degradation when quantizing transformers to 4-bit integers. The paper proposes a Hadamard quantizer to suppress outliers and improve accuracy.2. Gradients have a structural sparsity that can be exploited - only a few rows have large values while most rows are near zero. The paper leverages this sparsity through bit splitting and leverage score sampling to quantize gradients more accurately.By carefully analyzing the activation and gradient structures in transformers and developing tailored quantization techniques, the paper shows it is possible to train transformers using only 4-bit integer matrix multiplications without significant accuracy loss. This enables more efficient training on contemporary hardware like GPUs.In summary, the core research question is how to enable accurate 4-bit integer training for transformers, with the key ideas being methods to handle activation outliers and leverage gradient sparsity. The paper aims to show ultra-low precision training is viable through tailored quantization schemes.


## What is the main contribution of this paper?

Here is a summary of the main contributions of this paper:- The paper proposes a novel 4-bit integer (INT4) training algorithm for transformers. All the computationally expensive matrix multiplications (MMs) during training are quantized to INT4, which can accelerate training on hardware like GPUs that support INT4 arithmetic. - For forward propagation, the paper identifies that outliers in activations are the main cause of accuracy loss when quantizing to ultra-low bitwidths like 4-bit. To handle outliers, they propose a Hadamard quantizer (HQ) which transforms the activation matrix before quantization to spread and suppress outliers.- For backpropagation, the paper leverages the observation that activation gradients are structurally sparse - only a few rows have large values while most rows are near zero. To compute gradients accurately and efficiently, they propose bit splitting to represent gradients in higher and lower 4 bits, and leverage score sampling to only compute the most important entry-wise matrix multiplications.- Experiments show their proposed INT4 training algorithm achieves competitive accuracy compared to prior 4-bit training works, on tasks like language modeling, machine translation, image classification. Unlike prior works, their method is hardware-friendly and does not require custom number formats.- They implement a prototype on GPUs to demonstrate the potential for acceleration. The INT4 matrix multiplication operators are up to 2.2x faster than FP16 versions, and end-to-end training throughput is improved by 26-35% for training large language models.In summary, the key contribution is a hardware-friendly 4-bit training algorithm for transformers, enabled by novel quantization techniques to handle activation outliers and leverage gradient sparsity. The method achieves good accuracy without custom number formats, and has the potential to accelerate training significantly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a novel INT4 training algorithm for transformers that uses a Hadamard quantizer and leverage score sampling techniques to achieve competitive accuracy compared to prior 4-bit training methods, while being compatible with contemporary GPU hardware and providing up to 2.2x speedup over FP16 matrix multiplication.
