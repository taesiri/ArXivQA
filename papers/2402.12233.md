# [Empirical Study on Updating Key-Value Memories in Transformer   Feed-forward Layers](https://arxiv.org/abs/2402.12233)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies how information is processed and stored in the feed-forward networks (FFNs) of Transformer models. FFNs are conceptualized as key-value memories, where the keys determine which values (stored knowledge concepts) get activated for a given input. The question is - what is the best way to update this knowledge? Should we directly modify the values or change the keys to alter which values get activated?

Methodology:
The authors compare updating the keys vs values in FFN layers for various tuning tasks:
- Knowledge editing: Changing specific facts stored in the model while preserving irrelevant information 
- Multi-task tuning: Tuning on multiple datasets/tasks jointly
- Instruction tuning: Tuning model for a specific natural language instruction 

The tuning is done by either:
1) Updating values (directly changing stored knowledge) 
2) Updating keys (changing which values are activated)

The comparison is done on multiple models - GPT-J, GPT2-XL, LLaMA.

Key Results:
- Updating keys consistently outperforms updating values across models and tasks
- Modifying keys gives better generalization and specificity in knowledge editing
- Updating keys is more parameter efficient 
- Tuning keys is much faster compared to directly modifying values

Main Conclusions:
The key insight is that compared to directly changing the knowledge concepts stored in the model, altering the mechanism that controls this knowledge (i.e. keys) is more effective for tuning. This highlights that transformers use the FFN keys to gate the usage of knowledge rather than just storing facts in the values.

In summary, the paper provides an empirical comparison of different ways to update information in transformer feed-forward layers, demonstrating the superiority of modifying keys over values.
