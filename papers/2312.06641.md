# [Online Decision Making with History-Average Dependent Costs (Extended)](https://arxiv.org/abs/2312.06641)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper studies online sequential decision making where the costs depend on the time average of past decisions over a finite history horizon. This problem is recast as online optimization with stage-wise constraints coupling the decisions across time steps. The authors propose a novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm which uses history-dependent regularizers to enforce constraints while achieving low regret. They show that when the full history affects costs, no algorithm can achieve sublinear regret. However, when history length scales sublinearly as $o(T)$, their method achieves $O(\sqrt{TH})$ regret. The adaptive regularizers play a key role in both enforcing constraints and bounding regret. Experiments on synthetic data demonstrate superior performance over prior methods on heterogeneous and cyclic costs, while matching performance on identical stochastic costs. Overall, the paper provides important theoretical and algorithmic insights into online learning problems with historical dependence through averaging.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
The paper studies online sequential decision making where the cost incurred by the learner at each time step depends not just on their current action but also on the average of their past actions over a finite history horizon. This dependence introduces constraints that couple the learner's actions across time steps. The key questions addressed are:

(i) How can a learner make good decisions when costs depend on historical averages? 

(ii) How does this history coupling through averaging translate to constraints on the learner's actions?

(iii) How long can this history horizon be for the learner to still perform well?

Proposed Solution:
The problem is recast as an online optimization problem with stage-wise constraints. The paper proposes a novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm that uses history-dependent regularizers to explicitly account for the constraints while making decisions. This allows history averaging to be appropriately incorporated into the online learning framework.

Main Contributions:

- Introduces the problem of online learning with finite history average dependent costs and constraints. Connects it to learning with state dynamics.

- Proposes the innovative FTARL algorithm that employs history-based adaptive regularizers to handle constraints. Establishes regret bounds for FTARL.

- Shows that sublinear regret is impossible when full history affects costs. With history length in $o(T)$, proves a $\tilde{O}(\sqrt{TH})$ regret for FTARL.

- Provides simulation results that validate FTARL's strong performance against baseline algorithms from literature. Overall, the paper makes both theoretical and empirical contributions towards online learning with historical averaging constraints.


## Summarize the paper in one sentence.

 This paper studies online sequential decision making where the costs depend on the time average of past decisions over a finite history horizon, proposes an adaptive regularization algorithm to achieve sublinear regret bounds that scale with the square root of the horizon length, and shows no algorithm can attain sublinear regret when the history length scales linearly with the time horizon.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Recasting the problem of online sequential decision making with history-dependent payoffs as a problem of online optimization with stage-wise constraints. Specifically, the paper considers dependence on history that emerges solely through averaging and stage costs that rely explicitly on the average of past decisions.

2. Proposing a novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm that incorporates adaptive regularizers that depend explicitly on past decisions. This allows enforcing stage-wise constraints while enabling tight regret bounds to be established. 

3. Analyzing the implications of the length of history horizon on the design of no-regret algorithms. The paper shows that when stage costs depend on the full history, no algorithm can achieve sublinear regret. However, when the history dependence is restricted to a shorter horizon (specifically when H is in o(T)), the proposed FTARL algorithm can achieve O(sqrt(TH)) regret.

4. Presenting experimental results on synthetically generated data showing that the FTARL algorithm outperforms prior methods in some cases. The experiments also provide some insight into the tightness of the established regret bounds.

In summary, the main contribution is the novel FTARL algorithm and analysis showing that good regret bounds are possible for sequential decision making problems with historical dependence on past decisions emerging through averaging.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Sequential decision making
- Online optimization 
- History-dependent costs
- Time-averaged decisions
- Stage-wise constraints
- Follow-the-Adaptively-Regularized-Leader (FTARL) algorithm
- Regret bounds
- Prediction from Expert Advice
- Online learning with memory

The paper considers the problem of online sequential decision making where the costs depend on the time average of past decisions. It recasts this as an online optimization problem with stage-wise constraints. The proposed FTARL algorithm uses adaptive regularizers that depend on past decisions to enforce these constraints while achieving sublinear regret bounds. Key results include regret analysis for the algorithm and studying the effect of the length of the history horizon. Overall, it provides an approach to online learning problems where decisions are coupled across time through averaging.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm. How is the adaptive regularizer in FTARL designed differently compared to regularizers in standard Follow-The-Regularized-Leader algorithms? What is the motivation behind this difference?

2. Remark 1 in the paper states that the decisions $\{x^t\}_{t=1}^T$ induced by the FTARL algorithm satisfy the constraint in equation (3). Walk through the key steps in the proof of this remark to understand why the adaptive regularizer ensures this constraint is respected. 

3. The regret bound for FTARL in Theorem 2 has a linear dependence on the history horizon H. Compare and contrast this to the regret bound that would be achieved in the absence of any history dependence. What causes this linear scaling with H?

4. Claim 1 presents a thought experiment where no algorithm can achieve sublinear regret when the history horizon H scales linearly with T. Explain the construction of this thought experiment and discuss its implications on the choice of H.  

5. The design of the adaptive regularizer in Section IV-A involves a sequence $v^t_*$ generated according to a Follow-the-Perturbed-Leader style update in equation (10). What is the motivation behind using this perturbation based update within the regularizer? 

6. Walk through the key steps in the proof of Theorem 2 to understand how the use of the adaptive regularizers allows one to establish regret guarantees for FTARL. What challenges arise in this analysis from the history dependence?

7. Compare and contrast the regret guarantees for FTARL presented in this paper versus prior work such as the Low Switching Algorithm. What practical benefits might FTARL offer over these other approaches?

8. How does Corollary 1 provide regret guarantees for FTARL that do not explicitly depend on knowing the true horizon length H? Why might this be useful in practice?

9. The experimental results showcase superior empirical performance for FTARL over the Low Switching Algorithm on certain cost function classes. Speculate on why FTARL outperforms for these cost functions but matches performance for others.

10. This paper focused exclusively on history dependence via averaging. How might one extend the FTARL framework to handle more complex types of history dependence? What new challenges might arise?
