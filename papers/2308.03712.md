# [Scaling may be all you need for achieving human-level object recognition   capacity with human-like visual experience](https://arxiv.org/abs/2308.03712)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can current self-supervised learning methods reach human-level visual object recognition capabilities if model size, training data size, and input image resolution are scaled up to be more comparable to humans, using generic architectures and learning algorithms?The key hypothesis appears to be that human-level visual object recognition abilities can be achieved by simply scaling up model size, training data size, and input resolution simultaneously, without needing more specialized model architectures or learning algorithms. The authors test this by conducting experiments training vision transformer models of varying sizes on different amounts of human-like egocentric video data and at different image resolutions. They model how accuracy on ImageNet scales with these factors to project the model size, data size and resolution needed to reach human-level performance. Their main conclusion is that human-level accuracy seems achievable at sub-human scales of these resources using current self-supervised approaches.


## What is the main contribution of this paper?

The main contribution of this paper is a scaling experiment that considers the simultaneous scaling of model size, training data size, and image resolution for vision transformers trained with masked autoencoders. The key findings are:- Current self-supervised learning methods like masked autoencoders can reach human-level visual object recognition performance at sub-human scales of model size, data size, and image resolution if these factors are scaled up together. - In particular, the paper estimates that a 2.5B parameter vision transformer model trained on around 20,000 hours of human-like video data with 952x952 pixel images should be able to achieve roughly human-level accuracy on ImageNet classification. - This suggests that human-level visual competence is achievable from human-like visual experience using generic deep learning algorithms, without needing very large models or datasets or strong inductive biases.- The efficiency of masked autoencoders makes it feasible to run scaling experiments on large models and datasets without massive computing resources.In summary, the key contribution is demonstrating the potential to reach human-level visual recognition through simultaneous scaling of model size, data size, and input resolution using a simple self-supervised learning approach. The scaling analysis provides an estimate of how these factors need to be scaled up together.
