# [All You Need is Beyond a Good Init: Exploring Better Solution for   Training Extremely Deep Convolutional Neural Networks with Orthonormality and   Modulation](https://arxiv.org/abs/1703.01827)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper tries to address is how to train extremely deep convolutional neural networks (with over 100 layers) from scratch without using residual/shortcut connections. The key hypotheses are:1) Maintaining approximate orthogonality between filters in convolutional layers can help mitigate gradient vanishing/exploding during backpropagation and improve training of very deep plain networks.2) Modulating the magnitudes of backpropagated errors between layers based on second-order moment statistics can further stabilize training. Together, enforcing orthogonality constraints and error signal modulation allow successful end-to-end training of plain networks with over 100 layers, achieving comparable performance to residual networks.In summary, the paper explores new techniques like orthonormal regularization and error modulation to enable training of very deep feedforward CNNs, providing an alternative to shortcut connections for dealing with optimization difficulties in extreme network depth.
