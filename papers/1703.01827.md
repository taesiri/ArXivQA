# [All You Need is Beyond a Good Init: Exploring Better Solution for   Training Extremely Deep Convolutional Neural Networks with Orthonormality and   Modulation](https://arxiv.org/abs/1703.01827)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper tries to address is how to train extremely deep convolutional neural networks (with over 100 layers) from scratch without using residual/shortcut connections. The key hypotheses are:1) Maintaining approximate orthogonality between filters in convolutional layers can help mitigate gradient vanishing/exploding during backpropagation and improve training of very deep plain networks.2) Modulating the magnitudes of backpropagated errors between layers based on second-order moment statistics can further stabilize training. Together, enforcing orthogonality constraints and error signal modulation allow successful end-to-end training of plain networks with over 100 layers, achieving comparable performance to residual networks.In summary, the paper explores new techniques like orthonormal regularization and error modulation to enable training of very deep feedforward CNNs, providing an alternative to shortcut connections for dealing with optimization difficulties in extreme network depth.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a method to train extremely deep plain convolutional neural networks (CNNs) without skip connections using orthonormality and modulation. 2. It introduces an orthonormal regularizer to replace traditional weight decay regularization to keep the filters in each layer as orthogonal as possible during training. This helps propagate signals stably in the backward pass.3. It designs a backward error modulation mechanism based on the quasi-isometry assumption between consecutive layers. This helps maintain the magnitude of errors during backpropagation to prevent vanishing/exploding gradients. 4. Experiments show the proposed methods can train a 110-layer plain CNN from scratch and achieve 81.6% accuracy on CIFAR-10, outperforming other optimization methods. 5. With the proposed techniques, plain CNNs can match or exceed the performance of residual networks of similar depth on CIFAR-10 and ImageNet.6. The paper provides insights into understanding the training dynamics and degradation problem in optimizing extremely deep plain CNNs. The orthonormality and modulation principles are shown to be effective in alleviating this problem.In summary, the key contribution is proposing orthogonal regularization and error modulation techniques to enable stable training of very deep plain CNNs without skip connections, which has been difficult previously. The paper offers useful insights into deep network optimization.
