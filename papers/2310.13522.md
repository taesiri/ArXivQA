# [Teaching Language Models to Self-Improve through Interactive   Demonstrations](https://arxiv.org/abs/2310.13522)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we effectively train smaller language models to learn to self-improve and achieve better performance on mathematical and reasoning tasks?

The key points are:

- Recent work has shown that large language models (LLMs) can be prompted to self-improve by generating feedback on their own outputs and revising them. However, smaller models do not exhibit this capability.

- The paper proposes an algorithm called TriPosT to train smaller models to learn to self-improve through interactive experiences with LLMs. 

- The key idea is to have the smaller model generate initial attempts, then use LLMs to provide tailored feedback and improvements on those attempts. This experience is replayed to the smaller model during training.

- Experiments on math and reasoning datasets show TriPosT-trained smaller models can learn to self-improve and outperform models trained on just ground truth data or LLM demonstrations.

- Analysis indicates that learning from correcting its own mistakes is crucial for the smaller model, and that generating useful feedback/improvements is harder than producing a correct answer directly.

So in summary, the central hypothesis is that smaller models can learn to self-improve through interactive experiences tailored to their capability, which can improve their performance on mathematical and logical reasoning tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an algorithm called TriPosT to train smaller language models to perform self-improvement on math and reasoning tasks. 

Specifically, the key ideas are:

- Using the smaller model to interact with large language models (LLMs) to generate trajectories of attempts, feedbacks, and improvements tailored to the smaller model's capability. 

- Processing the collected trajectories by filtering incorrect ones and rebalancing the data to prevent over-attempting self-improvement.

- Training the smaller model on the processed trajectories using weighted supervised learning to focus on learning to generate useful feedback and improvements.

The authors show that models trained with TriPosT can learn to self-improve and achieve better performance on math and reasoning tasks compared to models trained on just rationales or LLM demonstrations. A key finding is that learning from mistakes and improvements tailored to its own capability is crucial for the smaller model.

In summary, the main contribution is an effective algorithm to train smaller models to perform task-specific self-improvement, closing the gap with large models on math/reasoning tasks. The interactive data collection and training process enables smaller models to learn from their own mistakes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the main point of this paper:

This paper proposes an iterative training algorithm called TriPosT that teaches smaller language models to self-improve on math and reasoning tasks by having them interact with large language models to learn from and correct their own mistakes.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related work:

- The key contribution of this paper is presenting a new training algorithm called TriPosT to teach smaller language models the ability to self-improve through interactive experiences. This builds on prior work showing that large language models can learn to self-improve when prompted, but that this ability is more difficult for smaller models. 

- The proposed TriPosT algorithm has some similarities to knowledge distillation, where smaller models are trained to mimic larger models. However, a key difference is that the demonstrations for the smaller model are generated interactively based on its own outputs, rather than just being static expert demonstrations. This interactive element is novel.

- The idea of training models to self-improve has been explored in some prior works, but they often train separate critic/corrector models or rely on rules/human feedback. A strength of this work is showing that interactive experiences with LLMs can teach self-improvement abilities without separate components.

- Using LLMs to provide training signal, whether through demonstrations or interactive experiences, is gaining more traction recently as a way to improve capabilities of smaller models. This work provides a nice example of how interactive experiences can be leveraged.

- The experimental validation on mathematical/logical reasoning tasks from BIG-Bench demonstrates clear improvements from the proposed TriPosT training. The analyses investigating how the training procedures lead to better self-improvement are insightful.

- One limitation acknowledged is the reliance on large models like Codex to generate the interactive experiences. Reducing this dependence could make the approach more widely usable.

Overall, the interactive training approach seems novel compared to prior work, and the paper makes nice contributions demonstrating how smaller models can learn to self-improve from experiences with LLMs. The analyses provide useful insights into this process.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Extend the experiments to use models with more than 7 billion parameters, now that techniques like flash attention can help reduce memory usage during training. The authors mainly tested LLaMA-7B due to computational constraints.

- Apply the approach to other tasks beyond math and reasoning, such as knowledge-grounded dialogue and dialogue safety. The current experiments focused on mathematical and logical reasoning tasks from BIG-Bench.

- Improve the feedback and improvement modules to handle more iterations of the algorithm. The authors found the expert LLMs struggled to provide useful feedback after 2-3 rounds, limiting the number of iterations.

- Explore other data formats or training techniques to focus model learning, beyond the (input, output) pairs. The results indicate generating feedback and improvements is much harder than just producing the final answer.

- Investigate how to make large language models more accessible, to reduce the tradeoff between using more capable LLMs vs. cost. Relying on large LLMs creates a practical limitation.

- Study other methods to encourage the model to learn from its mistakes and avoid unneeded self-improvement attempts. The authors suggest boosting algorithms or voting schemes as possible directions.

In summary, the main future directions are around scaling up the approach, applying it more broadly, improving the training process, and reducing reliance on large LLMs. The core idea of learning through self-critiquing seems promising but still needs more development.
