# [Neural Networks Learn Statistics of Increasing Complexity](https://arxiv.org/abs/2402.04362)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
This paper investigates the phenomenon of neural networks first learning simple statistics of the training data distribution before progressing to more complex ones, known as the distributional simplicity bias (DSB). The authors argue there is a need for more direct empirical evidence of this bias across commonly used models and training setups. 

Proposed Solution
The authors propose two criteria to determine if a model relies only on low-order statistics: (1) editing the low-order statistics should cause the model to treat examples as being from the distribution with the modified statistics, and (2) deleting higher-order statistics should not affect performance. They use optimal transport theory and maximum entropy principles to generate synthetic datasets to test these criteria.

For images, they match first and second order statistics across classes using coordinatewise quantile normalization and Gaussian optimal transport. They also sample from maximum entropy Gaussian and uniform distributions matching the first or second order statistics. 

For language, they prove an equivalence between token n-gram frequencies and sequence embedding moments. They sample maximum entropy sequences matching n-gram statistics and evaluate language models on these.

Contributions
- Provide empirical evidence for distributional simplicity bias in major image classification models and language models, using proposed criteria and synthetic datasets. 
- Show common non-monotonic generalization curves where performance on low-order statistics peaks then declines.
- Discover "double descent" in language models, attributed to in-context learning allowing recovery of performance.
- Propose efficient sampling methods to generate maximum entropy datasets matching low-order statistics.
- Theoretically connect n-gram frequencies to embedding space moments.

In summary, the paper strengthens the evidence for neural networks prioritizing simple statistics early in training through extensive experiments and proposes useful methods and theories towards studying this phenomenon.


## Summarize the paper in one sentence.

 This paper provides evidence that neural networks learn to exploit lower-order statistics (e.g. means and covariances) of data distributions before higher-order statistics (e.g. skewness and kurtosis), and proposes methods to test networks on their reliance on statistics of different orders.


## What is the main contribution of this paper?

 This paper presents new evidence for the distributional simplicity bias (DSB) in neural networks. The main contributions are:

1) The authors propose two criteria for assessing whether a model is using statistics up to a given order: (a) grafting low-order statistics from one class onto another should cause the model to treat the data as belonging to the target class, and (b) deleting higher-order statistics should not significantly impact model performance.

2) They describe methods for generating synthetic datasets to test these criteria, using optimal transport theory to graft statistics and maximum entropy principles to delete higher-order statistics. 

3) They find empirical evidence across image classification and language modeling tasks that neural networks initially rely more on low-order statistics, before becoming sensitive to higher-order statistics later in training. This non-monotonic dependence on data statistics provides support for the DSB.

4) They extend the DSB to discrete domains by proving an equivalence between token n-gram frequencies and statistical moments of embedded sequences.

In summary, the main contribution is strengthening the evidence for distributional simplicity biases in neural network learning dynamics using novel theoretical concepts and empirical methodology.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Distributional simplicity bias (DSB): The hypothesis that neural networks learn to exploit lower-order statistics (e.g. means, variances) of the input distribution before higher-order statistics (e.g. skewness, kurtosis).

- Moments: The paper shows a connection between the expected loss of a neural network and the moments (mean, variance, etc.) of the input distribution, providing some motivation for the DSB. 

- Optimal transport: Used to operationalize the concept of "grafting" the statistics from one data distribution onto another, to test if networks rely on those statistics. 

- Maximum entropy sampling: Used to generate synthetic data matching the low-order statistics of the real data, to test if networks are robust to deleting high-order statistics.

- $n$-gram statistics: The paper proves an equivalence between token $n$-gram frequencies and the moments of embedded token sequences, extending the DSB analysis to language models.

- Non-monotonicity: Many results show a U-shaped accuracy curve on synthetic data, indicating networks first rely on but then move beyond low-order statistics.

- Double descent: The language modeling results show an initial descent then increase then second descent in loss, explained by in-context learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1) The paper proposes two criteria that indicate whether a model is using statistics up to a given order. Could additional criteria be developed to provide further evidence for the distributional simplicity bias? For example, are there interventions beyond optimal transport that would also test sensitivity to low-order statistics?

2) The paper shows an equivalence between n-gram statistics and moments for discrete sequences. Does a similar equivalence hold for other types of discrete structures, like graphs or trees? Could the distributional simplicity bias be tested in graph neural networks using ideas from this paper?

3) The maximum entropy sampling method with box constraints requires custom optimization methods. Are there opportunities to leverage connections to maximum entropy models from statistics and information theory to develop more principled sampling algorithms? 

4) The double descent phenomenon observed in the language modeling experiments is fascinating. What explains the later performance recovery? Is it solely due to in-context learning or are there other factors? 

5) How sensitive are the results to hyperparameters like batch size, learning rate schedules, and optimizer choice? Are simplicity biases invariant to scale or do they depend on optimization details?

6) Can the tools developed here be used to gain insight about simplicity biases in self-supervised and semi-supervised learning? Do the same patterns hold when networks are trained on different kinds of objectives?

7) Many recent state-of-the-art vision models use attention mechanisms. How do attention maps change during training and how does this relate to the use of low-order vs high-order statistics?

8) The computation requirements limit the approach to small images. Could approximations like Nystr√∂m methods make it feasible to run experiments on higher resolution datasets like ImageNet?

9) The paper studies discriminative models like classifiers. Do simplicity biases look different for generative models like GANs and VAEs? Do they first learn to match unconditional statistics before higher-order conditional statistics?

10) Could the insights from this work inspire new regularization methods or optimization algorithms that accelerate learning by explicitly encouraging networks to gradually shift from simpler to more complex statistical features over training?
