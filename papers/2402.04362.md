# [Neural Networks Learn Statistics of Increasing Complexity](https://arxiv.org/abs/2402.04362)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
This paper investigates the phenomenon of neural networks first learning simple statistics of the training data distribution before progressing to more complex ones, known as the distributional simplicity bias (DSB). The authors argue there is a need for more direct empirical evidence of this bias across commonly used models and training setups. 

Proposed Solution
The authors propose two criteria to determine if a model relies only on low-order statistics: (1) editing the low-order statistics should cause the model to treat examples as being from the distribution with the modified statistics, and (2) deleting higher-order statistics should not affect performance. They use optimal transport theory and maximum entropy principles to generate synthetic datasets to test these criteria.

For images, they match first and second order statistics across classes using coordinatewise quantile normalization and Gaussian optimal transport. They also sample from maximum entropy Gaussian and uniform distributions matching the first or second order statistics. 

For language, they prove an equivalence between token n-gram frequencies and sequence embedding moments. They sample maximum entropy sequences matching n-gram statistics and evaluate language models on these.

Contributions
- Provide empirical evidence for distributional simplicity bias in major image classification models and language models, using proposed criteria and synthetic datasets. 
- Show common non-monotonic generalization curves where performance on low-order statistics peaks then declines.
- Discover "double descent" in language models, attributed to in-context learning allowing recovery of performance.
- Propose efficient sampling methods to generate maximum entropy datasets matching low-order statistics.
- Theoretically connect n-gram frequencies to embedding space moments.

In summary, the paper strengthens the evidence for neural networks prioritizing simple statistics early in training through extensive experiments and proposes useful methods and theories towards studying this phenomenon.
