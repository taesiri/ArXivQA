# [Enhancing Multilingual Capabilities of Large Language Models through   Self-Distillation from Resource-Rich Languages](https://arxiv.org/abs/2402.12204)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) trained on multilingual data still lag behind in most languages compared to resource-rich languages like English due to imbalanced data distribution. 
- Existing methods like translate-then-fine-tune have limitations in improving multilingual performance while retaining capabilities in resource-rich languages.

Proposed Solution:
- Introduce a method called Self-Distillation from Resource-Rich Languages (SDRRL) to enhance LLM multilingual capabilities.
- Leverage LLM's strong capabilities in resource-rich languages by having it generate high-quality responses as extra supervision signal.
- Translate resource-rich responses into other languages to construct transfer set for sentence-level knowledge distillation.
- Incorporate small external parallel corpus to align representations and improve multilingual generation.

Main Contributions:
- Propose a novel distillation method SDRRL that uses LLM's own responses in resource-rich languages as extra supervision signal.
- Conduct comprehensive experiments on LLaMA and SeaLLM covering 16 languages on comprehension and generation tasks.
- Show SDRRL significantly improves target language performance while maintaining capabilities in resource-rich languages.  
- Demonstrate generalizability to multiple LLM models and source languages (English and French).
- Provide analysis showing SDRRL helps align multilingual representations and alleviate issues like off-target responses.

In summary, the key innovation is leveraging resource-rich language capabilities within an LLM itself to enhance its multilingual performance through self-distillation, instead of solely relying on external translation. Experiments and analysis demonstrate clear improvements in LLM multilingual abilities.
