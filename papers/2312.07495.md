# [Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly   Detection](https://arxiv.org/abs/2312.07495)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the challenging problem of multi-class unsupervised anomaly detection (MUAD). MUAD aims to train a single model on normal images from multiple classes that can simultaneously detect anomalies on both seen and unseen classes during testing. This is more practical than training separate models per class, but also more difficult.  

Existing methods have limitations in performance or efficiency. Pyramid network-based methods are complex in design with heavier modules. Simple reconstruction errors used in existing methods may fail to model global context. The paper explores using a plain Vision Transformer (ViT) for MUAD to address these limitations.

Proposed Solution:
The paper proposes ViTAD, a novel plain ViT model for MUAD. It first abstracts existing reconstruction-based methods into a Meta-AD framework with Encoder, Fuser and Decoder modules. ViTAD then instantiates this framework with a symmetric plain ViT structure. Both the Encoder and Decoder comprise four ViT stages. A simple linear layer aggregates Encoder features for the Decoder.  

ViTAD is enhanced from macro and micro design perspectives without bells & whistles:
1) Macro: Remove skip connections, use self-supervised DINO pretraining, use last three stages for loss and inference.  
2) Micro: Modifications regarding batch norm, Fuser linear layer, positional embeddings and class token.

Only a pixel-level cosine similarity loss is used for end-to-end training. During inference, reconstruction errors from multiple stages provide the anomaly map.

Main Contributions:

1) Proposes comprehensive benchmarks with 8 evaluation metrics for MUAD.

2) Explores plain ViT for MUAD and develops an effective ViTAD model through principled design.

3) Extensive analysis and ablation studies leading to useful findings:
- Pyramidal Encoder/Decoder not necessary, plain ViT itself effectively models multi-scale features.
- Self-supervised model pretraining improves over supervised. 
- Simple linear Fuser suffices contrary to prior works.

4) Impressive SOTA results on MVTec (85.4 mAD) and VisA (75.6 mAD) datasets, with high efficiency requiring only 1.1 GPU hours for training.

In summary, the paper successfully applies a columnar plain ViT to MUAD for the first time and develops an elegant and effective solution. The design process and findings provide useful insights. Impressive benchmarks are set in both accuracy and efficiency.
