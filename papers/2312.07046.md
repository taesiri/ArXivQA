# [Rethinking Compression: Reduced Order Modelling of Latent Features in   Large Language Models](https://arxiv.org/abs/2312.07046)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces a novel approach called LLM-ROM for compressing large language models (LLMs) that is more effective and practical than existing methods like pruning and quantization. LLM-ROM works by performing localized reduced order modeling of the latent features in each layer of the LLM through low-rank decomposition of the feature space and reparameterization of the weights. A key advantage is that it operates layer-wise and does not require GPU resources, enabling the compression of billion-scale models under tight time and memory constraints. Experiments demonstrate that LLM-ROM outperforms the state-of-the-art LLM-Pruner method without any fine-tuning at compression rates of 80% and 50% on the LLaMA-7B model, even surpassing fine-tuned LLM-Pruner at 80%. The simplicity and efficacy of LLM-ROM provides a significant advancement for practical LLM compression. Overall, the paper presents reduced order modeling of latent features as a promising new technique for effectively compressing LLMs in a highly resource-efficient manner without compromising accuracy.
