# [ShortGPT: Layers in Large Language Models are More Redundant Than You   Expect](https://arxiv.org/abs/2403.03853)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) require massive compute and memory resources due to their gigantic size. However, many layers in LLMs exhibit redundancy, indicating opportunities for model compression.

Solution:
- The paper proposes a metric called Block Influence (BI) to quantify the influence of each layer on the model's hidden state transformations. 
- Layers with low BI scores are redundant and contribute minimally to model performance. The paper proposes a straightforward pruning technique called ShortGPT that removes such redundant layers based on their BI scores.

Main Contributions:  
- The paper reveals significant redundancy at the layer level in LLMs. For example, removing 25% of layers in a 13B parameter LLM causes just a 5% drop in performance.
- ShortGPT outperforms prior state-of-the-art pruning methods. It can maintain 95% of model performance while reducing 25% of parameters and computation.
- Analysis shows most redundancy exists across layers rather than across width (attention heads). 
- The simplicity of ShortGPT highlights the redundancy, suggesting opportunities to train more efficient LLM architectures. 
- ShortGPT is compatible with other techniques like quantization for further compression.

In summary, the paper proposes a novel metric BI to identify redundant layers in LLMs. Leveraging this, ShortGPT removes layers based on BI scores, providing superior pruning performance versus complexity trade-offs. The analysis also unveils substantial layer redundancy in LLMs.
