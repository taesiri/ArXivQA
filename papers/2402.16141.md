# [PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization](https://arxiv.org/abs/2402.16141)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Supervised fine-tuning of large language models (LLMs) on downstream tasks requires massive computational resources for full fine-tuning. 
- Parameter-efficient fine-tuning (PEFT) methods like LoRA have been proposed to reduce resource requirements.
- However, LoRA has a low-rank bottleneck - the update matrix ΔW is low-rank, limiting learning capability compared to full fine-tuning.

Proposed Solution:
- The paper proposes Periodic LoRA (PLoRA) to break through the low-rank bottleneck by accumulating multiple low-rank ΔW to get a higher-rank update.
- PLoRA divides training into multiple stages. In each stage, only the LoRA weights are updated. 
- At the end of each stage, the LoRA weights are unloaded into the backbone LLM weights, and LoRA matrices are reinitialized.
- This periodic unloading and accumulation of low-rank ΔW allows reaching higher ranks without increasing memory overhead.

Main Contributions:
- Introduces PLoRA method to break low-rank bottleneck and enhance learning capability of LoRA.
- Validates PLoRA for instruction tuning - shows consistent improvements over LoRA on benchmarks like GSM8K and MMLU.
- Analyzes training process to demonstrate PLoRA's stronger learning capability, approx. 1.8x of LoRA.
- Provides insights into optimal settings like unloading points, momentum, trainable layers for applying PLoRA.

In summary, the paper makes methodological and empirical contributions by proposing and analyzing PLoRA for more effective PEFT without increasing resource requirements. The results showcase PLoRA's potential to approach full fine-tuning capability.
