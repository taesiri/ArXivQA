# ChatBridge: Bridging Modalities with Large Language Model as a Language   Catalyst

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we build a unified multimodal model that can effectively interpret, correlate, and reason about diverse real-world modalities like text, images, videos, and audio, without relying on large amounts of paired training data across all modalities?The key hypotheses appear to be:1) Language can act as a universal interface or catalyst to bridge different modalities, even with just language-paired two-modality data (e.g. image-text, video-text). 2) By leveraging recent large language models (LLMs) and extending their capabilities to handle multimodal inputs, it is possible to create a multimodal LLM with emergent cross-modal understanding.3) A two-stage training process - first aligning each modality with language, and then instruction-tuning for user intent with multimodal instructions - can produce a model that generalizes effectively to diverse zero-shot multimodal tasks.In summary, the central research question is about developing a unified multimodal model that can bridge modalities and generalize well, without needing fully paired multimodal data. The key hypotheses focus on using language as a catalyst, integrating LLMs, and a two-stage training approach to achieve this goal.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities. The key ideas include:- Using language as a catalyst to correlate different modalities. The model only requires easy-acquired, language-paired two-modality data (e.g. image-text, video-text, audio-text pairs) during training. This leads to emergent multimodal collaboration abilities across modalities without relying on complete multimodal paired data.- A two-stage training process. First it prealigns each modality with language representations. Then it conducts multimodal instruction tuning using a newly collected dataset MULTIS covering diverse multimodal tasks. This improves the model's generalization ability. - The model architecture integrates modality-specific encoders, perceivers modules and a large language model. The perceivers project modality embeddings into the semantic space of the language model. - Strong quantitative and qualitative results on multimodal tasks covering text, image, video and audio modalities. The model shows effective zero-shot transfer capabilities on these tasks.In summary, the key contribution is presenting ChatBridge as an effective approach to learning a unified multimodal model that can interpret, correlate and reason about diverse real-world modalities by using language as a catalyst. The model demonstrates strong zero-shot performance on a wide range of multimodal tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes ChatBridge, a novel multimodal language model that leverages language as a catalyst to bridge different modalities by aligning them in a shared semantic space, requiring only easily acquired language-paired two-modality data and achieving strong zero-shot performance on a wide range of multimodal tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in multimodal learning:- The method only requires easy-to-acquire, language-paired two-modality data (e.g. image-text, video-text, audio-text pairs) rather than relying on more difficult to obtain data where all modalities co-occur. This is a more flexible and scalable approach compared to methods that require paired data across all modalities.- It leverages recent advances in large language models (LLMs) as a catalyst to bridge different modalities. Using the expressiveness of language to connect modalities is a novel way to achieve multimodal learning compared to approaches that use joint embeddings or separate expert models. - The two-stage training methodology aligns modalities with language first, then further aligns the model to user intents through multimodal instruction tuning. This improves generalization to new tasks compared to methods trained end-to-end on task-specific datasets.- The model incorporates diverse modalities - image, video, audio - whereas most prior work focuses on only one or two modalities like vision and language. Evaluating on a wider range of modalities tests broader multimodal capabilities.- The new MULTIS dataset for instruction tuning covers more multimodal tasks across more modalities compared to existing datasets. This provides more diverse training data to improve generalization.- Both quantitative metrics and qualitative examples are used to demonstrate multimodal reasoning and language understanding. The samples show more versatile capabilities than what is evaluated by metrics for a single task.Overall, this work explores a unique approach to multimodal learning that trains a flexible model able to generalize to new tasks and modalities. The comparisons show the benefits of using language as a universal medium to connect different modalities.
