# [VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation](https://arxiv.org/abs/2312.03275)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes Vision-Language Frontier Maps (VLFM), a zero-shot approach for navigating to unseen target objects in unfamiliar environments. VLFM constructs an occupancy map to identify frontiers and a language-grounded value map using the BLIP-2 vision-language model, RGB images, and text prompts with the target name to estimate each region's likelihood of leading to the target. The value map is used to select the most promising frontier for the robot to navigate to next using a pre-trained PointNav policy. Experiments in the Habitat simulator on Gibson, HM3D, and MP3D datasets show VLFM achieves state-of-the-art ObjectNav performance without task-specific training. It even exceeds some supervised methods trained directly for ObjectNav. Additionally, deployments on a Spot robot demonstrate VLFM can successfully navigate in real-world office environments based solely on detecting objects with GroundingDINO and estimating distance with ZoeDepth, without access to pre-built maps. The methodâ€™s modular design allows new models to be incorporated. Limitations are reliance on visible targets and lack of reuse of semantic knowledge across tasks. Overall, the work highlights the potential of zero-shot transfer of foundation models like vision-language models to real robotic systems for semantic reasoning and exploration.
