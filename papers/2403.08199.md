# [Deep Submodular Peripteral Network](https://arxiv.org/abs/2403.08199)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses two key challenges - (1) learning practical and scalable submodular functions from data that can be used for downstream tasks like summarization, active learning etc. (2) learning scoring functions from oracles that provide numerically graded pairwise comparisons between options.  

Existing works have mostly focused on the algorithmic aspects of optimizing submodular functions. There is little prior work on practically learning submodular functions from data. Similarly, graded pairwise comparisons have been studied before but not in modern deep learning contexts.

Proposed Solution:
The paper proposes Deep Submodular Peripteral Networks (DSPNs) - an expressive parametric family of submodular functions that can be trained using a novel "peripteral loss".

The DSPN has three components - pillar, aggregation and roof. The pillar embeds input objects into a non-negative space. The aggregation stage combines pillar outputs using a weighted matroid rank function to produce a submodular-preserving summary. Finally, the roof implements a Deep Submodular Function that maps the aggregation output to a scalar.

The peripteral loss is designed for graded pairwise comparisons. It encourages alignment between the relative scores produced by the DSPN vs the oracle for pairs of sets. The loss uses techniques like log barriers, hinge functions and gating to handle corner cases.

The paper also proposes several sampling strategies to generate the pairs of sets for training, including an active learning style "submodular feedback" approach.

Experiments demonstrate that DSPNs trained with the peripteral loss can effectively emulate computationally expensive oracle submodular functions. The learnt DSPNs also achieve strong performance on downstream tasks like experimental design.

Key Contributions:
1) Deep Submodular Peripteral Networks - expressive parametric family of submodular functions
2) Peripteral loss for learning from graded pairwise comparisons  
3) Sampling strategies like submodular feedback for generating training set pairs
4) Experiments showing DSPNs can learn from submodular oracle and achieve downstream gains

The work has implications for learning submodular functions from various types of oracles including humans, expensive computational processes etc. It also provides a modern graded pairwise comparison loss for preference learning.
