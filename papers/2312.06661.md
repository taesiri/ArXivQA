# [UpFusion: Novel View Diffusion from Unposed Sparse View Observations](https://arxiv.org/abs/2312.06661)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes UpFusion, a novel approach for performing sparse-view 3D inference and novel view synthesis from unposed images. Unlike prior methods that rely on known camera poses, UpFusion incorporates the input views as context in a conditional generative model to implicitly leverage multi-view information. Specifically, it encodes the input images into a set-level representation which provides instance-specific conditioning for a diffusion model trained to generate novel views based on query-view aligned features from a scene-level transformer. This allows synthesizing high-fidelity renderings while improving quality with additional context views, despite not having explicit pose supervision. To obtain a 3D consistent representation, UpFusion further distills neural radiance fields that best explain the rendered views under the learned generative model. Experiments on Co3D and GSO datasets demonstrate improved performance over pose-reliant baselines as well as competitive results compared to state-of-the-art single image 3D inference techniques. Moreover, UpFusion shows the ability to generalize to unseen categories and even self-captured in-the-wild images.
