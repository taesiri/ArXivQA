# Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods   in Natural Language Processing

## What is the central research question or hypothesis that this paper addresses?

This paper does not appear to have a clearly stated central research question or hypothesis. It seems to be a survey paper that provides a systematic overview and typology of methods involving pre-training, prompting, and predicting with language models. The key focus areas seem to be:- Introducing the paradigm of prompt-based learning as an evolution of language model pre-training and downstream task fine-tuning.- Providing a formal mathematical definition of prompting methods. - Organizing and analyzing different dimensions of prompting methods such as prompt engineering, answer engineering, multi-prompt learning strategies, training strategies, and applications.- Situating prompting methods in the broader NLP research landscape by making connections to related topics and analyzing research trends.- Identifying key challenges and open problems for future research on prompting methods.Overall, this paper aims to synthesize the current state of research on prompting methods through a comprehensive literature review and analysis. The goal appears to be providing a structured overview of this emerging research area rather than testing a specific hypothesis. The key value comes from categorizing the diverse set of works into a coherent framework.


## What is the main contribution of this paper?

This paper appears to be a survey paper on prompt-based learning in natural language processing. The main contributions seem to be:- Introducing and defining the paradigm of prompt-based learning, situating it within the broader context of NLP research. The paper argues that prompt-based learning represents a new paradigm distinct from traditional supervised learning and the pre-train/fine-tune paradigm.- Providing a formal description of prompting methods using mathematical notation and terminology like prompting function, prompt, answered prompt, etc. This establishes a unified framework for understanding the variety of prompting techniques.- Organizing and taxonomizing the literature on prompting methods across dimensions like pre-trained model choice, prompt engineering, answer engineering, multi-prompt learning, and training strategies. This structures the growing research in a coherent way. - Discussing applications of prompting methods to diverse NLP tasks like knowledge probing, text classification, question answering, etc. This helps relate prompting methods to downstream usage.- Making connections between prompting and other relevant research topics like few-shot learning, controlled text generation, etc. This helps situate prompting in the broader NLP landscape.- Providing a quantitative meta-analysis of prompting papers and a timeline of major developments. This gives a data-driven overview of the field's evolution.- Releasing accompanying online resources like a constantly updated survey website to support continued education on prompt-based learning.In summary, the main contribution is providing a comprehensive and structured overview of the emerging paradigm of prompt-based learning, establishing formalisms, taxonomies, connections to related topics, and resources to support understanding and future research in this area. The paper helps organize and make sense of a rapidly developing field.
