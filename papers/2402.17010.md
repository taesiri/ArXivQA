# [Can Large Language Models Recall Reference Location Like Humans?](https://arxiv.org/abs/2402.17010)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Traditional retrieval methods can only obtain predefined, segmented passages as references for knowledge-intensive tasks like question answering. This lacks flexibility in choosing reference start points and impacts reading naturalness. In contrast, humans can recall any seen passage and naturally start recall from any position. 

Proposed Solution:
This paper proposes a two-stage framework called LLMRefLoc that leverages large language models (LLMs) to independently recall reference passages from any starting position through prompting. 

In the first stage, the LLM is prompted to recall Wikipedia title identifiers to obtain a coarse-grained document set. Constrained decoding via a prefix tree ensures only existing titles are generated. 

In the second stage, a new FM-index is constructed from the top documents of stage one. The LLM is prompted to recall a fine-grained passage under this FM-index constraint, enabling generation from any position. A short prefix is first recalled then located to extract a full passage, significantly speeding up recall.

Main Contributions:
- Explores for the first time the capability of LLMs to recall fine-grained references under constraints through prompting
- Proposes a two-stage framework mimicking human search process of recalling coarse then fine-grained information 
- Verifies LLMs can recall and locate passages by generating just a short prefix
- excels in page and passage evaluations and improves downstream task performance across 6 datasets
- Requires no additional retrieval models or pre-segmentation, allowing flexible and natural passage recall
