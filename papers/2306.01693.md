# [Fine-Grained Human Feedback Gives Better Rewards for Language Model   Training](https://arxiv.org/abs/2306.01693)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we improve rewards for language model training via RLHF (reinforcement learning from human feedback) by using more fine-grained human feedback?The authors propose that using more fine-grained human feedback as training signals, instead of just overall human preferences, can help train language models more effectively and efficiently to generate desired outputs. Specifically, they collect human feedback that categorizes and localizes different types of errors (e.g. factual errors, irrelevant text) at different levels of granularity (e.g. sentence, sub-sentence). They then train separate reward models for each error type and granularity level, and incorporate these into a RLHF training framework called Fine-Grained RLHF. Experiments on detoxification and long-form QA aim to demonstrate the benefits of using such fine-grained rewards compared to standard holistic preference-based rewards.In summary, the central hypothesis is that fine-grained human feedback can lead to better rewards and improved language model training via RLHF compared to commonly used holistic preferences. The experiments aim to support this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introducing a new reinforcement learning framework called Fine-Grained Reinforcement Learning from Human Feedback (Fine-Grained RLHF). This framework enables training language models using reward functions that are fine-grained in two respects:- Density - Providing rewards frequently at a segment level (e.g. per sentence) rather than just at the end of the full generated text.- Multiple reward models - Using separate reward models for different types of feedback on undesired behaviors (e.g. factuality, relevance, completeness). 2. Demonstrating improved performance on two text generation tasks (detoxification and long-form QA) using fine-grained rewards compared to reinforcement learning with holistic/overall rewards on full sequences. This is shown through both automatic metrics and human evaluation.3. Showing the ability to customize language model behavior by adjusting the weights of different fine-grained reward models during training. This allows balancing different desired attributes like conciseness vs completeness.4. Analysis of the fine-grained reward models, including their performance, how they compete/tradeoff, and their impact on the trained policy model.5. Release of a new dataset called QA-Feedback with long-form question answering examples annotated with fine-grained human feedback.In summary, the key ideas are using more granular and multi-faceted human feedback to train language models more effectively and provide more control compared to holistic judgements on full sequences. The improved performance and customizability is demonstrated empirically on two tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a reinforcement learning framework called FINE that enables training language models using multiple fine-grained reward functions, each focusing on detecting a specific type of undesirable generation behavior at different levels of granularity, to better align model outputs with human preferences compared to training with a single holistic reward function.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other research in the field of using human feedback to improve language models:- This paper focuses on using fine-grained human feedback as a training signal, whereas most prior work has used holistic human preferences or ratings. The key innovation is using multiple reward models that provide dense rewards on specific error types and text segments. - Most prior RLHF (reinforcement learning from human feedback) papers collect holistic human preferences or ratings on overall model outputs. In contrast, this paper collects human feedback that explicitly marks text spans with specific error categories like factuality or redundancy.- The proposed Fine-Grained RLHF method outperforms preference-based RLHF baselines on long-form question answering and toxicity reduction. This suggests fine-grained signals are more effective than holistic signals, especially for long text.- Prior work has used human feedback signals like preferences and ratings to fine-tune or do RL over pre-trained LMs. This paper shows human feedback can also be used to train reward models for RL training.- Using multiple fine-grained reward models allows customizing model behaviors by adjusting their weights during RL training. This provides more control compared to using a single holistic reward model.- Most related work focuses on conversational tasks. This paper demonstrates using fine-grained rewards for long-form generation tasks like QA and open-ended text generation.- The idea of supervised fine-tuning on human feedback has been explored, but this paper shows RL training with reward models of feedback can be more effective.In summary, this paper's key contribution is using multiple fine-grained reward models trained on explicit human error annotations, instead of holistic preferences. This is shown to improve over preferences for long-form generation tasks. The approach also enables more control over model behaviors.
