# [Fine-Grained Human Feedback Gives Better Rewards for Language Model   Training](https://arxiv.org/abs/2306.01693)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we improve rewards for language model training via RLHF (reinforcement learning from human feedback) by using more fine-grained human feedback?The authors propose that using more fine-grained human feedback as training signals, instead of just overall human preferences, can help train language models more effectively and efficiently to generate desired outputs. Specifically, they collect human feedback that categorizes and localizes different types of errors (e.g. factual errors, irrelevant text) at different levels of granularity (e.g. sentence, sub-sentence). They then train separate reward models for each error type and granularity level, and incorporate these into a RLHF training framework called Fine-Grained RLHF. Experiments on detoxification and long-form QA aim to demonstrate the benefits of using such fine-grained rewards compared to standard holistic preference-based rewards.In summary, the central hypothesis is that fine-grained human feedback can lead to better rewards and improved language model training via RLHF compared to commonly used holistic preferences. The experiments aim to support this hypothesis.
