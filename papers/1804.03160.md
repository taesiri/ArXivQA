# [The Sound of Pixels](https://arxiv.org/abs/1804.03160)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to leverage visual and audio data from videos to jointly learn to separate and localize sounds without needing manual labels or supervision. Specifically, the paper introduces a system called PixelPlayer that takes an input video with a single audio track, and outputs separated audio tracks corresponding to different spatial locations in the video frames. The goal is to separate the mixed audio into its constituent sound sources and ground each sound source to a visual location, enabling applications like adjusting volume of specific sound sources.The key ideas are:- Using a Mix-and-Separate training framework that mixes audio from different videos to create complex training examples, and trains the model to separate the mixed audio conditioned on the corresponding visual frames. This allows learning without manual labels.- Using a model architecture with video, audio, and audio-visual fusion components to predict acoustic masks that separate pixel-specific sounds from the mixed input audio.- Leveraging visual and audio synchronization as a supervisory signal for learning to simultaneously localize and separate sounds based on their visual sources.So in summary, the central hypothesis is that by leveraging synchronized visual and audio signals in videos, the system can learn to parse sounds and associate them to visual objects without needing manual supervision.
