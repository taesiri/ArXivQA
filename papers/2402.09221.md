# [Spectral Filters, Dark Signals, and Attention Sinks](https://arxiv.org/abs/2402.09221)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is limited understanding of how large language models like LLMs work internally between the elementary operations and observable behaviors. Prior work has looked at properties of model components or identified phenomena, but there is no clear connection between the two levels.

Proposed Solution: 
- The paper introduces "logit spectroscopy", extending the "logit lens" approach, to perform spectral analysis of the content exchanged between components ("residual stream") and the embeddings/unembeddings matrices.
- They split the singular vectors of the unembedding matrix into "bands" and define "dark" signals as projections onto the subspace spanned by singular vectors with smallest singular values.

Key Findings:
- Dark signals are used to enable "attention sinks", a recently identified phenomenon where excess attention is allocated to the beginning-of-sentence token. This allows heads to not be activated when not needed.
- Suppressing parts of the unembedding spectrum keeps loss low if attention sinking is preserved. This shows the residual stream is used below full capacity.
- Tokens that draw more attention have more dark signals, but occupy a different subspace than the beginning token.

Main Contributions:
- Introduces logit spectroscopy to analyze signals between components by spectral decomposition.
- Provides detailed account of how attention sinking works via dark signals in LLMs. 
- Shows most of unembedding spectrum can be suppressed without performance loss if attention sinking is maintained.
- Discovers correlation between prevalent dark signals and attention received for a token.

The paper overall advances understanding of internal mechanics in large language models through a novel spectral analysis perspective. The findings on attention sinking, spectral filtering, and dark signals shed new light on model behaviors and capacity usage.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points made in the paper:

The paper introduces a spectral analysis methodology to study the inner workings of transformer language models, finding that signals in the least influential embedding subspace enable the crucial attention sinking mechanism which helps preserve long-range dependencies while minimizing interference with next token prediction.


## What is the main contribution of this paper?

 This paper introduces the concepts of "spectral filters" and "logit spectroscopy" to analyze the inner workings of large language models (LLMs) like LLaMa2. The key contributions are:

1) It shows that projecting intermediate representations of LLMs onto different subspaces defined by the singular vectors of the vocabulary embedding/unembedding matrices provides insights into how information flows through the models. 

2) It discovers that the signals in the subspace spanned by singular vectors with the smallest singular values (the "dark subspace") are crucial for enabling the recently discovered "attention sinking" phenomenon in LLMs. Attention sinking allows models to deal with irrelevant attention heads. 

3) It demonstrates that large portions of the unembedding spectrum, except those needed for attention sinking, can be suppressed in intermediate layers of LLaMa2 models without much increase in loss. This indicates layers operate below capacity.

4) It finds a correlation between the prevalence of "dark signals" in a token's representation and the average attention it receives from other tokens.

In summary, the key contribution is using spectral filters and logit spectroscopy to better understand attention sinking and information flow in the inner workings of state-of-the-art LLMs like LLaMa2. The techniques provide new insights into model properties and capacity.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts include:

- Spectral filters - Projections onto selected subspaces defined by the spectral decomposition of the vocabulary embedding and unembedding matrices. Used to analyze signals exchanged in transformers.

- Dark signals - Projections onto the subspace spanned by the right singular vectors of the unembedding matrix with the smallest singular values. Named by analogy with "dark matter" in physics.

- Attention sinks - The phenomenon where excess attention is allocated to the beginning-of-sentence (BoS) token to minimize interference when an attention head should not be activated. 

- Logit lens - Interpreting transformer components by projecting their contributions onto the vocabulary logits. Extended here into "logit spectroscopy".

- Residual stream - The shared communication channel between transformer components, carrying representations and additive updates. Dark signals are exchanged here.

- Sink-preserving filters - Spectral filters that remove some bands from the residual stream but preserve the dark signals needed for attention sinking.

So in summary, the key terms cover the spectral analysis methodology, the discovery of dark signals and their role in attention sinking, and extensions to the logit lens interpretation approach.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper introduces the concept of "spectral filters" to project intermediate representations onto selected subspaces. How do these spectral filters work mathematically? What are the key differences from standard dimensionality reduction techniques?

2. The paper finds that signals in the "dark subspace" with smallest singular values play an important role in attention sinking. What is attention sinking and what mechanism enables it? How do dark signals facilitate this?  

3. The paper shows it's possible to achieve low perplexity despite suppressing large portions of the unembedding spectrum using "sink-preserving" filters. What are these filters and how do they work? What are the implications for model compression?

4. Attention bars are tokens that receive a lot of attention from subsequent tokens. The paper analyzes the correlation between attention bars and prevalence of dark signals. What was found and what does this suggest about the function of attention bars?

5. The method relies on projecting weight matrices like the output projection matrix onto the right singular vectors of the unembedding matrix. What is the motivation behind using the unembedding SVD basis specifically? How does this fit into the "logit lens" interpretation approach?

6. How exactly are the spectral filters implemented and applied in the experiments? What are some key implementation details to make this computationally feasible with large Transformer models?

7. The paper studies multiple model sizes - 7B, 13B, and 70B parameters. How do results compare between models? Are phenomena like attention sinking consistent despite difference in model scale?

8. How sensitive are the main conclusions (about role of dark signals, sink-preserving filters etc.) to choices like number of spectral bands, % cutoff for dark subspace etc.? How could hyperparameters be tuned?

9. The paper focuses on LLaMA models. To what extent might conclusions generalize to other Transformer models? What unique aspects of LLaMA design mattered?

10. The method projects existing model weights onto spectral subspaces defined by embeddings. Could these techniques inspire new regularization methods to encourage separated spectral usage during training?
