# [À-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable   Prompting](https://arxiv.org/abs/2302.07994)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently build models that can be customized for each user based on their individual data access rights and preferences. Specifically, the paper aims to tackle three key challenges concurrently:1) Updating models with new data in an efficient way, without having to retrain from scratch.2) Allowing different subsets of the training data to be used by different users (compartmentalization). 3) Allowing users to customize the data used by their models (model customization).The paper refers to this as the "\`a-la-carte learning" problem, where users can specify custom subsets of available data to serve their specific needs. The key hypothesis is that this can be achieved efficiently using their proposed method of "À-la-carte Prompt Tuning" (APT).So in summary, the central research question is how to enable efficient and customizable model training and serving based on arbitrary user-selected data subsets, which they address through the APT approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called À-la-carte Prompt Tuning (APT) to enable efficient à-la-carte learning. Specifically, the key contributions are:1. Introducing the à-la-carte learning problem to concurrently address challenges like continual learning, machine unlearning, and model customization.2. Proposing APT, which converts datasets into prompts that can be composed at inference time. APT uses a modified attention mechanism to prevent interference between prompts. 3. Demonstrating that APT achieves accuracy within 5% of models trained on the full data, even when individual prompts use far less data. 4. Showing that APT with a prompt weighting scheme achieves state-of-the-art performance on continual learning benchmarks like Split CIFAR-100 and CORe50.In summary, the main contribution is proposing APT as an efficient and customizable solution to the à-la-carte learning problem, with strong empirical results on tasks like forgetting, continual learning, and model customization.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an efficient method called À-la-carte Prompt Tuning (APT) to construct customized vision models by composing prompts trained separately on different data sources, enabling applications like continual learning, machine unlearning, and model customization.
