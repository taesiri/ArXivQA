# [Train/Test-Time Adaptation with Retrieval](https://arxiv.org/abs/2303.14333)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to adapt pre-trained models both at train and test time by retrieving relevant information from a large unlabeled external dataset. Specifically, the paper proposes the Train/Test-Time Adaptation with Retrieval (T3AR) method to:

- Improve model adaptation at train time by incorporating retrieved samples from an external dataset into a contrastive self-supervised objective along with the downstream labeled data. 

- Enable test-time adaptation by selecting relevant samples from an external dataset to better approximate the target data distribution, without needing the original training data.

The key hypothesis is that retrieving and aggregating relevant samples from a large heterogeneous external dataset can significantly improve model adaptation, especially when the amount of adaptation data is limited. The paper shows this allows T3AR to outperform standard fine-tuning and existing test-time adaptation methods.

In summary, the central research question is how to leverage retrieval from a large external dataset to improve model adaptation at both train and test time, especially in low-data regimes. The key hypothesis is that intelligent retrieval and aggregation of external data can compensate for limited target data during adaptation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Train/Test-Time Adaptation with Retrieval (T3AR) for adapting machine learning models both at train time and test time using a retrieval module and a searchable pool of external data. The key ideas are:

- T3AR adapts a given model to a downstream task using refined pseudo-labels and a self-supervised contrastive objective. The contrastive loss uses retrieved real samples from an external pool to improve feature adaptation on the target data manifold. This allows adaptation without relying solely on synthetic data augmentations.

- The retrieval module allows incorporating new relevant data or removing samples from the external pool, enabling continuous adaptation as user preferences evolve after deployment. 

- T3AR is applied at train time to improve downstream fine-grained image classification over standard fine-tuning, especially when adaptation data is scarce (up to 13% relative improvement).

- At test time, T3AR exploits the external image pool to adapt representations, outperforming existing methods on domain adaptation benchmarks like DomainNet-126 and VISDA-C, particularly when few adaptation images are available (up to 8% improvement).

In summary, the main contribution is a method for train and test time adaptation that leverages retrieval from an evolving external data pool to improve model adaptation, especially in low-data regimes. The flexibility of updating the external data on-the-fly enables continuous adaptation to user needs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a method called Train/Test-Time Adaptation with Retrieval (T3AR) that improves model adaptation at both train and test time by using a retrieval module to select informative samples from an external unlabeled dataset to augment the target dataset.


## How does this paper compare to other research in the same field?

 This paper introduces Train/Test-Time Adaptation with Retrieval (T3AR), a method for adapting models at both train time and test time using a retrieval module and a searchable pool of external samples. Here are some key ways it compares and relates to other research:

- It is related to unsupervised domain adaptation (UDA), as it aims to adapt models to new target datasets without labels. However, it differs in that it does not require access to the original source training data. 

- It is related to test-time adaptation (TTA) methods, as it adapts models purely at test time. However, it incorporates retrieval of additional unlabelled data to improve adaptation, unlike most prior TTA methods.

- Retrieval augmented learning has been explored more in NLP than vision. This paper shows it can also be beneficial for vision models. The difference compared to NLP is that retrieved images may not closely match the target domain, so a contrastive loss is used.

- The retrieval module is related to memory augmented neural networks. However, rather than storing past training examples, it retrieves from a broad pool of unlabelled external data.

- The contrastive loss using retrieved negatives is related to self-supervised contrastive learning methods. However, negatives are sampled from the external retrieved data rather than purely from augmentations.

- The use of retrieval to find relevant data for a target task relates to active learning and few-shot learning methods. However, it does not assume availability of labels for querying.

In summary, this paper combines ideas from domain adaptation, test-time training, self-supervised learning, retrieval augmented learning, and active learning, but tailored to the problem of test-time model adaptation in vision using unlabelled external data. The experiments demonstrate benefits over existing domain adaptation and test-time training methods, especially in low-data regimes.
