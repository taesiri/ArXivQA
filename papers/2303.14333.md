# [Train/Test-Time Adaptation with Retrieval](https://arxiv.org/abs/2303.14333)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to adapt pre-trained models both at train and test time by retrieving relevant information from a large unlabeled external dataset. Specifically, the paper proposes the Train/Test-Time Adaptation with Retrieval (T3AR) method to:

- Improve model adaptation at train time by incorporating retrieved samples from an external dataset into a contrastive self-supervised objective along with the downstream labeled data. 

- Enable test-time adaptation by selecting relevant samples from an external dataset to better approximate the target data distribution, without needing the original training data.

The key hypothesis is that retrieving and aggregating relevant samples from a large heterogeneous external dataset can significantly improve model adaptation, especially when the amount of adaptation data is limited. The paper shows this allows T3AR to outperform standard fine-tuning and existing test-time adaptation methods.

In summary, the central research question is how to leverage retrieval from a large external dataset to improve model adaptation at both train and test time, especially in low-data regimes. The key hypothesis is that intelligent retrieval and aggregation of external data can compensate for limited target data during adaptation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Train/Test-Time Adaptation with Retrieval (T3AR) for adapting machine learning models both at train time and test time using a retrieval module and a searchable pool of external data. The key ideas are:

- T3AR adapts a given model to a downstream task using refined pseudo-labels and a self-supervised contrastive objective. The contrastive loss uses retrieved real samples from an external pool to improve feature adaptation on the target data manifold. This allows adaptation without relying solely on synthetic data augmentations.

- The retrieval module allows incorporating new relevant data or removing samples from the external pool, enabling continuous adaptation as user preferences evolve after deployment. 

- T3AR is applied at train time to improve downstream fine-grained image classification over standard fine-tuning, especially when adaptation data is scarce (up to 13% relative improvement).

- At test time, T3AR exploits the external image pool to adapt representations, outperforming existing methods on domain adaptation benchmarks like DomainNet-126 and VISDA-C, particularly when few adaptation images are available (up to 8% improvement).

In summary, the main contribution is a method for train and test time adaptation that leverages retrieval from an evolving external data pool to improve model adaptation, especially in low-data regimes. The flexibility of updating the external data on-the-fly enables continuous adaptation to user needs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a method called Train/Test-Time Adaptation with Retrieval (T3AR) that improves model adaptation at both train and test time by using a retrieval module to select informative samples from an external unlabeled dataset to augment the target dataset.


## How does this paper compare to other research in the same field?

 This paper introduces Train/Test-Time Adaptation with Retrieval (T3AR), a method for adapting models at both train time and test time using a retrieval module and a searchable pool of external samples. Here are some key ways it compares and relates to other research:

- It is related to unsupervised domain adaptation (UDA), as it aims to adapt models to new target datasets without labels. However, it differs in that it does not require access to the original source training data. 

- It is related to test-time adaptation (TTA) methods, as it adapts models purely at test time. However, it incorporates retrieval of additional unlabelled data to improve adaptation, unlike most prior TTA methods.

- Retrieval augmented learning has been explored more in NLP than vision. This paper shows it can also be beneficial for vision models. The difference compared to NLP is that retrieved images may not closely match the target domain, so a contrastive loss is used.

- The retrieval module is related to memory augmented neural networks. However, rather than storing past training examples, it retrieves from a broad pool of unlabelled external data.

- The contrastive loss using retrieved negatives is related to self-supervised contrastive learning methods. However, negatives are sampled from the external retrieved data rather than purely from augmentations.

- The use of retrieval to find relevant data for a target task relates to active learning and few-shot learning methods. However, it does not assume availability of labels for querying.

In summary, this paper combines ideas from domain adaptation, test-time training, self-supervised learning, retrieval augmented learning, and active learning, but tailored to the problem of test-time model adaptation in vision using unlabelled external data. The experiments demonstrate benefits over existing domain adaptation and test-time training methods, especially in low-data regimes.


## What future research directions do the authors suggest?

 The main future research directions suggested by the authors are:

- Testing Train/Test-Time Adaptation with Retrieval (T3AR) on larger pre-trained vision models like Vision Transformers, to assess scalability and benefits compared to methods like prompt tuning. They suggest T3AR could unlock smaller adaptable models rather than relying on unwieldy large ones. 

- Exploring active selection of samples from the auxiliary dataset during adaptation, rather than just passive nearest neighbor retrieval. This could improve efficiency and quality of the retrieved negatives.

- Developing reversible adaptation techniques built on top of T3AR, to handle antagonistic tasks where adapting to one task hurts another. The retrieval setup of T3AR could help isolate adapted models.

- Applying T3AR to continual learning settings, where the target dataset changes over time. The auxiliary dataset could accumulate all past target data and the retriever could determine what past data is relevant for adapting to the current target.

- Scaling up the retrieval module, using approximate nearest neighbor methods to handle much larger auxiliary datasets. Evaluating the throughput/accuracy trade-off.

- Studying the sensitivity to domain gap between target and auxiliary sets, and developing techniques to handle higher gaps.

The main ideas are leveraging T3AR's retrieval approach for model efficiency, active sampling, handling antagonistic tasks, continual learning, and scaling up the components like retrieval. Overall, it points to several interesting research avenues to develop T3AR further.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces Train/Test-Time Adaptation with Retrieval (T3AR), a method to adapt models both at train and test time by using a retrieval module and a searchable pool of external samples. T3AR retrieves images from the external pool that are relevant to the target data, and uses them along with refined pseudo-labels in a contrastive self-supervised objective function. This allows the model to better adapt to the target data distribution. T3AR can be used at train time to improve downstream classification over standard fine-tuning, especially when adaptation data is limited. It can also be applied at test time for more robust representations compared to existing methods, again particularly when few adaptation data are available. Overall, T3AR enables leveraging external unlabeled data to improve model adaptation by retrieving relevant information. The authors demonstrate benefits on several image classification datasets.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper introduces Train/Test-Time Adaptation with Retrieval (T3AR), a method to adapt models both at train and test time using a retrieval module and a searchable pool of external samples. The key idea is to improve adaptation on a target dataset T by retrieving informative samples from an unlabeled auxiliary dataset A. 

The method works as follows. For each sample in T, similar images are retrieved from A using a retriever model R. Then, a contrastive loss is used to pull closer features of different augmented views of the same image in T, while pushing away features of retrieved images in A with different pseudo-labels. This allows adaptation on the target data manifold using real images from A rather than just synthetic augmentations. Experiments show T3AR improves downstream classification over standard fine-tuning, especially when adaptation data is limited. It outperforms test-time adaptation methods on DomainNet-126 and VisDA-C benchmarks, particularly in low-data regimes. The results demonstrate the value of retrieval-based adaptation using an unlabeled external dataset.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes Train/Test-Time Adaptation with Retrieval (T3AR), a method to adapt models both at train and test time using a retrieval module and a searchable pool of external samples. Given a pre-trained model, a target dataset, and an external dataset, T3AR retrieves samples from the external dataset that are similar to the target data using a retrieval model like CLIP. It then adapts the pre-trained model to the target dataset using a contrastive loss that pulls together features of different augmentations of the target data, while pushing away features of retrieved external samples with different pseudo-labels. This allows the model to adapt to the target data distribution using informative external samples, rather than relying solely on synthetic augmentations. The adapted model is further supervised using the target pseudo-labels. By retrieving informative external data, T3AR is able to improve adaptation accuracy compared to standard fine-tuning, especially when limited target data is available.
