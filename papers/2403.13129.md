# [Better Call SAL: Towards Learning to Segment Anything in Lidar](https://arxiv.org/abs/2403.13129)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper challenges the current paradigm in Lidar Panoptic Segmentation (LPS), where models are trained to segment and classify a fixed set of object classes defined a priori in the training data. Instead, the authors propose a new promptable model called SAL (Segment Anything in Lidar) that can segment and classify objects from any class at test time without needing to retrain the model.

Proposed Solution: 
SAL consists of two main components:

1) A pseudo-label engine that generates supervision for the SAL model by distilling image-based models (SAM for segmentation and CLIP for zero-shot classification) to Lidar using a calibrated multi-modal sensor setup. This avoids the need for manual labeling.

2) A neural network model for zero-shot LPS that is trained on the pseudo-labels. It has a U-Net backbone for feature extraction and a transformer decoder to predict per-query instance masks, objectness scores, and CLIP feature tokens. The tokens allow zero-shot classification by matching against CLIP embeddings of arbitrary text prompts at test time.

Main Contributions:

1) SAL framework for zero-shot segmentation and classification of arbitrary objects in Lidar without manual labeling.

2) Pseudo-label engine to distill image models to generate Lidar supervision automatically.  

3) Zero-shot LPS model that segments instances, predicts CLIP tokens per instance for classification, and works for any class vocabulary provided as text prompts at test time.

4) Analysis showing SAL reaches 91% of the fully supervised state-of-the-art in class-agnostic segmentation. For zero-shot LPS it achieves encouraging 40-44% of supervised models, with a clear path for improvement by pseudo-labeling more data.

5) Demonstrates potential to train promptable segmentation models for Lidar without manual supervision, opening doors for universal perception models.
