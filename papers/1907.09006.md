# [Forward-Backward Decoding for Regularizing End-to-End TTS](https://arxiv.org/abs/1907.09006)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis appears to be:

How can forward-backward decoding be utilized to regularize end-to-end text-to-speech (TTS) models and improve their robustness and naturalness?

The key points are:

- End-to-end TTS models like Tacotron suffer from "exposure bias" where errors accumulate during autoregressive generation.

- The paper proposes using forward-backward decoding to leverage past and future context and minimize the mismatch between the two directions. 

- Two methods are introduced: 1) regularizing divergence between left-to-right (L2R) and right-to-left (R2L) models, 2) adding regularization between forward and backward decoder hidden states.

- A joint training strategy is used to enable the forward and backward models to improve each other interactively.

- Experiments show the proposed methods, especially bidirectional decoder regularization, significantly improve robustness and naturalness on challenging test cases while maintaining efficiency at inference time.

So in summary, the central hypothesis is that forward-backward decoding regularization can improve end-to-end TTS model performance by reducing exposure bias. The paper introduces two methods to test this hypothesis and provides experimental results demonstrating their effectiveness.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing two novel methods to improve the robustness and naturalness of end-to-end text-to-speech (TTS) synthesis by integrating forward and backward decoding sequences. 

Specifically, the two proposed methods are:

1. Model regularization by bidirectional agreement: Train two directional models - L2R (left-to-right) and R2L (right-to-left) and introduce regularization terms to minimize the mismatch between them. This helps alleviate the exposure bias problem.

2. Bi-directional decoder regularization: Train forward and backward decoders with shared encoder and add regularization to make their hidden states close. This allows leveraging future context during training. 

Both methods aim to improve agreement between forward and backward decoding to help the model better predict future outputs. A joint training strategy is proposed for both methods. 

Experiments show the methods, especially the bidirectional decoder regularization, achieve significant gains in robustness and naturalness on both in-domain and out-of-domain test sets. The bidirectional decoder model achieves a MOS close to human recordings.

In summary, the key contribution is using forward-backward decoding agreement as a novel form of regularization to improve end-to-end TTS models. The proposed methods are shown to be effective in improving robustness and naturalness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes two novel methods to improve the robustness and naturalness of end-to-end text-to-speech synthesis by introducing agreement between forward and backward decoding sequences during training.
