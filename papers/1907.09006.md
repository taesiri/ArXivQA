# [Forward-Backward Decoding for Regularizing End-to-End TTS](https://arxiv.org/abs/1907.09006)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis appears to be:

How can forward-backward decoding be utilized to regularize end-to-end text-to-speech (TTS) models and improve their robustness and naturalness?

The key points are:

- End-to-end TTS models like Tacotron suffer from "exposure bias" where errors accumulate during autoregressive generation.

- The paper proposes using forward-backward decoding to leverage past and future context and minimize the mismatch between the two directions. 

- Two methods are introduced: 1) regularizing divergence between left-to-right (L2R) and right-to-left (R2L) models, 2) adding regularization between forward and backward decoder hidden states.

- A joint training strategy is used to enable the forward and backward models to improve each other interactively.

- Experiments show the proposed methods, especially bidirectional decoder regularization, significantly improve robustness and naturalness on challenging test cases while maintaining efficiency at inference time.

So in summary, the central hypothesis is that forward-backward decoding regularization can improve end-to-end TTS model performance by reducing exposure bias. The paper introduces two methods to test this hypothesis and provides experimental results demonstrating their effectiveness.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing two novel methods to improve the robustness and naturalness of end-to-end text-to-speech (TTS) synthesis by integrating forward and backward decoding sequences. 

Specifically, the two proposed methods are:

1. Model regularization by bidirectional agreement: Train two directional models - L2R (left-to-right) and R2L (right-to-left) and introduce regularization terms to minimize the mismatch between them. This helps alleviate the exposure bias problem.

2. Bi-directional decoder regularization: Train forward and backward decoders with shared encoder and add regularization to make their hidden states close. This allows leveraging future context during training. 

Both methods aim to improve agreement between forward and backward decoding to help the model better predict future outputs. A joint training strategy is proposed for both methods. 

Experiments show the methods, especially the bidirectional decoder regularization, achieve significant gains in robustness and naturalness on both in-domain and out-of-domain test sets. The bidirectional decoder model achieves a MOS close to human recordings.

In summary, the key contribution is using forward-backward decoding agreement as a novel form of regularization to improve end-to-end TTS models. The proposed methods are shown to be effective in improving robustness and naturalness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes two novel methods to improve the robustness and naturalness of end-to-end text-to-speech synthesis by introducing agreement between forward and backward decoding sequences during training.


## How does this paper compare to other research in the same field?

 Here are a few key things I noticed about how this paper compares to other research in end-to-end text-to-speech synthesis:

- The paper focuses on improving the robustness and naturalness of end-to-end TTS models, a key challenge as these models are scaled to more diverse test sets. This goal aligns with much other recent work.

- The proposed methods aim to leverage future/global output information during training to address the "exposure bias" limitation of autoregressive models. This is a common approach in other sequence generation tasks but novel for TTS.

- The two proposed methods are: 1) Divergence regularization between forward and backward models, and 2) Bi-directional decoder regularization. The first method is similar to prior work, but the second method is more novel in exploiting bi-directional decoders.

- Experiments comprehensively evaluate the methods on both in-domain and challenging out-of-domain test sets. The bidirectional decoder method provides significant gains in robustness and naturalness. This demonstrates the efficacy of the approach.

- The results are state-of-the-art for end-to-end TTS working with limited training data. The proposed model achieves near human-level scores, outperforming the Tacotron 2 baseline.

- The techniques are compatible with other recent advances in end-to-end TTS, such as robust training, data augmentation, and neural vocoders.

Overall, the proposed bidirectional regularization approach makes an important contribution of improving robustness for end-to-end TTS models. The second method is elegant and achieves strong results. The techniques align well with prevailing approaches in the field.
