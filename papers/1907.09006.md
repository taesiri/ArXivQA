# [Forward-Backward Decoding for Regularizing End-to-End TTS](https://arxiv.org/abs/1907.09006)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis appears to be:

How can forward-backward decoding be utilized to regularize end-to-end text-to-speech (TTS) models and improve their robustness and naturalness?

The key points are:

- End-to-end TTS models like Tacotron suffer from "exposure bias" where errors accumulate during autoregressive generation.

- The paper proposes using forward-backward decoding to leverage past and future context and minimize the mismatch between the two directions. 

- Two methods are introduced: 1) regularizing divergence between left-to-right (L2R) and right-to-left (R2L) models, 2) adding regularization between forward and backward decoder hidden states.

- A joint training strategy is used to enable the forward and backward models to improve each other interactively.

- Experiments show the proposed methods, especially bidirectional decoder regularization, significantly improve robustness and naturalness on challenging test cases while maintaining efficiency at inference time.

So in summary, the central hypothesis is that forward-backward decoding regularization can improve end-to-end TTS model performance by reducing exposure bias. The paper introduces two methods to test this hypothesis and provides experimental results demonstrating their effectiveness.
