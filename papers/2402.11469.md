# [A Curious Case of Searching for the Correlation between Training Data   and Adversarial Robustness of Transformer Textual Models](https://arxiv.org/abs/2402.11469)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuned transformer models like BERT achieve state-of-the-art performance but are vulnerable to adversarial text attacks. 
- Existing robustness analysis takes a "model-first approach", evaluating models after fine-tuning and ignoring effects of training data.
- The paper aims to prove there is a strong correlation between training data and model robustness against attacks.

Proposed Solution:
- Extract 13 features capturing properties of fine-tuning corpora like embedding distributions, label distributions, dataset statistics. 
- Use features to train regression models to predict attack success rates of fine-tuned models, without needing to actually fine-tune the models.
- Analyze most influential features to understand how training data correlates with model robustness.

Key Contributions:
- Empirically demonstrates fine-tuning data features can effectively predict model robustness. Lightweight Random Forest model achieves low error for BERT and RoBERTa.
- Identifies embedding distributions and dataset statistics as most influential indicators of model robustness.
- Framework is 30-193x faster than traditional evaluation requiring fine-tuning and attack generation.
- Supports adversarial training, transfers between models, and robust to statistical randomness.
- Provides novel data-driven analysis of transformers' adversarial robustness, complementing existing model-focused understanding.

In summary, the paper presents a fast, interpretable method to analyze and predict transformer robustness from fine-tuning data features, proving training data itself exhibits strong correlation with model vulnerabilities.
