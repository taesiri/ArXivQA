# [Federated Class-Incremental Learning with Prompting](https://arxiv.org/abs/2310.08948)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How to effectively perform federated class-incremental learning (FCIL) where local and global models suffer from catastrophic forgetting on old classes due to the arrival of new classes, and the data distributions across clients are non-iid?

The key points are:

- The paper focuses on the practical problem of federated class-incremental learning (FCIL), where new classes continuously arrive over time at each client. 

- Existing federated learning methods fail in this setting as they assume fixed data and cannot deal well with new classes leading to catastrophic forgetting of old classes.

- Both the non-iid data distribution across clients and the streaming arrival of new classes make catastrophic forgetting worse in the federated setting.

- The paper proposes a novel rehearsal-free method called FCILPT to address these challenges in FCIL by utilizing prompt learning techniques.

So in summary, the central research question is how to effectively perform federated class-incremental learning, where catastrophic forgetting and non-iid data are key problems, and the paper proposes a prompt learning based approach as a solution.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a new method called Federated Class-Incremental Learning with Prompting (FCILPT) to address the challenging problem of federated class-incremental learning (FCIL). 

2. FCILPT is a rehearsal-free method that does not rely on memory buffers. It encodes task-relevant and task-irrelevant knowledge into prompts to preserve old and new knowledge on the clients, mitigating catastrophic forgetting.

3. It introduces learnable prompt keys and an instance-wise prompt query mechanism to accurately select suitable prompts for instances without knowing the task identity.

4. It sorts and aligns the prompt pools from different clients before aggregation to address the non-iid issue caused by missing classes among clients under the same task. 

5. Extensive experiments show FCILPT significantly outperforms existing methods on CIFAR-100, Mini-ImageNet and Tiny-ImageNet datasets.

In summary, the key contribution is proposing a novel prompt learning based approach FCILPT to address the important but challenging problem of federated class-incremental learning, while outperforming existing methods by a large margin. Theintroductions of prompt encoding, query mechanism and prompt alignment help mitigate catastrophic forgetting and non-iid issues in the federated incremental setting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new federated learning method called FCILPT that uses prompt learning techniques to incrementally learn new classes from distributed clients without forgetting old classes or requiring a memory buffer, and handles the non-iid data issue by aligning and aggregating prompt information related to each task.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in federated learning and class-incremental learning:

- Most prior work in federated learning assumes client data distributions are static and identical. This paper tackles the more realistic scenario where client data is non-stationary and new classes arrive over time. The proposed FCIL problem formulation captures this dynamic data setting better than typical federated learning formulations.

- Many class-incremental learning methods rely on memory buffers to store exemplars from old tasks/classes. Storing such data may not be allowable in privacy-sensitive federated settings. The proposed FCILPT method avoids buffers and instead encodes task knowledge into learnable prompts. This is a novel prompt-based rehearsal-free approach for FCIL.

- Existing class-incremental methods struggle with catastrophic forgetting of old classes when new ones appear. FCILPT mitigates this via prompt encoding of task-specific, task-similar, and task-irrelevant knowledge. The prompts preserve knowledge and alleviate forgetting.

- Prior FCIL work uses distillation-based losses but still sees performance drops as classes incrementally increase. FCILPT better handles incremental classes through prompt learning and aligning/integrating prompt info across clients to handle non-iid data.

- Most class incremental learning methods are designed for centralized settings. Adapting them to federated learning is non-trivial due to system heterogeneity and non-iid data distributions. FCILPT is specifically designed for the federated setting by aggregating aligned prompt pools across clients.

In summary, this paper advances the state-of-the-art in both federated learning and class-incremental learning for non-stationary distributed data through its novel prompt-based knowledge encoding approach. The evaluations demonstrate sizable improvements over prior art on benchmark datasets.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Developing more sophisticated techniques for prompt learning in federated learning settings: The authors propose prompt learning as an effective strategy for federated class-incremental learning. However, they point out that more advanced prompt optimization techniques tailored for federated learning could further enhance performance. This could involve adaptive prompt selection methods or meta-learning approaches to learn good initializations for prompt parameters.

- Exploring different neural network architectures: The current method uses a Vision Transformer (ViT) model architecture. The authors suggest studying how different model architectures like CNNs may interact with prompt learning in federated incremental learning scenarios. Architectures better suited for transfer learning via prompts could be developed.

- Adding personalization to the prompt learning framework: The current prompt learning approach produces a single global set of prompts. The authors suggest incorporating personalization into prompt learning, so that individual clients can learn personalized prompts tailored to their local data. This could help handle the non-IID data problem in federated learning more effectively.

- Evaluating on more complex real-world datasets: The current experiments are on image classification datasets like CIFAR-100. Testing the approach on more complex and noisy real-world datasets like medical imaging data could better validate its applicability. Pre-training the models on larger datasets may also help.

- Developing theoretical understandings of prompt learning: While prompt learning empirically performs well, its theoretical properties are not fully understood. Analyzing the generalization capabilities, representational capacities, and learning dynamics of prompt tuning could provide better insights.

In summary, the authors point to several interesting directions to enhance prompt learning for federated incremental learning and adapt it to real-world applications. Advancing the theoretical foundation of prompt learning also remains an open research problem.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel federated learning method called Federated Class-Incremental Learning with PrompTing (FCILPT) to address the practical challenge of federated class-incremental learning (FCIL). In FCIL, new classes arrive continuously over time at each client, causing catastrophic forgetting of old classes. Additionally, the data distribution between clients is non-iid due to the lack of certain classes. To mitigate catastrophic forgetting, FCILPT leverages prompt learning to encode task-relevant and task-irrelevant knowledge into prompts without using memory buffers. An instance-wise prompt query mechanism accurately selects suitable prompts for each instance. To handle the non-iid issue, FCILPT sorts and aligns the prompts from each client before aggregation on the server, fully integrating the knowledge for each task. Experiments on CIFAR-100, Mini-ImageNet and Tiny-ImageNet show FCILPT significantly outperforms previous state-of-the-art federated incremental learning methods. The results demonstrate the effectiveness of FCILPT in addressing key challenges of federated class-incremental learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes a new method called Federated Class Incremental Learning with Prompting (FCILPT) to handle the practical challenge of federated class-incremental learning (FCIL). In FCIL, local client models suffer from catastrophic forgetting of old classes when new classes arrive, and the data distributions across clients are non-IID. FCILPT uses prompt learning techniques to encode task-relevant and task-irrelevant knowledge into prompts to preserve old and new knowledge without using rehearsal memory buffers. It extends prompts with learnable keys and queries them in an instance-wise manner to select suitable prompts without knowing the task identity. To address non-IID data, FCILPT aligns and sorts prompt pools from clients before aggregation to integrate knowledge for the same tasks. Experiments on CIFAR-100, Mini-ImageNet, and Tiny-ImageNet show FCILPT significantly outperforms state-of-the-art methods.

In more detail, FCILPT uses three types of prompts - task-specific prompts to encode unique knowledge for each task, task-similar prompts to embed knowledge shared between similar tasks, and task-irrelevant prompts to capture common knowledge across all tasks. Learnable keys allow instance-wise querying to select relevant prompts. The prompts mitigate catastrophic forgetting in local clients. To address non-IID data, FCILPT sorts and aligns the prompts from each client's pool by relevance before aggregating, ensuring prompts with shared task knowledge are integrated. Without using rehearsal buffers, FCILPT demonstrates much higher accuracy than prior methods in the challenging FCIL setting across various datasets. The results validate the effectiveness of using prompt learning techniques to preserve federated incremental knowledge.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new federated learning approach called Federated Class-Incremental Learning with Prompting (FCILPT) to handle the practical problem of federated class-incremental learning (FCIL). In FCIL, new classes arrive over time and client data is non-iid. FCILPT utilizes prompt learning to encode task-relevant and task-irrelevant knowledge into a set of learnable prompts. This allows capturing and preserving knowledge about old and new classes without using memory buffers. Specifically, prompts encode task-specific, task-similar, and common knowledge. Before aggregation, prompts are sorted based on relevance to align information and mitigate issues from missing classes in clients' local data. The prompts are then aggregated and returned to clients along with the frozen pretrained model to enable learning new classes without forgetting old ones. Overall, FCILPT shows that prompts can effectively store knowledge and address catastrophic forgetting and non-iid data in federated class-incremental learning.


## What problem or question is the paper addressing?

 This paper proposes a new federated learning method called Federated Class Incremental Learning with PrompTing (FCILPT) to address the challenging problem of federated class-incremental learning (FCIL). 

The key problems and questions addressed are:

1) Catastrophic forgetting when learning new classes incrementally in the federated setting. Existing federated learning methods assume fixed client data and cannot handle new classes well.

2) Non-IID data distributions among clients. Different clients may have different subsets of classes, making aggregation challenging.

3) Privacy concerns with using memory buffers. Many class incremental learning methods use memory buffers to store exemplars, which may not be feasible in federated learning.

4) Lack of prior methods for federated class incremental learning. There is limited prior work on incremental learning in federated settings.

So in summary, this paper addresses the open challenge of enabling class incremental learning in practical federated learning scenarios with non-IID, dynamically changing data and privacy constraints. The key contribution is proposing a rehearsal-free prompting-based method to mitigate catastrophic forgetting and handle non-IID data distributions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract, here are some of the key terms and concepts:

- Federated learning - The paper focuses on federated learning, which involves training machine learning models on decentralized data located on devices like mobile phones without direct access to the raw data.

- Class-incremental learning - The paper addresses the problem of federated class-incremental learning, where new classes are continuously added over time. 

- Catastrophic forgetting - A key challenge in class-incremental learning is catastrophic forgetting, where learning new classes leads to forgetting old ones. The paper aims to mitigate this.

- Non-IID data - The paper considers the realistic scenario where data across clients is non-independent and identically distributed (non-IID). 

- Prompts - The proposed method encodes task knowledge into prompts to avoid relying on memory buffers for overcoming catastrophic forgetting.

- Task-relevant prompts - The prompts contain task-specific and task-similar knowledge to retain information about old and new classes.

- Task-irrelevant prompts - Additional prompts learn common knowledge across all tasks. 

- Prompt query - An instance-wise prompt query mechanism selects relevant prompts for each sample without knowing the task identity.

- Prompt aggregation - The paper aligns and aggregates prompts across clients before global aggregation to handle non-IID data.

So in summary, the key focus is on using prompts for federated class-incremental learning to deal with catastrophic forgetting and non-IID data distributions. The prompts encode different types of task knowledge to retain old and new information.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the problem being addressed in this paper? This helps establish the context and motivation for the work.

2. What existing methods are discussed, and what are their limitations? Reviewing prior work frames how the new method presented improves upon it. 

3. What is the key technical approach proposed in the paper? Understanding the technical details is core to summarizing the work.

4. How is the proposed method evaluated? Knowing the experimental setup, metrics, and datasets used to validate the method provides insight into its effectiveness.

5. What are the main results presented in the paper? The key results demonstrate the benefits of the proposed technique.

6. How does the proposed approach compare to existing methods? Comparisons highlight the advantages of the new method.

7. What are the limitations of the proposed technique? Understanding weaknesses gives a balanced perspective. 

8. What potential applications or use cases are discussed for the method? This provides context on the usefulness of the approach.

9. What future work directions are identified? Mentions of future work suggest ways the research could evolve.

10. What are the main conclusions drawn in the paper? The conclusions synthesize the key takeaways from the work.
