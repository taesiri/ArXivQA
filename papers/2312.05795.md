# [Large Multimodal Model Compression via Efficient Pruning and   Distillation at AntGroup](https://arxiv.org/abs/2312.05795)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deploying large multimodal models (LMMs) like AntGMM in real-world applications introduces challenges of increased latency (from 85ms to 700ms) and high energy consumption, conflicting with green AI goals. 
- Compressing LMMs is difficult due to the prohibitive cost of retraining them, multi-level redundancy, and complexity of text generation tasks.

Proposed Solution:
- A multi-stage LMM compression strategy involving pruning and distillation to balance efficiency and performance:
  1) Block Pruning: Remove some final blocks to reduce model depth
  2) Inter-Module Pruning: Reduce hidden dimensions of feedforward networks and attention 
  3) Input/Output Pruning: Shrink input/output dimensions of all blocks
- Calculate parameter importance for pruning decisions 
- Custom distillation loss using KL divergence and pairwise loss to enhance text generation capability  

Contributions:
- Constructed Multimodal Advertisement Audition Dataset (MAAD) from Alipay
- Reduced AntGMM size by 18x and improved inference by 7.7x with marginal 0.8% performance drop
- Achieved 90ms latency online (700ms originally), estimated 75 million kWh less annual electricity 
- Framework adopted in Alipay for 3 months since Sept 2023, maintaining performance while significantly enhancing efficiency
- Demonstrated effectiveness of multi-stage compression strategy for deploying LMMs aligned with green AI goals
