# [A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean   Language Models](https://arxiv.org/abs/2306.02254)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop advanced Korean language models with improved performance compared to existing multilingual models, in order to bridge the gap in capabilities between English and non-English languages?The key hypothesis appears to be:By creating large-scale Korean language models trained on extensive Korean corpora, it is possible to surpass the performance of current multilingual models on Korean natural language processing tasks.In particular, the paper introduces the Polyglot Korean models and evaluates their capabilities on Korean benchmark datasets, with the goal of demonstrating they can achieve state-of-the-art results for the Korean language. The underlying hypothesis is that language-specific models tuned for Korean will outperform multilingual models that have poorer Korean language abilities. By presenting the Polyglot Korean models, the authors aim to test this hypothesis and showcase techniques to improve non-English language model performance.In summary, the central research question revolves around improving Korean language modeling through the development of customized models superior to existing multilingual models. The key hypothesis is that language-specific models can boost capabilities for that language compared to generalized multilingual models.


## What is the main contribution of this paper?

The main contribution of this paper is the development and release of Polyglot Korean (Polyglot-Ko) language models. Specifically:- The authors collaborated with TUNiB to collect a large 1.2TB Korean dataset which was used to train Polyglot-Ko models. - Four Polyglot-Ko models with different sizes were trained: 1.3B, 3.8B, 5.8B, and 12.8B parameters. The 12.8B model is noted as the largest publicly available Korean language model suitable for commercial applications.- The models were evaluated on the KOBEST benchmark across different tasks like COPA, HellaSwag, BoolQ, and SentiNeg. Results show the Polyglot-Ko models achieve competitive performance, with the 12.8B model demonstrating the best results overall. - The training configurations, model architectures, datasets, and experiments are presented in detail.- The authors discuss limitations and future work, such as expanding Polyglot-Ko to 40B parameters and building multilingual models for East Asian and Romance languages.In summary, the main contribution is the development and release of the new Polyglot-Ko models to empower Korean natural language processing and address the gap in large Korean language models available. The models, training methodology, datasets, and comprehensive experiments are presented to demonstrate these high-performance Korean language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces Polyglot Korean language models ranging from 1.3B to 12.8B parameters, trained on a large-scale Korean dataset, which achieve strong performance on various Korean NLP benchmarks and represent an important contribution towards improving non-English language capabilities of multilingual models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work in the field of developing large-scale language models for Korean:- The focus on creating large-scale models specifically for Korean is quite novel. Most prior work has focused on multilingual models like mBERT, XGLM, BLOOM, etc. Developing monolingual models tailored to Korean represents a different approach.- The scale of the models explored in this paper, up to 12.8 billion parameters, is very large for a monolingual Korean model. For comparison, KoGPT has 6 billion parameters and is one of the largest Korean models out there. - The model architecture follows the standard transformer-based design that has become dominant in recent years. However, optimizations like rotating positional embeddings are used. Overall, the techniques are not radically different from other SOTA models.- For evaluation, the paper relies on the recently released KOBERT benchmark. Using this standardized benchmark allows for fair comparison to other Korean models. The results demonstrate these models achieve new SOTA for Korean.- The focus on computational resources and model scaling aligns with recent trends in the field. Leveraging partnerships with Stability AI and TUNiB to train large models reflects how access to compute is critical.- Releasing these as open source models for the research community is a positive contribution, as most large Korean models are proprietary. However, the limitations around potential harmful generation are important to keep in mind.Overall, I would say this paper makes solid incremental progress on the task of developing performant monolingual models for Korean. The computational resource requirements and partnerships involved in training huge models are representative of current trends. While not radically innovative, adding these SOTA Korean models to the open source ecosystem is valuable. The focus on monolingual vs. multilingual is perhaps the biggest differentiation from related work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing larger Polyglot Korean models with increased capacity, aiming for 40B parameters. This will allow further improvements in performance across diverse tasks.- Creating multilingual Polyglot models focused on East Asian languages (Korean, Chinese, Japanese, etc.) and Romance languages (Spanish, French, Italian, etc.). This will help advance NLP research and applications in those linguistic regions. - Improving the training process, addressing limitations like incorrect data preprocessing, suboptimal hardware, and model instability issues. Focusing on these aspects will enhance model performance and reliability.- Evaluating the models on a broader range of datasets and tasks beyond KOBEST. Assessing their capabilities across different domains will highlight strengths as well as areas needing improvement.- Developing better prompts and evaluation frameworks, especially for tasks like WiC where the models struggled. Customizing prompts and metrics could potentially improve performance.- Testing different model architectures beyond the standard Transformer to explore if variations can further boost capabilities.- Expanding the multilingual models to cover more languages, creating comprehensive polyglot models.- Releasing model checkpoints during training to enable tuning and analysis at different stages. - Open-sourcing the models and training code to increase accessibility for researchers worldwide.In summary, the key directions highlighted are expanding model and data scale, enhancing training procedures, evaluating on more tasks, improving prompts and metrics, exploring model architectures, increasing language coverage, and promoting openness and accessibility. Pursuing these avenues will help advance Polyglot models and multilingual NLP.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:This paper introduces Polyglot-Ko, a series of open-source Korean language models aimed at improving the performance of non-English languages in multilingual models. The authors collaborated with TUNiB to curate a 1.2TB Korean dataset which was used to train four Polyglot-Ko models with up to 12.8 billion parameters, the largest publicly available Korean model. The models were evaluated on the KOBERT benchmark and showed competitive performance on tasks like COPA, Hellaswag, BoolQ, and SentiNeg compared to other Korean models like KoGPT and ko-gpt-trinity. Limitations like training hardware constraints, data preprocessing errors, and potential for generating unacceptable content are acknowledged. Ongoing work on expanding the models to 40B parameters and building East-Asian and Romance multilingual models is discussed. The goal is to enable language model advancements for non-English languages.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces Polyglot-Ko, a series of large Korean language models developed by EleutherAI and partners as part of the Polyglot project. The Polyglot project aims to create advanced multilingual models with improved performance in non-English languages. The Polyglot-Ko models specifically focus on the Korean language and consist of four variants with parameters ranging from 1.3 billion to 12.8 billion. The largest 12.8 billion parameter model represents the biggest publicly available Korean model suitable for commercial use. The models were trained on a 1.2 TB Korean dataset collected in collaboration with TUNiB. Experiments evaluated the models on the KOBEST benchmark and compared performance to other Korean models including KoGPT, KoGPT-Trinity, and XGLM. Results showed the 12.8 billion parameter Polyglot-Ko model achieved state-of-the-art performance on reasoning tasks like COPA and BoolQ as well as sentiment analysis via SentiNeg. However, all models struggled with the WiC word-sense disambiguation task. The paper discusses limitations and future work including expansion to 40 billion parameters and development of multilingual models for East Asian and Romance languages. Overall, Polyglot-Ko demonstrates strong capabilities for the Korean language and can empower Korean NLP research and applications.


## Summarize the main method used in the paper in one paragraph.

The paper introduces Polyglot-Ko, a series of large-scale Korean language models developed by EleutherAI. The main method used is transformer-based pretraining on a large Korean text corpus. Specifically, the authors collaborated with TUNiB to collect a 1.2TB Korean dataset for training the models. They trained four models of increasing size - 1.3B, 3.8B, 5.8B, and 12.8B parameters - using EleutherAI's GPT-NeoX codebase. The models employ the standard transformer architecture with modifications like Rotary Positional Encodings. They were pretrained using a masked language modeling objective to predict the next token based on the previous context. Pretraining was done using high-performance compute clusters provided by Stability AI.After pretraining, the models were evaluated on the KOBEST benchmark containing tasks like COPA, BoolQ, and SentiNeg. Their performance was compared to other publicly available Korean models like KoGPT and xGLM. The results demonstrate that their 12.8B Polyglot-Ko model achieves state-of-the-art performance on several of the KOBEST tasks, showcasing the effectiveness of pretrained transformer models scaled up using large Korean corpora.In summary, the core training method is masked language model pretraining of transformers on a large Korean corpus collected by the authors. The pretrained models are scaled up to 12.8B parameters and evaluated on a standardized benchmark for the Korean language.
