# [A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean   Language Models](https://arxiv.org/abs/2306.02254)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop advanced Korean language models with improved performance compared to existing multilingual models, in order to bridge the gap in capabilities between English and non-English languages?The key hypothesis appears to be:By creating large-scale Korean language models trained on extensive Korean corpora, it is possible to surpass the performance of current multilingual models on Korean natural language processing tasks.In particular, the paper introduces the Polyglot Korean models and evaluates their capabilities on Korean benchmark datasets, with the goal of demonstrating they can achieve state-of-the-art results for the Korean language. The underlying hypothesis is that language-specific models tuned for Korean will outperform multilingual models that have poorer Korean language abilities. By presenting the Polyglot Korean models, the authors aim to test this hypothesis and showcase techniques to improve non-English language model performance.In summary, the central research question revolves around improving Korean language modeling through the development of customized models superior to existing multilingual models. The key hypothesis is that language-specific models can boost capabilities for that language compared to generalized multilingual models.


## What is the main contribution of this paper?

The main contribution of this paper is the development and release of Polyglot Korean (Polyglot-Ko) language models. Specifically:- The authors collaborated with TUNiB to collect a large 1.2TB Korean dataset which was used to train Polyglot-Ko models. - Four Polyglot-Ko models with different sizes were trained: 1.3B, 3.8B, 5.8B, and 12.8B parameters. The 12.8B model is noted as the largest publicly available Korean language model suitable for commercial applications.- The models were evaluated on the KOBEST benchmark across different tasks like COPA, HellaSwag, BoolQ, and SentiNeg. Results show the Polyglot-Ko models achieve competitive performance, with the 12.8B model demonstrating the best results overall. - The training configurations, model architectures, datasets, and experiments are presented in detail.- The authors discuss limitations and future work, such as expanding Polyglot-Ko to 40B parameters and building multilingual models for East Asian and Romance languages.In summary, the main contribution is the development and release of the new Polyglot-Ko models to empower Korean natural language processing and address the gap in large Korean language models available. The models, training methodology, datasets, and comprehensive experiments are presented to demonstrate these high-performance Korean language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces Polyglot Korean language models ranging from 1.3B to 12.8B parameters, trained on a large-scale Korean dataset, which achieve strong performance on various Korean NLP benchmarks and represent an important contribution towards improving non-English language capabilities of multilingual models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work in the field of developing large-scale language models for Korean:- The focus on creating large-scale models specifically for Korean is quite novel. Most prior work has focused on multilingual models like mBERT, XGLM, BLOOM, etc. Developing monolingual models tailored to Korean represents a different approach.- The scale of the models explored in this paper, up to 12.8 billion parameters, is very large for a monolingual Korean model. For comparison, KoGPT has 6 billion parameters and is one of the largest Korean models out there. - The model architecture follows the standard transformer-based design that has become dominant in recent years. However, optimizations like rotating positional embeddings are used. Overall, the techniques are not radically different from other SOTA models.- For evaluation, the paper relies on the recently released KOBERT benchmark. Using this standardized benchmark allows for fair comparison to other Korean models. The results demonstrate these models achieve new SOTA for Korean.- The focus on computational resources and model scaling aligns with recent trends in the field. Leveraging partnerships with Stability AI and TUNiB to train large models reflects how access to compute is critical.- Releasing these as open source models for the research community is a positive contribution, as most large Korean models are proprietary. However, the limitations around potential harmful generation are important to keep in mind.Overall, I would say this paper makes solid incremental progress on the task of developing performant monolingual models for Korean. The computational resource requirements and partnerships involved in training huge models are representative of current trends. While not radically innovative, adding these SOTA Korean models to the open source ecosystem is valuable. The focus on monolingual vs. multilingual is perhaps the biggest differentiation from related work.
