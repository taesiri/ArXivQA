# [Federated Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2004.11231)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new method called federated stochastic gradient Langevin dynamics (FSGLD) for Bayesian posterior inference on distributed non-IID data. The key ideas and contributions are:- Standard distributed stochastic gradient MCMC methods like distributed SGLD (DSGLD) suffer from high variance and bias when applied to federated non-IID data. This paper analyzes these issues both theoretically and empirically. - To address these problems, the authors propose "conducive gradients", which are auxiliary gradients computed from tractable approximations of the local likelihood on each client. These help reduce the variance and bias of the stochastic gradients.- The proposed FSGLD method incorporates conducive gradients into the gradient estimator of DSGLD. Theoretical analysis shows FSGLD converges to the true posterior even with non-IID data and infrequent communication.- Experiments on metric learning and Bayesian neural networks demonstrate FSGLD outperforms DSGLD on federated non-IID data, while achieving similar performance on IID data. FSGLD also converges faster and shows lower variance.So in summary, the central hypothesis is that conducive gradients can reduce the problems of high variance and bias in distributed stochastic MCMC methods like DSGLD when applied to federated non-IID data. The FSGLD method is proposed to leverage conducive gradients for more accurate and efficient Bayesian posterior inference in this setting.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel MCMC sampling method called federated stochastic gradient Langevin dynamics (FSGLD) that is tailored for Bayesian posterior sampling in federated learning settings with non-IID data. The key ideas are:- They propose using "conducive gradients", which are computed from local approximations of each client's likelihood, to augment the gradient estimates in distributed stochastic gradient Langevin dynamics (DSGLD). This helps reduce the variance of the stochastic gradients and correct for bias caused by heterogeneity in the non-IID data. - They show formally that adding the conducive gradients results in a valid gradient estimator with finite variance.- They provide a convergence analysis for both DSGLD and their proposed FSGLD method, relating the convergence bounds to the heterogeneity in the data shards.- Their experiments demonstrate that FSGLD outperforms DSGLD in non-IID federated settings, both in terms of convergence speed and accuracy of the sampling. FSGLD is able to handle delayed communication without diverging from the true posterior like DSGLD.In summary, the main contribution is proposing a practical and scalable MCMC method that can handle non-IID data distributions in federated learning, through the use of conducive gradients computed from local likelihood approximations. Theoretical analysis and experiments back up the effectiveness of their approach.
