# [In-context learning agents are asymmetric belief updaters](https://arxiv.org/abs/2402.03969)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) like GPT-3 exhibit a phenomenon called "in-context learning" where they can learn new tasks just from a few examples. However, there is limited understanding of how this learning process works internally. 

- This paper aims to shed light on the in-context learning dynamics of LLMs using concepts and experimental paradigms from cognitive science. Specifically, the authors investigate whether LLMs update their beliefs asymmetrically, similar to patterns observed in human learning.

Methodology
- The authors evaluate LLMs on two-alternative forced choice (2AFC) tasks adapted from psychology, where an agent must repeatedly choose between two options with stochastic rewards.

- They simulate LLMs on three variants of these tasks: (1) partial feedback about only the chosen option, (2) full feedback about both chosen and unchosen options, and (3) a mix of free choices and "forced choices" made by someone else.

- The authors fit cognitive models based on the Rescorla-Wagner model to the LLM's choices to quantify learning rates and compare them. This enables analyzing if beliefs are updated optimistically or pessimistically.

- As a comparison, they also evaluate idealized agents trained specifically on 2AFC tasks via meta-reinforcement learning.

Key Findings
- With partial feedback, LLMs display an "optimism bias", learning more from positive than negative prediction errors.

- With full feedback, this bias reverses for the unchosen option, with more learning from negative errors. 

- When no agency is implied in forced-choice trials, the asymmetry disappears.

- The meta-reinforcement learning agents display similar patterns, suggesting the biases are rational for maximizing rewards.

Implications
- Framing significantly influences in-context learning in LLMs, an effect also seen in human cognition.

- Design choices should be carefully considered when applying LLMs, as they impact learning outcomes.

- The results extend ideas that biased belief updating can be rational, rather than suboptimal, for certain problems.

- Understanding the biases emerging during in-context learning will be important for mitigating potential negative impacts as LLMs are deployed more broadly.
