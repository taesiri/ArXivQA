# [In-context learning agents are asymmetric belief updaters](https://arxiv.org/abs/2402.03969)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) like GPT-3 exhibit a phenomenon called "in-context learning" where they can learn new tasks just from a few examples. However, there is limited understanding of how this learning process works internally. 

- This paper aims to shed light on the in-context learning dynamics of LLMs using concepts and experimental paradigms from cognitive science. Specifically, the authors investigate whether LLMs update their beliefs asymmetrically, similar to patterns observed in human learning.

Methodology
- The authors evaluate LLMs on two-alternative forced choice (2AFC) tasks adapted from psychology, where an agent must repeatedly choose between two options with stochastic rewards.

- They simulate LLMs on three variants of these tasks: (1) partial feedback about only the chosen option, (2) full feedback about both chosen and unchosen options, and (3) a mix of free choices and "forced choices" made by someone else.

- The authors fit cognitive models based on the Rescorla-Wagner model to the LLM's choices to quantify learning rates and compare them. This enables analyzing if beliefs are updated optimistically or pessimistically.

- As a comparison, they also evaluate idealized agents trained specifically on 2AFC tasks via meta-reinforcement learning.

Key Findings
- With partial feedback, LLMs display an "optimism bias", learning more from positive than negative prediction errors.

- With full feedback, this bias reverses for the unchosen option, with more learning from negative errors. 

- When no agency is implied in forced-choice trials, the asymmetry disappears.

- The meta-reinforcement learning agents display similar patterns, suggesting the biases are rational for maximizing rewards.

Implications
- Framing significantly influences in-context learning in LLMs, an effect also seen in human cognition.

- Design choices should be carefully considered when applying LLMs, as they impact learning outcomes.

- The results extend ideas that biased belief updating can be rational, rather than suboptimal, for certain problems.

- Understanding the biases emerging during in-context learning will be important for mitigating potential negative impacts as LLMs are deployed more broadly.


## Summarize the paper in one sentence.

 This paper investigates how large language models update beliefs asymmetrically when learning in-context, displaying more optimism for chosen options and more pessimism for unchosen options, with these biases mediated by factors like counterfactual feedback and implied agency.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

The paper shows that in-context learning in large language models (LLMs) exhibits asymmetric belief updating tendencies that depend critically on how the problem is framed. Specifically, the authors find that LLMs display an optimism bias (learning more from positive than negative prediction errors) when outcomes for only the chosen option are observed. However, this bias reverses when counterfactual outcomes for the unchosen option are provided. Additionally, the bias disappears when no agency is implied in the task. The authors corroborate these findings by showing similar patterns in idealized agents derived through meta-reinforcement learning. Taken together, the results contribute to understanding how in-context learning works in LLMs by highlighting that problem framing significantly influences the learning dynamics, an effect also observed in human cognition.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the main keywords or key terms seem to be:

- Machine Learning
- ICML
- Large Language Models
- Rational Analysis 
- In-Context Learning
- Meta Learning
- Asymmetric belief updating
- Two-alternative forced choice (2AFC)
- Cognitive models
- Rescorla-Wagner model
- Optimism bias
- Confirmation bias  
- Meta-reinforcement learning (Meta-RL)

The paper studies the in-context learning dynamics of large language models using two-alternative forced choice tasks adapted from cognitive psychology. It fits cognitive models like the Rescorla-Wagner model to analyze the resulting behavior and finds asymmetric belief updating tendencies akin to the optimism bias and confirmation bias found in human studies. Furthermore, it trains idealized in-context learning agents using meta-reinforcement learning that display similar patterns, suggesting the observed characteristics are rational for these kinds of tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper uses cognitive models like the Rescorla-Wagner (RW) model to analyze the in-context learning dynamics of LLMs. What are the key assumptions and equations behind the RW model? How does the RWÂ± model extend the original RW model to capture asymmetric belief updating?

2) The paper fits the parameters of cognitive models to LLM behavior using a maximum a posteriori approach. What prior distributions were used for the learning rate and inverse temperature parameters? Why were these specific prior distributions chosen? 

3) The paper introduces a Meta-RL agent to simulate idealized in-context learning. Explain the inner and outer optimization loops used during Meta-RL training. What is the goal of each loop and how do they differ?  

4) How exactly was the context $c_t$ designed in the Meta-RL experiments? Why is passing the previous action and reward to the agent crucial for learning associations between choices and outcomes?

5) The paper finds optimism bias and confirmation bias in how the LLM integrates new information during in-context learning. Provide formal definitions of optimism bias and confirmation bias based on the fitted cognitive models. 

6) Both the LLM and Meta-RL agent display asymmetric belief updating, but the Meta-RL agent shows more extreme optimism and confirmation biases. What factors could explain this difference in degree?

7) The framing of the task prompts has a significant influence on in-context learning dynamics. What specific prompt design choices mediated the observed behavioral biases?

8) The paper speculates that asymmetric updating could be a rational response in certain environments. Based on the Meta-RL results, what further analyses could substantiate or disprove this speculation?  

9) The fitted cognitive models provide an interpretation of how in-context learning works. What are limitations of this model-based approach for understanding complex systems like LLMs?

10) The paper focuses exclusively on analyzing dynamics over the course of a task. What open questions remain about across-task generalization and transfer during in-context learning?
