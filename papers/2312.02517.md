# [Simplifying Neural Network Training Under Class Imbalance](https://arxiv.org/abs/2312.02517)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper demonstrates that carefully tuning standard components of neural network training pipelines, such as batch size, data augmentation, optimizers, and label smoothing, can achieve state-of-the-art performance on class-imbalanced image and tabular benchmarks without needing specialized class imbalance methods. The paper provides several key insights, including that smaller batch sizes are better for imbalance, augmentation has an amplified impact especially on minority classes, larger models tend to overfit more, and adding a self-supervised loss can improve representations. The authors propose adaptations to sharpness-aware minimization and label smoothing to improve optimization and regularization for imbalance. Evaluations show their approach matches or exceeds prior specialized techniques and is more robust on real-world datasets. The analysis also reveals that overfitting is a key factor hindering performance on imbalance, whereas successful methods regularize against this overfitting. By simplifying and optimizing standard training components, strong performance can be attained under imbalance without complex specialized solutions.
