# [Photorealistic Video Generation with Diffusion Models](https://arxiv.org/abs/2312.06662)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper presents WALT, a transformer-based method for photorealistic video generation using latent diffusion models. The key innovation is the use of a causal video encoder that maps both images and videos into a shared lower-dimensional latent space. This unified representation allows joint training on large image datasets while compressing videos spatially and temporally. To enable efficient processing, WALT utilizes a windowed attention architecture with alternating non-overlapping spatial and spatiotemporal self-attention blocks tailored for images and videos respectively. Without using classifier guidance, WALT demonstrates state-of-the-art performance on class-conditional video generation, frame prediction, and class-conditional image generation benchmarks. The authors also showcase WALT's scalability by training a cascade of models for high-resolution text-to-video generation. The base model is trained on nearly a billion image-text pairs and tens of millions of video-text pairs from diverse internet sources. Qualitative results demonstrate WALT can generate photorealistic and temporally consistent videos from language descriptions.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Generating high-quality, high-resolution, temporally consistent videos from text prompts remains an open challenge. Prior video diffusion models rely on UNet architectures which have limited scalability. Transformers have shown great promise for image generation but have not been successfully adapted for video diffusion modeling.

Key Ideas: 
1) Unified latent space: An autoencoder is used to map both images and videos into a shared lower-dimensional latent space. This allows leveraging both image and video datasets for training a single generative model. It also reduces the computational cost.

2) Windowed attention architecture: To handle long video sequences efficiently, the transformer uses alternating layers of spatial self-attention (within frames) and spatiotemporal self-attention (across frames). The local windowed attention reduces computational requirements. The spatial layers can handle both images and videos while the spatiotemporal layers specifically model video temporal dynamics.

3) Text conditioning: Cross-attention layers are added to the architecture to enable text-conditional video generation. Adaptive normalization conditioning is also used.

4) Multi-stage cascade: To generate high resolution videos, a cascade of transformer diffusion models are trained with increasing resolutions.

Contributions:

1) First successful application of transformers for latent video diffusion modeling with state-of-the-art results on multiple benchmarks including UCF-101, Kinetics-600 and ImageNet.

2) Efficient joint training approach leveraging both image and video datasets. Local windowed attention facilitates scaling.

3) High-resolution photorealistic text-to-video generation demonstrated, enabled by the efficient cascaded model architecture. Videos of resolution 512x896 over 3.6 seconds are shown as results.

In summary, the paper presents novel architecture design choices to effectively adapt transformers for latent video diffusion models, leading to state-of-the-art results and photorealistic conditional video generation while maintaining efficiency.
