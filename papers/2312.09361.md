# [RTRA: Rapid Training of Regularization-based Approaches in Continual   Learning](https://arxiv.org/abs/2312.09361)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Catastrophic forgetting (CF) is a major issue in continual learning where neural networks forget previously learned knowledge upon learning new information. 
- Regularization-based approaches mitigate CF by penalizing changes to important parameters during retraining on new tasks. But they have slow training.

Proposed Solution:  
- This paper proposes a modification called \approach (Rapid Training of Regularization-Approaches) to the Elastic Weight Consolidation (EWC) regularization scheme.  
- It uses the Natural Gradient (NG) instead of stochastic gradient descent for optimizing the EWC loss function. NG depends on the inverse Fisher information matrix which is already computed in EWC to determine parameter importance.

Main Contributions:
- First study to utilize NG in a continual learning setting which improves training speed of regularization methods without losing accuracy.  
- Evaluation using food classification dataset (iFood251) which is a new and challenging dataset for continual learning research.

Key Results:
- \approach achieves 7.71\% lower training time compared to EWC for the same accuracy on iFood251 dataset.
- Shows consistent accuracy but faster optimization across tasks demonstrating the advantage of using natural gradients.

Conclusion:
- The natural gradient allows faster retraining of neural networks in regularization-based class incremental learning scenarios. This is useful since model undergoes multi-task retraining.
- The approach can potentially benefit other continual learning methods that use gradient based optimization.
