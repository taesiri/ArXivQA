# [Ginger: An Efficient Curvature Approximation with Linear Complexity for   General Neural Networks](https://arxiv.org/abs/2402.03295)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Second-order optimization methods like the generalized Gauss-Newton (GGN) method can be more effective for training neural networks as they utilize curvature information of the loss function. However, computing and storing the full GGN matrix has quadratic memory complexity and cubic time complexity, making it infeasible for modern large neural networks. Prior works approximate the GGN matrix with lower complexity but make inaccurate approximations.

Proposed Solution:
This paper proposes "Ginger", an efficient eigendecomposition method to approximate the inverse GGN matrix. Unlike prior works that factorize the GGN matrix, Ginger directly models the inverse GGN matrix as the sum of a diagonal matrix and a low-rank matrix based on truncated SVD. This allows Ginger to optimally capture the top curvature information in linear space and time complexity.  

Specifically, Ginger models the inverse GGN matrix in an online fashion using exponential moving average (EMA). The EMA matrix is decomposed as:
G_t^{-1} = γI - U_t K_t U_t^T
where U_t contains the top eigenvectors, K_t contains the top eigenvalues, and γ is a damping factor. The matrices U_t and K_t are updated using efficient SVD update rules to maintain the top curvature components. The preconditioned gradient can then be computed using the Woodbury matrix identity in linear time.

Main Contributions:
- Proposes Ginger, a novel eigendecomposition of the inverse GGN matrix that captures top curvature information optimally in linear space and time.
- Provides theoretical analysis on the convergence guarantee and bounded eigenvalues.
- Empirically evaluates Ginger on image classification and language modeling tasks with CNN and Transformer models, where Ginger outperforms existing first-order and quasi-second-order optimizers.

In summary, the paper presents an efficient and accurate approach to approximate second-order optimization that is applicable to large modern neural networks. By modeling the inverse GGN matrix directly, Ginger makes better approximation than prior works.
