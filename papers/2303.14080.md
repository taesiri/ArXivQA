# [Best of Both Worlds: Multimodal Contrastive Learning with Tabular and   Imaging Data](https://arxiv.org/abs/2303.14080)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is:

How can we develop an effective self-supervised contrastive learning framework that leverages both tabular data and images to learn useful representations? 

Specifically, the paper proposes a multimodal contrastive learning approach that combines tabular data (such as clinical measurements, questionnaire responses, etc.) and images (such as MRI scans) to pretrain encoders in a self-supervised manner. The key ideas and contributions are:

- This is the first work to propose a contrastive self-supervised framework that jointly leverages both tabular and image data. Prior contrastive learning methods focused primarily on images or text. 

- The approach combines ideas from SimCLR (contrastive learning on images) and SCARF (contrastive learning on tabular data) to develop a multimodal contrastive training framework.

- The method is evaluated on predicting cardiac health from cardiac MRIs and tabular clinical data from 40K subjects in the UK Biobank. It shows improved performance over supervised and self-supervised baselines using images alone.

- Analysis reveals that certain types of tabular features like morphometric measures (size/shape related) are especially useful in the multimodal contrastive learning. 

- A new technique called "label as a feature" is introduced for supervised contrastive learning by simply appending target labels as a tabular feature during training.

In summary, the key hypothesis is that combining tabular and image data in a self-supervised contrastive framework can learn improved feature representations that translate to better downstream task performance compared to using images alone. The results on cardiac health prediction and analysis of feature importance seem to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposing the first self-supervised contrastive learning framework that combines tabular data (such as clinical measurements) and images. The method is based on SimCLR for images and SCARF for tabular data.

2. Demonstrating the effectiveness of the proposed multimodal contrastive learning approach on predicting cardiac health from MR images combined with tabular clinical data from 40,000 UK Biobank subjects. The method outperforms purely image-based and tabular-based contrastive learning baselines.

3. Showing the generalizability of the approach by applying it to combine car images and tabular advertisement data from the DVM dataset for predicting car models. Again the multimodal method outperforms unimodal baselines.

4. Analyzing the learned representations using attribution methods and finding that morphometric tabular features (describing shape/size) play an outsized role during contrastive pretraining. The inclusion of such features is shown to boost performance.

5. Introducing a simple but effective supervised contrastive learning technique called "label as a feature" (LaaF) where the class label is appended as a tabular feature. This is shown to outperform previous supervised contrastive methods on the tasks considered.

In summary, the key contribution is proposing and demonstrating the first multimodal contrastive learning framework for tabular and image data, with analyses providing insights into why it is effective. The method has potential usefulness for learning from multimodal medical data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a multimodal contrastive learning framework that combines imaging data like MRIs with tabular clinical data to improve representation learning, showing benefits especially for small datasets like those found clinically vs large biobank datasets used for pretraining.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other related research:

- This paper proposes a novel self-supervised contrastive learning framework that combines imaging and tabular data for representation learning. Most prior work in self-supervised learning has focused only on images or only on tabular data separately. Jointly learning from multimodal data like images and tables is an emerging area of research.

- The proposed method is based on combining SimCLR and SCARF, two leading contrastive self-supervised methods for images and tabular data respectively. Building on top of established methods is a strength, as it utilizes proven techniques as a starting point.

- The application to medical imaging and electronic health record data is relevant, as medical datasets often contain both images and clinical/demographic information. Applying self-supervised learning to make use of unlabeled medical data is an active research direction.

- The experiments demonstrate strong performance on predicting cardiac health from cardiac MR images and tabular data from 40,000 patients. Comparisons to supervised and single modality self-supervised baselines show the benefits of the multimodal approach.

- Analysis of the model using attribution methods provides interesting insights into how it leverages morphological features that have correlates across modalities during training. Understanding model behavior is an important contribution.

- The proposed "label as a feature" technique for supervised contrastive learning is novel and shows strong performance compared to prior supervised contrastive methods.

Overall, this paper introduces a new approach for self-supervised multimodal learning that pushes forward the state of the art in an important direction. The applications to medical data and model analysis add to the significance of the work. The comparisons to past work are fair and highlight the advances made.


## What future research directions do the authors suggest?

 Based on my review, the authors suggest the following future research directions:

- Explore the behavior of the multimodal contrastive framework on additional tasks like segmentation and regression. The authors hypothesize segmentation could benefit if morphometric features like organ/region sizes are included in the tabular data. Regression may also benefit from morphometric tabular features.

- Test the framework with more balanced datasets or explore any propagated biases learned from unbalanced datasets. The current work only used white subjects from the UK Biobank due to underrepresentation of other ethnicities. There are known racial disparities in the prediction tasks that could lead to spurious correlations. 

- Increase the importance/effect of the label during supervised contrastive pretraining for binary classification tasks. The authors found limited gains from their label-as-a-feature approach on the imbalanced binary cardiac tasks and suggest adapting the loss function or strategy to boost the effect of the label.

- Further explore combinations of the label-as-a-feature strategy with other supervised contrastive learning techniques. Initial results showed combining label-as-a-feature with methods like supervised contrastive learning and false negative elimination improves performance. More exploration on optimal combinations is suggested.

- Generalize the label-as-a-feature technique to non-tabular modalities like images and text. The authors currently only append labels as tabular features but suggest this could work for other data types.

- Scale up the framework and evaluate on larger and more diverse datasets. The current work looks at two datasets, so testing on more and larger datasets is recommended.

In summary, the main future directions are exploring the framework's applicability to different tasks and data types, improving label integration, evaluating potential biases, and scaling up the experiments. The core idea of multimodal contrastive learning for tabular and imaging data seems promising based on the initial results.
