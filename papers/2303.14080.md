# [Best of Both Worlds: Multimodal Contrastive Learning with Tabular and   Imaging Data](https://arxiv.org/abs/2303.14080)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is:

How can we develop an effective self-supervised contrastive learning framework that leverages both tabular data and images to learn useful representations? 

Specifically, the paper proposes a multimodal contrastive learning approach that combines tabular data (such as clinical measurements, questionnaire responses, etc.) and images (such as MRI scans) to pretrain encoders in a self-supervised manner. The key ideas and contributions are:

- This is the first work to propose a contrastive self-supervised framework that jointly leverages both tabular and image data. Prior contrastive learning methods focused primarily on images or text. 

- The approach combines ideas from SimCLR (contrastive learning on images) and SCARF (contrastive learning on tabular data) to develop a multimodal contrastive training framework.

- The method is evaluated on predicting cardiac health from cardiac MRIs and tabular clinical data from 40K subjects in the UK Biobank. It shows improved performance over supervised and self-supervised baselines using images alone.

- Analysis reveals that certain types of tabular features like morphometric measures (size/shape related) are especially useful in the multimodal contrastive learning. 

- A new technique called "label as a feature" is introduced for supervised contrastive learning by simply appending target labels as a tabular feature during training.

In summary, the key hypothesis is that combining tabular and image data in a self-supervised contrastive framework can learn improved feature representations that translate to better downstream task performance compared to using images alone. The results on cardiac health prediction and analysis of feature importance seem to validate this hypothesis.
