# [PREDILECT: Preferences Delineated with Zero-Shot Language-based   Reasoning in Reinforcement Learning](https://arxiv.org/abs/2402.15420)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Preference-based reinforcement learning (RL) relies on extensive human feedback in the form of preferences over trajectories, which limits its applicability in real-world robotics.  
- Preferences alone may not fully capture the causal relationships between states, actions and rewards, leading to "causal confusion."

Proposed Solution:
- Introduce PREDILECT, which expands the information collected per query to include both preferences and optional natural language prompts from humans. 
- Leverage the zero-shot reasoning capabilities of large language models (LLMs) to extract features and sentiment from the natural language prompts.
- Use the highlighted features and sentiment to regularize the training of the reward model, focusing on maximizing positive highlights and minimizing negative ones.

Main Contributions:
- Formulation to incorporate natural language prompts in preference queries, and map language responses to trajectory highlights using an LLM.
- Regularization approach to integrate preferences and language highlights into the reward learning process.  
- Demonstrated improved sample efficiency and ability to tailor policies based on language explanations in both simulated environments and a complex social navigation task.
- Analysis showing the LLM can accurately extract key features from free-form human language prompts.

In summary, the paper introduces an interactive framework called PREDILECT that combines preferences and natural language explanations to achieve more efficient and customizable reward learning for robot policies. Key ideas include leveraging LLMs for zero-shot reasoning on language, and using language highlights to regularize the underlying reward model.
