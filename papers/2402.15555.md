# [Deep Networks Always Grok and Here is Why](https://arxiv.org/abs/2402.15555)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Grokking is a phenomenon where deep neural networks (DNNs) exhibit delayed generalization, i.e. they generalize to test data long after achieving near-perfect training accuracy. Prior works have reported grokking in limited settings like transformers on algorithmic tasks. 

Key Contributions
- The authors demonstrate that grokking is widespread and occurs in convolutional and ResNet models on CIFAR and Imagenette datasets. They introduce a new concept called "delayed robustness" where models grok adversarial examples long after interpolating or generalizing.

- They develop an analytical explanation for delayed generalization and robustness based on quantifying the "local complexity" of a DNN's input-output mapping. Local complexity measures the density of linear regions tiling the input space.

- They provide first evidence of a phase transition in training where linear regions migrate from training data (smoothing mapping there) towards decision boundaries (complexifying mapping there). Grokking occurs after this transition as a robust partition of input space emerges.

Proposed Solution
- The authors use the spline perspective of DNNs to define local complexity - a novel progress measure for training. Local complexity undergoes a double descent during training. 

- First descent: Initialization effect.  
- Ascent: Accumulation of linear regions around data. 
- Second descent: Region migration away from data towards decision boundaries. This causes eventual grokking.

- Several ablation studies connect the training phases to DNN hyperparameters like depth, width and connect grokking to memorization. Batch norm is shown to inhibit grokking.

To summarize, the paper provides strong evidence that grokking is a general phenomenon deeply tied to the geometry of DNN functions, quantifiable via local complexity. Region migration causes the emergence of a robust partition explaining delayed generalization and robustness.
