# [MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for   Chest X-Ray Image Classification](https://arxiv.org/abs/2401.00728)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

The paper proposes a novel deep learning model called MultiFusionNet for accurate classification of chest X-ray images into three classes - COVID-19, pneumonia, and normal. Accurate and automated classification of chest X-rays can aid in the diagnosis of pulmonary diseases like COVID-19 and pneumonia. However, previous models have suffered from issues like feature loss across layers and inability to leverage features from multiple models. 

To address these limitations, MultiFusionNet utilizes a multilayer multimodal fusion approach. It fuses features from multiple layers within the ResNet50V2 and InceptionV3 models to prevent feature loss. It also combines these two models using early fusion to leverage their complementary strengths. A key contribution is a new module called Fusion of Different Sized Feature Maps (FDSFM) that can effectively fuse feature maps of different dimensions generated across layers.

The authors create a large dataset of 21,272 chest X-ray images by merging multiple public datasets. This helps address data scarcity issues plaguing prior work. Extensive experiments demonstrate that MultiFusionNet significantly outperforms state-of-the-art methods and even subsidiary models proposed by the authors themselves. It achieves classification accuracy of 97.21% and 99.60% for 3-class and 2-class tasks respectively. Additional analyses like confusion matrices, per-class metrics, ROC curves etc. further validate the promise of MultiFusionNet.

In summary, the key contributions are: (1) A novel multilayer multimodal fusion model for chest X-ray classification; (2) The FDSFM module to enable fusion of differently sized feature maps; (3) A significantly larger dataset of 21,272 images to mitigate data constraints; (4) Comprehensive experiments that demonstrate improved accuracy over prior state-of-the-art by 2-7%. The proposed model has strong potential for deployment in real-world clinical settings.
