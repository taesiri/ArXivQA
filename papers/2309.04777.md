# [Towards Robust Model Watermark via Reducing Parametric Vulnerability](https://arxiv.org/abs/2309.04777)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: how to make backdoor-based watermarks more robust against watermark removal attacks?Specifically, the authors investigate the vulnerability of existing backdoor-based watermarking methods, where they find there exist many watermark-removed models in the vicinity of the original watermarked model. This makes it easy for removal attacks like fine-tuning to erase the watermark behaviors. To address this vulnerability, the authors propose a minimax formulation to find and recover these watermark-removed models during training. They also observe and mitigate the domain shift between defenses and attacks when using BatchNorm. Through extensive experiments, they demonstrate their method can consistently improve the robustness of backdoor-based watermarks against several state-of-the-art removal attacks.


## What is the main contribution of this paper?

The main contributions of this paper are:- It demonstrates that there exist many watermark-removed models in the vicinity of the watermarked model in the parameter space, which may be easily utilized by fine-tuning and other removal methods. - It proposes a minimax formulation to find watermark-removed models in the vicinity and recover their watermark behaviors, to mitigate the vulnerability in the parameter space. This is shown to effectively improve the watermarking robustness against removal attacks.- It conducts extensive experiments against several state-of-the-art watermark-remove attacks to demonstrate the effectiveness of the proposed method. - It also conducts some exploratory experiments to have a closer look at the proposed method.In summary, the key contribution is the minimax formulation that finds and repairs vulnerable watermark-removed models nearby the original watermarked model, resulting in a more robust watermarking technique. The experiments verify its effectiveness against removal attacks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a minimax formulation to find and repair watermark-removed models in the vicinity of the original watermarked model during training, in order to improve the robustness of backdoor-based ownership verification against watermark removal attacks.


## How does this paper compare to other research in the same field?

Here are a few key comparisons between this paper and other related research on robust watermarking for deep neural networks:- This paper focuses on improving watermark robustness against removal attacks by finding and "repairing" watermark-removed models in the vicinity of the original watermarked model. Other works like exponentialized weighting and certified watermarking aim to make the watermark more resistant to parameter changes, but don't explicitly try to remove nearby watermark-free models.- The paper proposes a minimax formulation to find and recover from watermark-removed models during training. This is a novel approach compared to prior defenses which mainly modify the loss function or model parameters directly.- The paper identifies and addresses the issue of domain shift between watermark embedding and removal attacks caused by differences in BatchNorm statistics. Methods like exponentialized weighting don't account for this.- Experiments show the method is effective against multiple state-of-the-art removal attacks like fine-tuning, pruning, distillation etc. Many prior works only evaluated on a subset of attacks.- The defense operates purely during training and requires no modifications at inference time. Some other robust watermarking techniques like randomized smoothing require changes to the verification procedure.- The method seems to generalize well across different model architectures, datasets, and watermark types. Other specialized techniques may be more constrained.In summary, the key novelty of this paper is the minimax viewpoint to proactively find and mitigate watermark-free models during training. The paper demonstrates this is an effective approach and achieves strong empirical defense across diverse scenarios.


## What future research directions do the authors suggest?

Based on my understanding, the authors suggest the following future research directions:- Investigate more advanced threat models. In this work, the authors use a simplified threat model that constrains the perturbation magnitude of model parameters. They suggest investigating more realistic threat models where the adversary can make any changes to the model as long as the benign accuracy is maintained. Defending against such unrestricted threat models is an important future direction.- Explore certified robustness for black-box watermarks. The authors point out that their method serves as a prerequisite towards truly robust watermarks. An interesting direction is to achieve certified robustness for black-box watermarks, similar to certified robustness against adversarial examples. - Apply the proposed ideas to other domains. The minimax formulation and customized BatchNorm can be potentially applied to other domains beyond deep neural networks, such as graph neural networks, to improve robustness. Exploring the broader applicability of the proposed techniques is an interesting avenue for future work.- Investigate connections to adversarial training. The proposed minimax optimization has similarities to adversarial training. Studying the theoretical connections between the two and developing unified frameworks would be an important research direction.- Evaluate on larger-scale models and datasets. While the authors demonstrate the effectiveness on benchmark datasets, evaluating the approach on larger models like Transformers and datasets like ImageNet would be useful.In summary, the main future directions are: exploring more advanced threat models, achieving certified robustness, applying the ideas to other domains, understanding connections to adversarial training, and evaluation on larger-scale settings. Advancing research along these lines can lead to more robust and practical watermarking techniques.
