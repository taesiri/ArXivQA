# [Towards Robust Model Watermark via Reducing Parametric Vulnerability](https://arxiv.org/abs/2309.04777)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: how to make backdoor-based watermarks more robust against watermark removal attacks?Specifically, the authors investigate the vulnerability of existing backdoor-based watermarking methods, where they find there exist many watermark-removed models in the vicinity of the original watermarked model. This makes it easy for removal attacks like fine-tuning to erase the watermark behaviors. To address this vulnerability, the authors propose a minimax formulation to find and recover these watermark-removed models during training. They also observe and mitigate the domain shift between defenses and attacks when using BatchNorm. Through extensive experiments, they demonstrate their method can consistently improve the robustness of backdoor-based watermarks against several state-of-the-art removal attacks.


## What is the main contribution of this paper?

The main contributions of this paper are:- It demonstrates that there exist many watermark-removed models in the vicinity of the watermarked model in the parameter space, which may be easily utilized by fine-tuning and other removal methods. - It proposes a minimax formulation to find watermark-removed models in the vicinity and recover their watermark behaviors, to mitigate the vulnerability in the parameter space. This is shown to effectively improve the watermarking robustness against removal attacks.- It conducts extensive experiments against several state-of-the-art watermark-remove attacks to demonstrate the effectiveness of the proposed method. - It also conducts some exploratory experiments to have a closer look at the proposed method.In summary, the key contribution is the minimax formulation that finds and repairs vulnerable watermark-removed models nearby the original watermarked model, resulting in a more robust watermarking technique. The experiments verify its effectiveness against removal attacks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a minimax formulation to find and repair watermark-removed models in the vicinity of the original watermarked model during training, in order to improve the robustness of backdoor-based ownership verification against watermark removal attacks.


## How does this paper compare to other research in the same field?

Here are a few key comparisons between this paper and other related research on robust watermarking for deep neural networks:- This paper focuses on improving watermark robustness against removal attacks by finding and "repairing" watermark-removed models in the vicinity of the original watermarked model. Other works like exponentialized weighting and certified watermarking aim to make the watermark more resistant to parameter changes, but don't explicitly try to remove nearby watermark-free models.- The paper proposes a minimax formulation to find and recover from watermark-removed models during training. This is a novel approach compared to prior defenses which mainly modify the loss function or model parameters directly.- The paper identifies and addresses the issue of domain shift between watermark embedding and removal attacks caused by differences in BatchNorm statistics. Methods like exponentialized weighting don't account for this.- Experiments show the method is effective against multiple state-of-the-art removal attacks like fine-tuning, pruning, distillation etc. Many prior works only evaluated on a subset of attacks.- The defense operates purely during training and requires no modifications at inference time. Some other robust watermarking techniques like randomized smoothing require changes to the verification procedure.- The method seems to generalize well across different model architectures, datasets, and watermark types. Other specialized techniques may be more constrained.In summary, the key novelty of this paper is the minimax viewpoint to proactively find and mitigate watermark-free models during training. The paper demonstrates this is an effective approach and achieves strong empirical defense across diverse scenarios.


## What future research directions do the authors suggest?

Based on my understanding, the authors suggest the following future research directions:- Investigate more advanced threat models. In this work, the authors use a simplified threat model that constrains the perturbation magnitude of model parameters. They suggest investigating more realistic threat models where the adversary can make any changes to the model as long as the benign accuracy is maintained. Defending against such unrestricted threat models is an important future direction.- Explore certified robustness for black-box watermarks. The authors point out that their method serves as a prerequisite towards truly robust watermarks. An interesting direction is to achieve certified robustness for black-box watermarks, similar to certified robustness against adversarial examples. - Apply the proposed ideas to other domains. The minimax formulation and customized BatchNorm can be potentially applied to other domains beyond deep neural networks, such as graph neural networks, to improve robustness. Exploring the broader applicability of the proposed techniques is an interesting avenue for future work.- Investigate connections to adversarial training. The proposed minimax optimization has similarities to adversarial training. Studying the theoretical connections between the two and developing unified frameworks would be an important research direction.- Evaluate on larger-scale models and datasets. While the authors demonstrate the effectiveness on benchmark datasets, evaluating the approach on larger models like Transformers and datasets like ImageNet would be useful.In summary, the main future directions are: exploring more advanced threat models, achieving certified robustness, applying the ideas to other domains, understanding connections to adversarial training, and evaluation on larger-scale settings. Advancing research along these lines can lead to more robust and practical watermarking techniques.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper investigates the vulnerability of backdoor-based model watermarking techniques against watermark removal attacks. The authors find that there exist many models without the watermark behavior (low watermark success rate) in the vicinity of the original watermarked model in the parameter space. These models can be easily discovered by watermark removal attacks like fine-tuning. To address this issue, the authors propose an adversarial training approach with two components: 1) An adversarial parametric perturbation module that finds watermark-removed models nearby and recovers their watermark behaviors. 2) A customized batch normalization using only clean samples, to reduce the domain shift between training and attacks. Experiments show the proposed method significantly improves watermark robustness against state-of-the-art removal attacks like fine-tuning, fine-pruning and neural network laundering.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper investigates the vulnerability of backdoor-based model watermarking techniques against removal attacks. The authors find that there exist many models in the vicinity of the watermarked model that have the watermark removed but still maintain high accuracy on clean data. This makes it easy for removal attacks like fine-tuning to erase the watermark behaviors. To address this issue, the authors propose an adversarial parametric perturbation (APP) method. Specifically, they use maximization to find watermark-removed models in the vicinity and minimization to recover their watermark behaviors. They further propose a clean-sample based batch norm (c-BN) to reduce the domain shift between the defense and attacks. Extensive experiments show that their method can effectively improve the robustness of model watermarking against several state-of-the-art removal attacks. In summary, this paper makes the following contributions: (1) It reveals the existence of easily removable models around the watermarked model. (2) It proposes an APP method to find and recover these models to enhance robustness. (3) It introduces a c-BN technique to reduce the domain discrepancy. (4) Comprehensive experiments demonstrate the effectiveness of the proposed method against strong removal attacks. Overall, this work provides valuable insights into the vulnerability of model watermarking and presents an effective defense to strengthen watermark robustness.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a minimax formulation to train robust watermarked deep neural networks (DNNs) against watermark removal attacks. Specifically, it first uses maximization to find watermark-removed models in the vicinity of the original watermarked model by generating adversarial parametric perturbations. Then it applies minimization to recover the watermark behaviors of these perturbed models. To address the domain shift issue between the watermark embedding and removal phases, the method estimates BatchNorm statistics only using clean samples when handling watermark inputs. Extensive experiments demonstrate that this approach consistently improves watermark robustness against several state-of-the-art removal attacks.


## What problem or question is the paper addressing?

The paper is addressing the problem of vulnerability of backdoor-based model watermarking against removal attacks. The key questions it investigates are:- Why are existing backdoor-based watermarking methods vulnerable to removal attacks like fine-tuning? - How can we make the watermarking more robust against such removal attacks?Specifically, the paper finds that there exist many watermark-removed models in the vicinity of the original watermarked model in the parameter space. These models have low watermark success rates but maintain high accuracy on clean data. This makes it easy for removal attacks like fine-tuning to find one of these models and remove the watermark. To address this vulnerability, the paper proposes a minimax formulation that finds these watermark-removed models in the vicinity and recovers their watermark behavior. This is done by adding adversarial parametric perturbations during training and minimizing the watermark loss. The paper also handles the domain shift issue between defense and attacks by using clean samples to estimate batch norm statistics.In summary, the key questions addressed are: 1) Why existing watermarking is vulnerable, which is due to the existence of nearby watermark-removed models. 2) How to make watermarking more robust, which is by the proposed minimax formulation and customized batch norm.
