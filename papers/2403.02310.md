# [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/abs/2403.02310)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- LLM inference involves a prefill phase to process the input prompt and a decode phase to generate output tokens. Prefills benefit little from batching but decodes see large throughput gains.
- However, batching requests leads to interleaving of prefill and decode iterations, causing a tradeoff between throughput and tail latency (time between output tokens).
- Prioritizing prefills boosts throughput but pauses ongoing decodes, causing generation stalls and spikes in tail latency. Prioritizing decodes avoids stalls but compromises on throughput.

Proposed Solution: 
- The paper proposes Sarathi Serve, a scheduling framework that leverages chunked-prefills and stall-free batching.
- Chunked-prefills break up large input prompts into smaller chunks processed over multiple iterations. This bounds prefill latency.  
- Stall-free batching coalesces decode tokens with prefill chunks while throttling prefill tokens to minimize impact on ongoing decodes.  

Main Contributions:
- Identifies fundamental limitations in existing LLM schedulers in optimizing for both throughput and tail latency.
- Introduces techniques like chunked-prefills and stall-free batching to enable high throughput with low tail latency.
- Evaluates Sarathi Serve on multiple models and shows up to 2.6x higher throughput under tail latency constraints compared to state-of-the-art systems.

In summary, the paper addresses the throughput-latency tradeoff in LLM serving through novel scheduling techniques that chunk large input prompts and create mixed batches with bounded prefill latency, unlocking substantial performance gains.


## Summarize the paper in one sentence.

 This paper introduces Sarathi-Serve, a large language model inference scheduling system that uses techniques like chunked-prefills and stall-free batching to achieve high throughput while meeting tail latency constraints.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing a new LLM inference scheduler called Sarathi-Serve that optimizes both throughput and latency. The key ideas are:

1) Chunked-prefills: Breaking down large prefill computations into smaller chunks that can be piggybacked onto decode iterations. This allows packing more computations per iteration while bounding latency. 

2) Stall-free batching: Admitting new requests into a running batch without pausing ongoing decodes. This maximizes throughput while minimizing the impact on time-between-tokens (TBT) latency.

Together, these techniques enable Sarathi-Serve to achieve much higher throughput than prior systems like Orca and vLLM while meeting tail latency SLOs. The evaluations show 2.6-6.9x higher throughput within desired latency targets on various models and hardware configurations.

So in summary, the main contribution is a new inference scheduling technique that navigates the throughput-latency tradeoff by co-optimizing both objectives, outperforming prior state-of-the-art.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Large language models (LLMs)
- Prefill phase
- Decode phase
- Batching
- Throughput
- Latency
- Time-to-first-byte (TTFB)
- Time-between-tokens (TBT)
- Service level objectives (SLOs)  
- Scheduling policies
- Request-level batching
- Iteration-level batching
- Prefill-prioritizing 
- Decode-prioritizing
- Generation stalls
- Chunked prefills
- Stall-free batching
- Sarathi
- Sarathi-Serve

The paper introduces Sarathi-Serve, a system for efficient LLM inference scheduling. It leverages techniques like chunked prefills and stall-free batching to optimize throughput and latency tradeoffs compared to prior LLM serving systems. The key ideas focus on handling challenges with the prefill and decode phases of LLM inference through novel scheduling approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How exactly does chunked-prefills work to split a large prefill into smaller chunks? What considerations go into determining the optimal chunk size?

2. Could you explain in more detail the concept of stall-free batching? How does it allow new requests to join a running batch without pausing ongoing decodes? 

3. The paper argues that neither prefill-prioritizing nor decode-prioritizing schedules are ideal for optimizing both throughput and latency. Can you elaborate on the specific pitfalls of each approach?

4. What is the tile-quantization effect and how does it influence the choice of token budget when creating batches in Sarathi-Serve?

5. How does Sarathi-Serve balance the tradeoff between prefill overhead and decode latency when determining the optimal token budget? Walk through the key factors it considers.

6. One of the goals is to maximize both compute and bandwidth utilization during inference. Can you explain why decode batches tend to be memory-bound while prefill batches tend to be compute-bound? 

7. What causes generation stalls in systems using iteration-level batching? And how does stall-free batching specifically eliminate these stalls?

8. Why is time-to-first-byte not fully independent of time-between-tokens and throughput in Sarathi-Serve? Explain the inherent tradeoff.  

9. How does pipeline parallelism exacerbate the problem of pipeline bubbles? And what techniques does Sarathi-Serve use to minimize bubbles?

10. Could Sarathi-Serve be combined with model-level optimizations like mixture-of-experts or innovations like retentive networks? If so, how?
