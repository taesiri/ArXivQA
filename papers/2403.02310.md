# [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/abs/2403.02310)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- LLM inference involves a prefill phase to process the input prompt and a decode phase to generate output tokens. Prefills benefit little from batching but decodes see large throughput gains.
- However, batching requests leads to interleaving of prefill and decode iterations, causing a tradeoff between throughput and tail latency (time between output tokens).
- Prioritizing prefills boosts throughput but pauses ongoing decodes, causing generation stalls and spikes in tail latency. Prioritizing decodes avoids stalls but compromises on throughput.

Proposed Solution: 
- The paper proposes Sarathi Serve, a scheduling framework that leverages chunked-prefills and stall-free batching.
- Chunked-prefills break up large input prompts into smaller chunks processed over multiple iterations. This bounds prefill latency.  
- Stall-free batching coalesces decode tokens with prefill chunks while throttling prefill tokens to minimize impact on ongoing decodes.  

Main Contributions:
- Identifies fundamental limitations in existing LLM schedulers in optimizing for both throughput and tail latency.
- Introduces techniques like chunked-prefills and stall-free batching to enable high throughput with low tail latency.
- Evaluates Sarathi Serve on multiple models and shows up to 2.6x higher throughput under tail latency constraints compared to state-of-the-art systems.

In summary, the paper addresses the throughput-latency tradeoff in LLM serving through novel scheduling techniques that chunk large input prompts and create mixed batches with bounded prefill latency, unlocking substantial performance gains.
