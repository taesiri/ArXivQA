# [Easy and Efficient Transformer : Scalable Inference Solution For large   NLP model](https://arxiv.org/abs/2104.12470)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis appears to be:Whether it is possible to develop a highly optimized inference library called Easy and Efficient Transformer (EET) that can significantly accelerate transformer model inference compared to existing solutions. The authors propose techniques like mask fusion, thread block folding, and dynamic memory management to optimize transformer inference on GPUs. The paper then validates through experiments that EET achieves 1.4-4.2x speedup over the previous state-of-the-art FasterTransformer library for transformer decoder layers.So in summary, the central hypothesis is that with careful algorithmic and implementation optimizations, the authors can develop a transformer inference library (EET) that is substantially faster than prior work. The paper then provides techniques and experimental validation to support this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- The authors propose Easy and Efficient Transformer (EET), a novel transformer inference acceleration library.- They introduce two key optimizations: 1) Mask fusion - Fusing the attention and padding masks into the computation to avoid overhead of explicitly applying those masks.2) Thread block folding - A technique to extend the CUDA kernels to support larger model sizes and sequence lengths by folding thread blocks.- They design a dynamic CUDA memory manager to reduce memory footprint when deploying large models.- Comprehensive experiments show EET achieves 1.4-4.2x speedup over the previous state-of-the-art FasterTransformer library on transformer decoder layers.- EET supports much larger model sizes (up to 18 billion parameters) and longer sequence lengths (up to 4096) compared to prior inference libraries like FasterTransformer and LightSeq.In summary, the main contribution is proposing optimizations to achieve faster inference for large transformer models, while also supporting bigger models and longer sequences. The EET library outperforms prior work by a significant margin.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a highly optimized transformer inference library called Easy and Efficient Transformer (EET) that achieves significant speedups compared to existing libraries by using custom kernels with mask fusion and thread block folding techniques as well as a dynamic memory manager.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research on optimizing transformer inference:- The paper introduces a new library called Easy and Efficient Transformer (EET) for fast inference of large transformer models. This adds to existing inference libraries like TensorRT, LightSeq, and FasterTransformer.- A key contribution is custom CUDA kernels with optimizations like "mask fusion" to avoid computational overhead of attention masks, and "thread block folding" to support larger model sizes and sequence lengths. These optimize beyond what previous libraries have done.- The paper proposes a dynamic CUDA memory manager to reduce memory footprint when deploying large models. This is novel compared to manual memory management in other libraries. - Experiments show EET achieves 1.4-4.2x speedup over latest FasterTransformer, and around 5-20x over baseline PyTorch. This is a significant improvement over state-of-the-art optimizations.- The solutions focus on transformer decoder, which is important for text generation tasks. Libraries like TensorRT currently only support encoder architectures.- EET supports very large models (up to 12k hidden size tested) which exceeds capabilities of other libraries like LightSeq (capped at 1k).Overall, this paper pushes state-of-the-art in transformer optimization further with custom CUDA kernels and memory management. The EET library outperforms existing options significantly, especially for large models and sequence lengths. This could enable broader deployment of giant language models for industrial applications.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:- Further optimization of the transformer inference kernels and implementation. The authors note there is still room for improvement in optimizing the CUDA kernels and memory management strategies. They suggest this could involve techniques like kernel auto-tuning, more advanced memory pooling strategies, etc.- Supporting more diverse transformer architectures and tasks. The current EET library focuses primarily on optimizing inference for BERT and GPT-style architectures. The authors suggest expanding it to also optimize other types of transformers like T5, ViT, etc. and different downstream tasks beyond text generation.- Deployment of larger models. The optimizations allow fitting larger transformer models on a single GPU. The authors suggest this could enable research into even larger pretrained models beyond what can currently be deployed.- Quantization and compression. The authors note quantization and model compression techniques as complementary ways to optimize inference. They suggest integrating these into the EET framework could provide further speedups.- Distributed inference. The current EET library focuses on single GPU inference. The authors suggest exploring optimizations for distributed inference across multiple GPUs/machines as another direction.In summary, the main future directions are continued optimization of the inference engines, broadening support for more model types and tasks, enabling larger models, and distributed/quantized inference. The overall goal is to make state-of-the-art transformer models more efficient and practical to deploy for industrial applications.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper introduces Easy and Efficient Transformer (EET), a library for optimized inference of large transformer models. The authors propose techniques like mask fusion to avoid computational overhead from attention masks, thread block folding to support very large model sizes, and dynamic memory management to reduce memory footprint. Compared to other libraries like Fairseq and FasterTransformer, EET achieves 1.4-4.2x speedup on transformer decoder layers with an A100 GPU. The techniques allow EET to handle longer input sequences and larger model sizes than prior work. EET also provides model-level and operation-level APIs for easy integration into applications. The results demonstrate EET's effectiveness for efficient deployment of large transformer models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces Easy and Efficient Transformer (EET), a library for accelerating inference of large transformer models on GPUs. The authors optimize transformer inference through custom CUDA kernels and a flexible memory manager. First, they design highly optimized kernels that avoid explicitly computing attention and padding masks. This removes overhead associated with initializing and transferring these masks to the GPU. Second, they propose a "thread block folding" technique to support very large model sizes up to 12,288 and sequence lengths up to 4,096, overcoming limitations of prior work. Finally, they implement a dynamic CUDA memory manager that reduces memory footprint by reusing buffers and caches across layers and modules. Experiments demonstrate EET's effectiveness. Compared to Fairseq and the FasterTransformer library, EET achieves 1.4-4.2x speedups on the GPT-2 decoder layer and up to 0.99-1.27x speedups on the BERT encoder layer. EET also supports much larger models, enabling 18 billion parameter models on a single GPU compared to PyTorch's 10 billion parameter limit. The optimizations in EET enable fast and scalable inference for large transformer models, making them more feasible for real-world deployment.
