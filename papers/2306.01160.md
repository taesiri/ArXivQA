# [Faster Causal Attention Over Large Sequences Through Sparse Flash   Attention](https://arxiv.org/abs/2306.01160)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we extend FlashAttention to efficiently handle sparse attention patterns beyond just the standard causal triangular mask?The key ideas and contributions seem to be:- Developing a Sparse Causal Flash Attention (SCFA) GPU kernel that can handle arbitrary sparsity patterns expressed as ranges of keys per query.- Showing how SCFA enables efficient implementations of dynamic hash-based attention (extending ideas from Reformer) as well as query/key dropping attention. - Demonstrating speedups and maintained perplexity on language modeling tasks using SCFA for hash-based and query/key dropping attention, compared to standard FlashAttention.So in summary, the main goal is to expand FlashAttention to support more flexible sparse attention patterns, beyond just the fixed causal mask. This then enables faster implementations of promising dynamic sparse attention methods like hashing and query/key dropping.
