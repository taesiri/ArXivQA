# [Faster Causal Attention Over Large Sequences Through Sparse Flash   Attention](https://arxiv.org/abs/2306.01160)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we extend FlashAttention to efficiently handle sparse attention patterns beyond just the standard causal triangular mask?The key ideas and contributions seem to be:- Developing a Sparse Causal Flash Attention (SCFA) GPU kernel that can handle arbitrary sparsity patterns expressed as ranges of keys per query.- Showing how SCFA enables efficient implementations of dynamic hash-based attention (extending ideas from Reformer) as well as query/key dropping attention. - Demonstrating speedups and maintained perplexity on language modeling tasks using SCFA for hash-based and query/key dropping attention, compared to standard FlashAttention.So in summary, the main goal is to expand FlashAttention to support more flexible sparse attention patterns, beyond just the fixed causal mask. This then enables faster implementations of promising dynamic sparse attention methods like hashing and query/key dropping.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The authors develop an efficient GPU kernel called Sparse Causal Flash Attention (SCFA) that extends the FlashAttention method to handle irregular sparsity patterns in the attention matrix. FlashAttention is limited to regular lower triangular masking, while SCFA can handle arbitrary query/key selections and hash-based blocking.2. The authors use SCFA to revisit hash-based sparse attention, developing an algorithm that restricts attention to hash collision blocks. Unlike prior work like Reformer, their method computes the exact attention while avoiding the approximation and coverage issues of existing hashing schemes. 3. The authors propose an approach implemented in SCFA for dynamically selecting queries and keys to remove per head, instead of having to prune entire heads or tokens. This provides finer-grained sparsification. 4. They demonstrate experimentally that SCFA enables efficient training of Transformers with dynamic sparse attention patterns, significantly speeding up training over full FlashAttention for long sequences. The hash-based attention outperforms Reformer and matches perplexity of full attention baselines while speeding up training. Query/key dropping also accelerates training without much perplexity loss.In summary, the main contribution appears to be the SCFA kernel and how it enables and accelerates various forms of dynamic sparse attention during Transformer training. This allows revisiting promising sparsity paradigms like hashing and dropping, while avoiding limitations of prior implementations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an extension of the FlashAttention method to efficiently handle irregular sparsity patterns in the Transformer self-attention, enabling faster training of language models through techniques like hashing attention and dropping queries/keys.
