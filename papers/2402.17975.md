# [Sample-Efficient Preference-based Reinforcement Learning with Dynamics   Aware Rewards](https://arxiv.org/abs/2402.17975)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Preference-based reinforcement learning (PbRL) trains policies by learning reward functions from human preferences over agent behaviors. However, existing methods require an impractical number of preference samples. The paper hypothesizes that encoding environment dynamics into the learned reward function will improve sample efficiency by enabling better generalization over similar states and actions.  

Proposed Solution:
The paper proposes "Rewards Encoding Environment Dynamics" (REED), which incorporates a self-supervised temporal consistency auxiliary task into reward learning. Specifically, REED uses self-predictive representations (SPR) to learn a dynamics-aware state-action embedding that must predict the next state representation. This is jointly trained with the preference prediction task over trajectories. REED increases the effective training data for the reward function by using all transitions experienced by the policy rather than just the labelled trajectories.

Main Contributions:
- Proposes REED to incorporate explicit environment dynamics into preference-based reward learning via an SPR auxiliary task
- Demonstrates REED matches policy performance of baselines using 10x less preference labels
- Shows encoding dynamics leads to more stable reward learning across updates 
- Analyzes reward re-use and shows REED rewards enable better generalization to new policies
- Evaluates on locomotion and manipulation tasks with both state and image observations
- Compares against other PbRL methods like SURF, RUNE and shows REED most consistently improves sample efficiency

The core insight is that encoding dynamics into the reward function enables better generalization and stability for preference learning problems. REED provides order of magnitude sample efficiency gains across tasks, labellers and observation types.
