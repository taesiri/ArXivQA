# [Unsupervised Perceptual Rewards for Imitation Learning](https://arxiv.org/abs/1612.06699)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we learn reward functions for complex real-world robotic manipulation tasks from raw pixel observations of human demonstrations, without requiring manual engineering of rewards or additional instrumentation?The key ideas and contributions in addressing this question appear to be:1. Using unsupervised video segmentation to break down demonstrations into sub-goals/steps.2. Leveraging pre-trained deep visual features to learn classifiers that identify these sub-goals from raw pixels, thereby creating dense incremental reward functions. 3. Showing that these learned perceptual reward functions can be used by a real robot to learn complex skills like door opening through its own experience, even when trained on human demonstrations.So in summary, the central hypothesis seems to be that pre-trained deep visual features can be used to automatically learn reward functions and sub-goals for complex real-world manipulation skills from raw pixel demonstrations, thereby enabling more efficient and scalable imitation learning on robots. The experiments and results demonstrate this approach on real robotic door opening tasks learned from human demonstrations.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. A method for learning perceptual reward functions from only a few demonstrations of real-world tasks, using unsupervised discovery of intermediate steps. 2. The first vision-based reward learning method that can learn a complex robotic manipulation task directly from human demonstrations, without any robot state information. This is demonstrated through real-world robotic experiments on door opening.3. An analysis showing that the learned visual representations in a pre-trained deep model are general enough to be directly used for representing goals and sub-goals for new manipulation tasks, without needing to retrain the features.4. Evidence that there exist small subsets of discriminative features in these pre-trained models that can be used for step classification, which could help enable unsupervised step discovery in future work.In summary, the key innovation is using powerful pre-trained deep visual features to enable very efficient reward learning directly from human demonstrations, without needing extensive robot experience or instrumentation of the environment. This could help overcome two major obstacles in deploying RL for real-world robot learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method for robots to quickly learn visual reward functions and complex real-world manipulation skills from just a few human demonstrations, without needing manually engineered rewards or state representations.
