# [Unsupervised Perceptual Rewards for Imitation Learning](https://arxiv.org/abs/1612.06699)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we learn reward functions for complex real-world robotic manipulation tasks from raw pixel observations of human demonstrations, without requiring manual engineering of rewards or additional instrumentation?The key ideas and contributions in addressing this question appear to be:1. Using unsupervised video segmentation to break down demonstrations into sub-goals/steps.2. Leveraging pre-trained deep visual features to learn classifiers that identify these sub-goals from raw pixels, thereby creating dense incremental reward functions. 3. Showing that these learned perceptual reward functions can be used by a real robot to learn complex skills like door opening through its own experience, even when trained on human demonstrations.So in summary, the central hypothesis seems to be that pre-trained deep visual features can be used to automatically learn reward functions and sub-goals for complex real-world manipulation skills from raw pixel demonstrations, thereby enabling more efficient and scalable imitation learning on robots. The experiments and results demonstrate this approach on real robotic door opening tasks learned from human demonstrations.
