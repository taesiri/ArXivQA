# [Unsupervised Perceptual Rewards for Imitation Learning](https://arxiv.org/abs/1612.06699)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:How can we learn reward functions for complex real-world robotic manipulation tasks from raw pixel observations of human demonstrations, without requiring manual engineering of rewards or additional instrumentation?The key ideas and contributions in addressing this question appear to be:1. Using unsupervised video segmentation to break down demonstrations into sub-goals/steps.2. Leveraging pre-trained deep visual features to learn classifiers that identify these sub-goals from raw pixels, thereby creating dense incremental reward functions. 3. Showing that these learned perceptual reward functions can be used by a real robot to learn complex skills like door opening through its own experience, even when trained on human demonstrations.So in summary, the central hypothesis seems to be that pre-trained deep visual features can be used to automatically learn reward functions and sub-goals for complex real-world manipulation skills from raw pixel demonstrations, thereby enabling more efficient and scalable imitation learning on robots. The experiments and results demonstrate this approach on real robotic door opening tasks learned from human demonstrations.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. A method for learning perceptual reward functions from only a few demonstrations of real-world tasks, using unsupervised discovery of intermediate steps. 2. The first vision-based reward learning method that can learn a complex robotic manipulation task directly from human demonstrations, without any robot state information. This is demonstrated through real-world robotic experiments on door opening.3. An analysis showing that the learned visual representations in a pre-trained deep model are general enough to be directly used for representing goals and sub-goals for new manipulation tasks, without needing to retrain the features.4. Evidence that there exist small subsets of discriminative features in these pre-trained models that can be used for step classification, which could help enable unsupervised step discovery in future work.In summary, the key innovation is using powerful pre-trained deep visual features to enable very efficient reward learning directly from human demonstrations, without needing extensive robot experience or instrumentation of the environment. This could help overcome two major obstacles in deploying RL for real-world robot learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a method for robots to quickly learn visual reward functions and complex real-world manipulation skills from just a few human demonstrations, without needing manually engineered rewards or state representations.


## How does this paper compare to other research in the same field?

 This paper presents a novel method for learning reward functions for robotic reinforcement learning tasks directly from raw pixel inputs in a few human demonstrations, without needing any state information or instrumentation on the demonstrated objects. Here are some key ways it compares to other related work:- Most prior work on learning rewards from demonstrations has focused on low-dimensional state spaces and required many demonstrations. This method works directly from images and needs only a few demonstrations (as few as 2-3).- Some prior methods have used images or pixels for reward learning, but they often rely on specifying a target image or distance in pixel space. This can lack generalizability. This method learns more generalizable semantic reward functions from pre-trained deep features.- A few recent methods have combined deep learning and inverse RL, but they still require instrumentation (like robot joint angles and end effector poses). This method learns only from raw human videos, without any state information.- It's the first method to show complex real robotic skills can be learned from human video demonstrations using these learned visual rewards, like opening a real door. Most prior work was either simulation only or required robot demos.So in summary, the key advances are using deep visual features to learn generative visual rewards from just a few raw human demonstrations, and showing these can be used to learn real-world robotic manipulation skills. This helps move toward more practical imitation learning that doesn't require perfect state information or embodiment matching. The simple approach also helps scale these methods.


## What future research directions do the authors suggest?

 The paper suggests a few potential future research directions:1. Improving the unsupervised video segmentation method for discovering steps/subgoals. The authors use a simple recursive segmentation method, but note that more advanced unsupervised video segmentation techniques could allow handling a broader set of demonstrations.2. Reducing the search space for unsupervised segmentation using the insight that a small subset of discriminative features is sufficient. The authors show that a small number of pre-trained deep features (32-64) can classify steps reasonably well. This suggests the search space could be drastically reduced by only considering correlations between these most discriminative features rather than all features. 3. Studying the impact of viewpoint changes. The experiments used a fixed viewpoint between the demonstrator and robot. Analyzing how the method degrades with larger viewpoint differences could lead to more robust techniques.4. Enabling lifelong/continual learning. The reward functions could enable continuous improvement as more demonstrations are collected over the robot's lifetime. Studying how to leverage diverse experience over time to acquire increasingly general skills is an interesting direction.5. Generalizing to more modalities beyond vision. The authors focus on visual features, but note the approach could be extended to rewards based on other sensory modalities like audio or touch.Overall, the main future directions are improving the segmentation method, exploiting discriminative features to reduce segmentation search space, evaluating viewpoint robustness, lifelong learning, and extending beyond vision to other modalities. The authors propose an intriguing method for efficient visual reward learning and suggest promising ways to build on it.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes an unsupervised method for learning perceptual reward functions from a few demonstrations of real-world tasks, which can then be used by a reinforcement learning agent to learn to perform those tasks. The key ideas are 1) using the features from a pre-trained deep network as a representation for learning rewards, without finetuning the features, 2) automatically discovering important subgoals and steps of a task by segmenting the demonstrations based on changes in those feature activations, 3) learning simple classifiers for each step using the features, and combining them into a reward function, 4) showing that these learned rewards can be used by a real robot to learn complex skills like door opening using only a handful of raw human demonstrations as supervision. The main contributions are developing a method that can learn vision-based rewards for real robotic tasks from just a few raw human demos, showing that pretrained deep features can be used directly to represent goals without finetuning, and demonstrating real robotic learning of a complex skill using human demos and learned rewards.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a method for learning perceptual reward functions from a small number of demonstrations for real-world robotic manipulation tasks. The key idea is to leverage the powerful visual features learned by deep convolutional networks for image classification to identify important sub-goals and steps of a task. The method first segments the demonstrations into steps based on changes in the deep visual features. It then trains classifiers to identify each step, and combines them into a full reward function. This reward can then be used by a reinforcement learning algorithm on a robot to learn the demonstrated task. The approach is evaluated on two real-world tasks - door opening and pouring liquid. Qualitative results show the method can learn sensible reward functions from just a few demonstrations. Quantitative experiments also show it outperforms baselines in segmenting demonstrations and learning reward functions. Finally, they demonstrate that the method can be used to teach a robotic arm to open a door using rewards learned from videos of humans, without any robot demonstration data.In summary, this paper presents a novel approach to efficiently learn visual reward functions by exploiting deep pre-trained visual features. It requires only a small number of videos to learn complex real-world robotic skills. The method is simple but shows promising results on real-world door opening and liquid pouring tasks. By learning rewards from human demonstrations, it provides an intuitive approach to imitation learning without the need to mimic human embodiment.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. The method first segments the demonstrations into steps based on perceptual similarity. These segments correspond to sub-goals or steps of the task. The segments are then used to train classifiers for each step on top of the mid and high-level representations of a pre-trained deep model. This produces a reward function for each step. These intermediate reward functions are combined into a single reward function that partially rewards intermediate steps but emphasizes later rewards. This combined reward function can then be used by a reinforcement learning agent to learn the demonstrated task. The key aspects are using pre-trained deep visual features to enable learning from small numbers of demonstrations, unsupervised discovery of intermediate steps, and learning incremental reward functions that enable efficient reinforcement learning of the full task.


## What problem or question is the paper addressing?

 The paper describes an approach for learning reward functions for complex robotic manipulation skills from demonstrations. The main problems it is trying to address are:1. Designing reward functions for real-world robotic tasks often requires a lot of hand engineering and additional sensors just to measure task success. This makes deployment difficult. 2. Many real-world tasks have implicit intermediate steps or sub-goals that need to be executed in sequence. Even if the final outcome is measurable, it doesn't provide feedback on these intermediate steps.3. Prior vision-based reward learning methods either required robot state information and kinesthetic demonstrations, or only worked in low-dimensional state spaces with lots of data. Getting vision-based rewards from raw human demonstrations has been difficult.4. The features and representations used for reward learning need to be general enough to work across different scenes and situations without requiring excessive training data.To address these issues, the paper proposes an approach to learn vision-based reward functions from just a few raw video demonstrations using the powerful pre-trained features from deep networks. It aims to automatically identify sub-goals and key steps from the videos without manual annotation. The resulting dense, incremental reward functions can then be used by a robot to learn the demonstrated manipulation skills in real-world settings.The main contributions summarized are:- Learning incremental, dense rewards and discovering intermediate steps in an unsupervised manner from just a few real-world demonstrations - Demonstrating the first vision-based reward learning method that can acquire complex skills from raw human videos in real robot experiments- Showing that pre-trained deep features can be used directly to represent goals and rewards for new scenes without finetuning- Identifying that small subsets of pre-trained features are highly discriminative for new tasks, enabling more efficient search for correlations across demonstrations in future work
