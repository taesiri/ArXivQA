# [Fine-tuning can cripple your foundation model; preserving features may
  be the solution](https://arxiv.org/abs/2308.13320)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a new regularizer called LDIFS that minimizes the $\ell_2$ distance between features of the original foundation model and the model being fine-tuned. How does this differ from previous regularization methods like L2SP that focus on minimizing distance in parameter space? What are the relative advantages of using feature space distance?

2. The paper argues that minimizing distance in feature space better preserves the input-output behavior and encoded knowledge of the original foundation model. But could minimizing distance in feature space potentially limit the model's ability to learn completely new representations on the downstream task? How would you balance preserving prior knowledge versus learning new concepts?

3. When computing LDIFS, the paper extracts features from different layers of the network rather than just the last layer. What is the motivation behind using multi-layer features? Does using only the last layer's features lead to worse performance? 

4. How sensitive is the performance of LDIFS to the choice of layers used for extracting features? Did the authors experiment with different combinations or numbers of layers? Is there an optimal strategy for choosing which layers to use?

5. The LDIFS regularizer introduces a new hyperparameter Î»_{LDIFS} to control the relative weight of feature space distance. How does tuning this parameter impact model performance? Is LDIFS very sensitive to the exact value chosen?

6. For the continual learning experiments, how did the authors choose the order of tasks for fine-tuning? Could the ordering or curriculum impact how well LDIFS accumulates knowledge over multiple tasks?

7. The paper benchmarks several end-to-end fine-tuning methods in addition to LDIFS. What differences did the authors notice in how these methods balance downstream performance versus concept forgetting?

8. How does the choice of pre-trained foundation model impact the concept forgetting phenomena studied in this paper? Would a different model architecture lead to different conclusions?

9. The paper studies image classification tasks exclusively. Do you think concept forgetting would manifest differently in other domains like NLP? How could the LDIFS approach be adapted?

10. The authors propose that concept forgetting is an important problem for foundation models. But could some forgetting of irrelevant concepts also be beneficial? Is complete knowledge retention always the right goal?
