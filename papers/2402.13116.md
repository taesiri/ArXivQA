# [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper presents a comprehensive survey on knowledge distillation techniques for large language models (LLMs). The key problem highlighted is the gap between advanced yet inaccessible proprietary LLMs like GPT-4 and more available open-source models like LLaMA. Knowledge distillation serves as a bridge to transfer capabilities from proprietary giants to accessible models. 

The survey is structured around three pillars - algorithms, skill distillation, and verticalization. On algorithms, it examines eliciting knowledge from teacher LLMs through methods like labeling, expansion, curation, features, feedback and self-knowledge. Core distillation algorithms covered include divergence minimization, similarity optimization, reinforcement learning and ranking.

On skill distillation, the paper explores enhancing context following skills like instruction following, multi-turn dialogues and retrieval augmented capabilities. It also looks at aligning student models on preferences, values and thinking patterns. Abilities like tool usage, planning, NLP task specialization and multi-modality handling are also addressed.  

Finally, on domain-specific verticalization, tailoring distilled models for law, healthcare, finance, science and other verticals is analyzed. This showcases the practical utility of knowledge distillation.

The key contributions are providing a structured taxonomy bounding algorithms, skills and vertical domains, highlighting the interplay between data augmentation and distillation, examining both skill and domain enhancements enabled by distillation, and elucidating directions to make AI solutions more accessible, efficient and sustainable through knowledge transfer.

Overall, this paper significantly advances understanding of knowledge distillation for LLMs, serving as an insightful guide for researchers and practitioners.
