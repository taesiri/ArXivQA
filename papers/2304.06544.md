# [DNeRV: Modeling Inherent Dynamics via Difference Neural Representation   for Videos](https://arxiv.org/abs/2304.06544)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- Existing neural representation (INR) methods for videos do not fully exploit spatiotemporal redundancies. Index-based INRs ignore content-specific spatial features while hybrid INRs ignore contextual dependencies between frames. This leads to poor modeling capability for scenes with large motion or dynamics.

- The authors propose a new video INR method called Difference Neural Representation for Videos (DNeRV) to address these limitations. DNeRV consists of two streams - one for content and one for frame differences. This allows it to model both spatial features and temporal dependencies. 

- DNeRV introduces a collaborative content unit (CCU) for effective fusion of the two streams. The CCU helps capture adjacent dynamics through a gated mechanism.

- Experiments on video compression, inpainting and interpolation show DNeRV achieves competitive compression results and outperforms prior video INRs, especially on dynamic scenes. This demonstrates its effectiveness at video representation.

In summary, the key hypothesis is that modeling both content and temporal differences through a two-stream architecture with effective fusion can improve video INRs, particularly for dynamic scenes. DNeRV is proposed to test this hypothesis.
