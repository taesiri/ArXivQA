# [TVQA+: Spatio-Temporal Grounding for Video Question Answering](https://arxiv.org/abs/1904.11574)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question/hypothesis addressed is: Can jointly grounding video question answering in both the spatial and temporal domains (by detecting referenced objects and temporally localizing relevant clips) improve video QA performance and interpretability? The authors propose the new task of spatio-temporal video QA, which requires localizing relevant moments, detecting referenced objects, and answering questions about videos. They create a new dataset (TVQA+) with spatial and temporal annotations to support this task. Their proposed model, Spatio-Temporal Answerer with Grounded Evidence (STAGE), is designed to perform all three subtasks jointly in an end-to-end manner. Their hypothesis is that by supervising the model to ground the evidence spatially and temporally, it will learn improved representations and achieve better video QA performance compared to models that do not explicitly perform grounding. The interpretability of the model will also be improved via visualizations of the spatial and temporal attention/grounding.The experiments aim to demonstrate:(1) STAGE achieves significantly better QA accuracy compared to baseline methods on the TVQA+ dataset. (2) Explicitly supervising spatial and temporal grounding leads to gains in QA accuracy.(3) STAGE produces insightful visual explanations of its predictions via spatial and temporal attention.In summary, the main hypothesis is that joint spatio-temporal grounding of evidence will improve video QA accuracy and interpretability. The TVQA+ dataset and STAGE model are designed to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions of this work are:1. The authors collect a new dataset called TVQA+ which augments the existing TVQA video question answering dataset with 310,826 bounding boxes linking depicted objects to visual concepts in the questions and answers. This allows for joint spatio-temporal grounding of the QA pairs.2. The authors propose a novel end-to-end trainable neural framework called STAGE (Spatio-Temporal Answerer with Grounded Evidence) which can jointly localize relevant moments, detect referenced objects/people, and answer questions about videos. 3. Through comprehensive experiments and analyses, the authors demonstrate the effectiveness of the proposed model on spatio-temporal video QA using the new TVQA+ dataset. They show performance gains from having both temporal and spatial supervision. The interpretable visualizations of temporal and spatial attention are also useful.In summary, the key contribution is the proposal of spatio-temporal video QA, along with a new grounded dataset and model designed for this joint task. The new annotations and multitask learning approach allow the model to achieve significantly better video QA performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from this paper:The paper introduces TVQA+, a video question answering dataset with 310K bounding box annotations linking objects to visual concepts, and proposes a model called STAGE that jointly localizes relevant clips, detects referenced objects and people, and answers questions about videos to achieve new state-of-the-art performance on the TVQA benchmark.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other related research:- This paper introduces a new dataset, TVQA+, which provides spatial grounding (bounding box annotations) in addition to temporal grounding for a video QA dataset. Most prior video QA datasets like TVQA, MovieQA, etc. only have temporal annotations. The spatial grounding is a novel contribution.- The proposed model STAGE performs joint moment localization, object detection, and QA in an end-to-end framework. Other video QA methods like Two-Stream, ST-VQA don't explicitly localize objects or moments. Some visual grounding works like TALL, Charades-STA focus only on moment localization. - This paper shows strong performance gains from incorporating spatial and temporal supervision signals for video QA. Prior works have not explored this joint training. The visualization of attention also provides more interpretability.- Compared to generic image-text pretrained models like LXMERT, features fine-tuned on in-domain datasets perform better. This highlights the domain gap between existing VQA datasets and the subtitles + QA nature of TVQA/TVQA+.- There is still a significant gap between the STAGE model and human performance on TVQA+, indicating scope for improvement. The "why" and "how" questions are specifically challenging. More sophisticated reasoning mechanisms may be needed.Overall, the novel dataset annotation, end-to-end trainable model, and analyses of different supervision signals are valuable contributions compared to prior art. The interpretability and performance gains demonstrate the importance of grounded representations for video QA.
