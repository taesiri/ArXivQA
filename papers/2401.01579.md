# [An Invariant Information Geometric Method for High-Dimensional Online   Optimization](https://arxiv.org/abs/2401.01579)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Sample efficiency is crucial in black-box optimization problems with expensive function evaluations and zeroth-order feedback. 
- Bayesian optimization works well but has poor scalability to high dimensions due to cubic complexity. 
- Evolution strategies are more scalable but lack theoretical foundations to develop further.

Proposed Solution:
- Develop a framework (InvIGO) that incorporates historical information in a fully invariant and scalable way for online optimization. 
- Exemplify the framework with multi-dimensional Gaussians to derive an invariant and scalable optimizer (SynCMA) with fewer hyperparameters than other CMA optimizers.
- SynCMA fully incorporates historical information for both mean and covariance parameters in a stable way, which has not been done before.

Contributions:  
- First invariant optimizer framework (InvIGO) for online black-box optimization that incorporates historical information in a scalable way.
- Derived an invariant optimizer (SynCMA) from the framework using multi-dimensional Gaussians.
- SynCMA shows great competence against leading Bayesian (TuRBO) and evolution strategies (CMA-ES, DD-CMA) in high-dimensional Mujoco, rover planning, and synthetic tasks.
- Demonstrated the potential of property-oriented evolution strategies against Bayesian methods in high dimensions. 
- Showed the importance of full invariance in optimization.

In summary, the paper develops a novel invariant optimization framework and exemplifies it to derive an invariant and scalable optimizer SynCMA. Experiments show SynCMA matches or exceeds the performance of state-of-the-art Bayesian and evolution strategies methods in various high-dimensional tasks.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes an invariant information geometric framework for online black-box optimization that yields a new evolution strategy algorithm with competitive performance against leading Bayesian optimization methods on high-dimensional tasks.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

1) Building the first invariant optimizer framework (InvIGO) for online optimization with ignorant initial and zeroth-order feedback. This framework fully incorporates historical information while retaining invariant properties and computational complexity.

2) Exemplifying the InvIGO framework on multi-dimensional Gaussian distribution, which gives rise to an invariant and scalable optimizer called SynCMA. This algorithm inherits all the properties of InvIGO and has fewer hyperparameters compared to other CMA algorithms. It is also the first optimizer that stably incorporates historical information for both the mean and covariance parameters. 

3) Benchmarking SynCMA against leading Bayesian optimization and evolution strategy algorithms on various high dimensional tasks, including Mujoco locomotion, rover planning, and synthetic functions. The results demonstrate SynCMA's great competence, if not dominance, over the other optimizers in terms of sample efficiency. This shows the underdeveloped potential of property-oriented evolution strategies.

In summary, the main contribution is proposing the InvIGO framework and exemplifying it to derive the SynCMA algorithm, which demonstrates strong performance against state-of-the-art optimizers on high-dimensional tasks. This highlights the potential of invariant and property-oriented evolution strategies.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the key terms and keywords associated with it seem to be:

- Invariant optimizer
- Information geometry
- Evolution strategies 
- Bayesian optimization
- Sample efficiency
- High-dimensional optimization
- Online optimization
- Zeroth-order feedback
- Covariance matrix adaptation
- CMA-ES

The paper introduces a new invariant optimization algorithm called "\algo" which is derived from a framework called "\framework". It is compared empirically to leading Bayesian optimization and evolution strategy methods on tasks like Mujoco locomotion, rover trajectory planning, and synthetic benchmark functions. The key features highlighted are its invariance properties, ability to incorporate historical information, and sample efficiency in high dimensional spaces.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper claims the method is "invariant" - what exactly does this mean mathematically? What transformations is it invariant under? How would you rigorously prove the invariance property?

2. The key idea seems to be approximating the IGO objective with a KL divergence term. What is the motivation behind this approximation? How tight is this approximation mathematically? 

3. The method incorporates historical information using an exponential decay weighting. What is the impact of the decay parameter λ on performance? Is there an optimal theoretical value for λ?

4. How does the synchronous update rules for the mean and covariance in SynCMA differ from traditional CMA-ES? What is the motivation behind synchronous updates?

5. Proposition 1 connects SynCMA to standard CMA-ES under certain approximations. What is lost under these approximations? Is there still benefit to using the full SynCMA formulation?  

6. What are the computational and memory complexity of SynCMA compared to other CMA variants? Does incorporating historical information increase complexity?

7. The method has very few hyperparameters compared to CMA-ES. Does this make SynCMA easier to use in practice? How sensitive is performance to choice of hyperparameters?

8. What theoretical guarantees or convergence proofs exist for SynCMA? How do they compare to proofs for CMA-ES or other evolution strategies?

9. The experiments show strong performance on high-dimensional and non-convex problems. Why does SynCMA excel in these regimes compared to Bayesian Optimization?

10. What opportunities exist to extend SynCMA, for example by using different parametric family distributions or making hyperparameters adaptive? What extensions seem most promising?
