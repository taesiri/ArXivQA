# [Teach Me to Explain: A Review of Datasets for Explainable Natural   Language Processing](https://arxiv.org/abs/2102.12060)

## What is the central research question or hypothesis that this paper addresses?

After carefully reading the paper, it seems the central research question is:What can we learn from existing datasets that contain human-annotated textual explanations, and what recommendations can we provide for collecting high-quality explainable NLP datasets in the future? The authors review and summarize the literature on collecting textual explanations for explainable NLP. They identify three main types of textual explanations that have been collected in datasets: highlights, free-text, and structured. The paper analyzes strengths and shortcomings of existing data collection methodologies for each explanation type. It highlights important considerations from the interplay between data collection, modeling, and evaluation assumptions. The authors provide recommendations for future data collection based on what has been learned so far in this emerging research area. The overarching goal is to promote better practices and standardization when creating new textual explanation datasets for explainable NLP.


## What is the main contribution of this paper?

The main contribution of this paper is a comprehensive review and analysis of datasets for explainable natural language processing (ExNLP). Specifically:- The paper surveys and categorizes 65 existing ExNLP datasets into three main types: highlights, free-text explanations, and structured explanations. It provides an overview of each dataset and how the explanations were collected.- It highlights and discusses important lessons learned from analyzing existing ExNLP data collection methodologies. In particular, it focuses on two main case studies: (1) issues with using human-annotated highlights for evaluating model-generated highlights, and (2) the emergence of structured explanations as an alternative to purely free-text explanations. - It synthesizes best practices and gives recommendations for future ExNLP dataset construction. This includes suggestions for ensuring data quality, increasing diversity, embracing natural structure in explanations, and the importance of documenting data collection details and constraints.Overall, the main contribution is a thorough literature review of ExNLP datasets coupled with an insightful critical analysis and synthesis of recommendations for the research community. The paper helps standardize practices around ExNLP dataset creation and highlights important considerations for using existing datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper reviews existing datasets for explainable NLP, categorizes them into three types (highlights, free-text, and structured), analyzes their collection methodologies, and provides recommendations for future dataset creation based on lessons learned.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on explainable natural language processing (ExNLP) datasets:- Scope - This paper provides a broad review of 65 datasets with textual explanations, categorizing them into highlights, free-text, and structured explanations. Other surveys tend to focus on a specific explanation type or application area. For example, some reviews cover just free-text rationales or visual explanations. This paper aims to synthesize findings across different explanation formats.- Critical analysis - The authors provide a more critical analysis of ExNLP dataset creation compared to other surveys. They highlight potential issues like insufficient or non-comprehensive highlights, lack of annotation diversity leading to artifacts, and over-constraining free-text explanations. Other reviews tend to just summarize datasets without as much discussion of limitations.- Recommendations - A key contribution is the set of recommendations for creating higher quality ExNLP datasets. These cover best practices like using a "collect-and-edit" approach, assessing task explainability, and embracing natural structure in explanations. Other surveys focus less on guidelines for future data collection.- Up-to-date - At 65 datasets, this is one of the most comprehensive reviews to date. The authors cover very recent datasets and preprints through 2021. Other surveys cover fewer or older datasets since this is a rapidly expanding field.In summary, this review provides a uniquely broad, critical, and up-to-date analysis of ExNLP datasets compared to other literature surveys. The actionable recommendations for improving dataset creation are a notable addition not found in most reviews.


## What future research directions do the authors suggest?

Here are some of the main future research directions suggested by the authors:- Studying how to best define and utilize the second class of "explaining observed events or phenomena" datasets (discussed in Section 3 and Appendix A). The authors suggest these datasets currently do not directly serve the three main goals of ExNLP, but could potentially be useful in the future.- Exploring contrastive and negative explanations more thoroughly (Section 6.3), such as by collecting contrastive free-text or structured explanations. The authors believe these types of explanations could help train more robust and better-explaining models.- Developing methods to automatically measure meaning overlap between explanations, which is needed to quantify diversity (Section 6.4). The authors suggest drawing from fundamental NLP research.- Studying the quality-quantity tradeoff in automatic explanation collection methods (Section 5.1). The authors believe this is an important direction for future work.- Exploring other potential necessary properties of human explanations besides those discussed in the paper (end of Appendix A). The authors focus on properties most relevant to current ExNLP research but acknowledge others may exist.- Continuing to critically evaluate explanation collection methodologies to promote standardization and reveal implications for downstream uses (throughout). The authors call for explanation datasets and their constraints to be thoroughly documented.In summary, the main suggested directions are: better defining the emerging second class of ExNLP datasets, collecting new annotation types like contrastive explanations, developing methods to quantify explanation diversity, studying the quality-quantity tradeoff, identifying other key properties of human explanations, and continued critical evaluation of explanation collection practices.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper reviews datasets for explainable natural language processing (ExNLP) that contain human-annotated textual explanations. It summarizes 65 existing ExNLP datasets into three main types: highlights, free-text explanations, and structured explanations. The paper analyzes the strengths and shortcomings of different data collection methodologies used for these datasets. Two key points are highlighted - first, that common methods for collecting highlight explanations can result in datasets that are insufficient or non-comprehensive, causing issues for model training and evaluation. Second, that while some researchers discard template-like free-text explanations as low quality, embracing structure in explanations can be useful when it naturally arises from a task. The paper also provides recommendations for creating higher quality ExNLP datasets in the future, such as using a "collect-and-edit" approach and increasing diversity of explanations. Overall, the paper aims to promote discussion and standardization around ExNLP dataset development.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper provides a review of existing datasets for explainable natural language processing (ExNLP) research. The authors identify and organize 65 existing datasets into three main types: those with highlight, free-text, or structured explanations. They analyze the strengths and weaknesses of different data collection methodologies that have been used. The key points made are:Paragraph 1) For highlight explanations, the authors find that existing human annotations are not necessarily sufficient nor comprehensive for evaluating model-generated highlights. They recommend avoiding highlights with low sufficiency, collecting comprehensive highlights to evaluate faithfulness, and carefully reporting collection details like instructions given to annotators. They warn researchers to avoid making assumptions about properties like comprehensiveness based on general dataset descriptions. Paragraph 2) For free-text explanations, the authors use a case study of two datasets to show that not all template-like explanations are undesirable, and structured explanations should be embraced when appropriate for a task. The authors give recommendations to improve free-text explanation quality, like teaching the task, addressing ambiguity, using a collect-and-edit approach, recruiting diverse annotators, and collecting contrastive or negative explanations. They synthesize recommendations from related work to further increase diversity and reduce artifacts.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper presents a review of datasets for explainable natural language processing (ExNLP). The authors first define key terminology related to ExNLP, such as highlights, free-text explanations, and structured explanations. They then provide an overview of 65 existing ExNLP datasets, categorizing them based on the type of textual explanation they contain - highlights, free-text, or structured. The datasets are summarized in tables detailing the task, type of explanation, collection method, number of instances, etc. The authors then analyze what can be learned from the various dataset collection methodologies. They focus on two important points - the traditional process of collecting highlight explanations and its discrepancies with evaluating highlight models, and the emergence of structured explanations. They argue that assumptions made during data collection influence downstream uses of the datasets. The authors also discuss strategies to increase explanation quality and diversity. Overall, the main method is a comprehensive literature review categorized into highlight, free-text, and structured explanation datasets, along with a critical analysis of collection methodologies and recommendations for future work.
