# [When Side-Channel Attacks Break the Black-Box Property of Embedded   Artificial Intelligence](https://arxiv.org/abs/2311.14005)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new attack methodology that combines hardware side-channel attacks and software adversarial attacks to defeat the security of embedded deep learning systems. Specifically, the authors exploit physical side-channel leakages like electromagnetic emissions to extract sensitive information (logits) from an embedded neural network during inference. This allows them to remove the assumption made by gradient-free adversarial attack frameworks like ZOO that require access to logits, making these attacks more practical in black-box scenarios. They demonstrate their attack by targeting the softmax layer implementation of the NNOM deep learning framework running on a microcontroller. Using techniques like template attacks and deep learning-based side-channel attacks in a profiled attack setting, they show the ability to extract all logits within 5 traces. They then use the ZOO attack with the extracted logits to successfully generate adversarial examples and fool the neural network, confirming that hardware attacks can enhance software attacks against embedded AI. The attack has implications for the security of critical embedded systems using deep learning.


## Summarize the paper in one sentence.

 This paper demonstrates a method to generate powerful adversarial examples against embedded neural networks in a black box setting by exploiting side-channel attacks to extract the logits.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new attack methodology that combines hardware side-channel attacks and software adversarial attacks to defeat the security of embedded deep neural networks. Specifically:

1) The paper introduces a method to extract the logits (confidence scores) from an embedded DNN using side-channel attacks like template attacks or deep learning-based side-channel attacks. This allows breaking the black box assumption made by many adversarial attack frameworks that require access to logits/probabilities.

2) The extracted logits are then used as input to state-of-the-art gradient-free adversarial attack frameworks like ZOO to generate powerful adversarial examples in a real blackbox setting. 

3) The complete end-to-end attack methodology combining physical side-channel attacks and adversarial example generation is experimentally validated on a microcontroller running a quantized DNN using the NNOM framework.

In summary, the key contribution is demonstrating the synergy between hardware side-channel attacks and software adversarial attacks to defeat security of embedded AI systems in a practical black box threat model. This opens up new attack vectors that system designers need to consider.


## What are the keywords or key terms associated with this paper?

 Based on my analysis of the paper's content, some of the main keywords or key terms associated with this paper are:

- Deep learning
- Embedded systems 
- Black box attack
- Side-channel attack
- Adversarial examples
- Logits extraction
- Hardware attacks
- Software attacks
- Quantization
- Template attacks
- Deep learning-based side-channel attacks (DLSCA)

The paper proposes a new attack methodology combining hardware side-channel attacks and software adversarial attacks to defeat the security of embedded deep neural networks, specifically targeting the extraction of logits in a black box scenario to enable more powerful adversarial example generation. Key concepts explored include quantization, template attacks, DLSCA, logits extraction, and demonstrating the complementarity of hardware and software attacks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the main motivation behind combining hardware side-channel attacks with software adversarial attacks? Why is this a useful approach?

2. How does the attack extract the logits from the embedded neural network? What specific vulnerabilities in the softmax layer implementation are exploited? 

3. What assumptions does the threat model make about the attacker's capabilities? Are these realistic assumptions that could apply to real-world scenarios?

4. What modifications did the authors make to the NNOM framework's softmax function to ensure a uniform distribution of logits? Why was this an important step?

5. How do template attacks, multinomial logistic regression, and deep learning-based side channel attacks compare in terms of performance for extracting the logits? What are the tradeoffs?  

6. Approximately how many attack traces are needed overall to conduct an end-to-end attack and generate an adversarial example? How could this complexity be reduced?

7. How does directly using the targeted network for adversarial example generation via the ZOO attack framework compare to using a substitute network? What are the advantages and disadvantages?

8. Besides evaluating the ZOO attack framework, what other adversarial example generation methods could the proposed approach be combined with? Why wasn't this explored in more depth?

9. What impact can incorrectly estimating some of the logit values have on the attack? Could the framework be adapted to deal with this? 

10. What kinds of countermeasures could help defend against this type of attack combining side-channel and adversarial examples? What makes this a challenging attack to defend against?
