# [Sample Efficient Reinforcement Learning from Human Feedback via Active   Exploration](https://arxiv.org/abs/2312.00267)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new method for efficiently collecting human preference feedback to train reinforcement learning agents, with a focus on aligning large language models. The key idea is to actively select the most informative context-action pairs to obtain human preferences on, rather than random selection. This is formalized as an "active contextual dueling bandit" problem. The paper provides a novel algorithm for this setting with theoretical guarantees on worst-case regret. Experiments on synthetic data validate the theory. The method is then adapted and empirically evaluated for training language models on existing preference datasets as well as a new Jeopardy dataset. Results demonstrate superior sample efficiency over uniform selection baselines. The design of acquisition functions for actively selecting examples and estimating uncertainty in language models are noted as interesting open problems motivating future work. Overall, the principle of active data selection is shown to enable more efficient use of limited human feedback for preference-based learning.
