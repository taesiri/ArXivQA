# [Is context all you need? Scaling Neural Sign Language Translation to   Large Domains of Discourse](https://arxiv.org/abs/2308.09622)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction and method sections, the central hypothesis of this paper seems to be that incorporating contextual information from preceding sign language sequences and sparse automatic sign spottings can improve the performance of sign language translation models. Specifically, the authors hypothesize that:1) Using context from previous sign language sequences can help disambiguate ambiguous or weak visual cues in the current sign video being translated.2) Incorporating automatically spotted signs from the current video being translated can provide additional cues to improve translation. 3) Combining these contextual cues (from preceding sequences and current sparse spottings) with visual features from the sign video in a multi-modal neural translation architecture will achieve better performance compared to methods that rely only on the visual input.The paper proposes a novel multi-modal transformer network architecture that incorporates contextual encoders for previous sequences and current spottings, in addition to a video encoder. This allows the model to leverage complementary information sources to generate better spoken language translations. The central hypothesis is that this context-aware, multi-modal approach will outperform previous translation methods, especially on large and diverse sign language datasets where context is more important.
