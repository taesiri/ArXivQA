# [Is context all you need? Scaling Neural Sign Language Translation to   Large Domains of Discourse](https://arxiv.org/abs/2308.09622)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction and method sections, the central hypothesis of this paper seems to be that incorporating contextual information from preceding sign language sequences and sparse automatic sign spottings can improve the performance of sign language translation models. Specifically, the authors hypothesize that:1) Using context from previous sign language sequences can help disambiguate ambiguous or weak visual cues in the current sign video being translated.2) Incorporating automatically spotted signs from the current video being translated can provide additional cues to improve translation. 3) Combining these contextual cues (from preceding sequences and current sparse spottings) with visual features from the sign video in a multi-modal neural translation architecture will achieve better performance compared to methods that rely only on the visual input.The paper proposes a novel multi-modal transformer network architecture that incorporates contextual encoders for previous sequences and current spottings, in addition to a video encoder. This allows the model to leverage complementary information sources to generate better spoken language translations. The central hypothesis is that this context-aware, multi-modal approach will outperform previous translation methods, especially on large and diverse sign language datasets where context is more important.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel multi-modal transformer architecture for context-aware sign language translation. The key ideas are:- Using complementary transformer encoders to model the current sign phrase (video encoder, spotting encoder) as well as context from preceding sign sequences (context encoder). - Combining these encoders in a unified transformer decoder to generate spoken language translations.- Achieving state-of-the-art results on two datasets by incorporating context and automatic spottings, nearly doubling BLEU scores compared to baseline approaches.Specifically, the paper makes the following key contributions:- Proposes a new multi-modal transformer network that incorporates context from prior sentences and automatic sign spottings to disambiguate weak visual cues.- Conducts experiments to examine different approaches to capturing context, showing benefits of using prior sentences or spottings.- Achieves SOTA translation performance on two datasets - BOBSL, the largest public sign language translation dataset, and the SRF partition of the WMT-SLT 2022 challenge.- Shows significant gains by incorporating contextual information, improving BLEU-4 from 1.27 to 2.88 on BOBSL.In summary, the main novelty is in exploiting context for sign language translation via a multi-modal transformer approach, demonstrating substantial improvements over non-contextual baselines.
