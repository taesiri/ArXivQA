# Think you have Solved Question Answering? Try ARC, the AI2 Reasoning   Challenge

## What is the central research question or hypothesis that this paper addresses?

The paper does not appear to state an explicit research question or hypothesis. However, the overall focus is on introducing and analyzing a new question answering dataset called the AI2 Reasoning Challenge (ARC). Some key points about ARC:- It consists of 7787 natural science questions taken from standardized tests, partitioned into an "Easy" set and a "Challenge" set. - The questions are designed to require more complex reasoning and knowledge than previous QA datasets like SQuAD. - The Challenge set contains only questions that are answered incorrectly by both a retrieval algorithm and a word co-occurrence algorithm, making it difficult for baseline methods.- Along with the dataset, they provide a new corpus of science sentences relevant to ARC, and baseline results showing state-of-the-art QA models struggle on the Challenge set.So in summary, the paper is presenting and analyzing this new dataset as a way to encourage research on more advanced reasoning for question answering. The central hypothesis is that ARC represents the kind of complex reasoning required to make further progress in QA.


## What is the main contribution of this paper?

Based on the abstract provided, this paper introduces a new question answering dataset called the AI2 Reasoning Challenge (ARC). The key contributions seem to be:- ARC is a new question answering dataset consisting of 7787 natural science questions from standardized tests. It is divided into an "Easy" set and a "Challenge" set.- The Challenge set contains questions that are difficult for both retrieval-based algorithms and word co-occurrence algorithms. The aim is to encourage research on more advanced QA methods. - Along with the dataset, the paper also provides the ARC Corpus (1.4GB of relevant science sentences) and implementations of baseline neural QA models. - Experiments show that while existing models perform decently on the Easy set, none significantly outperform random guessing on the Challenge set. This highlights the difficulty of the Challenge set.- Overall, the paper poses ARC as a new challenge to push research towards questions requiring more complex reasoning and knowledge. The combination of the dataset, corpus, baselines, and leaderboard aims to provide a benchmark to measure progress.In summary, the main contribution seems to be the introduction and analysis of the ARC dataset as a new challenge for question answering research, focused on questions requiring deeper reasoning. The paper aims to motivate the community to develop more advanced QA methods that can perform better on the difficult Challenge set.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a new AI reasoning challenge called ARC, consisting of a dataset of 7787 science exam questions partitioned into an easy set and a more challenging set, plus a corpus of science sentences, in order to encourage research on more complex question answering requiring reasoning rather than just information retrieval or statistical correlation.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research in question answering:- The paper presents a new question answering dataset called ARC (AI2 Reasoning Challenge) that focuses on grade-school science questions. Many other QA datasets like SQuAD, TriviaQA, etc. have focused more on factoid questions from Wikipedia, news, etc. So ARC provides questions that require more reasoning, especially scientific reasoning.- The paper argues previous QA datasets have focused too much on "easy" questions that can be solved by surface-level cues like word matching. So they specifically construct a Challenge Set of questions in ARC that are not solvable by simple information retrieval or word co-occurrence methods. This pushes the field towards more complex reasoning.- The authors test several strong baseline systems on ARC like neural entailment models, reading comprehension models, etc. But none significantly beat random guessing on the Challenge Set. So ARC poses a new challenge to the field that current state-of-the-art models struggle on.- The paper releases not just the dataset but also a new science corpus and reference solvers to make it easy for others to work on ARC. Many other datasets just release questions. Providing the corpus and baselines makes it more accessible.- Overall, ARC draws attention to more complex reasoning in QA, provides a specific challenge problem to drive research, and offers resources beyond just a question dataset. This is a notable contribution compared to most prior QA dataset papers. The authors make a compelling case this can push QA research in new directions.In summary, the novelty of focusing on complex reasoning over factoid questions, constructing a Challenge Set, releasing supporting resources, and showing state-of-the-art models struggle on ARC distinguishes this paper from much of the prior work in QA dataset research. It makes a case for driving progress on deeper language understanding.
