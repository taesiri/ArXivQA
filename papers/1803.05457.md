# Think you have Solved Question Answering? Try ARC, the AI2 Reasoning   Challenge

## What is the central research question or hypothesis that this paper addresses?

The paper does not appear to state an explicit research question or hypothesis. However, the overall focus is on introducing and analyzing a new question answering dataset called the AI2 Reasoning Challenge (ARC). Some key points about ARC:- It consists of 7787 natural science questions taken from standardized tests, partitioned into an "Easy" set and a "Challenge" set. - The questions are designed to require more complex reasoning and knowledge than previous QA datasets like SQuAD. - The Challenge set contains only questions that are answered incorrectly by both a retrieval algorithm and a word co-occurrence algorithm, making it difficult for baseline methods.- Along with the dataset, they provide a new corpus of science sentences relevant to ARC, and baseline results showing state-of-the-art QA models struggle on the Challenge set.So in summary, the paper is presenting and analyzing this new dataset as a way to encourage research on more advanced reasoning for question answering. The central hypothesis is that ARC represents the kind of complex reasoning required to make further progress in QA.


## What is the main contribution of this paper?

Based on the abstract provided, this paper introduces a new question answering dataset called the AI2 Reasoning Challenge (ARC). The key contributions seem to be:- ARC is a new question answering dataset consisting of 7787 natural science questions from standardized tests. It is divided into an "Easy" set and a "Challenge" set.- The Challenge set contains questions that are difficult for both retrieval-based algorithms and word co-occurrence algorithms. The aim is to encourage research on more advanced QA methods. - Along with the dataset, the paper also provides the ARC Corpus (1.4GB of relevant science sentences) and implementations of baseline neural QA models. - Experiments show that while existing models perform decently on the Easy set, none significantly outperform random guessing on the Challenge set. This highlights the difficulty of the Challenge set.- Overall, the paper poses ARC as a new challenge to push research towards questions requiring more complex reasoning and knowledge. The combination of the dataset, corpus, baselines, and leaderboard aims to provide a benchmark to measure progress.In summary, the main contribution seems to be the introduction and analysis of the ARC dataset as a new challenge for question answering research, focused on questions requiring deeper reasoning. The paper aims to motivate the community to develop more advanced QA methods that can perform better on the difficult Challenge set.
