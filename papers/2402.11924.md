# [MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://arxiv.org/abs/2402.11924)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing multi-hop QA (MHQA) benchmarks have limitations in evaluating large language models' (LLMs) reasoning ability, including potential data contamination and not evaluating the reasoning chain behind the final answer.  

- Data contamination means the test data may have already been exposed to LLMs during pre-training, making it hard to objectively evaluate their capabilities. 

- Not evaluating the reasoning chain means it's unclear whether models are actually conducting multi-step reasoning or just getting the final answer correctly by chance.

Solution - New Benchmark:
- The authors propose MRKE, the first MHQA benchmark based on new, edited knowledge never seen before by LLMs, eliminating the risk of data contamination.

- MRKE contains edited variants of passages and questions from HotpotQA, with replaced named entities, noun phrases, synonyms etc. Translated to Chinese and back to create new unprecedented passages.

- MRKE evaluates intermediate reasoning chains by annotating sub-questions and answers corresponding to each reasoning hop.

Main Contributions:
- Show LLMs have a clear performance gap between original and edited HotpotQA data, indicating potential data contamination in existing benchmarks.

- Reveal LLMs get only 36.3% (GPT-4) of reasoning chains fully correct, indicating they often bypass proper reasoning and have inflated scores. 

- Propose joint Sub-QA + MHQA metrics to evaluate entire reasoning chains rather than just the final answer.

- Overall, provide more rigorous benchmark to evaluate LLMs' actual multi-hop reasoning abilities. Highlight need for developing models with stronger reasoning.


## Summarize the paper in one sentence.

 This paper proposes MRKE, a new multi-hop QA evaluation benchmark and method that eliminates the risk of data contamination in existing datasets and evaluates LLMs' reasoning ability by annotating and assessing performance on sub-questions and reasoning chains.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing MRKE, a new multi-hop QA evaluation benchmark based on knowledge editing of the HotpotQA dataset. This eliminates the risk of data contamination and allows more objective assessment of LLMs' reasoning abilities. 

2. Annotating the reasoning chain (sub-questions and intermediate answers) corresponding to the multi-hop questions in MRKE. This allows evaluation of whether LLMs are following the correct reasoning chain.

3. Proposing new evaluation methods that consider the joint performance of answering sub-questions and final multi-hop questions, as well as reasoning chain evaluation.

4. Experiments showing existing LLMs have a significant performance gap between original HotpotQA and the edited MRKE dataset, indicating potential data contamination issues with current benchmarks. The experiments also reveal limitations in LLMs' reasoning chains.

5. Analysis showing that incorporating sub-questions into the prompt helps improve LLM performance, highlighting the importance of the annotated reasoning chains.

In summary, the main contributions are introducing a new, cleaned multi-hop QA evaluation benchmark to more objectively evaluate LLMs, along with new annotation and evaluation methods focused on reasoning chains.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Large language models (LLMs) - The paper evaluates different LLMs like GPT-3.5, GPT-4, etc. on their multi-hop reasoning abilities.

- Multi-hop question answering (MHQA) - The paper introduces a new MHQA evaluation benchmark to assess LLMs' reasoning skills through multi-step questions. 

- Reasoning chains - The paper analyzes LLMs' capabilities in following the right reasoning chains of evidence across multiple questions to reach the final answer.

- Knowledge editing - The paper creates the benchmark by editing an existing MHQA dataset to replace entities and create new, unprecedented knowledge. 

- Sub-questions - The benchmark includes annotated sub-questions and intermediate answers corresponding to the multi-hop questions.

- Performance gap - The paper examines the gap between LLMs' performance on original and edited datasets to evaluate potential data contamination issues.

- Joint evaluation - A new evaluation method is proposed to jointly measure sub-QA and final MHQA performance.

- Data contamination - The paper argues that existing MHQA benchmarks face challenges of test data contamination in LLMs' training data.

In summary, the key focus is on rigorously evaluating LLMs' multi-hop reasoning abilities using knowledge editing and analysis of reasoning chains.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new benchmark called MRKE for evaluating large language models' (LLMs) reasoning abilities in multi-hop question answering. How is MRKE different from existing benchmarks like HotpotQA in terms of mitigating the risk of data contamination?

2. The paper reports lower performance of LLMs like GPT-3 and GPT-4 on MRKE compared to the original HotpotQA dataset. What does this performance gap indicate about the potential inflation of LLM abilities on HotpotQA and other existing QA datasets? 

3. The paper proposes a novel evaluation method called "joint performance" that combines sub-question answering and final multi-hop question answering. How does this method provide a more complete picture of LLM reasoning compared to just using overall accuracy?

4. The paper finds that LLMs still get a majority of final answers correct despite having the wrong reasoning chain. Why is identifying the correct reasoning chain also important for faithfully evaluating reasoning ability?

5. The knowledge editing framework uses both automatic LLM annotation and human feedback. What are the rationales behind this hybrid approach? What role does each play?

6. The edited passages in MRKE contain never-before-seen entities and facts. How does this approach better prevent memorization shortcuts compared to using naturally occurring text? 

7. The paper concludes combining sub-questions into the prompt improves performance over just asking the complex question alone. What does this indicate about the importance of decomposition for LLM reasoning?

8. What potential limitations exist in the knowledge edition methodology for generating novel facts and questions? How might these affect difficulty or answerability? 

9. The paper focuses evaluation on a subset of HotpotQA questions. What considerations would be necessary to scale up knowledge edition to much larger and more diverse datasets?

10. What directions for future work does this evaluation benchmark enable in improving and analyzing multi-hop reasoning in LLMs? What open questions remain about the exact mechanisms involved?
