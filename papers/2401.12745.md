# [On the Utility of Probing Trajectories for Algorithm-Selection](https://arxiv.org/abs/2401.12745)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Algorithm selection methods typically use instance features or landscape features to train ML models to predict best algorithm performance. However, these features only describe the instance, not the algorithm's perspective. 
- Instance similarity in feature space doesn't necessarily correlate to algorithm performance similarity.
- Computing landscape features (e.g. ELA) has high overhead cost.

Proposed Solution:  
- Use short "probing trajectories" from running an algorithm for a few iterations as input to algorithm selectors. Trajectories provide an "algorithm perspective". 
- Train time series classifiers on raw trajectories or extract features. Compare to ELA landscape features.
- Benefits: Removes need to compute extra features, provides algorithm view, trajectories can be reused to warmstart selected algorithm.

Key Contributions:
- Demonstrate trajectory-based selection can outperform ELA features using much lower budgets (e.g. 6x fewer evaluations).
- Raw PSO trajectory with 80 evaluations matches ELA classifier with 500 evaluations.
- 8/9 single trajectory classifiers and all concatenated trajectory classifiers beat ELA classifiers with more generations.
- Projections show similarity between instances from algorithm perspective doesn't fully match human perspective of function properties. 
- Approach is promising way to build systems that learn across instances by taking algorithm view.

The summary covers the key elements of the problem, proposed trajectory-based solution, results showing performance improvements over landscape features, and benefits of providing an algorithm perspective. It highlights the potential of this approach to enable better learning across problem instances in building more capable optimization systems.


## Summarize the paper in one sentence.

 This paper proposes using short probing trajectories from running an optimizer for a few iterations as input to train algorithm selectors, showing this trajectory-based approach can outperform methods using landscape features while requiring less budget.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel "algorithm-centric" method for describing instances to train algorithm selection models. Specifically, the paper proposes using short probing trajectories obtained by running an algorithm on an instance for a few iterations. These trajectories capture the algorithm's perspective on the instance and can be used as input to train algorithm selectors. 

The key benefits highlighted in the paper are:

- The trajectories remove the need to manually define or compute landscape features to create training data.

- They provide an "algorithm perspective" on instances rather than just describing instances in isolation. This may help models learn similarities between instances from the algorithm's point of view.

- The probing trajectories used to get a prediction can be reused to warmstart the selected algorithm, saving computational budget.

The approach is demonstrated to achieve good performance, comparable or better than landscape-based features, while using a fraction of the budget needed to compute those features. Hence it is promising as a low-budget alternative.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, the main keywords and key terms associated with this paper include:

- Algorithm Selection
- Black-Box Optimisation  
- Algorithm Trajectory
- Probing Trajectories
- Exploratory Landscape Analysis (ELA)
- Time-series features
- Machine learning
- Classification

The paper proposes using short "probing trajectories" obtained by running an optimization algorithm for a few iterations as input to train models for algorithm selection. This is contrasted with more typical approaches that use features derived from the problem instance itself (instance features) or from sampling the landscape (ELA features). The probing trajectories aim to provide an "algorithm perspective" on instance similarity. Experiments using trajectories, time-series features derived from them, and ELA features are conducted on the BBOB benchmark, showing promising results for the trajectory-based approach even with very low budgets. Keywords like algorithm selection, black-box optimization, algorithm trajectory, and probing trajectories reflect this main contribution. Terms like ELA features, time-series features, machine learning, and classification relate to the methods used in the comparative experiments.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper argues that previous algorithm selection approaches take an "instance perspective". What exactly does this mean and why is it potentially problematic? 

2. The probing trajectories are defined by either the best or current performance of a metaheuristic solver over a short number of function evaluations. What are the potential advantages and disadvantages of using best versus current performance?

3. The paper evaluates trajectories of length 2 and 7 generations. What is the rationale behind choosing these specific trajectory lengths? How might the performance change if even shorter or longer trajectories were used?

4. Time series features are extracted from the raw probing trajectories. What is the motivation behind extracting features versus using the raw trajectories directly? What types of time series features are extracted?

5. The method is demonstrated on the BBOB testbed. What are some key properties of this testbed? How might the results change on other optimization problem testbeds?

6. The results show strong performance of Particle Swarm Optimization (PSO) trajectories. Why might PSO trajectories be more informative than those from other algorithms? How could this inform the design of trajectory-based algorithm selectors?

7. The paper argues taking an "algorithm perspective" on instance similarity is useful. What evidence supports this? Does the UMAP visualization provide further insights into algorithm versus human perspectives on instance similarity?

8. Concatenated trajectories from multiple algorithms are used as input to the classifier. How does concatenation combine information across algorithms? What are open questions around scaling this approach to larger algorithm portfolios?

9. A key advantage of the method is the trajectories can be reused to warmstart the selected algorithm. What mechanisms enable trajectory reuse? How impactful is this in reducing the overall optimization budget?

10. The paper focuses on continuous optimization problems. What considerations would be important in extending the method to combinatorial domains like TSP? What new challenges might arise?
