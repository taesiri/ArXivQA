# [Unsupervised Continual Semantic Adaptation through Neural Rendering](https://arxiv.org/abs/2211.13969)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to continually adapt a semantic segmentation model to new scenes in an unsupervised fashion, while preventing catastrophic forgetting on previous scenes. 

Specifically, the authors propose using neural radiance fields (NeRFs) to fuse the predictions of a segmentation model from multiple viewpoints into a 3D representation for each scene. The rendered semantic labels from the NeRF act as pseudo-labels to adapt the segmentation model on the new scene. To prevent catastrophic forgetting, the compact NeRF models are stored in a long-term memory and used to render images and pseudo-labels from previous scenes, which are mixed with data from the current scene during training.

The key hypothesis is that the view-consistent pseudo-labels rendered from the NeRF models can be used to effectively adapt the segmentation model to new scenes without ground truth supervision, while the compact scene representations stored in long-term memory can help mitigate catastrophic forgetting in a continual learning setup across multiple scenes.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a method to continually adapt a semantic segmentation model to new scenes in an unsupervised fashion using neural rendering. Specifically, the paper trains a Semantic Neural Radiance Field (Semantic-NeRF) model on each new scene to generate view-consistent pseudo-labels, which are then used to adapt the segmentation model. 

2. Demonstrating that jointly training the Semantic-NeRF model and the segmentation model through iterative mutual supervision improves performance on the pseudo-labels and the segmentation predictions. This shows the benefit of enforcing 2D-3D knowledge transfer between the frame-level predictions and the scene-level 3D representation.

3. Introducing a NeRF-based experience replay strategy to mitigate catastrophic forgetting when adapting the model continually across multiple scenes. The compactness of NeRF allows storing the models for previous scenes in a long-term memory to generate data for replay from novel viewpoints at constant memory cost.

4. Evaluating the method on the ScanNet dataset and showing improved adaptation performance compared to baselines, including a recent voxel-based method and a state-of-the-art unsupervised domain adaptation approach. The evaluations also demonstrate the effectiveness of the proposed NeRF-based replay strategy at retaining knowledge.

In summary, the key novelty is in exploiting neural rendering to enable unsupervised adaptation and experience replay for continual learning of semantic segmentation across multiple scenes, and demonstrating through experiments the advantages compared to existing approaches. The introduced 2D-3D knowledge transfer and ability to generate data from novel views are also notable contributions.
