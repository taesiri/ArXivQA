# [Unsupervised Continual Semantic Adaptation through Neural Rendering](https://arxiv.org/abs/2211.13969)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to continually adapt a semantic segmentation model to new scenes in an unsupervised fashion, while preventing catastrophic forgetting on previous scenes. 

Specifically, the authors propose using neural radiance fields (NeRFs) to fuse the predictions of a segmentation model from multiple viewpoints into a 3D representation for each scene. The rendered semantic labels from the NeRF act as pseudo-labels to adapt the segmentation model on the new scene. To prevent catastrophic forgetting, the compact NeRF models are stored in a long-term memory and used to render images and pseudo-labels from previous scenes, which are mixed with data from the current scene during training.

The key hypothesis is that the view-consistent pseudo-labels rendered from the NeRF models can be used to effectively adapt the segmentation model to new scenes without ground truth supervision, while the compact scene representations stored in long-term memory can help mitigate catastrophic forgetting in a continual learning setup across multiple scenes.
