# [Unsupervised Continual Semantic Adaptation through Neural Rendering](https://arxiv.org/abs/2211.13969)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to continually adapt a semantic segmentation model to new scenes in an unsupervised fashion, while preventing catastrophic forgetting on previous scenes. 

Specifically, the authors propose using neural radiance fields (NeRFs) to fuse the predictions of a segmentation model from multiple viewpoints into a 3D representation for each scene. The rendered semantic labels from the NeRF act as pseudo-labels to adapt the segmentation model on the new scene. To prevent catastrophic forgetting, the compact NeRF models are stored in a long-term memory and used to render images and pseudo-labels from previous scenes, which are mixed with data from the current scene during training.

The key hypothesis is that the view-consistent pseudo-labels rendered from the NeRF models can be used to effectively adapt the segmentation model to new scenes without ground truth supervision, while the compact scene representations stored in long-term memory can help mitigate catastrophic forgetting in a continual learning setup across multiple scenes.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a method to continually adapt a semantic segmentation model to new scenes in an unsupervised fashion using neural rendering. Specifically, the paper trains a Semantic Neural Radiance Field (Semantic-NeRF) model on each new scene to generate view-consistent pseudo-labels, which are then used to adapt the segmentation model. 

2. Demonstrating that jointly training the Semantic-NeRF model and the segmentation model through iterative mutual supervision improves performance on the pseudo-labels and the segmentation predictions. This shows the benefit of enforcing 2D-3D knowledge transfer between the frame-level predictions and the scene-level 3D representation.

3. Introducing a NeRF-based experience replay strategy to mitigate catastrophic forgetting when adapting the model continually across multiple scenes. The compactness of NeRF allows storing the models for previous scenes in a long-term memory to generate data for replay from novel viewpoints at constant memory cost.

4. Evaluating the method on the ScanNet dataset and showing improved adaptation performance compared to baselines, including a recent voxel-based method and a state-of-the-art unsupervised domain adaptation approach. The evaluations also demonstrate the effectiveness of the proposed NeRF-based replay strategy at retaining knowledge.

In summary, the key novelty is in exploiting neural rendering to enable unsupervised adaptation and experience replay for continual learning of semantic segmentation across multiple scenes, and demonstrating through experiments the advantages compared to existing approaches. The introduced 2D-3D knowledge transfer and ability to generate data from novel views are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes using neural radiance fields to fuse semantic segmentation predictions from multiple viewpoints into a consistent 3D representation, which is then used to render pseudo-labels for adapting the segmentation network to new scenes in a continual learning fashion, while compactly storing past scenes in memory to prevent catastrophic forgetting.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other related work in semantic segmentation adaptation:

- The problem setting of continual semantic adaptation is novel compared to much prior work. As discussed, UDA methods typically focus on single-stage adaptation between source and target domains, while continual learning tackles class-incremental scenarios. This paper introduces a new multi-stage adaptation setting that is relevant for real-world deployment.

- The idea of using 3D fusion to generate pseudo-labels for unsupervised adaptation has similarities to some recent UDA methods, like CBST. However, using neural rendering with NeRFs for this purpose is new. NeRF allows enforcing multi-view consistency in a differentiable way during training.

- Leveraging NeRF models for replay and generating novel views for adaptation is a very interesting idea not explored before. It provides a way to continually adapt at constant memory cost while retaining past knowledge.

- Jointly training the 2D segmentation network and 3D NeRF scene representation enables implicit knowledge transfer between them. Showing improved adaptation performance from this 2D-3D co-training is a nice result.

- Comparisons to related UDA and continual learning methods like CoTTA and the voxel-based approach demonstrate the advantages of the proposed NeRF-based approach. Both adaptation performance and memory efficiency are improved.

Overall, I would say the main novelties are in the problem formulation, using NeRF for pseudo-label generation/replay, and showing benefits of joint 2D-3D training. The ideas seem promising for semantic segmentation adaptation in real-world systems. Limitations like handling dynamic scenes are also discussed.
