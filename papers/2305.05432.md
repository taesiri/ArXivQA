# [WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset](https://arxiv.org/abs/2305.05432)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is introducing and analyzing a new multimodal dataset called WikiWeb2M for studying multimodal webpage understanding. The key research questions/goals around this dataset appear to be:- Can retaining the full set of images, text, and structure from webpages in a unified format enable new research on multimodal webpage understanding? - What kinds of multimodal generation tasks such as page description, section summarization, and contextual image captioning can be facilitated by the proposed WikiWeb2M dataset?- How does having the complete webpage context compare to more limited context (e.g. just the section an image comes from) for tasks like contextual image captioning?- What are the statistics and characteristics of the WikiWeb2M dataset in terms of number of pages, sections, images, etc. and how does it compare to prior multimodal Wikipedia datasets like WIT?- What level of performance can standard multimodal models like T5 and ViT achieve on tasks enabled by WikiWeb2M and how might this benchmark help drive further research?In summary, the central research focus is presenting WikiWeb2M as a way to better study multimodal webpage understanding through unified text, image, and structure data as well as analyzing the dataset itself and benchmarking performance on enabled tasks.
