# [WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset](https://arxiv.org/abs/2305.05432)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is introducing and analyzing a new multimodal dataset called WikiWeb2M for studying multimodal webpage understanding. The key research questions/goals around this dataset appear to be:- Can retaining the full set of images, text, and structure from webpages in a unified format enable new research on multimodal webpage understanding? - What kinds of multimodal generation tasks such as page description, section summarization, and contextual image captioning can be facilitated by the proposed WikiWeb2M dataset?- How does having the complete webpage context compare to more limited context (e.g. just the section an image comes from) for tasks like contextual image captioning?- What are the statistics and characteristics of the WikiWeb2M dataset in terms of number of pages, sections, images, etc. and how does it compare to prior multimodal Wikipedia datasets like WIT?- What level of performance can standard multimodal models like T5 and ViT achieve on tasks enabled by WikiWeb2M and how might this benchmark help drive further research?In summary, the central research focus is presenting WikiWeb2M as a way to better study multimodal webpage understanding through unified text, image, and structure data as well as analyzing the dataset itself and benchmarking performance on enabled tasks.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of the Wikipedia Webpage 2M (WikiWeb2M) dataset. Specifically:- WikiWeb2M is the first dataset to retain the full set of images, text, and structure data available in a Wikipedia page. - It contains over 2 million English Wikipedia pages with all associated text sections, images, captions, and metadata. - This unified multimodal data enables new tasks like page description generation, section summarization, and contextual image captioning that require understanding content at different granularities.- Experiments show performance improvements on downstream tasks when using the full webpage context compared to only partial context (e.g. just the target image's section).- WikiWeb2M facilitates future research on multimodal webpage understanding through its release as an open source dataset.In summary, the key contribution is the release and benchmarking of the first large-scale multimodal Wikipedia webpage dataset that retains the complete webpage content, enabling new research directions in multimodal understanding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces WikiWeb2M, a new multimodal Wikipedia dataset with over 2 million pages that retains all images, text, and structure to enable tasks like page description generation, section summarization, and contextual image captioning.
