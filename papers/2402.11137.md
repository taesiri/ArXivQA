# [TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks](https://arxiv.org/abs/2402.11137)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Prior-data fitted networks (PFNs) are a recently proposed paradigm for machine learning that achieve state-of-the-art performance on small tabular classification datasets. However, PFNs have limitations that prohibit widespread adoption, including only accepting a fixed number of features, scaling poorly with dataset size, and only selecting from a fixed number of classes. 

Proposed Solution: 
This paper proposes TuneTables, a novel prompt-tuning strategy to overcome the limitations of PFNs. The key ideas are:

1) Use prompt tuning to compress large datasets into a smaller learned context that captures the essence of the full dataset. This allows PFNs to handle large datasets.

2) Extend the number of predicted classes beyond what the PFN was pre-trained on by fine-tuning the classifier layer. 

3) Control flow to automatically select the right PFN variant based on dataset statistics.

Contributions:

1) TuneTables scales PFNs to large datasets, making them competitive with GBDTs on datasets up to 50K samples.

2) TuneTables has lower inference time than vanilla PFNs since the full training set does not need to be in context during inference.

3) Shows how to use prompt tuning for multi-objective optimization to mitigate bias.

4) Demonstrates tuned prompts can be used as an interpretability tool to understand key features.

5) Extensive experiments analyzing prompt tuning for PFNs and comparisons to baselines.

In summary, this paper performs the first investigation into context optimization for PFNs using prompt tuning. This allows PFNs to scale to solve real-world large dataset problems not previously feasible.
