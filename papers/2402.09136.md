# [DolphCoder: Echo-Locating Code Large Language Models with Diverse and   Multi-Objective Instruction Tuning](https://arxiv.org/abs/2402.09136)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Code large language models (Code LLMs) have shown impressive performance on code-related tasks. However, existing instruction tuning approaches have two main weaknesses:
   1) They only consider a single correct answer, ignoring diversity of potential solutions.  
   2) Models can generate plausible code but struggle to identify subtle errors.

Proposed Solution:  
- The paper introduces DolphCoder, a diverse instruction model with self-evaluation for code generation. It has two main components:
   1) Diverse Instruction Tuning (DIT): Uses different ChatGPT prompts to elicit multiple chain-of-thought answers per instruction. This increases diversity and reasoning paths.
   2) Multi-Objective Tuning (MOT): Added a code evaluation objective to traditional instruction tuning. Training model to both generate and evaluate code is beneficial.  

Key Contributions:
- DolphCoder outperforms strong baselines like CodeLLama and WizardCoder on HumanEval and MBPP benchmarks.
- Key findings:
   1) Increasing diversity of responses and reasoning paths boosts code capability of LLMs
   2) Improving ability to evaluate code correctness also enhances ability to generate correct code.

In summary, the paper introduces a new diverse instruction tuning method with self-evaluation that achieves state-of-the-art performance. The key ideas are increasing diversity of solutions and combining code generation with code evaluation objectives.
