# [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question or hypothesis addressed in this paper seems to be: How can we provide a comprehensive overview and analysis of the evaluation of large language models (LLMs) across three key dimensions - what to evaluate, where to evaluate, and how to evaluate?The authors aim to conduct a holistic survey of existing research efforts and benchmarks related to LLM evaluation. By summarizing evaluation tasks, protocols, and benchmarks, the paper seeks to offer insights into the current capabilities and limitations of LLMs. The key research goals appear to be:- Categorize and review existing LLM evaluation tasks across areas like natural language, reasoning, ethics, science, etc. (what to evaluate)- Compile and analyze popular LLM evaluation datasets and benchmarks (where to evaluate) - Summarize common LLM evaluation approaches and metrics (how to evaluate)- Identify success and failure cases of LLMs based on current evaluations - Discuss grand challenges and future opportunities for LLM evaluation researchIn summary, the central research question seems to revolve around providing a structured, comprehensive analysis of LLM evaluation research across the three dimensions in the title - what, where, and how to evaluate LLMs. The paper aims to offer valuable insights and identify research gaps to guide future work on enhanced LLM evaluation.
