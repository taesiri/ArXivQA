# [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question or hypothesis addressed in this paper seems to be: How can we provide a comprehensive overview and analysis of the evaluation of large language models (LLMs) across three key dimensions - what to evaluate, where to evaluate, and how to evaluate?The authors aim to conduct a holistic survey of existing research efforts and benchmarks related to LLM evaluation. By summarizing evaluation tasks, protocols, and benchmarks, the paper seeks to offer insights into the current capabilities and limitations of LLMs. The key research goals appear to be:- Categorize and review existing LLM evaluation tasks across areas like natural language, reasoning, ethics, science, etc. (what to evaluate)- Compile and analyze popular LLM evaluation datasets and benchmarks (where to evaluate) - Summarize common LLM evaluation approaches and metrics (how to evaluate)- Identify success and failure cases of LLMs based on current evaluations - Discuss grand challenges and future opportunities for LLM evaluation researchIn summary, the central research question seems to revolve around providing a structured, comprehensive analysis of LLM evaluation research across the three dimensions in the title - what, where, and how to evaluate LLMs. The paper aims to offer valuable insights and identify research gaps to guide future work on enhanced LLM evaluation.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper are:1. It provides the first comprehensive survey on the evaluation of large language models (LLMs) from three key aspects: what to evaluate, where to evaluate, and how to evaluate. 2. For "what to evaluate", it summarizes existing evaluation tasks for LLMs across diverse areas like natural language processing, reasoning, ethics, social sciences, natural sciences, medical applications, etc.3. For "where to evaluate", it compiles and reviews 30 popular benchmarks used for LLM evaluation, categorizing them into benchmarks for general language tasks vs specific downstream tasks.4. For "how to evaluate", it discusses the commonly used automatic and human evaluation protocols for assessing LLMs. 5. It summarizes the key findings on the strengths and limitations of current LLMs based on the evaluation studies. 6. It outlines several grand challenges and opportunities for future research on LLM evaluation, emphasizing the need to treat evaluation as an essential discipline to drive LLM progress.In summary, this paper provides a structured, comprehensive overview of the landscape of LLM evaluation research across tasks, benchmarks, and protocols. It offers valuable insights into the current status of LLMs and provides guidance for advancing LLM evaluation as an integral area to develop more capable and reliable LLMs.
