# [Large Language Models are Built-in Autoregressive Search Engines](https://arxiv.org/abs/2305.09612)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether large language models (LLMs) like GPT-3 can be prompted with human instructions to directly generate web URLs for document retrieval in an open-domain question answering (ODQA) setting, without any explicit training as autoregressive search engines. The key hypothesis is that LLMs contain built-in capabilities to act as search engines that can map natural language questions to relevant document identifiers (URLs), which can then be retrieved to find evidence to answer the questions. The paper shows that with just a few demonstrations, LLMs can generate URLs where nearly 90% of the corresponding web documents contain correct answers to open-domain questions.In summary, the main research question is: Can LLMs be prompted to directly generate document URLs for evidence retrieval in ODQA, without explicit training? And the key hypothesis is that LLMs have inherent search engine capabilities that can be tapped into for document retrieval via URL generation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called LLM-URL that uses large language models (LLMs) to directly generate URLs for document retrieval in open-domain question answering (ODQA). The key ideas are:- LLMs can generate relevant URLs when prompted with a few query-URL demonstrations, without any explicit training as a retriever. This shows LLMs have built-in capabilities as search engines.- Retrieving documents by generating URLs with LLMs significantly outperforms existing retrieval methods like BM25 and dual-encoders, as measured by recall@k on three ODQA datasets. - Breaking retrieved documents into passages and ranking/filtering them reduces the number of irrelevant passages while maintaining high recall. - The retrieved documents from LLM-URL improve downstream ODQA performance compared to baseline methods when used with a reader model.In summary, the main contribution is showing LLMs can effectively act as retrievers for ODQA by generating URLs, outperforming existing retrieval methods. This is done without any task-specific fine-tuning, just using the LLM's knowledge learned during pre-training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using large language models like GPT-3 as built-in search engines that can generate relevant webpage URLs when prompted with a question, retrieving better evidence documents and improving performance on open-domain QA tasks compared to existing methods.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in the field of document retrieval for question answering:- This paper explores using large language models (LLMs) as built-in search engines to generate URLs, without any explicit training for retrieval. This is a novel approach compared to traditional information retrieval methods like BM25 or dual encoder models like DPR, which require dedicated training. - The idea of prompting LLMs to generate document identifiers is related to recent work on autoregressive search engines like DSI and SEAL. However, this paper uses existing URLs instead of synthetic ids, avoiding re-training and indexing costs.- The LLM-URL approach significantly outperforms baseline retrieval methods like BM25 and Contriever on document and passage retrieval metrics across multiple QA datasets. It is competitive with state-of-the-art trained retrievers.- For open-domain QA, LLM-URL + reader model beats baseline retrieval + reader models on 2 of 3 datasets, with mixed results compared to DSI/SEAL. Still impressive given no retrieval training.- The paper demonstrates Few-Shot learning by prompting with example Q-URL pairs, further improving performance without re-training. Showcases in-context learning abilities of LLMs.- Limitations include lack of knowledge update mechanisms compared to retrieve-and-read systems. Still relies on large LMs, which can be slow and inaccurate.Overall, this paper presents a novel way of leveraging LLMs' knowledge to perform strong document retrieval for QA without training a dedicated retriever. The results are quite competitive, demonstrating the search engine abilities hidden within LLMs' parameters. It's an interesting concept that contrasts with mainstream approaches in information retrieval.
