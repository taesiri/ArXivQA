# [Large Language Models are Built-in Autoregressive Search Engines](https://arxiv.org/abs/2305.09612)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether large language models (LLMs) like GPT-3 can be prompted with human instructions to directly generate web URLs for document retrieval in an open-domain question answering (ODQA) setting, without any explicit training as autoregressive search engines. The key hypothesis is that LLMs contain built-in capabilities to act as search engines that can map natural language questions to relevant document identifiers (URLs), which can then be retrieved to find evidence to answer the questions. The paper shows that with just a few demonstrations, LLMs can generate URLs where nearly 90% of the corresponding web documents contain correct answers to open-domain questions.In summary, the main research question is: Can LLMs be prompted to directly generate document URLs for evidence retrieval in ODQA, without explicit training? And the key hypothesis is that LLMs have inherent search engine capabilities that can be tapped into for document retrieval via URL generation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called LLM-URL that uses large language models (LLMs) to directly generate URLs for document retrieval in open-domain question answering (ODQA). The key ideas are:- LLMs can generate relevant URLs when prompted with a few query-URL demonstrations, without any explicit training as a retriever. This shows LLMs have built-in capabilities as search engines.- Retrieving documents by generating URLs with LLMs significantly outperforms existing retrieval methods like BM25 and dual-encoders, as measured by recall@k on three ODQA datasets. - Breaking retrieved documents into passages and ranking/filtering them reduces the number of irrelevant passages while maintaining high recall. - The retrieved documents from LLM-URL improve downstream ODQA performance compared to baseline methods when used with a reader model.In summary, the main contribution is showing LLMs can effectively act as retrievers for ODQA by generating URLs, outperforming existing retrieval methods. This is done without any task-specific fine-tuning, just using the LLM's knowledge learned during pre-training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using large language models like GPT-3 as built-in search engines that can generate relevant webpage URLs when prompted with a question, retrieving better evidence documents and improving performance on open-domain QA tasks compared to existing methods.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in the field of document retrieval for question answering:- This paper explores using large language models (LLMs) as built-in search engines to generate URLs, without any explicit training for retrieval. This is a novel approach compared to traditional information retrieval methods like BM25 or dual encoder models like DPR, which require dedicated training. - The idea of prompting LLMs to generate document identifiers is related to recent work on autoregressive search engines like DSI and SEAL. However, this paper uses existing URLs instead of synthetic ids, avoiding re-training and indexing costs.- The LLM-URL approach significantly outperforms baseline retrieval methods like BM25 and Contriever on document and passage retrieval metrics across multiple QA datasets. It is competitive with state-of-the-art trained retrievers.- For open-domain QA, LLM-URL + reader model beats baseline retrieval + reader models on 2 of 3 datasets, with mixed results compared to DSI/SEAL. Still impressive given no retrieval training.- The paper demonstrates Few-Shot learning by prompting with example Q-URL pairs, further improving performance without re-training. Showcases in-context learning abilities of LLMs.- Limitations include lack of knowledge update mechanisms compared to retrieve-and-read systems. Still relies on large LMs, which can be slow and inaccurate.Overall, this paper presents a novel way of leveraging LLMs' knowledge to perform strong document retrieval for QA without training a dedicated retriever. The results are quite competitive, demonstrating the search engine abilities hidden within LLMs' parameters. It's an interesting concept that contrasts with mainstream approaches in information retrieval.


## What future research directions do the authors suggest?

The authors suggest a few exciting future research directions:1. Tuning the few-shot demonstrations for specific retrieval tasks: The authors showed that providing a few query-URL pairs as demonstrations improves retrieval performance. They suggest further tuning these demonstrations for specialized retrieval tasks, similar to how Promptagator tunes prompts for specific tasks.2. Fine-tuning prompts per question: The authors suggest fine-tuning the prompt for each individual question may further improve retrieval performance, as shown in prior work using prompt tuning and demonstration clustering.3. Incorporating new knowledge: The authors point out a limitation is the inability to efficiently update the model's knowledge or adapt to new domains. They suggest exploring methods to incorporate new knowledge and documents into the model.4. Reducing hallucination errors: The authors note LLMs can hallucinate incorrect URL predictions. They suggest exploring ways to reduce these errors in future work. 5. Improving efficiency: The authors point out their approach involves large LMs and web requests, making it cumbersome. Improving computational and query efficiency could make the approach more practical.In summary, the main future directions are: prompt tuning for tasks, per-question prompt tuning, knowledge updating, reducing hallucinations, and improving efficiency. The authors propose interesting ways to build on their method and mitigate its current limitations.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper explores using large language models (LLMs) as built-in autoregressive search engines for document retrieval in open-domain question answering. Rather than training a dedicated neural retriever, the authors prompt LLMs like GPT-3 to directly generate URLs of relevant documents given a question. With just a few demonstrations, the LLM can generate URLs where nearly 90% of the corresponding documents contain answers to the questions. The retrieved documents are broken into passages and filtered to reduce noise before being fed to a reader model. Experiments on three QA datasets show this LLM-based retrieval approach consistently outperforms existing methods like BM25 and DPR in terms of retrieval metrics and downstream QA accuracy. The results demonstrate LLMs have strong latent abilities for search that can be unlocked with simple prompting, without needing explicit training on massive corpora. The approach provides an effective and scalable way to perform document retrieval for open-domain QA and other knowledge-intensive NLP tasks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper explores using large language models (LLMs) like GPT-3 for document retrieval in open domain question answering (ODQA). In ODQA, a retriever finds relevant documents and a reader extracts answers from them. Traditionally retrievers and readers are trained separately, requiring much data. This paper shows LLMs can play the role of retriever without training, by prompting them to generate URLs of relevant Wikipedia pages given a question. For example, prompting with a few demonstrations like "Who invented baseball? https://en.wikipedia.org/wiki/Abner_Doubleday" enables the LLM to generate URLs containing answers for new questions. Experiments demonstrate this LLM retriever, dubbed LLM-URL, outperforms traditional sparse and dense retrievers by a large margin on three ODQA datasets. Further, breaking retrieved documents into passages and ranking them maintains high recall while reducing input length for the reader. On two of the three datasets, the full LLM-URL retriever + reader pipeline achieves higher QA accuracy than baselines. The paper demonstrates LLMs' potential as zero-shot retrievers, though limitations include inability to update knowledge without retraining and higher computational cost. Key innovations are prompting LLMs to generate URLs conditioned on questions and in-context demonstrations.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes using large language models (LLMs) as built-in autoregressive search engines for document retrieval in open-domain question answering. Specifically, the authors prompt an LLM like GPT-3 with an open-domain question and ask it to generate relevant Wikipedia page URLs that are likely to contain the answer. The URLs are then used to retrieve the full documents, which are broken into passages, ranked, and filtered before being fed to a reader model to generate the final answer. The authors show that with just a few query-URL demonstrations, the LLM can generate URLs leading to documents containing the correct answer around 90% of the time, significantly outperforming baseline retrieval methods like BM25 and Contriever. Key to their approach is prompting the LLM to generate existing document identifiers (URLs) rather than training an autoregressive model from scratch, taking advantage of the knowledge already embedded in the LLM's parameters. Their experiments demonstrate strong performance on multiple open-domain QA datasets under both zero-shot and few-shot settings.
