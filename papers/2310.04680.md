# [The Cost of Down-Scaling Language Models: Fact Recall Deteriorates   before In-Context Learning](https://arxiv.org/abs/2310.04680)

## What is the central research question or hypothesis that this paper addresses?

 Based on the abstract, it seems the central research question is: 

How does scaling the number of parameters in large language models (LLMs) affect their core capabilities related to recalling facts seen during pre-training versus processing information presented in-context during inference?

The authors evaluate the effects of scaling LLMs (both up and down) via two techniques - pruning and dense scaling - on the model's abilities for fact recall and in-context learning. Their key findings are:

- Pruning more than 30% of weights significantly degrades the ability to recall facts seen during pre-training. However, in-context learning is preserved even with 60-70% pruning.

- Similarly, dense scaling down by reducing model width/depth degrades fact recall ability while largely preserving in-context learning. 

So the core question is understanding the disparate effects of scaling on fact recall versus in-context learning abilities in LLMs.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1) Curating a set of benchmarks to assess the effects of scaling large language models on their abilities for fact recall vs in-context learning. The benchmarks involve question answering datasets as well as parameterized function learning tasks.

2) Evaluating two scaling approaches (pruning and dense scaling) on an extensive set of 6 LLMs using the curated benchmarks. The key findings are:

- Pruning more than 30\% of weights significantly hurts fact recall abilities but in-context learning is preserved even after 60-70% pruning. 

- Dense scaling shows a similar disparity, with fact recall degrading readily while in-context learning is more robust.

3) The findings reveal scaling has an inherently different effect on fact recall versus in-context learning. This highlights the need to look beyond just aggregate metrics when evaluating scaling approaches and motivates further research directions like using pruning for interpretability and combining scaling with memory augmentation.

So in summary, the main contribution appears to be using carefully designed benchmarks to reveal the disparate effects of scaling approaches on the core abilities of fact recall and in-context learning in large language models. The insights from this analysis point to important future research directions.
