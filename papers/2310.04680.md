# [The Cost of Down-Scaling Language Models: Fact Recall Deteriorates   before In-Context Learning](https://arxiv.org/abs/2310.04680)

## What is the central research question or hypothesis that this paper addresses?

 Based on the abstract, it seems the central research question is: 

How does scaling the number of parameters in large language models (LLMs) affect their core capabilities related to recalling facts seen during pre-training versus processing information presented in-context during inference?

The authors evaluate the effects of scaling LLMs (both up and down) via two techniques - pruning and dense scaling - on the model's abilities for fact recall and in-context learning. Their key findings are:

- Pruning more than 30% of weights significantly degrades the ability to recall facts seen during pre-training. However, in-context learning is preserved even with 60-70% pruning.

- Similarly, dense scaling down by reducing model width/depth degrades fact recall ability while largely preserving in-context learning. 

So the core question is understanding the disparate effects of scaling on fact recall versus in-context learning abilities in LLMs.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1) Curating a set of benchmarks to assess the effects of scaling large language models on their abilities for fact recall vs in-context learning. The benchmarks involve question answering datasets as well as parameterized function learning tasks.

2) Evaluating two scaling approaches (pruning and dense scaling) on an extensive set of 6 LLMs using the curated benchmarks. The key findings are:

- Pruning more than 30\% of weights significantly hurts fact recall abilities but in-context learning is preserved even after 60-70% pruning. 

- Dense scaling shows a similar disparity, with fact recall degrading readily while in-context learning is more robust.

3) The findings reveal scaling has an inherently different effect on fact recall versus in-context learning. This highlights the need to look beyond just aggregate metrics when evaluating scaling approaches and motivates further research directions like using pruning for interpretability and combining scaling with memory augmentation.

So in summary, the main contribution appears to be using carefully designed benchmarks to reveal the disparate effects of scaling approaches on the core abilities of fact recall and in-context learning in large language models. The insights from this analysis point to important future research directions.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other related work:

- Scope of analysis: This paper provides an in-depth analysis on the effects of model scaling specifically on two core capabilities - fact recall and in-context learning. Much prior work has studied scaling laws and measured aggregate task performance. The focus on disentangling capabilities is novel.

- Task curation: The paper carefully curates a set of tasks to evaluate the fact recall and ICL abilities in isolation. Using overriding QA and parameterized ICL tasks is an innovative way to rigorously separate the two abilities. 

- Scaling techniques: The paper considers both pruning and dense scaling approaches to changing model size. Evaluating both provides insight into effects intrinsic to scaling rather than artifacts of a specific technique. 

- Key finding: A key finding is the disparity between how fact recall and ICL are affected by scaling. The paper shows both pruning and dense scaling impact fact recall much more readily. This core result reveals important nuances missed in typical evaluations.

- Implications: Based on the findings, the paper suggests promising research directions like using memory augmentation to offset degraded fact recall in smaller models. The disparate effects also motivate studying representations captured in the small set of weights that support ICL.

Overall, the in-depth focus on capabilities, careful task curation, and novel findings differentiate this work from typical research that measures aggregate performance to study scaling. The analysis provides unique insight into the subtler effects of scaling LLMs.
