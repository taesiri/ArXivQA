# [ConvLoRA and AdaBN based Domain Adaptation via Self-Training](https://arxiv.org/abs/2402.04964)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Domain adaptation methods often involve fine-tuning a source model on target data. For multi-target domain adaptation, having separate fine-tuned models for each target is expensive.

- Parameter efficient fine-tuning (PEFT) can adapt models with far fewer parameters, but has not been explored for medical image analysis or CNNs. 

Method:
- Propose Convolutional Low-Rank Adaptation (ConvLoRA), adapting the LoRA PEFT method to CNNs by decomposing convolution weights into smaller trainable matrices.

- Also utilize Adaptive Batch Normalization (AdaBN) to compute target-specific batch statistics instead of using source statistics.

- Integrate ConvLoRA and AdaBN into a U-Net model for brain MRI segmentation. Freeze pretrained weights, adapt ConvLoRA matrices in encoder, update batch norms with AdaBN.

Contributions:
- First work adapting LoRA PEFT from NLP models to CNNs, applied for multi-target domain adaptation in medical imaging.

- Achieve 99% parameter reduction compared to fine-tuning full model, while matching or exceeding performance.

- Demonstrate ConvLoRA and AdaBN provide efficient, accurate adaptation without dedicated target models.

- Approach is generic and integrates easily into CNN architectures, significantly reducing training costs.

In summary, the paper introduces a novel way to efficiently adapt CNNs for multi-target domain adaptation in medical imaging, reducing parameters by 99% while maintaining accuracy. The proposed ConvLoRA and AdaBN methods provide a flexible way to enhance adaptation without high cost.
