# [Length Generalization in Arithmetic Transformers](https://arxiv.org/abs/2306.15400)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How well can transformer models cope with challenges of learning basic integer arithmetic operations like addition and multiplication, and generalizing to longer input sequences than seen during training?2) Can techniques like relative position embeddings or training set priming enable transformer models to achieve better generalization on arithmetic tasks to longer integer sequences? 3) How does training set priming compare to fine-tuning as a technique to enable length generalization, in terms of sample efficiency and avoiding catastrophic forgetting?4) What are the key factors that influence the ability of transformer models to learn and generalize on arithmetic tasks like addition and multiplication?Specifically, the paper investigates whether relative position embeddings can allow length generalization on addition, finds that this technique fails for multiplication, and proposes a new technique called "train set priming" that involves adding a small number of longer examples to the training set. The key hypotheses seem to be:- Relative position embeddings will enable length generalization for addition but not multiplication.- Adding a small number of longer examples via train set priming will allow models trained on short sequences to generalize to much longer sequences for multiplication. - Train set priming will be more sample efficient and avoid catastrophic forgetting compared to fine-tuning.The experiments aim to test these hypotheses and analyze the factors influencing generalization ability of transformers on arithmetic problems.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It studies length generalization in transformers for basic arithmetic tasks like addition, modular addition, multiplication, and modular multiplication. The paper trains models on numbers with a certain number of digits (e.g. 5 digits) and tests their ability to generalize to numbers with more digits (e.g. 20 digits).2. For addition, it shows that using relative position embeddings instead of absolute position embeddings enables length generalization. Models trained on 5-digit numbers can generalize to 20-digit numbers for addition.3. For multiplication, it introduces a technique called "train set priming" where a small number of examples with longer sequences are added to the training set. This allows models trained on 5-digit x 3-digit numbers to generalize to 35-digit x 3-digit numbers. 4. It analyzes the amount of priming examples needed, and shows this scales logarithmically with training set size. It also shows the primed model can generalize to multiple lengths, not just the longest primed length.5. The techniques introduced allow transformers to achieve significant length generalization on arithmetic tasks, even though they were trained on much shorter numbers. This sheds light on improving their generalization abilities.In summary, the main contribution is enabling length generalization in transformers for basic arithmetic through relative position embeddings and train set priming. The analysis of these techniques is also a key contribution.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of length generalization in transformers:- The main novel contribution of this paper is proposing the "train set priming" technique to improve length generalization in transformers for multiplication tasks. This is different from prior work on length generalization which has focused more on modifying the position embeddings, attention mechanisms, or model architectures. - Compared to works that use relative position embeddings like Shaw et al. (2018), this paper shows that RPE alone is not sufficient for length generalization in multiplication. RPE helps for addition but not multiplication.- The train set priming idea is different from curriculum learning strategies explored in some prior works, as the paper shows curriculum priming does not help much. Just priming with the target length works best.- This paper focuses specifically on arithmetic tasks whereas most prior work on length generalization has looked at NLP tasks. The findings may be more directly relevant for making transformers better at math vs language.- The train set priming technique could be an alternative to fine-tuning for adapting transformers, one that avoids catastrophic forgetting. This could be relevant for NLP as well.- The scaling analysis of how the amount of priming examples needed changes with train set size and target length is novel. This helps understand how to set the priming hyperparameters.- The experiments are fairly comprehensive in exploring different encoders, position embeddings, model sizes, modular vs regular arithmetic, etc. This allows systematically isolating the impact of the priming technique.Overall, I would say this paper provides good insight into length generalization specifically for arithmetic tasks, proposes a simple but surprisingly effective priming technique, and includes extensive experiments and analysis to really understand the priming approach. The priming idea may be useful more broadly for transformer adaptation while avoiding catastrophic forgetting.


## What future research directions do the authors suggest?

Based on my reading of the paper, the authors suggest the following future research directions:1. Extending the train set priming framework to more complex arithmetic tasks beyond addition and multiplication, such as matrix operations. This could shed light on the capabilities and limitations of transformers for learning more advanced mathematics.2. Investigating the limits of length generalization in terms of the number and types of operations. For example, if a model is trained on adding k numbers, can it generalize to adding k+1 numbers? Or if trained on additions and multiplications separately, can it compose them together? This could reveal more about compositional generalization.3. Developing theoretical understanding of why train set priming works better than fine-tuning for length generalization. What are the key factors that make priming effective? Formal analysis could lead to principles for how to best prime models.4. Exploring whether train set priming could be applied in natural language processing. Can it help adapt pre-trained language models to new tasks/domains without catastrophic forgetting of the original data? This could make language model adaptation more practical. 5. Applying priming to other domains like computer vision, reinforcement learning, etc. to see if it provides benefits in those areas too.Overall, the authors suggest extending priming to more tasks/domains, developing theoretical understanding, and exploring whether it can improve adaptation and generalization in broader deep learning contexts beyond just arithmetic transformers.
