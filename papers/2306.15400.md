# [Length Generalization in Arithmetic Transformers](https://arxiv.org/abs/2306.15400)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) How well can transformer models cope with challenges of learning basic integer arithmetic operations like addition and multiplication, and generalizing to longer input sequences than seen during training?2) Can techniques like relative position embeddings or training set priming enable transformer models to achieve better generalization on arithmetic tasks to longer integer sequences? 3) How does training set priming compare to fine-tuning as a technique to enable length generalization, in terms of sample efficiency and avoiding catastrophic forgetting?4) What are the key factors that influence the ability of transformer models to learn and generalize on arithmetic tasks like addition and multiplication?Specifically, the paper investigates whether relative position embeddings can allow length generalization on addition, finds that this technique fails for multiplication, and proposes a new technique called "train set priming" that involves adding a small number of longer examples to the training set. The key hypotheses seem to be:- Relative position embeddings will enable length generalization for addition but not multiplication.- Adding a small number of longer examples via train set priming will allow models trained on short sequences to generalize to much longer sequences for multiplication. - Train set priming will be more sample efficient and avoid catastrophic forgetting compared to fine-tuning.The experiments aim to test these hypotheses and analyze the factors influencing generalization ability of transformers on arithmetic problems.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It studies length generalization in transformers for basic arithmetic tasks like addition, modular addition, multiplication, and modular multiplication. The paper trains models on numbers with a certain number of digits (e.g. 5 digits) and tests their ability to generalize to numbers with more digits (e.g. 20 digits).2. For addition, it shows that using relative position embeddings instead of absolute position embeddings enables length generalization. Models trained on 5-digit numbers can generalize to 20-digit numbers for addition.3. For multiplication, it introduces a technique called "train set priming" where a small number of examples with longer sequences are added to the training set. This allows models trained on 5-digit x 3-digit numbers to generalize to 35-digit x 3-digit numbers. 4. It analyzes the amount of priming examples needed, and shows this scales logarithmically with training set size. It also shows the primed model can generalize to multiple lengths, not just the longest primed length.5. The techniques introduced allow transformers to achieve significant length generalization on arithmetic tasks, even though they were trained on much shorter numbers. This sheds light on improving their generalization abilities.In summary, the main contribution is enabling length generalization in transformers for basic arithmetic through relative position embeddings and train set priming. The analysis of these techniques is also a key contribution.
