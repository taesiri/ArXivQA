# [OpenAgents: An Open Platform for Language Agents in the Wild](https://arxiv.org/abs/2310.10634)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using a combination of generative and retrieval models for question answering. What are the unique advantages and disadvantages of this hybrid approach compared to using just a generative or just a retrieval model? How does combining the two models lead to improved performance?

2. The generative module uses a sequence-to-sequence model with attention. What modifications were made to the standard seq2seq model architecture for the question answering task? How does the attention mechanism help the model focus on the relevant context? 

3. The retrieval module uses TF-IDF weighted bag of words as an information retrieval approach. What are some limitations of using TF-IDF and bag of words representation for ranking short text contexts? Would embedding-based similarity measures work better?

4. The re-ranking module uses a bi-directional LSTM to score question-context pairs. Why was an LSTM chosen over other neural architectures? What kind of linguistic features can the LSTM capture that are useful for re-ranking?

5. The training data contains only positive examples of question-context pairs. How does the lack of negative training examples impact what the model learns? Should the authors have created negative training examples?

6. The human evaluation uses BLEU, ROUGE and human judgments. BLEU and ROUGE may not correlate well with human judgments for short text QA. What other automatic evaluation metrics could have been used? How reliable are the human evaluations? 

7. Error analysis shows the model struggles with compositional and temporal reasoning. How could the model architecture be improved to handle compositional questions better? What additional training data could help with temporal reasoning?

8. How does the model handle questions that require reasoning about multiple contexts? Does the re-ranking module effectively aggregate evidence across different contexts?

9. The model was only evaluated on open-domain QA datasets. How would its performance compare to other models on closed-domain QA datasets? What adaptations would be needed?

10. The paper only examines extractive QA where the answer is a span in the context. How could the model be extended to handle abstractive QA where the answer is generated? What additional training objectives or architectures would be needed?
