# [Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial   Imitation Learning](https://arxiv.org/abs/2006.16785)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How important is enforcing Lipschitz continuity of the learned reward function for achieving good performance in off-policy generative adversarial imitation learning?The key hypothesis appears to be that enforcing Lipschitz continuity of the reward function is crucial for the method to work well. The authors posit that this smoothness constraint helps stabilize training and prevent overfitting issues that would otherwise arise in this challenging bilevel optimization setting involving learned parametric rewards.The paper investigates this question through theoretical analysis, deriving robustness guarantees for the state-action value function under a Lipschitz continuous reward. It also provides extensive empirical evidence demonstrating the necessity of the smoothness constraint on the reward, and shows that directly enforcing this via gradient penalty regularization yields much better results compared to other common regularization techniques. Overall, the paper makes a convincing case that "Lipschitzness is all you need" for this off-policy imitation learning approach to succeed.
