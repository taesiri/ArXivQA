# [Learning Spatial Adaptation and Temporal Coherence in Diffusion Models   for Video Super-Resolution](https://arxiv.org/abs/2403.17000)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement:
- Diffusion models have shown promise for image super-resolution but extending them to video super-resolution is challenging. Key issues are preserving spatial fidelity (visual appearance) across frames and maintaining temporal consistency between frames. 

- Simply applying image super-resolution diffusion models frame-by-frame fails to maintain consistency. Other methods that use diffusion models for video provide guidance at the spatial level only, making it hard to sufficiently regularize the diffusion process.

Proposed Solution: 
- The paper proposes a new method called SATeCo that introduces spatial feature adaptation (SFA) and temporal feature alignment (TFA) to provide pixel-level guidance across frames to calibrate the diffusion process.

- SFA dynamically estimates affine parameters using the low-resolution (LR) frames to modulate high-resolution (HR) frame features at the pixel-level. This injects spatial guidance to preserve fidelity.

- TFA strengthens feature interaction across frames by applying self-attention within local HR frame windows (tubelits) and cross-attention between HR tubelits and their LR counterparts. This aligns features temporally.

- SFA and TFA modules are inserted into the pre-trained decoder blocks of the UNet denoiser and VAE decoder to regularize diffusion in both latent and pixel space.

- A video upscaler refines the LR video before diffusion. A video refiner balances between upscaled LR frames and HR decoded frames.

Key Contributions:
- Novel spatial-temporal guidance learning using SFA and TFA modules to calibrate diffusion models for video super-resolution.

- Pixel-level feature modulation using estimated affine parameters for better spatial adaptation.  

- Tubelet-based self and cross-attention mechanism for temporal alignment across frames.

- State-of-the-art performance on REDS4 and Vid4 datasets, with gains in both spatial fidelity and temporal consistency over other diffusion-based video SR methods.
