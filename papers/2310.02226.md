# [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What is the effect of training and evaluating Transformer-based language models with artificially inserted "pause" tokens, which delay the model's output prediction?Specifically, the authors are interested in studying:1) Whether adding these pause tokens, which provide no explicit signals but simply delay the model's output, can enhance the model's performance on various downstream NLP tasks like question answering and commonsense reasoning.2) Whether the potential benefits arise from pause tokens inserted during pretraining, finetuning, or both stages of training. 3) The mechanisms by which pause tokens might improve performance - for example, by allowing more complex computational pathways or temporal dynamics within the model.The overarching hypothesis seems to be that delaying a language model's output response, if implemented properly during training, can enhance its generalization and reasoning abilities on certain tasks. The paper aims to test this through empirical studies on several Transformer architectures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing and evaluating a method to train language models to utilize additional computation at inference time by appending learnable "pause" tokens to the input. Specifically:- The authors propose pausing training, which involves inserting learnable <pause> tokens at random positions during pretraining and finetuning. This allows the model to process more tokens before committing to an output. - They evaluate pause training on large transformer models during pretraining on C4 and finetuning on a variety of downstream tasks. - Their key finding is that pause training, i.e. using <pause> tokens during both pretraining and finetuning, improves performance on the majority of downstream tasks compared to standard training. The gains are especially clear for the 1B parameter model, with 18% higher EM on SQuAD QA, 8% on CommonsenseQA, etc.- In contrast, only using the <pause> tokens during finetuning leads to smaller and less consistent gains. This suggests both stages of training are important for the model to take advantage of the extra computation from the pause tokens.- They also analyze the effect of varying the number of pause tokens and find that each task has a different optimal amount. The models are also relatively robust to mismatches between finetuning and inference.In summary, the main contribution is proposing pause training to allow language models to learn to benefit from additional computation at inference time, and demonstrating clear gains from this approach when applied during both pretraining and task finetuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes training language models to pause and think before generating an output, by inserting learnable "pause" tokens during pretraining and finetuning, which improves performance on downstream tasks like question answering and reasoning.
