# [On the Convergence of Decentralized Federated Learning Under Imperfect   Information Sharing](https://arxiv.org/abs/2303.10695)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Decentralized federated learning (DFL) is important for privacy-preserving distributed machine learning. However, most DFL algorithms assume perfect communication between agents, while in reality channels can be noisy.

- This paper studies the impact of noisy communication channels on the convergence of DFL algorithms. Specifically, it analyzes three DFL algorithms: FedNDL1 and FedNDL2 from prior work, and a new proposed method FedNDL3.

Proposed Solution: 
- In FedNDL1, noise is added to the parameters after local SGD updates before gossip averaging. 

- In FedNDL2, noise is added before local SGD updates and gossip averaging.

- In FedNDL3, noise is added to the stochastic gradients directly before gossip averaging. 

Key Results:
- Theoretical analysis shows that for FedNDL1 and FedNDL2, the impact of noise on convergence gets worse over iterations and with sparse communication topologies.

- For FedNDL3, noise has a diminishing effect over iterations and is independent of topology. This indicates it is more resilient to imperfect information.

- Experiments on regression verify that FedNDL3 converges smoothly in the presence of noise while FedNDL1 and FedNDL2 diverge.

Main Contributions:
- Formal analysis of the convergence guarantees of three DFL algorithms under noisy communication for non-convex smooth loss.

- Demonstrating both theoretically and empirically that communicating noisy gradients in FedNDL3 makes it robust compared to sending noisy parameters in FedNDL1/2.

- Establishing that existing DFL algorithms can fail under imperfect information while a simple modification of communicating gradients instead of parameters significantly improves resilience.

In summary, the key insight is that noisy gradients can be better than noisy parameters for decentralized learning over imperfect channels. The proposed FedNDL3 algorithm successfully mitigates the effects of noise for practical DFL applications.
