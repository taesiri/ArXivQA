# [Remember This Event That Year? Assessing Temporal Information and   Reasoning in Large Language Models](https://arxiv.org/abs/2402.11997)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates the capabilities of large language models (LLMs) in retaining and reasoning about temporal knowledge. It argues that effectively understanding the sequential nature of events and temporal trends is crucial for LLMs to be useful in real-world applications. However, current LLMs exhibit significant limitations in their ability to comprehend temporal information.

To demonstrate this, the paper shows an example where state-of-the-art open-source and closed-source LLMs fail to answer a query correctly that requires retaining and reasoning about temporal knowledge (see Figure 1). 

Proposed Solution:
To systematically analyze the temporal capabilities of LLMs, the paper introduces TempUN, a novel large-scale dataset with over 9 million temporal query-answer pairs spanning 8 categories of major global issues defined by the UN. 

The dataset comprises two variants: (i) TempUN - the full dataset with 462K instances derived from Our World in Data (ii) TempUN_s - a filtered subset with 1,907 instances that meet data availability and temporal dynamics criteria.

Using TempUN_s, the paper transforms each sample into: (i) Next Word Prediction queries (ii) 6 types of Multiple Choice Questions testing different aspects of temporal reasoning.

The paper experiments with 3 open-source (Phi-2, Mistral, LLaMA) and 3 closed-source (GPT-3.5, GPT-4, Gemini) LLMs on TempUN_s under zero-shot conditions and 3 training paradigms - Yearwise Finetuning, Continual Finetuning, Random Finetuning.

Main Contributions:

1. Introduces TempUN, the largest public dataset for evaluating temporal capabilities of LLMs, with over 9 million samples spanning diverse global issues and 76 years.

2. Reveals severe limitations in state-of-the-art LLMs' ability to retain and reason about temporal knowledge through zero-shot evaluation.

3. Finds that closed-source models exhibit a higher tendency to acknowledge lack of knowledge compared to open-source models which frequently provide incorrect responses.

4. Shows that advanced fine-tuning provides no significant improvements indicating persistent gaps in temporal comprehension.

The paper argues that enhancing LLMs' temporal capabilities is vital for real-world viability and provides the TempUN dataset to facilitate future research towards this goal.
