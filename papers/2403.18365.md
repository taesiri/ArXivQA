# [BLADE: Enhancing Black-box Large Language Models with Small   Domain-Specific Models](https://arxiv.org/abs/2403.18365)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT lack sufficient domain-specific knowledge to perform well on vertical domain tasks like legal and medical question answering. 
- Existing methods to adapt LLMs have limitations - continuous pretraining is expensive and may reduce capabilities, while retrieval augmentation can provide irrelevant or misleading information.

Proposed Solution - BLADE:
- Framework to enhance general black-box LLMs with small domain-specific white-box LMs containing specialized knowledge.
- Involves domain-specific pretraining, knowledge instruction tuning using pseudo-data from LLM, and Bayesian optimization to align outputs.

Key Contributions:
- Introduces BLADE - a new paradigm for effectively and efficiently adapting general large LMs to vertical domains using small domain LMs.
- Proposes two strategies - Knowledge Instruction Tuning and Bayesian Prompted Optimization to improve small LM's adaptation. 
- Experiments on legal and medical QA datasets show BLADE significantly outperforms existing domain adaptation approaches that rely on continuous pretraining or retrieval augmentation.

In summary, the paper presents BLADE as a novel, performant and cost-efficient solution to equip general language models with domain expertise for specialized tasks, without additional pretraining or human annotations. The introduced techniques help align and optimize the collaboration between small and large LMs.


## Summarize the paper in one sentence.

 This paper proposes BLADE, a framework to enhance general large language models for domain-specific tasks by collaborating with a small domain-specific language model that provides specialized knowledge.


## What is the main contribution of this paper?

 This paper proposes BLADE, a new framework for adapting general large language models (LLMs) to specific domains. The main contributions are:

1. It introduces a collaborative approach between general black-box LLMs and small white-box domain-specific LMs to solve problems in vertical domains. The small LMs preserve domain knowledge while the general LLMs contribute language comprehension and reasoning capabilities.

2. It proposes two strategies - Knowledge Instruction Tuning (KIT) and Bayesian Prompted Optimization (BPO) - to enhance the small LM's adaptation to the general LLM. KIT tunes the small LM to generate knowledge tailored to specific queries. BPO aligns the output of the small LM with the general LLM using derivative-free optimization.

3. It conducts extensive experiments on public legal and medical benchmarks. BLADE shows significant improvements over existing approaches like continuous pre-training and retrieval augmentation in adapting general LLMs to these domains.

In summary, the main contribution is proposing BLADE as an effective and cost-efficient solution to adapt general language models to specialized domains by collaborating a small domain-specific LM with a black-box general LLM.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Large language models (LLMs)
- Domain adaptation
- Vertical domains (e.g. legal, medical)
- Continuous pre-training
- Retrieval augmentation
- Black-box models
- Small domain-specific language models
- Knowledge Instruction Tuning (KIT)
- Bayesian Prompted Optimization (BPO)
- Legal question answering
- Medical question answering
- In-context learning
- Instruction following

The paper proposes a new framework called BLADE for adapting general large language models to specialized domains by enhancing them with small domain-specific language models. The key ideas include pre-training the small LM on domain data, using KIT to improve its instruction following abilities, and aligning it with the general LLM using BPO. Experiments on legal and medical QA datasets demonstrate the effectiveness of BLADE over existing domain adaptation approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does BLADE balance incorporating domain-specific knowledge while preserving the reasoning capabilities of general LLMs? What are the potential risks and mitigation strategies?

2. The paper mentions "small language models have reduced size and can be easily trained to memorize new knowledge." How does the size of small LMs impact performance? What is the optimal scale?

3. Knowledge Instruction Tuning uses pseudo data from the general LLM. How reliable and unbiased is this data? Are there methods to further verify quality?  

4. Bayesian Prompted Optimization aligns small LM outputs with the general LLM. How is the intrinsic dimensionality and projection matrix chosen? What constraints guide this?

5. BLADE shows strong results on multiple choice QA. How would it perform on generative tasks? Would additional techniques be required?

6. Could the small LM ever contradict or mislead the general LLM? If so, how can we detect and prevent this?

7. What types of language models would be most suitable as the small LM? Does architecture matter more than scale for this role?

8. How labor and data intensive is it to adapt BLADE to new specialized domains? What factors determine ease of adaptation?  

9. BLADE combines domain pretraining, KIT and BPO. What is the relative importance of each component? Where does effectiveness originate?

10. The paper focuses on legal and medical domains. What other professional fields could benefit from BLADE? What unique constraints may they present?
