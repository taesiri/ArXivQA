# [Can Language Models Learn to Listen?](https://arxiv.org/abs/2308.10897)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can language models learn to generate appropriate listener facial responses solely from the speaker's words in a dyadic conversation?The key hypothesis appears to be that gesture is a language component, so a transformer-based language model can be adapted to generate semantically meaningful and temporally aligned listener gestures by treating discrete atomic motion elements as additional language tokens. Specifically, the paper proposes fine-tuning a pretrained language model like GPT-2 on a novel task of predicting quantized motion tokens representing atomic gesture elements, given input consisting of temporally aligned speaker text tokens. The hypothesis is that this approach will allow the model to generate realistic and synchronous listener responses relying only on lexical semantics, without access to speaker audio or visual input.In summary, the central research question is whether language models can generate plausible listener gestures from just speaker transcripts, by framing gesture as an extension of language modeling. The key hypothesis is that the semantic and temporal signals contained in text are sufficient for generating realistic listener reactions.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be a framework for generating appropriate facial responses for a listener in dyadic social interactions based solely on the speaker's words. Specifically, the key ideas proposed are:- Representing atomic gesture elements from the listener's facial motions as novel language tokens using a VQ-VAE. This allows converting the continuous facial motion into a discrete sequence of tokens.- Fine-tuning a pretrained large language model (LLM) like GPT-2 to autoregressively predict these gesture tokens given the temporally aligned text tokens from the speaker. By interleaving text and gesture tokens based on word timestamps, they ensure the model generates each gesture token based only on past speaker words.- Showing that initializing with a LLM pretrained on just text significantly outperforms training from scratch, suggesting knowledge transfers from language modeling to gestural responses.- Demonstrating through quantitative metrics and human evaluation that the text-conditioned model produces listener responses that are temporally synchronous with the speaker's words and also semantically meaningful (e.g. smiling at something positive). - Analyzing the model's ability to leverage temporal and semantic signals from text alone to produce appropriate listener responses.In summary, the key contribution is showing that listener gestures can be effectively generated just from the speaker's words by transferring knowledge from large pretrained language models to this multimodal generation task. The paper provides both a model and analysis for text-driven gesture generation.
