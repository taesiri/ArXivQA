# [Attention: Marginal Probability is All You Need?](https://arxiv.org/abs/2304.04556)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions and hypotheses appear to be:- Can a unifying probabilistic framework be developed to understand different attention mechanisms in machine learning? The authors propose representing attention mechanisms as marginalizing over possible connectivity structures in a graphical model.- Can iterative attention mechanisms like slot attention and block slot attention be understood through a Bayesian perspective as collapsed variational inference? The authors show these iterative attention schemes arise naturally from a collapsed variational inference procedure. - Can this Bayesian perspective connect attention mechanisms in ML to those developed in computational neuroscience? The authors link their framework to predictive coding networks, an influential theory from neuroscience.- Does this probabilistic framing provide benefits like better understanding of hard vs soft attention and suggesting more efficient attention mechanisms? The authors discuss how their framework provides insights into hard attention as a stochastic approximation and suggests avenues for developing more efficient attention schemes.In summary, the central hypothesis is that a unified Bayesian perspective on attention can help explain and connect attention mechanisms in ML and neuroscience, while also providing insights that could lead to improved architectures. The paper aims to demonstrate this via examples showing how different attention schemes arise from their probabilistic framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Providing a unifying probabilistic framework for understanding attention mechanisms in machine learning. The key insight is viewing soft attention (e.g. self-attention, cross-attention) as marginalizing over possible graphical model structures. - Extending this view to iterative attention mechanisms like slot attention and block-slot attention by framing them as collapsed variational inference. This provides a theoretical grounding for these types of attention models.- Making a connection between this view of attention and Bayesian theories of attention in computational neuroscience through the lens of Predictive Coding Networks. This helps bridge machine learning and cognitive science conceptions of attention.- Overall, the probabilistic perspective provides a unified way to understand and analyze different attention architectures in ML and neuroscience. It also suggests ways to design new attention mechanisms in a more principled manner by specifying appropriate priors and distributions to marginalize over.In summary, the main contribution appears to be providing a Bayesian probabilistic framing to attention that helps unify and understand different attention architectures across machine learning and neuroscience. This new perspective enables more effective analysis and design of attention models.
