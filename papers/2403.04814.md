# [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](https://arxiv.org/abs/2403.04814)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing benchmarks for evaluating large language models (LLMs) on code generation tasks have limitations. Benchmarks like HumanEval and MBPP focus on generating standalone functions from descriptions, not reflecting real-world development workflows. The HumanEval-Infilling benchmark for fill-in-the-middle (FIM) tasks is small, only using 164 Python snippets. There is a need for a large-scale, syntax-aware FIM benchmark spanning multiple languages to assess LLMs' capabilities in common coding practices like expanding existing code.

Proposed Solution:
The authors introduce Syntax-Aware Fill-in-the-Middle (SAFIM), a robust FIM benchmark with 17,720 examples covering algorithmic blocks, control flows, and API calls over Python, Java, C++ and C#. SAFIM uses recent code from Codeforces and GitHub (after April 2022) to avoid test set contamination. It emphasizes syntax-aware completion aligned with code's abstract syntax tree. SAFIM implements various prompt designs and a novel syntax-aware post-processing technique to enable fair comparisons across diverse LLMs.

Key Contributions:
- SAFIM benchmark with 17,720 FIM examples in multiple languages and categories, far larger in scale and scope than prior benchmarks
- Deliberate use of recent code to prevent test set contamination risks 
- Suite of prompt designs compatible with different LLMs
- New syntax-aware truncation algorithm that boosts output quality and ensures fair assessment
- Extensive evaluation of 15 major LLMs, revealing insights on impact of pretraining methods over model size
- Foundation for future research on effective pretraining strategies for code LLMs

In summary, SAFIM pushes forward FIM evaluation for code LLMs through its large-scale, multi-language design, rigorous data selection, specialized prompt and post-processing methods, and extensive experimental analysis. It paves the path for future advances in pretraining techniques for coding tasks.
