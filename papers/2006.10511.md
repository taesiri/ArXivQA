# [Contrastive learning of global and local features for medical image   segmentation with limited annotations](https://arxiv.org/abs/2006.10511)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can contrastive learning frameworks for self-supervised representation learning be adapted and improved for volumetric medical image segmentation tasks with limited labeled data? More specifically, the authors propose and investigate:1) Novel contrasting strategies that leverage the structural similarity typically present across medical image volumes of the same anatomy (domain-specific cue) to better define similar and dissimilar pairs for the global contrastive loss. 2) A local version of the contrastive loss that encourages the network to learn distinctive representations of local image regions, in order to better distinguish neighboring areas, which is useful for segmentation (problem-specific cue).The central hypothesis is that incorporating these domain and problem-specific cues into contrastive self-supervised learning will yield better feature representations that improve segmentation accuracy when fine-tuned on small labeled datasets, compared to prior contrastive learning approaches.So in summary, the key question is how to adapt contrastive self-supervised learning for medical volumetric segmentation with scarce labels, with a focus on using domain and task-specific knowledge to improve the learned representations. The proposed local contrastive loss and similarity notions leveraging inter-volume structure are the main novelties.


## What is the main contribution of this paper?

This paper proposes novel contrastive learning strategies for medical image segmentation in settings with limited labeled data. The main contributions are:1. New contrasting strategies for the global contrastive loss that leverage naturally occurring similarity cues in volumetric medical images, by using corresponding slices across different volumes as similar pairs. 2. A local contrastive loss to learn distinctive representations of local regions in an image, which complements the global representations learned via the global loss.3. Evaluation of the proposed strategies on three MRI segmentation datasets shows substantial improvements compared to no pre-training, pre-training with only the global loss, and other self-supervised learning methods. 4. The proposed pre-training provides benefits complementary to data augmentation and semi-supervised learning techniques for learning with limited annotations.In summary, the key novelty is in designing contrastive learning strategies tailored for volumetric medical images and segmentation tasks by incorporating domain knowledge and problem-specific cues into the contrastive loss formulation, at both global and local levels. This allows learning useful representations from unlabeled medical volumes to significantly boost performance when only few labeled examples are available for the target segmentation task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in this paper:This paper proposes strategies to extend contrastive learning for volumetric medical image segmentation in a semi-supervised setting by incorporating domain-specific cues through novel contrasting strategies leveraging structural similarity of images, and problem-specific cues through a local contrastive loss to learn distinctive local representations.


## How does this paper compare to other research in the same field?

This paper proposes several novel strategies to extend contrastive learning for volumetric medical image segmentation in a semi-supervised setting with limited annotations. Specifically, the main contributions are:1. New contrasting strategies for the global contrastive loss that leverage the structural similarity present across different volumes of the same anatomical region. This allows defining more complex notions of similarity compared to just using data augmentation.2. A local contrastive loss to learn distinctive representations of local regions within an image, unlike prior contrastive learning works that focus on global image-level features. The local loss complements the global loss for segmentation tasks.3. Evaluation on MRI datasets shows substantial gains compared to no pre-training, pre-training with only global loss, pretext task-based pre-training, and other semi-supervised methods in the limited annotation regime.Some key differences to related works are:- Prior contrastive learning works focus on global image features for classification tasks, while this work proposes strategies for dense prediction tasks like segmentation.- Works on local representations for contrastive learning aim to improve global features. This work aims to learn complementary local features explicitly.- Most works define similarity via data augmentation. This work incorporates domain knowledge to define more complex similarity relations.- Several recent semi-supervised methods for medical imaging use pretext tasks, adversarial training, data augmentation etc. This work shows pre-training with contrastive losses outperforms them given the same encoder-decoder architecture.Overall, the proposed strategies advance contrastive representation learning specifically for volumetric medical image segmentation with limited labels. The gains demonstrate the benefits of designing losses grounded in domain and problem structure.
