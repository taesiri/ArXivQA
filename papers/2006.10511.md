# [Contrastive learning of global and local features for medical image   segmentation with limited annotations](https://arxiv.org/abs/2006.10511)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can contrastive learning frameworks for self-supervised representation learning be adapted and improved for volumetric medical image segmentation tasks with limited labeled data? More specifically, the authors propose and investigate:1) Novel contrasting strategies that leverage the structural similarity typically present across medical image volumes of the same anatomy (domain-specific cue) to better define similar and dissimilar pairs for the global contrastive loss. 2) A local version of the contrastive loss that encourages the network to learn distinctive representations of local image regions, in order to better distinguish neighboring areas, which is useful for segmentation (problem-specific cue).The central hypothesis is that incorporating these domain and problem-specific cues into contrastive self-supervised learning will yield better feature representations that improve segmentation accuracy when fine-tuned on small labeled datasets, compared to prior contrastive learning approaches.So in summary, the key question is how to adapt contrastive self-supervised learning for medical volumetric segmentation with scarce labels, with a focus on using domain and task-specific knowledge to improve the learned representations. The proposed local contrastive loss and similarity notions leveraging inter-volume structure are the main novelties.


## What is the main contribution of this paper?

This paper proposes novel contrastive learning strategies for medical image segmentation in settings with limited labeled data. The main contributions are:1. New contrasting strategies for the global contrastive loss that leverage naturally occurring similarity cues in volumetric medical images, by using corresponding slices across different volumes as similar pairs. 2. A local contrastive loss to learn distinctive representations of local regions in an image, which complements the global representations learned via the global loss.3. Evaluation of the proposed strategies on three MRI segmentation datasets shows substantial improvements compared to no pre-training, pre-training with only the global loss, and other self-supervised learning methods. 4. The proposed pre-training provides benefits complementary to data augmentation and semi-supervised learning techniques for learning with limited annotations.In summary, the key novelty is in designing contrastive learning strategies tailored for volumetric medical images and segmentation tasks by incorporating domain knowledge and problem-specific cues into the contrastive loss formulation, at both global and local levels. This allows learning useful representations from unlabeled medical volumes to significantly boost performance when only few labeled examples are available for the target segmentation task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in this paper:This paper proposes strategies to extend contrastive learning for volumetric medical image segmentation in a semi-supervised setting by incorporating domain-specific cues through novel contrasting strategies leveraging structural similarity of images, and problem-specific cues through a local contrastive loss to learn distinctive local representations.


## How does this paper compare to other research in the same field?

This paper proposes several novel strategies to extend contrastive learning for volumetric medical image segmentation in a semi-supervised setting with limited annotations. Specifically, the main contributions are:1. New contrasting strategies for the global contrastive loss that leverage the structural similarity present across different volumes of the same anatomical region. This allows defining more complex notions of similarity compared to just using data augmentation.2. A local contrastive loss to learn distinctive representations of local regions within an image, unlike prior contrastive learning works that focus on global image-level features. The local loss complements the global loss for segmentation tasks.3. Evaluation on MRI datasets shows substantial gains compared to no pre-training, pre-training with only global loss, pretext task-based pre-training, and other semi-supervised methods in the limited annotation regime.Some key differences to related works are:- Prior contrastive learning works focus on global image features for classification tasks, while this work proposes strategies for dense prediction tasks like segmentation.- Works on local representations for contrastive learning aim to improve global features. This work aims to learn complementary local features explicitly.- Most works define similarity via data augmentation. This work incorporates domain knowledge to define more complex similarity relations.- Several recent semi-supervised methods for medical imaging use pretext tasks, adversarial training, data augmentation etc. This work shows pre-training with contrastive losses outperforms them given the same encoder-decoder architecture.Overall, the proposed strategies advance contrastive representation learning specifically for volumetric medical image segmentation with limited labels. The gains demonstrate the benefits of designing losses grounded in domain and problem structure.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring different strategies for defining similar and dissimilar image pairs when computing the contrastive losses. The authors propose some strategies in this paper, but suggest there may be other ways to leverage problem-specific or domain-specific knowledge to define better pairs.- Applying the proposed local contrastive loss idea to other dense prediction tasks beyond segmentation, such as depth estimation or surface normal prediction. The authors hypothesize the local contrastive loss could be useful for other tasks that require distinguishing between local regions.- Evaluating the proposed pre-training strategies on a wider range of medical imaging datasets, especially 3D datasets, to further validate their usefulness. The authors demonstrate results on a few MRI datasets but suggest more comprehensive evaluation is needed.- Combining the proposed pre-training strategies with other techniques like generative models or synthesis of realistic pathological cases to further improve performance in low annotation settings.- Developing better strategies for uncertainty estimation when using models trained with limited annotations, to ensure reliability before clinical deployment.- Exploring how the learned representations could be transferred to different but related tasks and datasets via fine-tuning.- Investigating other potential ways to incorporate domain knowledge into contrastive learning framework for medical images.In summary, the main future directions are around exploring extensions of the proposed ideas to other tasks, datasets, and techniques, as well as further improving the strategies for leveraging domain/problem knowledge and estimating uncertainty with limited training data. The authors propose their work as a promising direction for low-annotation medical image analysis.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes strategies for extending contrastive learning techniques to improve segmentation of volumetric medical images in a semi-supervised setting with limited annotations. The authors leverage domain-specific and problem-specific cues. Specifically, they propose novel contrasting strategies that use structural similarity across volumetric medical images as a domain-specific cue. They also propose a local version of the contrastive loss to learn distinctive representations of local regions, providing a problem-specific cue useful for per-pixel segmentation. Experiments on three MRI datasets demonstrate substantial improvements over other self-supervision and semi-supervised methods when using limited annotations. With a simple data augmentation technique, the proposed approach reaches within 8% of benchmark performance using only two labeled MRI volumes for training, corresponding to just 4% of the full training data. The results indicate the proposed strategies enable more effective self-supervised pre-training for medical image segmentation when only limited annotations are available.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes strategies for extending contrastive learning for semi-supervised segmentation of volumetric medical images with limited annotations. Contrastive learning is a type of self-supervised learning which learns useful image representations by enforcing similarity between differently augmented views of the same image via a contrastive loss. The authors propose two main extensions: 1) Novel domain-specific contrasting strategies for medical volumes that leverage structural similarity across volumes, such as aligning corresponding 2D slices from different volumes as similar pairs. This provides more complex similarity cues compared to standard data augmentations. 2) A local contrastive loss that encourages local feature representations to be consistent under transformations but distinctive across different local regions, in order to learn useful features for segmentation. The proposed methods were evaluated on three MRI datasets for cardiac and prostate segmentation. The domain-specific contrasting strategies and local contrastive loss are shown to provide substantial gains over baseline contrastive learning, especially when using very few labeled volumes for fine-tuning. The proposed full method reaches within 8% of benchmark performance using only 2 labeled volumes, vs using 50-90% of labeled data for benchmark. Ablation studies validate benefits of both proposed extensions over baseline contrastive learning. The method is shown to provide further gains when combined with complementary techniques like data augmentation and semi-supervised learning. Experiments on a natural image dataset demonstrate wider applicability beyond medical imaging. Overall, the work presents effective strategies to improve contrastive self-supervised learning for volumetric segmentation using limited labeled data.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes extensions to contrastive learning, a self-supervised learning technique, to improve medical image segmentation with limited annotations. The key ideas are: 1) Using domain knowledge to devise better strategies for selecting pairs of similar and dissimilar images when computing the contrastive loss. This is done by leveraging the natural correspondence between slices in different aligned medical volumes. 2) Proposing a local version of the contrastive loss that ensures local feature representations are distinctive across different regions in an image. This helps produce better representations for dense prediction tasks like segmentation. The overall framework involves first pre-training an encoder with a global contrastive loss, then freezing the encoder and pre-training the decoder layers with a local contrastive loss. The pre-trained encoder and decoder are then fine-tuned on the downstream segmentation task using limited annotated images. Experiments on medical imaging datasets show substantial gains over no pre-training and over pre-training with only global contrastive loss.
