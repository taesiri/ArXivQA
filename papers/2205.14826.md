# [Robust Weight Perturbation for Adversarial Training](https://arxiv.org/abs/2205.14826)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is:How can we improve adversarial robustness in deep neural networks through better constraints on adversarial weight perturbation during training?The key points are:- Adversarial weight perturbation helps reduce robust overfitting in adversarial training, but can also undermine robustness improvements if done excessively without constraints. - The paper proposes a new criterion called "Loss Stationary Condition" (LSC) to better constrain the extent of weight perturbation during training.- Using LSC, the authors find it's essential to perturb weights on adversarial examples with small classification loss, but not those with large loss. Perturbing all examples excessively is harmful.- This motivates their proposed "Robust Weight Perturbation" (RWP) strategy to constrain perturbation to only small-loss adversarial examples during training.- Experiments show RWP significantly improves robustness over state-of-the-art adversarial training methods by preventing overfitting while avoiding the side effect of excessive perturbation.In summary, the central hypothesis is that better constrained weight perturbation through the proposed LSC criterion and RWP strategy will improve adversarial robustness compared to prior adversarial training methods. The paper aims to demonstrate this through theoretical analysis, proposed method, and experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing the Loss Stationary Condition (LSC) criterion to analyze adversarial weight perturbation. LSC provides a better understanding of robust overfitting in adversarial training. 2. Using LSC, the authors find that better perturbation of model weights is associated with perturbing adversarial data with small classification loss, rather than data with large loss. Perturbing data with large loss is not necessary and can even be harmful.3. Proposing a robust perturbation strategy called Robust Weight Perturbation (RWP) to constrain the extent of weight perturbation based on the loss value. This aims to prevent overfitting while avoiding excessive perturbation.4. Demonstrating through experiments that RWP significantly improves the robustness of various adversarial training methods on benchmark datasets. It outperforms prior adversarial weight perturbation techniques.In summary, the key contribution is proposing the LSC criterion to better understand robust overfitting, which then motivates a robust weight perturbation strategy (RWP) that achieves state-of-the-art adversarial robustness. The analysis of the relationship between loss value and effective perturbation is also an important contribution.
