# [SHARCS: Efficient Transformers through Routing with Dynamic Width
  Sub-networks](https://arxiv.org/abs/2310.12126)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces the concept of "sample hardness" to guide adaptive inference. How exactly is sample hardness defined and estimated in this work? What are the limitations of using the proposed heuristics to estimate sample hardness?

2. The router module is a key component of the proposed method. How is the router designed and trained? What objective does it optimize? What are other potential ways to design and train the router? 

3. The method uses sub-networks of varying widths within the original network. How exactly are these sub-networks constructed by reducing the width of different components? What are the trade-offs between reducing width versus depth or other dimensions?

4. How does the proposed method differ fundamentally from prior work on adaptive inference like early exiting? What unique advantages does it offer over those approaches? What disadvantages might it have?

5. Could the idea of sample hardness aware routing be integrated with other techniques like knowledge distillation, pruning etc. to achieve greater efficiency? If so, how can that be done effectively?

6. The experiments mainly focus on transformer encoders. How readily could this approach be applied to transformer decoders or encoder-decoder models? What modifications might be needed?

7. What inferences can be drawn about the method's effectiveness from the results on various datasets and encoder models? When does it outperform baselines the most? When does it struggle?

8. The paper mentions deploying different sub-networks on devices with varying compute constraints. How feasible would this be in practice? What engineering challenges need to be addressed?

9. How might the training overhead of this method compare to other adaptive inference techniques? Could the router design be improved to lower training costs?

10. The paper lists several limitations. What other limitations could this approach have? How might those be mitigated in future work? What enhancements seem most promising?


## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question/hypothesis of this paper is:

How can we develop an efficient sample-adaptive inference method for transformer models that adjusts the model capacity according to the difficulty of the input sample? 

The key ideas and contributions towards addressing this question seem to be:

1) Proposing a novel method called SHARCS that introduces a lightweight "router" module that can be trained on any transformer network to predict sample hardness and route inputs dynamically to sub-networks of varying widths.

2) A new way to generate "hardness" labels for training the router, based on the model's confidence scores on training samples over multiple epochs.

3) Demonstrating SHARCS' ability to achieve much better efficiency vs accuracy tradeoffs compared to prior sample-adaptive methods like early exiting, across various datasets and transformer architectures. 

4) Showing that SHARCS can further improve efficiency of already compressed models like DistilBERT, and that it complements other techniques like token dropping.

5) Analysis of SHARCS components like the router training strategy, number of sub-networks, confidence thresholds etc.

So in summary, the main hypothesis is around developing a novel sample-adaptive routing technique to improve transformer efficiency in a way that flexibly adapts model capacity to sample difficulty. The paper seems to provide compelling evidence for this through comprehensive experiments and analysis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

1. The paper proposes SHARCS, a novel sample-adaptive inference method that introduces a lightweight router module. The router is trained to predict sample hardness and dynamically assign inputs to sub-networks of varying widths within the model. 

2. SHARCS delivers substantial efficiency improvements in terms of accuracy vs FLOPs tradeoff across different datasets and transformer encoders. For example, it reduces the FLOPs of RoBERTa-base by 2.75x on QQP with only 1% accuracy drop.

3. The gains from SHARCS can be realized in real-world deployment with significant latency reduction. On QQP, it speeds up BERT-base inference by more than 2x on CPU with less than 1% accuracy drop.

4. SHARCS outperforms or complements prior sample-adaptive inference techniques like early exiting. It can also further improve efficiency of already compressed models like DistilBERT.

5. A novel way to generate sample hardness labels for training the router, using the model's confidence scores on training data over multiple epochs.

In summary, the main contribution seems to be proposing a router-based sample-adaptive inference method that is model-agnostic, efficient, and fast while delivering strong empirical performance. The router training approach is also novel.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel approach called SHARCS that introduces a light-weight router trained to dynamically assign input samples to sub-networks of varying widths within a transformer model according to sample hardness, enabling more efficient inference.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in efficient transformers and adaptive inference:

- It proposes a novel approach of training a router module that dynamically assigns input samples to sub-networks of different widths. Most prior work has focused on early exiting or token dropping for adaptive inference. Using varying network widths is a relatively new technique.

- The router is trained based on heuristically estimating sample hardness using the model's prediction history. This is a unique way of generating "hardness" labels to train the router that hasn't been explored before. Prior adaptive methods often rely on confidence scores or entropy for routing.

- Experiments show the method consistently outperforms or complements other sample-adaptive techniques like early exiting across datasets and models. It also further improves already compressed models like DistilBERT. This demonstrates its flexibility and general applicability.

- The router approach seems to maintain accuracy better compared to fully skipping layers, as is common in early exiting. This suggests that substantially reducing width may be more efficient than fully removing deeper layers.

- The method is shown to provide meaningful speedups in CPU inference latency, demonstrating its potential for practical deployment gains. This complements most prior work that reports FLOPS reduction.

Overall, the router-based technique to enable dynamic width configurations is a novel contribution compared to predominant approaches like early exiting and token dropping. The training methodology and generality across models/tasks are also differentiators from prior art.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing sample adaptive inference methods for other architectures like decoder-only and encoder-decoder models. The paper focused on applying the proposed approach to transformer encoders, but mentions it could be useful to integrate it into other types of models as well.

- Exploring the application to regression tasks. The current work is on classification tasks only. Extending the approach to handle regression could broaden its applicability.

- Combining the proposed approach with other complementary methods like token dropping to achieve further efficiency gains. The paper shows initial experiments pairing their method with token dropping that demonstrate promising results. More exploration in this area is suggested. 

- Improving the training efficiency and stability of the router module. The paper mentions the randomness in the training process can cause instability in generating the hardness labels. Addressing this could lead to better router performance.

- Considering different heuristics for estimating sample hardness beyond confidence scores. The paper tested entropy but there may be other ways to get useful hardness labels that could be explored.

- Evaluating the impact on other metrics like memory usage and model size in addition to inference latency/FLOPs. The focus was mainly on FLOPs but looking at other efficiency measures could give a fuller picture.

- Testing the approach on a broader range of datasets, tasks, and model types to better understand the generalizability. The initial results seem promising but more extensive evaluation is likely needed.

So in summary, the key directions highlighted are improving the training process, combining with complementary methods, expanding to new models/tasks, evaluating additional metrics, and further benchmarking on diverse datasets.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper introduces SHARCS, a novel method for efficient adaptive inference in transformers. SHARCS trains a lightweight router module that can predict the hardness of an input sample and route it to sub-networks of varying widths within the original network. Hardness labels for training the router are generated heuristically during training based on the model's confidence scores on each sample over a window of epochs. At inference, the router selects the sub-network width based on predicted hardness. Experiments show SHARCS outperforms or complements other sample-adaptive methods across tasks and models. Key benefits are 2-3x FLOPs reduction with minimal accuracy drop, improved efficiency even for compressed models like DynaBERT, and over 2x speedup for BERT with <1% accuracy decrease. The router and training approach are model-agnostic. SHARCS represents a new category of efficient adaptive inference via dynamic width adjustment.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper introduces Sample Hardness Aware Routing based on Confidence Scores (SHARCS) for adaptive inference in transformer models. The key idea is to train a lightweight "router" module that can predict the hardness of an input sample and route it to an appropriate sub-network with reduced width. This allows the model to allocate varying amounts of compute for different inputs. SHARCS first estimates sample hardness heuristically during training based on the model's confidence scores. These hardness estimates are used as labels to train the router. At inference time, the router assigns the sample to a sub-network with width proportional to the predicted hardness. Experiments show SHARCS outperforms or complements prior methods like early exiting, delivering better accuracy-FLOPs tradeoffs on GLUE. It also improves efficiency for compressed models like DistilBERT. Benefits are shown on CPU latency, with over 2x speedup on BERT with minimal accuracy drop.

In more detail, the paper proposes a 3-step approach: (1) Estimate sample hardness by tracking model confidence scores on training data over time. (2) Train a lightweight router module that takes model embeddings as input and predicts hardness. (3) Create sub-networks of varying widths within the model. The router assigns inputs to different sub-networks based on predicted hardness. Key results are that SHARCS surpasses prior sample-adaptive techniques, especially in low-FLOPs regimes, by better utilizing network capacity. It also enhances compressed models like DynaBERT. The router and hardness estimation approach are model-agnostic. CPU latency experiments demonstrate over 2x faster inference is possible with negligible accuracy decrease. The method can also complement techniques like token dropping for further gains.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method presented in the paper:

The paper proposes a novel approach called SHARCS (Sample Hardness Aware Routing based on Confidence Scores) for efficient adaptive inference in transformer models. The key idea is to train a lightweight router module that can predict the "hardness" of an input sample and route it to an appropriate sub-network with varying width. Since there are no ground-truth hardness labels, the router is trained using heuristic labels estimated from the model's confidence scores on the training samples over multiple epochs. Specifically, samples predicted within certain confidence bounds are assigned hardness labels. These heuristic hardness labels are used to train the router to predict hardness scores. At inference time, the router routes an input to a sub-network of proportionally reduced width based on the predicted hardness. This allows dynamically adjusting the model capacity and compute for each sample. Experiments show SHARCS can substantially improve efficiency in terms of accuracy vs FLOPs tradeoff compared to prior sample-adaptive inference techniques across various datasets and model architectures.


## What problem or question is the paper addressing?

 The paper is addressing the problem of improving the inference efficiency of transformer-based models like Large Language Models (LLMs) so they can run more efficiently on devices with limited computational resources. Some key points:

- LLMs and other large transformer models require a lot of computational resources, which can be problematic for deployment on resource-constrained devices.

- Prior work has proposed methods to improve efficiency like knowledge distillation, pruning, and quantization, but these produce static compressed models. 

- The paper proposes a sample-adaptive inference method called SHARCS that can dynamically allocate compute for each input based on its estimated "hardness".

- SHARCS introduces a light-weight "router" module that is trained to predict sample hardness and route inputs to sub-networks of varying widths to adjust compute on the fly.

- The router is trained using heuristically generated hardness labels based on the model's confidence scores on each sample during training.

- Experiments show SHARCS outperforms other sample-adaptive methods like early exiting, can further optimize already compressed models, and reduces inference latency.

In summary, the key focus is developing a sample-adaptive inference approach using a trainable routing module to allocate compute dynamically based on input hardness to improve efficiency of transformers for deployment.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Sample adaptive inference - Using varying amounts of compute to process different input samples based on their complexity. The two main approaches are early exiting and token dropping.

- Early exiting - Adding internal classifiers to intermediate layers of a model to exit computation early for simpler samples. Techniques include using confidence scores, entropy, or patience-based methods. 

- Token dropping - Progressively decreasing sequence length layer by layer to reduce compute.

- Hardness aware routing - A new approach proposed in this paper, where a router predicts sample hardness and routes inputs to sub-networks of varying widths.

- Sub-networks - The router routes inputs to different sub-networks of the full model that have varying widths (fraction of full model width).

- Router - A lightweight module trained to predict sample hardness using the model's confidence scores on training data. Routes samples to sub-networks during inference.

- Confidence scores - Used to heuristically estimate sample hardness during training, by comparing predicted probability of ground truth class to set thresholds. 

- Model agnostic - The router and training approach can work with different transformer architectures.

- Efficiency - The main focus is improving efficiency of large transformer models in terms of accuracy vs FLOPs tradeoff and inference latency.
