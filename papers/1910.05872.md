# [Self-supervised Label Augmentation via Input Transformations](https://arxiv.org/abs/1910.05872)

## What is the central research question or hypothesis that this paper addresses?

This paper does not seem to have a clearly stated research question or hypothesis. However, based on my reading, the main focus appears to be introducing a new method for improving classification accuracy on fully supervised learning tasks by utilizing self-supervised learning techniques. Specifically, the paper proposes a "self-supervised label augmentation" (SLA) method which trains a model on both the original labeled data as well as artificial labels generated through input transformations like rotations. This allows the model to learn useful representations without forcing strict invariance to the transformations. The key ideas seem to be:- Learning a joint distribution over original and self-supervised labels rather than separate objectives.- Allowing different predictions on augmented examples via assigned transformation labels. - Aggregating predictions over augmentations at test time for an ensemble effect.- Using self-distillation to transfer aggregated knowledge to a single model.The central hypothesis then seems to be that SLA can improve accuracy over baselines on fully supervised tasks like image classification. The empirical evaluations aim to validate this hypothesis on datasets like CIFAR and FC100.In summary, while not directly stated, the paper appears to introduce and evaluate SLA as a novel way to leverage self-supervision to boost performance on standard supervised learning problems. The main hypothesis seems to be that their proposed techniques will improve accuracy over baselines.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a self-supervised label augmentation (SLA) method that trains a model to jointly predict original and artificially constructed labels. This is done by augmenting the original labels with additional self-supervised labels created by applying transformations (e.g. rotations) to the input data. - Showing that SLA helps improve model accuracy on fully supervised image classification tasks, without needing extra unlabeled data. Experiments demonstrate gains on CIFAR, fine-grained classification, few-shot learning, and long-tailed recognition.- Introducing an aggregation method at test time that averages predictions across different input transformations to improve accuracy. This provides an ensemble-like effect using just a single model.- Proposing a self-distillation technique to transfer the aggregated predictive knowledge into a single model, avoiding the computational expense of aggregating multiple predictions at test time.- Demonstrating that SLA is simple to implement, compatible with other data augmentation methods, and provides significant and consistent accuracy improvements across various classification scenarios and datasets.In summary, the key ideas are to augment standard supervised learning with extra self-supervised labels rather than try to enforce invariance to those transformations, and leverage the multiple predictions at test time via aggregation and self-distillation. The simple approach provides gains across diverse fully supervised classification tasks.


## How does this paper compare to other research in the same field?

This paper presents a method for improving classification accuracy on fully-supervised learning tasks using self-supervised learning techniques. Here are some key points on how it relates to prior work:- Most prior work on self-supervised learning focuses on unsupervised pre-training or semi-supervised learning scenarios. This paper demonstrates benefits of using self-supervision even in fully labeled settings.- Previous approaches like rotation prediction typically use self-supervision as an auxiliary task in a multi-task learning framework. A key novelty here is the joint learning formulation over original and augmented labels, which avoids forcing unnecessary invariances. - The proposed label augmentation through input transformations is simple yet effective. It achieves strong gains over baselines in various experiments including few-shot learning and class imbalance problems.- The idea of aggregating predictions over different augmentations provides an ensemble effect using a single model. Self-distillation transfers this to accelerate inference.- The techniques are compatible with standard data augmentation and architectural choices. Experiments combine SLA with cutout, AutoAugment, MetaOptNet, etc.Overall, this work shows self-supervision can improve fully supervised learning, via a conceptually simple but well motivated modification over prior multi-task approaches. The label augmentation view and aggregation techniques are novel. The broad experimental validation on various datasets, network architectures, and limited supervision settings demonstrate the wide applicability of the ideas.
