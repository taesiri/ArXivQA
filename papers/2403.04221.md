# [Why Online Reinforcement Learning is Causal](https://arxiv.org/abs/2403.04221)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
Reinforcement learning (RL) aims to learn policies that maximize rewards through interacting with an environment. Causal modeling focuses on predicting the causal effects of interventions. This paper examines how and when causal modeling can benefit reinforcement learning. The key question is under what conditions can conditional probabilities estimated from RL experience correctly capture the causal effects needed for decision making. 

Proposed Solution:
The paper leverages the formal semantics of causal models, especially Pearl's do-calculus, to analyze when conditional probabilities coincide with interventional probabilities that represent causal effects. The key factors are:

1) Temporal ordering: Effects follow causes in time. So future rewards temporally follow current states and actions.

2) Confounding: A confounder is a common cause of two variables that can induce a spurious correlation. If there are no confounders between actions and rewards/next states, then conditional probabilities capture causal relationships.

The paper distinguishes two key dimensions of RL settings:

1) Online vs offline learning: In online learning, the agent interacts directly with the environment. In offline learning, the agent relies on a dataset collected by another agent.

2) Complete vs partial observability: The state may include latent variables unobserved by the agent.

Key Results:
- In online RL, there are no unobserved confounders between the agent's actions and subsequent rewards. So conditional probabilities estimate causal effects.

- In offline but completely observable RL, the dataset includes all causes of the behavioral agent's actions. So again, no confounding.  

- Only in offline + partially observable RL do conditional probabilities diverge from causal probabilities.

The paper also analyzes counterfactual queries in RL through structural causal models. A key distinction is between what-if and hindsight counterfactuals. What-if queries can be estimated from conditional probabilities under the same assumptions as causal effects. But hindsight counterfactuals differ even under online learning.

Main Contributions:
- Formal argument why in online (and completely observable offline) RL, conditional probabilities estimate causal effects 

- Analysis of differences between what-if and hindsight counterfactuals

- Discussion of utilizing causal models for model-based RL in different settings: online, offline, off-policy


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key point made in the paper:

In online reinforcement learning settings where the agent learns from its own experience interacting with the environment, conditional probabilities capture causal effects of actions and therefore traditional reinforcement learning techniques suffice for optimal decision making.


## What is the main contribution of this paper?

 The main contribution of this paper is an analysis of when conditional probabilities in reinforcement learning settings can be interpreted as causal probabilities (i.e. they accurately estimate the causal effect of actions). The key conclusions are:

1) In online RL, where the agent learns from interacting with the environment directly, conditional probabilities are equivalent to causal probabilities. This is because the agent's own actions are not confounded - the agent has full observability into the causes of its actions.

2) In offline RL, where the agent learns from a dataset possibly generated by a different behavior policy, actions may be confounded with rewards/transitions. In this case, conditional probabilities may not accurately estimate causal effects. Causal modelling techniques are needed.

3) The paper formally proves the equivalence between conditional and causal probabilities in online RL settings using Pearl's do-calculus. It also discusses how structural causal models can be used to compute interventional and counterfactual probabilities in offline settings.

4) The paper clarifies that in online RL, "what-if" counterfactual queries can be estimated from conditional probabilities, but more complex "hindsight" counterfactuals require causal models even in online settings.

In summary, the main insight is that the key distinction is whether the learning setting is online/interactive vs offline, and whether the observability is the same between the behavioral policy generating data and the learned policy. This analysis provides theoretical guidance on when causal techniques are likely to be beneficial in RL.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Causal modeling
- Reinforcement learning 
- Online learning
- Offline learning
- Observation-equivalence
- Action sufficiency
- Confounding
- Interventional probabilities
- Conditional probabilities
- Counterfactual probabilities
- What-if queries
- Hindsight queries
- Structural causal models
- Dynamic influence models
- Causal graphs
- Causal Bayesian networks

The paper argues that in online reinforcement learning settings where the agent interacts directly with the environment, conditional probabilities capture the causal effects of the agent's actions. This is because the agent's observations include the causes of its own actions, so there are no unobserved confounders. 

In contrast, in offline settings the agent may learn from a dataset generated by another agent's experience. If the environment is only partially observable, there may be unobserved confounders that influence the behavioral agent's actions as well as the resulting states and rewards. So causal modeling concepts like interventional and counterfactual probabilities become relevant.

The concepts of observation-equivalence and action sufficiency are key to formally arguing when conditional probabilities suffice to capture causal relationships in reinforcement learning. Structural causal models provide a framework for computing more advanced causal queries like counterfactuals.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper argues that in online reinforcement learning settings, conditional probabilities capture causal effects of actions. What is the underlying argument that establishes this claim? Explain the concepts of temporal ordering, confounding, action sufficiency and how they relate.

2. The paper makes a distinction between what-if and hindsight counterfactual queries. Explain this distinction and discuss whether both types of counterfactuals can be evaluated from conditional probabilities in online RL or if causal models are needed. 

3. The paper argues observational equivalence implies action sufficiency. Unpack this argument and explain exactly why the ability of the learning agent to observe the causes of the behavior policy's actions ensures there are no confounders. 

4. In the offline setting without complete observability, why may conditional probabilities fail to capture the causal effects of actions? Explain confounding in this context with an example.  

5. The paper discusses adjustment formulas that allow computing causal effects from conditional probabilities using Pearl's do-calculus, even with confounding. Provide an overview of backdoor and frontdoor adjustment and discuss their potential use in offline RL.

6. Structural causal models can evaluate observational, interventional and counterfactual probabilities from a single model. Explain how a SCM works, including concepts like the solution function, truncation for interventions, and abduction for counterfactuals.

7. Provide a detailed walkthrough of how observational, interventional and counterfactual reward probabilities are computed in the sports example, explaining the steps of abduction, intervention and prediction. 

8. The paper argues hindsight counterfactuals differ from conditional probabilities even under action sufficiency. Construct an example that illustrates this phenomenon and explain the insight it provides.  

9. What are some of the benefits of using causal models over traditional RL models discussed? Explain concepts like irrelevant variable elimination, stability across distribution shift, and expressiveness for counterfactuals.

10. The paper suggests causal behavioral cloning as an alternative to standard behavioral cloning that avoids conditioning on effects of actions. Explain this idea and why matching conditional probabilities can fail even under observation equivalence.
