# [PartIR: Composing SPMD Partitioning Strategies for Machine Learning](https://arxiv.org/abs/2401.11202)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Training large neural networks (NNs) requires combining multiple parallelization strategies like data, model, and optimizer sharding across accelerators. As strategies get more complex, tools need to be (1) expressive to compose strategies, and (2) predictable to estimate performance analytically. Existing tools couple model code and partitioning, making strategies hard to change. They also lack support for incremental composition and feedback.

Proposed Solution:
The paper presents Partir, a NN partitioning system focused on an incremental rewrite approach that is hardware and runtime agnostic. Key aspects:

- Simple but powerful API to compose sharding strategies as compiler tactics in a schedule. Tactics can be manual or invoke automatic partitioning.

- Separates model code from partitioning information, making strategies easy to change.

- Tactics translate to rewrite actions tilting or replicating values, or propagating decisions. Actions have a formal semantic interpretation.

- Per-operation Tile Mapping Registry encodes tensor algebra knowledge to drive a generic propagation pass.

- Lowers actions to device-local collectives and provides analytical performance simulation.

Main Contributions:

- Architecture separating concerns into different MLIR dialects: 
    - Partir:Core with tile/reduce loops and slices.
    - Partir:HLO with device-local collectives.
    - Formal proof of correct SPMD lowering.

- Powerful propagation pass built around the Tile Mapping Registry encoding linear algebra homomorphisms.

- Evaluation of predictability, debuggability, composability, performance, and overhead.

The system focuses on incrementality for predictable propagation without heuristics. It reduces manual decisions through conflict-free prioritized propagation. Partir is used to scale multiple research models.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

PartiR is a neural network partitioning system focused on an incremental approach to rewriting that enables composable sharding strategies across data, model, and optimizer state while remaining hardware- and runtime-agnostic through the use of an intermediate representation.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. The overall design of a partitioner system that allows users to compose SPMD sharding strategies incrementally via a schedule of tactics. The system decouples the model from its partitioning to make it easy to step-by-step debug and compose manual or automated sharding decisions.

2. A compiler stack that exposes tiling and replication actions as a compiler API to be targeted by high-level manual tactics or automated tools. Key dialects introduced are:

(i) \partir:Core, which introduces functional tiling and reduction loops on top of array IRs that can be given sequential or parallel semantics. 

(ii) \partir:HLO, which offers a device-local view of the SPMD computation and high-level collective communication primitives.

3. A pass to propagate partitioning decisions across a module, based on linear algebra homomorphisms. Propagation does not rely on cost-based heuristics and is implemented as simple rewrites at the \partir:Core level.

4. An evaluation of \partir{} for predictability, debugability, composability, performance of partitioned code, value of incrementality, and compile-time overhead.

In summary, the main contribution is the design and implementation of the \partir{} system for incrementally composing and debugging SPMD partitioning strategies in a predictable and performant manner.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Partitioning strategies - The paper discusses different strategies like batch parallelism, model parallelism, optimizer sharding, etc. for partitioning neural network training across multiple devices.

- Composability - The ability to combine and compose different partitioning strategies, both manual and automated, is a key focus of the Partir system.

- Incrementality - Partir takes an incremental approach to partitioning where strategies are applied as a series of rewriting tactics. This avoids undoing previous partitioning decisions. 

- Predictability - Partir aims to provide feedback like cost estimates and expected communication collectives after each partitioning tactic, making the process predictable for users.

- Propagation - Automatically propagating partitioning decisions across a module based on linear algebra homomorphisms, without relying on cost heuristics.

- Partir:Core - One of the key IR dialects introduced, which expresses partitioning through functional loops and slicing operators.

- Communication collectives - Explicit communication ops like all-reduce and reduce-scatter introduced at the Partir:HLO IR level after lowering from Partir:Core.

- Simulation - Partir provides a way to simulate partitioned models on target devices to estimate performance and memory usage.

So in summary - composability, incrementality, predictability, propagation, and dedicated IRs to explicitly represent partitioning and communication are some of the core concepts.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does PartIR's separation of partitioning strategies from model code through tactics and schedules facilitate changing strategies when system configurations change? What are the tradeoffs compared to baking strategies directly into the model code?

2. PartIR claims to be hardware and runtime agnostic by operating on the StableHLO dialect. What are the advantages and limitations of this approach compared to targeting specific hardware backends directly? 

3. The paper claims PartIR makes partitioning an "easier task" through incrementality and predictability. In what ways does this reduce the burden on users and how could it be improved further?

4. PartIR relies heavily on propagation of partitioning decisions based on linear algebra homomorphisms and a tile mapping registry (TMR). What are the limitations of this approach and what types of conflicts can still arise that require manual resolution? 

5. How does PartIR balance manual and automated partitioning tactics? What kinds of models or scenarios warrant a mix of tactics versus fully automated search?

6. The formal proof of correctness for lowering PartIR:Core to SPMD code is mentioned but details are omitted. What would such a proof entail and what value does it provide?

7. PartIR uses a simple analytical simulator for cost estimation and to drive automatic search. What are the tradeoffs versus more sophisticated learned cost models? How could the simulator be improved?

8. Support for spatial partitioning of convolutional neural networks seems limited in PartIR currently. What complications arise and how could it be enhanced to handle more complex cases?

9. The reshape operator is called out as a specific challenge for propagation in PartIR. Why does this operation cause issues and how are conflicts resolved currently? What approaches could make it more robust?

10. PartIR takes an incremental rewrite-based approach rather than propagating annotations as in some prior systems. What are the relative advantages and disadvantages of these two high-level approaches? Are there opportunities for hybridization?
