# [OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for   Video-Grounded Dialog](https://arxiv.org/abs/2402.13146)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Video dialog is an extremely challenging task at the intersection of computer vision and natural language processing. Existing models for this task struggle with:
1) Questions requiring joint spatial and temporal localization within videos
2) Long-term temporal reasoning across multiple dialog turns  
3) Accurately tracking objects across dialog turns
Previous models have only been evaluated on datasets that do not properly assess higher-order reasoning capabilities.

Proposed Solution:
The paper proposes Object Language Video Transformer (\olvit) - a novel video dialog model with two key components:

1) Object State Tracker (OST): Attends to most relevant objects across video frames to answer the current question.

2) Language State Tracker (LST): Tracks most important linguistic co-references to previous dialog turns for more efficient co-reference resolution. 

After each dialog turn, the OST and LST output continuous object and language state vectors that update a global dialog state maintained throughout the dialog. These state vectors are integrated in different ways into a transformer-based model that can operate in both discriminative (select answer from candidates) and generative (generate answer tokens auto-regressively) modes.

Main Contributions:

1) Introduction of \olvit with novel object and language state trackers for video dialog

2) State trackers output continuous representations of most relevant objects and dialog history to update global dialog state  

3) Evaluation on DVD (discriminative) and SIMMC 2.1 (generative) shows state-of-the-art performance:

    - 3.75% higher test accuracy on DVD classification task
    
    - 30.4% better BLEU-4 score on SIMMC 2.1 generation task

The results demonstrate \olvit's ability to perform better joint spatial-temporal localization, long-term reasoning, and object tracking compared to previous state-of-the-art video dialog models.
