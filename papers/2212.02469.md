# [One-shot Implicit Animatable Avatars with Model-based Priors](https://arxiv.org/abs/2212.02469)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the challenge of creating realistic animatable 3D human avatars from readily available camera inputs like a single image. The central hypothesis is that by leveraging model-based priors like a 3D body shape geometry prior and a visual clothing semantic prior, it is possible to train an animatable neural radiance field (NeRF) representation from just a single image as input. Specifically, the two key hypotheses are:1) A 3D body shape geometry prior from SMPL can provide constraints on body shape and pose to guide the model when geometry information is missing due to occlusions. 2) A visual semantic prior from a pre-trained vision model like CLIP can hallucinate plausible details for occluded regions by enforcing semantic consistency between views.The overall goal is to show that with these model-based priors, the proposed method called ELICIT can create realistic animatable avatars from single images without relying on large-scale training data like many existing methods. The experiments aim to demonstrate that ELICIT outperforms state-of-the-art methods on tasks like novel view synthesis and novel pose synthesis when only a single image is available as input.In summary, the paper aims to tackle the challenging problem of data-efficient avatar creation from monocular images by utilizing model-based priors to guide a one-shot learning process for optimizing an animatable implicit human representation. The key hypothesis is that these priors can compensate for the missing information in sparse single-image input and enable generating realistic free-view renderings.
