# [One-shot Implicit Animatable Avatars with Model-based Priors](https://arxiv.org/abs/2212.02469)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the challenge of creating realistic animatable 3D human avatars from readily available camera inputs like a single image. The central hypothesis is that by leveraging model-based priors like a 3D body shape geometry prior and a visual clothing semantic prior, it is possible to train an animatable neural radiance field (NeRF) representation from just a single image as input. Specifically, the two key hypotheses are:

1) A 3D body shape geometry prior from SMPL can provide constraints on body shape and pose to guide the model when geometry information is missing due to occlusions. 

2) A visual semantic prior from a pre-trained vision model like CLIP can hallucinate plausible details for occluded regions by enforcing semantic consistency between views.

The overall goal is to show that with these model-based priors, the proposed method called ELICIT can create realistic animatable avatars from single images without relying on large-scale training data like many existing methods. The experiments aim to demonstrate that ELICIT outperforms state-of-the-art methods on tasks like novel view synthesis and novel pose synthesis when only a single image is available as input.

In summary, the paper aims to tackle the challenging problem of data-efficient avatar creation from monocular images by utilizing model-based priors to guide a one-shot learning process for optimizing an animatable implicit human representation. The key hypothesis is that these priors can compensate for the missing information in sparse single-image input and enable generating realistic free-view renderings.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing ELICIT, a novel method to learn an animatable neural radiance field from a single image without relying on extra training data. 

2. Using two effective model-based priors to achieve free-view rendering of a 3D avatar from a single image:

- A visual model-based semantic prior that leverages CLIP models to hallucinate unseen parts of the clothed body.

- A human shape prior from SMPL to provide geometric clues and constrain body poses.

3. A hybrid sampling strategy during training that includes:

- Body-part aware sampling to refine details of different body parts. 

- Rotation-aware sampling to better recover heavily occluded views.

4. Demonstrating superior performance to recent human-specific neural rendering baselines on the tasks of novel view synthesis and novel pose synthesis using only a single image input.

5. Showing strong qualitative results on in-the-wild human images, enabled by the model-based priors rather than reliance on training data.

In summary, the key contribution is presenting a way to create an animatable free-viewpoint avatar from just a single image, using model-based priors to avoid the need for subject-specific training data. The proposed method ELICIT outperforms previous state-of-the-art methods in the single image setting.
