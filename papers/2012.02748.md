# [Challenging common interpretability assumptions in feature attribution   explanations](https://arxiv.org/abs/2012.02748)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether common assumptions made about the interpretability and utility of feature attribution explanations actually hold true. Specifically, the authors test the following assumptions:1. Simpler models are more interpretable than complex models. 2. Model-agnostic explanation methods work equally well across different data, tasks, and users.3. Any explanation is better than no explanation in helping a human decision maker.To test these assumptions, the authors conduct a large-scale human subjects experiment where they have participants judge which of two machine learning models will be more accurate based on feature attribution explanations. They compare different explanation methods (such as SHAP and linear models) across different data sets. Their key finding is that the assumptions do not universally hold - for example, simpler models are not always more interpretable, and explanations can sometimes result in worse decisions compared to no explanation. The central hypothesis is that these common interpretability assumptions need to be empirically evaluated rather than taken as ground truth.


## What is the main contribution of this paper?

The main contribution of this paper is an empirical evaluation of some common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. Specifically, the authors test the following assumptions:- Simpler models are more interpretable than complex models.- Model-agnostic explanation methods are generalizable across data, tasks, and users.  - Any explanation is better than no explanation.To test these assumptions, the authors conduct an experiment on Amazon Mechanical Turk where participants have to determine which of two machine learning models will perform better in the real world, based on model explanations. The key findings are:- The simplest model (ridge regression) does not consistently perform the best across contexts, challenging the assumption that simpler models are always more interpretable.- There is significant heterogeneity in interpretability across different data instances and users, contrary to the assumption that explanations are agnostic. - Explanations do not universally improve decisions compared to no explanation at all. In some cases, the "placebo" random explanation performs just as well as real explanations.Overall, the paper provides empirical evidence that calls into question some common axiomatic assumptions in XAI research, highlighting the need for rigorous human-centered evaluation. The large-scale experiment methodology demonstrates how the field can move beyond relying on proxy metrics and instead directly test the real-world benefits of explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper challenges common assumptions in explainable AI research by conducting a large-scale human subjects experiment. The key finding is that feature attribution explanations provide marginal utility for human decision makers and can even result in worse decisions due to cognitive and contextual confounders. The paper underscores the importance of human evaluation in XAI research.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on explainable AI (XAI):- Uses a human subjects experiment to directly evaluate XAI methods, rather than just proposing a new method. Most XAI papers focus on new methods without empirical evaluation. - Compares multiple explanation methods (ridge, lasso, SHAP) to a placebo control. This allows the authors to directly test if explanations provide utility beyond just model outputs.- Models subject ability and task difficulty using a psychometric approach (item response theory) rather than just overall accuracy. This accounts for heterogeneity in subjects and tasks. - Finds different explanation methods are better in different contexts, challenging the notion that simpler models are universally more interpretable.- Highlights the lack of standardized evaluation practices and replicability issues in XAI research. Calls for more rigorous empirical methodology.Overall, this paper stands out for its rigorous experimental methodology and focus on directly testing core XAI assumptions. It represents an important empirical contribution and methodological advance for the field. The use of a placebo control and modeling of individual differences are notable innovations compared to prior XAI evaluations.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Conducting more rigorous empirical evaluations of XAI systems with human subjects experiments, rather than relying solely on proxy metrics of interpretability. The authors argue that human evaluation is critical for understanding how explanations actually affect end users.- Exploring individual differences in interpretability, rather than assuming all users have the same needs. The authors' analysis shows significant person-level variation in the ability to interpret models.- Considering the effects of data and task heterogeneity when evaluating explanations. The analysis indicates the data instances being explained can significantly impact interpretability.- Developing better psychometric measurement models of interpretability that go beyond simple accuracy metrics. The authors propose item response theory models as one promising approach.- Focusing more on the effects of real-world contextual factors and potential cognitive biases when deploying explanation systems. The placebo explanation result highlights the need to account for confounding variables.- Conducting large-scale replication studies to better understand the generalizability of existing XAI evaluation findings. The authors were unable to replicate some prior results.In summary, the authors argue for more rigorous, context-aware empirical evaluation of XAI systems with actual end users rather than reliance on simplistic assumptions or proxy metrics. They suggest several methodological improvements to work toward this goal.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper challenges three common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. The authors test whether simpler models are more interpretable, whether model-agnostic methods generalize across tasks/users, and whether any explanation is better than no explanation. They find that feature attribution explanations provide only marginal utility for a human decision maker on their pricing task, and can even result in worse decisions due to cognitive/contextual confounders. Their results highlight the need to empirically evaluate XAI methods with human subjects rather than relying on proxy metrics of interpretability. The paper underscores the importance of considering individual differences and data/task dependencies when designing and evaluating explanation methods. Overall, it argues for a critical, empirical approach to XAI research focused on realistic human evaluation rather than mathematical notions of interpretability.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper empirically evaluates common assumptions in explainable AI (XAI) through a large-scale human subjects experiment. The authors focus on three main assumptions: 1) simpler models are more interpretable, 2) model-agnostic methods are user/data/task agnostic, and 3) any explanation is better than no explanation. To test these assumptions, they designed an experiment where participants were shown explanations from different models predicting Airbnb rental prices. Participants had to determine which model would be more accurate based on the explanation. The results challenged the assumed universal benefit of XAI methods. The simplest model (ridge regression) was not always the most interpretable, especially for sparse data. There was significant heterogeneity in interpretability across users and data instances. Explanations did not universally lead to better decisions and in some cases resulted in worse decisions due to cognitive biases and contextual factors. The authors conclude that XAI systems should not be assumed to provide universal benefit without empirical evaluation. They emphasize the need for human-centered evaluation in XAI research rather than relying solely on proxy metrics of interpretability.


## Summarize the main method used in the paper in one paragraph.

The paper conducted a mixed between/within-subjects repeated measures experiment on Amazon Mechanical Turk with 796 participants to empirically evaluate three common interpretability assumptions. The experiment presented regression models predicting Airbnb listing prices and compared explanations from a post-hoc feature attribution method to "inherently interpretable" models (ridge and lasso regression) and a placebo random explanation. Subjects were asked to determine which of two models would be more accurate based on the explanations. The factors varied were the explanation method, data sparsity, number of features shown, and the data instance explained. To estimate the effects of these factors on subjects' ability to identify the more accurate model, the authors fit a Bayesian multilevel logistic regression model with person and item parameters. They found that simpler models were not universally more interpretable, explanations did not necessarily improve decisions, and there was substantial heterogeneity in model interpretability due to individual differences and data instances. Overall, the large-scale experiment with a placebo control challenges common assumptions in explainable AI research.
