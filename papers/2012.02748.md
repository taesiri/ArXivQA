# [Challenging common interpretability assumptions in feature attribution   explanations](https://arxiv.org/abs/2012.02748)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether common assumptions made about the interpretability and utility of feature attribution explanations actually hold true. Specifically, the authors test the following assumptions:

1. Simpler models are more interpretable than complex models. 

2. Model-agnostic explanation methods work equally well across different data, tasks, and users.

3. Any explanation is better than no explanation in helping a human decision maker.

To test these assumptions, the authors conduct a large-scale human subjects experiment where they have participants judge which of two machine learning models will be more accurate based on feature attribution explanations. They compare different explanation methods (such as SHAP and linear models) across different data sets. Their key finding is that the assumptions do not universally hold - for example, simpler models are not always more interpretable, and explanations can sometimes result in worse decisions compared to no explanation. The central hypothesis is that these common interpretability assumptions need to be empirically evaluated rather than taken as ground truth.


## What is the main contribution of this paper?

 The main contribution of this paper is an empirical evaluation of some common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. Specifically, the authors test the following assumptions:

- Simpler models are more interpretable than complex models.
- Model-agnostic explanation methods are generalizable across data, tasks, and users.  
- Any explanation is better than no explanation.

To test these assumptions, the authors conduct an experiment on Amazon Mechanical Turk where participants have to determine which of two machine learning models will perform better in the real world, based on model explanations. The key findings are:

- The simplest model (ridge regression) does not consistently perform the best across contexts, challenging the assumption that simpler models are always more interpretable.

- There is significant heterogeneity in interpretability across different data instances and users, contrary to the assumption that explanations are agnostic. 

- Explanations do not universally improve decisions compared to no explanation at all. In some cases, the "placebo" random explanation performs just as well as real explanations.

Overall, the paper provides empirical evidence that calls into question some common axiomatic assumptions in XAI research, highlighting the need for rigorous human-centered evaluation. The large-scale experiment methodology demonstrates how the field can move beyond relying on proxy metrics and instead directly test the real-world benefits of explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper challenges common assumptions in explainable AI research by conducting a large-scale human subjects experiment. The key finding is that feature attribution explanations provide marginal utility for human decision makers and can even result in worse decisions due to cognitive and contextual confounders. The paper underscores the importance of human evaluation in XAI research.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on explainable AI (XAI):

- Uses a human subjects experiment to directly evaluate XAI methods, rather than just proposing a new method. Most XAI papers focus on new methods without empirical evaluation. 

- Compares multiple explanation methods (ridge, lasso, SHAP) to a placebo control. This allows the authors to directly test if explanations provide utility beyond just model outputs.

- Models subject ability and task difficulty using a psychometric approach (item response theory) rather than just overall accuracy. This accounts for heterogeneity in subjects and tasks. 

- Finds different explanation methods are better in different contexts, challenging the notion that simpler models are universally more interpretable.

- Highlights the lack of standardized evaluation practices and replicability issues in XAI research. Calls for more rigorous empirical methodology.

Overall, this paper stands out for its rigorous experimental methodology and focus on directly testing core XAI assumptions. It represents an important empirical contribution and methodological advance for the field. The use of a placebo control and modeling of individual differences are notable innovations compared to prior XAI evaluations.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Conducting more rigorous empirical evaluations of XAI systems with human subjects experiments, rather than relying solely on proxy metrics of interpretability. The authors argue that human evaluation is critical for understanding how explanations actually affect end users.

- Exploring individual differences in interpretability, rather than assuming all users have the same needs. The authors' analysis shows significant person-level variation in the ability to interpret models.

- Considering the effects of data and task heterogeneity when evaluating explanations. The analysis indicates the data instances being explained can significantly impact interpretability.

- Developing better psychometric measurement models of interpretability that go beyond simple accuracy metrics. The authors propose item response theory models as one promising approach.

- Focusing more on the effects of real-world contextual factors and potential cognitive biases when deploying explanation systems. The placebo explanation result highlights the need to account for confounding variables.

- Conducting large-scale replication studies to better understand the generalizability of existing XAI evaluation findings. The authors were unable to replicate some prior results.

In summary, the authors argue for more rigorous, context-aware empirical evaluation of XAI systems with actual end users rather than reliance on simplistic assumptions or proxy metrics. They suggest several methodological improvements to work toward this goal.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper challenges three common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. The authors test whether simpler models are more interpretable, whether model-agnostic methods generalize across tasks/users, and whether any explanation is better than no explanation. They find that feature attribution explanations provide only marginal utility for a human decision maker on their pricing task, and can even result in worse decisions due to cognitive/contextual confounders. Their results highlight the need to empirically evaluate XAI methods with human subjects rather than relying on proxy metrics of interpretability. The paper underscores the importance of considering individual differences and data/task dependencies when designing and evaluating explanation methods. Overall, it argues for a critical, empirical approach to XAI research focused on realistic human evaluation rather than mathematical notions of interpretability.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper empirically evaluates common assumptions in explainable AI (XAI) through a large-scale human subjects experiment. The authors focus on three main assumptions: 1) simpler models are more interpretable, 2) model-agnostic methods are user/data/task agnostic, and 3) any explanation is better than no explanation. To test these assumptions, they designed an experiment where participants were shown explanations from different models predicting Airbnb rental prices. Participants had to determine which model would be more accurate based on the explanation. 

The results challenged the assumed universal benefit of XAI methods. The simplest model (ridge regression) was not always the most interpretable, especially for sparse data. There was significant heterogeneity in interpretability across users and data instances. Explanations did not universally lead to better decisions and in some cases resulted in worse decisions due to cognitive biases and contextual factors. The authors conclude that XAI systems should not be assumed to provide universal benefit without empirical evaluation. They emphasize the need for human-centered evaluation in XAI research rather than relying solely on proxy metrics of interpretability.


## Summarize the main method used in the paper in one paragraph.

 The paper conducted a mixed between/within-subjects repeated measures experiment on Amazon Mechanical Turk with 796 participants to empirically evaluate three common interpretability assumptions. The experiment presented regression models predicting Airbnb listing prices and compared explanations from a post-hoc feature attribution method to "inherently interpretable" models (ridge and lasso regression) and a placebo random explanation. Subjects were asked to determine which of two models would be more accurate based on the explanations. The factors varied were the explanation method, data sparsity, number of features shown, and the data instance explained. To estimate the effects of these factors on subjects' ability to identify the more accurate model, the authors fit a Bayesian multilevel logistic regression model with person and item parameters. They found that simpler models were not universally more interpretable, explanations did not necessarily improve decisions, and there was substantial heterogeneity in model interpretability due to individual differences and data instances. Overall, the large-scale experiment with a placebo control challenges common assumptions in explainable AI research.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the key problem it is trying to address is evaluating some common assumptions made in explainable AI (XAI) research regarding the benefits and effectiveness of feature attribution explanations. Specifically, it challenges the following assumptions:

1. Simpler models are inherently more interpretable. The paper tests if simpler model explanations like linear regression actually help people identify a more accurate model compared to a more complex model explanation.

2. Model-agnostic explanation methods work well across different data, tasks, and users. The paper examines whether a model-agnostic explainer performs consistently better than simple model explanations across different data densities. 

3. Any explanation is better than no explanation. The paper checks if feature attribution explanations actually help or potentially harm decisions compared to providing no explanation.

To evaluate these assumptions, the authors conduct a large-scale human subjects experiment to directly test if feature attribution explanations improve people's ability to identify the more accurate machine learning model out of a pair. They compare different explanation methods as well as introduce factors like data density to see if the effectiveness of explanations varies across conditions. The main goal is to empirically determine if common XAI assumptions hold up to rigorous testing rather than just being proclaimed true without evidence. The paper aims to underscore the importance of human evaluation in XAI research.


## What are the keywords or key terms associated with this paper?

 Based on reading the abstract and skimming the paper, some key terms and concepts that seem most relevant are:

- Explainable AI (XAI) - The paper discusses assumptions made in explainable AI systems and evaluates their interpretability. 

- Feature attribution explanations - The paper focuses specifically on evaluating feature attribution methods as a type of explainable AI.

- Interpretability evaluation - A main goal of the paper is to empirically evaluate common assumptions about interpretability in XAI systems through human subjects experiments. 

- Cognitive/contextual confounders - The paper finds that feature attribution explanations provide limited utility due to cognitive and contextual factors that affect human interpretation.

- Replicability - The paper aims to replicate and evaluate the generalizability of findings from previous XAI experiments. 

- Psychometric modeling - The paper models subject responses using techniques from psychometrics to estimate a latent "ability to interpret".

- Heterogeneity - Key findings are that both data instances and human subjects show significant heterogeneity in how explanations are interpreted.

In summary, the key focus seems to be critically evaluating assumptions in XAI, especially feature attribution methods, using rigorous human subjects experiments and psychometric modeling. The main conclusions challenge assumed benefits of explanations and highlight issues with replicability and individual differences.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key points of this paper:

1. What are the 3 common interpretability assumptions that the authors challenge in this paper?

2. How did the authors design their experiment to test these assumptions? What factors and levels did they use?

3. What was the objective or goal of the experiment? How did they quantify success? 

4. What were the main findings from the experiment related to the "simpler models are more interpretable" assumption?

5. How did the authors model individual differences in interpretability using their psychometric model? 

6. What did the estimated parameters related to the explainer and sparsity factors show? How did this relate to prior work?

7. What are some limitations of this study that the authors acknowledge?

8. What suggestions do the authors make for future work on evaluating interpretability?

9. How does this work underscore the importance of human evaluation in XAI research?

10. What are the key takeaways from this paper in terms of best practices for empirically evaluating interpretability?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper compares different explanation methods like ridge regression, lasso regression, SHAP values, and random feature importance. How do the mathematical formulations and optimization objectives differ between these methods? How might these differences impact the quality and interpretability of explanations they generate?

2. The paper uses a psychometric model based on item response theory (IRT) to estimate subjects' latent "ability to interpret" explanations. How does this approach account for differences in subjects' prior knowledge and experience? What are the advantages of modeling interpretability as a latent trait versus using a direct self-reported measure?

3. The experiment varies the number of features shown in the explanation from 1 to 19. How might the cognitive load imposed by the explanation interface design impact subjects' ability to effectively utilize explanations with more features? Could an alternative visualization approach help overcome this?

4. How suitable are the black-box machine learning models used in this study (ridge/lasso regression) for evaluating explanation methods? Would results differ with more complex models like random forests or neural networks that have intrinsically lower interpretability?

5. The paper finds significant heterogeneity in how effective explanations are across different data examples. What characteristics of data instances might impact how readily they can be interpreted by humans? How could this inform the design of explanation systems?

6. What are possible psychological or cognitive mechanisms that could explain worse performance when explanations are provided versus no explanation? Under what conditions might explanations actually impair human judgment?

7. The paper focuses on evaluating explanations for regression tasks. How might the results differ for interpreting explanations for classification models? Are there different measures of explanation quality that would be more appropriate?

8. How robust is the overall evaluation approach to gaming or adversarial attacks? Could subjects learn to "game" the experiment over many trials to appear more competent at identifying the better model?

9. The experiment uses a between-subjects design for the main experimental factors. What are the tradeoffs between this approach versus a within-subjects design? How could a within-subjects study complement these findings?

10. The paper focuses on evaluating feature attribution explanations. How might the results differ for example-based explanations like influence functions or prototype selections? What comparative evaluations would help identify the most effective explanation types?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper challenges common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. The authors evaluate three assumptions: 1) simpler models are more interpretable, 2) model-agnostic methods generalize across contexts, and 3) any explanation is better than no explanation. They conduct an experiment on Mechanical Turk where 796 participants are shown explanations of machine learning models predicting Airbnb prices. The key result is that feature attribution explanations provide only marginal utility compared to a placebo, and can actually cause worse decisions due to cognitive biases. Overall, the paper argues XAI methods should not be assumed as universally beneficial without empirical evaluation. The experiment methodology demonstrates how to properly evaluate XAI systems while avoiding issues like relying on proxy metrics. The authors hope this work will encourage critical thinking and human-centered evaluation in XAI research.


## Summarize the paper in one sentence.

 The paper empirically evaluates common assumptions about model interpretability through a large-scale human subjects experiment and finds that feature attribution explanations provide only marginal utility for a human decision maker and in some cases result in worse decisions.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper empirically evaluates three common assumptions in explainable AI through a large-scale human subjects experiment. The authors compare different explanation methods (like SHAP and simpler linear models) in a realistic task of predicting Airbnb prices. They find that simpler models are not necessarily more interpretable, explanation methods are sensitive to factors like data sparsity, and explanations do not universally improve decisions - in some cases worsening performance. The paper challenges the assumed benefits of explanation methods and underscores the importance of rigorous human evaluation. Key results include heterogeneous effects of explainers based on context, large individual differences in interpretability, and the inadequacy of using model simplicity as a proxy for interpretability. Overall, the paper provides empirical evidence that many common assumptions in XAI research do not hold, and evaluators should beware of generalizing results across different tasks, users, and data contexts when assessing explanation methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper claims that post-hoc feature attribution explanations provide marginal utility for the Airbnb price prediction task. How robust is this result? Does it generalize to other prediction tasks and domains? Are there certain conditions where feature attributions would provide more utility?

2. The paper uses a "placebo explanation" random control. What are other possible control conditions that could have been used? Would an ablation study or a simpler explainable model like a linear regression serve as a better baseline? 

3. The paper finds that the effectiveness of explainers depends on contextual factors like data sparsity. How can explainers be made more robust to these factors? Can meta-learning or multi-task learning approaches help make explainers generalize better across contexts?

4. The paper proposes using pairwise comparisons and a psychometric model to evaluate explanations. What are the limitations of this approach compared to having users directly simulate or predict using the explanations? When would direct prediction be a better experimental design?

5. The paper argues explanations could lead to worse decisions due to cognitive biases. How prevalent is this effect likely to be in real-world settings? What user studies could be done to estimate the risk and magnitude of harmful effects from explanations?

6. The paper identifies individual differences in the ability to interpret explanations. How can explainers be adapted to different users' abilities, backgrounds, and contexts? What personalization approaches could make explanations more useful?

7. The paper finds differences in interpretability between sparse and dense datasets. What other data properties affect interpretability? How can datasets be characterized to select the most interpretable explainer or explanation for a given dataset?

8. The paper uses Gaussian Process regression to estimate accuracy differences between models. What risks does this introduce in terms of experimenter degrees of freedom? How sensitive are the results to the particular GP hyperparameters and training procedures used?

9. The paper focuses on evaluating feature attribution methods. How well would its conclusions generalize to other explanation types like examples, counterfactuals or textual rationales? What experiments could test the effectiveness of other explainers?

10. The paper examines explanation methods in isolation. How would the results change if explanations were provided within a full interactive machine learning system? What other human factors need to be considered in designing such systems?
