# Challenging common interpretability assumptions in feature attribution   explanations

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether common assumptions made about the interpretability and utility of feature attribution explanations actually hold true. Specifically, the authors test the following assumptions:1. Simpler models are more interpretable than complex models. 2. Model-agnostic explanation methods work equally well across different data, tasks, and users.3. Any explanation is better than no explanation in helping a human decision maker.To test these assumptions, the authors conduct a large-scale human subjects experiment where they have participants judge which of two machine learning models will be more accurate based on feature attribution explanations. They compare different explanation methods (such as SHAP and linear models) across different data sets. Their key finding is that the assumptions do not universally hold - for example, simpler models are not always more interpretable, and explanations can sometimes result in worse decisions compared to no explanation. The central hypothesis is that these common interpretability assumptions need to be empirically evaluated rather than taken as ground truth.


## What is the main contribution of this paper?

The main contribution of this paper is an empirical evaluation of some common assumptions in explainable AI (XAI) research through a large-scale human subjects experiment. Specifically, the authors test the following assumptions:- Simpler models are more interpretable than complex models.- Model-agnostic explanation methods are generalizable across data, tasks, and users.  - Any explanation is better than no explanation.To test these assumptions, the authors conduct an experiment on Amazon Mechanical Turk where participants have to determine which of two machine learning models will perform better in the real world, based on model explanations. The key findings are:- The simplest model (ridge regression) does not consistently perform the best across contexts, challenging the assumption that simpler models are always more interpretable.- There is significant heterogeneity in interpretability across different data instances and users, contrary to the assumption that explanations are agnostic. - Explanations do not universally improve decisions compared to no explanation at all. In some cases, the "placebo" random explanation performs just as well as real explanations.Overall, the paper provides empirical evidence that calls into question some common axiomatic assumptions in XAI research, highlighting the need for rigorous human-centered evaluation. The large-scale experiment methodology demonstrates how the field can move beyond relying on proxy metrics and instead directly test the real-world benefits of explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper challenges common assumptions in explainable AI research by conducting a large-scale human subjects experiment. The key finding is that feature attribution explanations provide marginal utility for human decision makers and can even result in worse decisions due to cognitive and contextual confounders. The paper underscores the importance of human evaluation in XAI research.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on explainable AI (XAI):- Uses a human subjects experiment to directly evaluate XAI methods, rather than just proposing a new method. Most XAI papers focus on new methods without empirical evaluation. - Compares multiple explanation methods (ridge, lasso, SHAP) to a placebo control. This allows the authors to directly test if explanations provide utility beyond just model outputs.- Models subject ability and task difficulty using a psychometric approach (item response theory) rather than just overall accuracy. This accounts for heterogeneity in subjects and tasks. - Finds different explanation methods are better in different contexts, challenging the notion that simpler models are universally more interpretable.- Highlights the lack of standardized evaluation practices and replicability issues in XAI research. Calls for more rigorous empirical methodology.Overall, this paper stands out for its rigorous experimental methodology and focus on directly testing core XAI assumptions. It represents an important empirical contribution and methodological advance for the field. The use of a placebo control and modeling of individual differences are notable innovations compared to prior XAI evaluations.
