# [SparseProp: Efficient Event-Based Simulation and Training of Sparse   Recurrent Spiking Neural Networks](https://arxiv.org/abs/2312.17216)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Spiking neural networks (SNNs) are more biologically plausible models of neural processing compared to rate-based networks. However, simulating and training SNNs is computationally expensive due to the need to solve coupled differential equations. 
- Conventional event-based SNN simulations have a computational complexity of O(N) per spike, making large-scale networks intractable.

Proposed Solution:
- The paper introduces SparseProp, a novel event-based algorithm to simulate sparse SNNs. 
- By exploiting network sparsity and representing neuron states as time to next spikes on a binary heap data structure, SparseProp reduces the computational complexity from O(N) to O(log(N)) per spike.

Main Contributions:
- SparseProp allows efficient state updates instead of iterating through all neurons at each spike time. This is achieved via a change of variable into a co-moving reference frame.
- Implementations are provided for networks of common spiking neurons models like LIF, QIF, EIF.
- For models lacking analytical spike time solutions, lookup tables based on numerical integration or Chebyshev polynomial fits are used.
- SparseProp is extended to heterogeneous networks and event-based training of SNNs.
- Computational cost scaling analyses show Linearithmic scaling of total cost with network size for SparseProp versus Quadratic scaling for conventional methods.
- Numerical experiments demonstrate over four orders of magnitude speedup in simulating a 1 million neuron SNN compared to previous approaches.

In summary, SparseProp enables efficient and numerically exact simulations and training of large-scale SNNs. This can facilitate the advancement of more sophisticated brain models in neuroscience and exploration of event-based SNNs in machine learning.
