# [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)

## What is the central research question or hypothesis that this paper addresses?

This paper does not have a single clear research question or hypothesis. Instead, it is a position paper that aims to critically examine the concept of "model interpretability" in machine learning. The key points made in the paper are:- The term "interpretability" is vague and poorly defined. Different researchers refer to different concepts when using this term.- Common claims like "linear models are interpretable while neural nets are not" are overly simplistic. The truth depends on how interpretability is defined.- There are diverse motivations for wanting interpretability, including trust, causality, transferability, informativeness, and fairness. These suggest interpretability serves objectives not captured by standard evaluation metrics.- Interpretability could refer to transparency (understanding the model internals) or post-hoc explanations (providing additional information). These are distinct notions.- Transparency might mean simulatability, decomposability or algorithmic transparency. Post-hoc explanations include text, visualizations, local explanations, etc.- Claims about interpretability should be qualified based on the specific definition being used. There are tradeoffs between interpretability and accuracy. Post-hoc explanations could mislead.So in summary, the paper does not have a single hypothesis, but rather aims to clarify the discourse around interpretability in machine learning through critical analysis. It highlights the diversity of meanings, motivations and mechanisms related to this concept.


## What is the main contribution of this paper?

This paper examines the motivations for and notions of interpretability in machine learning models. The main contributions are:1. Identifying that interpretability is not a singular, well-defined concept but rather encompasses multiple motivations (e.g. trust, causality, fairness) and model properties (e.g. simulatability, decomposability, post-hoc explanations). 2. Critically analyzing common claims about interpretability, such as linear models being more interpretable than neural networks. The paper shows these claims depend on the particular notion of interpretability being used.3. Discussing the feasibility and desirability of different interpretability notions. It cautions against overly simplistic views and highlights potential negative consequences like explanations that mislead.4. Providing a structured taxonomy of interpretability desiderata and properties that helps clarify this complex topic. This can focus future research by relating it to specific motivations and formalize claims by associating them with particular notions of interpretability.5. Making the argument that interpretability is an important but underspecified concept in machine learning research. The paper calls for more nuanced consideration of interpretability, rather than broad unsupported claims about it.In summary, the main contribution is a much-needed conceptual clarification and critical analysis of interpretability in machine learning. This establishes a framework for relating future interpretability research to precise motivations and formal definitions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper examines the diverse motivations for and notions of interpretability in machine learning models, finding the term to be overloaded with multiple meanings, and arguing claims about interpretability should be qualified and not made axiomatically.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on model interpretability:- It takes a critical perspective, questioning common assumptions and definitions rather than proposing a new algorithm or model. Many papers in this field focus on technical contributions without deeply examining the motivations and meaning of "interpretability."- It identifies interpretability as a multifaceted concept with several distinct notions like transparency, simulatability, and post-hoc explanations. Other papers tend to treat interpretability as a singular goal. This paper argues for more nuance.- It challenges the common claim that linear models are inherently more interpretable than neural networks. The paper argues this claim depends on the specific notion of interpretability and neither model class is fundamentally superior.- The paper surveys a wide range of motivations, model properties, and techniques related to interpretability. It aims for a broad conceptual analysis, rather than narrowly focusing on one specific approach.- It questions whether interpretability should always be sought, discussing potential tradeoffs with accuracy and arguing interpretability is not beneficial in all cases. Much existing work takes the value of interpretability for granted.Overall, this paper takes a more holistic, critical, and nuanced look at interpretability compared to most technical papers that propose and evaluate new models. It aims to clarify the discourse and underlying assumptions, rather than tackling interpretability through technical innovations. The broad conceptual analysis sets it apart from more narrowly focused contributions.


## What future research directions do the authors suggest?

The authors suggest several promising directions for future research:1. For some problems, the discrepancy between real-life and machine learning objectives could be mitigated by developing richer loss functions and performance metrics. Examples include research on sparsity-inducing regularizers and cost-sensitive learning.2. The analysis in this paper could be expanded to other ML paradigms like reinforcement learning. Reinforcement learners can address some of the interpretability objectives by directly modeling interaction between models and environments. However, this may allow models to experiment in the world and incur real consequences. Reinforcement learning relies on a scalar objective function, which may not capture complex goals like fairness.3. Develop post-hoc interpretation techniques that could prove useful for studying the natural world and generating hypotheses, areas where linear models currently seem better suited. For example, computing saliency maps over inputs could help generate hypotheses about causal relationships, similar to how linear models are currently used.4. Be cautious about blindly embracing post-hoc interpretability optimized to placate subjective demands, as it risks generating misleading but plausible explanations. Safeguards may need to be developed.5. Consider whether the desire for transparent models reflects institutional biases against new methods, even when they improve performance. We should ensure transparency is properly balanced with progress in capabilities.In summary, the authors identify promising research directions in developing richer objectives, expanding the analysis to other paradigms like reinforcement learning, generating post-hoc interpretations that could prove useful for science, and carefully considering the proper role of transparency.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper examines the concept of interpretability in machine learning models. It notes that while interpretability is often cited as an important property for models, especially as they are deployed in critical domains like medicine and criminal justice, there is no agreed upon definition of what interpretability actually means. The paper looks at various motivations for wanting interpretability, including trust, causality, transferability, informativeness, and fair decision-making. It then examines properties that are thought to make models interpretable, like simulatability, decomposability, algorithmic transparency, and post-hoc explanations. Throughout, the paper questions common assumptions, like linear models being inherently more interpretable than neural networks. It argues claims about interpretability should be qualified based on the specific notion being invoked, since interpretability refers to multiple distinct concepts. The paper concludes by discussing implications and future work around developing richer objectives and interpretability in reinforcement learning. Overall, the paper aims to bring more nuance and clarity to the discourse around interpretability in machine learning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper examines the concept of interpretability in machine learning models. The first paragraph summarizes the motivations and definitions of interpretability. The authors note that there are several motivations for wanting interpretability, including trust, causality, transferability, informativeness, and fair decision-making. However, there is no agreed upon technical definition of interpretability. Some definitions relate to transparency - being able to understand how the model works at a high level or intuit what individual components represent. Other definitions relate to post-hoc interpretations that do not explain the mechanisms of a model but provide additional information to help justify the model's outputs. The second paragraph summarizes the properties of interpretable models. The authors categorize techniques for achieving interpretability into ones focused on transparency and ones focused on post-hoc interpretations. Transparency techniques aim to make the entire model simulatable/understandable by a human through simplicity, or make individual components like parameters interpretable. Post-hoc techniques like generating text explanations, visualizations, local explanations, or explanations by example aim to provide interpretations of a model without needing to elucidate the mechanisms of how it functions. The authors note there are tradeoffs between accuracy and different notions of interpretability. They advise claims of interpretability should be qualified based on the specific notion intended.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes using a convolutional neural network (CNN) for image classification. The CNN architecture consists of convolutional layers, pooling layers, and fully connected layers. In the convolutional layers, filters are applied across the image to extract features. The pooling layers downsample the feature maps to reduce computation. The fully connected layers at the end classify the extracted features. During training, the CNN learns the filter weights and fully connected layer weights by propagating gradients of a loss function calculated on the training data. This allows the CNN to learn a hierarchical feature representation of the images for classification. Key details include using the ReLU activation function, dropout regularization, data augmentation, and batch normalization. The CNN is trained on the ImageNet dataset to classify images into 1000 different categories. The authors demonstrate that their CNN architecture achieves significantly better performance compared to previous methods on the ImageNet challenge.


## What problem or question is the paper addressing?

The paper appears to be addressing the issue of interpretability in machine learning models. Some key points:- There is a lot of discussion around interpretability of ML models, with claims like linear models are more interpretable than neural networks. However, there is no clear consensus on what "interpretability" actually means.- The paper seeks to clarify the discourse on interpretability by examining the different motivations for wanting interpretability, as well as the various properties/techniques that are thought to make a model interpretable.- The motivations identified include trust, causality, transferability, informativeness, and fair/ethical decision making. Each has a different perspective on why interpretability is important.- The properties and techniques are divided into "transparency" (simulatability, decomposability, algorithmic transparency) and "post-hoc interpretability" (text explanations, visualizations, local explanations, explanation by example).- The paper questions common claims, like linear models being more interpretable than neural nets. The truth depends on what specific notion of interpretability you are using.- Overall, the paper argues that interpretability is not a singular concept and claims about it need to be qualified based on what specific definition you are using. The discourse needs more clarity.In summary, the key problem is the lack of consensus on what interpretability means, which leads to unfounded or unclear claims. The paper aims to bring more structure to the discussion by categorizing motivations and properties related to interpretability.
