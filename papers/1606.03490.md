# The Mythos of Model Interpretability

## What is the central research question or hypothesis that this paper addresses?

This paper does not have a single clear research question or hypothesis. Instead, it is a position paper that aims to critically examine the concept of "model interpretability" in machine learning. The key points made in the paper are:- The term "interpretability" is vague and poorly defined. Different researchers refer to different concepts when using this term.- Common claims like "linear models are interpretable while neural nets are not" are overly simplistic. The truth depends on how interpretability is defined.- There are diverse motivations for wanting interpretability, including trust, causality, transferability, informativeness, and fairness. These suggest interpretability serves objectives not captured by standard evaluation metrics.- Interpretability could refer to transparency (understanding the model internals) or post-hoc explanations (providing additional information). These are distinct notions.- Transparency might mean simulatability, decomposability or algorithmic transparency. Post-hoc explanations include text, visualizations, local explanations, etc.- Claims about interpretability should be qualified based on the specific definition being used. There are tradeoffs between interpretability and accuracy. Post-hoc explanations could mislead.So in summary, the paper does not have a single hypothesis, but rather aims to clarify the discourse around interpretability in machine learning through critical analysis. It highlights the diversity of meanings, motivations and mechanisms related to this concept.


## What is the main contribution of this paper?

This paper examines the motivations for and notions of interpretability in machine learning models. The main contributions are:1. Identifying that interpretability is not a singular, well-defined concept but rather encompasses multiple motivations (e.g. trust, causality, fairness) and model properties (e.g. simulatability, decomposability, post-hoc explanations). 2. Critically analyzing common claims about interpretability, such as linear models being more interpretable than neural networks. The paper shows these claims depend on the particular notion of interpretability being used.3. Discussing the feasibility and desirability of different interpretability notions. It cautions against overly simplistic views and highlights potential negative consequences like explanations that mislead.4. Providing a structured taxonomy of interpretability desiderata and properties that helps clarify this complex topic. This can focus future research by relating it to specific motivations and formalize claims by associating them with particular notions of interpretability.5. Making the argument that interpretability is an important but underspecified concept in machine learning research. The paper calls for more nuanced consideration of interpretability, rather than broad unsupported claims about it.In summary, the main contribution is a much-needed conceptual clarification and critical analysis of interpretability in machine learning. This establishes a framework for relating future interpretability research to precise motivations and formal definitions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper examines the diverse motivations for and notions of interpretability in machine learning models, finding the term to be overloaded with multiple meanings, and arguing claims about interpretability should be qualified and not made axiomatically.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on model interpretability:- It takes a critical perspective, questioning common assumptions and definitions rather than proposing a new algorithm or model. Many papers in this field focus on technical contributions without deeply examining the motivations and meaning of "interpretability."- It identifies interpretability as a multifaceted concept with several distinct notions like transparency, simulatability, and post-hoc explanations. Other papers tend to treat interpretability as a singular goal. This paper argues for more nuance.- It challenges the common claim that linear models are inherently more interpretable than neural networks. The paper argues this claim depends on the specific notion of interpretability and neither model class is fundamentally superior.- The paper surveys a wide range of motivations, model properties, and techniques related to interpretability. It aims for a broad conceptual analysis, rather than narrowly focusing on one specific approach.- It questions whether interpretability should always be sought, discussing potential tradeoffs with accuracy and arguing interpretability is not beneficial in all cases. Much existing work takes the value of interpretability for granted.Overall, this paper takes a more holistic, critical, and nuanced look at interpretability compared to most technical papers that propose and evaluate new models. It aims to clarify the discourse and underlying assumptions, rather than tackling interpretability through technical innovations. The broad conceptual analysis sets it apart from more narrowly focused contributions.
