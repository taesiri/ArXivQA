# [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Evaluating large language models (LLMs) on standard benchmarks like MMLU, HELM, etc requires running the models on thousands of examples, making it very expensive. 
- There is a need for efficient strategies to estimate LLM performance using only a small subset of evaluation examples.

Proposed Solutions
- The paper proposes and compares several strategies:
   1. Stratified random sampling 
   2. Clustering examples based on (in)correctness patterns of models
   3. Novel strategies using Item Response Theory (IRT) to learn representations of examples and cluster them to find robust evaluation sets

- IRT-based strategies also allow improving estimation from any set of examples, not just clusters. 

Main Results
- The proposed IRT methods can reliably estimate LLM performance using only 100 examples per scenario - reducing computation by 100x-1000x.
- Experiments across four benchmarks demonstrate that the IRT methods provide good accuracy even when there is a distribution shift between train and test models.
- Based on the analysis, the paper releases tiny benchmark datasets, tools and pretrained IRT models to enable efficient evaluation of future LLMs.

In summary, the paper demonstrates that by using IRT and clustering, LLM performance on standard language benchmarks can be estimated using 100-1000x fewer examples, thus dramatically reducing evaluation costs. The released tiny benchmarks and tools enable the community to benefit from this finding.
