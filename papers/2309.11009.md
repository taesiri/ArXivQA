# [Controllable Dynamic Appearance for Neural 3D Portraits](https://arxiv.org/abs/2309.11009)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we create fully controllable and reanimatable neural 3D portraits from data captured in challenging real-world conditions with non-ambient lighting?

The key hypotheses appear to be:

1) Illumination-dependent effects like shadows and specularities can be approximated in a dynamic canonical space using an MLP conditioned on surface normals, facial expressions, head poses, etc.

2) Accurate surface normals for the deforming human head can be predicted using an MLP that leverages both 3DMM and scene normal priors. 

3) By modeling illumination effects in a dynamic canonical appearance space and predicting detailed surface normals, it is possible to realistically reanimate neural 3D portraits with explicit controls over facial expressions, head poses and camera viewpoints.

In summary, the main research question is how to create controllable neural 3D portraits from real-world data with non-ideal lighting. The key hypotheses are that illumination effects can be modeled in a dynamic canonical space and that accurate normals prediction is needed, which enables realistic reanimation controls.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

- Introducing Controllable Dynamic Appearance for Neural 3D Portraits (CoDyNeRF), a method that enables the creation of reanimatable neural 3D portraits from videos captured in challenging real-world lighting conditions. 

- Proposing a dynamic canonical appearance model that approximates illumination dependent effects directly in the canonical space. This is done by conditioning an MLP on predicted surface normals, facial expressions, head poses, and other cues related to shading and shadowing.

- Presenting a method to predict detailed and accurate surface normals for the deforming human head using an MLP conditioned on 3DMM normals and scene normals as priors. This is critical for the dynamic appearance modeling. 

- Demonstrating realistic re-animation of lighting and specularity effects on the human face as head-pose and facial expressions change.

In summary, the key contribution appears to be developing a deformable neural radiance field framework called CoDyNeRF that can create controllable 3D portrait animations from real videos captured in challenging lighting, with explicit handling of illumination effects through the dynamic appearance model and surface normal prediction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes CoDyNeRF, a method that enables creating reanimatable and photorealistic neural 3D portraits from videos captured in challenging real-world lighting conditions, by using a dynamic canonical appearance model conditioned on predicted surface normals and facial landmarks to approximate illumination-dependent effects like shadows and specularities.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on deformable neural radiance fields:

- It builds on recent works like RigNeRF, NerFACE, and Neural Head Avatars that focus on modeling and reanimating the human head/face using variants of NeRF with learnable deformations guided by 3DMM priors. The novelties in this paper are in handling illumination effects.

- Compared to RigNeRF and other prior deformable NeRF works, this paper argues that relying on a static canonical appearance space fails for real capture conditions with non-ambient lighting. The key idea is using a dynamic canonical appearance model that depends on surface normals, expressions, etc.

- For surface normal estimation, the paper proposes a method to combine coarse 3DMM normals with scene normals from the NeRF density field, arguing this is better than just using the noisy NeRF normals.

- Unlike NerFACE and Neural Head Avatars which focus just on the face region, this method reconstructs and reanimates the full portrait scene. It also demonstrates view synthesis capabilities beyond just novel expressions/poses.

- Compared to general dynamic scene NeRF methods like Neural Scene Flow or D-NeRF, this paper leverages explicit 3DMM face priors for modeling expressions and illumination effects on the face region.

- Experiments show advantages over prior portrait NeRF methods, especially in handling illumination effects from real capture. The method also appears to enable higher quality view synthesis compared to image-based methods like Head2Head.

Overall, the paper demonstrates high quality reanimation and view synthesis results by combining deformable NeRFs with explicit modeling of illumination effects conditioned on predicted normals and expressions. The dynamic appearance model seems to be the main novel contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Developing methods to handle more complex deformations beyond those modeled by the 3D Morphable Face Model (3DMM). The 3DMM has limitations in modeling certain fine-scale deformations, so going beyond 3DMM could allow for modeling finer details.

- Extending the method to full body modeling and animation, not just the face region. The current method focuses on facial animation but the authors suggest expanding it to full body.

- Incorporating semantic controls beyond just expressions and pose. For example, allowing control of specific facial features independently.

- Exploring different canonical spaces beyond a UV space, such as graph-based structures or point clouds. The UV space assumes texture map like topology which may not be optimal.

- Reducing the amount of training data required. The current method requires videos of a specific person, but generating high-quality results from less data could be useful.

- Enabling relighting effects by estimating lighting and reflectance properties. This could allow modifying illumination in the rendered results.

- Developing generative adversarial networks or other generative models of facial geometry and appearance to reduce overfitting.

- Exploring self-supervised techniques to avoid the need for detailed annotations. Relying less on labels like expressions could make training easier.

In summary, the main directions seem to be 1) going beyond 3DMM, 2) expanding beyond just the face region, 3) adding more semantic controls, 4) exploring new canonical space representations, and 5) reducing training data needs. Developing generative models and self-supervision also seem promising. The overall goal is to enhance the quality, flexibility and ease of training the models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes CoDyNeRF, a method that enables the creation of reanimatable and photorealistic neural 3D portraits using short smartphone-captured videos. The method uses a deformable neural radiance field (NeRF) with a per-point 3DMM guided deformation field to model facial expressions and head poses. The key contribution is a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and facial landmarks to capture illumination-dependent effects like shadows and specularities. Since directly predicting normals is difficult due to deformations, the method uses a novel MLP that leverages 3DMM and scene normals as priors. Once trained, CoDyNeRF can realistically reproduce lighting effects during reanimation with explicit control over head pose, facial expressions, and camera viewpoint. Experiments demonstrate superior performance over prior methods on held-out test frames.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents Controllable Dynamic Appearance for Neural 3D Portraits (CoDyNeRF), a method for creating reanimatable neural 3D portraits from videos captured in challenging lighting conditions. Recent work on neural radiance fields (NeRFs) has enabled modeling and reanimating portrait scenes with control over head-pose, facial expressions, and viewing direction. However, these methods assume photometric consistency as the face deforms, which is difficult to achieve in real capture conditions with changing lighting. CoDyNeRF addresses this by using a dynamic appearance model in a canonical space conditioned on predicted surface normals and facial deformations. 

The key ideas are: 1) Modeling illumination-dependent effects like shadows, shading, and specularities in the canonical space using an MLP conditioned on predicted normals and deformations. This avoids entangling lighting with expression parameters. 2) Predicting accurate surface normals using an MLP that leverages both 3DMM and scene normals as priors. The 3DMM normals act as a coarse prior for the deforming head region. 3) Regularizing the NeRF density field using a Cauchy loss for better geometry. Once trained on a short video, CoDyNeRF can realistically synthesize portraits with control over expression, pose, and view, while reproducing lighting effects. Experiments demonstrate more accurate rendering of illumination compared to prior work.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method called CoDyNeRF for creating fully reanimatable and photorealistic neural 3D portraits from short portrait videos captured using a consumer smartphone in real-world lighting conditions. The key idea is to use a dynamic canonical appearance space modeled by a MLP that is conditioned on predicted surface normals and facial deformations to approximate illumination dependent effects. This allows rendering realistic lighting effects like shadows and specularities when reanimating the portrait with novel expressions and poses. The surface normals are predicted using a MLP that leverages both 3DMM and scene normals as priors to handle the complex deformations. The MLP takes as input the 3DMM normals, gradient density normals from the NeRF, and distance to the mesh to predict accurate and detailed normals. These normals are used to supervise the NeRF gradients to ensure accurate geometry. The method uses a 3DMM-guided deformable NeRF with per-point deformations to map points to the dynamic canonical space. Once trained, the model can realistically reproduce lighting effects during reanimation with explicit control over facial expression, head pose and camera viewpoint.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper addresses the challenge of creating photorealistic and controllable neural 3D portraits from videos captured in real-world conditions with non-ambient lighting.  

- Existing methods like Nerf and deformable Nerfs assume consistent lighting across the video frames. But in real capture conditions, lighting effects like shadows, shading and specularities change with head pose and facial expressions. This makes it hard to learn an accurate canonical space appearance.

- The paper proposes a method called CoDyNeRF to model the dynamic illumination effects directly in a canonical space. This is done by conditioning the appearance prediction on surface normals, head pose/expression deformations, and other cues. 

- Accurately predicting the surface normals is challenging due to lack of ground truth geometry. The paper proposes a normals prediction network that utilizes both 3DMM and scene priors to predict detailed and accurate normals.

- With the dynamic appearance model and improved normals prediction, CoDyNeRF can realistically reproduce lighting effects like shadows, shading and specularities when reanimating the portrait with novel views and facial expressions.

In summary, the key contribution is developing a deformable Nerf method that can create controllable 3D portraits from real capture conditions with non-ambient lighting, by modeling a dynamic appearance and improved surface normals prediction.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Neural Radiance Fields (NeRFs): The paper focuses on extending NeRFs to model dynamic portrait scenes with controllable facial expressions and head poses. NeRFs are a neural representation for novel view synthesis.

- Dynamic scenes: The paper aims to extend NeRFs to handle dynamic scenes like human portraits. This involves modeling motion and deformation.

- Portraits: The application domain is portrait video, so human faces are a core aspect.

- Facial expression control: A key goal is enabling explicit control over facial expressions when reanimating portrait videos.

- Head pose control: Like facial expressions, the paper wants to enable explicit control over head pose during reanimation. 

- Illumination effects: Modeling illumination realistically is important, including shadows, shading, and specularities that change with pose/expression.

- Surface normals: Accurate surface normals are needed to render illumination effects properly. The paper proposes a method to predict detailed normals.

- 3D Morphable Model (3DMM): A 3DMM face model provides priors on face shape and deformation that guide the Neural Radiance Field.

- Reanimation: The end goal is reanimating portrait videos by controlling pose, expression, and viewpoint after training on a portrait video.

- Realistic capture conditions: Unlike past work, the approach focuses on handling real capture conditions with challenging lighting.

In summary, the key focus is on using NeRFs to create controllable 3D portrait animations from videos captured in uncontrolled conditions with non-ambient lighting. The core technical contributions are around modeling dynamic illumination effects and predicting normals.
