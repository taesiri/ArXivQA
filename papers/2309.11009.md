# [Controllable Dynamic Appearance for Neural 3D Portraits](https://arxiv.org/abs/2309.11009)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we create fully controllable and reanimatable neural 3D portraits from data captured in challenging real-world conditions with non-ambient lighting?

The key hypotheses appear to be:

1) Illumination-dependent effects like shadows and specularities can be approximated in a dynamic canonical space using an MLP conditioned on surface normals, facial expressions, head poses, etc.

2) Accurate surface normals for the deforming human head can be predicted using an MLP that leverages both 3DMM and scene normal priors. 

3) By modeling illumination effects in a dynamic canonical appearance space and predicting detailed surface normals, it is possible to realistically reanimate neural 3D portraits with explicit controls over facial expressions, head poses and camera viewpoints.

In summary, the main research question is how to create controllable neural 3D portraits from real-world data with non-ideal lighting. The key hypotheses are that illumination effects can be modeled in a dynamic canonical space and that accurate normals prediction is needed, which enables realistic reanimation controls.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

- Introducing Controllable Dynamic Appearance for Neural 3D Portraits (CoDyNeRF), a method that enables the creation of reanimatable neural 3D portraits from videos captured in challenging real-world lighting conditions. 

- Proposing a dynamic canonical appearance model that approximates illumination dependent effects directly in the canonical space. This is done by conditioning an MLP on predicted surface normals, facial expressions, head poses, and other cues related to shading and shadowing.

- Presenting a method to predict detailed and accurate surface normals for the deforming human head using an MLP conditioned on 3DMM normals and scene normals as priors. This is critical for the dynamic appearance modeling. 

- Demonstrating realistic re-animation of lighting and specularity effects on the human face as head-pose and facial expressions change.

In summary, the key contribution appears to be developing a deformable neural radiance field framework called CoDyNeRF that can create controllable 3D portrait animations from real videos captured in challenging lighting, with explicit handling of illumination effects through the dynamic appearance model and surface normal prediction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes CoDyNeRF, a method that enables creating reanimatable and photorealistic neural 3D portraits from videos captured in challenging real-world lighting conditions, by using a dynamic canonical appearance model conditioned on predicted surface normals and facial landmarks to approximate illumination-dependent effects like shadows and specularities.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on deformable neural radiance fields:

- It builds on recent works like RigNeRF, NerFACE, and Neural Head Avatars that focus on modeling and reanimating the human head/face using variants of NeRF with learnable deformations guided by 3DMM priors. The novelties in this paper are in handling illumination effects.

- Compared to RigNeRF and other prior deformable NeRF works, this paper argues that relying on a static canonical appearance space fails for real capture conditions with non-ambient lighting. The key idea is using a dynamic canonical appearance model that depends on surface normals, expressions, etc.

- For surface normal estimation, the paper proposes a method to combine coarse 3DMM normals with scene normals from the NeRF density field, arguing this is better than just using the noisy NeRF normals.

- Unlike NerFACE and Neural Head Avatars which focus just on the face region, this method reconstructs and reanimates the full portrait scene. It also demonstrates view synthesis capabilities beyond just novel expressions/poses.

- Compared to general dynamic scene NeRF methods like Neural Scene Flow or D-NeRF, this paper leverages explicit 3DMM face priors for modeling expressions and illumination effects on the face region.

- Experiments show advantages over prior portrait NeRF methods, especially in handling illumination effects from real capture. The method also appears to enable higher quality view synthesis compared to image-based methods like Head2Head.

Overall, the paper demonstrates high quality reanimation and view synthesis results by combining deformable NeRFs with explicit modeling of illumination effects conditioned on predicted normals and expressions. The dynamic appearance model seems to be the main novel contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Developing methods to handle more complex deformations beyond those modeled by the 3D Morphable Face Model (3DMM). The 3DMM has limitations in modeling certain fine-scale deformations, so going beyond 3DMM could allow for modeling finer details.

- Extending the method to full body modeling and animation, not just the face region. The current method focuses on facial animation but the authors suggest expanding it to full body.

- Incorporating semantic controls beyond just expressions and pose. For example, allowing control of specific facial features independently.

- Exploring different canonical spaces beyond a UV space, such as graph-based structures or point clouds. The UV space assumes texture map like topology which may not be optimal.

- Reducing the amount of training data required. The current method requires videos of a specific person, but generating high-quality results from less data could be useful.

- Enabling relighting effects by estimating lighting and reflectance properties. This could allow modifying illumination in the rendered results.

- Developing generative adversarial networks or other generative models of facial geometry and appearance to reduce overfitting.

- Exploring self-supervised techniques to avoid the need for detailed annotations. Relying less on labels like expressions could make training easier.

In summary, the main directions seem to be 1) going beyond 3DMM, 2) expanding beyond just the face region, 3) adding more semantic controls, 4) exploring new canonical space representations, and 5) reducing training data needs. Developing generative models and self-supervision also seem promising. The overall goal is to enhance the quality, flexibility and ease of training the models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes CoDyNeRF, a method that enables the creation of reanimatable and photorealistic neural 3D portraits using short smartphone-captured videos. The method uses a deformable neural radiance field (NeRF) with a per-point 3DMM guided deformation field to model facial expressions and head poses. The key contribution is a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and facial landmarks to capture illumination-dependent effects like shadows and specularities. Since directly predicting normals is difficult due to deformations, the method uses a novel MLP that leverages 3DMM and scene normals as priors. Once trained, CoDyNeRF can realistically reproduce lighting effects during reanimation with explicit control over head pose, facial expressions, and camera viewpoint. Experiments demonstrate superior performance over prior methods on held-out test frames.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents Controllable Dynamic Appearance for Neural 3D Portraits (CoDyNeRF), a method for creating reanimatable neural 3D portraits from videos captured in challenging lighting conditions. Recent work on neural radiance fields (NeRFs) has enabled modeling and reanimating portrait scenes with control over head-pose, facial expressions, and viewing direction. However, these methods assume photometric consistency as the face deforms, which is difficult to achieve in real capture conditions with changing lighting. CoDyNeRF addresses this by using a dynamic appearance model in a canonical space conditioned on predicted surface normals and facial deformations. 

The key ideas are: 1) Modeling illumination-dependent effects like shadows, shading, and specularities in the canonical space using an MLP conditioned on predicted normals and deformations. This avoids entangling lighting with expression parameters. 2) Predicting accurate surface normals using an MLP that leverages both 3DMM and scene normals as priors. The 3DMM normals act as a coarse prior for the deforming head region. 3) Regularizing the NeRF density field using a Cauchy loss for better geometry. Once trained on a short video, CoDyNeRF can realistically synthesize portraits with control over expression, pose, and view, while reproducing lighting effects. Experiments demonstrate more accurate rendering of illumination compared to prior work.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method called CoDyNeRF for creating fully reanimatable and photorealistic neural 3D portraits from short portrait videos captured using a consumer smartphone in real-world lighting conditions. The key idea is to use a dynamic canonical appearance space modeled by a MLP that is conditioned on predicted surface normals and facial deformations to approximate illumination dependent effects. This allows rendering realistic lighting effects like shadows and specularities when reanimating the portrait with novel expressions and poses. The surface normals are predicted using a MLP that leverages both 3DMM and scene normals as priors to handle the complex deformations. The MLP takes as input the 3DMM normals, gradient density normals from the NeRF, and distance to the mesh to predict accurate and detailed normals. These normals are used to supervise the NeRF gradients to ensure accurate geometry. The method uses a 3DMM-guided deformable NeRF with per-point deformations to map points to the dynamic canonical space. Once trained, the model can realistically reproduce lighting effects during reanimation with explicit control over facial expression, head pose and camera viewpoint.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper addresses the challenge of creating photorealistic and controllable neural 3D portraits from videos captured in real-world conditions with non-ambient lighting.  

- Existing methods like Nerf and deformable Nerfs assume consistent lighting across the video frames. But in real capture conditions, lighting effects like shadows, shading and specularities change with head pose and facial expressions. This makes it hard to learn an accurate canonical space appearance.

- The paper proposes a method called CoDyNeRF to model the dynamic illumination effects directly in a canonical space. This is done by conditioning the appearance prediction on surface normals, head pose/expression deformations, and other cues. 

- Accurately predicting the surface normals is challenging due to lack of ground truth geometry. The paper proposes a normals prediction network that utilizes both 3DMM and scene priors to predict detailed and accurate normals.

- With the dynamic appearance model and improved normals prediction, CoDyNeRF can realistically reproduce lighting effects like shadows, shading and specularities when reanimating the portrait with novel views and facial expressions.

In summary, the key contribution is developing a deformable Nerf method that can create controllable 3D portraits from real capture conditions with non-ambient lighting, by modeling a dynamic appearance and improved surface normals prediction.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Neural Radiance Fields (NeRFs): The paper focuses on extending NeRFs to model dynamic portrait scenes with controllable facial expressions and head poses. NeRFs are a neural representation for novel view synthesis.

- Dynamic scenes: The paper aims to extend NeRFs to handle dynamic scenes like human portraits. This involves modeling motion and deformation.

- Portraits: The application domain is portrait video, so human faces are a core aspect.

- Facial expression control: A key goal is enabling explicit control over facial expressions when reanimating portrait videos.

- Head pose control: Like facial expressions, the paper wants to enable explicit control over head pose during reanimation. 

- Illumination effects: Modeling illumination realistically is important, including shadows, shading, and specularities that change with pose/expression.

- Surface normals: Accurate surface normals are needed to render illumination effects properly. The paper proposes a method to predict detailed normals.

- 3D Morphable Model (3DMM): A 3DMM face model provides priors on face shape and deformation that guide the Neural Radiance Field.

- Reanimation: The end goal is reanimating portrait videos by controlling pose, expression, and viewpoint after training on a portrait video.

- Realistic capture conditions: Unlike past work, the approach focuses on handling real capture conditions with challenging lighting.

In summary, the key focus is on using NeRFs to create controllable 3D portrait animations from videos captured in uncontrolled conditions with non-ambient lighting. The core technical contributions are around modeling dynamic illumination effects and predicting normals.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to create a comprehensive summary of the paper:

1. What is the main focus/goal of the research presented in the paper? 

2. What problem is the paper trying to solve? What are the limitations of existing methods that the paper aims to address?

3. What is the proposed method or framework introduced in the paper? Give a brief overview of the key ideas and techniques.

4. What are the main components and steps involved in the proposed method? How do they work together? 

5. What kind of data is used for experiments/evaluation? How is it collected and processed?

6. What experimental results are presented in the paper? What metrics are used for evaluation?

7. How does the proposed method compare to existing state-of-the-art techniques, both quantitatively and qualitatively? What are the main advantages?

8. What are the limitations of the proposed method? What aspects could be improved in future work?

9. What are the main applications or use cases that could benefit from this research?

10. What are the key takeaways? What conclusions or future directions are suggested based on this work?

Asking these types of questions while reading the paper will help identify the core contributions, techniques, results and limitations to create a thorough yet concise summary that captures the essence of the work. The questions cover the problem definition, technical approach, experiments, results, comparisons, limitations and applications which provide a holistic view of the research presented.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a dynamic canonical appearance model to capture illumination effects. How does this compare to prior work like RigNeRF that uses a static canonical appearance? What are the advantages of using a dynamic appearance model?

2. The paper conditions the appearance MLP on several inputs like normals, reflection vectors, pose/expression deformations etc. What is the intuition behind using each of these inputs? How do they help capture illumination effects accurately?

3. The paper predicts detailed normals using an MLP with mesh normals and density field normals as input. Why is it challenging to estimate normals on a deforming human head directly? How does using the mesh normals help address this?

4. What is the Cauchy regularization used in the paper and why is it important for improving the underlying scene normals? How does it complement the importance sampling based regularizer?

5. The paper compares against several recent methods like RigNeRF, Neural Head Avatars and PointAvatar. What are the key differences in approach between these methods and the proposed method? How do these differences affect the results?

6. What are some of the limitations of the proposed method? For example, does it support relighting with novel lighting? How does it perform on extreme lighting conditions?

7. The method requires about 40 seconds of training data. How does this capture requirement compare to other neural avatars like NerFACE or neural actors? Could the method work with less data?

8. How does the method perform on subjects with accessories like glasses, hats etc? Would it require any modifications to handle such cases?

9. The method assumes the subject has a neutral expression during training data capture. How could this assumption be relaxed to support subjects with expresisons during capture?

10. The paper focuses on portrait scenes. Could the proposed ideas be extended to full body neural avatars? What challenges might arise in that setting?
