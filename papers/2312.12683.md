# [Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is   Needed?](https://arxiv.org/abs/2312.12683)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most large language models (LLMs) today are English-centric, having been pretrained predominantly on English data. This limits their capabilities when deployed in non-English settings.
- It is unclear how much multilinguality during finetuning is needed to improve the cross-lingual transfer abilities of these English-centric models. 

Methodology:
- The authors instruction-tune LLamas 2 and Falcon, varying the number of languages used during finetuning from 1 (English only) to 6 (English plus top 5 non-English languages seen during pretraining).
- They evaluate on generative tasks (open-ended chat, extractive QA) and structured tasks (commonsense reasoning, NLI) in 13 target languages with varying degrees of exposure during pretraining.

Key Findings:  
- Multilingual finetuning significantly improves cross-lingual transfer on generative tasks, plateauing at just 2-3 languages. It provides minimal gains on structured tasks.
- For chat, model scaling amplifies benefits, with Llama 2 70B matching English performance on major European languages with just 2 finetuning languages.
- Gains are from better input/output language agreement, not improved reasoning abilities.

Main Contributions:
- Shows that very limited multilinguality during finetuning can unlock cross-lingual transfer abilities of English-centric LLMs.
- Finds that model scaling reduces the minimal required language diversity. 
- Reveals that gains predominantly manifest in IO language agreement, less so in reasoning skills.
- Provides guidance on how much and which languages are most important for multilingual finetuning.

In summary, the paper demonstrates that we can effectively turn English-centric LLMs into polyglots with minimal intervention, though some language and task specific tuning is still beneficial.
