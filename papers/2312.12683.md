# [Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is   Needed?](https://arxiv.org/abs/2312.12683)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most large language models (LLMs) today are English-centric, having been pretrained predominantly on English data. This limits their capabilities when deployed in non-English settings.
- It is unclear how much multilinguality during finetuning is needed to improve the cross-lingual transfer abilities of these English-centric models. 

Methodology:
- The authors instruction-tune LLamas 2 and Falcon, varying the number of languages used during finetuning from 1 (English only) to 6 (English plus top 5 non-English languages seen during pretraining).
- They evaluate on generative tasks (open-ended chat, extractive QA) and structured tasks (commonsense reasoning, NLI) in 13 target languages with varying degrees of exposure during pretraining.

Key Findings:  
- Multilingual finetuning significantly improves cross-lingual transfer on generative tasks, plateauing at just 2-3 languages. It provides minimal gains on structured tasks.
- For chat, model scaling amplifies benefits, with Llama 2 70B matching English performance on major European languages with just 2 finetuning languages.
- Gains are from better input/output language agreement, not improved reasoning abilities.

Main Contributions:
- Shows that very limited multilinguality during finetuning can unlock cross-lingual transfer abilities of English-centric LLMs.
- Finds that model scaling reduces the minimal required language diversity. 
- Reveals that gains predominantly manifest in IO language agreement, less so in reasoning skills.
- Provides guidance on how much and which languages are most important for multilingual finetuning.

In summary, the paper demonstrates that we can effectively turn English-centric LLMs into polyglots with minimal intervention, though some language and task specific tuning is still beneficial.


## Summarize the paper in one sentence.

 This paper investigates how much multilingual data is needed during finetuning of English-centric large language models in order to elicit cross-lingual transfer abilities, finding that performance tends to plateau quickly after adding just 2-3 languages and that gains are most pronounced for open-ended generative tasks assuming input/output language agreement.


## What is the main contribution of this paper?

 The main contribution of this paper is investigating how much multilingual data is required during finetuning of English-centric large language models in order to elicit strong cross-lingual transfer abilities. Through experiments on multiple languages and tasks, the authors find that finetuning with as few as 2-3 languages can significantly improve zero-shot performance on generative tasks compared to English-only finetuning. The paper also analyzes which languages and tasks benefit most from multilingual finetuning. The key findings are that multilingual finetuning is most beneficial for open-ended generative tasks that assume input/output language agreement, while being less important for highly structured tasks. Overall, the work provides useful insights into efficiently deriving capable polyglot models from English-centric LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review of the content, some of the key terms and concepts associated with this paper include:

- English-centric LLMs - The paper focuses on large language models that have been predominantly pretrained on English data.

- Multilingual instruction tuning - Finetuning LLMs on a small set of high-quality conversational instructions in multiple languages to improve multilingual capabilities. 

- Cross-lingual transfer - The ability of a model tuned mostly on English data to generalize to other languages it has seen less frequently.

- Generative tasks - Downstream NLG applications like open-ended chat that assume input/output language agreement.

- Minimum multilinguality - A key question studied is how much multilingual data is needed to elicit cross-lingual transfer.

- Supervised languages - Languages seen during both pretraining and finetuning.

- Zero-shot languages - Languages seen during pretraining but not finetuning.

- Extremely low-resource languages - Languages that were likely never intentionally included in the training data.

The key findings are that multilingual instruction tuning with as few as 2-3 languages can encourage strong cross-lingual transfer on generative tasks, while being less impactful on structured understanding tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What was the motivation behind investigating how much multilinguality is needed during finetuning to elicit cross-lingual generalisation in English-centric large language models? Why did the authors feel this was an important research direction?

2. This paper proposed using multilingual instruction tuning with incremental amounts of non-English data. Can you explain why instruction tuning was chosen over other finetuning approaches and what advantages it offers for analysing cross-lingual transfer?  

3. The authors evaluated performance on 4 distinct downstream tasks. Can you describe each of those tasks and explain why they were selected as a representative set for analyzing the impact of multilingual instruction tuning?

4. When evaluating the helpfulness of open-ended chat responses using an LLM judge, the authors directly evaluated non-English responses rather than first translating to English. What was their rationale behind this decision and what are the potential limitations?  

5. The authors found that multilingual instruction tuning was most beneficial for open-ended generative tasks assuming input/output language agreement. Why do you think structured tasks like commonsense reasoning didn't see similar gains?

6. With only 2-3 languages needed during finetuning, why do you think additional languages didn't lead to further improvements in cross-lingual performance? What hypotheses might explain this plateau with few finetuning languages?

7. For the extremely low-resource languages not seen during pretraining, the authors found limited zero-shot transfer abilities from multilingual instruction tuning. What steps could be taken to improve performance in such languages while retaining English capability?  

8. When analyzing the impact of scaling model size from 7 billion to 70 billion parameters, what differences were observed regarding the minimal number of finetuning languages needed? Why do you think model size affects this?

9. The authors repeated experiments using the Falcon 7B model which has a different pretraining objective compared to LLaMA. How did the cross-lingual abilities compare between these two English-centric models and what might account for the differences seen?  

10. What are some key limitations of this work that could be addressed in future studies on analyzing and improving multilingual capabilities of English-centric LLMs? What open questions remain regarding cross-lingual transfer in such models?
