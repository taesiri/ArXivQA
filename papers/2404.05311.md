# [BruSLeAttack: A Query-Efficient Score-Based Black-Box Sparse Adversarial   Attack](https://arxiv.org/abs/2404.05311)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of generating sparse adversarial examples against deep neural networks in a black-box setting where only the confidence scores are observable. Sparse adversarial attacks aim to perturb only a small number of pixels in the input image to cause misclassification. However, constructing sparse adversarial examples is very challenging due to the combinatorial and non-differentiable nature of the search space. Prior score-based sparse attack methods lack query efficiency and high success rates, especially for high-resolution datasets like ImageNet.

Proposed Solution: 
The paper proposes a new score-based sparse attack algorithm called BruSLiAttack that is more query-efficient while achieving higher attack success rates. The key ideas are:

(1) Reformulate the problem to reduce the search space dimensionality by fixing a randomly generated "synthetic color image" to define perturbed pixel colors. 

(2) Guide the combinatorial search using a Bayesian framework to learn and leverage historical information about the influence of each pixel on model decisions. Intuitively, some pixels have higher impact on decisions than others.

(3) Incorporate prior knowledge of pixel dissimilarity between the synthetic and original images to select more influential pixels to perturb.

Main Contributions:

- Propose a new sparse attack algorithm that is significantly more query-efficient than prior art, achieving over 90% attack success rate on ImageNet with only 6K queries.

- Demonstrate the fragility of CNNs and Vision Transformers against sparse attacks, with Transformers shown to be relatively more robust.

- Show that robust training defenses based on adversarial training and randomized noise augmentation remain vulnerable to sparse attacks.

- Provide attack visualizations and evaluate on real-world Cloud Vision API to highlight the practical implications.

Overall, the work enables faster evaluation of model vulnerabilities against sparse perturbations, raising awareness of reliability concerns for deployed vision systems. The unique Bayesian guided combinatorial search approach is a key contribution for tackling discrete-continuous optimization.
