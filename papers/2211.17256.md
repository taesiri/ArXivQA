# [CLIPascene: Scene Sketching with Different Types and Levels of   Abstraction](https://arxiv.org/abs/2211.17256)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we automatically generate abstract scene sketches with controllable levels of abstraction along the dimensions of visual simplicity and stroke precision?

The authors propose a method to generate abstract sketches of an input photograph with varying levels of abstraction. Their key idea is to control abstraction along two axes:

1. Visual simplicity: Gradually simplify the sketch by removing detail. This is done by optimizing a network to remove strokes while maintaining scene semantics.

2. Stroke precision: Control the level of detail and precision in the strokes. This is done by optimizing the network using different layers of a CLIP model which provide feature maps at different levels of abstraction.

The central hypothesis is that by controlling abstraction along these two axes, they can automatically generate sketch matrices that provide a smooth range of abstractions capturing both global scene structure and local precision detail. The paper presents both qualitative and quantitative experiments to evaluate the ability of their method to provide control over abstraction levels compared to prior sketch generation techniques.

In summary, the key research question is how to automatically generate controllable abstractions of scene sketches rather than just a single output sketch. The two axes of abstraction allow control over both the complexity of the scene semantics as well as the level of stroke precision detail.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new method for generating abstract scene sketches with controllable levels of abstraction along two separate axes:

1. Fidelity Axis: The paper presents a method to generate scene sketches at varying levels of detail and precision. This is achieved by training a neural network at different layers of a CLIP vision transformer model. Lower layers lead to more detailed sketches while higher layers result in looser, more abstract sketches.

2. Simplicity Axis: The paper proposes a technique to iteratively simplify an input sketch by balancing two loss functions - one that preserves semantics based on CLIP and one that encourages sparsity. By controlling the relative strengths of these losses, sketches with varying levels of visual simplification can be generated from a given input sketch.

So in summary, the key contribution is allowing control over both the fidelity and simplicity of generated scene sketches through the use of CLIP losses and iterative neural network training. This provides users more flexibility than prior sketch generation methods in producing a range of abstractions. The paper demonstrates this through both qualitative results and quantitative experiments.
