# [CLIPascene: Scene Sketching with Different Types and Levels of   Abstraction](https://arxiv.org/abs/2211.17256)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we automatically generate abstract scene sketches with controllable levels of abstraction along the dimensions of visual simplicity and stroke precision?

The authors propose a method to generate abstract sketches of an input photograph with varying levels of abstraction. Their key idea is to control abstraction along two axes:

1. Visual simplicity: Gradually simplify the sketch by removing detail. This is done by optimizing a network to remove strokes while maintaining scene semantics.

2. Stroke precision: Control the level of detail and precision in the strokes. This is done by optimizing the network using different layers of a CLIP model which provide feature maps at different levels of abstraction.

The central hypothesis is that by controlling abstraction along these two axes, they can automatically generate sketch matrices that provide a smooth range of abstractions capturing both global scene structure and local precision detail. The paper presents both qualitative and quantitative experiments to evaluate the ability of their method to provide control over abstraction levels compared to prior sketch generation techniques.

In summary, the key research question is how to automatically generate controllable abstractions of scene sketches rather than just a single output sketch. The two axes of abstraction allow control over both the complexity of the scene semantics as well as the level of stroke precision detail.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new method for generating abstract scene sketches with controllable levels of abstraction along two separate axes:

1. Fidelity Axis: The paper presents a method to generate scene sketches at varying levels of detail and precision. This is achieved by training a neural network at different layers of a CLIP vision transformer model. Lower layers lead to more detailed sketches while higher layers result in looser, more abstract sketches.

2. Simplicity Axis: The paper proposes a technique to iteratively simplify an input sketch by balancing two loss functions - one that preserves semantics based on CLIP and one that encourages sparsity. By controlling the relative strengths of these losses, sketches with varying levels of visual simplification can be generated from a given input sketch.

So in summary, the key contribution is allowing control over both the fidelity and simplicity of generated scene sketches through the use of CLIP losses and iterative neural network training. This provides users more flexibility than prior sketch generation methods in producing a range of abstractions. The paper demonstrates this through both qualitative results and quantitative experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper presents a method to generate sketch abstractions of an input image along two axes - visual simplicity and stroke precision - by training neural networks to iteratively simplify an initial detailed sketch based on losses that balance sketch sparseness and semantic similarity to the input image.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in the same field:

- This paper presents a new method for generating abstract scene sketches with controllable abstraction along two axes - fidelity and simplicity. Most prior work has focused on only one axis of abstraction, usually simplification. Allowing control along two axes provides more flexibility.

- The technique uses a convolutional MLP to learn to generate sketch strokes, guided by CLIP losses. This is similar to prior work like Clipart StyleGAN and Clipasso that also train neural networks for abstract image generation using CLIP. The key difference is this paper uses scene decomposition and trains separate networks for foreground and background.

- For foreground sketching, this paper leverages ViT layers 2, 4, 7, and 8 for different levels of fidelity. Most prior work uses a single CLIP layer. Using multiple layers allows more precision in controlling abstraction level.

- For background sketching, this paper uses layers 2, 7, 8 and 11. Layer 11 leads to a very loose style not explored much before for scene sketching.

- For simplicity control, this paper gradually balances the CLIP loss against a stroke sparsity loss using an exponential schedule. Most prior work has focused only on sparsity. The exponential schedule enables smooth transitions between levels.

- For evaluation, this paper introduces a new automated recognizability metric using CLIP, in addition to human studies. Most prior work has only used human studies or fidelity metrics. The automated metric allows easier large-scale evaluation.

- Compared to other scene sketching papers, this method seems to allow finer control over abstraction style along the two axes. The evaluations also seem more extensive.

In summary, the key novelties compared to prior work are the use of multiple CLIP layers, exponential schedule for simplicity, and the automated recognizability metric. By better disentangling fidelity and simplicity, this method expands the abstraction space for sketch generation.
