# [Only 5\% Attention Is All You Need: Efficient Long-range Document-level   Neural Machine Translation](https://arxiv.org/abs/2309.14174)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we improve the efficiency of document-level neural machine translation while maintaining high translation quality?Specifically, the paper focuses on reducing the computational cost of the attention mechanism in the Transformer model when applied to long document translation. The quadratic complexity of standard Transformer attention becomes very expensive for long sequences. The paper proposes a method called Lightweight Attention Selection Transformer (LAST) to selectively attend to only the most important tokens based on an adaptive lightweight attention module. This allows pruning away unnecessary computation on irrelevant tokens.The main hypothesis seems to be that only a small fraction of tokens in a long context are truly relevant for the current translation step. So the paper investigates whether selectively attending to a sparse set of tokens can greatly reduce computational cost while retaining high translation performance on document-level datasets.In summary, the key research question is how to achieve an efficient Transformer for long document translation via a sparse attention mechanism, while maintaining high translation quality compared to standard Transformer models. The paper aims to demonstrate this is possible by attending to only ~5% of tokens.
