# [Only 5\% Attention Is All You Need: Efficient Long-range Document-level   Neural Machine Translation](https://arxiv.org/abs/2309.14174)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we improve the efficiency of document-level neural machine translation while maintaining high translation quality?Specifically, the paper focuses on reducing the computational cost of the attention mechanism in the Transformer model when applied to long document translation. The quadratic complexity of standard Transformer attention becomes very expensive for long sequences. The paper proposes a method called Lightweight Attention Selection Transformer (LAST) to selectively attend to only the most important tokens based on an adaptive lightweight attention module. This allows pruning away unnecessary computation on irrelevant tokens.The main hypothesis seems to be that only a small fraction of tokens in a long context are truly relevant for the current translation step. So the paper investigates whether selectively attending to a sparse set of tokens can greatly reduce computational cost while retaining high translation performance on document-level datasets.In summary, the key research question is how to achieve an efficient Transformer for long document translation via a sparse attention mechanism, while maintaining high translation quality compared to standard Transformer models. The paper aims to demonstrate this is possible by attending to only ~5% of tokens.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes an efficient transformer model called Lightweight Attention Selection Transformer (Lasformer) for long-range document-level neural machine translation. 2. It introduces a lightweight attention module to select important tokens and filter out unimportant ones. The distribution of the lightweight attention is guided by the original attention via an additional KL loss.3. It proposes an adaptive sparsity learning method to dynamically determine the optimal level of sparsity during training.4. It shares the learned sparse patterns across layers to further reduce computational cost.5. Experimental results show Lasformer reduces the attention computation cost to only 7% of the original Transformer, while maintaining high translation quality on long document datasets. An overall 1.2x speedup is achieved.In summary, the key contribution is an efficient Transformer that incorporates lightweight attention and adaptive sparsity learning to significantly reduce the quadratic computation cost of attention when handling long document translation. This is achieved while maintaining high translation performance.
