# [Only 5\% Attention Is All You Need: Efficient Long-range Document-level   Neural Machine Translation](https://arxiv.org/abs/2309.14174)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we improve the efficiency of document-level neural machine translation while maintaining high translation quality?Specifically, the paper focuses on reducing the computational cost of the attention mechanism in the Transformer model when applied to long document translation. The quadratic complexity of standard Transformer attention becomes very expensive for long sequences. The paper proposes a method called Lightweight Attention Selection Transformer (LAST) to selectively attend to only the most important tokens based on an adaptive lightweight attention module. This allows pruning away unnecessary computation on irrelevant tokens.The main hypothesis seems to be that only a small fraction of tokens in a long context are truly relevant for the current translation step. So the paper investigates whether selectively attending to a sparse set of tokens can greatly reduce computational cost while retaining high translation performance on document-level datasets.In summary, the key research question is how to achieve an efficient Transformer for long document translation via a sparse attention mechanism, while maintaining high translation quality compared to standard Transformer models. The paper aims to demonstrate this is possible by attending to only ~5% of tokens.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes an efficient transformer model called Lightweight Attention Selection Transformer (Lasformer) for long-range document-level neural machine translation. 2. It introduces a lightweight attention module to select important tokens and filter out unimportant ones. The distribution of the lightweight attention is guided by the original attention via an additional KL loss.3. It proposes an adaptive sparsity learning method to dynamically determine the optimal level of sparsity during training.4. It shares the learned sparse patterns across layers to further reduce computational cost.5. Experimental results show Lasformer reduces the attention computation cost to only 7% of the original Transformer, while maintaining high translation quality on long document datasets. An overall 1.2x speedup is achieved.In summary, the key contribution is an efficient Transformer that incorporates lightweight attention and adaptive sparsity learning to significantly reduce the quadratic computation cost of attention when handling long document translation. This is achieved while maintaining high translation performance.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research in efficient transformers and document-level machine translation:- It focuses specifically on improving efficiency for document-level machine translation by reducing the quadratic complexity of attention. Much previous work on efficient transformers has focused only on encoder-only tasks like classification, or tested models only on short sequences. - The method dynamically learns a sparse attention pattern through a lightweight selection layer, unlike other approaches that use fixed/handcrafted sparsity patterns. This allows more flexible and adaptive sparsity.- The paper comprehensively compares with other major approaches like low-rank methods, hashing-based sparsity, etc. on long document translation tasks. Most previous efficient transformer work was not evaluated on such long-range seq2seq tasks. - Results show the approach maintains strong performance on document-level translation while greatly reducing computational cost. Other recent methods showed big drops in BLEU when evaluated on long sequences.- Analysis provides insights into what sparse patterns emerge, differences across encoder vs decoder, how performance evolves during training, etc. This helps understand when and why the approach works.- Limitations are also clearly discussed regarding issues like the gap between theoretical vs actual speedups, and what factors limit gains on shorter sequences.Overall, the paper advances research on efficient transformers by presenting a method tailored to document-level MT that learns sparse patterns dynamically, and conducts much more rigorous testing on long sequences than prior work. The analyses also provide useful insights.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Developing more efficient transformer models for long-range sequence-to-sequence tasks like document-level machine translation. The authors note that their method focuses on reducing the attention cost, but other components like the feedforward layers also contribute to computational complexity and could be made more efficient.- Exploring different sparsity patterns beyond top-k selection. The paper uses a simple top-k sparsity pattern but notes that more complex sparsity patterns could be learned.- Applying the efficient attention mechanisms to other conditional generation tasks beyond machine translation, such as summarization, dialogue, etc. The authors only evaluate on machine translation so testing on other text generation tasks could be beneficial.- Evaluating the impact of efficient attention on very long documents, like books or movies. The authors suggest their method could have bigger efficiency gains on extremely long sequences but they only test up to thousands of tokens.- Combining the sparsity-based approach here with other techniques like low-rank approximations to potentially achieve further gains in efficiency. The paper focuses solely on sparsity but hybrid methods could help too.- Addressing the limitations around speedups being constrained by GPU optimizations and decoding. Improving the optimization and inference pipelines could help get closer to the theoretical efficiency improvements.In summary, the main directions are developing more comprehensive efficient transformer architectures tailored for long sequences, applying the efficient attention ideas to other generation tasks, and addressing the current practical limitations to achieve faster speedups on modern hardware. Evaluation on extremely long sequences is also noted as an interesting direction.
