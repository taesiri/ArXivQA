# [Smart-Infinity: Fast Large Language Model Training using Near-Storage   Processing on a Real System](https://arxiv.org/abs/2403.06664)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Training large language models (LLMs) requires large memory capacity to store model parameters, activations, and optimizer states. This necessitates using dozens of GPUs even for fine-tuning tasks.
- A popular solution is storage-offloaded training, which uses host memory and SSDs as extended memory. However, this causes severe bandwidth bottlenecks due to the much lower bandwidth of SSDs compared to GPU memory.
- Increasing the number of SSDs using RAID0 provides limited benefits as the interconnect between host and SSDs becomes the bottleneck beyond around 4 SSDs.

Proposed Solution: 
- The paper proposes "Smart-Infinity", a fast LLM training system using computational storage devices (CSDs) with near-storage processing accelerators. 
- The key idea is offloading the parameter update operation, which accounts for 75% of SSD traffic, to CSD accelerators. This removes most storage traffic through the host interconnect.
- An efficient data transfer handler structure is proposed to maximize CSD bandwidth utilization.
- To further reduce remaining gradient traffic to SSDs, gradient compression on GPUs and decompression on CSD accelerators are added.

Main Contributions:
- SmartUpdate: Offloads parameter update to CSD accelerators, reducing 75% of storage traffic.
- Optimized data handler for CSD internal transfers with buffer preallocation and swap overlapping.  
- SmartTopK: CSD-accelerated gradient compression/decompression to enhance scalability.
- Complete integration with PyTorch on a real system with up to 10 SmartSSDs.
- Up to 2.11x speedup over state-of-the-art baseline demonstrated on mixed-precision LLM training.

The key impact is enabling fast LLM training by mitigating the storage bandwidth bottleneck using near-storage accelerators in CSDs. Smart-Infinity is shown to be a practical solution that works on a real system with significant speedups.
