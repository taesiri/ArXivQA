# Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen   Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be:Leveraging retrieved relevant text from a large corpus can effectively transfer the strong generalization ability of large pre-trained language models to the video question answering task in a zero-shot manner, without needing additional cross-modal training.In particular, the paper proposes that retrieving descriptive text for a video using an off-the-shelf contrastive vision-language model, and then feeding both the retrieved text and question to a frozen pre-trained language model can achieve strong zero-shot video QA performance. This avoids the need for costly joint training of vision and language models on large-scale multi-modal datasets.The key ideas are:- Using efficient semantic text retrieval from a large corpus as a bridge between vision and language instead of generating captions or learning cross-modal alignments.- The retrieved text acts as 'soft video description', summarizing the visual content to provide informative contextual hints to the language model.- The full framework including the retriever, corpus, and language model can directly leverage existing models without fine-tuning, for flexible zero-shot generalization.So in summary, the central hypothesis is that retrieval augmentation is an effective and efficient alternative strategy for zero-shot video QA compared to prior cross-modal training or captioning-based paradigms. The modular R2A framework is introduced to validate this idea.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a novel framework called Retrieving-to-Answer (R2A) for zero-shot video question answering, which avoids the limitations of previous approaches like cross-modal alignment training or reliance on video captioning models. 2. Introducing an efficient and flexible R2A framework that can leverage readily available pre-trained models (e.g. CLIP and DeBERTa) without any fine-tuning, while still achieving state-of-the-art performance on multiple benchmarks.3. Demonstrating through extensive experiments that R2A outperforms prior work like Flamingo and VidIL, even with significantly fewer parameters and no cross-modal training. Specifically, R2A with only 1.3B parameters beats the 80B parameter Flamingo model.4. Showcasing the potential of using retrieval augmentation and large text corpora to help transfer the powerful capabilities of language models like DeBERTa to multimodal tasks like VideoQA in a zero-shot manner.5. Providing an analysis of different design choices related to components like retrieval models, text corpora, number of retrieved captions etc. and their impact on R2A's zero-shot VideoQA performance.In summary, the key innovation seems to be proposing and validating the idea of using efficient semantic text retrieval to help bridge vision and language modalities for zero-shot VideoQA, instead of cross-modal training or caption generation. The simplicity, efficiency, flexibility and strong performance of the R2A framework seem to be the major contributions demonstrated through comprehensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Retrieving-to-Answer (R2A) framework for efficient zero-shot video question answering, which retrieves semantically similar text from a large corpus to provide contextual information to a frozen pretrained language model to generate answers, achieving state-of-the-art performance without costly training.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other research in video question answering:- This paper proposes a novel framework called Retrieving-to-Answer (R2A) for zero-shot video question answering, which relies on retrieving semantically similar text from a large corpus instead of training cross-modal alignments or using video captioning. This is a new direction compared to most prior work.- Many existing methods require expensive cross-modal training between vision and language models on large datasets. In contrast, R2A requires no training and can directly leverage off-the-shelf pretrained models like CLIP and DeBERTa. This makes it more flexible and efficient.- Other recent works have adopted a language-based adaptation approach using caption models. However, they rely on caption models finetuned to the target domain, making them less generalizable. R2A retrieves text from a generic corpus, avoiding this issue.- R2A achieves new state-of-the-art results on multiple VideoQA benchmarks under the zero-shot setting. Notably, it outperforms the heavily trained Flamingo model with 80B parameters, despite having only 1.3B parameters itself.- The retrieval process in R2A is very fast and scalable compared to alternatives like video captioning. This is because the text corpus can be encoded offline in advance. - R2A is highly modular by design. All components like the retriever, language model, and text corpus can be easily swapped and upgraded without retraining.In summary, R2A pioneers a new retrieval-based paradigm for zero-shot VideoQA that is simpler, more efficient, and achieves better performance than prior training-based and captioning-based methods. The design is highly flexible and extendable.
