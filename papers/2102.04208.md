# [Contrastive Embeddings for Neural Architectures](https://arxiv.org/abs/2102.04208)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop neural architecture embeddings that are independent of the parameterization of the search space and capture meaningful information about the architectures' intrinsic properties and performance?The key ideas and contributions in addressing this question appear to be:- Using contrastive learning on the extended projected data Jacobian matrices (EPDJMs) of architectures at initialization to learn embeddings that group similar architectures close together. This allows creating embeddings without relying on the architectures' parametrizations. - Showing that the contrastive embeddings evolve meaningfully during training and connect regions of architectures with similar final performance. This suggests the embeddings capture intrinsic properties related to performance.- Demonstrating that the contrastive embeddings enable traditional black-box optimization algorithms to achieve strong performance on NAS benchmarks like NAS-Bench-201, indicating they capture useful structure.- Leveraging the parameterization-independent nature of the embeddings to enable transfer learning across different search spaces, which has not been shown before.So in summary, the central hypothesis is that contrastive learning on intrinsic architecture properties like the EPDJM can produce useful embeddings for neural architecture search that do not depend on the search space parameterization. The results seem to validate this hypothesis and show the benefits of such embeddings.
