# [Contrastive Embeddings for Neural Architectures](https://arxiv.org/abs/2102.04208)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop neural architecture embeddings that are independent of the parameterization of the search space and capture meaningful information about the architectures' intrinsic properties and performance?The key ideas and contributions in addressing this question appear to be:- Using contrastive learning on the extended projected data Jacobian matrices (EPDJMs) of architectures at initialization to learn embeddings that group similar architectures close together. This allows creating embeddings without relying on the architectures' parametrizations. - Showing that the contrastive embeddings evolve meaningfully during training and connect regions of architectures with similar final performance. This suggests the embeddings capture intrinsic properties related to performance.- Demonstrating that the contrastive embeddings enable traditional black-box optimization algorithms to achieve strong performance on NAS benchmarks like NAS-Bench-201, indicating they capture useful structure.- Leveraging the parameterization-independent nature of the embeddings to enable transfer learning across different search spaces, which has not been shown before.So in summary, the central hypothesis is that contrastive learning on intrinsic architecture properties like the EPDJM can produce useful embeddings for neural architecture search that do not depend on the search space parameterization. The results seem to validate this hypothesis and show the benefits of such embeddings.


## What is the main contribution of this paper?

The main contributions of this paper are:- Developing a method to produce neural architecture embeddings using contrastive learning on the data Jacobians, that does not depend on the parametrization of the search space.- Showing that the contrastive embeddings evolve during training in a way that connects areas of the search space with similar final performance.- Using traditional black-box optimization algorithms like Bayesian optimization with the contrastive embeddings to achieve good performance on NAS benchmarks.- Demonstrating that the contrastive embeddings provide a unified space across different search spaces, enabling transfer learning between them for the first time.In summary, the key contribution is using contrastive learning on the data Jacobians to develop parametrization-independent and unified architecture embeddings that capture meaningful properties about the performance of networks. These embeddings enable effective architecture search and transfer learning across search spaces with simple black-box optimization algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using contrastive learning on data Jacobians of neural networks at initialization to learn architecture embeddings independent of parametrization, enabling the use of traditional black-box optimization algorithms and transfer learning for neural architecture search.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of neural architecture search:- The key novelty of this paper is using contrastive learning to learn embeddings for neural network architectures that are independent of the search space parametrization. Most prior work on neural architecture search has relied on embeddings that are dependent on the specifics of how the search space is encoded. By making the embeddings invariant to the search space, this opens up new possibilities like transfer learning across search spaces.- The idea of using properties of neural networks at initialization for architecture search has been explored before, such as in the work on Neural Network Gaussian Processes and Data-Free Knowledge Distillation for Neural Architecture Search. However, this paper takes that idea further by using contrastive learning on the data Jacobian matrices to learn an embedding space.- Using black-box optimization algorithms like Bayesian optimization for architecture search is common, but the novel architecture embeddings enable even basic algorithms to achieve strong performance without much tuning on established benchmarks like NAS-Bench 201.- Demonstrating transfer learning between search spaces is an impressive result enabled by the unified embedding space. This ability to leverage knowledge across different search spaces has not been shown before to my knowledge.- The analysis of how the embeddings evolve during training provides interesting insights into the training dynamics. The idea of using this information to enable more efficient search is promising for future work.Overall, I would summarize that the key novelty is in using contrastive learning on data Jacobians to obtain search space-independent architecture embeddings. This is an elegant idea that opens up new capabilities like cross-search space transfer learning and improved performance of standard architecture search algorithms. The results convincingly demonstrate the value of this approach to neural architecture search.
