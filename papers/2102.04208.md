# [Contrastive Embeddings for Neural Architectures](https://arxiv.org/abs/2102.04208)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop neural architecture embeddings that are independent of the parameterization of the search space and capture meaningful information about the architectures' intrinsic properties and performance?The key ideas and contributions in addressing this question appear to be:- Using contrastive learning on the extended projected data Jacobian matrices (EPDJMs) of architectures at initialization to learn embeddings that group similar architectures close together. This allows creating embeddings without relying on the architectures' parametrizations. - Showing that the contrastive embeddings evolve meaningfully during training and connect regions of architectures with similar final performance. This suggests the embeddings capture intrinsic properties related to performance.- Demonstrating that the contrastive embeddings enable traditional black-box optimization algorithms to achieve strong performance on NAS benchmarks like NAS-Bench-201, indicating they capture useful structure.- Leveraging the parameterization-independent nature of the embeddings to enable transfer learning across different search spaces, which has not been shown before.So in summary, the central hypothesis is that contrastive learning on intrinsic architecture properties like the EPDJM can produce useful embeddings for neural architecture search that do not depend on the search space parameterization. The results seem to validate this hypothesis and show the benefits of such embeddings.


## What is the main contribution of this paper?

The main contributions of this paper are:- Developing a method to produce neural architecture embeddings using contrastive learning on the data Jacobians, that does not depend on the parametrization of the search space.- Showing that the contrastive embeddings evolve during training in a way that connects areas of the search space with similar final performance.- Using traditional black-box optimization algorithms like Bayesian optimization with the contrastive embeddings to achieve good performance on NAS benchmarks.- Demonstrating that the contrastive embeddings provide a unified space across different search spaces, enabling transfer learning between them for the first time.In summary, the key contribution is using contrastive learning on the data Jacobians to develop parametrization-independent and unified architecture embeddings that capture meaningful properties about the performance of networks. These embeddings enable effective architecture search and transfer learning across search spaces with simple black-box optimization algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using contrastive learning on data Jacobians of neural networks at initialization to learn architecture embeddings independent of parametrization, enabling the use of traditional black-box optimization algorithms and transfer learning for neural architecture search.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of neural architecture search:- The key novelty of this paper is using contrastive learning to learn embeddings for neural network architectures that are independent of the search space parametrization. Most prior work on neural architecture search has relied on embeddings that are dependent on the specifics of how the search space is encoded. By making the embeddings invariant to the search space, this opens up new possibilities like transfer learning across search spaces.- The idea of using properties of neural networks at initialization for architecture search has been explored before, such as in the work on Neural Network Gaussian Processes and Data-Free Knowledge Distillation for Neural Architecture Search. However, this paper takes that idea further by using contrastive learning on the data Jacobian matrices to learn an embedding space.- Using black-box optimization algorithms like Bayesian optimization for architecture search is common, but the novel architecture embeddings enable even basic algorithms to achieve strong performance without much tuning on established benchmarks like NAS-Bench 201.- Demonstrating transfer learning between search spaces is an impressive result enabled by the unified embedding space. This ability to leverage knowledge across different search spaces has not been shown before to my knowledge.- The analysis of how the embeddings evolve during training provides interesting insights into the training dynamics. The idea of using this information to enable more efficient search is promising for future work.Overall, I would summarize that the key novelty is in using contrastive learning on data Jacobians to obtain search space-independent architecture embeddings. This is an elegant idea that opens up new capabilities like cross-search space transfer learning and improved performance of standard architecture search algorithms. The results convincingly demonstrate the value of this approach to neural architecture search.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing methods to further improve the encoding of neural architectures. The authors state that existing search methods could benefit from advances in producing better architecture embeddings. They suggest cross-pollination between contrastive learning, black-box optimization, and neural architecture search to improve embeddings.- Analyzing and utilizing the evolution of embeddings during training. The authors show that embeddings change during training in a way that connects areas of the search space with similar final performance. They suggest this trajectory information could enable more efficient search methods.- Improving transfer learning across search spaces. The authors demonstrate transfer learning between search spaces is possible with their unified embedding space. They suggest this is a promising new research direction to learn universal properties of networks across search spaces.- Applying contrastive embeddings to additional search spaces and tasks. The authors tested their method on a limited set of image classification benchmarks. Applying it to other domains like natural language processing or a wider range of search spaces could demonstrate the general utility of the approach.- Using automated hyperparameter optimization methods for the search algorithms. The authors manually tuned hyperparameters but suggest automated methods like optimizing the GP marginal likelihood could improve performance.In summary, the main directions are improving embeddings, analyzing embedding trajectories, transfer learning across spaces, expanding to new applications, and automating hyperparameter tuning. The overall emphasis is on developing embeddings to capture intrinsic properties of architectures independently of search space parametrization.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a new method to generate neural architecture embeddings using contrastive learning on the extended projected data Jacobian matrices (EPDJMs) of architectures at initialization. The contrastive model is trained to produce similar embeddings for different initializations of the same architecture, while producing dissimilar embeddings for different architectures. This allows the creation of architecture embeddings independent of the search space parametrization. The authors show that traditional black-box optimization algorithms can effectively use these embeddings to perform neural architecture search, reaching state-of-the-art performance on NAS-Bench-201. Since the embeddings are unified across search spaces, transfer learning is demonstrated between the size and topology search spaces of NATS-Bench. The evolution of embeddings during training is also analyzed, suggesting their potential to provide insights into networks' training dynamics. Overall, the work introduces a novel embedding technique using intrinsic architecture properties and contrastive learning, with promising results for neural architecture search and analysis.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a novel method to generate architecture embeddings for neural networks using contrastive learning. The key idea is to leverage the extended data Jacobian matrix (EDJM) of architectures at initialization to train a contrastive model that identifies architectures with similar performance. Specifically, the EDJM of an architecture captures intrinsic properties related to its model capacity and is invariant to the parametrization of the search space. The contrastive model is trained on different initializations of architectures to produce similar embeddings for the same architecture and dissimilar embeddings for different architectures. The contrastive embeddings enable the use of traditional black-box optimization algorithms like Bayesian optimization to effectively perform architecture search. Experiments on NAS-Bench-201 and NATS-Bench demonstrate that the learned embeddings contain valuable information about architecture performance, significantly outperforming random search. Uniquely, the embeddings provide a unified space for architectures across different search spaces. This enables transfer learning between search spaces, as shown by training on one space and evaluating on another space in NATS-Bench. Overall, the work presents a novel perspective focused on learning meaningful architecture embeddings rather than specialized search algorithms.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes using contrastive learning to identify networks across different initializations based on their data Jacobians, and automatically produce architecture embeddings independent of the parametrization of the search space. The authors compute the Extended Data Jacobian Matrix (EDJM) of networks at initialization and use a low-rank projection of it as input to a contrastive network trained with the SimCLR algorithm. This results in embeddings that group together networks with similar performance while separating those with different performance. The authors then use these contrastive embeddings with traditional black-box Bayesian optimization algorithms to perform neural architecture search, achieving state-of-the-art performance. Since the embeddings are independent of search space, the authors also perform transfer learning between different search spaces for the first time.


## What problem or question is the paper addressing?

The main goal of this paper is to develop a method to produce meaningful embeddings of neural network architectures that can be used for neural architecture search. The key ideas and contributions are:- Develop a technique to embed neural architectures in a way that is independent of the parametrization of the search space. This is done by using contrastive learning on statistics computed from the architecture at initialization (specifically the Extended Projected Data Jacobian Matrix).- Show that the embeddings evolve during training in a way that connects areas of the search space with similar final performance. This suggests the embeddings could enable more efficient architecture search methods. - Demonstrate that traditional black-box optimization algorithms can effectively use the embeddings for architecture search on NAS-Bench-201, achieving performance competitive with specialized NAS algorithms.- Perform transfer learning between different search spaces by using the unified embedding space provided by the method. This is shown by training on one NATS-Bench search space and predicting performance in the other.So in summary, the key focus is developing parametrization-independent architecture embeddings using contrastive learning on initialization statistics, and showing this enables effective architecture search and transfer learning between search spaces. The main impact is in improving architecture embeddings rather than developing new specialized NAS algorithms.
