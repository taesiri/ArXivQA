# [Manipulating Neural Path Planners via Slight Perturbations](https://arxiv.org/abs/2403.18256)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural path planners are being increasingly used in robotics, but their neural network components come as black boxes, making them vulnerable to attacks. Specifically, attackers could inject hidden "backdoors" that when triggered cause the planner to exhibit unwanted malicious behaviors.
- For example, a delivery robot could be hijacked to go to the wrong destination, get trapped in a region, or waste energy.
- Such threats in safety-critical applications like autonomous vehicles could have severe consequences, but backdoor attacks remain relatively unexplored for neural path planners.

Proposed Solution:
- The paper proposes a novel approach to specify, inject, and identify backdoors in neural path planners. Both sampling-based (like RRT) and search-based (like A*) planners are considered.
- To specify behaviors, a context-free grammar is introduced with operators like "reach", "avoid", "stay" that can encode complex spatial-temporal constraints.
- To inject backdoors, the behaviors are made differentiable and optimized via gradients during training. Backdoors are inserted either by controlling training or poisoning small portions of the dataset.
- To defend against backdoors, the paper analyzes fine-tuning and trigger inversion. Fine-tuning is found insufficient to remove backdoors, while trigger inversion can effectively identify them if attack objectives are known.

Main Contributions:
- Demonstrating the feasibility of specifying and injecting a range of backdoor behaviors into neural path planners that can be triggered via slight perturbations.
- Showing properties like high trigger rates, persistence across environments, and low performance impact that make these attacks practical. 
- Analyzing limitations of defense strategies like fine-tuning for neural path planners and highlighting the need for more research into safety and reliability.
- Overall, bringing attention to the unexplored risks neural path planners face from backdoor attacks in safety-critical domains.

In summary, the paper conducts the first comprehensive study of backdoor threats in neural path planning, proposes methods to realize attacks, and calls for more future work to address such vulnerabilities.
