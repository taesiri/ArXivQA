# [Large-scale Vision-Language Models Learn Super Images for Efficient and   High-Performance Partially Relevant Video Retrieval](https://arxiv.org/abs/2312.00414)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes an efficient and high-performance method for partially relevant video retrieval (PRVR) by combining super images and large-scale vision-language models (VLMs). Super images are created by rearranging video frames in a grid layout, which reduces the number of visual encodings needed. This enables the use of computationally expensive but high-quality VLMs as encoders. The proposed query-attentive super image retrieval (QASIR) approach demonstrates strong zero-shot performance compared to prior methods, and further improvements when fine-tuned. Key insights include: 1) VLMs effectively generalize to represent super images for PRVR, 2) grid size and image resolution allow tradeoffs between performance and computation cost, and 3) choice of VLM significantly impacts overall performance. When combined with super images, large VLMs strike an effective balance between efficiency and state-of-the-art retrieval accuracy on PRVR benchmarks. The simplicity yet effectiveness of this approach provides a strong baseline for further research into efficient video encoding strategies for VLMs.
