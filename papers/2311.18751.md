# [Language Model Agents Suffer from Compositional Generalization in Web   Automation](https://arxiv.org/abs/2311.18751)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces a new benchmark called CompWoB for evaluating the compositional generalization capability of language model agents (LMAs) on web automation tasks. CompWoB systematically combines 50 base tasks from MiniWoB into more complex compositional tasks involving 2-8 base tasks. The authors evaluate several state-of-the-art LMAs on CompWoB in a zero-shot setting, where the LMAs were only trained on the base tasks. They find that while prompted LMAs like RCI perform very well on MiniWoB (90.6% success rate), their performance significantly degrades on CompWoB (down to 24.9%). In contrast, smaller finetuned LMAs exhibit better compositional generalization, only dropping from 85.4% on MiniWoB to 54.8% on CompWoB. The authors' proposed data-balanced finetuned LMA, HTML-T5++, performs the best on CompWoB with 61.5% success rate while matching human performance on MiniWoB. Overall, the paper highlights that existing LMAs still struggle with generalizing to compositional tasks and complex instructions, motivating building more robust LMAs for real-world deployment. Key results include the performance drop of prompted vs finetuned LMAs on CompWoB and the introduction of the new compositional benchmark to measure this.


## Summarize the paper in one sentence.

 This paper introduces a new benchmark called CompWoB to evaluate the compositional generalization capability of language model agents in web automation tasks, finding that existing prompted and finetuned LMAs struggle to handle task compositionality and complex instructions compared to their performance on base tasks.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Empirically showing that prompted LMAs even with GPT-4 suffer more from generalizing to compositional web automation tasks than transferred LMAs. Also showing that LMAs are highly sensitive to the order of instructions.

2) Developing "CompWoB", simulated web environments for LMAs to measure generalization to realistic task compositionality and complex instructions. 

3) Proposing a new data mixture strategy for finetuning LMAs. The HTML-T5++ model trained with this strategy achieves human-level performance on MiniWoB (95.2%) and the best zero-shot transfer performance on CompWoB (61.5%) among the LMAs.

So in summary, the main contributions are:
1) Evaluating LMA generalization to compositional tasks 
2) Introducing the CompWoB benchmark
3) Proposing the HTML-T5++ model with improved finetuning

The key conclusion is that while LMAs show promise, building robust and generalizable LMAs that can handle task compositionality remains an open challenge for real-world deployment. The paper analyzes the factors behind this and provides the CompWoB benchmark to facilitate further progress.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Language model agents (LMAs)
- Web automation
- Compositional generalization
- Task compositionality
- MiniWoB
- CompWoB (Compositional MiniWoB)
- Prompted LMAs (e.g. RCI, AdaPlanner, Synapse)
- Transferred LMAs (finetuned LMAs)
- Reverse-order instructions
- Task complexity
- Instruction length
- HTML subtree depth

The paper introduces a new benchmark called "CompWoB" to test the compositional generalization capabilities of various language model agents on web automation tasks. It shows that existing state-of-the-art prompted LMAs struggle to generalize to novel compositions of tasks seen during training. In contrast, transferred LMAs which are finetuned on base tasks perform better on the compositional tasks. The paper also analyzes the impact of factors like reverse-order instructions, model scale, and task complexity on the agents' performance. Overall, it highlights the need to build LMAs that are more robust and generalizable for real-world deployment.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a new benchmark called CompWoB for evaluating the compositional generalization capability of language model agents. What are the key ideas behind the design of CompWoB and what makes it better suited to test compositional generalization compared to existing benchmarks like MiniWoB?

2. The paper shows that prompted language model agents like RCI struggle on CompWoB even though they perform very well on MiniWoB. What factors contribute to their poor compositional generalization capability and how can these agents be improved? 

3. Transferred language model agents seem to generalize better on CompWoB compared to prompted agents. Why might this be the case? What differences in the training methodology allow for better generalization on novel compositional tasks?

4. The authors propose a new data balancing strategy during finetuning that leads to improved performance of HTML-T5++ on both MiniWoB and CompWoB. Explain this data balancing strategy and discuss why it helps with generalization. 

5. The paper analyzes the impact of instruction complexity by testing performance on reverse order instructions. Why do both prompted and transferred LM agents struggle on these tasks? What capabilities are lacking?

6. The authors evaluate RCI with GPT-4 and oracle exemplars, showing strong performance on CompWoB. While promising, discuss the limitations of this approach and why it may not translate to good real-world performance.

7. Explain the differences in correlations observed between success rate of LM agents and various task complexity measures like number of HTML tokens, HTML tree depth etc. What inferences can be made about the agents' capabilities based on this analysis?

8. How suitable are the base tasks chosen for CompWoB for evaluating compositional generalization? Critically analyze if better sets of base tasks could be designed and justify your proposal.

9. The paper focuses its analysis and experiments exclusively on simulated web environments. Discuss challenges that may arise in real world web automation and how the conclusions may change.

10. The key conclusion is that existing LM agents lack robustness to compositional tasks. Propose 3-4 new research directions that can address this limitation and discuss their merits or potential pitfalls.
