# [NoMAD-Attention: Efficient LLM Inference on CPUs Through   Multiply-add-free Attention](https://arxiv.org/abs/2403.01273)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language model (LLM) inference is very expensive on CPUs due to the vast quantities of multiply-add (MAD) operations required for attention mechanism computations. 
- Attention computations scale quadratically with sequence length, quickly overwhelming CPU compute capabilities.
- CPUs have limited parallel compute cores and are inefficient for highly repetitive workloads like attention. Thus attention computations become a key bottleneck.

Solution: 
- The paper proposes NoMAD-Attention, which eliminates most MAD operations in attention through fast in-register lookups by exploiting the SIMD registers in CPUs.
- NoMAD-Attention uses product quantization to transform dot products in attention to memory lookups. It constrains codebook sizes to allow lookup tables (LUTs) to fit into SIMD registers for fast access.
- The key cache memory layout is reorganized into a transposed, blocked format to enable batch parallel lookups using SIMD shuffle instructions.  

Key Contributions:
- Proposes NoMAD-Attention to replace expensive MAD operations in attention with fast in-register LUT lookups, enabling efficient CPU inference.
- Achieves up to 2x speedup on 4-bit quantized LLaMA-7B model at 16k context length without loss of accuracy.
- First work to accelerate attention computations by shifting compute paradigm from MAD to lookup using SIMD registers.
- Hardware-aware memory layout reorganization and codebook design to overcome limitations of SIMD registers.
- Compatible with pre-trained attention-based transformers without model fine-tuning.

In summary, the paper makes LLMs more accessible by enabling their efficient deployment on prevalent CPU hardware through a novel MAD-free attention mechanism designed specifically for efficient in-register lookup computation.
