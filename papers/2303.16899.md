# [AutoAD: Movie Description in Context](https://arxiv.org/abs/2303.16899)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an automatic audio description (AD) model that generates high quality descriptions of movies in a contextualized way?Specifically, the paper aims to address the following key challenges in movie AD generation:1. Movie AD is heavily dependent on context - visual context from the movie frames, previous AD context, and subtitle context. How can an AD model effectively leverage these different types of context?2. There is a lack of large-scale training data for movie AD generation. How can the model be trained when some types of data (e.g. visual frames) are missing or limited? 3. Existing movie AD datasets contain noise and errors. How can higher quality training data be obtained?To address these challenges, the main hypothesis is that an effective movie AD model can be developed by:1. Incorporating visual, previous AD and subtitle context using foundation models like GPT and CLIP.2. Pretraining components of the model on partial data where certain types of data may be missing.3. Creating higher quality AD datasets by cleaning existing ones and collecting new narrations.The paper presents an AutoAD model pipeline and experiments that aim to validate this hypothesis and show improved movie AD generation capabilities over previous approaches.


## What is the main contribution of this paper?

The main contribution of this paper is developing an automatic Audio Description (AD) model called AutoAD that can generate high-quality text descriptions for movies to aid the visually impaired. The key aspects are:- Incorporating various types of context into the model, including visual context from movie frames, textual context from previous AD sentences, and context from subtitles. This allows the model to generate more coherent and story-following descriptions.- Addressing the lack of training data by pretraining components of the model (e.g. visual encoder, language model) on large datasets where only partial data is available. For example, pretraining the language model on text-only AD data.- Creating a cleaned version of the existing Movie Audio Description (MAD) dataset called MAD-v2 by removing label noise and adding character names. This improves results when training on MAD.- Achieving strong audio description generation results compared to prior works, even zero-shot on some datasets, highlighting the benefits of leveraging context and pretraining.In summary, the main contribution is developing an end-to-end pipeline (AutoAD) to automatically generate high-quality and context-aware audio descriptions for movies to help the visually impaired, enabled by effectively incorporating various contextual signals and pretraining strategies.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:This paper proposes an automatic audio description (AD) model called AutoAD that generates movie descriptions by incorporating visual, subtitle, and previous AD context through adapters bridging frozen CLIP and GPT models; it also provides new training data by cleaning the MAD dataset annotations and collecting a large text-only AD dataset from AudioVault.
