# [AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning](https://arxiv.org/abs/2103.16002)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we develop new benchmarks to effectively measure compositional spatio-temporal reasoning capabilities in vision systems through question answering?The key points are:- Existing video QA benchmarks conflate multiple error sources into one accuracy metric and have biases models can exploit. They do not allow detailed analysis of model capabilities and weaknesses. - The authors introduce a new benchmark called AGQA that contains a large and balanced set of QA pairs for real-world videos. - AGQA's questions are generated using handcrafted programs over spatio-temporal scene graphs. This gives granular control over question composition and reasoning requirements.- AGQA introduces new train/test splits to specifically test for compositional generalization abilities like novel concept compositions, indirect references, and multi-step reasoning.- Experiments show current models struggle on AGQA. The best model only achieves 47.74% accuracy compared to 86.02% human accuracy. Models do not generalize on the compositional splits.So in summary, the central hypothesis is that new benchmarks like AGQA are needed to properly measure and drive progress on compositional spatio-temporal reasoning in vision systems. The paper presents evidence that current models are still lacking in these skills.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper seem to be:- Introducing AGQA, a new large-scale benchmark for compositional spatio-temporal reasoning in videos. The benchmark contains 192M unbalanced and 3.9M balanced question-answer pairs over 9.6K real-world videos.- Providing granular control over the question generation process through handcrafted programs and templates. This allows explicit testing of different reasoning abilities like generalization to novel compositions, indirect references, and multi-step reasoning.- Creating new training/test splits and metrics to specifically measure compositional generalization abilities, including to novel compositions, indirect references, and more steps.- Evaluating modern visual reasoning systems (PSAC, HME, HCRN) on AGQA and finding they struggle compared to humans, barely outperforming non-visual baselines. None of the models generalize to novel compositions.- Analyzing model performance across different reasoning skills, semantics, structures. Finding accuracy decreases as compositional complexity increases.- Providing insights into the limitations of current models and suggesting future directions like neuro-symbolic methods, meta-learning, memory networks to improve compositional reasoning.In summary, the key contribution seems to be introducing a large-scale benchmark to explicitly measure compositional spatio-temporal reasoning in a granular way, evaluating current models, and showing they are still far from human performance. The benchmark and analysis provide insights to drive progress on compositional video understanding.
