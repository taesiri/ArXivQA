# [AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning](https://arxiv.org/abs/2103.16002)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we develop new benchmarks to effectively measure compositional spatio-temporal reasoning capabilities in vision systems through question answering?The key points are:- Existing video QA benchmarks conflate multiple error sources into one accuracy metric and have biases models can exploit. They do not allow detailed analysis of model capabilities and weaknesses. - The authors introduce a new benchmark called AGQA that contains a large and balanced set of QA pairs for real-world videos. - AGQA's questions are generated using handcrafted programs over spatio-temporal scene graphs. This gives granular control over question composition and reasoning requirements.- AGQA introduces new train/test splits to specifically test for compositional generalization abilities like novel concept compositions, indirect references, and multi-step reasoning.- Experiments show current models struggle on AGQA. The best model only achieves 47.74% accuracy compared to 86.02% human accuracy. Models do not generalize on the compositional splits.So in summary, the central hypothesis is that new benchmarks like AGQA are needed to properly measure and drive progress on compositional spatio-temporal reasoning in vision systems. The paper presents evidence that current models are still lacking in these skills.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper seem to be:- Introducing AGQA, a new large-scale benchmark for compositional spatio-temporal reasoning in videos. The benchmark contains 192M unbalanced and 3.9M balanced question-answer pairs over 9.6K real-world videos.- Providing granular control over the question generation process through handcrafted programs and templates. This allows explicit testing of different reasoning abilities like generalization to novel compositions, indirect references, and multi-step reasoning.- Creating new training/test splits and metrics to specifically measure compositional generalization abilities, including to novel compositions, indirect references, and more steps.- Evaluating modern visual reasoning systems (PSAC, HME, HCRN) on AGQA and finding they struggle compared to humans, barely outperforming non-visual baselines. None of the models generalize to novel compositions.- Analyzing model performance across different reasoning skills, semantics, structures. Finding accuracy decreases as compositional complexity increases.- Providing insights into the limitations of current models and suggesting future directions like neuro-symbolic methods, meta-learning, memory networks to improve compositional reasoning.In summary, the key contribution seems to be introducing a large-scale benchmark to explicitly measure compositional spatio-temporal reasoning in a granular way, evaluating current models, and showing they are still far from human performance. The benchmark and analysis provide insights to drive progress on compositional video understanding.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on Action Genome Question Answering (AGQA) compares to other related research on video question answering benchmarks:- Dataset Size: AGQA contains 192 million unbalanced and 3.9 million balanced video-question-answer triplets, which is 3 orders of magnitude larger than prior video QA datasets that contain thousands to hundreds of thousands of examples. The large scale allows more robust training and evaluation.- Realism: AGQA is based on real-world videos from the Charades dataset, as opposed to synthetic videos in some other benchmarks. This provides more diversity and complexity. - Compositional Reasoning: The questions in AGQA explicitly test compositional reasoning about spatio-temporal events involving objects, relationships, and actions. Other video QA benchmarks often have simpler questions.- Question Types: AGQA has more variety in complex question types testing different forms of reasoning and semantics. Other benchmarks tend to have narrower question distributions.- Bias Reduction: AGQA's creators tried to reduce dataset bias via balancing of the answer distributions and question structures. Many other benchmarks have exploitable biases.- Evaluation: AGQA proposes new evaluation metrics and splits for compositional generalization, indirect references, and multi-step reasoning. Most other benchmarks use single train/test splits and accuracy metrics.- Performance: State-of-the-art video QA models only achieve 47-48% accuracy on AGQA, demonstrating it poses a difficult challenge. Performance on other benchmarks is often higher.In summary, AGQA advances the field of video QA research by providing a large-scale, realistic, compositional benchmark with less bias and more rigorous evaluation than prior datasets. The paper shows current models still struggle on this benchmark, motivating future work.
