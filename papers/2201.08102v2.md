# [Safe Deep RL in 3D Environments using Human Feedback](https://arxiv.org/abs/2201.08102v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Is the ReQueST approach feasible for safe deep reinforcement learning in complex 3D environments, using only human-provided data (for both the dynamics and reward models)?

In particular, the key questions examined are:

1) Can a pixel-based dynamics model of sufficient quality be learned from human exploratory trajectories in a complex 3D environment? 

2) Are the human data requirements viable, in terms of both quantity (amount of data needed) and quality (whether contractors can provide useful data)?

The paper aims to demonstrate that the answer is yes to both of these questions. It shows that ReQueST can be used to train competent agents with much less unsafe behavior compared to standard RL, using human trajectories and reward sketches as the sole data sources. The dynamics and reward models are successfully trained on feasible amounts of imperfect, real-world human data.

So in summary, the central hypothesis is that the ReQueST framework can enable safe deep RL using only human data, even for visually complex 3D environments. The paper provides evidence for this through empirical results on a 3D first-person object collection task.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Demonstrating that the ReQueST approach for safe deep reinforcement learning is feasible in complex 3D environments, using data sourced entirely from human contractors. Previous work on ReQueST had only been done in simple 2D environments.

2. Showing that effective dynamics and reward models can be learned from reasonable amounts of human data - 160 person-hours of safe exploratory trajectories and 10 person-hours of reward sketches in their experiments. 

3. Training an agent on a 3D first-person object collection task using ReQueST that exhibits an order of magnitude fewer instances of unsafe behavior compared to a standard reinforcement learning algorithm.

4. Providing an analysis of the data requirements, scaling properties, and failure modes when applying ReQueST, as well as comparisons to baseline approaches.

In summary, the main contribution is demonstrating that the ReQueST methodology can enable safe deep reinforcement learning in complex 3D environments using only data provided by human contractors, with reasonable data requirements. This helps establish ReQueST as a general-purpose and practical approach to safe exploration in reinforcement learning.
