# [Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?](https://arxiv.org/abs/2401.00414)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Face forgery detection models based on deep neural networks have shown remarkable performance in discriminating between real and artificially generated fake faces. However, their security is significantly threatened by backdoor attacks which inject triggers during model training. Existing backdoor attacks mainly use visible triggers like patches or perturbations, which can be defended against. More challenging natural backdoor triggers that utilize semantic image features remain under-explored. 

Proposed Solution:
The paper proposes a novel backdoor attack that embeds natural triggers in the latent space of generative models. The attack is studied from two perspectives:

1. Model Discrimination: An optimization method finds triggers by minimizing the loss of classifying generated poisoned images as real on a substitute model.

2. Data Distribution: It manipulates facial attributes in the long tail of training data distribution to create triggers. Both single attribute (e.g. smile) and multiple attributes (e.g. smile and age) are edited to generate custom triggers.

The triggers are embedded in the latent codes of StyleGAN and Stable Diffusion to create poisoned samples. After model training with poisoned data, images generated with triggers bypass detection while images without triggers are still correctly classified.

Main Contributions:

- Proposal of a novel natural backdoor attack against face forgery detection by embedding triggers in latent space
- Demonstration of higher attack success rate, lower detectability, and more robustness compared to existing attacks
- Revealing the vulnerability of face forgery detection models to backdoor attacks with natural triggers
- Extension of the attack framework to both GANs (StyleGAN) and diffusion models (Stable Diffusion)

The attack causes negligible accuracy drop on clean data while achieving over 99% attack success rate with less than 3% poisoning. It is also more resistant to defenses and less perceptible than existing attacks. The work thoroughly evaluates and reveals the vulnerability of face forgery detection systems.
