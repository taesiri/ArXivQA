# [Prefix Conditioning Unifies Language and Label Supervision](https://arxiv.org/abs/2206.01125)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: How can we effectively combine image-caption and image-label (classification) data for vision-language pretraining, in order to improve downstream zero-shot recognition performance? 

The key hypothesis is that simply combining the two datasets naively can negatively affect pretraining, due to differences in image domain coverage and vocabulary between the datasets. The paper proposes a new method called "Prefix Conditioning" to help disentangle the dataset biases and allow the model to learn in a mode tailored to each dataset, while still sharing knowledge between them. The hypothesis is that this approach will improve zero-shot recognition accuracy and robustness compared to naive dataset combination.

In summary, the paper introduces Prefix Conditioning to address the core problem of how to effectively unify image-caption and image-label supervision for better zero-shot recognition performance. The key hypothesis is that Prefix Conditioning can disentangle dataset biases and improve generalization.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Prefix Conditioning to unify image classification and image captioning datasets for better vision-language pre-training. The key ideas are:

- Prefix Conditioning prepends a learnable prefix token to the input text to indicate whether it comes from an image classification or captioning dataset. This allows the language encoder to disentangle dataset biases and switch between classification-focused and captioning-focused modes.

- At training time, the prefix tokens allow the model to share knowledge between datasets while handling their differences. At test time, using the caption prefix results in better generalization for zero-shot recognition tasks. 

- Experiments show Prefix Conditioning improves zero-shot recognition performance by over 6% on average when training on ImageNet-21K and CC12M datasets. The method is generic and can be combined with existing objectives like CLIP or UniCL.

- Analysis shows the prefix tokens change how the language encoder attends to the input text, focusing more on class names with the classification prefix vs. full sentences with the caption prefix. This results in more robust and generalizable embeddings.

In summary, Prefix Conditioning is a simple but effective way to unify diverse vision-language datasets through learnable conditioning tokens, improving generalization and zero-shot recognition performance. The key insight is handling dataset biases by dynamically switching the language encoder's mode.
