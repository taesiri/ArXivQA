# [Questions Are All You Need to Train a Dense Passage Retriever](https://arxiv.org/abs/2206.10658)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can an accurate and robust passage retriever be trained using only questions, without requiring labeled question-passage pairs or hard negatives?The key hypothesis is that a passage retriever can be trained in an unsupervised manner by using a pre-trained language model to provide "soft labels" on how relevant retrieved passages are for reconstructing the original question.In particular, the paper proposes and evaluates a new approach called ART (Autoencoding Retriever Training) which trains the passage retriever based on maximizing question reconstruction likelihood from the retrieved passages. This allows for unsupervised training of the retriever without needing annotated training data.The main goal is to show that the proposed ART method can match or exceed the performance of supervised methods like DPR on passage retrieval benchmarks, despite using only questions during training. The paper aims to demonstrate the effectiveness of this unsupervised autoencoding approach for training retrievers.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ART, a new unsupervised approach for training dense passage retrievers using only questions. The key ideas are:- ART uses an autoencoding framework where a question is encoded to retrieve passages, and then the passages are decoded to reconstruct the original question. - A pre-trained language model is used to score passage relevance for question reconstruction, providing soft supervision signals.- The retriever is trained by minimizing the divergence between the retriever's passage likelihood and the language model's relevance score. - This allows iterative improvement of the retriever without requiring any labeled question-passage pairs.- Experiments show ART matches or exceeds the performance of supervised methods like DPR on retrieval benchmarks, while requiring no annotated training data.In summary, the main contribution is proposing an effective unsupervised passage retriever training approach that removes the need for labeled data and task-specific losses. This is achieved through a novel autoencoding scheme and use of pre-trained language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces ART, a new unsupervised method for training dense passage retrievers using only questions, which matches or exceeds the performance of supervised methods without requiring labeled training data.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research on training dense passage retrievers:- Most prior work requires labeled training data of question-passage pairs and/or hard negative mining to train the retriever. This paper proposes an unsupervised method that only requires questions and a corpus of passages. Not needing labeled data is a significant advantage.- The proposed method trains the retriever based on question reconstruction loss using a frozen pretrained language model. This is a novel way to provide training signal without labeled data.- The model achieves state-of-the-art results on several QA retrieval benchmarks, outperforming supervised methods like DPR in low-resource settings and matching performance in high-resource settings. This demonstrates the effectiveness of the unsupervised training approach.- The model transfers well to out-of-distribution datasets, suggesting the training process leads to robust passage representations. This is an advantage over supervised methods that tend to overfit the training dataset.- The approach is sample efficient, outperforming BM25 with only 100 training questions and matching DPR performance with 1,000 questions. This could enable applying it to new domains quickly.- Scaling up model size further improves performance, suggesting potential to push accuracy higher with more compute.Overall, the key novelty is demonstrating unsupervised training of a retriever by reconstructing the question, which makes it more practical by removing the dependency on labeled data. The strong empirical results validate this is a promising research direction for dense retrieval.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Applying the ART approach to low-resource retrieval settings, including multi-lingual and cross-lingual question answering. The unsupervised nature of ART may be particularly beneficial when labeled data is scarce.- Extending ART to train cross-modality retrievers, such as for image or code search using textual queries. The framework could potentially be adapted to learn useful representations without requiring large amounts of aligned image-text or code-text data.- Making use of labeled data when available, such as by finetuning the pre-trained language model on passage-question aligned data to provide even stronger training signals.- Training multi-vector retrievers using ART, which could provide benefits over dual-encoder models like improved recall.- Exploring the behavior and benefits of ART in low-data regimes more thoroughly. The sample efficiency results are promising but more analysis would be useful.- Applying ART to other tasks beyond question answering retrieval, such as ad-hoc retrieval over heterogeneous corpora. The results on the BEIR benchmark show potential.- Using even larger pre-trained language models, which may further improve the quality of the question reconstruction signal. The impact of model scale could be investigated more.In summary, the main directions focus on extending ART to new modalities and tasks, better leveraging available labeled data, analyzing low-resource behavior, and scaling up the components of the ART framework. The unsupervised nature of ART lends itself well to being adapted to many different retrieval scenarios.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces ART (Autoencoding-based Retriever Training), a new unsupervised approach for training dense passage retrievers that does not require labeled question-passage pairs. The key idea is to use a question to retrieve a small set of passages, and then reconstruct the question by attending to these passages using a pre-trained language model scorer. The passage likelihoods computed by the retriever are optimized to match the relevance scores from the language model by minimizing their KL divergence. This encourages the retriever to retrieve passages that are more relevant for reconstructing the original question. Experiments on several QA datasets show that ART outperforms previous unsupervised methods like BM25 and matches or exceeds the performance of supervised methods like DPR, without using any labeled data. The model is highly sample efficient, generalizes well to out-of-domain questions, and benefits from scaling up the retriever model size. Overall, the work demonstrates that an accurate open-domain retriever can be trained using just questions and unlabeled passages.
