# [Questions Are All You Need to Train a Dense Passage Retriever](https://arxiv.org/abs/2206.10658)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

Can an accurate and robust passage retriever be trained using only questions, without requiring labeled question-passage pairs or hard negatives?

The key hypothesis is that a passage retriever can be trained in an unsupervised manner by using a pre-trained language model to provide "soft labels" on how relevant retrieved passages are for reconstructing the original question.

In particular, the paper proposes and evaluates a new approach called ART (Autoencoding Retriever Training) which trains the passage retriever based on maximizing question reconstruction likelihood from the retrieved passages. This allows for unsupervised training of the retriever without needing annotated training data.

The main goal is to show that the proposed ART method can match or exceed the performance of supervised methods like DPR on passage retrieval benchmarks, despite using only questions during training. The paper aims to demonstrate the effectiveness of this unsupervised autoencoding approach for training retrievers.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing ART, a new unsupervised approach for training dense passage retrievers using only questions. The key ideas are:

- ART uses an autoencoding framework where a question is encoded to retrieve passages, and then the passages are decoded to reconstruct the original question. 

- A pre-trained language model is used to score passage relevance for question reconstruction, providing soft supervision signals.

- The retriever is trained by minimizing the divergence between the retriever's passage likelihood and the language model's relevance score. 

- This allows iterative improvement of the retriever without requiring any labeled question-passage pairs.

- Experiments show ART matches or exceeds the performance of supervised methods like DPR on retrieval benchmarks, while requiring no annotated training data.

In summary, the main contribution is proposing an effective unsupervised passage retriever training approach that removes the need for labeled data and task-specific losses. This is achieved through a novel autoencoding scheme and use of pre-trained language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces ART, a new unsupervised method for training dense passage retrievers using only questions, which matches or exceeds the performance of supervised methods without requiring labeled training data.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on training dense passage retrievers:

- Most prior work requires labeled training data of question-passage pairs and/or hard negative mining to train the retriever. This paper proposes an unsupervised method that only requires questions and a corpus of passages. Not needing labeled data is a significant advantage.

- The proposed method trains the retriever based on question reconstruction loss using a frozen pretrained language model. This is a novel way to provide training signal without labeled data.

- The model achieves state-of-the-art results on several QA retrieval benchmarks, outperforming supervised methods like DPR in low-resource settings and matching performance in high-resource settings. This demonstrates the effectiveness of the unsupervised training approach.

- The model transfers well to out-of-distribution datasets, suggesting the training process leads to robust passage representations. This is an advantage over supervised methods that tend to overfit the training dataset.

- The approach is sample efficient, outperforming BM25 with only 100 training questions and matching DPR performance with 1,000 questions. This could enable applying it to new domains quickly.

- Scaling up model size further improves performance, suggesting potential to push accuracy higher with more compute.

Overall, the key novelty is demonstrating unsupervised training of a retriever by reconstructing the question, which makes it more practical by removing the dependency on labeled data. The strong empirical results validate this is a promising research direction for dense retrieval.


## What future research directions do the authors suggest?

 Based on my review of the paper, some of the main future research directions suggested by the authors include:

- Applying the ART approach to low-resource retrieval settings, including multi-lingual and cross-lingual question answering. The unsupervised nature of ART may be particularly beneficial when labeled data is scarce.

- Extending ART to train cross-modality retrievers, such as for image or code search using textual queries. The framework could potentially be adapted to learn useful representations without requiring large amounts of aligned image-text or code-text data.

- Making use of labeled data when available, such as by finetuning the pre-trained language model on passage-question aligned data to provide even stronger training signals.

- Training multi-vector retrievers using ART, which could provide benefits over dual-encoder models like improved recall.

- Exploring the behavior and benefits of ART in low-data regimes more thoroughly. The sample efficiency results are promising but more analysis would be useful.

- Applying ART to other tasks beyond question answering retrieval, such as ad-hoc retrieval over heterogeneous corpora. The results on the BEIR benchmark show potential.

- Using even larger pre-trained language models, which may further improve the quality of the question reconstruction signal. The impact of model scale could be investigated more.

In summary, the main directions focus on extending ART to new modalities and tasks, better leveraging available labeled data, analyzing low-resource behavior, and scaling up the components of the ART framework. The unsupervised nature of ART lends itself well to being adapted to many different retrieval scenarios.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces ART (Autoencoding-based Retriever Training), a new unsupervised approach for training dense passage retrievers that does not require labeled question-passage pairs. The key idea is to use a question to retrieve a small set of passages, and then reconstruct the question by attending to these passages using a pre-trained language model scorer. The passage likelihoods computed by the retriever are optimized to match the relevance scores from the language model by minimizing their KL divergence. This encourages the retriever to retrieve passages that are more relevant for reconstructing the original question. Experiments on several QA datasets show that ART outperforms previous unsupervised methods like BM25 and matches or exceeds the performance of supervised methods like DPR, without using any labeled data. The model is highly sample efficient, generalizes well to out-of-domain questions, and benefits from scaling up the retriever model size. Overall, the work demonstrates that an accurate open-domain retriever can be trained using just questions and unlabeled passages.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces ART, a new unsupervised method for training dense passage retrievers using only questions. The key idea is to use an autoregressive language model to score how well retrieved passages can reconstruct the original question. This provides soft supervision signals to train the retriever without needing aligned question-passage pairs. During training, the input question is used to retrieve a set of passages. These passages are then fed to the language model which computes their likelihood of generating the question tokens. The retriever is trained to minimize the divergence between the passage likelihoods from the retriever and language model. Experiments on several QA datasets show ART matches or exceeds the performance of supervised methods like DPR, while being much more data efficient and generalizing better to out-of-distribution questions. 

In more detail, ART consists of a dual encoder retriever and a pre-trained language model scorer. The dual encoder maps questions and passages to embeddings using BERT, and passage likelihood is computed using dot product scores. For the language model, T5 or T0 models pretrained on unlabeled text are used to score passage likelihood by autoregressively generating the question tokens conditioned on the passage. The retrieval embeddings and generation scores provide complementary signals - retrieval identifies relevant passages quickly while generation accurately estimates passage quality. ART is optimized by minimizing the KL divergence between these two distributions over the retrieved passages. The encoder is trained while the language model parameters are kept fixed. Comprehensive experiments demonstrate ART's effectiveness. When trained on question sets for downstream QA datasets, it outperforms DPR by 4-5 points on top 20/100 retrieval accuracy, despite DPR having full question-passage supervision. ART also shows strong generalization on out-of-distribution questions, and is highly data efficient, matching DPR performance with just 1/10 the training data.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces ART (Autoencoding-based Retriever Training), a new approach for training dense passage retrievers without requiring any labeled training data. 

The key idea is to use an autoencoding framework where an input question is encoded by retrieving a small set of evidence passages, and then decoded by using the retrieved passages to reconstruct the original question. Specifically:

1) A question is encoded by a dual encoder retriever to retrieve a small set of top-K passages. 

2) A large pre-trained language model is used to score each retrieved passage based on the likelihood of reconstructing the original question conditioned on the passage. This provides soft relevance labels.  

3) The retriever is trained by minimizing the divergence between the passage likelihoods from the retriever and the soft relevance labels from the language model. This encourages selecting passages that maximize question reconstruction probability.

The autoencoding process with questions as input and retrieved passages as latent variables allows training an effective retriever without any labeled data. Experiments show ART matches or exceeds the accuracy of supervised methods on multiple QA datasets by relying only on questions during training.


## What problem or question is the paper addressing?

 The paper is proposing a new method called ART (Autoencoding-based Retriever Training) for training dense passage retrievers in an unsupervised manner, without requiring labeled question-passage pairs for training. 

The key ideas and contributions are:

- Proposes a new passage retrieval autoencoding scheme where an input question is used to retrieve evidence passages, and then the passages are used to reconstruct the original question. 

- Shows that training the retriever based on question reconstruction enables effective unsupervised learning of passage and question encoders.

- Demonstrates that ART can match or surpass supervised methods like DPR on QA retrieval benchmarks, without needing any labeled data or task-specific losses.

- The method only requires access to unlabeled questions and passages during training. It uses a pre-trained language model to provide question generation scores which serve as soft relevance labels for training the retriever.

- Achieves improved sample efficiency and generalization compared to supervised methods like DPR.

In summary, the key contribution is an unsupervised alternative to existing supervised methods for training dense retrievers, which removes the need for labeled training data. The method trains the retriever based on its ability to reconstruct the original question conditioned on retrieved passages.
