# [Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/abs/2402.15896)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multimodal large language models (MLLMs) have shown impressive capabilities in processing and integrating information across text, image, and other modalities. An important focus is enhancing their generalization to new unseen multimodal tasks. Multimodal instruction tuning, which fine-tunes models on diverse instruction-based tasks, has been effective for this.
- However, as MLLMs grow in scale and complexity, parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) become essential. But directly applying LoRA for multimodal instruction tuning causes task interference between the diverse tasks, degrading performance.

Proposed Solution:
- This paper proposes Conditional Mixture of LoRA (MixLoRA), which constructs low-rank adaptation matrices tailored to each input instance's demands. This aims to mitigate task interference. 
- Key ideas: 
   (1) Treat the low-rank factors as experts that can be selectively combined to create specialized adaptation matrices.
   (2) Introduce a dynamic factor selection mechanism with independent selection for the factors of each matrix, and conditional selection between the matrices.

Main Contributions:
- Empirically demonstrates the existence of notable task interference in parameter-efficient multimodal instruction tuning
- Proposes the MixLoRA framework to dynamically construct input-specialized low-rank adaptation matrices to reduce interference
- Experiments on multimodal evaluation benchmarks demonstrate MixLoRA consistently outperforms standard LoRA, and generalizes effectively to diverse unseen multimodal tasks

In summary, the paper identifies and tackles the task interference challenge in applying parameter-efficient fine-tuning to multimodal instruction tuning. The proposed MixLoRA model introduces innovations in dynamically constructing specialized adaptation matrices per input to mitigate this issue.
