# [Large, Small or Both: A Novel Data Augmentation Framework Based on   Language Models for Debiasing Opinion Summarization](https://arxiv.org/abs/2403.07693)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Current opinion summarization models exhibit significant sentiment bias, reluctant to generate negative summaries even when given negative input reviews. This is largely due to the extremely unbalanced distribution of positive (72-84%) vs negative reviews in common datasets.

Proposed Solution:
- A novel data augmentation framework (LASS) combining large and small language models to generate additional negative reviews, balancing the sentiment distribution.  

- First, use a large language model (LLM) to rewrite a small set of positive reviews into negative counterfactuals based on a crafted prompt. This adheres to a minimal-edit principle to retain content coherence.

- Next, train a disentangle autoencoder (Dis-AE) on the LLM-generated counterfactual pairs to obtain separate content and sentiment representations. Apply constraints like sentiment classification loss and counterfactual reconstruction loss.

- Finally, generate large amounts of negative reviews by combining/decoding different content and sentiment representations from Dis-AE. Add perplexity and sentiment classification filters.

Main Contributions:

- Propose LASS, an economical semi-supervised approach leveraging both LLMs and a small generator to debias summarization models via data augmentation

- Design a prompt optimization strategy and disentanglement reconstruction model (Dis-AE) to produce high-quality counterfactual reviews 

- Experiments show LASS achieves 36% average increase in negative summary accuracy, comparable to using only LLMs but with significantly less (265k) synthetic samples. No decrease in ROUGE scores.

The key insight is that combining small and large language models can provide an affordable solution to debiasing, balancing performance and cost. The disentanglement modeling also enables better control over sentiment manipulation during data generation.
