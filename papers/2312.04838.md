# [Learning Generalizable Perceptual Representations for Data-Efficient   No-Reference Image Quality Assessment](https://arxiv.org/abs/2312.04838)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a novel framework called GRepQ for generalizable image quality assessment (IQA) that can operate effectively even with very limited training data. The key idea is to learn two complementary sets of features - low-level features that capture local quality variations using a new quality-aware contrastive loss, and high-level features that leverage semantic information from a vision-language model. The low-level features are learned by bringing closer distorted versions of images that have similar perceptual quality, enabling generalization across distortion types. The high-level features are learned by fine-tuning CLIP's image encoder using a group contrastive loss that separates images into higher and lower quality groups based on similarity to quality-relevant text prompts. These complementary sets of learned features are highly generalizable and can be combined to predict quality by training a simple regressor with just a few samples. The model demonstrates state-of-the-art performance in the data-efficient regime across multiple datasets. Additionally, zero-shot quality predictions can also be made without any training data. Through extensive experiments, the paper shows the effectiveness of the learned representations in both data-efficient and zero-shot settings across diverse distortion types.
