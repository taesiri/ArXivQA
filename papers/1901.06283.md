# Improving Sequence-to-Sequence Learning via Optimal Transport

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that using optimal transport loss as an additional training objective can improve sequence-to-sequence learning compared to standard maximum likelihood estimation. Specifically, the authors propose to add an optimal transport loss term to the standard MLE loss function when training seq2seq models. The optimal transport loss measures the distance between the distributions of the generated and reference sequences, encouraging semantic similarity. Thus, the main research questions are:- Can adding an optimal transport loss improve seq2seq model performance compared to standard MLE training?- Does optimal transport provide a useful sequence-level training signal that encourages global semantic consistency?- Can optimal transport help alleviate issues with exposure bias and train-test mismatch that exist with word-level MLE training?The authors test this hypothesis by applying their proposed optimal transport augmented training to a diverse set of seq2seq tasks, including machine translation, text summarization, and image captioning. The consistent improvements across tasks provide evidence that optimal transport is a useful regularizer that can enhance seq2seq learning.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel sequence-to-sequence (Seq2Seq) learning algorithm based on optimal transport (OT) to impose global sequence-level supervision. Specifically, the key contributions are:- Introducing an OT objective as a sequence-level loss to encourage semantic similarity between generated and reference sequences. This overcomes the limitation of standard maximum likelihood estimation which only provides word-level supervision.- Interpreting the proposed method as approximate Wasserstein gradient flows, showing it learns to match the sequence distribution induced by the generator to the true data distribution.- Applying the method to three NLP tasks - machine translation, text summarization, and image captioning - and demonstrating consistent improvements over strong MLE baselines.So in summary, the core novelty is using OT to construct a sequence-level training loss that promotes semantic invariance and long-range consistency. This is shown to outperform standard MLE and sequence-adversarial methods across various text generation benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new sequence-to-sequence learning approach that uses optimal transport to impose sequence-level supervision, helping models better preserve semantic information during generation compared to maximum likelihood estimation alone.
