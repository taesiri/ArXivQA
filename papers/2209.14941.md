# [EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual   Grounding](https://arxiv.org/abs/2209.14941)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve 3D visual grounding by enabling more fine-grained matching between language and visual features. Specifically, the key hypotheses are:1. Explicitly decoupling the text into semantic components and aligning each component separately with visual features can allow for finer-grained multimodal feature fusion and avoid imbalance/ambiguity issues in existing methods. 2. Dense alignment between all object-related text components (not just object names) and visual features can improve discriminability compared to sparse alignment.3. Removing object names and grounding only based on other attributes/relationships is a challenging setting that can thoroughly evaluate fine-grained alignment capacities.In summary, the central focus is on developing an explicit text decoupling and dense visual-textual alignment approach to 3D visual grounding, in order to achieve better cross-modal understanding and more robust performance even without relying on object names. The hypotheses aim to demonstrate the advantages of decoupled and dense alignment over coupled/implicit and sparse alignment used in prior works.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a text decoupling module that parses the input language description into multiple semantic components (e.g. main object, attributes, relationships, etc.). This allows for fine-grained alignment between language and visual features. 2. Introducing two losses - position alignment loss and semantic alignment loss - to supervise the dense matching between the decoupled text components and visual features. This enables more discriminative cross-modal feature learning.3. Proposing a new challenging 3D visual grounding task called "Grounding without Object Name", where object names are replaced with "object". This forces the model to localize objects based on other attributes instead of relying on object categories.4. Achieving state-of-the-art performance on two standard 3D visual grounding datasets - ScanRefer and SR3D/NR3D. The model also shows strong performance on the new proposed task without any retraining.In summary, the key novelty is the explicit text decoupling strategy and using dense alignment losses to match different textual components with visual features in a fine-grained manner. This results in more robust 3D visual grounding and avoids common issues like imbalance and ambiguity in existing methods. The new task further demonstrates the advantages of this approach.
