# [An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning](https://arxiv.org/abs/2403.00799)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- How to optimize the math reasoning abilities of large language models (LLMs) using supervised training data? 
- What is the optimal amount and type of reasoning paths to include in the training data?
- How to expand the models' ability boundary to handle different types of math problems?

Proposed Solution - Data Strategy:
1) Identify the minimal optimal set of reasoning paths by removing duplicates and keeping varied paths. The optimal amount is when the number of paths matches the number of distinct solutions per problem. This maximizes ability on in-domain data.

2) Expand the ability boundary by mixing minimal optimal sets from different types of math problems - adding data for out-of-domain problems that the model is weak on. This cumulatively enhances different abilities of the model.

3) Continuing to add overlapping data, even without direct corresponding data, can further enhance existing abilities. 

Overall this data strategy, called Mix of Minimal Optimal Sets (MMOS), optimizes and expands math reasoning abilities efficiently.

Main Contributions:
- Determined the optimal boundary for augmenting reasoning paths based on distinct solutions per problem
- Showed different abilities can be enhanced by mixing minimal sets of corresponding data types
- Models trained with MMOS match or exceed state-of-the-art performance with lower data construction costs
- Found modern LLMs have sufficient numerical robustness, datasets like GSM-HARD have annotation issues
- Developed an Automated Problem Generator for creating numerically perturbed data  

In summary, the paper explored an effective data strategy to optimize and expand the math reasoning abilities of LLMs via mixing diverse minimal optimal data sets.
