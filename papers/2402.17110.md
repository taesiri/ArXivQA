# [Sinkhorn Distance Minimization for Knowledge Distillation](https://arxiv.org/abs/2402.17110)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Sinkhorn Distance Minimization for Knowledge Distillation":

Problem:
- Knowledge distillation (KD) is used to transfer knowledge from a large teacher model to a smaller student model. Existing KD methods use divergence measures like KL, RKL, and JS divergence to match the output distributions of the teacher and student. However, these measures have limitations:
  - KL divergence causes "mode averaging", where the student learns a smoothed distribution that covers the teacher's distribution, losing distinct modes.
  - RKL divergence leads to "mode collapsing", where the student focuses on only highly probable regions of the teacher distribution.
  - JS divergence results in "mode underestimation", where the student insufficiently matches low probability regions.
- Another challenge is that the low-dimensional categorical outputs from the teacher provide limited information about the actual high-dimensional distributions.

Proposed Solution:
- Use the Sinkhorn distance, a variant of Wasserstein distance, as the divergence measure for KD. Compared to traditional measures:
  - It is a true metric and can better match distributions with little overlap.
  - It is differentiable, enabling optimization.
  - It approximates Wasserstein distance efficiently.
- Reformulate Sinkhorn KD from sample-wise to batch-wise to better capture distribution geometry:
  - Treat a batch of logits as high-dim observation of distributions.
  - Explicit matching constraints enable correct sample-wise alignment across batches.
  - Does not require modifying network architecture or output.

Main Contributions:  
- Propose Sinkhorn Knowledge Distillation (SinKD) using Sinkhorn distance to address limitations of existing divergence measures.
- Reformulate SinKD batch-wise to capture intricacies of distributions without changing model architecture.
- Show state-of-the-art performance compared to existing KD methods on GLUE and SuperGLUE benchmarks, in terms of comparability, validity and generalizability.
- Demonstrate SinKD can effectively transfer knowledge between various transformer architectures beyond BERT.
- Provide comprehensive analysis and guidelines on hyperparameters and design choices of SinKD.
