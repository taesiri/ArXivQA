# MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, it seems the central hypothesis of this paper is that integrating large language models like ChatGPT with existing specialized vision models through proper prompting and textualization can achieve advanced visual intelligence capabilities, without requiring expensive joint fine-tuning like in some recent vision-language models. The key ideas appear to be:- Leveraging ChatGPT's strong instructional capabilities to teach it how to invoke different vision experts through textual prompts.- Converting visual inputs like images/videos into textual placeholders (file paths) that ChatGPT can reason about. - Serializing the outputs of vision models into text that ChatGPT can further process.- Demonstrating that with the right prompt design and vision expert integration, this approach can enable ChatGPT to perform well on a variety of complex visual understanding tasks that normally require expensive joint training of vision and language models.So in summary, the central hypothesis seems to be that prompting and composing existing models is an effective alternative to joint finetuning for multimodal reasoning and understanding. The experiments aim to validate this hypothesis across different task settings.
