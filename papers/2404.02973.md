# [Scaling Laws for Galaxy Images](https://arxiv.org/abs/2404.02973)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Scaling laws suggest that larger neural networks trained on more data lead to better performance. But current laws are based only on ImageNet-like datasets. It's unclear if they apply to radically different domains like galaxy images.  

- Astronomers need neural networks that can efficiently adapt to new datasets from different telescopes and to new scientific questions with limited labels. Generic pretraining may not help enough with major domain gaps.

Methods
- The authors use a dataset of 840k galaxy images with over 100 million crowdsourced annotations, comparable in scale to ImageNet-1K. 

- They train neural networks of varying sizes on subsets of this data and test scaling laws. They also compare pretraining on ImageNet vs their galaxy datasets for finetuning on 5 new downstream tasks.

Key Findings
- Scaling dataset size leads to power law improvements in performance regardless of model family or task. Adding parameters only sometimes helps.

- Prestraining on galaxies then finetuning leads to 31% average error reduction across downstream tasks compared to ImageNet pretraining alone.  

- Model scaling provides relatively small gains; domain adaptation is more important.

Implications
- Scaling laws apply but gains from parameters are inconsistent across galaxy tasks. Published models are likely overparametrized.  

- In-domain pretraining is crucial for adapting to new datasets/telescopes. Modest labelled data in domain is more helpful than huge generic datasets.

In summary, the paper demonstrates the importance of in-domain pretraining for qualitative domain shifts, challenging the notion that generic scaling laws and foundation models directly transfer. Targeted in-domain data collection followed by finetuning is most efficient.
