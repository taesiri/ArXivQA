# [Weakly-Supervised 3D Visual Grounding based on Visual Linguistic   Alignment](https://arxiv.org/abs/2312.09625)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing methods for 3D visual grounding require a large number of expensive 3D bounding box annotations to establish correspondence between text queries and 3D scenes. This hinders large-scale data collection and model capability. 

Proposed Solution:
- The paper proposes a weakly supervised 3D visual grounding approach called 3D-VLA that does not require 3D bounding box annotations. 
- It utilizes the natural 3D-2D correspondence from camera calibration and 2D-text correspondence from large vision-language models like CLIP to implicitly align texts and 3D point clouds.
- Contrastive learning is used to get comparable 3D proposal embeddings. Task-aware classification is introduced for multi-modal adaptation.
- During inference, category-oriented proposal filtering is done to remove confusing predictions.

Main Contributions:
- First work to investigate weakly supervised 3D visual grounding by using large-scale vision-language models. 
- Implicitly establishes semantic relationships between texts and 3D point clouds without need for box annotations.
- Achieves comparable or better performance than fully supervised methods on ReferIt3D and ScanRefer datasets.
- Provides valuable insights to facilitate future research on weakly supervised 3D visual grounding.

In summary, the key idea is to use 2D images as a bridge to align texts and 3D point clouds for weakly supervised 3D grounding. Contrastive learning and task-aware classification enable comparable performance to supervised methods.
