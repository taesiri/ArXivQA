# [Explaining Learned Reward Functions with Counterfactual Trajectories](https://arxiv.org/abs/2402.04856)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Learning reward functions from human feedback or behavior is a promising approach for aligning AI systems with human values. However, current reward learning techniques struggle to reliably capture the complex and subtle aspects of human values. The paper argues that interpretability tools could help users understand potential flaws in learned reward functions by explaining how they assign rewards. This allows users to provide additional feedback to improve alignment.

Proposed Solution: 
The paper proposes "Counterfactual Trajectory Explanations" (CTEs) to interpret learned reward functions in reinforcement learning settings. CTEs consist of an original and altered "counterfactual" partial trajectory, along with the rewards assigned to them. By contrasting the trajectories and rewards, users can infer which behaviors the reward function incentivizes or disincentivizes. 

The paper introduces 6 quality criteria for good CTEs based on literature in explainable AI and psychology. It also proposes two algorithms ("Monte Carlo Trajectory Optimization" and "Deviate and Continue") that generate CTEs by optimizing for these criteria.

To evaluate CTEs, a "proxy-human" neural network is trained to predict rewards on unseen trajectories after learning from CTEs. Better predictions indicate more informative explanations. Experiments show CTEs significantly improve the proxy-human's accuracy, demonstrating their ability to provide understanding of the reward function.

Main Contributions:
- Proposes using counterfactual explanations, a popular XAI technique, to interpret learned reward functions 
- Identifies 6 quality criteria for informative CTEs grounded in XAI and psychology literature
- Introduces two algorithms optimizing these criteria to generate high-quality CTEs 
- Evaluates CTEs by measuring how well a "proxy-human" model can learn the reward function from them
- Demonstrates that CTEs can provide a degree of understanding about complex learned reward functions

The paper makes an important connection between XAI and reward learning, advancing interpretability for an increasingly critical AI capability. It provides initial evidence that counterfactuals are a useful paradigm for understanding learned rewards.
