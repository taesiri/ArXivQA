# OTTers: One-turn Topic Transitions for Open-Domain Dialogue

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper are:1. How can an open-domain dialogue system proactively and politely introduce new topics during a conversation? Specifically, how can the system provide a "commonsense link" explaining how a new topic relates to the previous conversation?2. The authors hypothesize that introducing a new topic by making a connection to the previous dialogue turn through a "bridging" utterance can lead to less abrupt topic transitions compared to other strategies like disengagement or abrupt shifts.3. The authors introduce a new crowdsourced dataset called OTTers focused on these "bridging" transitions between topics. They analyze the strategies and properties of this dataset.4. The authors explore how existing neural text generation models like GPT-2 and MultiGen can be adapted and evaluated on the task of generating bridging transitions between topics using the OTTers dataset. They hypothesize that commonsense knowledge grounding is necessary to generate coherent transitions.5. The authors compare the OTTers dataset to existing multi-topic dialogue datasets like PersonaChat and TopicalChat and hypothesize that these cannot be easily adapted to the specific task of one-turn topic transitions focused on bridging strategies.In summary, the key hypothesis is that topic transitions perceived as less abrupt can be generated by making commonsense connections between the new topic and previous context through bridging utterances, for which the authors collect a targeted dataset and explore baseline models.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The introduction of a new natural language generation task called "one-turn topic transitions" for open-domain dialogue systems. The goal is to generate a "bridging" utterance that smoothly transitions between two topics over a single turn in a conversation.2. The creation of a new dataset called OTTers for this task, consisting of human-written topic transitions with annotations. The paper presents an analysis of the different linguistic strategies humans use for topic transitions in this dataset.3. Showing how existing transformer-based text generation models like GPT-2 and MultiGen can be adapted and evaluated as baselines on this new task. The best performance was achieved by fine-tuning MultiGen, which incorporates commonsense reasoning, on the OTTers dataset. This indicates the importance of grounding topic transitions in commonsense knowledge.4. Demonstrating that existing multi-topic dialogue datasets like PersonaChat and TopicalChat are not well-suited for directly training and evaluating the proposed topic transition task.5. Overall, introducing a new NLP task, dataset, and baselines focused specifically on modeling smooth topic transitions in open-domain conversational systems through knowledge-grounded bridging utterances. This could help enable more natural mixed-initiative dialogues.In summary, the key contribution appears to be the formalization and dataset creation for a new subtask of open-domain dialogue that aims to produce more natural topic transitions. The authors show the promise of commonsense reasoning models like MultiGen for this task while also highlighting areas for future improvement.
