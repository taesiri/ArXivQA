# [Neural Parameter Allocation Search](https://arxiv.org/abs/2006.10598v4)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of Neural Parameter Allocation Search (NPAS), which is minimizing the number of parameters required to train a neural network while maintaining accuracy. The central hypothesis is that parameters can be shared across layers in an intelligent way to achieve this goal. Specifically, the paper proposes that:

- Parameters can be shared between layers of different types and sizes, not just identical layers as in prior work.

- The optimal parameter sharing strategy can be learned automatically instead of using hand-crafted heuristics. 

- Additional capacity can be added during training through parameter sharing, then reduced at test time (high-budget NPAS), improving accuracy.

- Parameter upsampling can allow layers to function properly even with far fewer parameters than needed (low-budget NPAS).

The paper introduces Shapeshifter Networks (SSNs) to implement these ideas and test the hypothesis that intelligent, flexible parameter sharing and allocation can enable high-accuracy networks with far fewer parameters. Experiments across vision, language, and vision-language tasks support this hypothesis.

In summary, the central hypothesis is that cross-layer parameter sharing can be made much more flexible and automated compared to prior work through learned mappings and up/downsampling, enabling significantly more compact neural networks. SSNs are introduced to implement and validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network with a fixed parameter budget. The paper also proposes Shapeshifter Networks (SSNs) to address NPAS, which can automatically learn where and how to share parameters in a network. Some key points:

- NPAS generalizes prior work on parameter sharing to support arbitrary network architectures and parameter budgets. It covers both reducing parameters (to save memory) and increasing parameters (to boost capacity). 

- SSNs are the first method that can automatically learn an effective parameter sharing strategy for any architecture. They learn where to share parameters in a preliminary training step, and use techniques like parameter downsampling and upsampling to generate weights from a limited parameter budget.

- Experiments across diverse tasks and models show SSNs enable training with far fewer parameters than prior work. They also show benefits from using additional parameters in a high-budget setting.

- SSNs are a general framework and can be combined with other techniques like pruning and distillation for further gains.

In summary, the main contribution is formally defining NPAS as a task, and proposing a novel framework in SSNs that can automatically and effectively learn to share parameters in any network to address NPAS. This is a general contribution for efficiently training neural networks.
