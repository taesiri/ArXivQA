# [Poly-View Contrastive Learning](https://arxiv.org/abs/2403.05490)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Poly-View Contrastive Learning":

Problem:
Most contrastive self-supervised learning methods maximize mutual information between two related views (positives) of a data sample while minimizing it between views of other samples (negatives). This paper investigates the benefits of using more than two views per sample, which they refer to as "poly-view contrastive learning". 

Solutions:
The paper proposes two families of objectives for poly-view contrastive learning:

1. Generalized mutual information (MI) objectives: 
   - Derive lower bounds on the MI between one view and the rest using the NWJ bound. This includes arithmetic and geometric means for aggregation.
   - Show these objectives have diminishing MI gap and tighter lower bounds as view multiplicity increases.

2. Sufficient statistics objectives:
   - Connect sufficient statistics to the InfoMax principle for self-supervised learning. 
   - Propose objectives that extract sufficient statistics across multiple views to recover latent generative factors.
   - Show objectives lower bound MI and converge to true conditional as view multiplicity increases on synthetic data.

The paper also analyzes multi-crop methods which average multiple two-view losses. It shows multi-crop reduces variance but does not improve MI lower bounds. 

Contributions:
- Propose two new families of poly-view contrastive learning objectives and analyze their theoretical properties.
- Demonstrate poly-view methods achieve superior accuracy to SimCLR on ImageNet with lower training epochs and smaller batches, creating a new accuracy vs efficiency Pareto front.
- Show fine-tuning experiments where poly-view models outperform SimCLR models pretrained for longer durations.
- Provides new perspective connecting self-supervised learning to sufficient statistics and generalized mutual information.

In summary, the paper presents principles and objectives for using multiple views in contrastive self-supervised learning to improve representation quality and training efficiency.
