# [Model-Based Epistemic Variance of Values for Risk-Aware Policy   Optimization](https://arxiv.org/abs/2312.04386)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new uncertainty Bellman equation (UBE) for quantifying epistemic uncertainty in reinforcement learning. The key insight is that prior UBE formulations overestimate the variance of value functions by ignoring inherent stochasticity (aleatoric uncertainty) of acting under a policy in an MDP. The authors' new UBE disentangles these two sources of uncertainty and proves its solution converges to the exact posterior variance of values. This provides a tighter signal for risk-aware policy optimization. The challenges of applying the theory beyond tabular settings are discussed and suitable approximations proposed. Based on these approximations, the authors introduce $Q$-Uncertainty Soft Actor-Critic (QU-SAC), a model-based RL algorithm that leverages the UBE variance estimates. Experiments in online exploration problems demonstrate improved exploration from the tighter uncertainty signal. Additional offline RL experiments showcase the efficacy of uncertainty-based conservative policy optimization with QU-SAC, achieving overall strong performance.
