# [Unsupervised LLM Adaptation for Question Answering](https://arxiv.org/abs/2402.12170)

## What is the main contribution of this paper?

 This paper proposes a new task called unsupervised large language model (LLM) adaptation for question answering. The key ideas and contributions are:

1) Proposing the task of adapting a pre-trained LLM to a new domain (set of documents) without any question-answer supervision from the target domain documents. This is done by fine-tuning the LLM on a public QA dataset (source data) and unlabeled documents from the target domain.

2) Introducing one synthetic and two real-world datasets for evaluating LLMs fine-tuned with this approach. 

3) Discovering that fine-tuned LLMs can answer questions about facts in the target domain documents, especially when the answers appear early in the documents. However, they struggle to access facts located later in documents.

4) Demonstrating that techniques like random token replacement during fine-tuning can help mitigate the positional bias issue and improve fact retrieval from later in documents.

5) Providing extensive analysis into factors like model size, choice of external QA dataset, effectiveness of data augmentation strategies, etc. that influence performance in this unsupervised LLM adaptation setting.

In summary, the key contribution is proposing and analyzing the task of annotation-efficient, unsupervised adaptation of LLMs to new domains/documents for question answering.
