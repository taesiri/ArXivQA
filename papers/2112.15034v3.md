# [Self Reward Design with Fine-grained Interpretability](https://arxiv.org/abs/2112.15034v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key aspects of this paper are:

- It proposes a new framework called Self Reward Design (SRD) for creating interpretable reinforcement learning models. 

- The goal is to achieve both interpretability and good performance without arbitrary reward specification.

- It advocates designing neural network models in a human-understandable way, where each component has clear semantic meaning. 

- This is in contrast to typical deep RL models which tend to be black boxes.

- The paper demonstrates the SRD framework on several examples: a 1D toy robot fish, a 2D robot navigating a grid world, and controlling a simulated half-cheetah.

- For each example, it shows how the model can be designed with deliberative interpretability down to directly setting the weights and biases. 

- It also shows how the models can be optimized via a self-reward mechanism without a pre-defined external reward function.

So in summary, the key hypothesis is that reinforcement learning problems can be solved with highly interpretable neural network designs augmented by self-reward based optimization, providing an alternative approach to deep RL. The paper aims to demonstrate the viability of this SRD framework through the examples.
