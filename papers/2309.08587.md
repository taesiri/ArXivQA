# [Compositional Foundation Models for Hierarchical Planning](https://arxiv.org/abs/2309.08587)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we create agents capable of solving novel long-horizon tasks which require hierarchical reasoning?

The key ideas and approach are:

- Propose a compositional foundation model called HiP (Hierarchical Planning Foundation Model) that leverages different "expert" foundation models trained on language, vision, and action data individually to construct long-horizon plans. 

- Given a language goal, use a large language model to create an abstract plan by generating a sequence of subgoals.

- Ground the abstract plan in the environment using a large video diffusion model to generate a detailed visual plan.

- Map the visual plan to actions using a large inverse dynamics model trained on ego-centric data.

- Enforce consistency between the different models through an iterative refinement procedure.

The hypothesis is that by leveraging existing foundation models trained on different modalities separately, and enforcing consistency between them, the agent will be able to effectively perform hierarchical reasoning to solve novel long-horizon tasks. The compositional design reduces the need for costly paired training data across modalities.

The experiments on three long-horizon tabletop manipulation tasks aim to validate whether this approach enables solving tasks that require reasoning across multiple levels of abstraction and timescales. The results generally support the viability of the proposed compositional foundation model approach.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing a compositional foundation model for hierarchical planning called HiP that leverages separate expert models for language, vision, and action to solve long-horizon tasks. Rather than training a single monolithic model on paired multimodal data, HiP combines models trained on different modalities in a hierarchical framework.

2. Using a large language model for high-level symbolic planning. The language model generates a sequence of subgoals given a natural language goal instruction.

3. Employing a video diffusion model for visual planning to generate detailed image trajectories achieving the subgoals. The video model is pretrained on internet videos and finetuned on task videos.

4. Leveraging an inverse dynamics model for low-level action planning to infer actions from the generated image trajectories. The inverse model is pretrained on egocentric videos to provide useful visual priors. 

5. Enforcing consistency between the separate models through an iterative refinement procedure. This helps align the outputs of the different models to create physically plausible and executable plans.

6. Demonstrating the effectiveness of HiP on three long-horizon robotic manipulation tasks, showing superior performance compared to various baselines. The method generalizes well to unseen combinations of objects and goals.

In summary, the key ideas are using separate pretrained models for different levels of reasoning, combining them compositionally, and aligning them through iterative refinement to enable hierarchical planning that can solve complex, long-horizon tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a compositional foundation model called HiP that combines separate expert models for language, vision, and action to enable hierarchical reasoning for solving long-horizon robot manipulation tasks through iterative refinement between the models to ensure consistency.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Developing larger, more general-purpose visual sequence and robot control models as the foundation models for visual planning and action planning in their framework. The current work relies on smaller-scale models trained from scratch due to compute limitations. The authors suggest that larger foundation models pre-trained on diverse video and robotics data could enhance the capabilities of their framework.

- Exploring more efficient and accurate methods for sampling from the joint distribution between the different models (task, visual, and action models). The current iterative refinement approach provides an approximation but more principled techniques could be developed.

- Incorporating other modalities such as touch and sound and their corresponding foundation models into the framework in addition to language, vision, and robotics. The compositional design could allow integrating diverse perceptual models.

- As vision-language models continue to improve, exploring if they can effectively replace the learned classifiers for visually grounding the language models. The current classifiers outperform the VLM, but future VLMs may match their capabilities.

- Incorporating recent advances for accelerating diffusion model sampling to improve the speed of visual planning.

Overall, the key directions are developing larger foundation models for the visual and action planning components, integrating additional modalities and their models, improving the joint consistency between models, and speeding up the sampling processes involved. The framework presents opportunities for integrating future advances in multiple areas of AI.


## How does this paper compare to other research in the same field?

 This paper presents a novel approach for hierarchical planning using compositional foundation models. Here are some key ways it compares to other related work:

- Most prior work has focused on training end-to-end monolithic models on paired vision, language and action data. This paper takes a more modular approach by combining separate foundation models for language, vision and action that are each trained on different modalities of data. This reduces the need for expensive paired multimodal datasets.

- Several previous papers have proposed finetuning large language models on robotics demonstrations with both visual and language inputs. This paper avoids finetuning large LMs, instead using them in a zero-shot way with a lightweight visual grounding model. This is more scalable as large LMs are expensive to finetune.

- Compared to hierarchical RL methods like hierarchical reinforcement learning, this paper learns a goal-conditioned hierarchy without needing environment rewards. The subgoal structure is instead derived from language and visual priors.

- The idea of composing individual expert models is related to prior work on model ensembling. However, this paper introduces a novel iterative refinement technique to promote consensus between the models and enable coherent hierarchical plans.

- For visual planning, this paper uses modern video diffusion models rather than more traditional approaches like RSSMs. It shows the advantages of leveraging large internet video datasets for pretraining.

- The use of inverse models for action planning captures useful priors from egocentric internet images. This is a different approach than planning directly in action space.

Overall, the key innovations are in the compositional architecture using separate foundation models, the iterative refinement for consistency, and the ability to leverage diverse internet data sources. The experiments demonstrate the benefits of this approach on long-horizon robot manipulation tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a compositional foundation model called HiP (Hierarchical Planning Foundation Models) for hierarchical planning to solve long-horizon robot manipulation tasks. HiP leverages multiple "expert" foundation models trained on language, vision, and action data individually and combines them together for planning. Specifically, HiP uses a large language model to construct symbolic plans, grounds them through a video diffusion model to generate image trajectory plans, and maps the image trajectories to actions using an inverse dynamics model. To enable effective hierarchical reasoning, HiP employs an iterative refinement procedure to enforce consistency between the models. The authors demonstrate HiP's effectiveness on three long-horizon tabletop manipulation tasks, showing it can successfully solve novel tasks and outperforms existing methods. The key advantage of HiP is that it reduces the need for expensive paired multimodal training data by utilizing models trained separately on different Internet data modalities. The results illustrate this compositional approach can enable capable and generalizable hierarchical planning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a compositional foundation model called HiP (Hierarchical Planning Foundation Model) for long-horizon decision making. HiP leverages separate foundation models trained on language, vision, and action data to construct long-horizon plans. Given a language goal, HiP first uses a large language model to create an abstract plan by generating a sequence of subgoals. It then uses a video diffusion model to generate a more detailed visual plan in the form of observation trajectories for each subgoal. Finally, it maps the visual plan to actions using an inverse dynamics model trained on ego-centric images. 

To ensure the different modules produce consistent outputs, HiP uses an iterative refinement procedure. During language subgoal generation, visual feedback is incorporated to ground the language model. Similarly, during video trajectory generation, action feedback is incorporated to guide video generation. On three long-horizon manipulation tasks, HiP outperforms baselines like goal-conditioned policies, video planners, and action planners. It generalizes well to unseen tasks and benefits from pretraining the video and action modules on internet videos and ego-centric images. The results demonstrate the promise of leveraging separate foundation models on different modalities to efficiently solve long-horizon tasks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a compositional foundation model called HiP (Hierarchical Planning Foundation Model) for long-horizon decision making. HiP is composed of three separately trained components: a large language model for task planning, a video diffusion model for visual planning, and an inverse dynamics model for action planning. Given a language goal, the language model generates a sequence of subgoals. Then for each subgoal, the video diffusion model generates an image trajectory plan conditioned on the subgoal and current image observation. Finally, the inverse dynamics model converts the image trajectory into actions to execute. To ensure consistency between the components, HiP uses an iterative refinement procedure - the language model gets feedback from an image likelihood estimator, and the video diffusion model gets feedback from the inverse dynamics model. This allows sampling subgoals, videos, and actions that have high likelihood under all components of the model. The key idea is to leverage several pretrained foundation models, rather than collecting expensive paired multimodal data to train a single monolithic model. The compositional design and iterative refinement allow HiP to do hierarchical reasoning and planning for long-horizon tasks.


## What problem or question is the paper addressing?

 Based on my reading of the paper, it seems the key problem the authors are trying to address is how to create agents capable of solving novel long-horizon tasks that require hierarchical reasoning across different levels - abstract planning, concrete visual reasoning, and low-level action execution. 

The key questions appear to be:

- How can we construct a "foundation model" for long-term planning and decision making that does not require collecting a massive paired dataset across language, vision, and action modalities?

- Can we instead leverage separate foundation models trained on these different modalities using already available data on the internet and in robotics datasets? 

- How can we combine and coordinate these separate models operating at different levels of abstraction to generate plans and actions that are consistent across the hierarchy?

So in summary, the key focus seems to be on developing a scalable and data-efficient approach to hierarchical planning by composing modular foundation models trained on diverse disconnected datasets, and using techniques like iterative refinement to promote consistency between the different levels.
