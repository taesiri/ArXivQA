# Selective Annotation Makes Language Models Better Few-Shot Learners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we reduce the manual annotation cost while retaining high performance when using large language models for in-context learning?Specifically, the paper proposes and evaluates a two-step framework consisting of:1) Selective annotation: Carefully choosing a small set of diverse, representative examples from unlabeled data to annotate before test time. This determines the total annotation budget.2) Prompt retrieval: At test time, retrieving similar examples from the small annotated set to use as prompts for each individual test instance. The key hypothesis is that by selecting a small set of annotated examples wisely in the first step, the in-context learning performance can be improved substantially while greatly reducing the amount of required manual annotation compared to prior work. The paper introduces a graph-based "vote-k" method for selective annotation that aims to balance diversity and representativeness. Through experiments on 10 diverse NLP datasets, the paper shows that their proposed annotation framework with vote-k selection significantly outperforms random annotation baseline and requires much less annotation than prior in-context learning methods to achieve similar performance.In summary, the core research question is how to minimize the annotation cost while retaining strong in-context learning performance, which is addressed through the proposed two-step annotation framework and selective annotation method. The results support the hypothesis that selecting annotated examples wisely can greatly reduce annotation requirements for in-context learning.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a two-step framework for efficient in-context learning that decouples selective annotation and prompt retrieval. This allows studying the annotation cost of in-context learning explicitly. 2. It shows the importance of the first step of selective annotation, which has been overlooked in prior work on in-context learning. The proposed graph-based vote-k method selects a diverse and representative set of examples to annotate.3. Extensive experiments on 10 diverse NLP datasets demonstrate that the proposed vote-k method substantially improves in-context learning performance over random annotation baselines. With only 18-100 annotated examples, vote-k achieves strong performance on par with or better than supervised finetuning with much larger training data.4. Analysis shows the effectiveness of the framework holds for varying language model sizes and is more pronounced when there is a domain shift between training and test data. Comparisons to other selective annotation methods also validate the advantages of vote-k.5. The work provides guidance for efficient annotation of new tasks using in-context learning. It shows that with a small set of carefully selected examples, in-context learning can match or exceed the performance of finetuning on sizable training sets at a fraction of the annotation cost.In summary, the key contribution is demonstrating the importance of selective annotation in an explicit two-step in-context learning framework, and providing an effective graph-based method for choosing diverse, representative examples to annotate. This enables very efficient in-context learning for new tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a two-step framework for efficient in-context learning that selects a diverse, representative subset of unlabeled data to annotate upfront before retrieving task-specific examples at test time.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of in-context learning with large language models:- The key innovation in this paper is proposing and evaluating a two-step framework that decouples the selective annotation/sample selection step from the prompt retrieval step. Most prior work on in-context learning has focused just on prompt retrieval methods and assumes access to a large pool of labeled examples. By explicitly evaluating sample selection strategies, this paper provides more careful analysis of the total annotation cost required for in-context learning.- The paper introduces a novel graph-based sample selection method called vote-k that aims to select a diverse and representative set of examples. This differs from prior work on active learning that tends to select examples based on uncertainty. The empirical results demonstrate the effectiveness of vote-k for in-context learning across a range of datasets.- The paper provides systematic comparison between in-context learning and finetuning under limited annotation budgets. The results demonstrate in-context learning can match finetuning performance with substantially less annotation cost. This adds to recent analysis on the sample efficiency benefits of in-context learning compared to finetuning.- The analysis of the impact of domain shifts on sample selection is novel and shows even larger gains for vote-k under domain shift conditions. This could motivate new research directions on domain adaptation for in-context learning.- The experiments cover a broad set of tasks and datasets, providing a thorough evaluation. Many recent papers have focused on just 1-2 datasets. The consistent gains for vote-k across tasks is a strong result.- The comparisons to prior sample selection methods from the literature helps situate the proposed approach relative to related techniques for active learning and core-set selection.Overall, I think this paper makes excellent progress in rigorously analyzing annotation costs for in-context learning. The two-step framework and empirical methodology enable deeper understanding of how to optimize data efficiency when applying large LMs.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing methods to select diverse and representative prompts/examples for few-shot learning with large language models. The authors suggest that carefully selecting the prompts and examples used for few-shot learning could substantially improve performance and reduce annotation cost. They propose some methods in this direction, but suggest further work is needed.- Studying the interplay between language model scale, task data/prompts, and few-shot learning performance. The authors show results using models of varying sizes, but suggest more systematic study is needed.- Scaling up the evaluation to additional datasets and tasks. The authors demonstrate results on diverse tasks, but note there are many other tasks and datasets that should be studied.- Developing methods that are computationally efficient and do not require large language model inference. The authors propose a "fast" variant of their method, but suggest more work on efficient methods.- Studying whether insights transfer to modalities beyond text. The work focuses on language tasks, but the idea of carefully selecting examples for few-shot learning may generalize.- Analyzing model confidence scores and uncertainty for sample selection. The authors use model confidence in their proposed method, but suggest further analysis of uncertainty is worthwhile.- Considering interactive/adaptive sampling procedures. The proposed approach does static sampling, but the authors suggest interactive approaches based on language model feedback could be promising.In summary, the main directions are around better understanding and improving few-shot learning, with a focus on prompt/example selection, efficiency, scaling, and model analysis. The core idea of carefully selecting data for few-shot learning seems to have significant room for further exploration.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a two-step framework for efficient in-context learning with large language models. The first step selects a small subset of diverse, representative examples from unlabeled data to annotate before test time. This determines the total annotation budget. The second step retrieves similar examples from the annotated pool as prompts for each test instance. The paper introduces a graph-based method called vote-k for the first step that aims to balance diversity and representativeness. Through experiments on 10 diverse NLP datasets, the paper shows that careful selection of examples to annotate substantially improves in-context learning performance and stability compared to random selection. With only 18-100 annotated examples, the proposed method achieves strong performance on par with or better than supervised finetuning that uses orders of magnitude more labeled data. The results demonstrate that large pre-trained language models can perform well on new tasks with very limited annotation due to their capability to adapt via prompting.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a two-step framework for efficient in-context learning with large language models. The first step is selective annotation, where a small number of diverse, representative examples are chosen from unlabeled data to be annotated before test time. This determines the total annotation budget. The second step is prompt retrieval, where at test time, examples similar to the test instance are retrieved from the annotated pool to serve as in-context examples. The paper introduces a graph-based selective annotation method called vote-k that aims to balance diversity and representativeness. Experiments on 10 diverse NLP datasets demonstrate that vote-k substantially outperforms random annotation selection. With only 18-100 annotated examples, vote-k combined with similarity-based prompt retrieval achieves strong performance, outperforming finetuning approaches with 10-100x less annotation cost. Further analysis explores the effectiveness of selective annotation under varying conditions like model size and domain shift. The paper provides useful guidance for efficient use of large language models.
