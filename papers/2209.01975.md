# Selective Annotation Makes Language Models Better Few-Shot Learners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we reduce the manual annotation cost while retaining high performance when using large language models for in-context learning?Specifically, the paper proposes and evaluates a two-step framework consisting of:1) Selective annotation: Carefully choosing a small set of diverse, representative examples from unlabeled data to annotate before test time. This determines the total annotation budget.2) Prompt retrieval: At test time, retrieving similar examples from the small annotated set to use as prompts for each individual test instance. The key hypothesis is that by selecting a small set of annotated examples wisely in the first step, the in-context learning performance can be improved substantially while greatly reducing the amount of required manual annotation compared to prior work. The paper introduces a graph-based "vote-k" method for selective annotation that aims to balance diversity and representativeness. Through experiments on 10 diverse NLP datasets, the paper shows that their proposed annotation framework with vote-k selection significantly outperforms random annotation baseline and requires much less annotation than prior in-context learning methods to achieve similar performance.In summary, the core research question is how to minimize the annotation cost while retaining strong in-context learning performance, which is addressed through the proposed two-step annotation framework and selective annotation method. The results support the hypothesis that selecting annotated examples wisely can greatly reduce annotation requirements for in-context learning.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a two-step framework for efficient in-context learning that decouples selective annotation and prompt retrieval. This allows studying the annotation cost of in-context learning explicitly. 2. It shows the importance of the first step of selective annotation, which has been overlooked in prior work on in-context learning. The proposed graph-based vote-k method selects a diverse and representative set of examples to annotate.3. Extensive experiments on 10 diverse NLP datasets demonstrate that the proposed vote-k method substantially improves in-context learning performance over random annotation baselines. With only 18-100 annotated examples, vote-k achieves strong performance on par with or better than supervised finetuning with much larger training data.4. Analysis shows the effectiveness of the framework holds for varying language model sizes and is more pronounced when there is a domain shift between training and test data. Comparisons to other selective annotation methods also validate the advantages of vote-k.5. The work provides guidance for efficient annotation of new tasks using in-context learning. It shows that with a small set of carefully selected examples, in-context learning can match or exceed the performance of finetuning on sizable training sets at a fraction of the annotation cost.In summary, the key contribution is demonstrating the importance of selective annotation in an explicit two-step in-context learning framework, and providing an effective graph-based method for choosing diverse, representative examples to annotate. This enables very efficient in-context learning for new tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a two-step framework for efficient in-context learning that selects a diverse, representative subset of unlabeled data to annotate upfront before retrieving task-specific examples at test time.
