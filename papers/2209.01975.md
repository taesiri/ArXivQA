# Selective Annotation Makes Language Models Better Few-Shot Learners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we reduce the manual annotation cost while retaining high performance when using large language models for in-context learning?Specifically, the paper proposes and evaluates a two-step framework consisting of:1) Selective annotation: Carefully choosing a small set of diverse, representative examples from unlabeled data to annotate before test time. This determines the total annotation budget.2) Prompt retrieval: At test time, retrieving similar examples from the small annotated set to use as prompts for each individual test instance. The key hypothesis is that by selecting a small set of annotated examples wisely in the first step, the in-context learning performance can be improved substantially while greatly reducing the amount of required manual annotation compared to prior work. The paper introduces a graph-based "vote-k" method for selective annotation that aims to balance diversity and representativeness. Through experiments on 10 diverse NLP datasets, the paper shows that their proposed annotation framework with vote-k selection significantly outperforms random annotation baseline and requires much less annotation than prior in-context learning methods to achieve similar performance.In summary, the core research question is how to minimize the annotation cost while retaining strong in-context learning performance, which is addressed through the proposed two-step annotation framework and selective annotation method. The results support the hypothesis that selecting annotated examples wisely can greatly reduce annotation requirements for in-context learning.
