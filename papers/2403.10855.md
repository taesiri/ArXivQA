# [Reinforcement Learning with Options and State Representation](https://arxiv.org/abs/2403.10855)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper aims to develop improved reinforcement learning (RL) methods that can handle complex, high-dimensional environments. Specifically, it tries to address the challenges of sparse rewards and scaling RL to large state spaces. 

Proposed Solution - Hierarchical RL: 
The main proposal is to use hierarchical reinforcement learning (HRL) to decompose the learning task. The hierarchy has a high-level "manager" agent and lower-level "employee" agents. The manager sets goals/subgoals for the employees to accomplish. This allows tackling complex tasks by breaking them down.

Key Methods Presented:
1. Hierarchical Trust Region Policy Optimization (TRHPO): Extends trust region policy optimization (TRPO) to hierarchical policies with a manager policy and option policies. Includes a bound on the KL divergence to constrain policy updates.

2. Eigenoptions: Options based on eigenfunctions of the environment's state graph Laplacian matrix. These options reflect the geometric structure of the environment and give invariance to symmetries. A spectral network is developed to learn these eigenfunctions.

Main Contributions:
- Derives a bound on the KL divergence between hierarchical policies to adapt TRPO to multi-level policies
- Proposes using regularized information maximization to learn distinguishable options
- Introduces invariant eigenoptions based on spectral methods to exploit environment structure
- Develops a spectral network with a method to sequentially update eigenvectors

The HRL methods are tested on a 4-rooms environment. The TRHPO outperforms regular TRPO in sparse reward settings. Limitations around convergence to local optima are discussed. Tests for clustering and eigenoptions show promise but further work is highlighted around scaling and integration with HRL.
