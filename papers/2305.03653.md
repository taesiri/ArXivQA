# [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653)

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, it seems the central research question this paper addresses is how to leverage large language models (LLMs) for query expansion in information retrieval. Specifically, the authors propose using the generative abilities of LLMs to expand queries with additional related terms, rather than relying on traditional pseudo-relevance feedback approaches. The main hypotheses appear to be:1) LLMs can be prompted in various ways (zero-shot, few-shot, chain-of-thought) to produce useful query expansions.2) Chain-of-thought prompts will perform best at query expansion because they generate more verbose explanations with many potential keywords. 3) Incorporating pseudo-relevance feedback documents into the prompts can further improve performance.4) Larger LLM model sizes will lead to better query expansion performance.So in summary, the central research question is how to leverage LLMs for query expansion, with a focus on designing effective prompting approaches and studying how factors like prompt type, model size, and pseudo-relevance feedback impact performance. The core hypothesis is that LLMs can generate expansions that are more useful than traditional PRF methods.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It formulates various prompts to perform query expansion with large language models (LLMs), including zero-shot, few-shot, and chain-of-thought (CoT) prompts. 2. It finds that CoT prompts perform best for query expansion, likely because CoT prompts instruct the model to break down its reasoning step-by-step, generating many relevant keywords.3. It studies the performance of these prompts across various model sizes, to understand the practical capabilities and limitations of using LLMs for query expansion.In summary, the paper demonstrates that LLMs can be prompted in various ways to generate query expansions that improve retrieval performance, with CoT prompts being particularly effective. The analysis across model sizes provides insights into how well this approach may work with smaller, more practical models versus large research models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using large language models for query expansion in information retrieval by prompting the model to generate expansions, finding chain-of-thought prompts that ask the model to explain its reasoning produce the best expansions.


## How does this paper compare to other research in the same field?

Here are a few key things to consider when comparing this paper to other research on query expansion:- This paper focuses specifically on using large language models (LLMs) for query expansion, while much prior work has explored other techniques like pseudo-relevance feedback (PRF). Using LLMs is a relatively new approach for query expansion.- The prompts explored in this paper, especially the chain-of-thought (CoT) prompts, are novel. Other papers have not extensively studied different prompting approaches for getting LLMs to generate useful query expansions. - The authors thoroughly evaluate prompt variations both with and without PRF document context across multiple model sizes. This provides insight into the tradeoffs between model scale, prompting approaches, and using PRF. - The open-sourced Flan models studied make this research more reproducible than some recent works that rely on proprietary LLMs. However, the largest models are still quite large.- The gains over PRF baselines, while noticeable in some cases, are incremental. This suggests LLMs have room for improvement to match or surpass classical query expansion techniques.- The focus is on query expansion specifically for sparse retrieval with BM25. Dense retrieval may benefit less from query expansion, as other papers have noted.Overall, this paper provides a novel exploration of prompt-based query expansion with LLMs. It stands out for its systematic evaluation of different prompts and model scales. The gains shown are promising but not yet decisive compared to strong PRF baselines. There is clearly more work to be done developing techniques to most effectively leverage LLMs for query expansion.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Studying the performance of their LLM-based query expansion methods with other types of models besides the Flan models they focused on. They mention exploring other large language models like GPT, Palm, LaMDA, etc.- Applying distillation methods to transfer the abilities of large query expansion models to smaller more practical models. This could help address the computational limitations of using huge LLMs.- Evaluating their prompts in dense retrieval settings, not just sparse retrieval. The paper focused on BM25 where query expansion is more impactful, but dense retrievers may benefit less.- Productionizing and deploying LLM-based query expansion in real systems. The authors mention the computational costs may be prohibitive currently. Finding ways to serve these models efficiently is an open problem. - Exploring additional prompt formulations and ways to instruct the LLM. The paper studied some prompt templates but there may be better ways to formulate the task.- Expanding the analysis to other information retrieval tasks besides query expansion. The generative abilities of LLMs could likely help in areas like document expansion, query understanding, etc.In summary, the main future directions are around applying LLMs to other IR tasks, using different types of models, deploying the methods efficiently, and exploring better prompting techniques. Overall, the authors are excited about the potential for LLMs to become integral parts of IR systems.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes using large language models (LLMs) for query expansion in information retrieval. Unlike traditional pseudo-relevance feedback methods that rely on an initial set of retrieved documents, the authors suggest prompting an LLM with the query and using its generative capabilities to produce expansions. They study various prompt formulations including zero-shot, few-shot, and chain-of-thought (CoT) prompts. Experiments on MS MARCO and BEIR datasets demonstrate that CoT prompts work best as they encourage the model to provide a verbose, step-by-step explanation containing many potential expansion terms. Overall, LLM-based query expansion outperforms traditional PRF methods, with chain-of-thought prompts and the inclusion of pseudo-relevant documents in the prompt proving most effective. The results indicate LLMs can leverage their inherent knowledge to improve retrieval recall without needing to be trained or fine-tuned. However, performance is heavily dependent on model size, with large models required to match traditional baselines.
