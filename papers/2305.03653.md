# [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653)

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, it seems the central research question this paper addresses is how to leverage large language models (LLMs) for query expansion in information retrieval. Specifically, the authors propose using the generative abilities of LLMs to expand queries with additional related terms, rather than relying on traditional pseudo-relevance feedback approaches. The main hypotheses appear to be:1) LLMs can be prompted in various ways (zero-shot, few-shot, chain-of-thought) to produce useful query expansions.2) Chain-of-thought prompts will perform best at query expansion because they generate more verbose explanations with many potential keywords. 3) Incorporating pseudo-relevance feedback documents into the prompts can further improve performance.4) Larger LLM model sizes will lead to better query expansion performance.So in summary, the central research question is how to leverage LLMs for query expansion, with a focus on designing effective prompting approaches and studying how factors like prompt type, model size, and pseudo-relevance feedback impact performance. The core hypothesis is that LLMs can generate expansions that are more useful than traditional PRF methods.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It formulates various prompts to perform query expansion with large language models (LLMs), including zero-shot, few-shot, and chain-of-thought (CoT) prompts. 2. It finds that CoT prompts perform best for query expansion, likely because CoT prompts instruct the model to break down its reasoning step-by-step, generating many relevant keywords.3. It studies the performance of these prompts across various model sizes, to understand the practical capabilities and limitations of using LLMs for query expansion.In summary, the paper demonstrates that LLMs can be prompted in various ways to generate query expansions that improve retrieval performance, with CoT prompts being particularly effective. The analysis across model sizes provides insights into how well this approach may work with smaller, more practical models versus large research models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using large language models for query expansion in information retrieval by prompting the model to generate expansions, finding chain-of-thought prompts that ask the model to explain its reasoning produce the best expansions.


## How does this paper compare to other research in the same field?

Here are a few key things to consider when comparing this paper to other research on query expansion:- This paper focuses specifically on using large language models (LLMs) for query expansion, while much prior work has explored other techniques like pseudo-relevance feedback (PRF). Using LLMs is a relatively new approach for query expansion.- The prompts explored in this paper, especially the chain-of-thought (CoT) prompts, are novel. Other papers have not extensively studied different prompting approaches for getting LLMs to generate useful query expansions. - The authors thoroughly evaluate prompt variations both with and without PRF document context across multiple model sizes. This provides insight into the tradeoffs between model scale, prompting approaches, and using PRF. - The open-sourced Flan models studied make this research more reproducible than some recent works that rely on proprietary LLMs. However, the largest models are still quite large.- The gains over PRF baselines, while noticeable in some cases, are incremental. This suggests LLMs have room for improvement to match or surpass classical query expansion techniques.- The focus is on query expansion specifically for sparse retrieval with BM25. Dense retrieval may benefit less from query expansion, as other papers have noted.Overall, this paper provides a novel exploration of prompt-based query expansion with LLMs. It stands out for its systematic evaluation of different prompts and model scales. The gains shown are promising but not yet decisive compared to strong PRF baselines. There is clearly more work to be done developing techniques to most effectively leverage LLMs for query expansion.
