# [GMML is All you Need](https://arxiv.org/abs/2205.14986)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we enable vision transformers to be trained efficiently from scratch on small/medium datasets through self-supervised pre-training, without relying on large datasets like ImageNet?The key hypotheses seem to be:1) By manipulating images using the proposed Group Masked Model Learning (GMML) approach during pre-training, the model can learn useful contextual representations from all concepts in an image, not just foreground objects.2) This will allow the vision transformer to generalize well to downstream tasks, even when trained on limited data, overcoming the typical data hungry nature of transformers. 3) The proposed GMML pre-training approach will outperform state-of-the-art supervised and self-supervised methods, especially on smaller datasets, due to its ability to extract contextual information from all concepts.In summary, the main research question is how to make vision transformers data-efficient through a self-supervised pre-training approach, with the central hypothesis being that the proposed GMML method can enable this. The experiments aim to validate if GMML allows transformers to train on and generalize well from small/medium datasets.


## What is the main contribution of this paper?

The main contribution of this paper seems to be the proposal of a self-supervised learning method called "Group Masked Model Learning" (GMML) for pre-training vision transformers. The key ideas and contributions are:- GMML manipulates images by masking random groups of connected tokens/patches, which are more likely to represent meaningful semantic concepts compared to masking individual tokens. The model tries to recover the masked regions based on contextual information. This process helps the model learn semantic image representations. - They propose a transformer-based autoencoder architecture for implementing GMML, with a lightweight decoder (just a couple of linear layers). This avoids the need for complex convolutional decoder blocks like in CNN autoencoders.- GMML makes transformers data-efficient and able to train on small/medium sized datasets, without needing massive datasets like ImageNet. It outperforms supervised pre-training on several benchmark datasets.- It does not suffer from "trivial solution" problems that many other self-supervised methods face, hence avoids issues like collapsed representations. It does not need tricks like large batches, momentum encoders, etc.- GMML marked a milestone by being one of the first self-supervised methods to consistently outperform supervised pre-training. This could indicate a shift towards self-supervised pre-training becoming the norm like in NLP.In summary, the key novelty is the GMML technique to learn semantic image representations in a self-supervised manner, which makes transformers data-efficient and achieves new state-of-the-art results on multiple vision tasks. The paper provides ablation studies and visualizations to analyze GMML.
