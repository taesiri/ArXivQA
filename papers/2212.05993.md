# [RGBD2: Generative Scene Synthesis via Incremental View Inpainting using   RGBD Diffusion Models](https://arxiv.org/abs/2212.05993)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1. Can we develop an effective method for generative 3D scene synthesis from sparse RGBD view observations? The paper introduces a new task of learning to synthesize complete 3D scenes from sparse multi-view RGBD images. 

2. Can diffusion models be adapted to operate on rendered RGBD views for high-quality view completion? The paper proposes using an RGBD diffusion model to inpaint missing regions in novel views rendered from an intermediate scene mesh.

3. Does an incremental view inpainting approach lead to consistent multi-view scene geometry? The method involves progressively generating views along camera trajectories and fusing them to obtain a complete 3D mesh.

4. Can a model trained on RGBD scans generalize to unseen scenes and synthesize geometry and appearance in a scalable manner? The goal is to learn across multiple scenes and apply the model to new scenes with sparse inputs while ensuring scalability.

In summary, the main research focus appears to be on developing a generative scene synthesis approach using RGBD diffusion for view inpainting in an incremental fashion to reconstruct 3D geometry from sparse inputs in a consistent and scalable manner.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It introduces a new task of generative scene synthesis from sparse RGBD views. This involves learning across multiple scenes to enable scene synthesis from just a few input RGBD images.

2. It presents a new approach called RGBD^2 that can generate novel RGBD views along a camera trajectory in a sequential manner. The scene geometry is simply the fusion of these generated views. 

3. It uses an intermediate surface mesh for rendering novel views, and an RGBD diffusion model to inpaint missing regions in these rendered views. The mesh helps ensure multi-view consistency.

4. The use of images as the representation makes the method scalable to scenes of varying sizes, and the lack of per-scene optimization at test time makes it efficient.

5. Extensive experiments demonstrate the method's ability to synthesize high-quality scenes from very sparse inputs, outperforming existing approaches like NeRF and its variants.

In summary, the key novelty is the formulation of the new task and the proposed RGBD^2 approach that combines meshes, rendering, and diffusion models in an iterative view inpainting framework for consistent generative scene synthesis. The method is scalable, efficient, and produces superior results compared to previous works.
