# [Step-unrolled Denoising Autoencoders for Text Generation](https://arxiv.org/abs/2112.06749v3)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop an effective non-autoregressive text generation model that can produce high-quality samples while avoiding issues like slow sampling speed that plague autoregressive models. The key hypothesis proposed in the paper is that a model called Step-Unrolled Denoising Autoencoder (SUNDAE) can achieve strong performance by using an iterative denoising procedure during both training and generation. Specifically, the paper hypothesizes that "unrolling" the denoising process over multiple steps during training will better prepare the model for the inputs it will encounter during iterative generation. This unrolling mechanism is the key distinguishing feature of the SUNDAE model.The paper tests this central hypothesis by evaluating SUNDAE on unconditional text generation tasks and machine translation benchmarks. The results aim to demonstrate that SUNDAE can produce samples of comparable or higher quality than prior non-autoregressive and autoregressive models, while also decoding faster due to its non-autoregressive iterative approach. Evaluating the strengths and limitations of SUNDAE across different tasks and datasets is the main thrust of the empirical work in the paper.In summary, the central research question is how to develop an effective non-autoregressive text generation model, with the key hypothesis being that unrolling denoising over multiple steps is an effective training technique for this class of iterative generative models. The strengths and weaknesses of this approach are evaluated empirically on a diverse set of text modeling tasks.


## What is the main contribution of this paper?

The main contribution of this paper is a new generative model of text called Step-Unrolled Denoising Autoencoder (SUNDAE). The key ideas are:- It is a non-autoregressive model, meaning it does not generate text left-to-right like traditional language models. This allows for faster and more flexible generation.- It is based on the framework of denoising autoencoders. During training, text is corrupted by replacing some tokens randomly. The model tries to reconstruct the original uncorrupted text. - Critically, during training the model unrolls the denoising process for multiple steps, rather than just one step like regular denoising autoencoders. This is the key contribution called "unrolled denoising" that gives the model its name.- During sampling, the model iterates over several steps, updating the text at each step to improve it, starting from random noise.The main results are:- It achieves state-of-the-art results for non-autoregressive machine translation on WMT English-German without distillation.- It generates plausible unconditional samples on large datasets like C4 and GitHub Python code.- It can do text inpainting by filling in arbitrary blanks in a template, which is not possible with left-to-right autoregressive models.So in summary, the main contribution is proposing this new unrolled denoising training procedure for text generation, and showing it can enable non-autoregressive models to achieve strong generative modeling results without relying on autoregressive distillation.
