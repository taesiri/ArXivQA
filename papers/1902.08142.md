# [Evaluating the Search Phase of Neural Architecture Search](https://arxiv.org/abs/1902.08142)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How effective are the search strategies of current state-of-the-art neural architecture search (NAS) algorithms compared to random search? 

The key hypotheses tested in the paper are:

1) The search strategies of current NAS algorithms do not significantly outperform random search.

2) The widespread use of weight sharing in NAS algorithms degrades the ranking of candidate architectures, making the search process less effective.

In particular, the authors evaluate leading NAS algorithms like ENAS, DARTS and NAO on standard benchmarks and find their search strategies perform similarly or worse than random search. They also show weight sharing shuffles the architecture ranking during search, reducing correlation with true performance. 

Overall, the paper aims to analyze the search phase of NAS algorithms in a controlled manner to evaluate the effectiveness of different search strategies. The key finding is that current NAS techniques are not significantly better than random search due to constrained search spaces and the negative impact of weight sharing.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework to evaluate the search phase of neural architecture search (NAS) algorithms. The key ideas are:

- Comparing NAS algorithms to random search: The paper shows that state-of-the-art NAS algorithms like ENAS, DARTS and NAO do not significantly outperform random search on typical benchmark tasks like PTB and CIFAR-10. This suggests the search space may be constrained such that even random architectures perform well.

- Evaluating in reduced search spaces: The paper evaluates NAS algorithms in reduced spaces where all architectures can be exhaustively evaluated. This reveals the algorithms fail to find the best architectures, confirming they are ineffective searches.

- Analyzing impact of weight sharing: The paper shows the ranking of architectures during search with weight sharing does not correlate with their true ranking when trained normally. This degrades the search process and explains why NAS algorithms perform similar to random. Removing weight sharing improves correlation and search quality.

In summary, the key contribution is a framework to analyze the search phase of NAS algorithms by comparing to random search and evaluating in reduced spaces. This reveals limitations of current NAS methods like the negative impact of weight sharing. The paper argues this analysis approach will be key to designing more effective NAS algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes an evaluation framework to analyze the search phase of NAS algorithms by comparing them to random search, and finds that state-of-the-art NAS techniques do not significantly outperform random search due to the constrained search space and negative impact of weight sharing on ranking candidate architectures.


## How does this paper compare to other research in the same field?

 This paper focuses on evaluating the search phase of neural architecture search (NAS) algorithms in a rigorous manner. Some key ways it compares to prior work:

- Most prior NAS papers have evaluated algorithms solely based on the end results on a target task like image classification. This paper argues that the search phase itself should also be directly evaluated, not just the final architectures found. 

- To enable analyzing the search phase, the paper proposes comparing NAS algorithms to random search over the same search space. Most prior work either did not compare to random search, or did not do so in a thorough and fair manner. 

- The paper finds that state-of-the-art NAS algorithms actually perform similarly to random search, a surprising result. This suggests the search space constrains solutions more than these algorithms themselves.

- By examining reduced search spaces where all architectures can be evaluated, the paper shows these NAS algorithms fail to find the very best architectures. This demonstrates the search is not very effective.

- The paper identifies weight sharing as a key factor degrading NAS performance. It empirically shows weight sharing hurts ranking of architectures during search, limiting the algorithms.

- The analysis approach of studying reduced search spaces and the overall framework of evaluating search directly is novel compared to most prior NAS research.

In summary, while most NAS papers focus on architectures and tasks, this paper provides critical analysis about the search phase itself. The simple yet revealing comparisons to random search and examinations of weight sharing provide new insights into understanding and improving NAS.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing relaxed weight sharing strategies. The authors show that weight sharing harms the ranking of architectures during NAS search. They suggest designing new weight sharing approaches to address this issue.

- Evaluating different weight sharing schemes. The paper demonstrates the negative impact of weight sharing on NAS search, but does not deeply analyze different variants of weight sharing. The authors call for more research systematically evaluating different weight sharing techniques.

- Designing search spaces to enable effective search. The paper shows the importance of the search space in determining NAS algorithm performance. The authors recommend studying how to construct search spaces that contain diverse high-quality architectures findable by NAS methods.

- Analyzing the role of the performance predictor. NAO uses a performance predictor to guide architecture search. More analysis is needed on whether and why this is more effective than other approaches like RL or gradient-based search.

- Developing better random search baselines. The paper shows many NAS methods perform on par with or worse than random search. More work is needed on designing stronger random search policies for comparison.

- Evaluating search algorithms offline. Approaches like NASBench allow cheap offline evaluation by pre-computing architecture performance. The authors suggest expanding benchmark datasets like this.

- Studying search algorithm convergence. Little analysis exists on how quickly different NAS algorithms converge to high-quality architectures. More research is needed in this area.

In summary, the main suggestions are to better understand weight sharing, construct better search spaces, develop improved random search baselines, and thoroughly analyze the search phase itself, e.g. convergence. The overall goal is to design NAS methods reliably able to discover architectures better than random search.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space and validating the best architecture. Typically, NAS algorithms are compared solely based on the end results on a downstream task. However, this fails to explicitly evaluate the effectiveness of the search strategies themselves. The authors propose comparing NAS policies to random architecture selection. They find that on Penn Treebank and CIFAR-10 datasets, state-of-the-art NAS algorithms including DARTS, NAO, and ENAS perform similarly to a random search policy on average. Furthermore, they identify that weight sharing, which is widely used to reduce computational requirements, degrades the ranking of NAS candidates such that it does not reflect their true standalone performance. This reduces the effectiveness of the search process. The authors argue that explicitly evaluating the NAS search phase itself, rather than just the end results, will be key to designing search strategies that consistently discover architectures superior to random ones.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space to find the best model, and then validating that model. Typically, NAS algorithms are compared only based on the final validation results. However, this fails to explicitly evaluate how effective the search strategy itself is. 

The authors propose evaluating NAS by comparing the search to random architecture selection. They experiment with state-of-the-art NAS algorithms on the Penn Tree Bank and CIFAR-10 datasets. Surprisingly, they find these algorithms perform similarly to random search, indicating the search space is sufficiently constrained. Using reduced search spaces where all architectures can be evaluated, they show the algorithms fail to find top candidates. They identify weight sharing, widely used to reduce computation, significantly degrades ranking of candidates. Without weight sharing, search is superior to random. Overall, the proposed evaluation framework reveals deficiencies in existing NAS search strategies, paving the way for better techniques.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to evaluate the search phase of neural architecture search (NAS) algorithms, rather than just comparing their final architectures on a target task. The key ideas are:

- Compare NAS algorithms to a simple random search baseline that samples architectures from the same search space. Run multiple trials of each with different random seeds for a fair comparison.

- Use reduced search spaces where all architectures can be exhaustively evaluated to analyze if the NAS algorithms can actually find the top candidates.

- Identify weight sharing, used to reduce computational cost, as negatively impacting the ranking of architectures during search. This degrades search effectiveness.

- Propose metrics like Kendall Tau to measure correlation between rankings with and without weight sharing. Show weak correlation, indicating weight sharing changes relative architecture quality.

- Removing weight sharing and using true ranking to train NAS sampling policies improves search results and surpasses random search.

In summary, the paper proposes evaluating the NAS search phase itself through comparisons to random search and analysing factors like weight sharing. This is in contrast to just comparing end results. The analysis provides insights into deficiencies of current NAS algorithms.


## What problem or question is the paper addressing?

 The paper is addressing the evaluation of neural architecture search (NAS) algorithms. Specifically, it argues that existing NAS techniques are typically evaluated solely based on the end results of the architectures they discover on downstream tasks. However, this fails to explicitly evaluate the effectiveness of the search strategies themselves. 

The key questions the paper tries to address are:

- How do state-of-the-art NAS algorithms compare to simple random search in terms of the quality of architectures discovered? 

- Does the ranking of architectures during the search phase accurately reflect their true performance when trained from scratch?

- What factors may be leading to suboptimal search performance for current NAS methods?

To answer these questions, the authors propose evaluating NAS algorithms by comparing them to random search over the same search space. They also analyze the search process in reduced search spaces where all architectures can be exhaustively evaluated. Through these analyses, they identify issues with the search spaces and the use of weight sharing that help explain the limited gains of NAS methods over random search.

Overall, the paper argues for the importance of directly evaluating the NAS search phase itself, rather than just evaluating architectures on downstream tasks. The proposed analysis framework aims to provide insights into designing more effective NAS search strategies.


## What are the keywords or key terms associated with this paper?

 Here are some of the key terms and concepts from this paper:

- Neural Architecture Search (NAS): Automatically searching for neural network architectures for a given task.

- Search phase: The phase where NAS algorithms search the architecture space to find good candidates.

- Evaluation phase: The phase where the best architecture found by the search is trained from scratch and evaluated. 

- Weight sharing: A technique used by many NAS methods to greatly reduce the computational cost of the search by having candidate models share weights.

- Reduced search space: A smaller search space where all possible architectures can be evaluated exhaustively. Used to analyze the search phase.

- Kendall Tau: A metric to measure the correlation between two rankings. Used to compare architecture rankings with and without weight sharing. 

- Random search: A baseline search method that randomly samples architectures. State-of-the-art NAS algorithms are compared to this simple approach.

- RNN/CNN search spaces: The architecture search spaces for recurrent and convolutional neural networks commonly used in NAS research.

- PTB, CIFAR-10: Standard datasets used to evaluate NAS techniques for RNNs and CNNs respectively.

- Perplexity, accuracy: Evaluation metrics used for these tasks.

So in summary, the key focus is on evaluating the NAS search phase itself by comparing to random search, using reduced search spaces to enable exhaustive analysis, and identifying weight sharing as a core reason for the surprisingly mediocre performance of NAS techniques.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to summarize the key points of the paper:

1. What is the main contribution or purpose of this paper?

2. What problem is the paper trying to solve in neural architecture search (NAS)? 

3. What are the limitations or drawbacks of existing approaches for evaluating NAS algorithms?

4. What specific method does the paper propose for evaluating NAS search strategies? How does it work?

5. How does the proposed approach compare NAS algorithms to random search? Why is this comparison meaningful?

6. What datasets were used to evaluate NAS algorithms with the proposed approach? What architectures and algorithms were tested?

7. What were the main findings from evaluating NAS algorithms on standard search spaces? How did they compare to random search?

8. Why did the authors also evaluate NAS algorithms on reduced search spaces? What did this analysis reveal? 

9. What role does weight sharing play in the effectiveness of NAS search strategies? How did the authors evaluate this?

10. What are the key conclusions and implications of this work? How could it influence future NAS research?
