# [Evaluating the Search Phase of Neural Architecture Search](https://arxiv.org/abs/1902.08142)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How effective are the search strategies of current state-of-the-art neural architecture search (NAS) algorithms compared to random search? 

The key hypotheses tested in the paper are:

1) The search strategies of current NAS algorithms do not significantly outperform random search.

2) The widespread use of weight sharing in NAS algorithms degrades the ranking of candidate architectures, making the search process less effective.

In particular, the authors evaluate leading NAS algorithms like ENAS, DARTS and NAO on standard benchmarks and find their search strategies perform similarly or worse than random search. They also show weight sharing shuffles the architecture ranking during search, reducing correlation with true performance. 

Overall, the paper aims to analyze the search phase of NAS algorithms in a controlled manner to evaluate the effectiveness of different search strategies. The key finding is that current NAS techniques are not significantly better than random search due to constrained search spaces and the negative impact of weight sharing.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework to evaluate the search phase of neural architecture search (NAS) algorithms. The key ideas are:

- Comparing NAS algorithms to random search: The paper shows that state-of-the-art NAS algorithms like ENAS, DARTS and NAO do not significantly outperform random search on typical benchmark tasks like PTB and CIFAR-10. This suggests the search space may be constrained such that even random architectures perform well.

- Evaluating in reduced search spaces: The paper evaluates NAS algorithms in reduced spaces where all architectures can be exhaustively evaluated. This reveals the algorithms fail to find the best architectures, confirming they are ineffective searches.

- Analyzing impact of weight sharing: The paper shows the ranking of architectures during search with weight sharing does not correlate with their true ranking when trained normally. This degrades the search process and explains why NAS algorithms perform similar to random. Removing weight sharing improves correlation and search quality.

In summary, the key contribution is a framework to analyze the search phase of NAS algorithms by comparing to random search and evaluating in reduced spaces. This reveals limitations of current NAS methods like the negative impact of weight sharing. The paper argues this analysis approach will be key to designing more effective NAS algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes an evaluation framework to analyze the search phase of NAS algorithms by comparing them to random search, and finds that state-of-the-art NAS techniques do not significantly outperform random search due to the constrained search space and negative impact of weight sharing on ranking candidate architectures.


## How does this paper compare to other research in the same field?

 This paper focuses on evaluating the search phase of neural architecture search (NAS) algorithms in a rigorous manner. Some key ways it compares to prior work:

- Most prior NAS papers have evaluated algorithms solely based on the end results on a target task like image classification. This paper argues that the search phase itself should also be directly evaluated, not just the final architectures found. 

- To enable analyzing the search phase, the paper proposes comparing NAS algorithms to random search over the same search space. Most prior work either did not compare to random search, or did not do so in a thorough and fair manner. 

- The paper finds that state-of-the-art NAS algorithms actually perform similarly to random search, a surprising result. This suggests the search space constrains solutions more than these algorithms themselves.

- By examining reduced search spaces where all architectures can be evaluated, the paper shows these NAS algorithms fail to find the very best architectures. This demonstrates the search is not very effective.

- The paper identifies weight sharing as a key factor degrading NAS performance. It empirically shows weight sharing hurts ranking of architectures during search, limiting the algorithms.

- The analysis approach of studying reduced search spaces and the overall framework of evaluating search directly is novel compared to most prior NAS research.

In summary, while most NAS papers focus on architectures and tasks, this paper provides critical analysis about the search phase itself. The simple yet revealing comparisons to random search and examinations of weight sharing provide new insights into understanding and improving NAS.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing relaxed weight sharing strategies. The authors show that weight sharing harms the ranking of architectures during NAS search. They suggest designing new weight sharing approaches to address this issue.

- Evaluating different weight sharing schemes. The paper demonstrates the negative impact of weight sharing on NAS search, but does not deeply analyze different variants of weight sharing. The authors call for more research systematically evaluating different weight sharing techniques.

- Designing search spaces to enable effective search. The paper shows the importance of the search space in determining NAS algorithm performance. The authors recommend studying how to construct search spaces that contain diverse high-quality architectures findable by NAS methods.

- Analyzing the role of the performance predictor. NAO uses a performance predictor to guide architecture search. More analysis is needed on whether and why this is more effective than other approaches like RL or gradient-based search.

- Developing better random search baselines. The paper shows many NAS methods perform on par with or worse than random search. More work is needed on designing stronger random search policies for comparison.

- Evaluating search algorithms offline. Approaches like NASBench allow cheap offline evaluation by pre-computing architecture performance. The authors suggest expanding benchmark datasets like this.

- Studying search algorithm convergence. Little analysis exists on how quickly different NAS algorithms converge to high-quality architectures. More research is needed in this area.

In summary, the main suggestions are to better understand weight sharing, construct better search spaces, develop improved random search baselines, and thoroughly analyze the search phase itself, e.g. convergence. The overall goal is to design NAS methods reliably able to discover architectures better than random search.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space and validating the best architecture. Typically, NAS algorithms are compared solely based on the end results on a downstream task. However, this fails to explicitly evaluate the effectiveness of the search strategies themselves. The authors propose comparing NAS policies to random architecture selection. They find that on Penn Treebank and CIFAR-10 datasets, state-of-the-art NAS algorithms including DARTS, NAO, and ENAS perform similarly to a random search policy on average. Furthermore, they identify that weight sharing, which is widely used to reduce computational requirements, degrades the ranking of NAS candidates such that it does not reflect their true standalone performance. This reduces the effectiveness of the search process. The authors argue that explicitly evaluating the NAS search phase itself, rather than just the end results, will be key to designing search strategies that consistently discover architectures superior to random ones.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space to find the best model, and then validating that model. Typically, NAS algorithms are compared only based on the final validation results. However, this fails to explicitly evaluate how effective the search strategy itself is. 

The authors propose evaluating NAS by comparing the search to random architecture selection. They experiment with state-of-the-art NAS algorithms on the Penn Tree Bank and CIFAR-10 datasets. Surprisingly, they find these algorithms perform similarly to random search, indicating the search space is sufficiently constrained. Using reduced search spaces where all architectures can be evaluated, they show the algorithms fail to find top candidates. They identify weight sharing, widely used to reduce computation, significantly degrades ranking of candidates. Without weight sharing, search is superior to random. Overall, the proposed evaluation framework reveals deficiencies in existing NAS search strategies, paving the way for better techniques.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to evaluate the search phase of neural architecture search (NAS) algorithms, rather than just comparing their final architectures on a target task. The key ideas are:

- Compare NAS algorithms to a simple random search baseline that samples architectures from the same search space. Run multiple trials of each with different random seeds for a fair comparison.

- Use reduced search spaces where all architectures can be exhaustively evaluated to analyze if the NAS algorithms can actually find the top candidates.

- Identify weight sharing, used to reduce computational cost, as negatively impacting the ranking of architectures during search. This degrades search effectiveness.

- Propose metrics like Kendall Tau to measure correlation between rankings with and without weight sharing. Show weak correlation, indicating weight sharing changes relative architecture quality.

- Removing weight sharing and using true ranking to train NAS sampling policies improves search results and surpasses random search.

In summary, the paper proposes evaluating the NAS search phase itself through comparisons to random search and analysing factors like weight sharing. This is in contrast to just comparing end results. The analysis provides insights into deficiencies of current NAS algorithms.
