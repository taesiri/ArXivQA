# [Evaluating the Search Phase of Neural Architecture Search](https://arxiv.org/abs/1902.08142)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How effective are the search strategies of current state-of-the-art neural architecture search (NAS) algorithms compared to random search? 

The key hypotheses tested in the paper are:

1) The search strategies of current NAS algorithms do not significantly outperform random search.

2) The widespread use of weight sharing in NAS algorithms degrades the ranking of candidate architectures, making the search process less effective.

In particular, the authors evaluate leading NAS algorithms like ENAS, DARTS and NAO on standard benchmarks and find their search strategies perform similarly or worse than random search. They also show weight sharing shuffles the architecture ranking during search, reducing correlation with true performance. 

Overall, the paper aims to analyze the search phase of NAS algorithms in a controlled manner to evaluate the effectiveness of different search strategies. The key finding is that current NAS techniques are not significantly better than random search due to constrained search spaces and the negative impact of weight sharing.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework to evaluate the search phase of neural architecture search (NAS) algorithms. The key ideas are:

- Comparing NAS algorithms to random search: The paper shows that state-of-the-art NAS algorithms like ENAS, DARTS and NAO do not significantly outperform random search on typical benchmark tasks like PTB and CIFAR-10. This suggests the search space may be constrained such that even random architectures perform well.

- Evaluating in reduced search spaces: The paper evaluates NAS algorithms in reduced spaces where all architectures can be exhaustively evaluated. This reveals the algorithms fail to find the best architectures, confirming they are ineffective searches.

- Analyzing impact of weight sharing: The paper shows the ranking of architectures during search with weight sharing does not correlate with their true ranking when trained normally. This degrades the search process and explains why NAS algorithms perform similar to random. Removing weight sharing improves correlation and search quality.

In summary, the key contribution is a framework to analyze the search phase of NAS algorithms by comparing to random search and evaluating in reduced spaces. This reveals limitations of current NAS methods like the negative impact of weight sharing. The paper argues this analysis approach will be key to designing more effective NAS algorithms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes an evaluation framework to analyze the search phase of NAS algorithms by comparing them to random search, and finds that state-of-the-art NAS techniques do not significantly outperform random search due to the constrained search space and negative impact of weight sharing on ranking candidate architectures.


## How does this paper compare to other research in the same field?

 This paper focuses on evaluating the search phase of neural architecture search (NAS) algorithms in a rigorous manner. Some key ways it compares to prior work:

- Most prior NAS papers have evaluated algorithms solely based on the end results on a target task like image classification. This paper argues that the search phase itself should also be directly evaluated, not just the final architectures found. 

- To enable analyzing the search phase, the paper proposes comparing NAS algorithms to random search over the same search space. Most prior work either did not compare to random search, or did not do so in a thorough and fair manner. 

- The paper finds that state-of-the-art NAS algorithms actually perform similarly to random search, a surprising result. This suggests the search space constrains solutions more than these algorithms themselves.

- By examining reduced search spaces where all architectures can be evaluated, the paper shows these NAS algorithms fail to find the very best architectures. This demonstrates the search is not very effective.

- The paper identifies weight sharing as a key factor degrading NAS performance. It empirically shows weight sharing hurts ranking of architectures during search, limiting the algorithms.

- The analysis approach of studying reduced search spaces and the overall framework of evaluating search directly is novel compared to most prior NAS research.

In summary, while most NAS papers focus on architectures and tasks, this paper provides critical analysis about the search phase itself. The simple yet revealing comparisons to random search and examinations of weight sharing provide new insights into understanding and improving NAS.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing relaxed weight sharing strategies. The authors show that weight sharing harms the ranking of architectures during NAS search. They suggest designing new weight sharing approaches to address this issue.

- Evaluating different weight sharing schemes. The paper demonstrates the negative impact of weight sharing on NAS search, but does not deeply analyze different variants of weight sharing. The authors call for more research systematically evaluating different weight sharing techniques.

- Designing search spaces to enable effective search. The paper shows the importance of the search space in determining NAS algorithm performance. The authors recommend studying how to construct search spaces that contain diverse high-quality architectures findable by NAS methods.

- Analyzing the role of the performance predictor. NAO uses a performance predictor to guide architecture search. More analysis is needed on whether and why this is more effective than other approaches like RL or gradient-based search.

- Developing better random search baselines. The paper shows many NAS methods perform on par with or worse than random search. More work is needed on designing stronger random search policies for comparison.

- Evaluating search algorithms offline. Approaches like NASBench allow cheap offline evaluation by pre-computing architecture performance. The authors suggest expanding benchmark datasets like this.

- Studying search algorithm convergence. Little analysis exists on how quickly different NAS algorithms converge to high-quality architectures. More research is needed in this area.

In summary, the main suggestions are to better understand weight sharing, construct better search spaces, develop improved random search baselines, and thoroughly analyze the search phase itself, e.g. convergence. The overall goal is to design NAS methods reliably able to discover architectures better than random search.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space and validating the best architecture. Typically, NAS algorithms are compared solely based on the end results on a downstream task. However, this fails to explicitly evaluate the effectiveness of the search strategies themselves. The authors propose comparing NAS policies to random architecture selection. They find that on Penn Treebank and CIFAR-10 datasets, state-of-the-art NAS algorithms including DARTS, NAO, and ENAS perform similarly to a random search policy on average. Furthermore, they identify that weight sharing, which is widely used to reduce computational requirements, degrades the ranking of NAS candidates such that it does not reflect their true standalone performance. This reduces the effectiveness of the search process. The authors argue that explicitly evaluating the NAS search phase itself, rather than just the end results, will be key to designing search strategies that consistently discover architectures superior to random ones.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. Existing NAS techniques rely on two stages - searching over the architecture space to find the best model, and then validating that model. Typically, NAS algorithms are compared only based on the final validation results. However, this fails to explicitly evaluate how effective the search strategy itself is. 

The authors propose evaluating NAS by comparing the search to random architecture selection. They experiment with state-of-the-art NAS algorithms on the Penn Tree Bank and CIFAR-10 datasets. Surprisingly, they find these algorithms perform similarly to random search, indicating the search space is sufficiently constrained. Using reduced search spaces where all architectures can be evaluated, they show the algorithms fail to find top candidates. They identify weight sharing, widely used to reduce computation, significantly degrades ranking of candidates. Without weight sharing, search is superior to random. Overall, the proposed evaluation framework reveals deficiencies in existing NAS search strategies, paving the way for better techniques.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to evaluate the search phase of neural architecture search (NAS) algorithms, rather than just comparing their final architectures on a target task. The key ideas are:

- Compare NAS algorithms to a simple random search baseline that samples architectures from the same search space. Run multiple trials of each with different random seeds for a fair comparison.

- Use reduced search spaces where all architectures can be exhaustively evaluated to analyze if the NAS algorithms can actually find the top candidates.

- Identify weight sharing, used to reduce computational cost, as negatively impacting the ranking of architectures during search. This degrades search effectiveness.

- Propose metrics like Kendall Tau to measure correlation between rankings with and without weight sharing. Show weak correlation, indicating weight sharing changes relative architecture quality.

- Removing weight sharing and using true ranking to train NAS sampling policies improves search results and surpasses random search.

In summary, the paper proposes evaluating the NAS search phase itself through comparisons to random search and analysing factors like weight sharing. This is in contrast to just comparing end results. The analysis provides insights into deficiencies of current NAS algorithms.


## What problem or question is the paper addressing?

 The paper is addressing the evaluation of neural architecture search (NAS) algorithms. Specifically, it argues that existing NAS techniques are typically evaluated solely based on the end results of the architectures they discover on downstream tasks. However, this fails to explicitly evaluate the effectiveness of the search strategies themselves. 

The key questions the paper tries to address are:

- How do state-of-the-art NAS algorithms compare to simple random search in terms of the quality of architectures discovered? 

- Does the ranking of architectures during the search phase accurately reflect their true performance when trained from scratch?

- What factors may be leading to suboptimal search performance for current NAS methods?

To answer these questions, the authors propose evaluating NAS algorithms by comparing them to random search over the same search space. They also analyze the search process in reduced search spaces where all architectures can be exhaustively evaluated. Through these analyses, they identify issues with the search spaces and the use of weight sharing that help explain the limited gains of NAS methods over random search.

Overall, the paper argues for the importance of directly evaluating the NAS search phase itself, rather than just evaluating architectures on downstream tasks. The proposed analysis framework aims to provide insights into designing more effective NAS search strategies.


## What are the keywords or key terms associated with this paper?

 Here are some of the key terms and concepts from this paper:

- Neural Architecture Search (NAS): Automatically searching for neural network architectures for a given task.

- Search phase: The phase where NAS algorithms search the architecture space to find good candidates.

- Evaluation phase: The phase where the best architecture found by the search is trained from scratch and evaluated. 

- Weight sharing: A technique used by many NAS methods to greatly reduce the computational cost of the search by having candidate models share weights.

- Reduced search space: A smaller search space where all possible architectures can be evaluated exhaustively. Used to analyze the search phase.

- Kendall Tau: A metric to measure the correlation between two rankings. Used to compare architecture rankings with and without weight sharing. 

- Random search: A baseline search method that randomly samples architectures. State-of-the-art NAS algorithms are compared to this simple approach.

- RNN/CNN search spaces: The architecture search spaces for recurrent and convolutional neural networks commonly used in NAS research.

- PTB, CIFAR-10: Standard datasets used to evaluate NAS techniques for RNNs and CNNs respectively.

- Perplexity, accuracy: Evaluation metrics used for these tasks.

So in summary, the key focus is on evaluating the NAS search phase itself by comparing to random search, using reduced search spaces to enable exhaustive analysis, and identifying weight sharing as a core reason for the surprisingly mediocre performance of NAS techniques.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to summarize the key points of the paper:

1. What is the main contribution or purpose of this paper?

2. What problem is the paper trying to solve in neural architecture search (NAS)? 

3. What are the limitations or drawbacks of existing approaches for evaluating NAS algorithms?

4. What specific method does the paper propose for evaluating NAS search strategies? How does it work?

5. How does the proposed approach compare NAS algorithms to random search? Why is this comparison meaningful?

6. What datasets were used to evaluate NAS algorithms with the proposed approach? What architectures and algorithms were tested?

7. What were the main findings from evaluating NAS algorithms on standard search spaces? How did they compare to random search?

8. Why did the authors also evaluate NAS algorithms on reduced search spaces? What did this analysis reveal? 

9. What role does weight sharing play in the effectiveness of NAS search strategies? How did the authors evaluate this?

10. What are the key conclusions and implications of this work? How could it influence future NAS research?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes evaluating NAS algorithms by comparing them to random search. Why is random search an appropriate baseline for comparison? What are the benefits of using random search over other potential baselines?

2. When comparing the NAS algorithms to random search, the paper uses the same number of epochs and multiple random seeds for both. Why is this important for a fair comparison? How could the conclusions change if this methodology was not followed?

3. The paper finds the NAS algorithms perform similarly to random search in standard spaces. What explanations are proposed for this surprising result? How is this further analyzed using reduced search spaces?

4. How is the reduced RNN search space constructed in the paper? Why is a reduced space useful for analyzing the NAS algorithms? What are the limitations of reduced search spaces?

5. The paper identifies weight sharing as negatively impacting the ranking of architectures during search. How is this analyzed? Why does weight sharing introduce ranking differences compared to no weight sharing?

6. How is the correlation between weight sharing and no weight sharing rankings measured in the paper? Why is Kendall Tau an appropriate metric? What values of Kendall Tau indicate no correlation?

7. What adaptations were made to the NAS algorithms when evaluating them in the reduced search spaces? How could these modifications impact the relative performance of the algorithms?

8. For the NASBench experiments, how is the probability of surpassing random search calculated? What assumptions does this make? How should this metric be interpreted?

9. The paper suggests relaxed weight sharing could improve NAS algorithms. What approaches could be taken to relax weight sharing? What challenges might this introduce?

10. How well does the paper evaluate whether the conclusions generalize across different search spaces, tasks, and algorithms? What additional experiments could be run to further validate generalizability?


## Summarize the paper in one sentence.

 The paper proposes evaluating the search phase of neural architecture search algorithms by comparing them to random search.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes a framework to evaluate the search phase of neural architecture search (NAS) algorithms. The authors argue that comparing NAS algorithms solely based on downstream task performance fails to assess the effectiveness of the search strategies themselves. To address this, they propose comparing NAS algorithms to random architecture selection using the same search space and training hyperparameters. Experiments on PTB and CIFAR-10 datasets show that state-of-the-art NAS algorithms including ENAS, DARTS and NAO perform similarly to random search, indicating the search strategies are not more effective than random sampling. The authors identify two key factors limiting NAS algorithm performance - the constrained search space and the negative impact of weight sharing on ranking architectures during search. They suggest evaluating NAS algorithms by their ability to find architectures superior to random sampling, which will be key to developing more robust search strategies.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes evaluating NAS algorithms by comparing them to random search. Why is random search an appropriate baseline for comparison? What are the benefits of using random search over other potential baselines?

2. The paper finds that state-of-the-art NAS algorithms perform similarly to random search. What implications does this have for the effectiveness of current NAS techniques? How could NAS algorithms be improved to consistently outperform random search?

3. The paper identifies the search space design and weight sharing as two key factors impacting NAS algorithm performance. How do the search space design and weight sharing strategy specifically affect the NAS process and results?

4. For the reduced RNN and CNN search spaces, how were these spaces designed? What considerations went into determining the size and composition of these reduced spaces?

5. In the reduced spaces, what specifically causes the low correlation between weight sharing (WS) rankings and true rankings? How does the amount of weight sharing impact this correlation?

6. How does the ranking disorder induced by weight sharing negatively impact the NAS search phase? Why does a poor ranking of architectures degrade the overall search process?

7. The paper shows NAS algorithms without weight sharing can surpass random search. What is the intuition behind why removing weight sharing improves performance?

8. The evaluation methodology relies on multiple random seeds for robustness. How sensitive are the results to the number of random seeds used? How was the number of seeds chosen?

9. For adapting NAS algorithms to reduced spaces, what modifications were made? How do these adaptations reflect the real algorithm behavior in full spaces?

10. What future work does the paper propose based on the insights gained? What specific steps could be taken to design NAS techniques that surpass random search?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper evaluates the search phase of neural architecture search (NAS) algorithms by comparing them to random search. The authors find that on language modeling (Penn Tree Bank) and image classification (CIFAR-10), state-of-the-art NAS algorithms like ENAS, DARTS, and NAO perform similarly or sometimes worse than random search. To further analyze this, the authors reduce the search space to enable exhaustive evaluation of all possible architectures. They find NAS algorithms still fail to discover the best architectures in the reduced spaces. The paper identifies weight sharing, commonly used to make NAS more efficient, as a key factor harming NAS performance. Weight sharing dramatically changes the ranking of architectures from their true standalone performance. This ranking disorder then degrades the effectiveness of the search phase. Overall, the paper provides an evaluation framework to analyze the NAS search phase and reveals current algorithms rely heavily on constrained search spaces. The findings suggest improving search robustness by developing better weight sharing strategies as an important direction.
