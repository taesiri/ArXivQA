# [Positional Information is All You Need: A Novel Pipeline for   Self-Supervised SVDE from Videos](https://arxiv.org/abs/2205.08851)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. The authors propose a novel method to compute robust moving object masks, called SPIMO masks, by exploiting the implicit learning of moving objects by CNNs trained for monocular depth estimation. They show these masks are more effective at removing moving objects compared to prior auto-masking techniques. 2. They present a new training pipeline that utilizes random image resizing/cropping and forward warping losses to allow learning depth from high-resolution videos using cropped patches. This makes training more efficient.3. They introduce adaptive per-pixel quantization of depth/disparity values, which helps dedicate more sampling levels to the appropriate depth ranges per pixel. 4. They employ a two-stage training strategy, where the first stage learns on all pixels, and the second stage uses the SPIMO masks to ignore moving objects during training. A boosting technique further refines depths of moving objects.5. Their complete pipeline with the above contributions achieves state-of-the-art results for self-supervised monocular depth estimation on KITTI and CityScapes datasets, using much fewer parameters than prior work.In summary, the key novelty is in exploiting positional information to generate robust masks for removing moving objects, along with the new training strategy, to advance self-supervised depth learning from monocular videos. The adaptive quantization also helps produce more accurate depth maps.
