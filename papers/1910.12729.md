# [Towards Unsupervised Speech Recognition and Synthesis with Quantized   Speech Representation Learning](https://arxiv.org/abs/1910.12729)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn speech representations that are aligned with linguistic units like phonemes in a primarily unsupervised manner, using only a small amount of paired speech-phoneme data? The key ideas and contributions of the paper are:- Proposing a Sequential Representation Quantization AutoEncoder (SeqRQ-AE) framework that learns quantized speech representations from primarily unpaired audio data.- Using vector quantization and temporal segmentation to make the learned representations phoneme-synchronized and with a number of distinct representations close to the number of phonemes. - Demonstrating that the learned vowel representations have relative positions parallel to the IPA vowel chart defined by linguists.- Showing that with less than 20 minutes of paired data, the method outperforms prior work on phoneme recognition and synthesizing intelligible speech.So in summary, the central hypothesis is that with proper quantization and segmentation, representations that match linguistic units can be learned in a primarily unsupervised way, which is validated through experiments on speech recognition and synthesis tasks.
