# [SayPlan: Grounding Large Language Models using 3D Scene Graphs for
  Scalable Robot Task Planning](https://arxiv.org/abs/2307.06135)

## What is the central research question or hypothesis that this paper addresses?

 Based on a quick skim of the paper, it seems the central research question is how to enable large language models (LLMs) to generate feasible plans for robotics tasks that are grounded in large, complex environments spanning multiple floors and rooms. 

The key challenges the paper identifies are:

1) Representing large-scale environments in a way that LLMs can reason over without exceeding token limitations.

2) Ensuring LLM-generated plans adhere to constraints of the real physical environment. 

3) Avoiding erroneous or infeasible actions when generating long-horizon plans across expansive environments.

To address these challenges, the paper introduces "SayPlan", which uses 3D scene graphs to represent environments and proposes two main innovations:

1) A "semantic search" stage where the LLM explores a collapsed scene graph to identify a focused subgraph relevant for the task. This reduces the token footprint for the LLM.

2) An "iterative replanning" stage where the LLM plan is verified against a scene graph simulator and replanned with feedback to correct errors and ensure executability. 

The overall hypothesis seems to be that by combining these techniques with 3D scene graphs as the grounding representation, SayPlan will enable scalable and executable robotic task planning using LLMs across large, multi-room environments. Experiments across various tasks and environments are presented to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Introducing SayPlan, a novel framework that leverages Large Language Models (LLMs) for robot task planning using 3D scene graph representations of the environment. This allows for natural language instructions to be grounded in expansive, multi-floor environments.

2. A scalable approach to grounding LLMs in large environments using two key techniques:

- Semantic search: The LLM performs a graph search to identify a subgraph containing only the task-relevant entities, allowing it to plan with a reduced representation that fits within token limits.

- Iterative replanning: A pipeline that iteratively verifies plans using a scene graph simulator, providing feedback to the LLM to correct inconsistencies and avoid planning failures. 

3. Evaluation of SayPlan on large-scale environments spanning up to 3 floors, 36 rooms and 140 objects. The experiments validate the framework's ability to generate and implement successful plans based on natural language instructions across these complex scenes.

4. Demonstration of real robot task execution by translating SayPlan's high-level plans to a physical mobile manipulator using a vision-based motion planner. This shows the potential for scalable, language-driven robot planning in expansive real-world environments.

In summary, the key novelty and contribution appears to be using 3D scene graphs and targeted LLM search techniques to enable natural language robot planning at scales not demonstrated by prior work, while ensuring executability through an iterative replanning approach. The real robot experiments then validate this approach's applicability to physical environments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on skimming through the sections and reading the abstract, here is a one sentence summary of the paper: 

The paper introduces SayPlan, a novel approach that leverages 3D scene graphs and large language models to enable scalable task planning for robotics across large multi-room environments.


## How does this paper compare to other research in the same field?

 Here are a few key points of comparison between this paper and other related work in large language models for robotics planning:

- Scene grounding: This paper focuses on grounding the LLM's plan in a 3D scene graph representation of the environment. Other works have explored grounding in visual observations or object detections from images/video, or symbolic planning languages like PDDL. Grounding in a scene graph provides a richer semantic representation than raw perceptual input, and more flexibility than a formal planning language.

- Scalability: A core contribution of this paper is scaling LLM planning to large, multi-room environments using semantic search over scene graphs. Prior work has largely focused on smaller, single room domains. The proposed techniques for search over hierarchical scene graphs and iterative replanning enable scaling to more complex spaces.

- Long horizon planning: By integrating classical path planning and iterative replanning, this method aims to generate long horizon plans across multiple steps and rooms. Earlier LLM robotics planning has focused more on shorter horizon tasks confined to simpler spaces. The replanning framework helps ensure feasible long horizon plans.

- Real robot demonstration: This paper shows real mobile robot executions of LLM-generated plans, which is still relatively rare in LLM robotics research. The ability to deploy system on a physical robot in dynamic environments represents an important step towards real-world viability.

- Model scale: This paper experiments with large models like GPT-3.5 and GPT-4. Some prior work has used smaller LLMs. The scale of model does impact reasoning capability, so leveraging large LLMs is notable, though smaller models may also work with the right techniques.

So in summary, the focus on grounding in 3D scene graphs, scaling planning via graph search and replanning, generating complex long horizon plans, and real robot demonstration differentiate this paper from much of the closely related LLM robotics planning research. The ideas help push towards more practical, general, and scalable LLM-based robotics.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Explore fine-tuning large language models (LLMs) specifically for graph-based reasoning tasks. The paper notes limitations in the graph reasoning capabilities of current LLMs, such as performing simple distance-based or count-based reasoning over graph nodes. Fine-tuning could help improve performance on these types of reasoning tasks.

- Incorporate more advanced graph reasoning tools to aid the LLM's decision-making during semantic search and replanning. The authors note that current LLMs struggle with certain types of graph-based reasoning that are trivial for humans. Integrating dedicated graph reasoning modules could help overcome these limitations.

- Develop methods to update the scene graph representation online to account for a dynamic environment. The current framework relies on a static pre-built scene graph, limiting adaptability. Integrating incremental or online scene graph SLAM could address this issue.

- Expand the capabilities of the scene graph simulator to provide more detailed feedback. The paper notes that capturing all possible planning failures becomes difficult for more complex tasks in the current framework. Advancing the simulator could enable handling more complex failure cases during replanning.

- Evaluate the framework on more complex tasks involving diverse predicates and affordances. The paper's experiments focus on fairly simple navigation and manipulation tasks. Testing on more complex tasks would better reveal the capabilities and limits of the approach.

- Explore other potential applications of the semantic search technique, beyond task planning. The authors' method for efficiently searching large scene graphs could have value for other semantic search or reasoning problems.

In summary, key directions involve enhancing the LLMs' graph reasoning abilities, expanding the scene graph representations and simulators, and evaluating the approach on more complex planning tasks and applications. Advancing these areas could help further scale and improve the performance of LLM-based planning with scene graphs.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces SayPlan, a new framework that uses 3D scene graphs to help ground large language models for scalable robotic task planning. SayPlan operates in two key stages - semantic search and iterative replanning. In the semantic search stage, the large language model performs a kind of graph search on a collapsed scene graph to identify the subset of nodes relevant to the task. This allows it to focus only on the task-relevant aspects of large environments without exceeding token limits. The iterative replanning stage has the language model generate an initial high-level plan, which is refined through simulations on the full scene graph. Failures during simulation produce feedback that prompts the language model to modify the plan until it is executable. Evaluated on a range of semantic search and planning tasks in large multi-room/multi-floor environments, SayPlan demonstrates strong performance in producing feasible plans from natural language instructions across expansive scenes. The key innovations are using scene graph search to identify task-relevant subgraphs, integrating classical path planning to reduce the LLM's planning horizon, and iterative replanning with a simulator to ensure plan executability.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents SayPlan, a novel framework that leverages large language models (LLMs) and 3D scene graphs for scalable task planning in robotics. SayPlan operates in two key stages - semantic search and iterative replanning. In the first stage, the LLM conducts a graph search on a collapsed scene graph to identify a task-relevant subgraph for planning. This allows it to focus only on parts of a large environment that are needed to solve the task. In the second stage, the LLM generates a high-level plan which is completed by a classical path planner. The plan then goes through iterative replanning with a scene graph simulator to correct any inconsistencies and ensure executability. 

SayPlan is evaluated on semantic search and planning tasks across two large multi-room environments. For planning, it outperforms baselines in generating correct and executable plans. The key advantages of SayPlan are its ability to scale planning to large environments while maintaining a low token footprint, and the iterative replanning process which enables near perfect plan executability. Experiments validate SayPlan's efficacy in generating and implementing successful planning strategies based on natural language instructions.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces SayPlan, a novel framework that leverages large language models (LLMs) and 3D scene graphs for robotic task planning across expansive multi-floor environments. SayPlan operates in two key stages - semantic search and iterative replanning. In the semantic search stage, the LLM conducts a graph search on a collapsed 3D scene graph to identify a subgraph containing only the nodes relevant for solving the given task instruction. This allows the LLM to plan over large environments without exceeding token limits. In the iterative replanning stage, the LLM generates an initial high-level plan which is completed with optimal navigation paths from a classical planner. To ensure executability, this plan goes through multiple iterations of verification in a scene graph simulator and replanning by the LLM based on feedback until no constraints are violated. The key innovations are the graph search mechanism for creating focused subgraphs from large 3D scene graphs and the iterative replanning pipeline to correct inconsistencies, enabling scalable and executable robotic task planning using natural language instructions.
