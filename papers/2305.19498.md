# [Perception and Semantic Aware Regularization for Sequential Confidence   Calibration](https://arxiv.org/abs/2305.19498)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the calibration of deep sequence recognition (DSR) models. The key hypothesis is that incorporating perceptually and semantically similar sequences as additional supervision signals during training can help regulate the overconfidence of DSR models and achieve better calibration.

Specifically, the paper hypothesizes that:

1) Tokens/sequences that are perceptually similar (in visual/auditory features) to the target sequence are often confused by the model and assigned overly confident predictions. Regularizing with these sequences can improve perceptual discrimination. 

2) Sequences that are semantically/contextually similar to the target sequence also receive overconfident predictions. Regularizing with these sequences can teach the model about alternative valid sequences.

3) The degree of overconfidence varies across samples based on difficulty. Adaptively modulating the regularization intensity based on sample hardness can improve calibration across easy and difficult examples.

The proposed Perception and Semantic aware Sequence Regularization (PSSR) method implements this hypothesis by constructing perceptually and semantically similar sequence sets for each sample, and using them to regularize the training in an adaptive manner. Experiments on scene text recognition and speech recognition verify that PSSR effectively calibrates DSR models and outperforms prior state-of-the-art methods.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It identifies two causes of overconfidence in deep sequence recognition (DSR) models - perception similarity and semantic correlation of sequences. 

2. It proposes a Perception and Semantic aware Sequence Regularization (PSSR) framework to mitigate overconfidence in DSR models. PSSR incorporates perceptually and semantically similar sequences as regularization during training.

3. It introduces an adaptive calibration intensity module based on sample difficulty to achieve finer-grained regularization.

4. It provides comprehensive experiments on scene text recognition and speech recognition tasks, showing that PSSR improves calibration and sets new state-of-the-art results. 

5. It also demonstrates the benefits of PSSR for the downstream task of active learning.

In summary, the key contribution is proposing a novel regularization method to calibrate DSR models by exploiting perception and semantic similarities between sequences. This improves calibration while maintaining accuracy across various models, datasets and tasks.
