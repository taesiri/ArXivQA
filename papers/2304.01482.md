# [Defending Against Patch-based Backdoor Attacks on Self-Supervised   Learning](https://arxiv.org/abs/2304.01482)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we defend self-supervised learning models against patch-based backdoor attacks without using any trusted data or image labels? The paper proposes a new defense algorithm called PatchSearch to identify and filter out poisoned images that contain an adversary's trigger patch. This allows training a final model on clean data that is robust to backdoor attacks. The key aspects are:- PatchSearch does not require any trusted clean data or image labels, making it suitable for defending self-supervised models. - It locates candidate trigger patches in images using clustering and Grad-CAM. It assigns poison scores to rank images and identify highly poisonous ones.- An iterative search focuses only on likely poisonous clusters to efficiently find poisoned images. - A poison classifier further improves poison detection recall.- PatchSearch outperforms baselines like i-CutMix and prior work needing trusted data.- It works well even for adaptive attacks and attacking new downstream tasks.So in summary, the core research contribution is a new defense algorithm to mitigate backdoor attacks on self-supervised learning without needing any trusted data or labels.


## What is the main contribution of this paper?

The main contribution of this paper is proposing PatchSearch, a novel defense algorithm for defending self-supervised learning models against patch-based backdoor attacks. The key ideas are:- Use clustering on the SSL representations to group similar images together. Since poisoned images are visually similar, they are expected to cluster together. The cluster centroids can then be used as "pseudo-labels" in place of a supervised classifier. - Use Grad-CAM with the pseudo-labels to localize candidate trigger patches in images. Paste these patches in other images to quantify their poisonousness.- Do an iterative search to find highly poisonous clusters and rank all images by poison scores. Use top ranked poisonous images to train an accurate poison classifier. - Remove images deemed as poisonous by the classifier to clean up the training set. - Show that PatchSearch outperforms baselines like i-CutMix and prior work that relies on trusted data. Demonstrate PatchSearch works on different architectures like ResNet and ViT, at different poison injection rates, etc.In summary, the key contribution is an end-to-end pipeline for defending SSL against backdoor attacks by efficiently searching for and removing poisoned samples, without needing any trusted data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new defense algorithm called PatchSearch to detect and remove poisoned samples from the training data of self-supervised learning models, in order to defend against backdoor attacks that aim to cause misclassification of images containing a certain trigger patch.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper focuses specifically on defending self-supervised learning models against patch-based backdoor attacks. Much prior work has explored defending supervised models, but defenses tailored for self-supervised learning are still an emerging area.- The proposed defense PatchSearch does not require any trusted or clean data. Many prior defense methods rely on having some amount of clean data, which may not always be practical. Not needing trusted data is a notable advantage.- PatchSearch is shown to outperform existing defenses like i-CutMix augmentation and the trusted data + knowledge distillation method from Saha et al. Demonstrating improved defense efficacy over prior arts is a meaningful contribution.- The paper explores defenses for multiple self-supervised learning methods including BYOL, MoCo-v2, and MoCo-v3. Testing the defense across different algorithms makes the results more robust.- The threat model considered is a strong patch-based poisoning attack. The defense is successful against this challenging attack method, illustrating its effectiveness.- Ablation studies provide useful insights, like combining PatchSearch and i-CutMix for complementarity, the need for a poison classifier, etc. Thorough ablations strengthen the paper's contributions.Overall, by specializing in defending self-supervised learning, not needing any trusted data, outperforming prior defenses, and providing extensive experiments, this paper pushes forward the state-of-the-art in backdoor defense research. The techniques and analysis seem solid and add valuable knowledge to the field.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Exploring more advanced/adaptive forms of patch-based poisoning attacks that could potentially evade the defenses proposed in this work. The authors mention this as a limitation and suggest further work is needed in designing attacks as well as corresponding defenses.- Evaluating the effectiveness of PatchSearch against other kinds of backdoor triggers beyond small patch-based triggers, such as image-wide or blended triggers. The current method makes assumptions about the patch-based nature of triggers that may limit its effectiveness against future attacks.- Extending the evaluation to other SSL methods beyond MoCo, BYOL, and MAE evaluated in this paper. As the authors mention, their method may have different effectiveness against different SSL algorithms.- Exploring defenses that do not rely on any form of access to the poisoned training data, in contrast to PatchSearch which processes the poisoned data itself. Developing methods that can defend at test time without access to any training data could be an interesting direction.- Considering the impact of factors like architecture, training dataset, downstream task, etc. on the effectiveness of attacks and defenses. More extensive evaluation along these axes could reveal insights. - Developing certified defenses with provable robustness guarantees against patch-based poisoning attacks, as opposed to the empirical defense approach of PatchSearch.In summary, the authors lay out a research agenda around developing more sophisticated attacks tailored to SSL, designing corresponding defenses potentially without access to any training data, proving formal guarantees, and evaluating across diverse settings. Advancing along these directions could lead to more robust SSL methods.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a defense algorithm called PatchSearch to defend self-supervised learning models against patch-based backdoor attacks. These attacks insert a trigger patch into some training images to poison the model. The defense first trains a model on the poisoned data, then uses the trained model to identify poisoned images by locating suspicious patches and removing likely poisoned images. Specifically, it clusters the dataset, assigns poison scores to images based on patch classifications, iteratively searches for highly poisonous images, and trains a poison classifier. Experiments on ImageNet-100 show that PatchSearch successfully defends against the backdoor attack, outperforming baselines like the strong augmentation i-CutMix. Combining PatchSearch and i-CutMix gives a model close in performance to a clean model. Overall, the paper presents an effective defense method for patch-based backdoor attacks on self-supervised learning without needing any trusted clean data.
