# [Defending Against Patch-based Backdoor Attacks on Self-Supervised   Learning](https://arxiv.org/abs/2304.01482)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we defend self-supervised learning models against patch-based backdoor attacks without using any trusted data or image labels? The paper proposes a new defense algorithm called PatchSearch to identify and filter out poisoned images that contain an adversary's trigger patch. This allows training a final model on clean data that is robust to backdoor attacks. The key aspects are:- PatchSearch does not require any trusted clean data or image labels, making it suitable for defending self-supervised models. - It locates candidate trigger patches in images using clustering and Grad-CAM. It assigns poison scores to rank images and identify highly poisonous ones.- An iterative search focuses only on likely poisonous clusters to efficiently find poisoned images. - A poison classifier further improves poison detection recall.- PatchSearch outperforms baselines like i-CutMix and prior work needing trusted data.- It works well even for adaptive attacks and attacking new downstream tasks.So in summary, the core research contribution is a new defense algorithm to mitigate backdoor attacks on self-supervised learning without needing any trusted data or labels.


## What is the main contribution of this paper?

The main contribution of this paper is proposing PatchSearch, a novel defense algorithm for defending self-supervised learning models against patch-based backdoor attacks. The key ideas are:- Use clustering on the SSL representations to group similar images together. Since poisoned images are visually similar, they are expected to cluster together. The cluster centroids can then be used as "pseudo-labels" in place of a supervised classifier. - Use Grad-CAM with the pseudo-labels to localize candidate trigger patches in images. Paste these patches in other images to quantify their poisonousness.- Do an iterative search to find highly poisonous clusters and rank all images by poison scores. Use top ranked poisonous images to train an accurate poison classifier. - Remove images deemed as poisonous by the classifier to clean up the training set. - Show that PatchSearch outperforms baselines like i-CutMix and prior work that relies on trusted data. Demonstrate PatchSearch works on different architectures like ResNet and ViT, at different poison injection rates, etc.In summary, the key contribution is an end-to-end pipeline for defending SSL against backdoor attacks by efficiently searching for and removing poisoned samples, without needing any trusted data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new defense algorithm called PatchSearch to detect and remove poisoned samples from the training data of self-supervised learning models, in order to defend against backdoor attacks that aim to cause misclassification of images containing a certain trigger patch.
