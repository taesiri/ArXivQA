# [Defending Against Patch-based Backdoor Attacks on Self-Supervised   Learning](https://arxiv.org/abs/2304.01482)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we defend self-supervised learning models against patch-based backdoor attacks without using any trusted data or image labels? The paper proposes a new defense algorithm called PatchSearch to identify and filter out poisoned images that contain an adversary's trigger patch. This allows training a final model on clean data that is robust to backdoor attacks. The key aspects are:- PatchSearch does not require any trusted clean data or image labels, making it suitable for defending self-supervised models. - It locates candidate trigger patches in images using clustering and Grad-CAM. It assigns poison scores to rank images and identify highly poisonous ones.- An iterative search focuses only on likely poisonous clusters to efficiently find poisoned images. - A poison classifier further improves poison detection recall.- PatchSearch outperforms baselines like i-CutMix and prior work needing trusted data.- It works well even for adaptive attacks and attacking new downstream tasks.So in summary, the core research contribution is a new defense algorithm to mitigate backdoor attacks on self-supervised learning without needing any trusted data or labels.
