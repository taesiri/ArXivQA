# [Defending Against Patch-based Backdoor Attacks on Self-Supervised   Learning](https://arxiv.org/abs/2304.01482)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: How can we defend self-supervised learning models against patch-based backdoor attacks without using any trusted data or image labels? 

The paper proposes a new defense algorithm called PatchSearch to identify and filter out poisoned images that contain an adversary's trigger patch. This allows training a final model on clean data that is robust to backdoor attacks. The key aspects are:

- PatchSearch does not require any trusted clean data or image labels, making it suitable for defending self-supervised models. 

- It locates candidate trigger patches in images using clustering and Grad-CAM. It assigns poison scores to rank images and identify highly poisonous ones.

- An iterative search focuses only on likely poisonous clusters to efficiently find poisoned images. 

- A poison classifier further improves poison detection recall.

- PatchSearch outperforms baselines like i-CutMix and prior work needing trusted data.

- It works well even for adaptive attacks and attacking new downstream tasks.

So in summary, the core research contribution is a new defense algorithm to mitigate backdoor attacks on self-supervised learning without needing any trusted data or labels.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing PatchSearch, a novel defense algorithm for defending self-supervised learning models against patch-based backdoor attacks. The key ideas are:

- Use clustering on the SSL representations to group similar images together. Since poisoned images are visually similar, they are expected to cluster together. The cluster centroids can then be used as "pseudo-labels" in place of a supervised classifier. 

- Use Grad-CAM with the pseudo-labels to localize candidate trigger patches in images. Paste these patches in other images to quantify their poisonousness.

- Do an iterative search to find highly poisonous clusters and rank all images by poison scores. Use top ranked poisonous images to train an accurate poison classifier. 

- Remove images deemed as poisonous by the classifier to clean up the training set. 

- Show that PatchSearch outperforms baselines like i-CutMix and prior work that relies on trusted data. Demonstrate PatchSearch works on different architectures like ResNet and ViT, at different poison injection rates, etc.

In summary, the key contribution is an end-to-end pipeline for defending SSL against backdoor attacks by efficiently searching for and removing poisoned samples, without needing any trusted data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new defense algorithm called PatchSearch to detect and remove poisoned samples from the training data of self-supervised learning models, in order to defend against backdoor attacks that aim to cause misclassification of images containing a certain trigger patch.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related research:

- This paper focuses specifically on defending self-supervised learning models against patch-based backdoor attacks. Much prior work has explored defending supervised models, but defenses tailored for self-supervised learning are still an emerging area.

- The proposed defense PatchSearch does not require any trusted or clean data. Many prior defense methods rely on having some amount of clean data, which may not always be practical. Not needing trusted data is a notable advantage.

- PatchSearch is shown to outperform existing defenses like i-CutMix augmentation and the trusted data + knowledge distillation method from Saha et al. Demonstrating improved defense efficacy over prior arts is a meaningful contribution.

- The paper explores defenses for multiple self-supervised learning methods including BYOL, MoCo-v2, and MoCo-v3. Testing the defense across different algorithms makes the results more robust.

- The threat model considered is a strong patch-based poisoning attack. The defense is successful against this challenging attack method, illustrating its effectiveness.

- Ablation studies provide useful insights, like combining PatchSearch and i-CutMix for complementarity, the need for a poison classifier, etc. Thorough ablations strengthen the paper's contributions.

Overall, by specializing in defending self-supervised learning, not needing any trusted data, outperforming prior defenses, and providing extensive experiments, this paper pushes forward the state-of-the-art in backdoor defense research. The techniques and analysis seem solid and add valuable knowledge to the field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Exploring more advanced/adaptive forms of patch-based poisoning attacks that could potentially evade the defenses proposed in this work. The authors mention this as a limitation and suggest further work is needed in designing attacks as well as corresponding defenses.

- Evaluating the effectiveness of PatchSearch against other kinds of backdoor triggers beyond small patch-based triggers, such as image-wide or blended triggers. The current method makes assumptions about the patch-based nature of triggers that may limit its effectiveness against future attacks.

- Extending the evaluation to other SSL methods beyond MoCo, BYOL, and MAE evaluated in this paper. As the authors mention, their method may have different effectiveness against different SSL algorithms.

- Exploring defenses that do not rely on any form of access to the poisoned training data, in contrast to PatchSearch which processes the poisoned data itself. Developing methods that can defend at test time without access to any training data could be an interesting direction.

- Considering the impact of factors like architecture, training dataset, downstream task, etc. on the effectiveness of attacks and defenses. More extensive evaluation along these axes could reveal insights. 

- Developing certified defenses with provable robustness guarantees against patch-based poisoning attacks, as opposed to the empirical defense approach of PatchSearch.

In summary, the authors lay out a research agenda around developing more sophisticated attacks tailored to SSL, designing corresponding defenses potentially without access to any training data, proving formal guarantees, and evaluating across diverse settings. Advancing along these directions could lead to more robust SSL methods.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a defense algorithm called PatchSearch to defend self-supervised learning models against patch-based backdoor attacks. These attacks insert a trigger patch into some training images to poison the model. The defense first trains a model on the poisoned data, then uses the trained model to identify poisoned images by locating suspicious patches and removing likely poisoned images. Specifically, it clusters the dataset, assigns poison scores to images based on patch classifications, iteratively searches for highly poisonous images, and trains a poison classifier. Experiments on ImageNet-100 show that PatchSearch successfully defends against the backdoor attack, outperforming baselines like the strong augmentation i-CutMix. Combining PatchSearch and i-CutMix gives a model close in performance to a clean model. Overall, the paper presents an effective defense method for patch-based backdoor attacks on self-supervised learning without needing any trusted clean data.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes PatchSearch, a novel defense algorithm to protect self-supervised learning (SSL) models against patch-based backdoor attacks. In these attacks, an adversary injects poisoned images containing a trigger patch into the unlabeled training data. When used to pre-train a victim model, the poisoned data causes the model to misclassify images containing the trigger patch during downstream tasks. 

PatchSearch aims to identify and remove the poisoned images without requiring any labeled data or access to clean samples. It first trains an SSL model on the poisoned dataset. Then, it uses clustering, saliency maps, and iterative search to score images for poisonousness and identifies highly suspicious samples. These are used to train a poison classifier which filters out remaining poisoned images. Experiments show PatchSearch successfully defends SSL models by greatly reducing attack success rates. It outperforms baselines like strong data augmentations. PatchSearch is also complementary to augmentations like i-CutMix which boost model accuracy. Together they result in models resilient to backdoor attacks without sacrificing performance.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a defense algorithm called PatchSearch to mitigate patch-based backdoor attacks on self-supervised learning (SSL) models. The key ideas are:

- Train an SSL model on the poisoned dataset. Use the representations from this model to cluster the training data with k-means. 

- For a given image, use the cluster assignments as pseudo-labels and Grad-CAM to locate a candidate trigger patch. Paste this patch on other images and see if it changes their cluster assignment to measure the poisonousness of the patch.

- Do an iterative search to find clusters likely to contain poisons. Only score images from those clusters to efficiently find highly poisonous samples. 

- Use the top poisonous images to train a poison classifier. Remove images predicted as poisonous by the classifier to clean up the training set.

- Finally, train the SSL model on the cleaned up dataset to get a backdoor-free model.

The key novelty is efficiently searching for poisons without labels by relying on representations and clustering. PatchSearch also outperforms defenses that use additional trusted data.


## What problem or question is the paper addressing?

 Based on my reading of the paper, it appears the main problem the authors are trying to address is defending against patch-based backdoor attacks on self-supervised learning models. 

Specifically, recent work has shown that self-supervised learning models can be vulnerable to backdoor attacks where an attacker injects poisoned samples into the unlabeled training data. These poisoned samples contain a trigger patch and cause the model to misbehave when images with that trigger are presented during test time. 

The authors aim to develop defenses against such backdoor attacks that do not rely on access to trusted clean data or labels. They propose a method called PatchSearch that can identify and remove poisoned samples from the training data before training the final self-supervised learning model.

In summary, the key questions/problems addressed are:

- How to defend self-supervised learning models against backdoor attacks using poisoned training data? 

- How to identify and remove the poisoned samples without access to trusted clean data or labels?

- Can we develop defenses that are effective even with a high poison injection rate?

- How does the proposed defense compare with baselines and prior work on backdoor defenses for self-supervised learning?

So in essence, the main focus is on developing labeling and trusted data-free defenses for patch-based backdoor attacks on self-supervised learning.
