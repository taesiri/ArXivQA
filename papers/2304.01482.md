# [Defending Against Patch-based Backdoor Attacks on Self-Supervised   Learning](https://arxiv.org/abs/2304.01482)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we defend self-supervised learning models against patch-based backdoor attacks without using any trusted data or image labels? The paper proposes a new defense algorithm called PatchSearch to identify and filter out poisoned images that contain an adversary's trigger patch. This allows training a final model on clean data that is robust to backdoor attacks. The key aspects are:- PatchSearch does not require any trusted clean data or image labels, making it suitable for defending self-supervised models. - It locates candidate trigger patches in images using clustering and Grad-CAM. It assigns poison scores to rank images and identify highly poisonous ones.- An iterative search focuses only on likely poisonous clusters to efficiently find poisoned images. - A poison classifier further improves poison detection recall.- PatchSearch outperforms baselines like i-CutMix and prior work needing trusted data.- It works well even for adaptive attacks and attacking new downstream tasks.So in summary, the core research contribution is a new defense algorithm to mitigate backdoor attacks on self-supervised learning without needing any trusted data or labels.


## What is the main contribution of this paper?

The main contribution of this paper is proposing PatchSearch, a novel defense algorithm for defending self-supervised learning models against patch-based backdoor attacks. The key ideas are:- Use clustering on the SSL representations to group similar images together. Since poisoned images are visually similar, they are expected to cluster together. The cluster centroids can then be used as "pseudo-labels" in place of a supervised classifier. - Use Grad-CAM with the pseudo-labels to localize candidate trigger patches in images. Paste these patches in other images to quantify their poisonousness.- Do an iterative search to find highly poisonous clusters and rank all images by poison scores. Use top ranked poisonous images to train an accurate poison classifier. - Remove images deemed as poisonous by the classifier to clean up the training set. - Show that PatchSearch outperforms baselines like i-CutMix and prior work that relies on trusted data. Demonstrate PatchSearch works on different architectures like ResNet and ViT, at different poison injection rates, etc.In summary, the key contribution is an end-to-end pipeline for defending SSL against backdoor attacks by efficiently searching for and removing poisoned samples, without needing any trusted data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new defense algorithm called PatchSearch to detect and remove poisoned samples from the training data of self-supervised learning models, in order to defend against backdoor attacks that aim to cause misclassification of images containing a certain trigger patch.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper focuses specifically on defending self-supervised learning models against patch-based backdoor attacks. Much prior work has explored defending supervised models, but defenses tailored for self-supervised learning are still an emerging area.- The proposed defense PatchSearch does not require any trusted or clean data. Many prior defense methods rely on having some amount of clean data, which may not always be practical. Not needing trusted data is a notable advantage.- PatchSearch is shown to outperform existing defenses like i-CutMix augmentation and the trusted data + knowledge distillation method from Saha et al. Demonstrating improved defense efficacy over prior arts is a meaningful contribution.- The paper explores defenses for multiple self-supervised learning methods including BYOL, MoCo-v2, and MoCo-v3. Testing the defense across different algorithms makes the results more robust.- The threat model considered is a strong patch-based poisoning attack. The defense is successful against this challenging attack method, illustrating its effectiveness.- Ablation studies provide useful insights, like combining PatchSearch and i-CutMix for complementarity, the need for a poison classifier, etc. Thorough ablations strengthen the paper's contributions.Overall, by specializing in defending self-supervised learning, not needing any trusted data, outperforming prior defenses, and providing extensive experiments, this paper pushes forward the state-of-the-art in backdoor defense research. The techniques and analysis seem solid and add valuable knowledge to the field.
