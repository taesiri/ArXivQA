# [Can Large Language Models do Analytical Reasoning?](https://arxiv.org/abs/2403.04031)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper explores the analytical reasoning capabilities of cutting-edge large language models (LLMs) on sports data. Specifically, it examines the ability of models like GPT-3, GPT-4, Claude, Gemini, and Llama to count the total points scored by each team within a quarter in NBA and NFL games. This requires understanding the contextual play-by-play descriptions and performing consistent addition operations.

Methods:
The authors test five LLMs with three prompting techniques - natural instructions, instructions with JSON formatting, and chain of thought (CoT) prompting. They also use a divide-and-conquer approach that breaks the play-by-play data into smaller segments which are processed individually by the models before aggregating the results.

Key Findings:
1) GPT-4 performs the best, followed by Claude. The other models struggle with the analytical reasoning task.
2) The divide-and-conquer approach is most effective for GPT-4, boosting its NBA quarter score accuracy to 60%. The CoT strategy also helps GPT-4 and Claude significantly.
3) Surprisingly, most models fail to accurately count NBA quarter scores but show decent performance on NFL quarters. 

Analyses of Factors Affecting Complexity:
The authors analyze how context length, information density, and presence of related information impact task complexity. Key findings:
- Increasing context length increases difficulty
- Higher information density (more scoring plays per clip) increases difficulty 
- Including related but non-essential info generally helps model performance

Main Contributions:
1) Evaluation of LLMs on a sports analytical reasoning task requiring contextual understanding and math operations.
2) Analysis of different reasoning techniques like divide-and-conquer and CoT prompting.
3) Insights into the factors that contribute to task complexity for analytical reasoning.
4) Demonstration that despite advances, major gaps remain in LLMs' reasoning abilities on complex real-world tasks.

In summary, while models like GPT-4 show promise, there is still significant scope for progress on analytical reasoning tasks that require a nuanced understanding of context and math/logic based operations. The paper provides a blueprint for constructing such assessments to evaluate progress in this direction.
