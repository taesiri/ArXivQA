# [FormNetV2: Multimodal Graph Contrastive Learning for Form Document   Information Extraction](https://arxiv.org/abs/2305.02549)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we develop an effective multimodal pre-training approach for form document understanding that jointly leverages text, layout, and image modalities?Specifically, the key hypotheses appear to be:1) Incorporating image features from the region bounded by pairs of neighboring tokens can provide useful visual relationship cues for form understanding.2) Using graph contrastive learning on a multimodal graph representation of the document can enable joint self-supervised pre-training of all modalities without needing specialized objectives for each.3) This multimodal graph contrastive pre-training approach can lead to improved performance on downstream form document information extraction tasks compared to prior methods.The paper introduces FormNetV2, which builds on the FormNetV1 architecture by adding image features and graph contrastive learning. The experiments evaluate FormNetV2 on several form document datasets and aim to validate the above hypotheses by showing improved results compared to state-of-the-art models.
