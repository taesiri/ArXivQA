# [Softmax Attention with Constant Cost per Token](https://arxiv.org/abs/2404.05843)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Conventional transformer attention has quadratic complexity in sequence length. This limits its applicability for long sequences.
- Many approximations have been proposed but often sacrifice performance.

Proposed Solution:  
- Modify the conventional attention mechanism to use log-sums-of-exponentials instead of dot products. 
- Show this enables a decomposition into two log-cumulative-sums-of-exponentials.
- For autoregressive attention, this provides a constant time and space complexity per token.

Key Contributions:
- A simple modification to attention using logarithms and exponentials.  
- Mathematical proof showing this enables linearization via log-cumulative-sums.
- Constant per-token cost for autoregressive attention.
- Implementation in a small language model, achieving competitive performance.

Overall, the paper proposes a modified attention mechanism using logarithms/exponentials that enables decomposition into log-cumulative-sums. This provides a path to constant per-token cost for autoregressive attention. An implementation shows promising initial results. Further scale-up and benchmarking is needed to fully validate against state-of-the-art transformers.
