# [Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)](https://arxiv.org/abs/2402.17608)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper explores whether enhancing pre-trained Encoder-Decoder models like T5 with explicit linguistic knowledge can improve their performance on downstream tasks that rely on understanding linguistic properties of sentences. Specifically, it tests this on the task of predicting sentence complexity levels. 

Methodology:
The authors propose a two-step methodology:

1) Intermediate fine-tuning: T5 models are fine-tuned on an intermediate set of tasks requiring prediction of 10 linguistic properties of sentences related to complexity, derived from Universal Dependencies. Properties span raw text, POS tags, morphology, syntax etc. Fine-tuning is done in a multi-task setting over several epochs to make models "linguistically informed" (LiT5).

2) Target task fine-tuning: The LiT5 models are then fine-tuned on the sentence complexity prediction task using datasets of Italian and English sentences annotated with complexity scores.

Experiments are done with monolingual (English, Italian) and multilingual T5 models of increasing size. Training data for target task is also varied to test low resource scenarios. A cross-lingual setting is introduced to test linguistic fine-tuning in one language and target task in another.

Key Findings:

- Intermediate linguistic fine-tuning over epochs allows models to acquire better linguistic knowledge, especially smaller models.

- LiT5 models outperform pre-trained ones on target task, especially smaller models and in low resource scenarios. linguistically informing smaller models boosts efficiency.

- Positive impact seen in both monolingual and cross-lingual settings, highlighting portability of methodology. Enhancing multilingual models with language-specific linguistic knowledge is more effective.

- Only a few linguistic properties help target task, though a multi-task intermediate setting is better than single-task.

Main Contributions:

- Demonstrates benefit of enhancing Encoder-Decoder NLMs with explicit linguistic knowledge through intermediate fine-tuning

- Compares impact across model sizes, languages and data availability scenarios

- Tests cross-lingual viability of methodology and adaptations needed 

- Provides insights on most useful linguistic properties for enhancing complexity prediction

The paper highlights the potential for efficient, sustainably-trained smaller models, refined with linguistic knowledge, to surpass larger pre-trained models. It also reveals intricacies w.r.t languages, model sizes and data availability when conducting such linguistically-informed transfer learning.
