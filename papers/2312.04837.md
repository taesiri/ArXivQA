# [Localized Symbolic Knowledge Distillation for Visual Commonsense Models](https://arxiv.org/abs/2312.04837)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper proposes a new method called "Localized Symbolic Knowledge Distillation (LSKD)" to train visual reasoning models that can understand scenes at both the global and local level, by allowing users to specify objects/regions in an image through "pointing". LSKD involves using an LLM (ChatGPT) to generate commonsense inference statements conditioned on literal descriptions of images from VL models, including global image captions as well as local region descriptors. To address inaccuracies from the VL models, they train a critic model on human annotations to filter low-quality statements. The resulting 1M statement "Localized Commonsense Knowledge Corpus" is used to finetune VL models like BLIP-2 to support localized reasoning and reference-grounded tasks. Experiments show state-of-the-art zero-shot performance on 3 localized visual reasoning benchmarks over baselines, and human evaluation finds the student model can better answer localized visual questions than the teacher LLM. Key advantages are the method's scalability without human involvement, its applicability to improve existing VL models without architecture changes, and enabling more precise reasoning grounded to image regions.
