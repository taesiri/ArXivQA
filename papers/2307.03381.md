# [Teaching Arithmetic to Small Transformers](https://arxiv.org/abs/2307.03381)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can small transformer models efficiently learn basic arithmetic operations like addition, multiplication, and square root using only the standard autoregressive next-token prediction objective?The authors aim to understand the key factors that allow for the fast emergence of arithmetic capabilities in small transformers trained from scratch, without relying on pretraining or scale. Specifically, they investigate the effects of:- Data formatting: Reversing the output order, detailed scratchpad/chain-of-thought formatting - Data sampling: Balancing samples based on number of digits and carry operations- Mixing text and arithmetic data during training - Prompting: Few-shot prompting with examples - Pretraining: Fine-tuning larger pretrained models- Model scaleThrough extensive experiments and analysis, the authors demonstrate that high-quality, instructive data is crucial for eliciting arithmetic skills efficiently. They also highlight challenges like length generalization and make connections to matrix completion for explaining the rapid phase transitions observed. Overall, the paper provides insights into how to accelerate the emergence of basic arithmetic in small transformers using carefully designed training data.
