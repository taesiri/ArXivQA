# [Vector Search with OpenAI Embeddings: Lucene Is All You Need](https://arxiv.org/abs/2308.14963)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether a dedicated vector store is necessary to support vector search capabilities using transformer-based embeddings, or if existing search infrastructure centered around Lucene is sufficient. The paper challenges the prevailing narrative that a vector store is a required new component in an enterprise "AI stack" to enable dense retrieval with embeddings. Instead, it argues that Lucene's existing capabilities for indexing and searching vectors are adequate, making a separate vector store hard to justify from a cost-benefit perspective.The hypothesis is that Lucene provides "good enough" vector search to leverage the benefits of transformer embeddings, without the downside of increased complexity from introducing a wholly new software component into an organization's existing search infrastructure.The paper aims to support this hypothesis through an experimental demonstration of using OpenAI's Ada embeddings with Lucene indexing and search on the MS MARCO passage ranking task. The results show that this combination achieves effectiveness comparable to other state-of-the-art techniques, supporting the claim that Lucene meets the vector search needs for deploying bi-encoder dense retrieval in practice.In summary, the central research question is whether an organization really needs a dedicated vector store for production readiness with neural embeddings, or if Lucene can provide the necessary vector search capabilities without major downsides. The hypothesis is the latter, which the paper attempts to demonstrate empirically.


## What is the main contribution of this paper?

 The main contribution of this paper is to show that vector search using state-of-the-art OpenAI embeddings can be implemented directly in Lucene, without the need for a dedicated vector store. Specifically:- The authors index OpenAI's ada2 embeddings for the MS MARCO passage corpus directly in Lucene using HNSW indexes.- Experiments on MS MARCO and TREC DL benchmark datasets show that effectiveness is comparable to models that use dedicated vector stores like Faiss. - The implementation only relies on off-the-shelf components - OpenAI's embedding API for dense vectors and Lucene for indexing/search. No custom AI-specific code is needed.- This demonstrates that Lucene's capabilities are sufficient for vector search in a bi-encoder architecture, challenging the prevailing narrative that a dedicated vector store is required for state-of-the-art effectiveness.In summary, the paper argues that the additional complexity of introducing a new vector store into an enterprise architecture is hard to justify given Lucene's capabilities. The authors posit that Lucene is adequate for providing vector search in a cost-effective manner by building on existing infrastructure.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper challenges the prevailing narrative that dedicated vector stores are necessary for vector search, arguing that Lucene's capabilities are sufficient, thus avoiding increased complexity from introducing new software components.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on vector search and retrieval:- The paper challenges the prevailing narrative that a dedicated vector store/database is necessary for effective vector search with neural embeddings. It argues that Lucene's capabilities are sufficient. This goes against what many other papers assume or propose.- It provides an end-to-end demonstration of using Lucene for vector search with OpenAI embeddings on the MS MARCO dataset. Most prior work either focuses just on the embedding model or the retrieval method separately. The end-to-end empirical demonstration is a strength.- The results achieved with Lucene + OpenAI embeddings are competitive with state-of-the-art specialized dense retrieval methods on MS MARCO. This helps support the paper's argument that Lucene can be effective despite not being specialized for dense retrieval.- The paper emphasizes simplicity, reproducibility, and easy integration with existing infrastructure like Lucene/Elasticsearch. In contrast, much of the prior work on dense retrieval and vector stores builds custom implementations and platforms.- It does not propose any new embedding or indexing methods. The focus is on demonstrating capabilities and an alternative viewpoint to specialized vector stores, not advancing retrieval techniques.Overall, a unique aspect of this paper is critically examining the notion that vector search necessitates a specialized vector store. By empirically demonstrating that Lucene can deliver competitive results, it makes a persuasive case for leveraging existing search infrastructure over introducing additional complexity. The reproducibility and integration with real-world systems is also a differentiator from many theoretical retrieval papers.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Continuing to improve the performance of vector search in Lucene to close the gap with dedicated vector stores like Faiss. The paper acknowledges that Lucene currently lags behind in areas like indexing speed, query latency/throughput.- Exploring alternative approaches like fully managed vector search services (e.g. Vespa) or building vector search into relational databases (e.g. pgvector for Postgres). These could provide simpler paths to adopting vector search without adding complexity.- Evaluating vector search with Lucene on a broader range of tasks and test collections beyond MS MARCO passage ranking. The authors only present preliminary results on MS MARCO so far.- Examining tradeoffs between precision and efficiency more thoroughly for vector search in Lucene, such as tuning different HNSW parameters.- Implementing more sophisticated vector search architectures beyond basic bi-encoders, e.g. cross-encoders, reranking, hybrid sparse/dense. Lucene provides a platform to explore advanced techniques.- Continuing to build integrations between Lucene and embedding APIs like OpenAI's as these services evolve. Keeping Lucene vector search capabilities up-to-date.- Addressing reproducibility issues more thoroughly. The paper required some "janky" hacks to get Lucene working fully. Making vector search in Lucene more accessible.- Considering multimodal search, not just text. Lucene provides a pathway to supporting vector search over images, audio, etc.In summary, the authors position Lucene as a viable open-source platform for vector search that warrants ongoing research investment rather than shifting to proprietary vector stores. Their work highlights multiple avenues for future exploration.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:The paper challenges the prevailing view that enterprises require a dedicated "vector store" or "vector database" component in their AI architecture stack in order to support vector search with neural embeddings. The authors argue that Lucene, a popular open source search library already widely deployed in industry, has recently added capabilities for indexing and searching dense vectors that make a separate vector store unnecessary. They demonstrate state-of-the-art effectiveness on the MS MARCO passage ranking benchmark using OpenAI's ada2 embedding API and Lucene's vector search implementation. The results show that by combining these mature, off-the-shelf components, vector search can be achieved without complex custom AI infrastructure. The authors conclude that the benefits of a dedicated vector store likely do not outweigh the costs of added architectural complexity given Lucene's existing capabilities and traction.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:The paper argues against the prevailing view that enterprises require a dedicated "vector store" or "vector database" in their AI stack to support dense retrieval with neural embeddings. The authors contend that Lucene, the widely-adopted open source search library, already provides adequate capabilities for vector indexing and search via recent additions like HNSW indexes. Through an empirical demonstration on the MS MARCO dataset using OpenAI's ada2 embeddings and Lucene 9 for indexing, they show that state-of-the-art effectiveness can be achieved without a specialized vector store. The paper cautions that while Lucene lags behind dedicated vector stores in some aspects like performance and maturity, there are signs of strong ecosystem commitment and opportunities for improvement. Overall, the authors posit that the benefits of a separate vector store likely do not outweigh the complexity cost for most enterprises invested in Lucene.In summary, the central thesis is that Lucene can already support effective vector search without a dedicated vector store. The authors demonstrate this via an implementation using OpenAI embeddings and Lucene, while conceding some current limitations but expressing optimism about future progress. They argue the architectural complexity of adding a new vector store is unlikely to be justified given Lucene's capabilities.


## Summarize the main method used in the paper in one paragraph.

 The paper demonstrates vector search with OpenAI embeddings using Lucene on the MS MARCO passage ranking dataset. The key points are:- They encode the entire MS MARCO corpus using the OpenAI ada2 embedding API to generate passage embeddings. The embeddings are indexed in Lucene using HNSW indexes. - At query time, they generate query embeddings with the OpenAI API and perform nearest neighbor search using Lucene to retrieve the most similar passages. - They show that this approach, using an off-the-shelf embedding API and Lucene for search, can achieve effectiveness comparable to state-of-the-art methods on the MS MARCO dataset. - Their main argument is that dedicated vector stores are not necessary for deploying neural search in production, since capabilities like HNSW search exist in Lucene already. So by building on existing search infrastructure like Lucene, enterprises can adopt neural methods without much additional complexity.In summary, the paper demonstrates an end-to-end neural search pipeline using OpenAI embeddings and Lucene, challenging the need for specialized vector databases.
