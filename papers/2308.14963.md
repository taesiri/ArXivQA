# [Vector Search with OpenAI Embeddings: Lucene Is All You Need](https://arxiv.org/abs/2308.14963)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether a dedicated vector store is necessary to support vector search capabilities using transformer-based embeddings, or if existing search infrastructure centered around Lucene is sufficient. The paper challenges the prevailing narrative that a vector store is a required new component in an enterprise "AI stack" to enable dense retrieval with embeddings. Instead, it argues that Lucene's existing capabilities for indexing and searching vectors are adequate, making a separate vector store hard to justify from a cost-benefit perspective.The hypothesis is that Lucene provides "good enough" vector search to leverage the benefits of transformer embeddings, without the downside of increased complexity from introducing a wholly new software component into an organization's existing search infrastructure.The paper aims to support this hypothesis through an experimental demonstration of using OpenAI's Ada embeddings with Lucene indexing and search on the MS MARCO passage ranking task. The results show that this combination achieves effectiveness comparable to other state-of-the-art techniques, supporting the claim that Lucene meets the vector search needs for deploying bi-encoder dense retrieval in practice.In summary, the central research question is whether an organization really needs a dedicated vector store for production readiness with neural embeddings, or if Lucene can provide the necessary vector search capabilities without major downsides. The hypothesis is the latter, which the paper attempts to demonstrate empirically.
