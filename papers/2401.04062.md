# [Variance Reduction in Ratio Metrics for Efficient Online Experiments](https://arxiv.org/abs/2401.04062)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- A/B testing experiments are critical for data-driven decision making in online platforms, but they are expensive to run and often yield inconclusive results due to high variance in ratio metrics like click-through rate and retention. 
- The goal is to increase the statistical power and efficiency of A/B tests on ratio metrics, either by improving confidence at the same sample size or reducing required sample size for the same confidence.

Methodology:
- Apply variance reduction techniques using control variates that are correlated with the metric but independent of the treatment. This directly reduces the variance term in the z-test.
- Extend prior CUPED method to use pre-experiment metrics as covariates in a regression model to estimate control variate. Also propose using GBDT predictions as unbiased estimators to avoid overfitting.
- Measure variance reduction, $p$-value improvements, relative $z$-scores to quantify potential sample size reduction, and type-I errors.

Key Contributions:
- Empirically demonstrate that commonly used CUPED approach with multiple pre-experiment covariates leads to overfitting bias that offsets variance gains.
- Show that using just GBDT predictions as control variates leads to $p$-value reductions in 77% of cases, allowing 30% smaller sample sizes without loss in confidence.
- Type-I error remains around 5%, ensuring effects are not just from overfitting.
- Approach is validated on ShareChat's real-world A/B testing logs to demonstrate practical impact.

In summary, the paper makes both theoretical and empirical contributions to efficiently apply variance reduction to improve sensitivity of ratio metrics in A/B tests, with direct business impact.


## Summarize the paper in one sentence.

 This paper applies variance reduction techniques, specifically gradient boosted decision tree predictors, to ratio metrics in A/B testing experiments to increase statistical power and efficiency.


## What is the main contribution of this paper?

 Based on reviewing the paper, the main contribution appears to be:

The application and empirical validation of variance reduction techniques tailored specifically for ratio metrics in the context of A/B testing experiments. The key findings are:

- Using gradient boosted decision tree (GBDT) predictors as control variates leads to better sensitivity improvements compared to the conventional CUPED method with multiple covariates. This highlights the risk of overfitting and approximation bias when using multiple covariates.

- The GBDT approach increased statistical confidence (lower p-values) for 77% of historical experiments, allowing either more conclusive results or reduced sample sizes. This directly reduces experimentation cost and time.

- A median relative z-score of 1.19 suggests the GBDT method could achieve the same confidence with 30% fewer data points. This further showcases the potential cost and time savings.

In summary, the main contribution is showing both theoretically and empirically on a large-scale short-video platform that tailored variance reduction techniques can greatly improve the efficiency and sensitivity of ratio metrics in A/B testing.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Online controlled experiments / A/B testing
- Variance reduction techniques
- Ratio metrics (e.g. click-through rate, user retention)
- Sensitivity 
- Sample size reduction
- Control variates
- CUPED method
- Gradient Boosted Decision Trees (GBDTs)
- Approximation bias
- Average treatment effect (ATE)

The paper discusses applying variance reduction techniques like control variates and GBDT predictions to ratio metrics in A/B testing experiments, in order to improve the sensitivity/statistical power and reduce the required sample size. Key concepts revolve around variance reduction, bias-variance tradeoffs, ratio metrics, and efficient experimentation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using gradient boosted decision tree (GBDT) predictors rather than multiple covariates in the control variate. Why does adding more covariates not necessarily improve sensitivity according to the paper?

2. How does the bias-variance tradeoff come into play when considering multiple covariates versus GBDT predictors for control variates? Explain the overfitting issue.

3. The empirical results show higher variance reduction when using pre-experiment metrics but lower sensitivity improvements compared to using GBDT predictors. Explain this discrepancy. 

4. What is the intuition behind why lower variance does not necessarily lead to higher statistical power and lower p-values? Expand on why GBDTs avoid this issue.

5. The paper mentions GBDTs are unbiased estimators that account for complex relationships among pre-experiment metrics. Elaborate on what types of complex relationships GBDTs can capture that linear models cannot.

6. One of the main goals mentioned is to increase the sensitivity of ratio metrics used in A/B testing. Explain in detail how the proposed method achieves this goal.

7. How exactly are the GBDT predictors constructed and trained? What data is used and what is the model fitting procedure?

8. The empirical methodology relies on known A/B tests outcomes. Discuss the limitations of evaluating using historical data and how results could differ with new experiments.  

9. The paper claims up to 30% reduction in sample size could be achieved while retaining statistical power. Walk through the calculations that support this and the assumptions made.

10. How straightforward is implementation at scale across different types of platforms and metrics? What practical challenges need to be overcome?
