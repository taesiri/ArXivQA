# [VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding](https://arxiv.org/abs/2312.02310)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Existing video understanding models using large language models (LLMs) have limitations in aligning video and text features effectively. They use simple frame sampling and projection methods which lead to suboptimal video comprehension.  

Proposed Solution - VaQuitA:
- Implements a CLIP-score based frame sampling method to select frames more relevant to the input question. This aligns video data better to the question.

- Uses a Video Perceiver module to process video features and a Visual-Query Transformer (VQ-Former) to interleave video and text features. This enhances feature-level alignment. 

- Discovers that adding a "Please be critical" prompt before the input question improves LLM's video understanding ability significantly.

Key Contributions:
- Proposes CLIP-based video frame sampling for better video-text data alignment.

- Introduces Video Perceiver and VQ-Former for refined feature-level fusion of video and textual content.

- Uncovers effect of "Please be critical" prompt for boosting LLM's video comprehension.

- Sets new state-of-the-art results on zero-shot video QA tasks and generates high-quality multi-turn video conversations.

The core novelty lies in strengthening alignment of video and text information, both at the data and feature levels. This leads to models with better context understanding and reasoning ability for video question answering.
