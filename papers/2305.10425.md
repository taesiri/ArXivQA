# [SLiC-HF: Sequence Likelihood Calibration with Human Feedback](https://arxiv.org/abs/2305.10425)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses seem to be:- Can Sequence Likelihood Calibration (SLiC), a recently proposed contrastive learning method, be effectively adapted to leverage human preference feedback data to improve summarization models beyond standard supervised fine-tuning?- Can SLiC adapted for human feedback (SLiC-HF) provide a simpler, more efficient yet competitive alternative to prior Reinforcement Learning from Human Feedback (RLHF) methods like PPO for summarization?- Can off-policy/offline human preference data collected for different models still be effectively leveraged by SLiC-HF, without needing to collect new feedback data?The key ideas tested in the paper are:- Using SLiC-HF with a ranking model trained on human preference data to determine positive/negative pairs for contrastive calibration.- Applying SLiC-HF directly on off-policy human preference data collected for different models. - Comparing SLiC-HF to supervised fine-tuning baselines as well as prior RLHF methods on summarization.- Evaluating the computational efficiency, simplicity and performance of SLiC-HF relative to RLHF algorithms like PPO.In summary, the central hypothesis is that SLiC-HF can effectively leverage human preference data, even if collected off-policy, to improve summarization models beyond standard supervised learning baselines, in a simpler and more efficient way compared to prior RLHF techniques. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:- Showing how Sequence Likelihood Calibration (SLiC) can be used to effectively learn from human preferences (SLiC-HF), presenting a simpler and more efficient alternative to Reinforcement Learning from Human Feedback (RLHF). - Demonstrating that feedback/preference data collected for a different model (off-policy) can be effectively leveraged by SLiC-HF, making it unnecessary to collect new feedback data.- Providing a general SLiC-HF recipe based on open-sourced T5 models that outperforms RLHF on the Reddit TL;DR summarization task.- Showing SLiC-HF significantly improves supervised fine-tuning (SFT) baselines on the TL;DR task as judged by humans.- Applying SLiC-HF to a T5-XXL 11B parameter SFT model further improves results over smaller models.In summary, the main contribution is presenting SLiC-HF as a simpler, more efficient and effective alternative to RLHF for learning from human preferences to improve text generation models. The off-policy effectiveness enables easy use of existing preference datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes Sequence Likelihood Calibration with Human Feedback (SLiC-HF), a simple and efficient method to improve language generation models using human preference data, which is shown to be competitive with more complex reinforcement learning approaches on the Reddit TL;DR summarization task.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in summarization with human feedback:- The idea of using human feedback data to improve summarization models is not new. Prior works like Reinforcement Learning from Human Feedback (RLHF) and HUMAN have also collected human preference judgments on model outputs to refine the models. This paper builds on that line of work.- However, this paper proposes a novel method called SLiC-HF that uses Sequence Likelihood Calibration (SLiC) to incorporate the human feedback. This is a simpler and more efficient alternative to complex reinforcement learning algorithms like PPO which were used in past RLHF papers.- The paper shows that SLiC-HF can effectively leverage human feedback data collected for a different model (similar to off-policy RL data) to improve a new model. This could allow more easy transfer of existing human feedback datasets.- Experiments demonstrate SLiC-HF improves over supervised baselines and achieves competitive performance compared to RLHF, while being much simpler to implement and tune.- The computational and memory efficiency benefits of SLiC-HF over PPO are clearly analyzed. The decoding parallelism and caching optimizations are novel aspects.- The comparison between pairwise ranking models and pointwise reward models is insightful. The pairwise nature of human judgments is better matched in SLiC-HF.- Scaling experiments show significant gains from scaling up model size for SLiC-HF, suggesting it can effectively leverage very large models.Overall, this paper makes several strong contributions over prior work on training summarization models with human feedback through the novel SLiC-HF method and thoughtful experiments and analysis. The simplicity and efficiency of SLiC-HF are major advantages that are well highlighted.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some potential future research directions suggested by the authors include:- Applying SLiC-HF to other language generation tasks besides summarization, such as dialogue, translation, etc. They suggest exploring the effectiveness of SLiC-HF when using different reward functions.- Studying the use of SLiC-HF with non-human feedback, such as from other AI models. The authors note that SLiC-HF is indifferent to whether the feedback originates from humans or AI.- Scaling up SLiC-HF to even larger models. The authors show SLiC-HF benefits from using larger transformer models, so studying its effectiveness with models beyond 11B parameters could be interesting.- Comparing different approaches to generating the candidate sequences in SLiC-HF, beyond just sampling from the fine-tuned model. This may allow for better coverage of the sequence space.- Analyzing the theoretical properties of SLiC-HF more formally compared to RLHF algorithms like PPO. The empirical results show SLiC-HF is competitive, but more theoretical analysis could provide insights.- Applying offline RL algorithms to the human feedback data and comparing to SLiC-HF. The paper frames SLiC-HF as simpler alternative to RLHF, but offline RL is another approach worth exploring.- Studying whether SLiC-HF can enable better generalization to out-of-distribution examples compared to supervised fine-tuning alone.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes Sequence Likelihood Calibration with Human Feedback (SLiC-HF) as a way to improve language models using human preference data. SLiC-HF is based on the Sequence Likelihood Calibration (SLiC) method, which aligns a language model's sequence likelihoods to an arbitrary ranking function. The key idea is to use human preference judgments as the ranking function, so that the model assigns higher likelihood to sequences people prefer. The authors show SLiC-HF can leverage human feedback collected for other models (similar to off-policy RL), avoiding the need to collect new data. Experiments on TL;DR summarization of Reddit posts demonstrate SLiC-HF significantly improves supervised fine-tuning baselines and provides a simpler, more efficient alternative to past Reinforcement Learning from Human Feedback work. Specifically, a 770M parameter SLiC-HF model performs comparably to a 6B parameter RLHF model, and further gains are achieved by scaling up SLiC-HF to 11B parameters. The authors highlight SLiC-HF's advantages in compute efficiency, ease of implementation, and its use of pairwise rankings instead of pointwise rewards. Overall, the work demonstrates the promise of simple contrastive methods like SLiC-HF to align large language models with human preferences.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes SLiC-HF, a method to improve language models using human feedback data. SLiC-HF applies Sequence Likelihood Calibration (SLiC) to calibrate a model's sequence likelihoods according to human preferences. The authors show SLiC-HF can effectively leverage human feedback data collected for different models, similar to off-policy reinforcement learning. Experiments on Reddit TL;DR summarization show SLiC-HF significantly improves supervised fine-tuning baselines and provides a simpler, more efficient alternative to RLHF-PPO previously used. SLiC-HF applied to a 770M parameter model performs similarly to a 6B parameter RLHF-PPO model. Further scaling SLiC-HF to 11B parameters significantly improves results. The authors discuss compute efficiency and ranking advantages of SLiC-HF over RLHF-PPO.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes Sequence Likelihood Calibration with Human Feedback (SLiC-HF), which uses the Sequence Likelihood Calibration (SLiC) approach to align a language model's sequence likelihood with human preferences. SLiC is a sequence-level contrastive method that seeks to optimize the expected reward of model generated text sequences. The key idea is to calibrate the model's sequence likelihood by contrasting positive and negative text sequences, encouraging the model to assign more probability mass to sequences preferred by humans. The authors show how SLiC can be adapted to learn from human preference data, which is collected via side-by-side comparison of model generated summaries. They consider two approaches: 1) SLiC-HF sample-rank, where candidate summaries are first sampled from the model, then ranked using a trained ranking model or reward model to determine positive and negative pairs, and 2) SLiC-HF direct, where human preference data is directly used without sampling. The main finding is that SLiC-HF significantly improves supervised fine-tuning baselines and provides a simpler, more efficient alternative to RLHF methods used in prior work. Experiments on Reddit TL;DR summarization demonstrate SLiC-HF can effectively leverage human preference data to improve summarization quality.


## What problem or question is the paper addressing?

 This paper presents a method called SLiC-HF for improving language models using human feedback data. The key points are:- Past work has used Reinforcement Learning from Human Feedback (RLHF) to improve models by optimizing them to get higher rewards from a reward model trained on human preferences. This is complex and inefficient. - This paper shows how Sequence Likelihood Calibration (SLiC), a simpler contrastive method, can also effectively learn from human preferences when combined with human feedback data (SLiC-HF).- Experiments on summarization show SLiC-HF significantly improves standard supervised fine-tuning and performs competitively compared to past RLHF work, while being simpler, more efficient, and easier to implement.- A recipe is provided for applying SLiC-HF to leverage existing human feedback datasets to improve open-source T5 models.In summary, the key contribution is presenting SLiC-HF as a simpler and more efficient alternative to RLHF for learning from human feedback, and showing it works well for the task of summarization. The paper focuses on the TL;DR summarization task using Reddit data and compares SLiC-HF models against baselines and prior RLHF models.
