# [Fcaformer: Forward Cross Attention in Hybrid Vision Transformer](https://arxiv.org/abs/2211.07198)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new vision transformer model called FcaFormer, which aims to improve model performance by densifying the attention pattern rather than sparsifying it like many prior works. 

The key ideas are:

- Introduce forward cross attention between blocks in the same stage, where tokens from previous blocks are reused as extra input to the attention modules in later blocks. 

- Use a token merge and enhancement (TME) module to generate representative cross tokens from previous blocks through depthwise convolutions. 

- Introduce learnable scale factors (LSFs) to calibrate the statistics of cross tokens before they are integrated with regular tokens.

The central hypothesis is that reusing tokens across blocks and enhancing interactions of tokens from different semantic levels will improve model performance with limited extra computational cost. Experiments on image classification, segmentation and detection verify that the proposed FcaFormer models outperform baseline vision transformers consistently across tasks and scales.

To summarize, the paper proposes a new densified attention pattern that reuse tokens across transformer blocks, and demonstrates its effectiveness empirically on various vision tasks.
