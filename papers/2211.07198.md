# [Fcaformer: Forward Cross Attention in Hybrid Vision Transformer](https://arxiv.org/abs/2211.07198)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new vision transformer model called FcaFormer, which aims to improve model performance by densifying the attention pattern rather than sparsifying it like many prior works. 

The key ideas are:

- Introduce forward cross attention between blocks in the same stage, where tokens from previous blocks are reused as extra input to the attention modules in later blocks. 

- Use a token merge and enhancement (TME) module to generate representative cross tokens from previous blocks through depthwise convolutions. 

- Introduce learnable scale factors (LSFs) to calibrate the statistics of cross tokens before they are integrated with regular tokens.

The central hypothesis is that reusing tokens across blocks and enhancing interactions of tokens from different semantic levels will improve model performance with limited extra computational cost. Experiments on image classification, segmentation and detection verify that the proposed FcaFormer models outperform baseline vision transformers consistently across tasks and scales.

To summarize, the paper proposes a new densified attention pattern that reuse tokens across transformer blocks, and demonstrates its effectiveness empirically on various vision tasks.


## What is the main contribution of this paper?

 This paper proposes a new vision transformer called FcaFormer, which improves model performance by densifying the attention pattern rather than sparsifying it like many previous works. The key contributions are:

- It proposes a new FcaFormer block that introduces forward cross attention to reuse and integrate tokens from previous blocks in the same stage. This enhances interactions between tokens of different semantic levels.

- Two new components are introduced - learnable scaling factors (LSFs) and a token merge and enhancement (TME) module. LSFs help calibrate the statistics of cross tokens before integrating them. TME aggregates cross tokens spatially and enhances regular tokens.

- FcaFormer models built with the proposed blocks achieve SOTA accuracy under similar model size constraints. For example, FcaFormer-L1 achieves 80.3% top-1 accuracy on ImageNet with only 6.2M parameters, outperforming EfficientNet-L1 by 1.1%.

- Experiments on image classification, semantic segmentation, and object detection demonstrate the consistent effectiveness of FcaFormer blocks over baseline models like DeiT, Swin Transformers and ConvNeXT.

- Detailed analysis shows the extra computation cost of densified attention is limited, as most cost is in the FFN part. FcaFormer can reuse representations to achieve better accuracy with fewer parameters.

In summary, the key novelty is introducing forward cross attention to reuse and densify connections across transformer blocks, enabled by the proposed LSFs and TME modules. This opens a new direction for designing efficient vision transformer models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a new vision transformer architecture called FcaFormer which densifies attention patterns by reusing features from previous blocks as additional inputs to later blocks, and shows this improves accuracy and efficiency versus baseline ConvNet and ViT models.
