# Efficient Explanations from Empirical Explainers

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be whether efficient "Empirical Explainers" can be trained to approximate the attribution maps of more computationally expensive explainers, as a way to mitigate the computational burden and ecological impact of generating neural explanations. Specifically, the authors propose using feature attribution modelling to train Empirical Explainers that learn from data to predict the attribution maps that would be generated by expensive explainers like Integrated Gradients and Shapley Values. They then evaluate whether these Empirical Explainers can generate explanations with significant accuracy compared to the expensive methods, but at a much lower computational cost.The central hypothesis appears to be that Empirical Explainers can model expensive explainers with enough fidelity to be useful in applications where some approximation error is tolerable, thereby providing a more efficient way to generate explanations for neural models. The authors test this hypothesis by training and evaluating Empirical Explainers on language tasks.In summary, the main research question is whether efficient Empirical Explainers can be trained to approximate expensive explainers well enough to mitigate the computational costs and ecological impacts of generating neural explanations in certain applications. The hypothesis is that this is feasible, which the authors evaluate empirically.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing the task of feature attribution modelling to efficiently approximate expensive attribution methods like Integrated Gradients and Shapley Values.2. Introducing Empirical Explainers - models trained to predict the attribution maps of expensive explainers, which require only a single pass through the Empirical Explainer at test time rather than multiple passes through the original model. 3. Evaluating Empirical Explainers on four language tasks with Integrated Gradients and Shapley Values as the target explainers. The results show the Empirical Explainers can approximate the attributions of the expensive methods fairly accurately, while being much more efficient.In summary, the key contribution seems to be introducing the idea of training efficient Empirical Explainers to model the behavior of expensive attribution methods, providing a way to get approximate explanations much more efficiently. The proposed Empirical Explainers demonstrate this is feasible in the language domain, providing a path to greener and more efficient explanations in cases where approximation errors are tolerable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes training Empirical Explainers, smaller neural models that efficiently approximate the attribution maps of computationally expensive explanation methods for neural networks, as a way to reduce the ecological impact of generating explanations in production environments.
