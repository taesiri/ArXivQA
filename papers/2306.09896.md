# [Demystifying GPT Self-Repair for Code Generation](https://arxiv.org/abs/2306.09896)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How effective is self-repair with textual feedback for improving code generation by large language models, and what factors influence its effectiveness? The key hypotheses tested in the paper are:1) Self-repair can boost the performance of large language models on challenging code generation tasks, when taking into account the computational cost of generating the repairs.2) The strength of the model used for generating feedback influences the effectiveness of self-repair, with stronger feedback models enabling better repair. 3) Replacing the model's self-generated feedback with feedback from human experts further improves the success rate of repair.The paper evaluates these hypotheses through experiments with GPT-3.5 and GPT-4 on programming challenges from the APPS dataset. It introduces a new metric, pass@t, that accounts for the cost of self-repair by measuring pass rate against total tokens sampled. Using this metric, the authors find that self-repair helps for GPT-4 but not GPT-3.5, and performance is highly dependent on diversity of initial samples. Replacing GPT-3.5's feedback with GPT-4's improves repair, and human feedback further boosts success rate compared to the model's self-generated feedback.In summary, the central research question is assessing the promise and limitations of self-repair with natural language feedback for code generation models, with a focus on the importance of the strength of the feedback model. The hypotheses test how factors like model scale, feedback quality, and human input impact the effectiveness of self-repair.
