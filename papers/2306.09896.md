# [Demystifying GPT Self-Repair for Code Generation](https://arxiv.org/abs/2306.09896)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How effective is self-repair with textual feedback for improving code generation by large language models, and what factors influence its effectiveness? The key hypotheses tested in the paper are:1) Self-repair can boost the performance of large language models on challenging code generation tasks, when taking into account the computational cost of generating the repairs.2) The strength of the model used for generating feedback influences the effectiveness of self-repair, with stronger feedback models enabling better repair. 3) Replacing the model's self-generated feedback with feedback from human experts further improves the success rate of repair.The paper evaluates these hypotheses through experiments with GPT-3.5 and GPT-4 on programming challenges from the APPS dataset. It introduces a new metric, pass@t, that accounts for the cost of self-repair by measuring pass rate against total tokens sampled. Using this metric, the authors find that self-repair helps for GPT-4 but not GPT-3.5, and performance is highly dependent on diversity of initial samples. Replacing GPT-3.5's feedback with GPT-4's improves repair, and human feedback further boosts success rate compared to the model's self-generated feedback.In summary, the central research question is assessing the promise and limitations of self-repair with natural language feedback for code generation models, with a focus on the importance of the strength of the feedback model. The hypotheses test how factors like model scale, feedback quality, and human input impact the effectiveness of self-repair.


## What is the main contribution of this paper?

The main contribution of this paper appears to be analyzing the effectiveness of self-repair techniques for code generation with large language models like GPT-3.5 and GPT-4. Specifically:- They propose a new evaluation metric called "pass@t" that measures pass rate against the total number of tokens sampled, to account for the cost of generating feedback and carrying out repairs.- Using this metric, they find that GPT-3.5 is unable to effectively carry out self-repair on complex coding challenges, while GPT-4 shows modest gains.- They show that replacing GPT-3.5's feedback with GPT-4's improves repair performance, suggesting the feedback stage is a bottleneck. - Replacing GPT-4's feedback with that from human experts further improves repair performance, indicating there is still room for improvement in the models' ability to provide useful debugging information.Overall, the main contribution seems to be a detailed analysis and empirical characterization of the strengths and limitations of self-repair techniques for code generation when using state-of-the-art LLMs like GPT-3.5 and GPT-4. The new evaluation metric and ablations help reveal insights into when and how self-repair can be effectively leveraged.
