# [Automatic Optimisation of Normalised Neural Networks](https://arxiv.org/abs/2312.10672)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural networks with layerwise parameter normalization are useful for control applications since normalization bounds the Lipschitz constant and improves stability. 
- However, standard gradient descent methods are inconsistent with the geometry of normalized parameters, which lie on spheres. 
- Prior Riemannian optimization methods are expensive and have not seen widespread adoption. Manually tuning learning rates for normalized networks is also difficult.

Proposed Solution:
- The paper proposes two automated optimization methods tailored for layerwise normalized neural networks:
   1. Hessian-Free Automatic Differentiation (AD) method: Determines step size by minimizing a 2nd order Taylor approximation of the loss along the update direction.
   2. Majorization-Minimization (MM) method: Constructs an upper bound on the loss and minimizes this majorant to determine step size.

- Both methods explicitly account for the spherical geometry by parameterizing the update as motion along a great circle.

- The AD method uses automatic differentiation to compute the 2nd order directional derivative to determine optimal step size. No manual tuning or learning rate scheduling is needed.

- The MM method constructs a conservative analytic upper bound on the loss. Minimizing this majorant gives the step size.

Contributions:
- Provides a principled initialization and update procedure for normalized networks.
- New optimization methods that automatically adapt step size while respecting spherical geometry.
- Experiments show rapid initial convergence and consistent performance across architectures.
- Hessian-free AD method gave fastest convergence, achieving under 1N RMS error with only 200 iterations.
- Methods enable automated training of normalized networks without manual tuning.

In summary, the paper introduces optimization techniques tailored for normalized neural networks that automatically determine step size while accounting for the non-Euclidean parameter geometry. The methods enable stable automated training without costly Hessian computations or manual tuning. Experiments demonstrate consistent and rapid convergence across tasks and architectures.
