# [Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense   of Privacy](https://arxiv.org/abs/2403.01218)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Inexact machine unlearning aims to efficiently "unlearn" the influence of certain training data points on a model without having to retrain from scratch. However, it is unclear how to properly evaluate whether these techniques actually remove data influence.

- Commonly used evaluation approaches rely on membership inference attacks (MIAs) that are adapted to the unlearning setting (U-MIAs). However, existing U-MIAs may be too weak and provide an overoptimistic view of privacy gains. 

- The paper argues that stronger, per-example U-MIAs are needed to properly evaluate inexact unlearning techniques. These attacks create decision rules tailored to each sample, rather than population-level rules.

Methodology
- The paper adapts a state-of-the-art per-example MIA called LiRA to the unlearning setting (U-LiRA). This attack constructs sample-specific models of the training vs non-training data distributions.

- Several inexact unlearning algorithms from recent papers are evaluated on image classification and language models using U-LiRA. Their privacy gains are compared to those estimated by weaker population U-MIAs.

Key Results
- U-LiRA is much more successful against unlearning algorithms than population U-MIAs, showing privacy gains have been overestimated.

- There is high variance in vulnerability across different examples after unlearning. Many algorithms reduce risk for some examples while increasing it for others.

- Unlearning often worsens privacy for examples in the retain set that were not selected to be forgotten.

- Differences in how quickly examples are unlearned make it fundamentally difficult to equally protect all examples.

Implications
- Stronger U-MIAs are critical for avoiding false senses of privacy in unlearning evaluations. The unlearning research community should adopt more rigorous threat models.

- There are inherent tensions between individual vs population privacy and differences in example memorization that pose challenges for inexact unlearning methods.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper argues that existing evaluations of machine unlearning algorithms, which rely on weak membership inference attacks, overestimate the privacy protections they provide; using a stronger per-example attack reveals greater privacy leakage and variance in vulnerability across examples for several state-of-the-art unlearning techniques.


## What is the main contribution of this paper?

 This paper makes several key contributions to the study of machine unlearning and evaluating the privacy protections it provides:

1. It shows that commonly used "population" unlearning membership inference attacks (U-MIAs) in prior work are much weaker than a per-example U-MIA that creates tailored decision rules for each sample. Using their stronger per-example attack, the authors find that existing unlearning algorithms provide much less privacy protection than previously thought.

2. The paper reveals that there is large variance in how vulnerable different examples are to the per-example U-MIA after unlearning. While some examples become less vulnerable, others become more vulnerable. This makes it fundamentally difficult for current unlearning algorithms to equally protect all examples.

3. It finds that the privacy leakage of examples in the retain set often increases due to unlearning algorithms fine-tuning on that set. This means unlearning can jeopardize privacy at a population level even if it helps some individuals.

4. The authors show that sources of variance during unlearning optimization, like mini-batch ordering, can significantly impact privacy. They also demonstrate the difficulty of constructing an unlearning algorithm that successfully defends against the per-example U-MIA.

5. The paper concludes by arguing that the unlearning community needs to adopt more formal threat models and be more careful about the choice of evaluation adversaries to avoid overestimating the privacy guarantees. It also makes recommendations for developing evaluations with theoretical privacy guarantees.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts discussed are:

- Machine unlearning
- Inexact unlearning
- Membership inference attacks (MIAs)
- Unlearning membership inference attacks (U-MIAs)
- Population U-MIAs
- Per-example U-MIAs
- LiRA (Likelihood Ratio membership inference Attack)
- U-LiRA (LiRA adapted for machine unlearning evaluation)
- Privacy leakage
- Forget set
- Retain set
- Vision classifiers 
- Language models
- Attack accuracy
- Predicted membership probability

The paper discusses evaluating inexact machine unlearning algorithms, which aim to efficiently "unlearn" examples from a trained model without having to retrain from scratch. It focuses on adapting membership inference attacks, which aim to determine whether a data sample was included in training or not, to the unlearning setting. The key findings are that commonly used "population" attacks can underestimate privacy leakage, while stronger "per-example" attacks reveal greater vulnerability. The paper also examines issues around variance during unlearning optimization and effects on the retain set. Overall, it encourages more rigorous evaluation of inexact unlearning schemes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods and findings in this paper:

1. The authors categorize existing unlearning membership inference attacks (U-MIAs) into "population" and "per-example" attacks. How do these two categories differ and why do the authors argue that per-example U-MIAs provide a stronger evaluation of unlearning algorithms?

2. The authors propose adapting an existing per-example membership inference attack called LiRA to the machine unlearning setting, which they call U-LiRA. What assumptions and approximations does LiRA make in order to be computationally tractable? How could these impact the attack's effectiveness when adapted to unlearning?

3. On CIFAR-10 vision models, U-LiRA shows substantially higher attack success rates across different unlearning algorithms compared to a baseline population U-MIA. What factors contribute to U-LiRA's improved performance? How do the complexities of the vision and language models impact relative U-MIA performance?  

4. The authors show high variance in vulnerability across different examples, with some examples actually increasing in vulnerability after unlearning. What underlying factors drive differences in how easily different examples are unlearned? How might this impact the ability to equally protect all examples?

5. The authors attempt to construct a "U-LiRA-aware" unlearning algorithm to encourage overlap of in and out distributions. Why does this naive approach fail to successfully unlearn examples? What fundamental challenges exist in defeating U-LiRA with current unlearning techniques?

6. The authors highlight that U-LiRA's Gaussian assumptions can fail in some cases, reducing attack performance. What modifications could make U-LiRA more robust to violations of normality? How should the unlearning community design attacks that avoid these failure modes?  

7. Many unlearning algorithms are shown to increase privacy leakage on retain set examples not selected for unlearning. Why does this occur and how should unlearning algorithms account for this in their design? What tensions exist between individual and population privacy?

8. The paper argues that lacking formal threat models makes evaluating progress in unlearning research difficult. What key threat model components need to be explicitly defined to enable rigorous evaluations? What can the community borrow from differential privacy and adversarial machine learning?

9. The authors categorize exact unlearning techniques and discuss their limitations in scaling to large modern models. What advancements are needed in exact unlearning schemes to make them more practical? Can insights from inexact methods help address these scalability issues?  

10. The authors measure UID accuracy rather than formally bounding information leakage. What techniques could the community employ to derive theoretical privacy guarantees for unlearning algorithms analogous to the bounds developed for standard membership inference attacks?
