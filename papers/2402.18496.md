# [Language Models Represent Beliefs of Self and Others](https://arxiv.org/abs/2402.18496)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is limited understanding of whether large language models (LLMs) truly comprehend theory of mind (ToM) or just mimic the capability through pattern recognition. 
- It is unclear if LLMs develop internal representations of others' mental states and whether they can distinguish between their own beliefs versus others' when there is conflicting information.

Proposed Solution:  
- Probe the internal activations of LLMs using linear classifiers to decode agents' belief status, assessing if belief representations exist.
- Manipulate identified belief directions to alter model behavior and evaluate impact on ToM reasoning performance. 
- Test if findings generalize to diverse social reasoning tasks involving different causal patterns.

Key Contributions:
- Discovered interpretable belief directions that linearly separate out representations of the model's own beliefs vs others' beliefs.
- Showed that manipulating these directions significantly influences ToM accuracy, underscoring their functionality.
- Demonstrated generalization across some tasks, suggesting potential universal belief representations.  

Overall, the paper makes progress in elucidating whether LLMs genuinely exhibit ToM capabilities. By decoding and intervening on internal belief states, it provides evidence that LLMs may develop representations that distinguish between mental states of self and others. Additional research is still needed to fully determine the trustworthiness, scalability and social alignment of LLM reasoning.
