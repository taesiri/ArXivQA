# [EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models   with Semi-structured Data](https://arxiv.org/abs/2312.15696)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Applying large language models (LLMs) to specific domains poses challenges such as lack of domain knowledge, limited ability to utilize relevant knowledge, and difficulty adapting to unique data formats. 
- Training LLMs from scratch for each domain is infeasible, and annotated data is scarce within domains.

Proposed Solution: 
- Focus on continual pre-training of LLMs using unlabeled general and e-commerce corpora to adapt models to the e-commerce domain.  
- Explore impact of mixing different data sources during continual pre-training, designing a strategy to effectively leverage abundant semi-structured e-commerce data.

Methods:
- Construct EcomGPT models by continual pre-training BLOOM LLMs on mixed data from general and e-commerce sources.  
- Propose data mixing strategy to establish connections between diverse data sources and enhance diversity within each training sample.
- Build benchmarks to evaluate few-shot in-context learning and zero-shot performance after instruction tuning on e-commerce tasks.

Key Results:
- Continual pre-training enhances performance on domain-specific tasks without compromising general capabilities.  
- Data mixing strategy enables more effective infusion of domain knowledge compared to separate sampling.
- Performance gains vary across tasks depending on reliance on domain knowledge and data format.

Main Contributions:
- Analysis of continual pre-training for adapting LLMs to specialized domains 
- Data mixing strategy for leveraging semi-structured data
- Comprehensive e-commerce benchmarks for evaluating few-shot and zero-shot performance
