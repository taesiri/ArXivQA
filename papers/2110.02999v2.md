# [Generative Modeling with Optimal Transport Maps](https://arxiv.org/abs/2110.02999v2)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a method for learning optimal transport (OT) maps between probability distributions, and applies it to generative modeling tasks. The key ideas are:

- They formulate the problem of learning the OT map as a min-max optimization, by leveraging the Kantorovich dual formulation of OT. This allows end-to-end training of the OT map using stochastic gradient methods.

- They extend the formulation to handle distributions in different dimensional spaces, by introducing an embedding function. This allows applying OT maps to generative modeling tasks like generating images from noise. 

- They prove error bounds relating the approximation error of the learned OT map to the duality gap of the optimization problem.

- They demonstrate the effectiveness of using the learned OT maps directly as generative models on tasks like image generation, denoising, colorization etc. The performance is comparable to or better than existing methods.

So in summary, the main hypothesis is that OT maps can be effectively learned via a min-max formulation and used directly as generative models, with comparable or improved performance to existing GANs and OT-based methods. The contributions are providing the formulation and analysis for learning OT maps, extending it to unequal dimensions, and demonstrating its effectiveness on generative modeling tasks.


## What is the main contribution of this paper?

 This paper proposes a method to learn optimal transport (OT) maps between probability distributions in high-dimensional spaces, such as images. The main contributions are:

1. They derive an end-to-end algorithm to compute OT maps for the quadratic cost (Wasserstein-2 distance) by formulating it as a min-max optimization problem. 

2. They extend the approach to handle distributions in spaces of unequal dimensions, like mapping noise to images, and provide theoretical error bounds.

3. They demonstrate the effectiveness of using the computed OT maps as generative models on tasks like image generation, denoising, colorization, and inpainting.

4. They show comparable performance to methods that use OT cost as a loss, while having the advantage that the map itself is optimal.

5. Unlike previous works that compute OT maps in latent spaces of autoencoders, their method works directly in the ambient input/output spaces.

In summary, the main contribution is a practical end-to-end method to learn optimal transport maps between distributions in high dimensions, and showing its usefulness as a generative model on computer vision tasks. Theoretical analysis and competitive results are provided compared to prior OT-based generative modeling approaches.
