# [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303)

## Summarize the paper in one sentence.

 This paper investigates how well large language models of different sizes can follow overriding instructions that conflict with knowledge in the model's weights or prompt context, finding that larger models perform better at obedience even when instructed to ignore safety guidelines, indicating a need for external moderation mechanisms.


## What is the main contribution of this paper?

 Based on my review, the main contribution of this paper appears to be:

Investigating and benchmarking large language models on the task of instruction following in conflicting situations, such as instruction overrides. The key findings include:

1) Larger models perform best at following overriding instructions that conflict with internal knowledge or contextual information. They are highly obedient, even when given harmful instructions.

2) When scaling context length via rope scaling, a buffer is needed from the perplexity cliff to maintain good instruction following capabilities.  

3) There is a fundamental tension between enhancing instruction following/override abilities, and adhering to safety guidelines. The most effective approach for safe AI may involve mechanisms external to the LLM itself, akin to human cognitive control processes.

4) A proposed framework involves using a faster parallel model to "think" ahead, then moderating/inhibiting unsafe thoughts before the main model can speak them. This allows coherent output while avoiding issues.

In summary, the main contribution appears to be analysis and proposals related to instruction following, overrides, and safety in large language models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a framework for moderating language model outputs that is inspired by neuroscience and the separation of language comprehension and production in the human brain. Can you expand more on the specifics of how this framework mimics the biological processes and architecture of language control in the brain? How is the speculative decoding process analogous to speech planning before speaking?

2. You utilize perplexity analysis and rope scaling to enable longer context lengths while maintaining instruction following capabilities. Can you elaborate on the tradeoffs between perplexity, context length, and performance on downstream tasks like instruction following? What are some ways to optimize these tradeoffs? 

3. The paper argues that alignment methods that directly modify model weights can degrade performance on reasoning tasks. However, other work has shown alignment can be achieved with minimal impacts. Can you compare and contrast your approach to alignment via external moderation versus internal alignment techniques? What are the relative pros and cons?

4. How does your proposed framework for controlled language generation compare to other methods like bag of words discriminators or control codes? What are some key advantages your method provides over these existing techniques?

5. The framework leverages a two model structure with a main LLM and faster speculative decoder LLM. What considerations went into choosing the size and architectures for each of these models? How sensitive is performance to these model choices?

6. You propose inhibiting token probabilities when certain unwanted tokens are detected from the speculative decoder. How did you choose the inhibition factor lambda? What impacts does the level of inhibition have on moderation capability versus fluency of output?

7. What challenges did you face in implementing and evaluating this framework? How might the framework change or need to be adapted for real-world deployment to chatbots and other applications?

8. Could this controlled generation framework also allow for controlled generation of images, videos or other modalities? If so, how might the technique change for non-language tasks? If not, what are the limitations?

9. The paper focuses on instruction following tasks to evaluate the capabilities of different sized LLMs. What other tests could complement the analysis to provide a more comprehensive comparison of model performance? 

10. One conclusion is that safe and trustworthy AI may require mechanisms external to the LLM itself. Do you think there are any approaches internal to model training that could also contribute to safety? What combination of techniques could create the safest outcomes?
