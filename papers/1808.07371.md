# [Everybody Dance Now](https://arxiv.org/abs/1808.07371)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to transfer the dance/motion from a source video of one person to a target video of a different person. Specifically, the authors propose a method to take a source video of someone dancing, and transfer that dance motion to a target person after only a few minutes of the target person performing some standard moves.

The key idea is to use pose as an intermediate representation between the source and target videos. The authors extract pose stick figures from the source video frames using off-the-shelf pose detectors. They then learn a mapping between these pose stick figures and frames of the target person using an image-to-image translation model. This allows them to transfer motion from source to target by inputting the source poses into the trained model to generate images of the target person in those same poses.

So in summary, the central hypothesis is that pose can serve as an effective intermediate representation to enable compelling "do as I do" motion transfer between two video subjects. The paper then presents a method and results to validate this.


## What is the main contribution of this paper?

 The main contribution of this paper is a simple but effective method for "do as I do" motion transfer in videos. The key ideas are:

- Using pose stick figures obtained from off-the-shelf pose detectors as an intermediate representation to transfer motion between a source and target video. 

- Learning a mapping between pose stick figures and images of the target person through an image-to-image translation model based on adversarial training.

- Enhancing the basic image-to-image translation model with temporal smoothing and a separate facial generator to create more realistic videos.

- Releasing a dataset of long single-dancer videos for training and short YouTube videos for transfer.

- Addressing the issue of detecting fake synthesized videos by training a neural network classifier.

The method is able to convincingly transfer complex dance motions between very different subjects with only a few minutes of target training data. The simple approach produces surprisingly compelling and detailed video results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a simple method for transferring the dance motions of a source person in a video to a target person after filming the target performing some standard moves, using pose stick figures as an intermediate representation and training an image-to-image translation model.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of video-to-video translation for motion transfer:

- It proposes a simple but effective approach using 2D pose as an intermediate representation between source and target videos. Many prior works rely on 3D models, multi-view captures, or other more complex representations. Using only 2D pose makes this method more accessible.

- It demonstrates motion transfer on complex, in-the-wild video sources rather than more constrained settings. The ability to handle real-world videos makes the approach more practical. 

- It introduces both temporal smoothing and face enhancement to generate higher quality video results. These components improve upon single frame image synthesis methods and lead to more realistic and temporally coherent videos.

- The method achieves compelling results using only a few minutes of the target subject performing standard moves. Other data-driven approaches often require more paired training data.

- The authors explicitly address fake content detection by training a model to identify synthesized videos, unlike most other video generation papers. This is an important consideration for responsible research on synthesis.

- Large scale user studies quantify performance improvements over baselines like nearest neighbors and single image synthesis. Rigorous comparison and evaluations lend credibility to the method.

- The authors release new open source data of long videos suitable for training and evaluation. Public datasets enable further research and benchmarking.

Overall, this work pushes the state of the art in video-based motion transfer with a simple but effective approach, rigorous evaluation, and considerations of potential misuse through fake detection. The practicality and strong results help move the field forward.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the Limitations and Discussion section:

1. Improving results by combining target videos with different clothing/lighting, improving pose detection, and mitigating artifacts in loose clothing/hair.

2. Understanding what poses and how much training data is needed to learn an effective model. This relates to work on determining the most influential training examples.

3. Addressing the limitations of their pose normalization solution, which does not account for different limb lengths or camera positions between subjects.

4. Handling cases where the source motion contains extreme poses not seen in the target's training data. Artifacts can occur when extrapolating too far from the training distribution.

5. Exploring potential applications like synchronized multi-subject dancing or using the system commercially for augmented reality performances. 

6. Further work on detecting fake videos, ensuring the detector generalizes across subjects and motions.

In summary, they suggest future work on improving the quality of results, understanding model training and generalization, enhancing the pose normalization, exploring applications, and developing better fake detection systems. The key areas seem to be refining the synthesis quality, expanding the flexibility of the motion retargeting, and researching the societal impacts of fake content generation.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a method for transferring the dance motions of a person in a source video to a target person in a new video. The key idea is to use the detected 2D poses from the source video as an intermediate representation to guide the synthesis of the target person. Specifically, the pose stick figures from the source video are first normalized to match the body proportions and framing of the target person. Then a video-to-video translation model is trained on target videos to translate pose stick figures to realistic images of the target person. To enable realistic video synthesis, the model is trained with a temporal smoothing loss and incorporates a separate network to synthesize higher-quality facial details. The method is able to produce compelling video results of amateurs dancing with the motions of skilled dancers or pop stars. The quality of the synthesized videos also motivates developing a system to detect fake generated videos.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a method for transferring the dance motions from a source video of one person onto a target video of a different person. The key idea is to use detected 2D poses from the source video as an intermediate representation between the source and target. To transfer the motion, the authors first extract pose stick figures from each frame of the source video using an off-the-shelf pose detector. They then learn a mapping between these pose stick figures and frames of the target subject using an image-to-image translation model based on a conditional GAN. The model is trained on pairs of target video frames and corresponding pose stick figures. At test time, they input the pose stick figures from the source into the trained model to generate frames of the target subject performing the same motions. They also introduce techniques to make the video generation temporally coherent, including generating two consecutive output frames and adding a specialized facial GAN. The method produces surprisingly compelling videos of motion transfer between different subjects. The authors also build a "fake detector" to distinguish real from synthesized videos, motivated by the high quality of the results.

In summary, this paper presents a simple but effective approach for human motion transfer in video based on using 2D pose as an intermediate representation. The method trains an image translation model between pose stick figures and frames of a target subject. This allows transferring the motions from an arbitrary source video onto the target by passing the source poses through the trained model. The quality of the generated videos is high enough to merit building a detector to identify such synthesized fakes.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents a method for transferring the dance motions from a source video of one person to a target video of a different person. The key idea is to use human pose as an intermediate representation between the source and target. The method first detects 2D poses from the source video using an off-the-shelf pose detector. It then normalizes these poses so they are adapted to the target person's body proportions and position in the frame. The normalized poses are converted into pose stick figures. A generative adversarial network (GAN) is trained to translate pose stick figures of the target person from training data into realistic images of that person. This GAN contains a generator and discriminator network and is trained with adversarial and perceptual losses. To create coherent videos, the model predicts two consecutive frames - taking the previous frame as input to the next. It also uses a separate GAN to add realistic details to the face region. Finally, to transfer motion from source to target, it feeds the source poses into the trained GAN to output frames of the target person in the same poses.


## What problem or question is the paper addressing?

 The paper is addressing the problem of transferring the motions of a dancer in a source video to a target person in a new video, such that the target person appears to be dancing with the same motions as the source. The key question is how to synthesize realistic video of the target person performing complex dance motions captured in the source video, even though the target person was never recorded performing those exact moves.

Some key points:

- The paper proposes a "do as I do" motion transfer approach to automatically transfer dance motions from a source to a target person.

- They use an intermediate pose representation obtained from pose detection to establish correspondences between source and target subjects. 

- A image-to-image translation model is learned to generate video frames of the target person conditioned on the pose stick figures from the source.

- Temporal smoothing and a face GAN are used to improve video quality and coherence.

- The method allows transferring complex motions from in-the-wild source videos to target subjects recorded performing simple motions, enabling compelling synthesis of the target dancing.

- Applications like synchronized multi-person dancing are enabled by applying the same source to different target models.

- The high quality results motivate an investigation into fake detection to distinguish real from synthesized video.

So in summary, the key focus is on transferring intricate dance motions between subjects in videos using pose as an intermediate representation, and generating high quality, temporally coherent results.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Motion transfer - The main focus of the paper is transferring motion from a source video/subject to a target video/subject.

- Pose estimation - Estimating the poses of the subjects in the source and target videos is a key intermediate step in the motion transfer process. The paper uses OpenPose for pose estimation.

- Image-to-image translation - The problem is framed as translating from the estimated pose stick figures to images/video of the target subject. Pix2pix and related GAN methods are used.

- Temporal modeling - The paper models temporal coherence across frames when synthesizing the output video.

- Facial modeling - A separate model is used to generate more realistic faces.

- Perceptual studies - Human perceptual studies on Amazon Mechanical Turk are used to evaluate the results.

- Fake detection - A method is introduced to detect fake/synthesized videos.

- Applications - Potential applications like synchronized dancing videos are discussed.

- Dataset - A dataset of videos is collected and will be released to enable further research.

So in summary, the key terms cover pose estimation, image-to-image translation, temporal/facial modeling, human studies, fake detection, and datasets for video-based motion transfer.
