# [Continual Learning in Neural Networks](https://arxiv.org/abs/1910.02718)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop methods for continual learning that can learn online from non-stationary data streams without catastrophic forgetting of previous knowledge or tasks. Specifically, the paper proposes and evaluates a regularization-based approach called "Memory Aware Synapses" (MAS) that computes importance weights for neural network parameters online and penalizes changes to important parameters when learning new tasks or data. The key ideas are:- To estimate importance weights for neural network parameters based on the sensitivity of the learned function output to changes in those parameters, allowing computation on unlabeled data- To accumulate importance weights online using a cumulative moving average as new data is encountered - To use the accumulated importance weights as a regularizer when optimizing the loss on new data, penalizing changes to parameters deemed important for previous data/tasksThe paper evaluates MAS on supervised image classification tasks learned sequentially, showing it can reduce catastrophic forgetting compared to other continual learning methods. A key benefit highlighted is the ability of MAS to estimate importance weights adaptively from unlabeled test data, allowing the method to specialize to specific test conditions.In summary, the main hypothesis is that computing and accumulating importance weights for network parameters online, and using these to regularize learning on new data, can enable effective continual learning without catastrophic interference as the data distribution shifts over time. The experiments aim to validate whether the proposed MAS approach can achieve this goal.


## What is the main contribution of this paper?

The main contribution of the paper "Importance Weight Regularization for Online Continual Learning" is developing a method for online continual learning that can learn continuously from streaming data without separate tasks or explicit task boundaries. Specifically, the key contributions are:1. Extending the Memory Aware Synapses (MAS) importance weight regularization method to work in an online continual learning setting, where data comes sequentially and the distribution may be gradually shifting. 2. Developing protocols to determine when to update the importance weights, which data to use for the update, and how to accumulate importance weights across updates. This allows MAS to work online without explicitly defined tasks.3. Introducing a small "hard sample" buffer that stores difficult examples to help stabilize online training and provide better samples for importance weight estimation.4. Evaluating the online continual learning approach on face recognition from TV shows and robot collision avoidance. Results show the method helps accumulate knowledge over time and adapt to distribution shifts, outperforming online baselines.5. Demonstrating that importance weight regularization is useful not just for avoiding catastrophic forgetting across tasks, but also for stabilizing online learning within a single environment.In summary, the key novelty is developing an online variant of MAS that can handle streaming non-i.i.d. data without separate tasks, and showing its benefits for online continual learning in two applications. The method provides a way to apply importance weight regularization in an online setting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a new method for lifelong learning in neural networks that computes importance weights for parameters based on the sensitivity of the output function to changes in the parameters, allowing the model to determine what previous knowledge can be forgotten when learning new tasks.


## How does this paper compare to other research in the same field?

This paper presents an approach for continual learning of neural networks without task boundaries or prior assumptions on the data stream. It compares favorably to other research in the field in the following ways:- Most prior continual learning methods assume a task incremental setting, where tasks are learned sequentially with clear task boundaries and i.i.d. data within each task. This paper tackles the more realistic and challenging scenario of online continual learning without task boundaries.- The paper formulates replay buffer sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. This is a novel way to select representative samples for the replay buffer in an online fashion. - The proposed methods for sample selection, both the IQP-based approach and the greedy approximation, do not require task boundary information. They are shown to be robust to imbalanced data streams, outperforming reservoir sampling.- Experiments on multiple benchmarks (disjoint MNIST, permuted MNIST, disjoint CIFAR-10) demonstrate that the proposed techniques work well even without knowledge of task boundaries and compare favorably to state-of-the-art methods that do assume task boundaries.- The approach does not store task-specific information or require a task oracle, making it more generally applicable to online continual learning settings.Overall, this work pushes the boundaries of continual learning closer to real-world scenarios without strict assumptions. The formulation of replay sample selection as constraint reduction and the proposed selection techniques are novel contributions applicable to online continual learning. The empirical results validate the effectiveness of the approach, showing it can learn continually without prior knowledge of tasks.


## What future research directions do the authors suggest?

The paper discusses several potential future research directions for continual lifelong learning:- Developing methods that can learn effectively from limited data. Most current continual learning methods require large amounts of data per task, but humans can learn new concepts from few examples. Developing algorithms that can learn continually from limited data is an important direction.- Developing methods that can learn inter-task relationships and transfer knowledge between tasks. Current methods tend to learn tasks independently, but being able to identify and leverage relationships between tasks (e.g. seeing the similarities between learning two different languages) could improve learning efficiency.- Developing systems that integrate memory, processing, and representation learning within the same framework. Biological learning systems tightly couple memory, information processing, and representation learning, but most AI systems treat these components separately. Integrated systems could work more effectively.- Moving beyond incremental task learning scenarios to more open-ended, unstructured lifelong learning settings. Most current continual learning research uses specific incremental task benchmarks, but less structured continual learning scenarios are needed to move towards general AI.- Developing continual reinforcement learning methods. Most existing continual learning research focuses on supervised learning, but continual reinforcement learning remains a major challenge. Developing methods that can learn continually in an online, goal-driven way is important.- Studying social interaction for continual learning. Human learning is deeply social, but current continual learning methods do not incorporate social learning. Studying how networked agents that learn collaboratively and teach each other could benefit continual learning.- Developing theory and metrics for continual learning. Current empirical approaches lack formal measures of learning efficiency and theories of optimal continual learning. Developing theory can guide progress.So in summary, key future directions include handling limited data, transfer learning, integrated systems, less structured benchmarks, reinforcement learning settings, social interaction, and formal theory.
