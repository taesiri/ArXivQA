# [Continual Learning in Neural Networks](https://arxiv.org/abs/1910.02718)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop methods for continual learning that can learn online from non-stationary data streams without catastrophic forgetting of previous knowledge or tasks. Specifically, the paper proposes and evaluates a regularization-based approach called "Memory Aware Synapses" (MAS) that computes importance weights for neural network parameters online and penalizes changes to important parameters when learning new tasks or data. The key ideas are:- To estimate importance weights for neural network parameters based on the sensitivity of the learned function output to changes in those parameters, allowing computation on unlabeled data- To accumulate importance weights online using a cumulative moving average as new data is encountered - To use the accumulated importance weights as a regularizer when optimizing the loss on new data, penalizing changes to parameters deemed important for previous data/tasksThe paper evaluates MAS on supervised image classification tasks learned sequentially, showing it can reduce catastrophic forgetting compared to other continual learning methods. A key benefit highlighted is the ability of MAS to estimate importance weights adaptively from unlabeled test data, allowing the method to specialize to specific test conditions.In summary, the main hypothesis is that computing and accumulating importance weights for network parameters online, and using these to regularize learning on new data, can enable effective continual learning without catastrophic interference as the data distribution shifts over time. The experiments aim to validate whether the proposed MAS approach can achieve this goal.
