# [Aligning and Prompting Everything All at Once for Universal Visual   Perception](https://arxiv.org/abs/2312.02153)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper presents APE (Aligning and Prompting Everything), a universal visual perception model capable of performing detection, segmentation, and grounding tasks by formulating them as instance-level sentence-object matching. A key innovation is equalizing the granularity of foreground and background concepts by decomposing category-level segmentation into proxy object-level learning. This allows APE to handle both things and stuff without manually distinguishing them. To enable efficient querying at scale, visual grounding is reformulated as open-vocabulary detection with gated region-sentence fusion. APE achieves state-of-the-art or competitive performance on over 160 datasets with only one set of weights, showing its practicality as a general vision model. Ablations demonstrate the effectiveness of individual components like gated fusion, equalized granularity learning, and joint vision-language alignment. The model efficiency, task coverage, and strong empirical results confirm that effective universal visual perception with aligning and prompting at scale is feasible.
