# [Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in   Indic Languages](https://arxiv.org/abs/2401.03677)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Online gender-based violence is a growing issue that can compound vulnerabilities and limit opportunities for victims. Automated approaches are needed to detect gendered abuse in Indic languages to help mitigate this problem. 

- There is a lack of Indic language datasets to enable automated detection of gendered abuse for Indian language content.

Proposed Solution:
- The authors organized a shared task at ICON 2023 focused on detecting gendered abuse in Hindi, Tamil, and Indian English based on a new dataset. 

- The shared task had 3 subtasks: 1) detect gendered abuse using only provided data 2) use transfer learning to improve detection 3) jointly predict gendered abuse and explicit language.

- A dataset with over 20,000 Twitter posts labeled for gendered abuse and explicit language was created and released.

- 9 teams registered and 2 submitted systems that were evaluated. The best system used an ensemble of CNN and BiLSTM models and achieved 0.616 F1 for Subtask 1.

Key Contributions:
- Creation and release of a novel dataset for detecting gendered abuse in Indic languages
- Organization of a shared task to spur research on this problem using the new dataset
- Analysis of systems submitted to the task, with the best system achieving 0.616 F1 score
- Advancement of capabilities for automated detection of gendered abuse to help mitigate this societal issue

The paper makes key contributions around the dataset, shared task organization, and analysis of initial detection systems to advance research on the important problem of detecting gendered abuse in Indic languages.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper reports on the findings of the ICON 2023 shared task on detecting gendered abuse in online text in Hindi, Tamil, and Indian English, which received 9 registrations and had the best system obtain F1 scores of 0.616 on detecting gendered abuse and 0.582 on detecting explicit language.


## What is the main contribution of this paper?

 Based on reviewing the paper, the main contribution appears to be:

Organizing and hosting a shared task on gendered abuse detection in Indic languages (Hindi, Tamil, and Indian English) as part of the ICON 2023 conference. The shared task involved developing classifiers to detect gendered abuse using a new dataset created for this purpose. Specifically, there were 3 subtasks:

1) Build a classifier to detect gendered abuse directed at marginalized genders using only the provided dataset. 

2) Use transfer learning from other datasets to build a classifier for subtask 1.

3) Build a multi-task classifier to jointly predict gendered abuse and explicit language.

The paper summarizes the task setup, dataset statistics, participating teams and results. It helped advance research in abuse detection for Indic languages through the new dataset and systems developed. Overall, it contributed towards efforts to mitigate online gender-based violence.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it include:

- Gendered abuse detection
- Online harassment
- Hate speech
- Toxic language
- Indic languages (Hindi, Tamil, Indian English)
- Shared task
- Dataset
- Annotation
- Classification
- Convolutional Neural Networks (CNN)
- Long Short-Term Memory (LSTM)
- Transfer learning
- Multilingual models

The paper presents an overview and results of a shared task on detecting gendered abuse in online text, specifically focusing on Indic languages. It describes the dataset creation and annotation process, task setup and schedule, participating systems and teams, and results using machine learning approaches like CNNs and LSTMs. Key terms reflect the topics of gender-based violence, abuse detection, multilingual NLP, and the methodologies used.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions using an ensemble CNN and BiLSTM architecture. Can you explain in more detail how these two models were combined into an ensemble? What were the motivations and expected benefits of using an ensemble approach?

2. The paper states that transfer learning was utilized in Subtask 2 from existing datasets. Can you elaborate on the specifics of how transfer learning was implemented? What datasets were used and how were they incorporated into the models? 

3. For the word embeddings used as inputs, the paper mentions using both GloVe and FastText. What were the rationales for choosing these two types of embeddings? Were there any experiments done in comparing their performance?

4. The paper cites using a sequence length of 100 words for the input layers. How was this sequence length chosen? Were any experiments done with different sequence lengths to optimize this hyperparameter?

5. Can you explain why the Adam optimizer and categorical cross-entropy loss were chosen for training the models? Were any other optimization algorithms or loss functions experimented with?

6. The abstract mentions that the best F1 scores achieved were around 0.6. What ideas do you have for improving on these results in future work? Where do you see the biggest opportunities for enhancement?  

7. For the transfer learning task, were there any domain adaptation techniques used to better match the distribution of the external datasets to the target task dataset? If not, do you think that could have improved performance?

8. How much effort was devoted to tackling class imbalance in the training data? Do you think more advanced sampling or weighting schemes could have helped?

9. Can you discuss any experimentation done around different neural network architectures? Would Transformer-based models like BERT potentially improve results?

10. The paper mentions a participatory and inclusive annotation process. Do you think this could have introduced any biases that made the task more challenging compared to a typical annotation process? How might those be addressed?
