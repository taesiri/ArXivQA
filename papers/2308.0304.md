# [Skills-in-Context Prompting: Unlocking Compositionality in Large   Language Models](https://arxiv.org/abs/2308.0304)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it aims to address is: How can we design an effective prompting strategy to unlock compositional generalization capabilities in large language models?Specifically, the key research questions appear to be:- How can we teach large language models to compose basic skills in innovative ways to solve more complex reasoning problems beyond the examples they have seen before (i.e. compositional generalization or "easy-to-hard" generalization)?- How can we develop a prompting approach that allows LLMs to explicitly ground each reasoning step on more elementary skills? - Can an effective prompting strategy initiate strong synergies between basic skills and composition capabilities in LLMs, enabling them to generalize systematically to solve harder, unseen problems?- Can prompting allow LLMs to leverage both skills provided explicitly in the prompts as well as their internal pre-trained skills and knowledge?The central hypothesis seems to be that by developing a prompting strategy called "Skills-in-Context" (SKiC) that provides basic skills, examples of skill composition, and the problem to be solved all within the same context, the LLM can learn to effectively ground its reasoning steps on skills it has already mastered. This should unlock latent compositional generalization capabilities, allowing the LLM to solve more complex problems by flexibly composing skills.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a novel prompting strategy called "Skills-in-Context" (SKiC) prompting to unlock compositional generalization abilities in large language models (LLMs). Specifically, the key ideas and contributions are:- SKiC prompting teaches LLMs to explicitly ground each reasoning step on more basic skills when solving complex problems. It does this by providing two main components in the prompt: (1) the skills needed, with descriptions and examples, and (2) examples showing how to compose the skills to solve more complex problems.- SKiC prompting is a simple one-stage approach, unlike prior methods requiring multiple stages or model calls. It can easily replace other one-stage prompting strategies.- Experiments across a diverse set of tasks (symbolic manipulation, arithmetic, QA, etc) show SKiC prompting achieves significantly improved generalization, especially on harder out-of-distribution problems. It even achieves near perfect accuracy on some tasks.- SKiC prompting allows LLMs to go beyond just using the skills provided in the prompt, and leverage their internal skills learned during pretraining. This enables solving problems requiring innovative skill compositions.- The method achieves new state-of-the-art results on challenging benchmarks requiring compositional reasoning, such as GSM8K and MATH dataset for mathematical reasoning.In summary, the key contribution is proposing a simple yet effective prompting strategy to unlock and improve the compositional generalization capabilities of large language models, by teaching them to ground their reasoning on basic skills and compose them to solve more complex problems.
