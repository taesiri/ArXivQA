# [Neural Intrinsic Embedding for Non-rigid Point Cloud Matching](https://arxiv.org/abs/2303.01038)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to establish correspondences between non-rigidly aligned point clouds. The key hypotheses are:

1. Learning an embedding that respects the intrinsic geometry of the underlying surface can help establish reliable correspondences between non-rigid shapes. 

2. Training the embedding network using relative geodesic errors and statistical distribution matching can improve local intrinsic geometry preservation.

3. Replacing handcrafted spectral embeddings with a learned embedding in a deep functional maps framework can lead to a simple yet effective pipeline for weakly supervised non-rigid point cloud matching.

4. The proposed pipeline is robust to common artifacts like noise and varying sampling density, and generalizes well even when trained on a small dataset.

In summary, the main hypothesis is that a learned neural intrinsic embedding can enable effective non-rigid point cloud matching in a weakly supervised setting, while being robust and generalizable. The paper presents a method and experiments to validate these hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a learning-based framework called Neural Intrinsic Embedding (NIE) and Neural Intrinsic Mapping (NIM) for non-rigid point cloud matching. 

The key ideas are:

1. NIE embeds each point in a point cloud into a high-dimensional space that respects the intrinsic geometry of the underlying surface. It is trained to make the Euclidean distance between embeddings approximate the geodesic distance on the surface. 

2. NIM is a weakly supervised framework for non-rigid point cloud matching, built upon NIE. It learns to extract optimal features from input point clouds and estimate correspondences without ground truth correspondence labels.

3. The entire pipeline only requires point clouds that are rigidly aligned, along with geodesic distances for the training point clouds. It does not need triangulation or other structural information.

4. Experiments show the method achieves good matching performance compared to state-of-the-art baselines, generalizes well, and is robust to noise and partiality. 

In summary, the key contribution is a simple yet effective learning-based approach to leverage intrinsic geometry for non-rigid point cloud matching in a weakly supervised manner. The method is insensitive to point sampling density and robust to common artifacts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes Neural Intrinsic Embedding (NIE) to learn intrinsic geometry-aware embeddings for point clouds, and Neural Intrinsic Mapping (NIM) for weakly supervised non-rigid point cloud matching, which achieves comparable or better performance than state-of-the-art methods requiring more supervision or structural input.


## How does this paper compare to other research in the same field?

 Here are a few key thoughts on how this paper compares to other research on non-rigid point cloud matching:

- The main contribution is proposing a learning-based framework called Neural Intrinsic Embedding (NIE) to embed point clouds into a high-dimensional space that respects the intrinsic geometry. This is different from common spectral embedding methods like functional maps that rely on eigen-decomposition of Laplacian operators. 

- NIE aims to be more robust to noise and artifacts in point clouds compared to spectral methods. The authors design losses and network modifications to handle issues like sampling density bias.

- Based on NIE, the paper presents a weakly supervised matching network called Neural Intrinsic Mapping (NIM). This is related to Deep Functional Maps frameworks but replaces spectral embedding with the learned NIE embeddings.

- A key difference is NIM is trained without ground truth correspondence labels, using only a self-supervised loss based on geodesic distances. So it requires less supervision than similar learning-based matching networks.

- Experiments show NIE and NIM perform on par or better than state-of-the-art intrinsic and extrinsic matching methods, especially in cases of noise, partiality, and generalization across datasets.

- Limitations are the sensitivity to pose differences and reliance on nearly rigid alignment for training. Extending to be invariant to SO(3) transformations could be interesting future work.

Overall, the main novelties seem to be the proposed NIE embedding and weakly supervised NIM network compared to both classic spectral methods and recent learning-based approaches. The results demonstrate competitive matching performance with greater robustness and less supervision required.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Improving robustness to different poses/alignments: The authors note their method is currently sensitive to inaccurate rigid alignment of the point clouds due to unusual poses. They suggest incorporating recent advances in SO(3)-invariant networks could help enhance the pipeline in this regard.

- Generalizing to broader shape classes: The current method focuses on human shapes. Expanding the approach to more diverse shape classes like animals, objects, etc could be an interesting direction.

- Weakly supervised learning: The authors highlight the weakly supervised nature of their framework as a benefit. Further exploring ways to reduce the supervision, like approximating geodesics without any ground truth data, could be worthwhile. 

- Incorporating topological information: The current method relies primarily on geometric information like geodesics. Incorporating topological cues about the underlying surface could potentially improve the learned embeddings.

- Applications to point cloud synthesis: The learned embeddings could potentially be useful for synthesizing novel point clouds by interpolating in the embedded space. This could be an interesting application direction to explore.

- Combining extrinsic and intrinsic models: The paper focuses on intrinsic models leveraging geometric information. Combining these with extrinsic deformation models could be a promising direction for future hybrid methods.

In summary, the key opportunities highlighted are improving robustness, generalization, reducing supervision, incorporating more shape information like topology, exploring novel applications like point cloud synthesis, and combining intrinsic and extrinsic modeling approaches.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a neural intrinsic embedding (NIE) and a neural intrinsic mapping (NIM) framework for non-rigid point cloud matching. The NIE embeds each point in a point cloud into a high-dimensional space that respects the intrinsic geometric structure of the underlying surface. It is trained to approximate geodesic distances on the surface using Euclidean distances between the embeddings. The NIM is then a weakly-supervised mapping network built on top of the NIE embeddings. It is trained with a self-supervised loss to predict correspondences between non-rigidly deformed point clouds. 

The key benefits of this framework are that it only requires point clouds as input, with no need for surface meshes or other structural information. It is also robust to noise, partial data, and varying sampling density. Experiments demonstrate that the learned embeddings effectively encode intrinsic geometry, and the mapping network performs on par with or better than state-of-the-art methods that use more supervision or structural input. The framework generalizes well across datasets and is robust to artifacts like noise and partiality. Limitations include sensitivity to pose misalignment between point clouds. Overall, this is a novel learning-based pipeline for intrinsic point cloud analysis that could enable non-rigid shape matching in real-world scenarios.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a neural network framework called Neural Intrinsic Embedding (NIE) to learn an embedding of point clouds that respects the intrinsic geometry of the underlying surface. NIE is trained using only point clouds and their corresponding geodesic distance matrices, without any need for ground truth correspondences. The key idea is to train the network such that the Euclidean distance between embedded points approximates the geodesic distance between the corresponding surface points. To achieve this, they use a modified Dynamic Graph CNN architecture and train it with a combined loss function that includes relative geodesic error, KL divergence between geodesic and embedded distance distributions, and a bijectivity loss. Based on the learned NIE, they further propose a Neural Intrinsic Mapping (NIM) network for weakly supervised non-rigid point cloud matching. NIM replaces the standard spectral embedding in Deep Functional Maps with the learned NIE and uses a self-supervised cyclic loss to train the feature extractor. Experiments show that their method performs well on near-isometric matching and generalizes across datasets without needing ground truth supervision.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes Neural Intrinsic Embedding (NIE) and Neural Intrinsic Mapping (NIM) for non-rigid point cloud matching. NIE embeds each point in a high-dimensional space that respects the intrinsic geometry of the underlying surface. It is trained so that Euclidean distances between embedded points approximate geodesic distances on the surface. NIM is a weakly supervised framework for non-rigid point cloud matching based on NIE. It replaces the spectral embeddings used in previous deep functional maps methods with the learned NIE embeddings. A self-supervised loss based on approximated geodesic distances guides the learning of optimal features for computing functional maps between shapes. The pipeline only requires rigid alignment of training point clouds and their geodesic distances. Experiments show the method achieves good matching performance compared to baselines, generalizes well, and is robust to noise and partiality. Key benefits are not needing ground truth correspondences for training and working directly on point clouds without mesh information. Limitations include sensitivity to pose misalignment between shapes.


## What problem or question is the paper addressing?

 The paper proposes a learning-based framework for non-rigid point cloud matching. The main problems and questions it addresses are:

1. Establishing correspondences between non-rigidly aligned point clouds is challenging due to the complexity of modeling non-rigid deformations. The paper aims to address this problem through a learning-based intrinsic approach. 

2. Traditional spectral embedding methods like functional maps rely on eigen-decomposition of Laplacian operators defined on meshes or point clouds. This process is inefficient, non-differentiable, and sensitive to artifacts like noise and partiality. The paper proposes a neural network to learn an intrinsic embedding directly from point clouds in a robust and efficient manner.

3. Most learning-based methods require ground truth point correspondences for supervision, which can be difficult to obtain. The paper presents a weakly supervised framework that only requires access to geodesic distances for the training shapes.

4. Generalization is a key challenge in learning non-rigid matching models. The paper introduces techniques like relative geodesic loss and modified neighborhood aggregation to reduce bias from point sampling density and improve generalization. 

5. The paper aims to demonstrate that the proposed Neural Intrinsic Embedding and Mapping networks achieve robust matching performance on par with or exceeding state-of-the-art methods, despite requiring less supervision and no mesh information.

In summary, the key focus is developing a learning framework for establishing reliable point correspondences between non-rigid shapes in a way that is efficient, robust, generalizable, and requires minimal supervision. The core ideas are learning an intrinsic embedding space and a weakly supervised matching network.


## What are the keywords or key terms associated with this paper?

 Here are some of the key terms and keywords I identified in this paper:

- Point clouds - The paper focuses on processing and matching point clouds, which are primitive 3D data representations.

- Non-rigid registration - The paper aims to establish correspondences between non-rigidly aligned point clouds, i.e. point clouds that have undergone complex deformations. This is referred to as non-rigid registration or non-rigid matching. 

- Intrinsic vs extrinsic methods - The paper discusses two approaches to non-rigid point cloud matching: intrinsic methods that rely on geometric properties like geodesics, and extrinsic methods that use local rigid/affine transformations.

- Spectral embeddings - Intrinsic methods like functional maps utilize spectral embeddings based on the Laplacian operator defined on shapes.

- Deep functional maps - Recent intrinsic methods leverage deep learning to extract features and compute functional maps between shapes in a data-driven manner.

- Geodesics - The paper proposes using geodesics, which capture intrinsic geometry, to supervise the training of neural embeddings.

- Weakly supervised learning - The paper presents a weakly supervised framework that only requires point clouds aligned to training data and their geodesic distances, without correspondence labels.

- Robustness - Experiments show the method is robust to noise, varying sampling density, and partiality in the point clouds.

In summary, the key focus is on learning neural embeddings and mappings for non-rigid point cloud matching in a weakly supervised, intrinsic geometry-aware manner.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a Neural Intrinsic Embedding (NIE) to learn point cloud embeddings that respect the intrinsic geometry. How exactly does the proposed method embed points such that the Euclidean distance between embeddings approximates the geodesic distance on the underlying surface? What loss functions and network modifications are used?

2. The paper mentions that NIE is designed to be aware of intrinsic geometry, computationally efficient, and robust to common point cloud artifacts. How does the proposed method achieve these goals compared to prior intrinsic embedding techniques like spectral embeddings?

3. The Neural Intrinsic Mapping (NIM) network is presented as a weakly supervised learning framework for non-rigid point cloud matching based on NIE. How does NIM differ from prior Deep Functional Map networks in terms of the embeddings and loss functions used? 

4. The paper claims NIM only requires point clouds to be rigidly aligned during training, with access to geodesic distances. Why is this considered a weakly supervised setting? What other supervision has been required by prior approaches?

5. How does the modified DGCNN architecture used in NIE alleviate sampling density bias? Why is this issue important to address for learning embeddings on non-rigid point clouds?

6. The paper demonstrates NIE and NIM achieve state-of-the-art or competitive performance for non-rigid point cloud matching. What are some key ablations and comparisons that validate the proposed method?

7. What limitations of the proposed framework are discussed? How robust is the method to changes in extrinsic pose or alignment of the point clouds?

8. How does the proposed pipeline compare qualitatively and quantitatively to prior intrinsic embedding techniques like LIE on the tasks of geodesic approximation and generalized matching?

9. What future work is suggested to address the limitations of the current method? How could equivariance to rotations be incorporated?

10. How does the proposed unsupervised loss based on geodesic distortion compare to losses relying on correspondence supervision? What are the tradeoffs?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a learning-based framework called Neural Intrinsic Embedding (NIE) to embed unstructured point clouds into a high-dimensional space that respects the intrinsic geometry of the underlying surfaces. NIE is trained so that Euclidean distances between embedded points approximate geodesic distances on the surface. Building on NIE, the authors present Neural Intrinsic Mapping (NIM), a weakly supervised network for non-rigid point cloud registration. NIM replaces eigenfunction bases in traditional functional maps with the learned NIE embeddings, and uses a self-supervised loss to learn optimal features. Experiments demonstrate that without ground truth correspondences, NIM achieves comparable or better performance than state-of-the-art methods that use more supervision. Key benefits are that NIM requires only approximately rigidly aligned point clouds and geodesic distances for training, generalizes well, and is robust to noise, partiality, and disconnectedness. The main limitation is sensitivity to extrinsic pose differences. Overall, this is a novel learning framework for point cloud matching that exploits intrinsic geometry while requiring minimal supervision.


## Summarize the paper in one sentence.

 This paper proposes a learning-based framework Neural Intrinsic Embedding (NIE) to embed unstructured point clouds into a high-dimensional space that respects the intrinsic geometry, and a weakly supervised matching network Neural Intrinsic Mapping (NIM) that achieves on par or better performance than state-of-the-art methods requiring more supervision or geometric input on near-isometric point cloud matching.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points in this paper:

This paper proposes Neural Intrinsic Embedding (NIE), a learning framework to embed unstructured point clouds into a high-dimensional space that respects the intrinsic geometry of the underlying surface, as approximated by geodesic distances. NIE is based on a modified DGCNN architecture and trained with losses that enforce relative geodesic distance preservation, statistical closeness of local distance distributions, and bijectivity. Building on NIE, the authors propose Neural Intrinsic Mapping (NIM), a weakly supervised framework for non-rigid point cloud matching. NIM replaces eigenfunction bases with learned NIE embeddings and optimizes a feature extractor using a self-supervised cyclic distortion loss, requiring only approximate rigid alignment and geodesic distances for training sets. Experiments show NIM achieves state-of-the-art performance on near-isometric matching tasks while generalizing well across datasets. The methods are also robust to noise, missing data, and pose variation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a Neural Intrinsic Embedding (NIE) to embed point clouds into a high-dimensional space that respects the intrinsic geometry. How exactly does NIE learn to embed points such that the Euclidean distance between embeddings approximates the geodesic distance on the underlying surface? What loss functions and network architecture allow it to achieve this?

2. The paper mentions using a relative geodesic loss instead of an absolute geodesic loss. Why is using a relative loss better for preserving local intrinsic geometry compared to an absolute loss? What problems could arise from using an absolute loss?

3. The paper proposes adding a bijectivity loss and a KL loss during NIE training. What is the purpose of each of these losses? How do they help improve the quality of the learned embeddings?

4. The paper modifies the standard DGCNN architecture for NIE. What issue were they trying to address with this modification? How does using farthest point sampling and re-assigning neighbors help alleviate sampling density bias?

5. The NIM network replaces spectral embeddings with NIE embeddings. How does using learned embeddings instead of eigenfunctions of the Laplace-Beltrami operator change the pipeline? What benefits does using NIE provide?

6. Instead of using structural losses on the functional maps like previous deep functional maps works, NIM uses a cyclic mapping loss. Why is this loss useful when ground truth correspondences are not available? 

7. What assumptions does the overall NIM pipeline make about the training data? Could it work with non-isometric shapes or shapes with different connectivity?

8. How is the method evaluated? What metrics are used to quantify the quality of the embeddings and correspondences? Are they appropriate for this task?

9. What are the limitations of the proposed method? When does it fail to estimate good correspondences? Could it be made more robust?

10. The method requires geodesic distance matrices for training shapes. How are these computed? Could the method work with approximate geodesic distances instead of accurate ones?
