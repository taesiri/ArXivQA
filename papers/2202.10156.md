# [1-WL Expressiveness Is (Almost) All You Need](https://arxiv.org/abs/2202.10156)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is whether the limited expressiveness of the Weisfeiler-Leman (WL) graph isomorphism test and message passing neural networks (MPNNs) is actually a limiting factor for their performance on real-world graph datasets. The paper hypothesizes that despite the theoretical limitations of WL and MPNNs, their expressiveness is sufficient to identify most graphs and achieve high accuracy on standard benchmark datasets. Thus, the limited expressiveness may not be a major limiting factor in practice.To test this hypothesis, the paper analyzes the fraction of graphs identifiable by WL and the upper bounds on classification accuracy for WL-based models across several common graph datasets. The results show that WL can identify the vast majority of graphs in most datasets and allow near perfect accuracy, suggesting its expressiveness is not a major limitation. The paper also finds that simple WL-based neural networks can fit these datasets well, further supporting the claim.In summary, the central hypothesis is that despite theoretical expressiveness limits, WL and MPNNs are sufficiently expressive for real-world graph data, so developing more expressive methods may not be necessary to achieve strong performance. The paper aims to test this claim through empirical analysis.


## What is the main contribution of this paper?

 The main contribution of this paper is an analysis of the expressiveness of the Weisfeiler-Leman (WL) graph isomorphism test and message passing neural networks (MPNNs) on common graph classification datasets. The key findings are:- The WL test is able to distinguish almost all graphs in most standard datasets, even with just 1-2 iterations. This shows the limited expressiveness of WL is often not an issue in practice.- The theoretical upper bound classification accuracy using WL representations is close to 100% on many datasets. This means WL expressiveness is sufficient to get near perfect performance. - Simple WL-based models like a WL feature MLP can fit most datasets well. Popular MPNNs like GCN, GAT and GIN can also achieve high accuracy on many datasets. This shows their WL-limited expressiveness is not a major limitation.- Motif-based representations, while more expressive, do not provide substantial improvements over WL features in the analyzed datasets.In summary, the paper concludes that the theoretically limited expressiveness of WL and MPNNs does not restrict their performance in practice on many standard graph classification datasets. Hence, developing new MPNN architectures is still relevant even if their expressiveness is bounded by WL. The paper challenges the notion that more expressive GNNs are necessary to achieve good performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper analyzes the expressiveness of the Weisfeiler-Leman graph isomorphism test and message passing neural networks on standard graph datasets, finding that their theoretically limited expressiveness does not limit performance in practice.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on the expressiveness of graph neural networks:- The main contribution of this paper is an empirical analysis showing that the limited expressiveness of 1-WL and message passing neural networks (MPNNs) is often not a major limiting factor for performance on real-world graph classification datasets. This supports the continued development of MPNNs despite their theoretical limitations. - Many previous works have focused on developing new graph neural network architectures that are provably more expressive than 1-WL and MPNNs. This paper provides evidence that such enhanced expressiveness may not lead to better performance in practice.- The authors thoroughly evaluate the expressiveness of 1-WL on common benchmark datasets by computing the fraction of distinguishable graphs and upper bounds on classification accuracy. This level of empirical analysis of WL expressiveness on real datasets has not been done before.- Compared to theoretical works deriving limitations of WL and MPNNs on contrived graph examples, this paper takes a more practical perspective by evaluating expressiveness on real-world data. The findings suggest WL limitations may be less relevant in practice.- The paper includes experiments training simple WL-based models and MPNNs on the datasets, showing they can often achieve high accuracy. This goes beyond just analyzing WL expressiveness and provides evidence that WL-based models can effectively fit these datasets.In summary, this empirical study provides new insights into the practical relevance of WL/MPNN limitations using real-world data rather than contrived examples. The findings suggest expressiveness may be less of a bottleneck than previously thought based on theory, supporting continued research into MPNNs and other WL-based graph neural networks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Develop more efficient and scalable implementations of higher-order WL algorithms and higher-order GNNs. The authors mention that a key challenge with using higher-order structures is the increased computational complexity. New algorithms and implementations could help address this issue.- Explore the trade-off between expressiveness and generalization ability more closely. While the 1-WL expressiveness seems sufficient for many datasets, it remains unclear if more expressive models can improve generalization in some cases. Studies on model complexity and overfitting could provide more insights.- Develop theoretically grounded criteria for determining the required graph neural network (GNN) expressiveness for a given problem. The paper shows empirically that 1-WL is often enough, but more theoretical analysis could lead to better guidelines.- Study the trainability of different GNN architectures in depth and relate it to model expressiveness. The results indicate trainability affects empirical expressiveness, so understanding what makes some models easier to train is important.- Evaluate different message passing architectures and training procedures. Since model expressiveness does not fully determine performance, improvements in these areas could still lead to better generalization even without increasing expressiveness.- Apply the analysis to more diverse and larger graph datasets. The paper focuses on standard benchmark datasets, but emerging challenging datasets could reveal new insights.In summary, the authors recommend focusing on more efficient and scalable implementations, better understanding model complexity, developing theoretical guidelines, studying trainability, and evaluating different architectures/training procedures as key directions for future research in this area. Evaluating on new datasets is also suggested.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper analyzes whether the limited expressiveness of the Weisfeiler-Leman (WL) graph isomorphism test and message passing neural networks (MPNNs) is actually a limiting factor for performance on standard graph classification datasets. The authors compute the fraction of graphs identifiable by WL and the upper bound classification accuracy for several datasets. They find that WL can identify almost all graphs and achieve near 100% accuracy upper bounds on most datasets. They also show that a simple WL-MLP model and several MPNN architectures can fit the datasets well. The results indicate that the limited WL/MPNN expressiveness is not a major limiting factor in practice, and development of new MPNNs should not be abandoned due to theoretical expressiveness concerns. Overall, the paper concludes WL/MPNN expressiveness is sufficient for many real-world graph classification tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper analyzes whether the limited expressiveness of the Weisfeiler-Leman (WL) graph isomorphism test and message passing neural networks (MPNNs) is actually a limiting factor for performance on standard graph datasets. The authors compute the fraction of graphs that can be distinguished by WL in several popular datasets. They find that WL is able to distinguish almost all graphs after only 1-2 iterations in most datasets. They also compute an upper bound on classification accuracy for WL-based models and find it is close to 100% in many cases. Furthermore, they show that simple WL-based neural networks and MPNNs like GCN, GAT, and GIN can fit these datasets well. Based on these findings, the authors conclude that the limited expressiveness of WL and MPNNs does not limit their performance on real-world graph datasets. This suggests that developing new MPNN architectures with WL expressiveness is still worthwhile, despite their theoretical limitations. The paper also analyzes motif-based graph representations as an alternative to WL, but finds they have lower expressiveness and accuracy upper bounds in many cases. Overall, the paper provides evidence that WL expressiveness is sufficient for many graph learning tasks, so limited expressiveness may not be a major barrier to developing effective graph neural networks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes utilizing the expressiveness of the Weisfeiler-Leman (WL) graph isomorphism test to analyze whether the limited expressiveness of message passing neural networks (MPNNs) is actually a limiting factor for performance on real-world graph classification datasets. The authors compute the fraction of graphs that can be uniquely identified by the 1-WL graph coloring algorithm in several popular datasets. They find that in most datasets, a large fraction of graphs can already be distinguished with just 1-2 iterations of WL. They also estimate an upper bound for the classification accuracy achievable with WL-based models, and find it is very close to 100% in many datasets. To complement this theoretical analysis, they train a simple WL-based MLP and several MPNN architectures on the datasets and find they can fit them reasonably well, further suggesting limited expressiveness is not an issue. Overall, the results indicate the restricted expressiveness of WL/MPNNs does not limit performance in practice on many real graph datasets.


## What problem or question is the paper addressing?

 The paper is addressing the question of whether the limited expressiveness of the 1-WL graph isomorphism test and message passing neural networks (MPNNs) is actually a limiting factor for their performance on real-world graph classification datasets. Specifically, it has been shown theoretically that MPNNs are limited in their expressiveness by the 1-WL test, meaning they cannot distinguish certain non-isomorphic graphs. This has motivated work on developing more expressive graph neural network architectures. However, the paper investigates whether this limited expressiveness due to 1-WL is really a problem in practice on common benchmark datasets. The authors analyze the fraction of non-isomorphic graphs that can be identified by 1-WL in several datasets, and find that it is very high in most cases. They also estimate upper bounds on the classification accuracy, and find it is close to 100% on many datasets even with 1-WL representations.In summary, the paper concludes that despite the theoretical limitations of 1-WL and MPNNs, their expressiveness is sufficient to effectively learn representations and classify graphs on most real-world datasets. The development of more expressive models may not be necessary to achieve high accuracy on these datasets.
