# [Adaptive Shortcut Debiasing for Online Continual Learning](https://arxiv.org/abs/2312.08677)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new method called DropTop for debiasing shortcut learning in online continual learning (OCL). OCL faces challenges from shortcut bias which relies on simple cues and hurts transferability to new tasks. DropTop introduces two novel techniques - attentive debiasing with feature map fusion and adaptive intensity shifting. Attentive debiasing fuses low and high level features to identify shortcut regions for dropping. Adaptive intensity shifting determines the appropriate proportion of features to drop over time based on loss reductions. Without needing prior knowledge or auxiliary data, DropTop automatically handles the varying extent of shortcut bias in online learning. Extensive experiments on benchmark datasets with multiple OCL algorithms show DropTop substantially improves average accuracy and forgetting. For example, it increases average accuracy for experience replay by 10.2% on CIFAR-10 and reduces forgetting by 63.2% for a prompt tuning method on ImageNet variants. The consistent gains demonstrate the importance and effectiveness of debiasing shortcuts in online continual learning.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Adaptive Shortcut Debiasing for Online Continual Learning":

Problem:
- Deep neural networks (DNNs) often rely too much on "shortcut" features that are easy to learn but not robust, like color, texture, or background cues, instead of more complex "intrinsic" features. This "shortcut bias" hinders performance on new tasks.
- Shortcut bias is especially problematic for online continual learning (OCL), where models get a stream of data from new tasks and have limited compute/memory. OCL models are prone to overfit to shortcuts due to limited data. This causes poor transferability to new tasks and high forgetting of old tasks.  
- Existing debiasing methods require either predefined knowledge of undesirable biases or auxiliary out-of-distribution data, neither of which are available in OCL. So new methods are needed to debias OCL models adaptively.

Proposed Solution:
- The authors propose "DropTop", which suppresses shortcut features in an OCL model by dropping the most highly activated features in intermediate layers. This prevents over-reliance on those features.
- To determine which features to drop, DropTop fuses attention maps from low-level and high-level layers, providing both spatial details and semantic information.
- Importantly, DropTop adaptively shifts how aggressively it drops features over time based on the training loss reduction. This accounts for the changing dominance of shortcuts.

Main Contributions:
- First method to address the significant but understudied problem of shortcut bias specifically for online continual learning
- Novel techniques of feature map fusion and adaptive intensity shifting that work completely online to determine key shortcut features and appropriate aggressiveness of debiasing
- Extensive experiments showing DropTop consistently improves state-of-the-art OCL methods by up to 10.4% higher accuracy and 63.2% lower forgetting across datasets
- Theoretical analysis and empirical demonstration of how shortcut bias exacerbates negative transfer and catastrophic forgetting in OCL
