# [Improving Zero-shot Generalization and Robustness of Multi-modal Models](https://arxiv.org/abs/2212.01758)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the key research question this paper tries to address is: 

How to improve the top-1 zero-shot classification accuracy of multi-modal models like CLIP and LiT on ImageNet dataset?

The authors noticed a large gap between the top-1 and top-5 zero-shot classification accuracy of CLIP model on ImageNet. They investigated the failure cases and found many mistakes are caused by ambiguity in the text prompts and sensitivity of the text encoder. 

To improve the top-1 accuracy, the authors propose two main techniques:

1) Develop a confidence estimation method to identify low-confidence predictions that are likely to be incorrect, by measuring consistency across different text prompts and image perturbations.

2) Improve the accuracy on those low-confidence examples by augmenting their class labels using the WordNet hierarchy, incorporating semantic information from parent nodes (top-down) and child nodes (bottom-up). 

The key hypothesis is that by identifying likely mistakes through the confidence estimation, and fixing them using the label augmentation based on WordNet, they can significantly boost the top-1 zero-shot classification accuracy on ImageNet for multi-modal models like CLIP and LiT.

The experiments verify their hypothesis, showing up to 17.13% improvement on the low-confidence subset and 3.6% overall accuracy improvement on ImageNet using their proposed techniques.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It identifies several failure modes for zero-shot ImageNet classification using multi-modal models like CLIP and LiT. The analysis suggests that the text encoder in these models is very sensitive to the choice of prompts. 

2. It proposes a simple yet efficient method for zero-shot confidence estimation that is suited for multi-modal models. The confidence score is based on measuring the consistency of predictions under different text prompts and image transformations.

3. It develops a label augmentation technique using WordNet hierarchy that utilizes both ancestor (top-down) and children (bottom-up) labels. Applying this to the low confidence subset significantly improves their prediction accuracy.

In summary, the key contributions are:

- Analysis of failure modes in multi-modal zero-shot classification

- A zero-shot confidence score specifically designed for multi-modal models

- A label augmentation method using WordNet hierarchy that improves accuracy, especially for uncertain predictions

The proposed techniques are model-agnostic, require no additional training data or model modification, and consistently improve zero-shot classification accuracy across various datasets and model architectures. The paper demonstrates the effectiveness of these ideas on CLIP and LiT models.

So in essence, the main contribution is a simple yet effective post-hoc method to identify low-confidence predictions in multi-modal models, and use label hierarchy to improve their accuracy in a zero-shot manner.
