# [DiffFit: Unlocking Transferability of Large Diffusion Models via Simple   Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2304.06648)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes a simple yet effective parameter-efficient fine-tuning strategy called DiffFit for large pre-trained diffusion models like DiT. - The main goal is to enable fast adaptation of such models to new domains/datasets without compromising performance while being efficient in terms of computations and storage.- The central hypothesis is that fine-tuning only the bias terms and adding learnable scaling factors in certain layers would suffice for fast adaptation while requiring much fewer trainable parameters and computations compared to full fine-tuning.- Experiments on 8 downstream datasets validate this hypothesis, showing DiffFit achieves superior or competitive results to full fine-tuning using only 0.12% of the parameters and 2x faster training.- Further experiments surprisingly show DiffFit can also effectively adapt a pretrained low-resolution model to high-resolution by treating it as a domain shift problem.- Theoretical analysis provides intuitions on why tuning scaling factors enables matching shifted distributions.In summary, the central hypothesis is that fine-tuning only bias and scaling factors is sufficient for fast and efficient diffusion model adaptation across domains and resolutions, which is validated empirically and theoretically. The key research question is whether this simple yet effective strategy can unlock transferability of large diffusion models.
