# PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:How far can we push the limits of dialogue generation pre-training by scaling up model size, dataset size, and compute? Specifically, the paper introduces PLATO-XL, an 11 billion parameter dialogue model trained on massive social media conversations, and explores its capabilities on chitchat, knowledge grounded conversations, and task-oriented dialogues. The key hypotheses tested in the paper are:1) Conversation quality can continue to benefit from enlarged model scale if appropriate pre-training techniques are used. 2) PLATO-XL with its huge parameters can absorb common knowledge, alleviate hallucination and inconsistency issues in dialogues.3) Multi-party aware pre-training helps distinguish speaker information and maintain consistency. 4) PLATO-XL has potential as a foundation model for conversational AI, with superior performance on chitchat, knowledge grounded and task-oriented conversations.In summary, the paper aims to push the limits of pre-trained dialogue models by scaling up PLATO-XL and explores its capabilities across multiple conversational tasks. The central hypothesis is that with appropriate techniques, larger models like PLATO-XL can continue to improve conversation quality and serve as a strong foundation for conversational AI.


## What is the main contribution of this paper?

The main contributions of this paper are:- Presenting PLATO-XL, a large-scale pre-trained dialogue generation model with up to 11 billion parameters, trained on both Chinese and English social media conversations. - Adopting the unified transformer architecture for high computation and parameter efficiency to train such a huge model.- Proposing multi-party aware pre-training to better distinguish and utilize the characteristic information in social media conversations.- Demonstrating superior performance of PLATO-XL over other models in both Chinese and English chitchat through comprehensive evaluations.- Exploring the ability of PLATO-XL on knowledge grounded dialogue and task-oriented conversation, where it also achieves state-of-the-art results.- Releasing the source code and English model to facilitate research in large-scale dialogue pre-training.In summary, this paper explores the limit of pre-training for dialogue generation through designing PLATO-XL and verifies its potential as a foundation model for conversational AI across multiple tasks. The proposed techniques for efficient training and multi-party encoding are key to the success of PLATO-XL.
