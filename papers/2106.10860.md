# [Multiplying Matrices Without Multiplying](https://arxiv.org/abs/2106.10860)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently approximate matrix multiplication. Specifically, the authors propose a new approximate matrix multiplication algorithm called MADDNESS that aims to greatly outperform existing methods in terms of speed and accuracy tradeoffs.

The key ideas behind MADDNESS are:

- Using a learned hash function instead of expensive distance computations to assign vectors to prototypes/buckets. This encoding step is orders of magnitude faster than in prior methods.

- Optimizing the prototypes to minimize reconstruction error, allowing better utilization of the encoding's capacity. 

- Using averaging operations instead of additions to aggregate bucket lookup results. This allows keeping sums in low-bitwidth integers for longer to exploit SIMD instructions.

- Providing theoretical guarantees on the overall approximation error based on quantization error bounds and generalization theory.

The overall goal is to get significant speedups over exact matrix multiplication and other approximate methods on real-world matrices, especially when one matrix is much larger than the other. The hypothesis is that a mixture of hashing, averaging, and byte shuffling can outperform the sparsified/factorized/quantized matrix products commonly proposed in prior work.

In summary, the central hypothesis is that the algorithmic innovations in MADDNESS will enable superior speed-quality tradeoffs compared to prior approximate matrix multiplication techniques. The paper evaluates this hypothesis through extensive experiments on real-world matrix multiplication tasks.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is an approximate matrix multiplication algorithm called MADDNESS that outperforms existing methods. The key ideas behind MADDNESS are:

- Using a learned hash function based on balanced binary regression trees to quickly encode the rows of the input matrix A. This encoding is orders of magnitude faster than vector quantization methods like product quantization.

- Optimizing the vector quantization prototypes by reconstructing the training matrix from them using ridge regression. This allows the prototypes to capture mutual information between different subspaces. 

- Using averaging operations instead of addition to aggregate results when doing table lookups. This allows faster computation with 8-bit integers while introducing only a small, bounded amount of bias.

The authors evaluate MADDNESS extensively on hundreds of real-world matrices from domains like image classification, time series classification, and image filtering. The results show it achieves up to 100x speedups over exact matrix multiplication and 10x speedups over prior approximate methods.

The paper also provides theoretical analysis of the algorithm, including generalization error bounds and closed-form expressions for the bias introduced by the approximate integer summation.

In summary, the main contribution is a new approximate matrix multiplication method with strong empirical performance and theoretical grounding, enabled by novel techniques for fast encoding, prototype optimization, and low-bitwidth aggregation. The results suggest this approach could be promising for accelerating machine learning workloads.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a method for quantizing lookup tables for approximate matrix multiplication that computes table-specific offset and scale factors to linearly map the largest and smallest table entries to 255 and 0, enabling the use of 8-bit integers and SIMD vector operations.
