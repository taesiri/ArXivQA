# [GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via   Stationary Distribution Correction Estimation](https://arxiv.org/abs/2312.10802)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via Stationary Distribution Correction Estimation":

Problem:
Existing offline imitation learning (IL) techniques face challenges in learning policies for long-horizon tasks and require significant retraining when task specifications change. These limitations make them difficult to apply to real-world robotic tasks which are often long-horizon and have varying goals across instances.

Solution - GO-DICE Algorithm:
The paper proposes a new algorithm called GO-DICE for offline goal-conditioned IL that addresses the above limitations. The key ideas are:

1) Hierarchical Policy Learning: GO-DICE discerns a hierarchy of sub-tasks (options) from demonstrations and learns separate policies for (a) sub-task transitions (high-level policy) and (b) action execution (low-level policy). This facilitates long-horizon reasoning.

2) Goal Conditioning: Both high-level and low-level policies are conditioned on goals, minimizing the need for retraining when goals change. 

3) Distribution Matching: Inspired by the DICE family of IL techniques, policy learning happens by matching stationary state-action distributions induced by the learnt policy and demonstrations. This allows learning from imperfect demonstrations too.

Additionally, when partial annotations of sub-tasks are available from humans, GO-DICE can utilize them to further improve learning.

Main Contributions:

1) A new algorithm GO-DICE that combines ideas of hierarchical Reinforcement Learning, goal conditioning, stationary distribution matching, and semi-supervision for offline IL.

2) Empirical evaluation on challenging robotic simulation tasks showing GO-DICE's superior performance over recent offline IL baselines in long-horizon tasks and under changing goals.

3) Analysis providing insights such as GO-DICE's ability to leverage imperfect demonstrations and human annotations of sub-tasks to further boost performance.

In summary, the paper makes notable contributions in developing a practical offline IL approach for real-world robotic tasks under limitations of long horizons, changing goals, and imperfect expert data availability.
