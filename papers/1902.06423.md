# [CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix   Space Model](https://arxiv.org/abs/1902.06423)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we enhance simple word embedding-based sentence encoders with word order awareness in an efficient way?Specifically, the paper proposes and evaluates a method to add word order sensitivity to the Continuous Bag-of-Words (CBOW) model by combining it with the Compositional Matrix Space Model (CMSM) via matrix multiplication. The key hypotheses tested in the paper are:1) The CMSM, when trained properly, can capture word order information better than the commutative, order-insensitive CBOW model. 2) CBOW and CMSM learn complementary types of linguistic information from text.3) A hybrid CBOW-CMSM model can combine their strengths to create a sentence encoder that is both efficient and encodes useful linguistic properties like word order.4) This hybrid model will outperform both CBOW and CMSM alone on relevant downstream tasks.So in summary, the central research question is about efficiently injecting word order awareness into simple word embedding-based encoders like CBOW, with the key hypothesis being that a hybrid CBOW-CMSM model can achieve this. The paper provides empirical support for this hypothesis through comparative evaluations on probing and downstream tasks.


## What is the main contribution of this paper?

The main contribution of this paper is presenting an efficient, unsupervised learning scheme for training the Compositional Matrix Space Model (CMSM) of language. Specifically:- They propose an efficient training algorithm called Continual Multiplication Of Words (CMOW) that adapts the CBOW algorithm from word2vec to train CMSMs on large unlabeled corpora. - They introduce two key modifications to make CBOW suitable for training CMSMs: a novel initialization strategy and modifying the training objective.- They empirically show that CMOW and CBOW learn complementary linguistic properties. CBOW is better at word content memorization while CMOW better captures other linguistic properties like word order.- They propose a hybrid CBOW-CMOW model that combines the strengths of both approaches, outperforming them individually on downstream tasks.In summary, the main contribution is presenting the first unsupervised training scheme for CMSMs and showing it can be effectively combined with CBOW in a hybrid model to improve performance on downstream NLP tasks. This helps narrow the gap between simple word embedding models and more complex recurrent encoders in terms of representational power.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new hybrid model called CBOW-CMOW that combines Continuous Bag-of-Words (CBOW) and Continual Multiplication of Words (CMOW) to create sentence embeddings that retain CBOW's ability to memorize word content while improving the encoding of other linguistic properties.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Exploring other aggregation functions besides summation and multiplication for composing word embeddings into sentence embeddings. The paper focuses on comparing summation (used in CBOW) and multiplication (used in CMOW), but mentions there may be other useful functions to consider.- Applying the CMOW model and training approach to other embedding techniques besides CBOW, such as fastText. The authors propose their hybrid CBOW-CMOW model could serve as a blueprint for incorporating CMOW into other existing embedding methods.- Investigating dynamic programming techniques to further optimize computation of the matrix multiplications in CMOW. The associativity of matrix multiplication means techniques like caching frequent bigrams could help reduce sequential steps needed for encoding.- Evaluating the CMOW model on a wider range of downstream tasks beyond what is included in the SentEval framework used in the paper. The authors note most of the SentEval tasks focus on word content memorization, which favors CBOW, so testing on more tasks requiring word order information could be beneficial.- Comparing CMOW to more recent benchmark sentence encoding techniques like BERT rather than just CBOW/fastText. The authors acknowledge their models are not state-of-the-art but focus on analyzing CBOW vs CMOW.- Exploring unsupervised objectives beyond the word2vec-style prediction task used in the paper. The authors suggest their training scheme could incentivize better learning of semantic properties.- Analyzing what linguistic properties and features are captured by CMOW vs. CBOW embeddings using techniques like probing classifiers. The authors empirically demonstrate complementary strengths but do not deeply analyze what causes them.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a novel method called Continual Multiplication Of Words (CMOW) for training the Compositional Matrix Space Model (CMSM) on large unlabeled datasets. CMSM represents words as matrices rather than vectors, allowing it to capture word order information unlike previous approaches like CBOW that use vector addition. The key contributions are: 1) An efficient unsupervised training scheme for CMSM using modifications to the word2vec CBOW objective and initialization strategy. 2) Empirical analysis showing CMOW better captures linguistic properties while CBOW is better at word content memorization. 3) A hybrid CBOW-CMOW model combining their complementary strengths, which improves performance on downstream tasks requiring word order information by 1.2% on average over CBOW. The proposed methods offer a computationally efficient way to make simple word aggregation models like CBOW more expressive in capturing word order and other linguistic properties.
