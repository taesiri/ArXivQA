# [CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix   Space Model](https://arxiv.org/abs/1902.06423)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we enhance simple word embedding-based sentence encoders with word order awareness in an efficient way?Specifically, the paper proposes and evaluates a method to add word order sensitivity to the Continuous Bag-of-Words (CBOW) model by combining it with the Compositional Matrix Space Model (CMSM) via matrix multiplication. The key hypotheses tested in the paper are:1) The CMSM, when trained properly, can capture word order information better than the commutative, order-insensitive CBOW model. 2) CBOW and CMSM learn complementary types of linguistic information from text.3) A hybrid CBOW-CMSM model can combine their strengths to create a sentence encoder that is both efficient and encodes useful linguistic properties like word order.4) This hybrid model will outperform both CBOW and CMSM alone on relevant downstream tasks.So in summary, the central research question is about efficiently injecting word order awareness into simple word embedding-based encoders like CBOW, with the key hypothesis being that a hybrid CBOW-CMSM model can achieve this. The paper provides empirical support for this hypothesis through comparative evaluations on probing and downstream tasks.


## What is the main contribution of this paper?

The main contribution of this paper is presenting an efficient, unsupervised learning scheme for training the Compositional Matrix Space Model (CMSM) of language. Specifically:- They propose an efficient training algorithm called Continual Multiplication Of Words (CMOW) that adapts the CBOW algorithm from word2vec to train CMSMs on large unlabeled corpora. - They introduce two key modifications to make CBOW suitable for training CMSMs: a novel initialization strategy and modifying the training objective.- They empirically show that CMOW and CBOW learn complementary linguistic properties. CBOW is better at word content memorization while CMOW better captures other linguistic properties like word order.- They propose a hybrid CBOW-CMOW model that combines the strengths of both approaches, outperforming them individually on downstream tasks.In summary, the main contribution is presenting the first unsupervised training scheme for CMSMs and showing it can be effectively combined with CBOW in a hybrid model to improve performance on downstream NLP tasks. This helps narrow the gap between simple word embedding models and more complex recurrent encoders in terms of representational power.
