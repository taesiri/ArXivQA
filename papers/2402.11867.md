# [LoRA Training in the NTK Regime has No Spurious Local Minima](https://arxiv.org/abs/2402.11867)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Low-rank adaptation (LoRA) has become the standard approach for efficient fine-tuning of large pre-trained language models. However, there is limited theoretical understanding of why LoRA works well in practice. Specifically, questions around the trainability, generalization capabilities, and role of the LoRA rank remain unanswered. 

Proposed Solution:
This paper provides a theoretical analysis of LoRA fine-tuning under the neural tangent kernel (NTK) regime. The key assumptions are that fine-tuning happens in a neighborhood around the pre-trained parameters where a linear approximation holds, and a small number (N) of fine-tuning data points are used. 

Main Contributions:

1) Existence of low-rank solutions: The paper shows that full fine-tuning without LoRA admits a low-rank solution with rank r ~ √N. This provides justification for using a low-rank parametrization like LoRA. 

2) Elimination of spurious local minima with LoRA: Using a rank r >> √N with LoRA eliminates bad local minima, allowing gradient descent to find the globally optimal low-rank solutions.

3) Generalization guarantee: The low-rank solution found by LoRA generalizes well, with an excess risk bound that depends on the nuclear norm of the optimal solution. 

In addition to the theory, experiments on NLP datasets validate that LoRA finds solutions matching full fine-tuning. An interesting observation is that lower LoRA rank slows down convergence, suggesting a trade-off between efficiency and training speed.

Overall, this is the first work providing optimization and generalization guarantees for LoRA fine-tuning of large language models, advancing our theoretical understanding. Key open questions include analyzing the role of LoRA rank and relaxing the linearization assumption.
