# [Amplifying Training Data Exposure through Fine-Tuning with   Pseudo-Labeled Memberships](https://arxiv.org/abs/2402.12189)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper explores a new attack strategy against neural language models (LMs) called "adversarial fine-tuning to amplify training data exposure." The key problem is that LMs are vulnerable to training data extraction (TDE) attacks due to memorizing large portions of their training data, which can contain private or sensitive information. 

The paper proposes a novel attack where the adversary fine-tunes a target pre-trained LM using its own self-generated text to intensify the model's retention of its original pre-training dataset. This aims to increase the risk of exposing more private training data through standard TDE attacks. 

Two main challenges are generating text closely aligned with the actual (unknown) pre-training data and ensuring accurate labels on the self-generated text for effective fine-tuning. To address these, the paper proposes:

1. Pseudo-labeling self-generated text based on machine-generated probabilities from DetectGPT, a state-of-the-art zero-shot human/machine text classifier. Lower probabilities are assumed more likely to incorporate human-written training data.

2. Fine-tuning the target LM using reinforcement learning from human feedback (RLHF), rewarding responses more reminiscent of training data based on the pseudo-labels. This aligns the LM's behavior towards favoring memorized data.

Experiments on 6 model sizes of OPT (125M to 13B parameters) show the attack amplifies training data exposure by 4-8x for LMs over 1B parameters. Extractions contain up to 1163 verbatim words, indicating significantly heightened privacy risks from adversarial fine-tuning.

The paper discusses mitigation strategies like reducing reliability of machine-generated text detection and fine-tuning LMs to reduce rather than amplify training data retention. Overall, this explores a dangerous new attack vector that calls for further research into securing the privacy of LMs.
