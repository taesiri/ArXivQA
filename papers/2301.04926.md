# [CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP](https://arxiv.org/abs/2301.04926)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to leverage CLIP's 2D image-text pre-learned knowledge to help 3D scene understanding. Specifically, the authors investigate:

1) Whether CLIP knowledge can help a 3D network achieve annotation-free 3D semantic segmentation. 

2) Whether a CLIP pre-trained 3D network outperforms other self-supervised methods when fine-tuned on labelled data for 3D semantic segmentation.

3) How to efficiently transfer CLIP's image and text features to a 3D network via a novel cross-modal pre-training framework.

The key hypothesis is that CLIP's semantic knowledge and visual features can help regularize a 3D network to learn better representations that benefit various 3D scene understanding tasks like annotation-free segmentation and few-shot segmentation.
