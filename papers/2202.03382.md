# [Corrupted Image Modeling for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2202.03382)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis seems to be:

Can a flexible framework for self-supervised visual pre-training, called Corrupted Image Modeling (CIM), learn rich visual representations for both vision transformers (ViTs) and convolutional neural networks (CNNs)?

The key points are:

- CIM avoids directly manipulating artificial [MASK] tokens like in masked image modeling (MIM). Instead, it uses an auxiliary generator to corrupt the input image.

- The generator consists of a small trainable BEiT model that predicts visual tokens for masked image regions. These are sampled and mapped to corrupted images. 

- The enhancer network then tries to reconstruct the original uncorrupted image pixels (generative objective) or identify replaced visual tokens (discriminative).

- After pre-training, the generator is discarded and the enhancer is transferred.

- CIM is proposed as a general, flexible framework suitable for pre-training both ViT and CNN architectures, unlike MIM which is more constrained. 

So in summary, the central hypothesis is that CIM can serve as a unified approach to pre-train rich visual representations for diverse model architectures, not just ViTs specialized for MIM. The paper aims to demonstrate this through empirical evaluations on ImageNet classification and segmentation tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution seems to be proposing a new self-supervised visual pre-training framework called Corrupted Image Modeling (CIM). The key ideas of CIM are:

- Instead of using artificial [MASK] tokens to corrupt the input image like in masked image modeling (MIM), CIM uses a small trainable BEiT network as an auxiliary generator to produce corrupted images. 

- The generator learns to predict visual tokens for the masked image patches. These predicted tokens are sampled and used to reconstruct a corrupted image via a frozen image tokenizer.

- The corrupted image is fed to an enhancer network, which is trained on a pixel reconstruction task (ResPix) or a discriminative token replacement detection task (RevDet). 

- After pre-training, the generator is discarded and the enhancer can be used as a visual encoder. 

- CIM can pre-train both vision transformers (ViTs) and convolutional networks (CNNs) using this unified framework. Prior MIM approaches were mainly suited for ViTs.

So in summary, the key contribution is proposing CIM as a more flexible and general visual pre-training approach compared to prior MIM methods. CIM avoids limitations of using [MASK] tokens, and can pre-train both CNNs and ViTs in a unified framework. The experimental results demonstrate CIM achieves strong performance on image classification and segmentation benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a new self-supervised visual pre-training framework called Corrupted Image Modeling (CIM) that uses an auxiliary generator to corrupt the input image instead of artificial mask tokens, and trains an enhancer network to recover the original image pixels or detect replaced patches, enabling pre-training of both vision transformers and CNNs with a unified non-Siamese approach.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in the field of self-supervised visual pre-training:

- The proposed Corrupted Image Modeling (CIM) framework offers a more flexible approach to pre-training compared to prior work like Masked Image Modeling (MIM). CIM avoids directly manipulating mask tokens and can work with various architectures including CNNs, not just Vision Transformers. This is a key advantage over MIM methods like BEiT and MAE which rely on patch masks.

- CIM pre-trains models using a corrupted image generated by a small trainable BEiT generator, rather than just applying predefined augmentations like many self-supervised methods do. This data generation process seems more robust and provides additional supervision to the enhancer model.

- The paper shows SOTA results on ImageNet classification using CIM to pre-train both ViT and ResNet models. This demonstrates the effectiveness of CIM across architectures. Many recent self-supervised methods have focused specifically on pre-training Vision Transformers.

- For semantic segmentation on ADE20K, CIM also achieves strong performance when transferred to this task. This shows the representations learned are useful for dense prediction tasks beyond just image classification.

- Compared to contrastive self-supervised approaches like SimCLR and BYOL, CIM does not rely on a Siamese network structure. The generator and enhancer have a more asymmetric relationship. This is a distinctive design among recent methods.

- The ablations provide some useful insights. For example, CIM works better with random vs blockwise masking, and benefits from sharing some early layers between the generator and enhancer. The sampling strategy for mask tokens is also analyzed.

Overall, CIM seems to offer a flexible and high-performing approach to self-supervised visual pre-training. The ability to effectively pre-train CNNs and ViTs with the same framework is a notable advantage over prior arts like MIM. The results demonstrate state-of-the-art capabilities on multiple benchmarks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions the authors suggest are:

- Exploring different image corrupting strategies and generators to improve the diversity and quality of the corrupted images fed to the enhancer network during pre-training. The current image corrupting process using the small BEiT still has room for improvement.

- Studying alternative image tokenizers beyond the DALL-E tokenizer used in the paper, such as VQGAN, which can provide higher throughput and better image generation quality. This could potentially improve the efficiency and effectiveness of CIM pre-training.

- Scaling up CIM pre-training to larger vision models, like ViT-Large and bigger convolutional networks, to further demonstrate its generalization ability and test the limits of the approach.

- Extending CIM to other vision tasks beyond image classification and segmentation evaluated in the paper, such as object detection, to show its transfer learning abilities more broadly.

- Exploring whether ideas from CIM could be incorporated into masked image modeling approaches to improve their flexibility, like allowing them to pre-train CNNs directly.

- Developing better quantitative evaluation metrics and indicators for measuring the quality of the generator and relating it to the resulting representation quality of the enhancer.

- Studying the theoretical connections between CIM and other representation learning approaches to better understand its properties.

In summary, the key future directions focus on scaling up the approach to bigger models and tasks, improving the image corrupting process, exploring alternative tokenizers, extending CIM to other architectures like CNNs, and developing better evaluation metrics and theory around the framework. The authors position CIM as a promising starting point for further research into flexible and unified visual representation learning.
