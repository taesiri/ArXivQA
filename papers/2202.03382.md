# [Corrupted Image Modeling for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2202.03382)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis seems to be:

Can a flexible framework for self-supervised visual pre-training, called Corrupted Image Modeling (CIM), learn rich visual representations for both vision transformers (ViTs) and convolutional neural networks (CNNs)?

The key points are:

- CIM avoids directly manipulating artificial [MASK] tokens like in masked image modeling (MIM). Instead, it uses an auxiliary generator to corrupt the input image.

- The generator consists of a small trainable BEiT model that predicts visual tokens for masked image regions. These are sampled and mapped to corrupted images. 

- The enhancer network then tries to reconstruct the original uncorrupted image pixels (generative objective) or identify replaced visual tokens (discriminative).

- After pre-training, the generator is discarded and the enhancer is transferred.

- CIM is proposed as a general, flexible framework suitable for pre-training both ViT and CNN architectures, unlike MIM which is more constrained. 

So in summary, the central hypothesis is that CIM can serve as a unified approach to pre-train rich visual representations for diverse model architectures, not just ViTs specialized for MIM. The paper aims to demonstrate this through empirical evaluations on ImageNet classification and segmentation tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution seems to be proposing a new self-supervised visual pre-training framework called Corrupted Image Modeling (CIM). The key ideas of CIM are:

- Instead of using artificial [MASK] tokens to corrupt the input image like in masked image modeling (MIM), CIM uses a small trainable BEiT network as an auxiliary generator to produce corrupted images. 

- The generator learns to predict visual tokens for the masked image patches. These predicted tokens are sampled and used to reconstruct a corrupted image via a frozen image tokenizer.

- The corrupted image is fed to an enhancer network, which is trained on a pixel reconstruction task (ResPix) or a discriminative token replacement detection task (RevDet). 

- After pre-training, the generator is discarded and the enhancer can be used as a visual encoder. 

- CIM can pre-train both vision transformers (ViTs) and convolutional networks (CNNs) using this unified framework. Prior MIM approaches were mainly suited for ViTs.

So in summary, the key contribution is proposing CIM as a more flexible and general visual pre-training approach compared to prior MIM methods. CIM avoids limitations of using [MASK] tokens, and can pre-train both CNNs and ViTs in a unified framework. The experimental results demonstrate CIM achieves strong performance on image classification and segmentation benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a new self-supervised visual pre-training framework called Corrupted Image Modeling (CIM) that uses an auxiliary generator to corrupt the input image instead of artificial mask tokens, and trains an enhancer network to recover the original image pixels or detect replaced patches, enabling pre-training of both vision transformers and CNNs with a unified non-Siamese approach.
