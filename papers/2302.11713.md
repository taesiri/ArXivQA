# Can Pre-trained Vision and Language Models Answer Visual   Information-Seeking Questions?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:"Do pre-trained vision and language models understand how to answer visual information-seeking questions?" Specifically, the authors aim to analyze whether recent advances in pre-trained multi-modal models (e.g. PaLI, CLIP) enable them to answer questions that require looking up knowledge beyond what is present in the image itself. To study this question, the authors introduce a new visual question answering (VQA) benchmark called InfoSeek that focuses on visual information-seeking questions, where the answer is not obvious from the image and requires external knowledge.Using this new benchmark, the authors evaluate various pre-trained vision-language models under different conditions - with and without access to a knowledge base. Their key findings are:- Current state-of-the-art pre-trained models still struggle to answer visual info-seeking questions, but fine-tuning on the InfoSeek dataset improves performance.- Pipeline models that first recognize the visual entity and then query a knowledge base significantly outperform end-to-end models, showing the value of external knowledge.- However, end-to-end models show better generalization on rare/tail entities.Overall, the central hypothesis is that existing pre-trained vision-language models have limited capability for visual information-seeking tasks, but this can be improved through fine-tuning on in-domain data or incorporating external knowledge. The InfoSeek benchmark provides a way to analyze these models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introduces a new visual question answering (VQA) dataset called InfoSeek that focuses specifically on visual information-seeking questions. This helps fill a gap, as existing VQA datasets do not sufficiently cover this type of question.2. Performs a multi-stage human annotation process to create a subset of InfoSeek (8.9K examples) with natural information-seeking questions. Also creates a large-scale subset (1.3M examples) by combining visual datasets with Wikidata. 3. Proposes two evaluation protocols called "No KB" and "With KB" to analyze different types of models - end-to-end vision-language models vs pipeline models using an explicit knowledge base.4. Experiments with state-of-the-art vision-language models like PaLI and OFA under the "No KB" setting. Finds they can answer some info-seeking questions after fine-tuning but struggle with generalizing to new entities or questions requiring deeper knowledge.5. Shows that pipeline models utilizing an external knowledge base significantly outperform end-to-end models, especially on real-world questions. Highlights the impact of better visual entity recognition.  6. Discovers end-to-end models uniquely outperform on rare entities, revealing promise for combining both model families.In summary, the key contribution is introducing a new benchmark and analysis methodology to understand visual information-seeking capabilities of pre-trained vision-language models. The analysis reveals current limitations and future directions.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on visual information-seeking questions and knowledge-based VQA:- It introduces a new large-scale dataset, InfoSeek, specifically designed to benchmark visual info-seeking questions. Many prior VQA datasets focus more on visual attributes or common sense knowledge rather than information seeking.- The paper proposes two evaluation protocols - with and without access to a knowledge base (KB). This allows for clear analysis of different model families and whether they acquire knowledge through pre-training or rely on external KB retrieval. - The analysis reveals challenges for current visual-language models on info-seeking questions, especially on generalizing to new entities. End-to-end models struggle with time/numerical questions requiring deeper knowledge compared to pipeline models.- The paper highlights the unique strength of end-to-end models on tail entities, while pipeline models excel on popular head entities. This suggests promising directions for combining both approaches.- Overall, the analysis and newly introduced benchmark address gaps in prior VQA research and provide insights into knowledge acquisition in pre-trained visual-language models. The findings pave the way for developing next-generation multi-modal pre-training.In summary, this paper pushes forward research in knowledge-based VQA through its analysis, proposed protocols, and introduction of the InfoSeek dataset for assessing visual information-seeking capabilities. The findings reveal current limitations of pre-trained models and opportunities for progress.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest include:- Improving visual entity recognition. The authors find this is a bottleneck for pipeline models and better entity recognition could significantly improve performance on answering visual info-seeking questions. They suggest exploring different approaches like end-to-end models which have an advantage for rare entities.- Incorporating more fine-grained knowledge into pre-training. The authors find end-to-end models lack deeper knowledge to answer certain question types like time and numerical questions. They suggest pre-training on more diverse data with fine-grained information could help improve these models.- Combining end-to-end and pipeline models. The authors highlight the unique advantages of end-to-end models for rare entities and pipeline models for popular entities. They suggest exploring ways to combine these complementary strengths.- Expanding the diversity of knowledge. The authors note their dataset currently focuses more on physical attributes and factual information. They suggest expanding to more social and abstract knowledge could reveal new challenges.- Studying human dialogues and multi-turn QA. The current work looks at single turn QA but the authors suggest studying dialogues and conversations could lead to more natural QA systems.In summary, key directions include improving visual entity recognition, incorporating more diverse fine-grained knowledge into pre-training, combining end-to-end and pipeline models, expanding knowledge types, and exploring multi-turn dialogues. The authors propose their analysis and dataset can serve as a starting point for exploring these areas further.
