# Can Pre-trained Vision and Language Models Answer Visual   Information-Seeking Questions?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:"Do pre-trained vision and language models understand how to answer visual information-seeking questions?" Specifically, the authors aim to analyze whether recent advances in pre-trained multi-modal models (e.g. PaLI, CLIP) enable them to answer questions that require looking up knowledge beyond what is present in the image itself. To study this question, the authors introduce a new visual question answering (VQA) benchmark called InfoSeek that focuses on visual information-seeking questions, where the answer is not obvious from the image and requires external knowledge.Using this new benchmark, the authors evaluate various pre-trained vision-language models under different conditions - with and without access to a knowledge base. Their key findings are:- Current state-of-the-art pre-trained models still struggle to answer visual info-seeking questions, but fine-tuning on the InfoSeek dataset improves performance.- Pipeline models that first recognize the visual entity and then query a knowledge base significantly outperform end-to-end models, showing the value of external knowledge.- However, end-to-end models show better generalization on rare/tail entities.Overall, the central hypothesis is that existing pre-trained vision-language models have limited capability for visual information-seeking tasks, but this can be improved through fine-tuning on in-domain data or incorporating external knowledge. The InfoSeek benchmark provides a way to analyze these models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introduces a new visual question answering (VQA) dataset called InfoSeek that focuses specifically on visual information-seeking questions. This helps fill a gap, as existing VQA datasets do not sufficiently cover this type of question.2. Performs a multi-stage human annotation process to create a subset of InfoSeek (8.9K examples) with natural information-seeking questions. Also creates a large-scale subset (1.3M examples) by combining visual datasets with Wikidata. 3. Proposes two evaluation protocols called "No KB" and "With KB" to analyze different types of models - end-to-end vision-language models vs pipeline models using an explicit knowledge base.4. Experiments with state-of-the-art vision-language models like PaLI and OFA under the "No KB" setting. Finds they can answer some info-seeking questions after fine-tuning but struggle with generalizing to new entities or questions requiring deeper knowledge.5. Shows that pipeline models utilizing an external knowledge base significantly outperform end-to-end models, especially on real-world questions. Highlights the impact of better visual entity recognition.  6. Discovers end-to-end models uniquely outperform on rare entities, revealing promise for combining both model families.In summary, the key contribution is introducing a new benchmark and analysis methodology to understand visual information-seeking capabilities of pre-trained vision-language models. The analysis reveals current limitations and future directions.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on visual information-seeking questions and knowledge-based VQA:- It introduces a new large-scale dataset, InfoSeek, specifically designed to benchmark visual info-seeking questions. Many prior VQA datasets focus more on visual attributes or common sense knowledge rather than information seeking.- The paper proposes two evaluation protocols - with and without access to a knowledge base (KB). This allows for clear analysis of different model families and whether they acquire knowledge through pre-training or rely on external KB retrieval. - The analysis reveals challenges for current visual-language models on info-seeking questions, especially on generalizing to new entities. End-to-end models struggle with time/numerical questions requiring deeper knowledge compared to pipeline models.- The paper highlights the unique strength of end-to-end models on tail entities, while pipeline models excel on popular head entities. This suggests promising directions for combining both approaches.- Overall, the analysis and newly introduced benchmark address gaps in prior VQA research and provide insights into knowledge acquisition in pre-trained visual-language models. The findings pave the way for developing next-generation multi-modal pre-training.In summary, this paper pushes forward research in knowledge-based VQA through its analysis, proposed protocols, and introduction of the InfoSeek dataset for assessing visual information-seeking capabilities. The findings reveal current limitations of pre-trained models and opportunities for progress.
