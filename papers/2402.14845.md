# [Purifying Large Language Models by Ensembling a Small Language Model](https://arxiv.org/abs/2402.14845)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) rely on abundant training data from external sources, which frequently contains uncurated instances that lead to issues like copyright infringement, data poisoning, and privacy violations. 
- Thoroughly cleansing the training data to develop "purified" LLMs from scratch is very labor-intensive and destructive to model capability.
- Existing well-trained LLMs still suffer from such negative effects due to uncurated data.

Proposed Solution: 
- Ensemble the untrusted LLM with a benign and small language model (SLM) trained only on curated data.  
- Show theoretically and empirically that this ensemble approach can effectively preserve LLM's performance while mitigating issues caused by uncurated data.
- The ensemble is equivalent to moderating the LLM's logit values with the SLM's at each sampling step. It allows flexibly creating models with customizable trade-offs between standard performance and negative effects.

Main Contributions:
- Propose a simple but effective ensemble method to purify LLMs relying on uncurated data by leveraging a benign SLM.
- Conduct comprehensive experiments on 9 LLMs facing issues of copyright infringement, data poisoning, and privacy violations. Results validate the efficacy of the proposed solution.  
- Show the ensemble allows efficiently adjusting models to meet varying standards on negative effects without changing LLM parameters. It also enables seamless integration with other enhancement methods.
- Analyze in depth the trade-off between standard performance and purification. Find the approach can even boost model performance in some cases.

In summary, the paper presents a practical logits ensemble technique to mitigate multiple negative effects of using uncurated data for training large language models, with theoretical guarantees and empirical verification. The proposed solution is simple, flexible and integration-friendly.
