# [Smaller Language Models are capable of selecting Instruction-Tuning   Training Data for Larger Language Models](https://arxiv.org/abs/2402.10430)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Instruction tuning of large language models (LLMs) on large datasets incurs high training costs. 
- Prior work has shown models tend to learn easier examples first before progressing to more difficult ones. Hard examples are important for good generalization.

Proposed Solution: 
- Introduce a "learning percentage (LP)" metric to measure sample difficulty from the model's perspective. Samples with a high LP at epoch 1 are considered easy, while low LP indicates difficult samples.

- Show smaller LM models (1B to 13B parameters) can use this metric to effectively select challenging subsets of instruction tuning datasets. Models trained on just the hard subsets match or exceed the performance of models trained on full datasets.

Main Contributions:
- Learning percentage (LP) provides an effective automatic measure of sample difficulty from the model's viewpoint. 

- LP hardness transfers across model sizes. Smaller LMs can select effective hard subsets for larger LMs. The quality improves as the smaller LM size increases.

- Introduce a fast approximate LP metric with comparable performance. Demonstrate state-of-the-art models can be trained on very small (1-3%) subsets selected by smaller models and match models trained on full datasets.

In summary, the paper shows smaller LMs can select high-quality training subsets for larger LMs, enabling more efficient instruction tuning. The LP metric is shown to be an effective automatic hardness measure for this task.
