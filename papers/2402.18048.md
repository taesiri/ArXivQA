# [Characterizing Truthfulness in Large Language Model Generations with   Local Intrinsic Dimension](https://arxiv.org/abs/2402.18048)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) tend to generate plausible but untruthful answers (hallucinations). Detecting these hallucinations is crucial for building trust between humans and LLMs.  
- Existing methods rely on uncertainty estimation from model outputs, which is intractable for generative tasks. Other methods seek truthful directions in representations, but these directions are task-specific and unstable.

Proposed Solution: 
- Use the local intrinsic dimension (LID) of LLM representations to characterize truthfulness. LID measures the minimal dimensions needed to represent information.
- Truthful outputs should lie in more structured, lower-dimensional manifolds with smaller LIDs. Hallucinations mix model and human distributions, increasing LID.
- Improve standard LID estimation by: selecting optimal layers/tokens, accommodating representation non-linearity.

Main Contributions:
- Propose using LID of representations to detect hallucinations, which outperforms uncertainty methods by 8% AUROC. More powerful than probing truthful directions.
- Reveal several insights about LLM intrinsic dimensions: LIDs exhibit a "hunchback shape"; Mixing human/model answers increases LID; LIDs grow and correlate with generalization during instruction tuning.
- Overall, demonstrate LID as an effective tool for understanding and detecting hallucinations in LLMs.
