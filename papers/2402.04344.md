# [Does Confidence Calibration Help Conformal Prediction?](https://arxiv.org/abs/2402.04344)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Conformal prediction is an emerging technique for constructing prediction sets that are guaranteed to contain the true label with high probability. Existing methods typically apply temperature scaling to calibrate the classifier, assuming confidence calibration benefits conformal prediction. However, the effect of calibration on conformal prediction has not been thoroughly analyzed. 

Key Insights:
- The paper first empirically shows that post-hoc calibration methods like temperature scaling surprisingly increase the size of prediction sets, despite improving calibration. 
- More surprisingly, over-confident models with small temperature values result in more compact prediction sets, while still maintaining coverage guarantees.
- Theoretically, the paper proves that high confidence reduces the probability of appending new classes to the prediction set. However, an extremely small temperature harms coverage guarantees.

Proposed Solution: 
The paper proposes Conformal Temperature Scaling (ConfTS) to optimize temperature scaling specifically for conformal prediction. ConfTS minimizes the "compactness gap", defined as the gap between the conformity score threshold and score of the ground-truth label. This aligns temperature scaling with the key goal of conformal prediction - compact sets with coverage guarantees.  

Main Contributions:
- Empirically demonstrates that calibration methods increase prediction set sizes, while over-confidence decreases sizes.
- Provides theoretical analysis showing high confidence reduces probability of expanding prediction sets.  
- Proposes ConfTS method to minimize compactness gap and derive optimal temperature for conformal prediction.
- Experiments show ConfTS enhances existing conformal prediction methods like APS and RAPS by reducing set sizes substantially, while maintaining coverage rates.

In summary, the paper convincingly shows that confidence calibration should be applied judiciously for conformal prediction. The proposed ConfTS method effectively improves conformal prediction performance.
