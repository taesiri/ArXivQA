# [Is a PET all you need? A multi-modal study for Alzheimer's disease using   3D CNNs](https://arxiv.org/abs/2207.02094)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that fusing structural MRI (sMRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) does not improve classification accuracy for Alzheimer's disease (AD) compared to using FDG-PET alone. 

The authors note that several previous studies have shown improved AD classification accuracy when combining sMRI and FDG-PET with deep learning models. However, this conflicts with established clinical knowledge that FDG-PET better captures AD-specific pathologies compared to sMRI. 

To test their hypothesis, the authors evaluate single- and multi-modal deep neural networks using sMRI, FDG-PET, and combined sMRI+FDG-PET for binary AD classification. Through several experiments, they demonstrate that FDG-PET alone performs best, matching clinical intuition.

In summary, the central hypothesis is that multi-modal fusion of sMRI and FDG-PET does not improve AD classification accuracy over FDG-PET alone, contradicting some previous deep learning literature but confirming clinical knowledge about the utility of these modalities. The authors test this via systematic experiments with uni- and multi-modal networks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposes a framework for systematically evaluating multi-modal deep neural networks to determine the contribution of each modality. This includes training on correct paired data, as well as on random/mismatched modalities.

- Critically re-evaluates single vs multi-modal deep neural networks for Alzheimer's disease diagnosis using MRI and FDG-PET. Experiments show that FDG-PET alone performs best, conforming with clinical knowledge that it better captures AD patterns. 

- Argues that most prior work on multi-modal fusion for AD is disconnected from clinical knowledge and does not properly evaluate modality contributions.

- Suggests future multi-modal fusion work should follow their evaluation framework to assess modality importance. Also suggests moving beyond binary classification to differential dementia diagnosis where multi-modal fusion aligns more with clinical need.

In summary, the key contribution is a rigorous framework to evaluate multi-modal neural networks, which is applied to question the value of MRI+PET fusion for AD diagnosis specifically. The results conform more with established clinical knowledge and highlight issues in prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper critically re-evaluates single- and multi-modal deep learning models for Alzheimer's disease diagnosis based on MRI and PET imaging, and finds that PET alone performs best, conforming with established clinical knowledge that PET better captures AD pathology compared to MRI.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on multi-modal learning for Alzheimer's disease diagnosis:

- Prior work has often found that combining MRI and PET leads to improved classification accuracy over using a single modality alone. However, this paper finds that PET alone performs just as well as multi-modal approaches for binary Alzheimer's vs healthy classification. Their results align more closely with established clinical knowledge about the utility of PET vs MRI.

- The paper proposes a systematic evaluation framework to rigorously assess the contribution of individual modalities in multi-modal networks. Many prior works have not evaluated this as thoroughly via ablation studies. 

- The paper focuses on binary Alzheimer's vs healthy classification, whereas most prior work also includes a third mild cognitive impairment (MCI) class. Performance on the 3-class problem is significantly lower.

- The paper examines multiple fusion techniques (early, late, middle fusion) whereas some prior works evaluated only one. They find all fusion approaches perform similarly when using the correct paired data.

- The paper uses a fairly simple 3D CNN architecture compared to some recent works utilizing more complex models like Generative Adversarial Networks or transformer networks. Their results suggest a simple CNN is sufficient.

- The paper uses ADNI, a very widely used dataset in the field. Many prior works use the same data source as well for continuity across studies.

Overall, a key distinction of this work is critically re-evaluating the need for multi-modal data in Alzheimer's classification, when PET alone may be sufficient based on medical knowledge. The paper provides useful guidelines for evaluating multi-modal fusion more rigorously in future work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Focus on differential diagnosis of dementia rather than just binary classification of Alzheimer's disease (AD) vs healthy controls. The authors argue that multi-modal fusion is more clinically relevant when trying to distinguish between different types of dementia, rather than just detecting AD.

- Follow their proposed evaluation framework when assessing multi-modal models. The authors recommend systematically evaluating the contribution of each modality and using techniques like ablations and relevance maps to quantify how much each input modality contributes to predictions.

- Use larger datasets and evaluate on multiple datasets. The authors had limited data for the 3-class problem with AD/MCI/controls and suggest evaluating hypotheses on larger datasets.

- Look beyond just improving accuracy by small margins on binary classification tasks. The authors argue the field should focus more on clinically meaningful problems where fusing multimodal data aligns with medical needs.

- Investigate whether fusion approaches make sense for a given application before applying them. The default assumption shouldn't be that multimodal is always better.

- Take into account established medical/clinical knowledge about biomarkers when designing and evaluating models. The authors show the importance of considering the medical context.

- Evaluate different classification models beyond CNNs. The authors plan to test their hypotheses using different model architectures.

In summary, the main suggestions are to align the research more closely with clinical needs, rigorously evaluate the contribution of modalities, use larger datasets, and take into account domain knowledge when designing multimodal fusion models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a study evaluating single- and multi-modal deep neural networks for Alzheimer's disease (AD) diagnosis based on MRI and FDG-PET imaging data. The authors find that contrary to previous work, a single-modality network using only FDG-PET performs best for binary classification of AD patients versus healthy controls. This aligns with established clinical knowledge that FDG-PET better captures AD-specific neurodegeneration compared to MRI. The paper proposes a framework to systematically evaluate the contribution of individual modalities in multi-modal models, and shows that MRI adds little information beyond what is provided by FDG-PET alone. The authors argue future work should follow their framework to assess modality importance, and focus on multi-modal fusion that conforms to clinical need like differential diagnosis of dementia subtypes rather than just AD detection.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper evaluates single- and multi-modal deep learning models for Alzheimer's disease (AD) diagnosis based on structural MRI and FDG-PET imaging. Previous work has shown that combining MRI and PET improves classification accuracy for AD vs healthy controls. However, this conflicts with established clinical knowledge that PET better captures AD specific patterns than MRI. 

In this work, the authors systematically evaluate the contribution of MRI and PET using different fusion strategies - early, middle, and late fusion. Their experiments show that PET alone performs best for AD vs control classification, agreeing with clinical knowledge. The authors argue that future work on multi-modal fusion for AD diagnosis should follow their evaluation framework to assess modality contributions. They encourage the community to focus on differential dementia diagnosis rather than just AD vs control, where multi-modal fusion is more clinically relevant. Overall, this work demonstrates the importance of grounding deep learning research in established clinical knowledge.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a framework for evaluating single- and multi-modal deep neural networks (DNNs) for Alzheimer's disease (AD) diagnosis based on fluorodeoxyglucose positron emission tomography (FDG-PET) and structural magnetic resonance imaging (sMRI). The authors consider three fusion strategies: early fusion (combining raw modalities before input to the network), late fusion (concatenating latent representations from separate networks), and middle fusion (exchanging intermediate feature maps between networks). To assess the contribution of each modality, they perform experiments by pairing correct/random sMRI and FDG-PET at test time. They also visualize relevance maps to quantify the contribution of each modality to the final prediction. Their experiments demonstrate that a single-modality FDG-PET network performs best, conforming with clinical knowledge that FDG-PET better captures AD patterns. The authors argue that future multi-modal fusion work should systematically evaluate individual modalities using their proposed framework rather than focusing solely on improving accuracy.
