# [Attacking Perceptual Similarity Metrics](https://arxiv.org/abs/2305.08840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: Are perceptual similarity metrics robust against imperceptible adversarial perturbations?The authors investigate whether it is possible to craft imperceptible adversarial perturbations that can fool perceptual similarity metrics and cause them to incorrectly judge the similarity between images. The key hypothesis appears to be that current perceptual similarity metrics, even recent learned ones that correlate well with human judgments, are susceptible to adversarial attacks with small imperceptible perturbations that can flip the metrics' similarity rankings.The authors systematically test this hypothesis by attacking a variety of perceptual similarity metrics, both traditional and learned ones, using different white-box and black-box adversarial attack techniques. Their experiments demonstrate that all the tested metrics are vulnerable to having their rankings overturned by such adversarial perturbations.So in summary, the central research question is about the robustness of perceptual similarity metrics against adversarial attacks, with the key hypothesis being that current metrics lack robustness even to small imperceptible perturbations. The paper presents a systematic investigation of this hypothesis through adversarial attacks on various metrics.


## What is the main contribution of this paper?

 The main contribution of this paper is systematically investigating whether existing perceptual similarity metrics are susceptible to adversarial attacks. The key findings are:- Both traditional quality metrics (e.g. SSIM) and recent deep learning based metrics (e.g. LPIPS) are vulnerable to common white-box attacks like FGSM and PGD. Even a black-box attack like the One-pixel attack can successfully fool these metrics. - The authors craft adversarial examples using the spatial attack stAdv on LPIPS, which can transfer and attack other metrics in a black-box setting. Combining stAdv with PGD further improves the transferability.- A benchmark is provided to evaluate the robustness of various similarity metrics against the stAdv attack and stAdv+PGD attack. The results show all metrics are prone to these attacks to some degree. Although learned metrics correlate better with human judgment, traditional metrics can be more robust against adversarial perturbations.In summary, this is the first comprehensive study investigating and demonstrating the susceptibility of perceptual similarity metrics to adversarial attacks. The benchmark and analysis provide a useful starting point for future research on designing more robust similarity metrics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:The paper systematically examines the robustness of perceptual similarity metrics to imperceptible adversarial perturbations and finds that both traditional and learned metrics are susceptible to such attacks.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on attacking perceptual similarity metrics:- Focus on imperceptible adversarial perturbations: This paper focuses specifically on adversarial attacks that are imperceptible to humans, as judged by perceptual similarity metrics. Most prior work has examined geometric distortions like translation, scaling, etc. that are more noticeable. The emphasis on truly imperceptible perturbations is novel.- Systematic evaluation of attack methods: The paper systematically tests a variety of white-box and black-box attack techniques from the adversarial ML literature and examines their effectiveness against perceptual similarity metrics. Prior work has tended to focus on just one or two attack methods. The broad evaluation across methods is more thorough.- Exploration of transferable attacks: A key contribution is showing that adversarial examples crafted to fool one metric can transfer to others in a black-box manner. This is an important finding regarding the generalizability of adversarial vulnerabilities in this space. - Benchmarking many metrics: The paper benchmarks a wide range of recent perceptual similarity metrics using the attacks. Most prior work has only examined one or two metrics. Testing such a broad set makes the robustness comparisons more meaningful.- Combining spatial and L-inf attacks: The paper shows that combining a spatial attack like stAdv with an L-inf attack like PGD can increase transferability. The synergistic effect between spatial and pixel perturbations is a novel finding.Overall, the systematic evaluation of imperceptible adversarial attacks against a large set of perceptual similarity metrics differentiates this work from prior research and provides useful insights into the robustness of these metrics to different threat models. The findings motivate the need for developing more robust perceptual similarity metrics.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing new perceptual similarity metrics that are more robust to adversarial attacks. The authors demonstrate that current metrics, even recent learned ones, are susceptible to adversarial perturbations. They suggest further research to create more robust similarity metrics.- Exploring defenses against adversarial attacks on perceptual similarity metrics. The authors point out the vulnerability exposed by their work could allow malicious uses. They recommend studying defenses or mitigation strategies against such adversarial threats.- Expanding the analysis to additional datasets and distortion types. The authors test their attacks on a limited set of datasets and distortion types. They suggest applying a similar analysis on more diverse data to further test robustness. - Considering the trade-off between accuracy and robustness. The authors find more accurate learned metrics are often less robust than traditional metrics. They propose studying this trade-off further when developing new metrics.- Investigating connections to adversarial robustness of other vision systems. The authors discuss relation of their work to recent perceptual threat models. They recommend exploring how robust similarity metrics could enable stronger defenses more broadly.- Developing better perceptual models to evaluate adversarial perturbations. The authors highlight recent work suggesting better perceptual models allow stronger adversarial robustness guarantees. They propose this as an area for further research.In summary, the main future directions focus on developing more robust similarity metrics, studying defenses against such adversarial attacks, expanding the analysis to more diverse data, and investigating connections of this work to broader adversarial robustness.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:The paper investigates the robustness of perceptual similarity metrics to imperceptible adversarial perturbations. It shows that both traditional quality metrics and recent deep learning-based metrics are susceptible to common white-box attacks like FGSM and PGD as well as black-box attacks like the One-pixel attack, which can flip the metrics' judgment on perceptual similarity. The authors generate adversarial examples by attacking the widely used LPIPS metric with the spatial attack stAdv and combine it with PGD to increase transferability. They use these transferable adversarial examples to benchmark the robustness of several similarity metrics and find that learned metrics correlate better with humans but are generally less robust than traditional metrics. Overall, the study demonstrates that existing perceptual similarity metrics lack robustness against adversarial perturbations and points to the need for more research on this topic.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper investigates the robustness of perceptual similarity metrics to imperceptible adversarial perturbations. The authors systematically test a variety of traditional and learned similarity metrics using different adversarial attack methods. They show that all the tested metrics are susceptible to being fooled by small adversarial perturbations that are imperceptible to humans. The authors first demonstrate the vulnerability of similarity metrics to white-box attacks like FGSM and PGD, as well as the black-box One-Pixel attack. They then generate transferable adversarial examples by attacking the widely used LPIPS metric with the spatial attack stAdv. They show these examples transfer to other similarity metrics in a black-box setting. By combining stAdv perturbations with PGD, they further increase transferability across metrics. Their benchmark reveals that learned metrics, though more accurate, are generally less robust than traditional metrics. The study indicates the need for developing more robust perceptual similarity metrics, which could in turn lead to stronger adversarial defenses.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a systematic approach to evaluate the robustness of perceptual similarity metrics against imperceptible adversarial perturbations. The key method is as follows:The authors first select a dataset containing triplets of a reference image and two distorted versions, along with human judgments on which distorted image is more similar to the reference. To attack a metric, they select the distorted image that humans judge as more similar to the reference, and add imperceptible adversarial noise to make the metric wrongly judge it as less similar than the other distorted image. The adversarial noise is crafted using methods like FGSM, PGD, One-pixel attack, and stAdv spatial attack. The stAdv attack on LPIPS is then transferred to other metrics in a black-box setting. The stAdv attack is also combined with PGD to enhance transferability. Through extensive experiments, the authors demonstrate that both traditional and learned similarity metrics are susceptible to such adversarial attacks, showcasing the need for more robust similarity metrics.


## What problem or question is the paper addressing?

 The paper is addressing the question of how robust perceptual similarity metrics are against imperceptible adversarial perturbations. The key points are:- Perceptual similarity metrics are increasingly used to evaluate image quality and optimize image processing models. However, it's unclear if small adversarial perturbations could fool these metrics.- The authors systematically test a variety of similarity metrics against different adversarial attacks that make imperceptible changes to images. - They find that all the tested metrics can be fooled by these attacks - their similarity judgments are changed even though the perturbations are invisible to humans.- The authors use white-box and black-box attack methods, as well as spatially transforming attacks, to fool the metrics. Combining attack types increases transferability.- Their results indicate current perceptual similarity metrics are not robust against these small perturbations. The authors suggest this is an important open problem for further research to develop more robust metrics.In summary, the paper examines the adversarial robustness of perceptual similarity metrics, which has not been well studied before, and shows even recent learned metrics are susceptible to imperceptible adversarial perturbations. The results highlight the need for more robust similarity metrics aligned with human perception.
