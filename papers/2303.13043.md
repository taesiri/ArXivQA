# [Top-Down Visual Attention from Analysis by Synthesis](https://arxiv.org/abs/2303.13043)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can a visual perception system achieve task-guided top-down attention through the framework of Analysis by Synthesis?

The key hypotheses are:

- Top-down visual attention arises naturally from optimizing a sparse reconstruction objective modulated by a high-level prior in an Analysis by Synthesis system. 

- A vision system that variationally approximates Analysis by Synthesis with a prior-modulated objective will be able to achieve controllable top-down attention.

In summary, the paper proposes that top-down attention can be achieved by incorporating a high-level prior into a visual analysis system based on the Analysis by Synthesis framework. The central hypothesis is that this will allow task-specific guidance of attention. The authors test this hypothesis by building a vision transformer model called AbSViT that approximates Analysis by Synthesis and demonstrate its controllable top-down attention capabilities.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Analysis-by-Synthesis Vision Transformer (AbSViT), a novel vision transformer model with top-down modulated attention that is inspired by the Analysis by Synthesis (AbS) theory of human perception. Specifically:

- The paper shows that AbS, which explains top-down attention in human vision, is equivalent to sparse reconstruction with an additional top-down modulation signal. This provides a computational perspective connecting AbS and top-down attention.

- Based on the connection, the authors propose AbSViT, a vision transformer model with a variational approximation to AbS. AbSViT contains a feedforward encoding path and a feedback decoding path. The feedback tokens are modulated by a prior and fed back to the self-attention layers to achieve top-down attention.

- Experiments show AbSViT allows controllable top-down attention given different priors on multi-object images. When applied to vision-language tasks where language acts as the prior, AbSViT consistently outperforms baselines like ViT. It also serves as a better backbone for image classification, robustness, and semantic segmentation.

In summary, the key contribution is proposing AbSViT, a vision transformer that achieves top-down attention inspired by the AbS theory, and showing its effectiveness on both vision-language tasks and general vision tasks compared to bottom-up models like ViT.
