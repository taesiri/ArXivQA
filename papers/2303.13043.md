# [Top-Down Visual Attention from Analysis by Synthesis](https://arxiv.org/abs/2303.13043)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can a visual perception system achieve task-guided top-down attention through the framework of Analysis by Synthesis?

The key hypotheses are:

- Top-down visual attention arises naturally from optimizing a sparse reconstruction objective modulated by a high-level prior in an Analysis by Synthesis system. 

- A vision system that variationally approximates Analysis by Synthesis with a prior-modulated objective will be able to achieve controllable top-down attention.

In summary, the paper proposes that top-down attention can be achieved by incorporating a high-level prior into a visual analysis system based on the Analysis by Synthesis framework. The central hypothesis is that this will allow task-specific guidance of attention. The authors test this hypothesis by building a vision transformer model called AbSViT that approximates Analysis by Synthesis and demonstrate its controllable top-down attention capabilities.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Analysis-by-Synthesis Vision Transformer (AbSViT), a novel vision transformer model with top-down modulated attention that is inspired by the Analysis by Synthesis (AbS) theory of human perception. Specifically:

- The paper shows that AbS, which explains top-down attention in human vision, is equivalent to sparse reconstruction with an additional top-down modulation signal. This provides a computational perspective connecting AbS and top-down attention.

- Based on the connection, the authors propose AbSViT, a vision transformer model with a variational approximation to AbS. AbSViT contains a feedforward encoding path and a feedback decoding path. The feedback tokens are modulated by a prior and fed back to the self-attention layers to achieve top-down attention.

- Experiments show AbSViT allows controllable top-down attention given different priors on multi-object images. When applied to vision-language tasks where language acts as the prior, AbSViT consistently outperforms baselines like ViT. It also serves as a better backbone for image classification, robustness, and semantic segmentation.

In summary, the key contribution is proposing AbSViT, a vision transformer that achieves top-down attention inspired by the AbS theory, and showing its effectiveness on both vision-language tasks and general vision tasks compared to bottom-up models like ViT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes Analysis-by-Synthesis Vision Transformer (AbSViT), a ViT model with a variational approximation of analysis by synthesis that achieves controllable top-down attention and improves performance on vision-language tasks as well as image classification, segmentation, and robustness.


## How does this paper compare to other research in the same field?

 Here are some key points about how this paper compares to other related work:

- This paper presents a new perspective on top-down visual attention, explaining it through the lens of Analysis by Synthesis (AbS). Prior work has proposed AbS as a potential model for top-down attention, but existing explanations are mainly conceptual. This work provides a more concrete derivation showing how AbS leads to top-down attention.

- The paper proposes a new model, AbSViT, that implements top-down attention by approximating AbS in a Vision Transformer. This provides a principled way to incorporate top-down attention in ViT models. Prior work on top-down attention uses different formulations not based on AbS (e.g. spatial masking) and are incompatible with transformer architectures.

- The AbSViT model achieves strong results on vision-language tasks like VQA and image retrieval, where top-down signals like language questions can guide attention. This demonstrates the advantage of trainable top-down attention over standard bottom-up attention in ViTs for tasks requiring focusing on task-relevant objects.

- Beyond vision-language tasks, AbSViT also improves image classification, robustness, and segmentation, showing it can serve as a general backbone. Prior work on top-down attention focuses more narrowly on vision-language tasks. The improvements indicate the object-centric representations from top-down attention are useful more broadly.

- The paper ablates design choices of AbSViT, like adding top-down signals only to values in self-attention, and using a variational AbS loss. This provides justification for the architecture derived from the AbS interpretation.

In summary, this work makes conceptual and modeling contributions for top-down attention, and shows strong empirical results. The AbS perspective and AbSViT model meaningfully advance research on task-driven attention mechanisms in vision transformers.
