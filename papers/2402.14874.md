# [Distillation Contrastive Decoding: Improving LLMs Reasoning with   Contrastive Decoding and Distillation](https://arxiv.org/abs/2402.14874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Contrastive Decoding (CD) is an effective method for enhancing reasoning capabilities of large language models (LLMs), but has two main limitations: 
1) It requires an "amateur" model from the same model family in addition to the "expert" model, increasing computational demands. 
2) Appropriate smaller amateur models are not always available.

Proposed Solution - Distillation Contrastive Decoding (DCD):
- Combines ideas from Contrastive Decoding (CD) and Contrastive Chain-of-thought Prompting (CP).  
- Uses distillation techniques like dropout and quantization to simulate a weaker "amateur" model instead of requiring a separate smaller model.
- Employs valid chain-of-thought examples for expert model and invalid examples for distilled amateur model. 

Main Contributions:
- Introduces DCD to enhance LLM reasoning without needing extra smaller models, reducing memory usage.  
- Shows DCD outperforms CD by 1.89% on GSM8K and 3.53% on StrategyQA using Llama2 models.
- Demonstrates high adaptability of DCD to stronger models by improved GSM8K accuracy of +6.8% on Mistral-7B.
- Finds optimal amateur information comes from moderate dropout rates, not very low capacity models.
- Establishes correlation between base model knowledge and DCD improvement.

In summary, the paper proposes Distillation Contrastive Decoding to overcome limitations of Contrastive Decoding by eliminating the need for separate amateur models and reducing memory requirements, while improving reasoning performance across multiple benchmark tasks.
