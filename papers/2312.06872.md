# [ELSA: Partial Weight Freezing for Overhead-Free Sparse Network   Deployment](https://arxiv.org/abs/2312.06872)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deploying deep neural networks on devices with limited compute/memory is challenging as it requires choosing an appropriate network size. The standard approach of training multiple networks of different sparsities has drawbacks: it requires larger app size or network downloads.

Proposed Solution: 
- The paper proposes ELSA (Efficient Layer Sparsification Approach) which can embed one or more sparse networks within a single dense network. At prediction time, sparse networks of different sparsity levels can be extracted effortlessly from the dense network.

Key Ideas:
- Train an initial dense neural network. Sparsify this network using any sparsification technique while freezing weights of the sparse subnetwork. Then re-densify the network by training only non-frozen weights. This embeds a sparse network within the dense network.
- This process can be iterated multiple times to embed sparse networks of different sparsity levels within the same dense network. 
- At prediction time, sparse networks can be extracted by simply zeroing out non-frozen weights according to predefined masks. No fine-tuning needed.

Main Benefits:
- Allows easy deployment of multiple sparse networks nested within a single dense network. Reduces app size and eliminates need for network downloads.
- places no restrictions on network architecture, loss functions or optimization techniques used. Fully flexible and modular.
- Experiments show embedded sparse networks have negligible loss in accuracy compared to separately trained sparse networks.

Key Contributions:
- Novel idea of nesting sparse networks within dense networks for easy deployment
- Iterative multi-level scheme to embed networks of different sparsities 
- Overhead-free approach by encoding sparsity masks within network weights
- Empirical validation of the approach on ImageNet, CIFAR datasets.
