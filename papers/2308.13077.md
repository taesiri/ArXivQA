# [Preserving Modality Structure Improves Multi-Modal Learning](https://arxiv.org/abs/2308.13077)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we learn effective joint multi-modal embeddings that generalize well to unseen data, particularly in the presence of noisy/weak supervision and misalignment between modalities?The key points seem to be:- Current multi-modal contrastive learning methods struggle to generalize on out-of-domain data. They focus on strict alignment between modalities while ignoring semantic relationships between samples within a modality.- The authors hypothesize that preserving the modality-specific semantic structure in the joint embedding space will improve generalizability. - To achieve this, they propose a semantic-structure-preserving consistency (SSPC) approach that uses global anchors to represent semantic relationships between samples. - They introduce a novel Multi-Assignment Sinkhorn-Knopp algorithm to assign multiple anchors per sample to flexibly model relationships.- The consistency between anchor assignments in the input vs joint spaces is enforced via a SSPC loss to retain modality-specific structure.- Experiments show state-of-the-art performance on both in-domain and out-of-domain datasets, supporting the hypothesis that preserving semantic structure improves generalizability.In summary, the key research question is how to learn multi-modal embeddings that generalize better, with a proposed solution of using anchors and consistency loss to preserve within-modality semantic structure.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel approach for multi-modal self-supervised learning that preserves the modality-specific semantic structure between samples to improve generalizability. Specifically, the key ideas proposed are:- A semantic-structure-preserving consistency (SSPC) loss to retain only useful information from the modality-specific features that is beneficial for both cross-modal learning and preserving semantic structure.- Modeling the relationship between samples using multiple learned anchors, where the anchor assignments capture both shared and unique aspects between samples. This provides a flexible way to model sample relationships.- A new Multi-Assignment Sinkhorn-Knopp (Multi-SK) algorithm to enable multiple anchor assignments per sample by optimizing a many-to-many assignment problem.- Enforcing consistency between anchor assignments in the input vs joint embedding spaces using the SSPC loss to preserve modality-specific semantic structure.The proposed approach is evaluated on multi-modal retrieval and classification tasks on MSR-VTT, YouCook2 and other datasets. Results show improved generalizability and state-of-the-art performance on both in-domain and out-of-domain datasets compared to prior arts.In summary, the key novelty is in preserving semantic structure to improve multi-modal learning, using flexible modeling of sample relationships with multiple learned anchors, and a new algorithm for multiple anchor assignment. The approach demonstrates improved generalizability and SOTA results.
