# [Boosting Weakly-Supervised Temporal Action Localization with Text   Information](https://arxiv.org/abs/2305.00607)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: 

How to leverage text information in action category labels to boost weakly-supervised temporal action localization (WTAL) and obtain more accurate localization results?

The key points are:

- Current WTAL methods only use video information for training classifiers, while ignoring the text information in action category labels. 

- This paper proposes to leverage text information in two aspects: 1) Text-Segment Mining (TSM) for discriminative objective to enlarge inter-class differences; 2) Video-Text Language Completion (VLC) for generative objective to enhance intra-class integrity.

- TSM treats text queries as experts to match related segments while filtering irrelevant ones. VLC focuses on related segments to complete masked text. 

- Combining TSM and VLC with a self-supervised constraint, the method achieves new state-of-the-art results on THUMOS14 and ActivityNet datasets.

- The proposed method can also improve existing WTAL methods by integrating TSM and VLC objectives.

In summary, the core hypothesis is that leveraging text information in labels can boost weakly-supervised temporal action localization, which is validated through the proposed TSM and VLC objectives as well as experiments.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. The authors propose a new framework to leverage text information to boost weakly-supervised temporal action localization (WTAL) from two aspects: 

(a) Text-segment mining (TSM) provides a discriminative objective to enlarge inter-class differences and reduce over-complete localization. 

(b) Video-text language completion (VLC) provides a generative objective to enhance intra-class integrity and find more complete temporal boundaries.

2. The authors design TSM to use action label texts as queries to mine related video segments. VLC focuses on completing masked action words in sentences using comprehensive video segments. 

3. The authors combine TSM and VLC via a self-supervised constraint and achieve state-of-the-art performance on THUMOS14 and ActivityNet datasets.

4. The authors show the proposed TSM and VLC can be integrated into existing methods and boost their performance.

In summary, the key contribution is using text information in two ways - discriminative mining and generative completion - to improve weakly-supervised temporal action localization, through a novel framework consisting of TSM and VLC objectives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel framework to leverage text information from action class labels to boost weakly-supervised temporal action localization (WTAL) through two objectives - text-segment mining to discriminate between action classes and video-text language completion to enhance intra-class integrity, achieving state-of-the-art performance on THUMOS14 and ActivityNet datasets.
