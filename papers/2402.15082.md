# [PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables   Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2402.15082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) achieve great success when fine-tuned on downstream tasks, but storing a separate copy of the full model for each task is expensive. 
- Existing parameter-efficient methods are limited in capacity for knowledge transfer, do not leverage knowledge from multiple source tasks, and do not utilize task correlations.

Proposed Solution:
- The paper proposes MoME, a novel parameter-efficient multi-task transfer learning framework.
- It has two stages - source task training to acquire task representations and experts, and target task adaptation to transfer knowledge guided by task correlations.

Key Points:
- Task prompts are initialized using task descriptions and trained on source tasks to get task representations. Parallel expert modules are added to transformer layers to store task knowledge.
- For target adaptation, an attention module constructs correlation features between source and target task prompts. A MoE gate incorporating these features selects relevant source experts.
- A target task adapter further enhances expert modules to directly acquire target task knowledge. An expert balanced loss ensures priority to more relevant experts.

Main Contributions:
- Introduction of the two-stage MoME framework for parameter-efficient multi-task transfer.
- Task descriptions for consistent prompt initialization and task correlation features to guide expert selection.
- Significantly and consistently outperforms prior state-of-the-art across multiple datasets and tasks. 
- Analysis shows target tasks tend to select the most relevant source expert, demonstrating efficiency.

In summary, MoME provides an effective approach for parameter-efficient multi-task knowledge transfer in PLMs, guided by task correlations. The two-stage training extracts and migrates the most valuable knowledge across tasks.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes MoME, a novel parameter-efficient multi-task transfer learning framework that introduces mixture-of-experts structures to sufficiently acquire and store task-specific knowledge in feed-forward networks and leverages learned task correlation features to guide the transfer of pertinent knowledge from multiple source tasks to a target task.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Introduction of MoME: Proposing MoME, a two-stage parameter-efficient, multi-task transfer learning framework with a Mixture-of-Experts (MoE) approach. In the first stage, it employs multiple source task training to acquire task-specific knowledge. In the second stage, it transfers knowledge to the target task guided by task correlations.

2) Consistent Outperformance: MoME consistently outperforms full fine-tuning and other baselines across a spectrum of tasks, demonstrating its robustness and adaptability. 

3) Analytical Experiments: Additional experiments analyze the distribution of weights assigned to different task experts, revealing MoME's tendency to incorporate knowledge from the most relevant source task expert during target task adaptation. This provides insights into the framework's efficiency and scalability.

In summary, the main contribution is proposing the novel MoME framework for parameter-efficient multi-task transfer learning, which leverages task correlations to guide knowledge transfer and consistently achieves state-of-the-art performance. The additional experiments also provide useful analysis into how the framework works.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Multi-task transfer learning: Transferring knowledge from multiple source tasks to a target task in a parameter efficient way.

- Mixture-of-experts (MoE): Using a mixture of experts approach to sufficiently acquire and store multi-task knowledge in feedforward networks. 

- Task correlation features: Constructing correlation features between tasks to guide the model to transfer valuable and pertinent knowledge between tasks.

- Two-stage framework: A two stage framework of (1) multiple source task training and (2) task correlation guided target task adaptation. 

- Task descriptions: Using task descriptions to initialize prompt vectors in order to capture consistent task representations.

- Expert modules: Introducing trainable expert modules parallel to feedforward networks to acquire task-specific knowledge.

- Correlation attention: An attention mechanism to construct correlation features between source and target task prompts.

- Mixture-of-task-correlation (MoT) gate: A gating mechanism to transfer knowledge from source to target tasks based on task correlations.

In summary, the key ideas focus on efficiently transferring knowledge between multiple source tasks and a target task using task correlation features and mixture-of-experts approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a two-stage framework involving source task training and target task adaptation. What is the motivation behind splitting the process into two stages? How does it help with transferring knowledge between tasks?

2. The paper utilizes task descriptions to initialize prompts in both source and target tasks. What is the purpose of this initialization? How does it help maintain consistency between task representations?

3. The paper employs a Mixture-of-Experts (MoE) approach by inserting expert modules in parallel with the feedforward networks. Why is knowledge stored in the FFNs and how do the experts facilitate knowledge acquisition from each task?  

4. What are the differences between the expert modules used in this work compared to previous approaches like MixDA? How does training them on labelled source task data improve performance?

5. Explain the working of the Task Correlation Attention module. How does it help construct correlation features between source and target tasks? What is the impact of using these features?

6. How does the Mixture-of-Task-Correlation (MoT) gate assign weights to the different experts? What is the purpose of the expert balanced loss and how does it improve knowledge transfer?

7. Why does the method utilize task-specific adapters along with the expert modules? When do these adapters directly acquire knowledge from the target task?  

8. The ablation studies analyze the impact of different components like task descriptions, correlation features, and MoE experts. Summarize the key findings and insights gained.

9. The analysis of weight distributions provides interesting insights into task correlations. What trends were observed between tasks of similar categories/formats? How does this explain the efficiency of knowledge transfer?

10. What are some limitations of the proposed approach? How can the issues of increased latency and two-stage training process be potentially addressed in future work?
