# [Depthwise Convolution is All You Need for Learning Multiple Visual   Domains](https://arxiv.org/abs/1902.00927)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we build a single neural network that can deal with images across different visual domains? The paper proposes a multi-domain learning approach to address this question. The key ideas and hypotheses are:- Images from different visual domains may share some universal structure that can be captured via a common parameterization in a neural network model.- Depthwise separable convolution can be used to exploit the structural regularity hidden in different domains. - Images across domains share cross-channel correlations but have domain-specific spatial correlations.- Sharing the pointwise convolution (which captures cross-channel correlations) while having domain-specific depthwise convolution (which captures spatial correlations) can lead to an effective multi-domain learning model.So in summary, the central hypothesis is that a compact and extensible multi-domain learning model can be developed based on depthwise separable convolutions, by sharing pointwise convolution across domains while keeping depthwise convolution domain-specific. The paper aims to validate this hypothesis through experiments on the Visual Decathlon benchmark.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel multi-domain learning approach using depthwise separable convolutions. Specifically:- They propose a compact network architecture for multi-domain learning based on depthwise separable convolutions. This factorization of standard convolutions allows sharing of pointwise convolutions across domains while learning domain-specific depthwise convolutions. - They validate the assumption that images across domains share cross-channel correlations (captured by pointwise convolutions) while having domain-specific spatial correlations (captured by depthwise convolutions).- They achieve state-of-the-art results on the Visual Decathlon benchmark using only 50% of the parameters compared to previous methods. The proposed approach obtains the highest accuracy while requiring the least amount of parameters.- They provide analysis and visualization of the concepts learned by depthwise and pointwise convolutions using network dissection. This reveals that depthwise convolutions capture more semantic concepts compared to pointwise convolutions.- They also explore soft-sharing of depthwise convolutions across domains using a gating mechanism, though this provides smaller improvements.In summary, the key contribution is a very parameter-efficient multi-domain learning approach that leverages the different roles of depthwise and pointwise convolutions. The compact architecture and strong empirical results on Visual Decathlon highlight its effectiveness.
