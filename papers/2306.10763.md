# [Guiding Language Models of Code with Global Context using Monitors](https://arxiv.org/abs/2306.10763)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is:How can we guide language models to generate code that is consistent with global context from the code repository, without requiring architectural changes or additional training?The key ideas and contributions are:- Proposing the notion of "monitors" as an interface between language models and static analysis tools. The monitor watches the code generated by the LM and invokes static analysis to get suggestions from global context. - Instantiating this approach for the task of generating type-consistent identifier names after a dereference operation, using a type-based static analysis.- Introducing a new dataset called PragmaticCode of Java projects with dependencies for evaluating approaches that leverage global context.- Empirically demonstrating that monitor-guided decoding consistently improves metrics like compilation rate, next-token accuracy, and match with ground truth across models of varying scale.- Showing that smaller LMs combined with monitor-guided decoding can outperform larger LMs, opening up possibilities like using smaller on-device models.- Analyzing the effect of identifier complexity, prompt augmentation techniques, and decoding strategies like FIM when combined with monitor-guided decoding.In summary, the central hypothesis is that monitor-guided decoding can effectively leverage global context from static analysis to guide language models to generate more accurate, compilable code, without any architectural changes or retraining. The results on multiple models and setups validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing the idea of using monitors as an interface between language models (LMs) and static analysis to guide code generation. The monitor watches the LM generating code, invokes static analysis in the background when certain triggers occur, and uses the information from static analysis to reshape the logits/probabilities produced by the LM to guide it towards more semantically valid code. 2. Introducing a new dataset called PragmaticCode of real-world Java projects with complete build environments and dependencies. This allows evaluating code generation approaches like the proposed monitor-guided decoding that rely on global context from the whole codebase/dependencies.3. Instantiating monitor-guided decoding for generating type-consistent identifiers in Java code by triggering the monitor on object dereferences and using a type-based static analysis to provide suggestions.4. Evaluating various scale LMs with and without monitor-guided decoding and showing it improves metrics like compilation rate, match with ground truth identifiers, etc. The key findings are:- Monitor-guided decoding consistently improves all scale LMs compared to their base versions without monitors.- Smaller LMs with monitors can outperform larger base LMs on metrics like compilation rate.- Monitor-guided decoding combines well with other techniques like prompt engineering and decoding strategies.So in summary, the main contribution is proposing monitors as an interface to integrate static analysis with language models to guide code generation, along with an instantiation for type-consistent identifier generation and an evaluation showing its benefits. The PragmaticCode dataset and analyses are also an important contribution to facilitate research in pragmatic code generation.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares and contributes to related work:- The key idea of using monitors and static analysis to provide global context and guide language models of code is novel. Most prior work has focused on augmenting the prompt or modifying the model architecture and training objectives. This paper introduces a complementary technique of applying output constraints during inference.- The paper makes excellent use of mature static analysis tools like those found in IDEs to assist language models. Leveraging decades of tooling designed to help human developers is an impactful insight.- The proposed approach does not require retraining or finetuning the language model. Many related techniques require additional training which can be expensive and time-consuming. This makes the monitor-guided decoding more practical.- The paper contributes two new datasets for evaluating code generation models in a realistic setting - \dataset and \evaldataset. Many benchmarks use standalone functions, whereas these datasets are based on real repositories with dependencies.- The results demonstrate consistent and sizable improvements in compilation rates and match with ground truth across models and scale. An interesting finding is that smaller models with monitor guidance can outperform much larger models without it.- Monitor-guided decoding is shown to combine well with other techniques like prompt augmentation and fill-in-the-middle decoding. The gains are complementary showing the approach is broadly applicable.Overall, I feel this paper makes a significant contribution over related work by introducing a practical and effective technique for improving language models of code using mature static analysis tools. The gains are achieved with minimal changes to existing models. The datasets and empirical methodology are also impactful for future research on code generation in realistic settings.
