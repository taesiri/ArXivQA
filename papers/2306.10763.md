# [Guiding Language Models of Code with Global Context using Monitors](https://arxiv.org/abs/2306.10763)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is:How can we guide language models to generate code that is consistent with global context from the code repository, without requiring architectural changes or additional training?The key ideas and contributions are:- Proposing the notion of "monitors" as an interface between language models and static analysis tools. The monitor watches the code generated by the LM and invokes static analysis to get suggestions from global context. - Instantiating this approach for the task of generating type-consistent identifier names after a dereference operation, using a type-based static analysis.- Introducing a new dataset called PragmaticCode of Java projects with dependencies for evaluating approaches that leverage global context.- Empirically demonstrating that monitor-guided decoding consistently improves metrics like compilation rate, next-token accuracy, and match with ground truth across models of varying scale.- Showing that smaller LMs combined with monitor-guided decoding can outperform larger LMs, opening up possibilities like using smaller on-device models.- Analyzing the effect of identifier complexity, prompt augmentation techniques, and decoding strategies like FIM when combined with monitor-guided decoding.In summary, the central hypothesis is that monitor-guided decoding can effectively leverage global context from static analysis to guide language models to generate more accurate, compilable code, without any architectural changes or retraining. The results on multiple models and setups validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing the idea of using monitors as an interface between language models (LMs) and static analysis to guide code generation. The monitor watches the LM generating code, invokes static analysis in the background when certain triggers occur, and uses the information from static analysis to reshape the logits/probabilities produced by the LM to guide it towards more semantically valid code. 2. Introducing a new dataset called PragmaticCode of real-world Java projects with complete build environments and dependencies. This allows evaluating code generation approaches like the proposed monitor-guided decoding that rely on global context from the whole codebase/dependencies.3. Instantiating monitor-guided decoding for generating type-consistent identifiers in Java code by triggering the monitor on object dereferences and using a type-based static analysis to provide suggestions.4. Evaluating various scale LMs with and without monitor-guided decoding and showing it improves metrics like compilation rate, match with ground truth identifiers, etc. The key findings are:- Monitor-guided decoding consistently improves all scale LMs compared to their base versions without monitors.- Smaller LMs with monitors can outperform larger base LMs on metrics like compilation rate.- Monitor-guided decoding combines well with other techniques like prompt engineering and decoding strategies.So in summary, the main contribution is proposing monitors as an interface to integrate static analysis with language models to guide code generation, along with an instantiation for type-consistent identifier generation and an evaluation showing its benefits. The PragmaticCode dataset and analyses are also an important contribution to facilitate research in pragmatic code generation.
