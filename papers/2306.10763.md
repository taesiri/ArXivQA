# [Guiding Language Models of Code with Global Context using Monitors](https://arxiv.org/abs/2306.10763)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is:How can we guide language models to generate code that is consistent with global context from the code repository, without requiring architectural changes or additional training?The key ideas and contributions are:- Proposing the notion of "monitors" as an interface between language models and static analysis tools. The monitor watches the code generated by the LM and invokes static analysis to get suggestions from global context. - Instantiating this approach for the task of generating type-consistent identifier names after a dereference operation, using a type-based static analysis.- Introducing a new dataset called PragmaticCode of Java projects with dependencies for evaluating approaches that leverage global context.- Empirically demonstrating that monitor-guided decoding consistently improves metrics like compilation rate, next-token accuracy, and match with ground truth across models of varying scale.- Showing that smaller LMs combined with monitor-guided decoding can outperform larger LMs, opening up possibilities like using smaller on-device models.- Analyzing the effect of identifier complexity, prompt augmentation techniques, and decoding strategies like FIM when combined with monitor-guided decoding.In summary, the central hypothesis is that monitor-guided decoding can effectively leverage global context from static analysis to guide language models to generate more accurate, compilable code, without any architectural changes or retraining. The results on multiple models and setups validate this hypothesis.
