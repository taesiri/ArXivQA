# [Extracting Class Activation Maps from Non-Discriminative Features as   well](https://arxiv.org/abs/2303.10334)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we extract class activation maps (CAMs) that cover the complete objects, including both discriminative and non-discriminative features, from classification models trained only with image-level labels?

The key points are:

- Standard CAMs extracted from classification models often focus only on discriminative parts of objects (e.g. heads of animals), missing non-discriminative but important parts (e.g. bodies and legs). 

- The authors propose a method called LPCAM to extract CAMs that cover entire objects by leveraging both discriminative and non-discriminative features.

- LPCAM clusters the local features from all spatial locations into prototypes representing local visual semantics (e.g. heads, legs). It then aggregates spatial similarity maps between these prototypes and image features to generate the CAM.

- Experiments show LPCAM improves CAMs in multiple weak supervision segmentation methods on PASCAL VOC and COCO datasets.

In summary, the paper introduces LPCAM to generate more complete CAMs by using both discriminative and non-discriminative features, instead of only relying on discriminative classifier weights like standard CAM methods.
