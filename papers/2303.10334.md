# [Extracting Class Activation Maps from Non-Discriminative Features as   well](https://arxiv.org/abs/2303.10334)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we extract class activation maps (CAMs) that cover the complete objects, including both discriminative and non-discriminative features, from classification models trained only with image-level labels?

The key points are:

- Standard CAMs extracted from classification models often focus only on discriminative parts of objects (e.g. heads of animals), missing non-discriminative but important parts (e.g. bodies and legs). 

- The authors propose a method called LPCAM to extract CAMs that cover entire objects by leveraging both discriminative and non-discriminative features.

- LPCAM clusters the local features from all spatial locations into prototypes representing local visual semantics (e.g. heads, legs). It then aggregates spatial similarity maps between these prototypes and image features to generate the CAM.

- Experiments show LPCAM improves CAMs in multiple weak supervision segmentation methods on PASCAL VOC and COCO datasets.

In summary, the paper introduces LPCAM to generate more complete CAMs by using both discriminative and non-discriminative features, instead of only relying on discriminative classifier weights like standard CAM methods.


## What is the main contribution of this paper?

 The main contribution of this paper is a novel method called LPCAM (Local Prototype CAM) to generate class activation maps (CAMs) with better coverage on foreground objects in weakly supervised semantic segmentation (WSSS). The key ideas are:

1. Using clustering to derive class-specific local prototypes that capture both discriminative and non-discriminative visual patterns of the class. This avoids the bias of original CAM which relies on classifier weights that only focus on discriminative parts. 

2. Sliding prototypes over the conv feature map to generate similarity maps, and aggregating them to get the final CAM. This allows preserving non-discriminative regions during normalization.

3. Additional use of negative context prototypes to suppress false positive regions.

The method is evaluated by plugging into multiple state-of-the-art WSSS pipelines, and shows consistent improvements on PASCAL VOC and MS COCO datasets. So the main contribution is proposing a better way to compute CAMs that leverages both discriminative and non-discriminative features, through the use of class-specific local prototypes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new method called LPCAM to generate better class activation maps for weakly supervised semantic segmentation by using clustered local prototypes that capture both discriminative and non-discriminative features of objects.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other research on weakly-supervised semantic segmentation:

- The main contribution of this paper is proposing a new method (LPCAM) to generate better class activation maps by capturing both discriminative and non-discriminative features. This is a novel approach compared to prior work like adversarial erasing or refinement methods. 

- Most prior work focuses on either better seed generation or mask refinement. This paper tackles seed generation. The proposed LPCAM can be plugged into various mask refinement methods like IRN, EDAM, etc. to further boost performance. So it is complementary to refinement techniques.

- Compared to other seed generation methods, LPCAM has the advantage of not requiring re-training or modifications to the classification model. Methods like adversarial erasing require iteratively re-training the model. LPCAM simply clusters the features from a trained model to derive prototypes.

- The experiments demonstrate consistent improvements by plugging LPCAM into state-of-the-art WSSS pipelines like IRN, EDAM, AMN on PASCAL VOC and COCO datasets. The gains over prior art are not huge but consistent. 

- The approach is well-motivated by analyzing issues with conventional CAMs only capturing discriminative features. The ablation studies provide insights on the impact of using class vs context prototypes.

- The method does have limitations like potentially expanding to confusing background contexts that co-occur with objects, as shown in a failure case example.

In summary, LPCAM offers a simple but effective way to improve CAMs for WSSS by capturing non-discriminative features without re-training. The consistent gains across methods and datasets demonstrate its usefulness as a plug-in module.
