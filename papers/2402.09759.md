# [Efficient Language Adaptive Pre-training: Extending State-of-the-Art   Large Language Models for Polish](https://arxiv.org/abs/2402.09759)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Developing large language models (LLMs) for low-resource languages like Polish is challenging due to insufficient training data. Currently, there are no high-quality open-source Polish LLMs with over 100 billion tokens.
- Relying on external AI solutions from big tech companies entails financial costs and limited data control. There is a need for localized LLMs to ensure data sovereignty.

Proposed Solution:
- Perform language adaptive pre-training (LAPT) by fine-tuning an existing English LLM (Mistral-7B) on a high-quality Polish dataset of 276 million tokens.
- Further fine-tune the resulting model (Curie-7B-v1) on downstream tasks from the KLEJ benchmark to evaluate performance.

Key Outcomes:
- Curie-7B-v1 achieves the lowest perplexity score of 3.02 on a Polish test set, demonstrating proficiency in the language.
- On 8 out of 9 KLEJ tasks, Curie-7B-v1 obtains an average score of 89.35%, nearly matching the top model's 90.7% which was trained on far more data. 
- The entire LAPT process requires only 2-3% of the data size used by other models, highlighting efficiency.

Main Contributions:
- Demonstrates the viability of quickly adapting English LLMs to new languages with limited computational resources. 
- Introduces an open-source Polish LLM that achieves state-of-the-art performance in text generation and understanding.
- Provides a model to facilitate development of Polish language AI solutions for enterprises.
- Underscores the potential of localized LLMs for ensuring data sovereignty.

In summary, the paper presents an efficient method to adapt existing LLMs to new languages using far less data, with results showing Curie-7B-v1 reaching parity with top Polish models. The model and methods promise to expand access to high-quality language-focused AI.
