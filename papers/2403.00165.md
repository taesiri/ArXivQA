# [TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text   Classification with Minimal Supervision](https://arxiv.org/abs/2403.00165)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the problem of hierarchical text classification with minimal supervision. The goal is to categorize documents into a label taxonomy tree using only the name of each node/class as supervision signal. This is challenging since the label space is large, structured, and may contain fine-grained and long-tail classes. Most existing methods require substantial labeled data. Recently, large language models (LLMs) show promise for text classification through zero-shot prompting but struggle on large hierarchical label spaces.

Method: 
The paper proposes TELEClass which integrates taxonomy enrichment and tailored use of LLMs. First, it identifies document "core classes" using an LLM. Second, it enriches the taxonomy by extracting class-indicative terms from the corpus using statistical and embedding-based analysis. This provides richer supervision signals. Third, it refines the core classes using an embedding-based similarity score between documents and enriched class representations. Finally, it trains a text classifier on the refined core classes and path-based pseudo-documents generated by the LLM.

Contributions:
The main contributions are:
(1) Taxonomy enrichment with class-indicative terms mined from the corpus.
(2) Two ways of effectively utilizing LLMs - taxonomy-aware prompting for core class annotation and path-based conditioning for data generation.
(3) A new framework, TELEClass, for minimally-supervised hierarchical text classification that outperforms previous methods.

In experiments, TELEClass shows superior performance over both weakly-supervised baselines and LLM zero-shot prompting approaches on two datasets. Ablations validate the efficacy of different components.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a minimally supervised hierarchical text classification method called TELEClass that enriches the taxonomy with corpus-mined terms and utilizes large language models for taxonomy-tailored data annotation and augmentation to train an effective text classifier.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes TELEClass, a new method for minimally supervised hierarchical text classification, which only requires the class names as supervision. 

2. It enriches the label taxonomy with class-indicative topical terms mined from the text corpus. This provides additional features to help interpret the classes and facilitate classification.

3. It explores two ways of utilizing large language models (LLMs) in hierarchical text classification: using LLMs for core class annotation to improve pseudo-label quality, and using LLMs to generate augmented data for each path in the taxonomy to solve the data scarcity issue.

4. Experiments show TELEClass can outperform previous weakly-supervised hierarchical text classification methods and zero-shot LLM prompting methods on two public datasets.

In summary, the key contribution is proposing the TELEClass framework that integrates taxonomy enrichment and tailored usage of LLMs to advance minimally supervised hierarchical text classification.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Hierarchical text classification
- Label taxonomy 
- Minimal supervision
- Taxonomy enrichment
- Class-indicative topical terms
- Large language models (LLMs)
- Zero-shot prompting
- Pseudo-labeling 
- Embedding-based document-class matching
- Path-based data augmentation
- Multi-label training
- Text-matching network

The paper proposes a new method called TELEClass for minimally supervised hierarchical text classification. The key ideas include enriching the label taxonomy using class-specific terms mined from the corpus, utilizing LLMs for data annotation and creation tailored to the hierarchy, path-based data augmentation, and training a text classifier with embedding-based matching scores and multi-label objectives. The goal is to reduce human annotation efforts while maintaining accuracy.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes taxonomy enrichment to improve the pseudo-label quality for classifier training. What specific techniques are used to identify class-indicative topical terms from the text corpus? How do they measure the popularity, distinctiveness, and semantic similarity of terms?

2. The paper explores two ways of adopting large language models (LLMs) for hierarchical text classification. What are they and how do they specifically utilize LLMs to improve pseudo-label quality and solve data scarcity issues? 

3. What is the motivation behind using path-based data augmentation to generate pseudo training data instead of just generating data for missing classes in the taxonomy? How does path-based data augmentation help regularize potential bias in the pseudo-labels?

4. What encoding methods are used to obtain the document and class representations for refinement of core classes? Why are these representations more reliable for corpus-level comparison versus using textual entailment scores?

5. Why can't large language models effectively understand and classify documents in a hierarchical label space when prompted naively? What taxonomy-guided strategies help improve their performance?

6. The paper proposes an initial core class annotation step. Why is a separate step needed before taxonomy enrichment and how does it facilitate better enrichment? What are the limitations it aims to address?

7. For the classifier architecture, why are class embeddings initialized then detached from the base LM instead of being end-to-end trained? What benefits does this provide?

8. How does the paper balance and scale the losses from the refined core class data versus the generated path-based data during classifier training? What enables direct comparison between the losses?

9. What inferences can be made about the different contribution levels of taxonomy enrichment and path-based data generation on the Amazon versus DBPedia datasets based on the ablation study results?

10. What are some potential ways the ideas from this paper could be extended to other related tasks such as fine-grained entity typing or settings with an extremely large label space?
