# [Dataset Distillation via Adversarial Prediction Matching](https://arxiv.org/abs/2312.08912)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper proposes a novel perspective on dataset distillation by formulating it as an adversarial prediction matching problem. The key idea is to minimize the prediction discrepancy on the real data distribution between teacher models trained on the original large dataset and student models trained on the small distilled dataset. To solve this effectively, an adversarial framework is introduced where the student model is trained to match the teacher's predictions on synthetic samples, while these samples are adversarially updated to approach "hard samples" that can cause substantial disagreement between the teacher and student predictions. Compared to prior distillation methods, this approach avoids shortsightedness by matching converged teacher predictions rather than local gradients or trajectories, enables single-level optimization for efficient memory usage, and requires storing only one teacher checkpoint. Experiments show the method can distill ImageNet with just 6.5GB GPU memory and outperforms state-of-the-art approaches, synthesizing datasets just 10% the size of originals yet achieving 94% of their accuracy. The distilled datasets also demonstrate superior cross-architecture generalization and efficacy for neural architecture search. Overall, by formulating dataset distillation as an adversarial prediction matching problem, the proposed method advances the state-of-the-art in effectiveness, efficiency, and flexibility.
