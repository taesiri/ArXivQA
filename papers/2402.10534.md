# [Using Left and Right Brains Together: Towards Vision and Language   Planning](https://arxiv.org/abs/2402.10534)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing large multi-modality models (LMMs) like GPT-3 can process textual and visual inputs but rely solely on linguistic reasoning for planning and decision making. They lack capabilities for visual imagination and reasoning.

- In contrast, human cognition utilizes both hemispheres of the brain - left for language, logic and sequential reasoning; right for spatial awareness and visual intuition. 

Proposed Solution:
- The paper proposes a Visual-Language Planning (VLP) framework to enable both visual and linguistic reasoning for multi-modality tasks.

- VLP has two key components working in parallel - Language Planning using LLM like ChatGPT to break down questions into logical steps, and Vision Planning using video prediction model to imagine potential future visual states.

- An LMM then synthesizes the language and vision plans to make the final decision, like the central decision making part of a human brain.

Key Contributions:
- Novel idea of incorporating visual planning along with language planning for multi-modality decision making, inspired by human brain's hemispheric functions.

- Implement VLP by integrating ChatGPT for language planning and Stable Diffusion for video prediction to realize the framework.

- Demonstrate VLP's superiority over standalone LMMs across vision-language tasks as well as pure vision and pure language tasks.

- Analysis shows both vision and language planning contribute positively. Vision planning is more useful for vision-centric tasks and language planning for linguistic tasks.

- Case studies illustrate how VLP results in more vivid, logical and contextually accurate responses compared to vanilla LMMs.

In summary, the paper makes an important contribution in enhancing multi-modality reasoning for AI systems by proposing and demonstrating a human brain inspired visual-language planning approach.
