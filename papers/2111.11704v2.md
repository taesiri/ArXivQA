# [Deep Point Cloud Reconstruction](https://arxiv.org/abs/2111.11704v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It proposes a new task called "point cloud reconstruction" which aims to jointly address several common issues with raw point clouds obtained from 3D scanning devices: noise, sparsity, irregularity, and outliers. 

- It presents a novel two-stage deep learning architecture for point cloud reconstruction. The first stage is a voxel generation network that densifies the point cloud and removes outliers. The second stage is a voxel re-localization network that converts the voxels back to points and further refines the point locations.

- A core component of the voxel re-localization network is the use of transformers with a new "amplified positional encoding" scheme. This encoding adaptively amplifies high frequency signals in the positional encoding based on voxel distances to help infer detailed points.

- Through experiments on ShapeNet, ScanNet, and ICL-NUIM datasets, the method demonstrates state-of-the-art performance compared to prior work on point cloud upsampling, denoising and completion. The results also show good generalization to real-world scans.

In summary, the key hypothesis is that jointly addressing point cloud reconstruction in a unified framework leads to better performance compared to tackling the tasks of densification, denoising and completion separately. The two-stage voxel-based architecture with the amplified positional encoding is proposed as an effective approach for point cloud reconstruction.


## What is the main contribution of this paper?

 This paper proposes a deep learning approach for point cloud reconstruction. The main contributions are:

1. New problem formulation: The paper formulates a new task called "point cloud reconstruction" which aims to jointly resolve inherent issues in raw point clouds obtained from 3D scanners, such as noise, sparsity and irregularity. 

2. Novel two-stage architecture: A deep neural network is proposed consisting of two stages - a voxel generation network and a voxel re-localization network. The first network converts the point cloud to voxels and refines it. The second network converts voxels back to points for further refinement.

3. Amplified positional encoding: A novel positional encoding method is introduced that amplifies high frequency signals in the encoding based on voxel distances. This is designed to help the voxel re-localization network perform adaptive point refinement.

4. Experiments: Extensive experiments are conducted on ShapeNet, ScanNet and ICL-NUIM datasets. The method demonstrates state-of-the-art performance for point cloud reconstruction, and strong generalization ability to real-world 3D scans without fine-tuning.

In summary, the key contribution is a new deep learning pipeline for jointly handling various inherent issues in raw point clouds in a unified manner, enabled by novel components like the amplified positional encoding. The experiments demonstrate its effectiveness for high quality point cloud reconstruction.
