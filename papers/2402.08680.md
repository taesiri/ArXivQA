# [Mitigating Object Hallucination in Large Vision-Language Models via   Classifier-Free Guidance](https://arxiv.org/abs/2402.08680)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem: 
- Large vision-language models (LVLMs) have a tendency to hallucinate non-existing objects when generating image descriptions, compromising their accuracy and reliability. This is known as "object hallucination".

- Prior works try to address this via fine-tuning on curated datasets or leveraging advanced LLMs like GPT-3.5 for post-generation corrections. However, these entail significant costs and may overwrite original generations.

Proposed Solution:
- The paper introduces MARINE, a framework to mitigate object hallucinations in LVLMs during generation in a training-free and API-free manner.  

- MARINE enriches visual context by integrating open-source vision models like DETR. It directly aligns predicted object probabilities to text without fine-tuning the alignment.

- A soft prompt is formed from this along with a "focusing" text. Classifier-free guidance is then used during generation to incorporate this prompt.

Main Contributions:
- Demonstrated state-of-the-art performance in reducing hallucinations across 6 LVLMs using metrics like CHAIR and POPE, outperforming prior methods.

- Enhances detailedness of generations as assessed by GPT-4V, while preserving diversity in responding to different questions.

- Training-free and API-free approach that is computationally efficient. Compatible with any vision model and projection function.

- Analyzed impact of guidance strength and visual context noise on mitigating hallucinations. Showed how guidance controls logit distributions.

In summary, the paper makes significant contributions in efficiently mitigating a critical issue of object hallucination in LVLMs via a flexible framework that enriches visual context and guides the generation process.
