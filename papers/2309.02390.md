# [Explaining grokking through circuit efficiency](https://arxiv.org/abs/2309.02390)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:Why does a neural network's test performance dramatically improve upon continued training, after it has already achieved near perfect training accuracy (a phenomenon referred to as "grokking")?The paper proposes an explanation that the network first learns an inefficient "memorizing" solution that achieves good training performance through brute force memorization. However, there also exists a more "efficient" generalizing solution that achieves the same training performance with a lower parameter norm. As training continues, weight decay pushes the network to transition from the inefficient memorizing solution to the more efficient generalizing solution, resulting in the jump in test performance. The paper aims to provide evidence for this explanation by making and confirming predictions around the relative efficiency of memorizing vs generalizing solutions, the relationship between efficiency and dataset size, and novel behaviors like "ungrokking" and "semi-grokking" that arise from analyzing the efficiency crossover point. Overall, the central question is understanding why test performance improves dramatically in grokking after near perfect train performance, and the proposed explanation is the transition from an inefficient memorizing solution to a more efficient generalizing solution.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:1. It proposes an explanation for the phenomenon of "grokking" in neural networks, where networks exhibit a sudden transition from poor generalization to perfect generalization during training. 2. The key idea is that there are two families of computational mechanisms ("circuits") that can achieve good training performance - one that generalizes (G) and one that memorizes (M). The paper argues that G is more "efficient" than M in terms of converting parameters into logits.3. It shows theoretically that with these assumptions, continuing training will strengthen G and weaken M, causing the transition in generalization. A simple simulation confirms this.4. It makes and verifies several novel predictions based on analyzing the hypothesized "critical dataset size" where G and M have equal efficiency:- Efficiency of G is constant, while efficiency of M decreases with dataset size.- "Ungrokking": networks can transition back from good to poor generalization if retrained on smaller datasets. - "Semi-grokking": middling generalization if train on dataset size where G and M efficiencies are similar.5. Overall, the paper provides significant evidence for the efficiency-based explanation by making and confirming surprising predictions about grokking and related behaviors.In summary, the key contribution is a new computational efficiency-based explanation for grokking in neural networks, supported by theoretical analysis and novel empirical results. The notion of circuit efficiency may also help explain other phenomena in deep learning more broadly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes that grokking occurs because neural networks can learn two types of solutions - one that memorizes the training data but does not generalize, and one that generalizes well but is slower to learn; weight decay then gradually shifts preference from the memorizing solution to the more efficient generalizing solution, causing the improvement in test accuracy.
