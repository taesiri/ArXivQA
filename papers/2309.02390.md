# [Explaining grokking through circuit efficiency](https://arxiv.org/abs/2309.02390)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:Why does a neural network's test performance dramatically improve upon continued training, after it has already achieved near perfect training accuracy (a phenomenon referred to as "grokking")?The paper proposes an explanation that the network first learns an inefficient "memorizing" solution that achieves good training performance through brute force memorization. However, there also exists a more "efficient" generalizing solution that achieves the same training performance with a lower parameter norm. As training continues, weight decay pushes the network to transition from the inefficient memorizing solution to the more efficient generalizing solution, resulting in the jump in test performance. The paper aims to provide evidence for this explanation by making and confirming predictions around the relative efficiency of memorizing vs generalizing solutions, the relationship between efficiency and dataset size, and novel behaviors like "ungrokking" and "semi-grokking" that arise from analyzing the efficiency crossover point. Overall, the central question is understanding why test performance improves dramatically in grokking after near perfect train performance, and the proposed explanation is the transition from an inefficient memorizing solution to a more efficient generalizing solution.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:1. It proposes an explanation for the phenomenon of "grokking" in neural networks, where networks exhibit a sudden transition from poor generalization to perfect generalization during training. 2. The key idea is that there are two families of computational mechanisms ("circuits") that can achieve good training performance - one that generalizes (G) and one that memorizes (M). The paper argues that G is more "efficient" than M in terms of converting parameters into logits.3. It shows theoretically that with these assumptions, continuing training will strengthen G and weaken M, causing the transition in generalization. A simple simulation confirms this.4. It makes and verifies several novel predictions based on analyzing the hypothesized "critical dataset size" where G and M have equal efficiency:- Efficiency of G is constant, while efficiency of M decreases with dataset size.- "Ungrokking": networks can transition back from good to poor generalization if retrained on smaller datasets. - "Semi-grokking": middling generalization if train on dataset size where G and M efficiencies are similar.5. Overall, the paper provides significant evidence for the efficiency-based explanation by making and confirming surprising predictions about grokking and related behaviors.In summary, the key contribution is a new computational efficiency-based explanation for grokking in neural networks, supported by theoretical analysis and novel empirical results. The notion of circuit efficiency may also help explain other phenomena in deep learning more broadly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes that grokking occurs because neural networks can learn two types of solutions - one that memorizes the training data but does not generalize, and one that generalizes well but is slower to learn; weight decay then gradually shifts preference from the memorizing solution to the more efficient generalizing solution, causing the improvement in test accuracy.


## How does this paper compare to other research in the same field?

Based on my reading of the paper's abstract and introduction, here is a brief comparison to other related research:- The paper seems to focus specifically on explaining the "grokking" phenomenon, where neural networks exhibit a sudden transition from poor generalization to strong generalization during training. Other papers have also tried to explain grokking, but this paper proposes a new conceptual explanation based on the relative efficiency of "memorizing" vs "generalizing" circuits in the network. - The paper introduces two novel predicted behaviors ("ungrokking" and "semi-grokking") that follow from their conceptual explanation of grokking. As far as I can tell, these specific phenomena have not been discussed or demonstrated previously in the literature on grokking.- The paper provides empirical support for their efficiency-based explanation by measuring how generalization and memorization efficiency vary with dataset size. Other papers have characterized the development of generalizing vs memorizing computation during grokking, but not through the lens of efficiency.- The paper relates grokking to some broader topics like implicit regularization and the role of dataset size, but does not make connections to some other potentially relevant areas like catastrophic forgetting, lottery ticket hypotheses, etc. Other papers have situated grokking in relation to more concepts.Overall, this paper seems to provide a novel conceptual explanation for grokking based on efficiency, makes testable predictions from that explanation, and provides empirical support. The main unique contributions seem to be the efficiency-based theory, the prediction of ungrokking/semi-grokking, and the efficiency measurements. The situating of grokking in the broader literature seems somewhat limited compared to other works. But within its more focused scope, the paper appears to advance the specific understanding of grokking.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Further investigate the theoretical puzzles left unexplained in grokking, such as why the time to grok rises super-exponentially as dataset size decreases, and how the random initialization interacts with efficiency to determine which circuits are found by gradient descent.- Explore the potential for understanding deep learning more broadly through the lens of circuit efficiency, beyond just explaining grokking. For example, applying the concepts to more realistic settings like language modeling. - Extend the notion of circuit efficiency to account for other constraints that gradient descent navigates, beyond just parameter norm. For example, also considering things like model capacity, interference between circuits, fitting the training data, etc.- Study the role of circuit efficiency in related phenomena like the transition from in-context learning to in-weights learning during language model training. The authors hypothesize this may also be explained by the generalising in-context solution being more efficient.- Investigate if there are other regularization effects with a similar impact to weight decay, which could potentially extend their theory to explain grokking even in the absence of explicit weight decay.- Look at whether the general concepts around circuits, efficiency, and speed of learning could provide insight about phase transitions and mixture of algorithms more broadly in deep learning.In summary, the main suggestions are to further develop the theoretical understanding of efficiency, extend the ideas to more realistic settings, connect it to related phenomena, and potentially use it as a lens to understand other aspects of deep learning as well. The circuit efficiency viewpoint seems promising as a tool for gaining insight.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes an explanation for the phenomenon of "grokking" in neural networks, where a network transitions from memorizing the training data to generalizing well after achieving near perfect training accuracy. The key idea is that there are two families of computational mechanisms ("circuits") within the network - one which memorizes the training data (\Mem) and one which generalizes (\Gen). \Gen is more "efficient" in that it can produce the same training accuracy with smaller parameter norms. Meanwhile, \Mem is learned more quickly than \Gen initially. Thus, the network first relies on \Mem, achieving low training loss but poor generalization. With continued training, gradient descent strengthens \Gen and weakens \Mem due to \Gen's higher efficiency, eventually leading to a transition where \Gen dominates and generalization improves. The paper makes predictions based on analyzing the efficiency tradeoff, and confirms them through experiments, including demonstrating two new behaviors: "ungrokking", where a grokked network reverts to poor generalization, and "semi-grokking", where a network transitions to only middling generalization. Overall, the efficiency explanation provides significant evidence for understanding the dynamics behind grokking.
