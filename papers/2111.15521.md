# [Node-Level Differentially Private Graph Neural Networks](https://arxiv.org/abs/2111.15521)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we learn accurate graph neural network (GNN) models while preserving node-level differential privacy?The key points are:- GNNs are susceptible to leaking sensitive node information, as each node's representation depends on features of its graph neighborhood. - Standard differential privacy techniques like DP-SGD are designed for non-graph settings and don't directly apply.- The paper proposes a method to adapt DP-SGD to provide formal node-level privacy guarantees for GNNs.- The technical contributions involve: (1) a graph sampling scheme to bound node occurrences across mini-batches, and (2) an extension of the privacy amplification theorem to account for gradient terms depending on multiple nodes.- Experiments demonstrate that their private GNN method outperforms baselines without graph structure and approaches non-private GNN accuracy, while preserving strong node-level privacy.In summary, the paper develops a principled approach to train accurate and node-level private GNNs, formalizing the problem and providing an algorithmic solution. The empirical results validate that the method enables differentially private learning of GNNs with high utility.
