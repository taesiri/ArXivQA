# [Collaborative Inference via Dynamic Composition of Tiny AI Accelerators   on MCUs](https://arxiv.org/abs/2401.08637)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Tiny AI accelerators (e.g. MAX78000) bring AI inference capabilities to resource-constrained microcontrollers (MCUs), enabling on-device AI. However, they face challenges due to limited onboard memory and lack of support for multi-model, distributed inference. 

- Offloading inference to the cloud or nearby devices is not always viable for wearables due to privacy concerns, connectivity issues, and communication overhead.

- Collaborating tiny AI accelerators on distributed on-body devices can help address these limitations but brings technical challenges related to resource heterogeneity, multi-tenancy, and optimal task distribution.

Proposed Solution:
- The paper proposes Synergy, a system to support multi-tenant AI models through dynamic composition of tiny AI accelerators on wearables.

- Key innovation is the "virtual computing space" which offers a unified, virtualized view of distributed computational resources. This allows specifying application logic without mapping to physical devices.

- At runtime, Synergy explores task distribution options, generates execution plans mapping logical tasks to physical devices, and selects the optimal plans to maximize overall throughput across concurrent models.

- Two key capabilities: (1) Joint optimization across execution plans, considering resource availability and dependencies (2) End-to-end pipeline planning from sensors to output targets.

Contributions:
- Concept of virtual computing space to simplify programming and hide heterogeneity of wearable environments

- Runtime orchestration technique to dynamically distribute tasks across available tiny AI accelerators for optimal inference

- Extensive evaluation demonstrating significant throughput gains over baselines by holistic planning and communication optimization

- Demonstrated adaptability to changes in number of devices, models, heterogeneous accelerators, source/target configurations

In summary, the paper addresses key limitations of tiny AI accelerators through an innovative system based on virtualization and collaborative inference optimization across distributed on-body devices.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper presents Synergy, a system that dynamically composes multiple tiny AI accelerators across wearable devices into a virtual computing space to efficiently distribute execution of multi-tenant deep learning models and pipelines for collaborative on-device inference.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing Synergy, a system to support multi-tenant AI models through the dynamic composition of tiny AI accelerators across distributed wearable devices. The key innovations of Synergy include:

1) The concept of a "virtual computing space" which provides a unified, virtualized view of distributed computational resources and allows device-agnostic programming of application logic. This hides the heterogeneity and dynamics of the underlying physical devices.

2) A runtime orchestration module that dynamically distributes tasks of AI model pipelines to available tiny AI accelerators, aiming to maximize overall system throughput. It considers various options like sensor/actuator mapping, model splitting, joint resource constraints, etc. 

3) Extensive evaluations comparing Synergy with 7 baselines including state-of-the-art model partitioning techniques. Results show Synergy consistently achieves higher throughput (on average 8.0x higher) by effectively composing multiple tiny AI accelerators.

In summary, the key innovation is enabling collaborative execution of multi-tenant models across resource-constrained wearable devices with tiny AI accelerators, through dynamic task distribution and virtualization of underlying heterogeneous resources.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it are:

- Tiny AI accelerators: refers specifically to small, low-power AI hardware accelerators like the MAX78000 and MAX78002 that can enable on-device deep learning on resource-constrained microcontrollers (MCUs).

- TinyML: the field focused on machine learning for microcontrollers with extreme memory and compute constraints. Related techniques aim to minimize model size.

- Virtual computing space: a key innovation in the paper - an abstract layer that provides a unified view of distributed AI accelerators across wearable devices and hides heterogeneity.

- Dynamic composition: the idea of dynamically distributing model execution across available tiny AI accelerators at runtime to maximize throughput. Involves model splitting techniques.  

- Multi-tenant models: supporting concurrent execution of multiple models on the same set of accelerators by dynamically allocating resources.

- Holistic orchestration: jointly optimizing distribution of multiple model pipelines by considering resource constraints, pipeline dependencies, communication costs, etc. in a unified manner.

- End-to-end pipeline: the paper considers full pipelines from sensors to target devices, not just model execution. Planning incorporates sensing and output delivery.

So in summary, key terms revolve around leveraging distribute on-device tiny AI accelerators for multi-model concurrent execution via holistic runtime optimization techniques like virtualization, dynamic composition and end-to-end pipeline planning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper introduces the concept of a "virtual computing space". What is the key purpose of this concept and how does it help address the challenges of dynamic and heterogeneous wearable environments?

2. The paper proposes a runtime orchestration module. What are the four key stages of the runtime orchestration process and what does each stage aim to achieve? 

3. Execution plan generation is one of the stages in runtime orchestration. What are the key considerations when generating execution plan candidates for each pipeline?

4. What is joint plan generation and why is it an important step before selecting the best plan for execution? What are some of the constraints considered during this stage?

5. The paper proposes a pipeline ordering strategy during joint plan selection. Why is this ordering based on data intensity of pipelines rather than other metrics? 

6. How does the paper estimate task latency for different operations like sensing, memory access, communication and inference? What are some findings from the layer-wise latency analysis?

7. What is the key insight or rationale behind the joint plan selection policy adopted in this work for maximizing overall throughput?

8. Model synthesis is needed before deployment to target devices. What does this process involve and why is it important?

9. How does the concept of virtual computing space and device-agnostic programming abstraction help simplify application development and integration?

10. The paper evaluates the method on different workloads and against several baselines. What are some of the key inferences from the experimental results? Which factors contribute the most to performance gains?
