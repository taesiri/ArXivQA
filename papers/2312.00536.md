# [Trained MT Metrics Learn to Cope with Machine-translated References](https://arxiv.org/abs/2312.00536)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates how machine translation (MT) evaluation metrics behave when provided with machine-translated references instead of human references. The authors first show that non-trained metrics like BLEU and chrF exhibit a substantial drop in correlation with human judgments when using machine-translated references. In contrast, trained metrics like COMET maintain most of their accuracy. To further analyze this, the authors fine-tune the Prism metric on human judgments of MT output, using a bidirectional pairwise ranking approach. As expected, fine-tuning significantly improves Prism's correlation with human judgments of unseen test data. Crucially, it also makes Prism more robust to machine-translated references compared to the non-fine-tuned version. This indicates that training a metric on human evaluations allows it to better cope with imperfect, machine-translated references. The key conclusions are: 1) Machine-translated references diminish the accuracy of non-trained reference-based metrics, 2) Trained metrics are more robust to such references, 3) Fine-tuning a metric like Prism on human judgments improves overall accuracy while also increasing robustness to machine-translated references. The findings suggest that training can help metrics make the most of noisy references commonly found in practice.
