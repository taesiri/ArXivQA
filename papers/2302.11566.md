# [Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via   Self-supervised Scene Decomposition](https://arxiv.org/abs/2302.11566)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to reconstruct detailed 3D avatars from monocular in-the-wild videos without requiring ground truth supervision or priors from large datasets of human scans. The key hypothesis is that by jointly modeling the human and background scene implicitly using separate neural fields, and optimizing them along with human pose in a global formulation, the method can achieve robust scene decomposition and high-quality 3D reconstruction of the human subject.Specifically, the core hypotheses are:- Modeling the human and background jointly using separate implicit neural fields can enable self-supervised scene decomposition and human reconstruction without ground truth data.- Leveraging a canonical human shape representation and pose-conditioned deformation can provide a consistent shape constraint across frames.- A global optimization over pose, shape, appearance and background can enable joint refinement for better reconstruction. - Novel scene decomposition objectives like opacity regularization can improve separation of human from background.In summary, the main research question is monocular human reconstruction from in-the-wild videos without supervision, addressed via hypotheses of joint modeling, canonical space supervision, global optimization and scene decomposition objectives.


## What is the main contribution of this paper?

The main contribution of this paper is a method called Vid2Avatar for reconstructing detailed 3D avatars from monocular in-the-wild videos through self-supervised scene decomposition. Specifically, the key ideas and contributions are:- The method models both the human and background scene implicitly using separate neural fields that are optimized jointly to decompose the scene. This avoids reliance on external segmentation modules.- A temporally consistent canonical space representation is used for modeling the human shape and appearance. This enables learning from deformed observations.- Novel objectives are introduced, including an opacity sparseness loss and a ray classification loss, to encourage clean separation between the dynamic human and static background.- The training is formulated as a global optimization over the human and background models and per-frame pose parameters.- Evaluations demonstrate state-of-the-art performance on segmentation, novel view synthesis, and 3D reconstruction without ground truth supervision.- A new semi-synthetic dataset called SynWild is contributed for quantitative evaluation of monocular human reconstruction methods.In summary, the key contribution is the proposed method for self-supervised monocular scene decomposition and high-quality 3D avatar reconstruction from in-the-wild videos, enabled by joint modeling of the human and background, canonical space representation, and novel objectives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Vid2Avatar, a method to reconstruct detailed 3D human avatars from monocular in-the-wild videos through self-supervised scene decomposition, jointly optimizing implicit neural representations of the human and background to separate them without requiring ground truth data or external segmentation modules.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other related work in monocular human performance capture:- Unlike template-based methods like MonoPerfCap, DeepCap, etc., this method does not require a pre-scanned template or manual rigging. It learns the shape and appearance model directly from video.- Compared to regression-based methods like PIFu, PIFuHD, etc. that directly regress surfaces from images, this method is trained without requiring 3D supervision data. It also maintains temporal coherence over the sequence. - Unlike other video-based neural rendering methods like NeuralBody, Anthropic, etc., this method does not rely on pre-segmented inputs from an external module. It jointly optimizes for scene decomposition and surface reconstruction.- The coarse-to-fine sampling strategy and proposed scene decomposition objectives allow separating the human from complex backgrounds more robustly compared to other neural radiance field approaches.- The canonical space representation enforces temporal consistency better than methods that optimize per-frame shape and appearance independently.- The results demonstrate more detailed and physically plausible reconstructions than recent state-of-the-art video-based methods like SelfRecon and ICON.In summary, the key novelty of this work is in achieving detailed and robust human reconstructions from monocular in-the-wild videos without ground truth supervision or pre-segmentation, via the proposed scene decomposition and canonical space optimization approach. The comparisons validate the improvements over existing methods on various tasks.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors are:- Improving generalization to more complex clothing and human poses. The authors note limitations in handling loose, free-flowing clothing like skirts. Extending the method to better handle more complex garments and poses is suggested.- Incorporating temporal smoothing. The paper models each frame independently. Adding temporal consistency constraints or modeling could potentially improve coherence over time.- Exploring self-supervised training. The method currently requires pose estimates as input. Developing techniques to learn in a completely self-supervised manner without pose inputs could be valuable.- Higher resolution reconstruction. The results are currently limited in resolution due to memory constraints. Scaling up the reconstruction by using techniques like hierarchical sampling could allow higher frequency details to be recovered.- Extending to multi-person reconstruction. The current method is designed for single person reconstruction. Extending it to jointly model multiple people is noted as an interesting direction.- Reducing reliance on pose estimation. As noted above, unreasonable pose initialization can lead to failure cases. Exploring ways to make the method more robust to pose estimation errors could help improve generalization.In summary, the key future directions focus on improving the robustness, detail, and flexibility of the approach through advances in areas like self-supervision, multi-person modeling, temporal consistency, and higher resolution reconstruction.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents Vid2Avatar, a method to reconstruct detailed 3D human avatars from monocular in-the-wild videos without requiring any ground truth supervision or priors from large datasets of clothed human scans. The key idea is to model the dynamic human and static background scene jointly using two separate neural fields that are composited together via neural rendering techniques. The human shape and appearance are represented in a canonical pose space to enable a single coherent representation. Novel objectives are proposed to encourage separation of the human and background fields even when ambiguous, such as for feet on the floor. The method is trained via a global optimization over all video frames that jointly optimizes the background field, canonical human field, and per-frame pose parameters. Experiments demonstrate state-of-the-art results for human avatar reconstruction from monocular video across several datasets. The method does not require any external segmentation and instead learns to separate the human from the background in a self-supervised fashion purely from video input.
