# [Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via   Self-supervised Scene Decomposition](https://arxiv.org/abs/2302.11566)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to reconstruct detailed 3D avatars from monocular in-the-wild videos without requiring ground truth supervision or priors from large datasets of human scans. The key hypothesis is that by jointly modeling the human and background scene implicitly using separate neural fields, and optimizing them along with human pose in a global formulation, the method can achieve robust scene decomposition and high-quality 3D reconstruction of the human subject.Specifically, the core hypotheses are:- Modeling the human and background jointly using separate implicit neural fields can enable self-supervised scene decomposition and human reconstruction without ground truth data.- Leveraging a canonical human shape representation and pose-conditioned deformation can provide a consistent shape constraint across frames.- A global optimization over pose, shape, appearance and background can enable joint refinement for better reconstruction. - Novel scene decomposition objectives like opacity regularization can improve separation of human from background.In summary, the main research question is monocular human reconstruction from in-the-wild videos without supervision, addressed via hypotheses of joint modeling, canonical space supervision, global optimization and scene decomposition objectives.


## What is the main contribution of this paper?

The main contribution of this paper is a method called Vid2Avatar for reconstructing detailed 3D avatars from monocular in-the-wild videos through self-supervised scene decomposition. Specifically, the key ideas and contributions are:- The method models both the human and background scene implicitly using separate neural fields that are optimized jointly to decompose the scene. This avoids reliance on external segmentation modules.- A temporally consistent canonical space representation is used for modeling the human shape and appearance. This enables learning from deformed observations.- Novel objectives are introduced, including an opacity sparseness loss and a ray classification loss, to encourage clean separation between the dynamic human and static background.- The training is formulated as a global optimization over the human and background models and per-frame pose parameters.- Evaluations demonstrate state-of-the-art performance on segmentation, novel view synthesis, and 3D reconstruction without ground truth supervision.- A new semi-synthetic dataset called SynWild is contributed for quantitative evaluation of monocular human reconstruction methods.In summary, the key contribution is the proposed method for self-supervised monocular scene decomposition and high-quality 3D avatar reconstruction from in-the-wild videos, enabled by joint modeling of the human and background, canonical space representation, and novel objectives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Vid2Avatar, a method to reconstruct detailed 3D human avatars from monocular in-the-wild videos through self-supervised scene decomposition, jointly optimizing implicit neural representations of the human and background to separate them without requiring ground truth data or external segmentation modules.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other related work in monocular human performance capture:- Unlike template-based methods like MonoPerfCap, DeepCap, etc., this method does not require a pre-scanned template or manual rigging. It learns the shape and appearance model directly from video.- Compared to regression-based methods like PIFu, PIFuHD, etc. that directly regress surfaces from images, this method is trained without requiring 3D supervision data. It also maintains temporal coherence over the sequence. - Unlike other video-based neural rendering methods like NeuralBody, Anthropic, etc., this method does not rely on pre-segmented inputs from an external module. It jointly optimizes for scene decomposition and surface reconstruction.- The coarse-to-fine sampling strategy and proposed scene decomposition objectives allow separating the human from complex backgrounds more robustly compared to other neural radiance field approaches.- The canonical space representation enforces temporal consistency better than methods that optimize per-frame shape and appearance independently.- The results demonstrate more detailed and physically plausible reconstructions than recent state-of-the-art video-based methods like SelfRecon and ICON.In summary, the key novelty of this work is in achieving detailed and robust human reconstructions from monocular in-the-wild videos without ground truth supervision or pre-segmentation, via the proposed scene decomposition and canonical space optimization approach. The comparisons validate the improvements over existing methods on various tasks.
