# [Efficient and Modular Implicit Differentiation](https://arxiv.org/abs/2105.15183)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:How can we develop an efficient and modular approach to automatically differentiate the solution of optimization problems with respect to their parameters?The key hypotheses appear to be:1) By defining a mapping function F that captures the optimality conditions of the optimization problem, we can leverage implicit differentiation and autodiff of F to efficiently compute derivatives. 2) This approach will be more efficient than explicitly differentiating through unrolled optimization algorithm iterations.3) Defining F in a modular way separate from the optimization solver will make this approach widely applicable. The authors propose automatic implicit differentiation as a framework to address these hypotheses, and demonstrate its efficiency and modularity through theoretical analysis and experiments on several applications. The main novelty seems to be providing a generic and easy-to-use approach to implicit differentiation that leverages autodiff systems like JAX, rather than requiring manual case-by-case derivations and implementations.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a framework for automatic implicit differentiation of optimization problem solutions. The key ideas are:- The user provides a Python function F capturing the optimality conditions of the optimization problem to differentiate. This makes the framework very flexible and modular. - Leveraging autodiff of F combined with the implicit function theorem allows automatic computation of derivatives, removing the need for tedious manual derivations.- The framework is efficient as it can leverage state-of-the-art solvers, unlike approaches based on unrolling.- The paper demonstrates the framework on a large catalog of optimality conditions, including stationary points, KKT conditions, proximal gradient, projected gradient, mirror descent, Newton, and more. This shows the wide applicability.- Theoretical analysis is provided on the error when differentiating approximate solutions. This is validated empirically.- The ease of solving complex bi-level optimization problems is demonstrated through experiments on hyperparameter optimization, dataset distillation, and task-driven dictionary learning.- An application to sensitivity analysis for molecular dynamics is presented.Overall, the paper significantly lowers the barrier for using implicit differentiation in practice through an efficient, flexible and modular framework integrated within JAX.
