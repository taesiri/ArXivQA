# [DREAM: Debugging and Repairing AutoML Pipelines](https://arxiv.org/abs/2401.00379)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper identifies and focuses on two common bugs in AutoML systems: performance bugs where search takes too long, and ineffective search bugs where the AutoML system is unable to find an accurate enough model. Through debugging and analyzing AutoKeras, the authors find that existing AutoML systems overlook optimization opportunities in the search space, search method, and search feedback. This results in the two bugs mentioned above.

Proposed Solution (\sys):
The paper proposes \sys, an automatic debugging and repair system for AutoML pipelines. \sys has three key mechanisms to detect and repair AutoML bugs:

1. Search Space Expansion: Expands the search space by adding more feasible actions like weight initializers, optimizers, and multi-step fine-tuning strategies. This allows finding more optimal models.

2. Feedback-Driven Search: Uses detailed training feedback like gradients and weights to determine the optimal action for modifying architectures/hyperparameters in each trail. This fixes ineffective searches.  

3. Feedback Monitoring: Monitors model training to collect detailed feedback and detects performance/ineffective search bugs using formalized symptoms.

Main Contributions:

- Identifies root causes of common AutoML bugs through debugging AutoKeras 
- Proposes three mechanisms in \sys to automatically detect and repair the identified AutoML bugs
- Develops \sys prototype on AutoKeras and TensorFlow and evaluates it on four datasets
- Shows \sys can effectively repair bugs - average accuracy of 83\% compared to best AutoKeras accuracy of 55\%
- Demonstrates efficiency of \sys - overhead of less than 1\% compared to model training
- Ablation studies validate the contribution of each proposed mechanism
- Publicly releases code, data and experiments to reproduce the technique

In summary, the paper presents a novel AutoML debugging and repairing technique to address common performance and ineffective search bugs by expanding search spaces and using feedback-driven search. The average accuracy improvements demonstrate its effectiveness.


## Summarize the paper in one sentence.

 This paper proposes DREAM, an automatic debugging and repairing system for AutoML systems that fixes performance bugs and ineffective search bugs by expanding the search space and using a feedback-driven search strategy.


## What is the main contribution of this paper?

 The main contributions of this paper can be summarized as follows:

1. The paper identifies and analyzes two common bugs in AutoML systems - performance bugs where the search takes too long, and ineffective search bugs where the system is unable to find an accurate enough model. 

2. The paper proposes a system called DREAM to automatically debug and repair AutoML systems. DREAM has three key mechanisms - search space expansion to allow more optimization opportunities, feedback-driven search using detailed training and evaluation data to guide the search, and feedback monitoring to collect the necessary data.

3. The paper implements a prototype of DREAM and evaluates it on four public datasets. The results show DREAM can effectively repair AutoML bugs - the repaired searches achieve 83.21% accuracy on average compared to only 54.66% for the best baseline method. DREAM also has low runtime and memory overheads.

4. The paper conducts ablation studies to evaluate the contribution of different components of DREAM. It also analyzes the impact of using different search priorities.

In summary, the key contribution is the novel DREAM system to automatically debug and repair common performance and accuracy bugs in AutoML systems by expanding optimization opportunities and leveraging detailed feedback. Both the system and the experimental evaluation methodology are valuable additions to the field.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Automated Machine Learning (AutoML)
- Performance bugs
- Ineffective search bugs 
- Search space expansion
- Feedback-driven search
- Feedback monitoring
- Debugging
- Repairing
- Model training
- Model evaluation
- Validation accuracy
- Search space
- Search strategy

The paper focuses on debugging and repairing common bugs in AutoML systems, specifically performance bugs where AutoML takes too much time to find a good model, and ineffective search bugs where AutoML is unable to find an accurate enough model. The proposed system, DREAM, monitors AutoML processes to detect these bugs and then repairs them through expanding the search space and using a feedback-driven search strategy that leverages detailed training and evaluation data. So the key ideas focus around AutoML, debugging/repairing, search space, feedback monitoring, etc. Those would be the main keywords and terminology associated with this paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes expanding the search space of AutoML systems to include additional model hyperparameters and training configurations. What are some of the challenges in determining what to include in this expanded search space? How can the search space expansion be done in a principled and systematic way?

2. The feedback-driven search strategy relies on summarizing the feedback data into architecture, convergence status, gradients, and weights. What are some alternative ways this data could be summarized or represented? How sensitive is the search strategy to these representations? 

3. The paper calculates conditional probabilities to determine action priorities for the feedback-driven search. What are some alternative ways to set these priorities? Could a learned model or evolutionary approach work better? What are the tradeoffs?

4. What types of bugs or issues could arise in the feedback monitoring component itself? How could the system detect and handle bugs in its own monitoring?

5. Could the search space expansion and feedback-driven search lead to overfitting on the validation data used during the AutoML search? How could this be detected or avoided?

6. The feedback-driven search relies on manually set thresholds for things like vanishing gradients. How sensitive is the performance to these thresholds? Could they be set automatically?

7. What types of models or datasets would be most challenging for the proposed system? When would it be likely to still struggle?

8. The experiments are limited to image datasets. How well would the methods transfer to other data types like text, time series data, etc? What modifications might be needed?

9. How does the proposed system compare to other AutoML debugging or repair methods? What are the limitations compared to these other approaches?

10. The paper focuses on model accuracy but designing metrics to balance accuracy and efficiency is an open challenge. How could the system incorporate efficiency and other objectives into the search?
