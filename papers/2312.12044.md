# [XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX](https://arxiv.org/abs/2312.12044)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Meta-reinforcement learning (meta-RL) methods require extensive pre-training on diverse tasks to achieve good generalization, making experimentation resource-intensive. 
- Existing meta-RL benchmarks use simple environments, preventing uncovering limits/scaling properties of algorithms.  
- XLand has complex procedural task generation but is not open-source. MiniGrid is simple and not scalable enough for meta-RL.

Solution:
- The authors introduce XLand-MiniGrid, a library of gridworld environments for meta-RL research in JAX for GPU/TPU acceleration.
- It combines procedural task diversity akin to XLand with visual simplicity of MiniGrid.
- Rules and goals are used to generate diverse tasks during episodes and trials.
- API is based on dm_env for compatibility with meta-RL algorithms.
- Observations use compact grid encoding instead of images.  

Contributions:
- Implemented MiniGrid classic environments and new meta-RL environments with 108 distinct tasks.
- Showed 107x speedup over MiniGrid, scaling to 2^13 parallel environments with 10M steps/sec on 1 GPU.  
- Demonstrated scaling to environments with >20 rules and larger grid sizes.
- The environments enable affordable large-scale meta-RL research.

In summary, the paper introduces the XLand-MiniGrid library of fast, scalable and diverse gridworld environments for meta-reinforcement learning research, enabling experimentation on complex tasks with limited compute resources.
