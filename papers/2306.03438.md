# [Large Language Models of Code Fail at Completing Code with Potential   Bugs](https://arxiv.org/abs/2306.03438)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How do large language models of code (LLMs) perform on the task of buggy code completion, where the code context contains potential bugs?The key hypothesis appears to be that the presence of potential bugs in the code context will significantly degrade the performance of LLMs on code completion. The authors define the novel problem of "buggy code completion" (bCC), where the model must complete unfinished code that contains potential bugs - code patterns that are likely to lead to bugs. They introduce two new datasets for systematically studying bCC with controlled and real-world potential bugs.Through experiments on bCC using the latest LLMs like CodeGen and InCoder, the authors find that:- The presence of even a single potential bug drastically reduces code completion performance, with test case pass rates dropping from 40-55% on clean code to below 5% on buggy code.- The buggy code context essentially destroys the benefits of providing partial code context, with pass rates being even worse than completing without any context.- Attempted methods to make the LLMs bug-aware do improve performance but still remain far below completion on clean code.So in summary, the central hypothesis that potential bugs will significantly degrade LLM performance on code completion is validated through the empirical results. The paper helps characterize and understand this previously unexplored problem setting.


## What is the main contribution of this paper?

The main contribution of this paper is defining and systematically studying the novel problem of buggy-code completion (BCC). Specifically:- The paper introduces and formulates the BCC problem, which is inspired by the practical scenario of using code completion with buggy code contexts. BCC involves generating functional code completions from contexts that contain potential bugs.- The paper constructs two new datasets for evaluating BCC: BuggyHumanEval and BuggyFixEval. BuggyHumanEval contains synthetic bugs derived from semantic-altering edits to solutions. BuggyFixEval contains more realistic bugs derived from incorrect and correct user submissions. - The paper empirically evaluates the BCC performance of state-of-the-art code language models like CodeGen and InCoder. It finds that the models' performance significantly degrades in the presence of potential bugs compared to normal code completion.- The paper investigates several post-hoc methods like removal, completion->rewriting, and rewriting->completion, for mitigating the negative effect of potential bugs. While these help, there remains a substantial performance gap compared to completion from clean contexts.- The paper provides ablation studies and case analyses for better understanding model behaviors on BCC.In summary, the key contribution is defining and conducting a systematic study of the new BCC problem to understand and improve code language models' robustness to potential bugs during code generation. The datasets, empirical results and analyses help establish BCC as an important open problem at the intersection of code generation and program repair.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper introduces a new code completion task called buggy-code completion (BCC), where the goal is to complete partial code snippets that contain potential bugs. The key findings are that the presence of potential bugs significantly degrades the performance of state-of-the-art code completion models like CodeGen and InCoder, reducing test case pass rates from 40-55% on clean code to 0.5-4% on buggy code. The paper also explores some simple techniques like removing or rewriting the potential bugs before completion, which improve performance but still leave a large gap compared to completing clean code. Overall, the main takeaway is that existing language models struggle at adapting to and handling potential bugs during code completion.
