# [Large Language Models of Code Fail at Completing Code with Potential   Bugs](https://arxiv.org/abs/2306.03438)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How do large language models of code (LLMs) perform on the task of buggy code completion, where the code context contains potential bugs?The key hypothesis appears to be that the presence of potential bugs in the code context will significantly degrade the performance of LLMs on code completion. The authors define the novel problem of "buggy code completion" (bCC), where the model must complete unfinished code that contains potential bugs - code patterns that are likely to lead to bugs. They introduce two new datasets for systematically studying bCC with controlled and real-world potential bugs.Through experiments on bCC using the latest LLMs like CodeGen and InCoder, the authors find that:- The presence of even a single potential bug drastically reduces code completion performance, with test case pass rates dropping from 40-55% on clean code to below 5% on buggy code.- The buggy code context essentially destroys the benefits of providing partial code context, with pass rates being even worse than completing without any context.- Attempted methods to make the LLMs bug-aware do improve performance but still remain far below completion on clean code.So in summary, the central hypothesis that potential bugs will significantly degrade LLM performance on code completion is validated through the empirical results. The paper helps characterize and understand this previously unexplored problem setting.
