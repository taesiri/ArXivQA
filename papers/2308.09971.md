# [Disposable Transfer Learning for Selective Source Task Unlearning](https://arxiv.org/abs/2308.09971)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new transfer learning paradigm called "disposable transfer learning" (DTL) to address the issue of unauthorized exploitation of pre-trained models through piggyback learning. The central hypothesis is that it is possible to selectively dispose of the source task knowledge from a transfer learned model while retaining the performance on the target task. This allows the model owner to protect the exclusive performance of their pre-trained model on the internal source data after adapting the model to a specific target task through transfer learning.The key research questions addressed are:- How can we selectively unlearn the source task knowledge after transfer learning? The paper proposes a novel "gradient collision" loss to achieve this.- How do we evaluate if source task knowledge is successfully disposed? The paper proposes using "piggyback learning accuracy" on source or other data as a metric. - Is the proposed DTL method effective? Experiments show DTL models have much lower piggyback learning accuracy while retaining target accuracy.In summary, the central hypothesis is around achieving selective unlearning for transfer learning models to prevent unauthorized piggybacking, and the key questions are around how to achieve and evaluate this novel DTL paradigm.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. It proposes a new transfer learning paradigm called disposable transfer learning (DTL), which allows selectively disposing of source task knowledge after transfer learning while retaining performance on the target task. 2. It introduces a novel loss function called gradient collision (GC) loss for scrubbing off source task knowledge during the knowledge disposal stage of DTL. GC loss minimizes inner-products of gradients from different source data examples to make their gradients collide/oppose each other.3. It proposes using piggyback learning accuracy (PL accuracy) as an evaluation metric for DTL models. PL accuracy measures the model's susceptibility to piggyback learning by fine-tuning on other datasets after DTL. Low PL accuracy implies successful unlearning of source knowledge.4. The experiments demonstrate that models trained with GC loss can retain target task performance comparable to typical transfer learning, while significantly reducing PL accuracy. This shows GC loss can effectively dispose of source knowledge to prevent piggyback learning of the model's pre-trained capabilities.In summary, the key innovation is proposing the DTL framework and GC loss to address the issue of protecting pre-trained model capabilities after transfer learning. The paper shows this can prevent piggyback learning, measured by the proposed PL accuracy metric.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on disposable transfer learning relates to other research in knowledge transfer and unlearning:- It tackles a novel problem setting of selectively unlearning source task knowledge after transfer learning. Most prior work has focused on catastrophic forgetting in continual learning or privacy-preserving unlearning of small subsets of training data. This paper addresses unlearning at a much larger scale and across heterogeneous datasets.- The proposed gradient collision (GC) loss takes a unique approach to inducing forgetting by minimizing inner products of sample gradients. This differs from common approaches like training on corrupted data or reversing gradient updates. The analysis shows GC loss effectively "collides" gradients to hinder learning.- Piggyback learning accuracy is introduced as a metric to evaluate unlearning. Rather than just measuring source task accuracy, it estimates transferability by fine-tuning on other datasets. This better captures susceptibility to unwanted knowledge transfer.- Efficient computation of GC loss using reformulated gradients and Hessian-vector products is presented. This enables scaling to large datasets and distributed training.- Experiments demonstrate GC loss outperforms alternative unlearning baselines at disposing source knowledge while retaining target accuracy. The analysis gives insights into the trade-off between degree of forgetting vs performance.Overall, this paper makes multiple novel contributions in terms of the problem formulation, technical approach, evaluation methodology, and experimental analysis. It opens up a new direction for unlearning in transfer learning contexts that contrasts with much of the existing work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a novel transfer learning paradigm called disposable transfer learning (DTL) which aims to selectively dispose of source task knowledge after completing transfer learning, in order to prevent unauthorized piggyback learning of the transfer-learned model; this is achieved through a new loss function called gradient collision loss.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Improving the unlearning objective: The authors mention their GC loss is a first step, but better integration of the knowledge retaining and unlearning objectives could be explored. Developing a unified objective rather than just a weighted sum of separate losses may lead to better performance.- Scaling up to larger datasets and models: The experiments in the paper were on relatively small image datasets like CIFAR-10/100. Testing disposable transfer learning on larger datasets and model architectures would be important future work.- Applications to real-world model publishing scenarios: The authors propose disposable transfer learning as a solution for model owners who want to publish their model while protecting their training data. Evaluating the approach on realistic model publishing use cases is needed. - Alternative knowledge disposal methods: The gradient collision loss is one approach to unlearning, but designing alternative objectives or modifications could further enhance knowledge disposal. Exploring different paradigms beyond just modifying the loss function may be fruitful.- Theoretical understanding: Providing formal guarantees or theoretical analysis of the properties of disposable transfer learning and the gradient collision loss could strengthen the approach.- Mitigating risks of gradient collision: The limitations section notes GC loss can degrade generalization performance in extreme cases, so developing methods to mitigate risks is important.In summary, the main suggestions are to scale up the approach, test on real applications, develop the theory and objectives further, and understand the limitations. Advancing disposable transfer learning along these directions can make it more practical and robust.


## Summarize the paper in one paragraph.

The paper "Disposable Transfer Learning for Selective Source Task Unlearning" proposes a novel transfer learning paradigm called disposable transfer learning (DTL). The key idea is to selectively unlearn the source task knowledge after adapting a pre-trained model to a target task, in order to prevent unwanted piggyback learning of the source knowledge. The authors introduce a new loss function called Gradient Collision (GC) loss to achieve source task unlearning. GC loss minimizes the inner products between sample gradients to make them collide. They also propose piggyback learning (PL) accuracy to quantify model susceptibility to piggybacking, measured by fine-tuning on other datasets. Experiments on image classification show GC loss successfully unlearns source knowledge while retaining target task performance, evidenced by significantly lower PL accuracy compared to baselines. The paper demonstrates DTL can prevent exploiting pre-trained models for piggyback learning, establishing compatibility between transfer learning and selective unlearning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes a novel transfer learning paradigm called disposable transfer learning (DTL) to address potential risks of exploiting pre-trained models after they are released. DTL aims to retain the performance on a target downstream task while disposing of the ability to perform the original source pre-training task. This prevents unauthorized users from exploiting the generic capabilities of the pre-trained model through techniques like piggyback learning. To achieve selective unlearning, the authors propose a new loss function called Gradient Collision (GC) loss which minimizes inner-products between sample gradients to "collide" them and hinder learning. They also introduce piggyback learning accuracy as a metric to evaluate how susceptible a model is to unauthorized fine-tuning after DTL. Experiments demonstrate that models trained with GC loss retain target accuracy while significantly reducing piggyback learning accuracy compared to baselines, indicating successful unlearning of the source task. An efficient implementation of GC loss is provided that allows distributed training. Overall, DTL with GC loss provides an effective approach for preventing leakage of pre-trained representations after transfer learning while adapting the model only for an authorized downstream task. The proposed paradigm and GC loss enables making transfer learning and unlearning compatible.
