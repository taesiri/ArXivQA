# [Disposable Transfer Learning for Selective Source Task Unlearning](https://arxiv.org/abs/2308.09971)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new transfer learning paradigm called "disposable transfer learning" (DTL) to address the issue of unauthorized exploitation of pre-trained models through piggyback learning. The central hypothesis is that it is possible to selectively dispose of the source task knowledge from a transfer learned model while retaining the performance on the target task. This allows the model owner to protect the exclusive performance of their pre-trained model on the internal source data after adapting the model to a specific target task through transfer learning.The key research questions addressed are:- How can we selectively unlearn the source task knowledge after transfer learning? The paper proposes a novel "gradient collision" loss to achieve this.- How do we evaluate if source task knowledge is successfully disposed? The paper proposes using "piggyback learning accuracy" on source or other data as a metric. - Is the proposed DTL method effective? Experiments show DTL models have much lower piggyback learning accuracy while retaining target accuracy.In summary, the central hypothesis is around achieving selective unlearning for transfer learning models to prevent unauthorized piggybacking, and the key questions are around how to achieve and evaluate this novel DTL paradigm.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. It proposes a new transfer learning paradigm called disposable transfer learning (DTL), which allows selectively disposing of source task knowledge after transfer learning while retaining performance on the target task. 2. It introduces a novel loss function called gradient collision (GC) loss for scrubbing off source task knowledge during the knowledge disposal stage of DTL. GC loss minimizes inner-products of gradients from different source data examples to make their gradients collide/oppose each other.3. It proposes using piggyback learning accuracy (PL accuracy) as an evaluation metric for DTL models. PL accuracy measures the model's susceptibility to piggyback learning by fine-tuning on other datasets after DTL. Low PL accuracy implies successful unlearning of source knowledge.4. The experiments demonstrate that models trained with GC loss can retain target task performance comparable to typical transfer learning, while significantly reducing PL accuracy. This shows GC loss can effectively dispose of source knowledge to prevent piggyback learning of the model's pre-trained capabilities.In summary, the key innovation is proposing the DTL framework and GC loss to address the issue of protecting pre-trained model capabilities after transfer learning. The paper shows this can prevent piggyback learning, measured by the proposed PL accuracy metric.
