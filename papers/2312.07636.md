# [Go beyond End-to-End Training: Boosting Greedy Local Learning with   Context Supply](https://arxiv.org/abs/2312.07636)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Standard end-to-end (E2E) training of deep neural networks requires storing intermediate activations for backpropagation, resulting in large memory footprint on GPUs and restricted model parallelization.
- An alternative is greedy local learning which partitions the network into gradient-isolated modules and trains them independently under local supervision. However, as the number of partitions increases, performance degrades substantially, severely limiting its scalability. 

Proposed Solution: 
- Theoretically analyze greedy local learning from an information theory perspective. Show that irreversible loss of task-relevant mutual information is the crucial bottleneck.  
- Propose Context Supply (ContSup) scheme to incorporate additional context between isolated modules to compensate for information loss. Allows modules to retain more information and boost performance.

Main Contributions:
- Identify the information bottleneck in greedy local learning that causes performance degradation as number of partitions increases.
- Propose Context Supply scheme to provide supplementary context between modules to mitigate information loss.
- Achieve state-of-the-art greedy local learning performance on CIFAR-10, SVHN and STL-10 datasets, significantly boosting accuracy and scalability.
- Show ContSup reduces GPU memory footprint substantially (up to 16x) while maintaining high accuracy compared to end-to-end training.
- Simple scheme that can flexibly extend topological connections across modules and combine with other greedy learning methods.

In summary, the paper provides valuable theoretical analysis to uncover the bottleneck in greedy local learning methods. The proposed Context Supply scheme effectively addresses this bottleneck by supplying contextual information across modules. Experiments show ContSup achieves excellent efficiency in terms of accuracy, scalability and memory footprint.
