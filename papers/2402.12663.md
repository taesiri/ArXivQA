# [SoftQE: Learned Representations of Queries Expanded by LLMs](https://arxiv.org/abs/2402.12663)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Query expansion methods using large language models (LLMs) like GPT-3 can improve performance of dense retrievers. However, adding an LLM to a real-time search pipeline is often too expensive. 

Proposed Solution:
- The authors propose SoftQE, which incorporates knowledge from LLMs into query encoders without needing the LLM at inference time.

- SoftQE maps input query embeddings to embeddings of LLM-expanded queries. The expanded queries are generated offline using an LLM. 

- A teacher encoder is first trained using the expanded queries. SoftQE then trains a student encoder to align representations of original and expanded queries, in addition to a standard contrastive loss.

Main Contributions:

- SoftQE performs comparably or better than strong baselines like DPR, SimLM and E5 on in-domain datasets like MS MARCO and TREC DL.

- On out-of-domain BEIR datasets, SoftQE improves performance over baselines by 2.83 percentage points on average. This shows improved generalization.

- SoftQE circumvents dependency on LLMs at inference time, avoiding increased cost/latency.

- The gains from SoftQE diminish when combined with cross-encoder distillation, but still yield improvements in the zero-shot setting.

- This is the first work to distill representations through natural language generation, which could inspire efficient solutions for other tasks.
