# [MVHuman: Tailoring 2D Diffusion with Multi-view Sampling For Realistic   3D Human Generation](https://arxiv.org/abs/2312.10120)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Generating photo-realistic 3D human models from text prompts is challenging due to the lack of diverse 3D human datasets for training diffusion models directly in 3D. Existing methods rely on fine-tuning or distilling 2D diffusion models, but they produce oversaturated textures, are inefficient, or fail to generate complete and proportional human bodies.

Method: 
The paper proposes MVHuman, a novel scheme to generate 3D human radiance fields using multi-view images sampled from a pre-trained 2D diffusion model without any fine-tuning. 

The key idea is a tailored multi-view sampling process that carefully constructs "consistency-guided noises" to replace the originally predicted noises in the sampling steps. This forces the sampling processes to gradually denoise the initial random noises into consistent ones across views. Specifically, at each step, they warp and blend the predicted original signals from adjacent views to obtain a consistent prediction, which is then transformed back to replace the original noises.

Additionally, an optimization on the latent codes and a modification of the self-attention block are proposed to further improve cross-view consistency.

With the resulting multi-view images, the paper refines the geometry using image cues and trains a radiance field. A two-stage neural blending scheme blending both full-body and closeup views is adopted to enable free-view rendering.

Main Contributions:

- Proposes a scheme to generate 3D human assets by directly leveraging a pre-trained 2D diffusion model through a tailored multi-view sampling process, avoiding costly fine-tuning or distillation.

- Introduces the concept of "consistency-guided noises" to gradually denoise the initial random noises into consistent ones across views, enabling consistent multi-view image generation.

- Demonstrates high-quality 3D human generation results that avoid common artifacts like oversaturation or geometric distortions, significantly outperforming previous arts.

In summary, the paper presents an alternative paradigm to effectively utilize 2D generative models for high-fidelity 3D human generation via novel view consistency constraints, without needing any 3D-specific model tuning or training.
