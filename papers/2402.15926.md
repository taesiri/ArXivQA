# [Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of   the Loss Improves Optimization Efficiency](https://arxiv.org/abs/2402.15926)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies gradient descent (GD) with a constant (and potentially large) step size for training linear and nonlinear predictors in classification tasks. Specifically, it considers logistic regression and two-layer neural networks under the neural tangent kernel (NTK) regime.

- Classical GD theory requires a small step size for guaranteed convergence, but in practice larger steps often work better. So there is a gap in understanding GD with large steps that lead to non-monotonic training loss. 

- The paper refers to such large-step GD that exhibits oscillatory loss as operating at the "edge of stability" (EOS). The goal is to understand if EOS-GD still converges, and if a large step confers any benefits.

Proposed Solution:
- The paper shows EOS-GD provably converges under linear separability assumptions. Moreover, EOS-GD transitions to a stable monotonic convergence phase within a finite number of iterations.

- EOS-GD attains a fast asymptotic convergence rate in the stable phase, leading to an overall accelerated optimization if the step size is set optimally to balance the EOS and stable phases. 

- The results are shown for logistic regression, general classification losses, and two-layer nonlinear networks under the NTK regime. Extensions to stochastic GD are also provided.

Main Contributions:
- Proof that constant-step GD converges under linear separability despite entering an initial EOS phase with oscillatory loss.

- Characterization of the convergence rate in the EOS phase and the stable monotonic phase. 

- Demonstration that the asymptotic rate in the stable phase scales favorably with larger step sizes.

- An optimal step size allocation to balance the EOS and stable phases leads to an overall acceleration effect.

- Versatile analysis framework that applies to general losses, nonlinear neural networks, and stochastic gradient descent.
