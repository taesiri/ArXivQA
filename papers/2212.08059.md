# [Rethinking Vision Transformers for MobileNet Size and Speed](https://arxiv.org/abs/2212.08059)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

Can we design a transformer-based vision model that is simultaneously lightweight, fast, and high-performance for efficient deployment on mobile devices?

The key points are:

- Existing transformer models like ViT tend to have higher latency and more parameters than lightweight CNNs like MobileNet, making them less suited for mobile devices where latency and size are crucial. 

- The authors aim to create a transformer model that matches MobileNet's size and speed while exceeding its accuracy.

- They propose a new model called EfficientFormerV2 that achieves this through novel architectural designs and a joint size-speed architecture search method.

In summary, the paper investigates whether it's possible to create a mobile-friendly vision transformer that is comparable to MobileNet in size and speed but with higher accuracy, which would be useful for deploying CV models on resource-constrained mobile devices. The EfficientFormerV2 model is their proposed solution.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel vision transformer architecture called MobileViT that is efficient and suitable for mobile deployments. The key points are:

- They introduce a hybrid CNN-Transformer model that leverages lightweight MobileNet blocks in the early stages to extract features efficiently, while using attention blocks in later stages to model global dependencies. 

- They employ several techniques to improve efficiency such as using depthwise separable convolutions, avoiding patch embeddings by using MobileNet as the backbone, and limiting the sequence length by applying attention only at later stages.

- They perform extensive experiments on ImageNet image classification task and show MobileViT achieves 74.7% top-1 accuracy with only 2.3M parameters. This is better than MobileNetV3 with similar model size.

- They also benchmark the latency on a Pixel 4 phone and show MobileViT is faster than vision transformers like ViT and DeiT, and close to lightweight CNN models like MobileNetV2 and MobileNetV3.

- The efficiency comes from the hybrid design that combines strengths of both CNNs and transformers. MobileNet extracts local features efficiently in early stages, while attention in later stages provides global contextual modeling.

In summary, the main contribution is proposing MobileViT, an efficient vision transformer suitable for mobile devices by combining MobileNet CNN backbone with transformer encoder in a hybrid design. Extensive experiments show it achieves better accuracy and efficiency than prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel mobile-friendly vision transformer backbone called EfficientFormerV2 that achieves state-of-the-art accuracy on ImageNet with MobileNet-level model size and speed by rethinking inefficient designs in transformers and applying a fine-grained joint search to optimize for both number of parameters and latency.
