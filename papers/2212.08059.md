# [Rethinking Vision Transformers for MobileNet Size and Speed](https://arxiv.org/abs/2212.08059)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

Can we design a transformer-based vision model that is simultaneously lightweight, fast, and high-performance for efficient deployment on mobile devices?

The key points are:

- Existing transformer models like ViT tend to have higher latency and more parameters than lightweight CNNs like MobileNet, making them less suited for mobile devices where latency and size are crucial. 

- The authors aim to create a transformer model that matches MobileNet's size and speed while exceeding its accuracy.

- They propose a new model called EfficientFormerV2 that achieves this through novel architectural designs and a joint size-speed architecture search method.

In summary, the paper investigates whether it's possible to create a mobile-friendly vision transformer that is comparable to MobileNet in size and speed but with higher accuracy, which would be useful for deploying CV models on resource-constrained mobile devices. The EfficientFormerV2 model is their proposed solution.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel vision transformer architecture called MobileViT that is efficient and suitable for mobile deployments. The key points are:

- They introduce a hybrid CNN-Transformer model that leverages lightweight MobileNet blocks in the early stages to extract features efficiently, while using attention blocks in later stages to model global dependencies. 

- They employ several techniques to improve efficiency such as using depthwise separable convolutions, avoiding patch embeddings by using MobileNet as the backbone, and limiting the sequence length by applying attention only at later stages.

- They perform extensive experiments on ImageNet image classification task and show MobileViT achieves 74.7% top-1 accuracy with only 2.3M parameters. This is better than MobileNetV3 with similar model size.

- They also benchmark the latency on a Pixel 4 phone and show MobileViT is faster than vision transformers like ViT and DeiT, and close to lightweight CNN models like MobileNetV2 and MobileNetV3.

- The efficiency comes from the hybrid design that combines strengths of both CNNs and transformers. MobileNet extracts local features efficiently in early stages, while attention in later stages provides global contextual modeling.

In summary, the main contribution is proposing MobileViT, an efficient vision transformer suitable for mobile devices by combining MobileNet CNN backbone with transformer encoder in a hybrid design. Extensive experiments show it achieves better accuracy and efficiency than prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel mobile-friendly vision transformer backbone called EfficientFormerV2 that achieves state-of-the-art accuracy on ImageNet with MobileNet-level model size and speed by rethinking inefficient designs in transformers and applying a fine-grained joint search to optimize for both number of parameters and latency.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other research on efficient vision transformers:

- Focus on Mobile Deployment: This paper focuses specifically on designing vision transformers that are lightweight and fast for mobile deployment. Many other works aim to improve transformer efficiency more generally, not just for mobile.

- Joint Optimization of Size and Speed: A key contribution is jointly optimizing for both small model size and fast inference speed. Many prior works optimize just one metric like FLOPs or parameters. Optimizing one alone can hurt the other.

- New Architectural Designs: The paper proposes novel mobile-friendly transformer architecture changes like unified FFN, stride attention, and dual-path attention downsampling. Many other papers build directly off models like ViT or DeiT without major architectural redesign.

- Fine-Grained Search: A fine-grained search algorithm is used to optimize depth, width, and expansion ratios. This enables more flexible distribution of parameters and computations compared to coarse-grained search in other works.

- Achieves MobileNet-Level Efficiency: The EfficientFormerV2 models finally achieve MobileNet-level efficiency, being similarly fast and small while having much better accuracy. Prior efficient vision transformers are still slower or much larger than MobileNet.

- Downstream Task Results: The paper includes promising detection, segmentation, and classification results. Some other efficient transformer papers focus only on ImageNet classification.

In summary, this paper makes transformers practically efficient on mobile through novel designs and search techniques. The joint optimization for size and speed is an important contribution over prior works.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Exploring other critical metrics besides model size and speed, such as memory footprint and CO2 emissions. The authors focused on optimizing for size and speed in this work, but suggest applying their joint optimization methodology to other important metrics as well.

- Adapting the preference or importance weights between metrics in the joint optimization objective. In this work, they set size and speed as equally important to calculate the MES score. However, they note that in industry scenarios the preferences may vary, so adapting the weights could be useful.

- Extending the fine-grained joint search approach to other transformer architectures and tasks beyond image classification. The authors demonstrated their search method on the EfficientFormerV2 architecture for image classification, but suggest it could be beneficial to apply it more broadly to other vision transformer architectures and tasks.

- Studying how to effectively transfer the architectures found through joint search on image classification to downstream tasks. The authors showed promising results transferring EfficientFormerV2 to detection and segmentation, but suggest further exploration of transfer learning for efficient vision transformers.

- Continuing to improve training techniques like distillation for efficient vision transformers, since model performance depends heavily on training.

- Exploring automated ways to search the model family under different hardware constraints, rather than manually designing each model variant.

In summary, the main future directions are extending the joint optimization approach to other metrics, tasks, and architectures, adapting the search methodology to new constraints, and continuing to improve training methods to further boost performance of efficient vision transformers. The authors propose joint search as a promising path forward for optimization.
