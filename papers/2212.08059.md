# [Rethinking Vision Transformers for MobileNet Size and Speed](https://arxiv.org/abs/2212.08059)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

Can we design a transformer-based vision model that is simultaneously lightweight, fast, and high-performance for efficient deployment on mobile devices?

The key points are:

- Existing transformer models like ViT tend to have higher latency and more parameters than lightweight CNNs like MobileNet, making them less suited for mobile devices where latency and size are crucial. 

- The authors aim to create a transformer model that matches MobileNet's size and speed while exceeding its accuracy.

- They propose a new model called EfficientFormerV2 that achieves this through novel architectural designs and a joint size-speed architecture search method.

In summary, the paper investigates whether it's possible to create a mobile-friendly vision transformer that is comparable to MobileNet in size and speed but with higher accuracy, which would be useful for deploying CV models on resource-constrained mobile devices. The EfficientFormerV2 model is their proposed solution.
