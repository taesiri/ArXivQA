# [Certified Patch Robustness via Smoothed Vision Transformers](https://arxiv.org/abs/2110.07719v1)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how to improve certified defenses against adversarial patch attacks using vision transformers (ViTs). The main hypothesis is that ViTs are better suited than convolutional neural networks (CNNs) for constructing certifiably robust models against patch attacks via smoothing-based defenses like derandomized smoothing. 

Specifically, the key claims made in this paper are:

- ViTs significantly outperform CNNs at classifying the image ablations used in derandomized smoothing, which leads to higher standard and certified accuracies.

- ViTs can simply drop masked tokens to avoid unnecessary computation on ablated regions of the image. This allows for substantially faster inference compared to CNNs when using smoothing to certify robustness.

- Combining these properties allows the construction of certifiably robust models using smoothed ViTs that have higher accuracy, faster inference, and improved guarantees compared to prior work based on smoothed CNNs.

In summary, the central hypothesis is that the token-based nature and attention mechanism of ViTs make them uniquely well-suited for building certifiably robust models against patch attacks when used within the smoothing framework. The experiments aim to demonstrate the superior performance of smoothed ViTs across metrics like accuracy, speed, and robustness guarantees.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Demonstrating that Vision Transformers (ViTs) are better suited than convolutional neural networks (CNNs) for certified patch defense using derandomized smoothing. Specifically, the paper shows that ViTs can maintain higher accuracy than CNNs on the heavily ablated images used in smoothing, and can process these ablated images much faster by dropping irrelevant tokens. 

2. Proposing a smoothed Vision Transformer model for certifiable adversarial patch robustness. This model achieves significantly higher certified robustness and standard accuracy compared to prior defenses based on CNNs.

3. Modifying the ViT architecture and smoothing procedure to reduce the computational cost of certification. This includes dropping masked tokens from ablated images and using strided ablations. Together, these modifications make the inference time of the smoothed ViT comparable to a standard non-robust CNN.

4. Conducting an in-depth analysis on how different choices in the smoothing procedure (ablation size, stride, type) affect the tradeoffs between robustness, standard accuracy, and inference speed. This provides guidance on how to optimize smoothed ViTs.

5. Achieving state-of-the-art certified robustness on ImageNet while maintaining standard accuracy comparable to non-robust models. The proposed smoothed ViT improves certified accuracy substantially (e.g. 13% on ImageNet) compared to prior work.

In summary, the key contribution is using Vision Transformers to enable certifiable defenses that have much higher accuracy and faster inference than prior certified patch defenses based on CNNs. The paper provides both modeling contributions (smoothed ViT) and detailed analysis to improve and understand certified patch robustness.
