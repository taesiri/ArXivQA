# Making Large Language Models Better Reasoners with Step-Aware Verifier

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enhance the reasoning capabilities of large language models through better prompting techniques. Specifically, the paper proposes a new method called DiVeRSe (Diverse Verifier on Reasoning Step) with three key innovations:1. Using diverse prompts to elicit different reasoning paths from the language model. The hypothesis is that different prompts can lead the model to explore different ways of reasoning to reach the correct answer. 2. Introducing a verifier to score the quality of each reasoning path and weigh their contributions through a voting scheme. The hypothesis is that not all reasoning paths generated by the model are equally reliable.3. Verifying each reasoning step individually and attributing the final answer's correctness to each step. The hypothesis is that some steps may be correct even if the final answer is wrong. Identifying these cases can further improve the reasoning process.In summary, the central hypothesis is that by increasing the diversity of reasoning paths, filtering them through a verifier, and analyzing them at a step-wise level, the reasoning capabilities of large language models can be significantly enhanced as measured on various benchmark datasets. DiVeRSe aims to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a novel method called DiVeRSe (Diverse Verifier on Reasoning Step) to enhance the reasoning abilities of large language models. The key ideas are:1. Using diverse prompts, instead of a single prompt, to elicit different reasoning paths from the model. This increases diversity and the chances of finding the correct reasoning path. 2. Introducing a verifier that scores each reasoning path to guide a voting mechanism for selecting the best final answer. This helps identify good reasoning paths from bad ones.3. Extending the verifier to judge each step of the reasoning path, rather than just the final answer. This allows attributing correctness/errors to individual steps. The method is evaluated on various reasoning tasks and consistently outperforms prior state-of-the-art methods, achieving new SOTA results on 6 out of 8 benchmarks. The step-wise verifier in particular provides interpretability into the model's reasoning process. Overall, DiVeRSe advances the reasoning capabilities of large language models through its innovations in using diverse prompts, weighted voting, and step-level verification.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method called DiVeRSe that improves the reasoning capabilities of large language models by using diverse prompts, a voting verifier, and step-wise verification of reasoning chains.
