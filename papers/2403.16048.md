# [Edit3K: Universal Representation Learning for Video Editing Components](https://arxiv.org/abs/2403.16048)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement:
The paper focuses on learning universal visual representations for six major types of video editing components, including video effects, animations, transitions, filters, stickers, and text. These editing components are commonly applied to raw image/video materials during video creation. Learning representations for these editing components is challenging because their visual appearances are entangled with the raw materials. Existing video representation learning methods do not distinguish between editing components and raw materials. 

Key Contributions:
1) Proposes the first large-scale dataset (Edit3K) with over 600K videos covering 3,094 editing components across the six categories. Each video contains a single disentangled editing component applied on image/video materials.

2) Benchmarks existing representation learning approaches like contrastive learning and masked auto-encoders, and shows they do not effectively distinguish editing components from raw materials.

3) Proposes a novel embedding guidance architecture with tailored contrastive learning formulation. The model provides embedding feedback to guide the encoder and uses a queue memory bank for extra negative samples.

4) Achieves new state-of-the-art for editing component retrieval/recognition on Edit3K dataset and transition recommendation on external AutoTransition dataset. Also shows superior embedding space via user studies.

5) Visualizations indicate the model focuses on editing components without explicit supervision. The work could benefit multiple video creation tasks involving editing components.

In summary, the paper introduces the problem of learning representations for atomic video editing components by disentangling them from raw materials through novel methodologies and a large-scale dataset. The effectiveness is shown quantitatively and qualitatively on various experiments and comparisons to existing techniques.
