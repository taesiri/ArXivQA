# [FedPerfix: Towards Partial Model Personalization of Vision Transformers   in Federated Learning](https://arxiv.org/abs/2308.09160)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we perform partial model personalization for Vision Transformers (ViTs) in the context of federated learning to improve performance on heterogeneous client data distributions?The key points related to this question are:- Previous work on partial model personalization in federated learning has focused on CNNs, leaving a gap for understanding how to apply it to ViTs.- ViTs have shown great success in centralized training, suggesting their potential benefits for federated learning. But how to best leverage ViTs in personalized federated learning is still an open question. - The paper empirically evaluates the sensitivity of different ViT components to data distribution shifts. This indicates the self-attention layers and classification head as most crucial for personalization.- The paper proposes a novel method called FedPerfix that adapts prefix tuning to personalize the sensitive ViT components while still leveraging global model information. - Experiments demonstrate FedPerfix achieves state-of-the-art performance for ViT-based personalized federated learning across diverse datasets.In summary, the central hypothesis is that selectively personalizing the self-attention layers and classification head of ViTs using techniques adapted from prefix tuning can improve performance on heterogeneous client data in federated learning. The paper provides empirical analysis to identify the sensitive components of ViTs and proposes the FedPerfix method to address this question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It performs an empirical study to determine which layers of a Vision Transformer (ViT) model are most sensitive to data distribution changes in the context of personalized federated learning. By evaluating the performance impact of keeping different types of layers localized, they find the self-attention layers and classification head are most crucial.2. It proposes a novel partial model personalization method called FedPerfix that is specifically designed for ViTs. FedPerfix leverages the idea of prefix tuning from transfer learning to enable efficient adaptation of the sensitive self-attention layers to each client's local data distribution. 3. It evaluates FedPerfix on image classification datasets with different degrees of heterogeneity. The results demonstrate state-of-the-art performance compared to other personalized federated learning methods, while requiring fewer resources.In summary, the key contributions are performing an empirical study to locate sensitive parts of ViT for personalization, proposing a novel approach called FedPerfix to efficiently personalize those parts using prefixes, and experimentally validating its effectiveness and efficiency compared to existing methods. The work provides valuable insights into personalizing ViTs for federated learning.
