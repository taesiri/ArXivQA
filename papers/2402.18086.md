# [Generalizable Two-Branch Framework for Image Class-Incremental Learning](https://arxiv.org/abs/2402.18086)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Deep neural networks suffer from catastrophic forgetting - when learning new information, they quickly forget previously learned information. This is a major challenge for continual learning, where models need to learn new concepts over time without forgetting old ones.

Proposed Solution: 
The paper proposes a two-branch framework consisting of a main branch (existing continual learning model) and a lightweight side branch (convolutional network). The output of each block in the main network is modulated by the output of a corresponding block in the side branch. This modulation results in sparse activations that alleviate catastrophic forgetting. The framework can enhance various existing continual learning approaches with CNN or vision transformer backbones with minor additions.

Key Contributions:

- Proposes a simple yet effective two-branch network framework, called G2B, that can enhance existing continual learning models by modulating activations to be sparse via a lightweight side branch.

- Achieves new state-of-the-art performance by combining G2B with existing models like DER and DyTox on CIFAR-100 and ImageNet datasets, demonstrating consistent improvements.

- Shows G2B is widely compatible, model agnostic as it improved regularization, rehearsal and dynamic expansion based continual learning methods with both CNNs and vision transformers.

- G2B adds minimal overhead, is trained end-to-end without any special optimization, and is insensitive to model size or depth of side branch. Thorough analysis validates contributions.

In summary, the paper presents an effective and easy-to-implement two-branch framework to enhance existing continual learning models, leading to new state-of-the-art results on multiple benchmarks. The modulation based sparse activation is a simple technique to alleviate catastrophic forgetting in incremental learning settings.
