# [Complexity Matters: Dynamics of Feature Learning in the Presence of   Spurious Correlations](https://arxiv.org/abs/2403.03375)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of learning under spurious correlations, where machine learning models tend to learn "shortcut" or spurious features that are correlated with the label but not causally related. Specifically, the paper investigates how the relative complexity and correlation strength of spurious vs core (causal) features impacts the dynamics of feature learning in neural networks. 

Prior works posit that spurious features are "simpler" to learn than core features, exhibiting a "simplicity bias", but the impact of simplicity on learning dynamics is not well studied. The paper aims to formally characterize simplicity using computational complexity, and rigorously study its effect on feature learning over time.

Proposed Solution:
The paper proposes using Boolean functions to represent spurious and core features in a synthetic dataset. Boolean functions like parity and threshold staircase allow precisely controlling feature complexity. The dataset contains a core feature that perfectly predicts the label, and a spurious feature correlated through a tunable parameter. 

Using this dataset, the paper conducts comprehensive experiments analyzing the dynamics of a 2-layer neural network trained with SGD and cross-entropy loss. The key findings are:

1. Simpler spurious features and higher correlation slow down core feature learning. There is a concave relationship between complexity and slowdown.

2. With staircase functions, core and spurious features are learned concurrently, challenging assumptions of separable learning phases.  

3. Spurious features persist in learned representations even after sufficient core feature learning, especially for simpler spurious features.

4. Last layer retraining works by reducing reliance on the spurious subnetwork. Adding balanced data steadily improves robustness.

The paper further uses theoretical analysis for parity+XOR case to justify observations 1 and 3 regarding slowdown and spurious feature persistence.

Contributions:
The key contributions are - (i) New Boolean dataset allowing fine-grained control over feature complexity; (ii) Extensive experiments uncovering interesting dynamics of core vs spurious feature learning; (iii) Theoretical analysis providing insight into empirically observed behaviors; (iv) Implications justifying last layer retraining and limitations of existing debiasing algorithms.

The work enhances understanding of how relative complexity of features impacts learning under spurious correlation, with both empirical and theoretical evidence.
