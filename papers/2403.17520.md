# [Boosting Adversarial Training via Fisher-Rao Norm-based Regularization](https://arxiv.org/abs/2403.17520)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adversarial training methods like PGD-AT, TRADES, etc. improve model robustness but often degrade standard accuracy. Mitigating this tradeoff remains an open challenge. 

- Prior work has attributed this to factors like training bias, insufficient data, or architecture. But the root cause lies in model complexity which encompasses all these factors.

- This motivates exploring the degradation of standard generalization in adversarial training through the lens of model complexity.


Methodology:
- For ReLU MLPs, the paper leverages Fisher-Rao norm to establish bounds on Rademacher complexity wrt cross-entropy loss. This reveals dependence on a complexity-related variable Γce.

- Γce shows sensitivity to model width and adversarial training objectives. It positively correlates with the generalization gap early in training, and negatively later on.

- Building on this observation, a novel regularization framework called Logit-Oriented Adversarial Training (LOAT) is proposed. 


Contributions:
- Established theoretical connections between model complexity, Fisher-Rao norm and standard generalization in adversarial training.  

- Identified a complexity-related variable Γce that correlates with generalization gap in an epoch-dependent manner.

- Proposed LOAT framework that combines logit regularization and adaptive adversarial logit pairing for different stages of training.

- Showed LOAT can boost performance of various adversarial training methods like PGD-AT, TRADES etc. on diverse architectures, with negligible overhead.
