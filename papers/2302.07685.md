# [Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to develop an efficient video generative model that can synthesize high-quality, realistic videos. Specifically, the paper aims to address the key challenges in video generation of handling the high dimensionality and complexity of video data while being scalable and computationally efficient. The main hypothesis is that representing videos in a low-dimensional latent space and modeling the distribution with diffusion models can lead to an effective and efficient generative model for high-resolution, temporally coherent video synthesis. The key ideas are:1) Designing an autoencoder to project videos into a low-dimensional latent space with three 2D image-like latent vectors instead of a 3D tensor. This factorization allows efficiently capturing the spatial and temporal variations. 2) Developing a diffusion model architecture specialized for the proposed latent space that relies on 2D convolutions instead of 3D, enabling efficient training and sampling. 3) Jointly training the model to generate arbitrary length videos by learning unconditional generation of clips and conditional generation between consecutive clips.Overall, the central hypothesis is that the proposed projected latent space paired with an efficient diffusion model architecture can enable scalable, high-quality video synthesis while overcoming the computation and memory bottlenecks of prior video generative models. The experiments aim to validate the effectiveness and efficiency of the proposed model called Projected Latent Video Diffusion Model (PVDM).


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel latent diffusion model called Projected Latent Video Diffusion Model (PVDM) for efficient and high-quality video generation. The key ideas are:- An autoencoder that projects a video into three 2D image-like latent vectors by factorizing the complex 3D structure of videos. This allows efficient diffusion model design. - A diffusion model architecture specialized for the 2D latent space that avoids 3D convolutions and enables video generation under limited compute.- A training strategy for the diffusion model to generate videos of arbitrary lengths.Specifically, the autoencoder projects the video along each axis into 2D latents to capture common contents, vertical motion, and horizontal motion. The diffusion model is designed with 2D convolutions on the image-like latents. It is trained to model the joint distributions of unconditional and conditional generation. This allows generating longer videos by conditioning on previous frames.Experiments show PVDM improves efficiency and video quality over prior arts. For example, it reduces the FVD score by 1773 on a 128-frame UCF-101 benchmark and improves Inception Score to 74.4. It also has lower memory requirements and faster sampling than recent video diffusion models.In summary, the main contribution is an efficient latent diffusion model for high-quality and scalable video synthesis, enabled by novel projected 2D latent representations of videos.
