# [Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to develop an efficient video generative model that can synthesize high-quality, realistic videos. Specifically, the paper aims to address the key challenges in video generation of handling the high dimensionality and complexity of video data while being scalable and computationally efficient. The main hypothesis is that representing videos in a low-dimensional latent space and modeling the distribution with diffusion models can lead to an effective and efficient generative model for high-resolution, temporally coherent video synthesis. The key ideas are:1) Designing an autoencoder to project videos into a low-dimensional latent space with three 2D image-like latent vectors instead of a 3D tensor. This factorization allows efficiently capturing the spatial and temporal variations. 2) Developing a diffusion model architecture specialized for the proposed latent space that relies on 2D convolutions instead of 3D, enabling efficient training and sampling. 3) Jointly training the model to generate arbitrary length videos by learning unconditional generation of clips and conditional generation between consecutive clips.Overall, the central hypothesis is that the proposed projected latent space paired with an efficient diffusion model architecture can enable scalable, high-quality video synthesis while overcoming the computation and memory bottlenecks of prior video generative models. The experiments aim to validate the effectiveness and efficiency of the proposed model called Projected Latent Video Diffusion Model (PVDM).
