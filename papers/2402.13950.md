# [Making Reasoning Matter: Measuring and Improving Faithfulness of   Chain-of-Thought Reasoning](https://arxiv.org/abs/2402.13950)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent work has shown that large language models (LLMs) perform better on reasoning tasks when asked to provide step-by-step reasoning (chain-of-thought or CoT) before answering. 
- However, it is unclear whether the LLMs' final answers actually follow from or are "faithful" to the reasoning chains they provide. 
- There is a gap in interpreting the relationship between the generated CoT traces and the final answers.

Proposed Solution:
- The paper performs a causal mediation analysis on 12 LLMs to quantify the causal effect of the CoT traces on the final answers. 
- They find significant variation across models and tasks in how strongly the CoT traces affect the final predictions.
- They propose a new method called Frodo that trains small LM modules to:
  - Reliably generate correct reasoning chains (inference module)
  - Faithfully reason over those chains to produce answers (reasoning module)

Key Contributions:
- First work to apply causal mediation analysis to measure faithfulness of LLM CoT reasoning.
- Empirically demonstrates that LLMs do not reliably use their own CoT when generating answers.
- Proposes Frodo method to improve correctness and faithfulness of CoT reasoning in small LMs.
- Experiments show Frodo outperforms baselines on accuracy, robustness to counterfactuals, and out-of-distribution generalization.
- Provides insights into designing models that can reason in a more understandable and trustworthy way.
