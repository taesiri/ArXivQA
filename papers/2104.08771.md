# [Cross-Attention is All You Need: Adapting Pretrained Transformers for   Machine Translation](https://arxiv.org/abs/2104.08771)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How powerful is cross-attention in adapting pretrained transformers for machine translation to new language pairs? Specifically, the paper investigates the role of cross-attention layers when fine-tuning a pretrained machine translation model on a new language pair. It examines whether fine-tuning just the cross-attention layers can achieve comparable performance to fine-tuning the full model. The key hypotheses are:1) Fine-tuning only the cross-attention layers, while freezing the encoder and decoder modules, can adapt well to new language pairs and achieve close to full model fine-tuning performance. 2) The pretrained values of the cross-attention layers, encoding translation knowledge, are important for effective fine-tuning.3) Fine-tuning only cross-attention results in the new language embeddings becoming aligned with the original embeddings, unlike full fine-tuning.Through experiments fine-tuning pretrained French-English models on new language pairs, the paper provides empirical evidence supporting these hypotheses. Overall, it aims to demonstrate the power of cross-attention for transfer learning in machine translation and gain insights into its crucial role.
