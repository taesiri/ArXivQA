# [Cross-Attention is All You Need: Adapting Pretrained Transformers for   Machine Translation](https://arxiv.org/abs/2104.08771)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How powerful is cross-attention in adapting pretrained transformers for machine translation to new language pairs? Specifically, the paper investigates the role of cross-attention layers when fine-tuning a pretrained machine translation model on a new language pair. It examines whether fine-tuning just the cross-attention layers can achieve comparable performance to fine-tuning the full model. The key hypotheses are:1) Fine-tuning only the cross-attention layers, while freezing the encoder and decoder modules, can adapt well to new language pairs and achieve close to full model fine-tuning performance. 2) The pretrained values of the cross-attention layers, encoding translation knowledge, are important for effective fine-tuning.3) Fine-tuning only cross-attention results in the new language embeddings becoming aligned with the original embeddings, unlike full fine-tuning.Through experiments fine-tuning pretrained French-English models on new language pairs, the paper provides empirical evidence supporting these hypotheses. Overall, it aims to demonstrate the power of cross-attention for transfer learning in machine translation and gain insights into its crucial role.


## What is the main contribution of this paper?

The main contribution of this paper is showing the power of cross-attention in the Transformer architecture for machine translation, specifically in the context of transfer learning. The key findings are:- Fine-tuning only the cross-attention parameters of a pretrained translation model is nearly as effective as fine-tuning the entire model when adapting to translate a new language pair. This suggests cross-attention alone can effectively adapt to the new task while the encoder and decoder are frozen.- Fine-tuning solely cross-attention results in the new language embeddings becoming aligned with the corresponding embeddings in the pretrained model. This alignment effect does not happen when fine-tuning the full model.- The aligned embeddings can be utilized for mitigating catastrophic forgetting of the original task and enabling zero-shot translation between languages aligned to a common parent language.- Fine-tuning just cross-attention is a more lightweight way to adapt models to new languages compared to full fine-tuning, requiring fewer parameters to be updated and stored.In summary, the paper shows the power and importance of cross-attention for transfer learning in machine translation, and demonstrates how the resulting aligned embeddings can be exploited in useful ways. The constrained fine-tuning approach also has practical advantages for model adaptation.
