# [Diversifying Knowledge Enhancement of Biomedical Language Models using   Adapter Modules and Knowledge Graphs](https://arxiv.org/abs/2312.13881)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT have led to great advances in NLP but still struggle with domain-specific language and lack structured knowledge. This is especially problematic in complex domains like biomedicine. 
- Methods exist to inject knowledge graphs into PLMs but often require full fine-tuning which is compute-intensive. Lightweight approaches like adapters are promising but under explored.

Methodology:  
- Use adapter modules inserted into PubMedBERT and BioLinkBERT to inject biomedical knowledge from UMLS and OntoChem knowledge graphs.
- Partition graphs into smaller subgraphs, train an adapter module for each using masked entity prediction. 
- Fuse adapters together with AdapterFusion to get full knowledge representation.
- Test on 4 downstream biomedical NLP tasks: document classification, question answering, textual entailment.

Results:
- Knowledge-enhanced models boost performance in several cases, especially on the question answering datasets PubMedQA (up to +7%) and BioASQ7b (+3%).
- Computationally lightweight approach - only adapter parameters trained, approx 1-2% of full PLM size.  
- Analysis shows models can correctly answer questions by relying on injected factual knowledge about entities.
- Demonstrate OntoChem is a viable alternative knowledge source to UMLS.

Contributions:
- Novel way to inject biomedical knowledge graphs into PLMs using adapters. Computationally efficient.
- Thorough experiments on multiple models, tasks and knowledge sources. New SOTA on BioASQ7b. 
- In-depth analysis of resulting knowledge integration and prediction capabilities.
- First use of OntoChem knowledge graph to enhance NLP models.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper develops an approach to improve biomedical language models by injecting domain knowledge from structured knowledge graphs into the models using lightweight adapter modules, demonstrating performance improvements on downstream biomedical NLP tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is developing an approach to inject structured biomedical knowledge from knowledge graphs into pre-trained language models using lightweight adapter modules. Specifically:

- They utilize two biomedical knowledge graphs (UMLS and OntoChem) and partition them into smaller subgraphs for more efficient training.

- They inject the knowledge from these graphs into two pre-trained biomedical language models (PubMedBERT and BioLinkBERT) using adapter modules, which avoids catastrophic forgetting.

- They demonstrate improved performance on several downstream biomedical NLP tasks like document classification, question answering, and natural language inference. 

- They provide a detailed analysis of the results, including a qualitative analysis showing how the injected knowledge helps the model answer questions correctly.

- They demonstrate that the lesser-used OntoChem knowledge graph is a viable alternative knowledge source for enhancing biomedical language models.

In summary, the main contribution is an effective methodology to inject structured knowledge into language models to improve performance on biomedical tasks, using lightweight adapters and lesser-used knowledge sources.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key keywords and terms associated with it include:

- Natural Language Processing (NLP)
- Pre-trained language models
- Knowledge Graphs
- Domain Knowledge 
- Knowledge Enhancement
- Adapters
- Biomedicine 
- Biomedical NLP
- Unified Medical Language System (UMLS)
- OntoChem Ontology
- Document classification
- Question answering
- Natural language inference

The paper focuses on using adapter modules to inject structured biomedical knowledge from knowledge graphs like UMLS and OntoChem into pre-trained language models like PubMedBERT and BioLinkBERT. It tests the performance of these knowledge-enhanced models on downstream biomedical NLP tasks like document classification, question answering, and natural language inference. The key goal is to bridge the gap between large language models and domain-specific biomedical knowledge.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper utilizes graph partitioning before injecting knowledge into the language models. What are the benefits of partitioning knowledge graphs instead of using them directly? How does the partitioning algorithm work? 

2. The paper experiments with two different versions of the OntoChem knowledge graph - one with fused relations and one with typed relations. What is the difference between these two versions and why might the typed relations perform better for certain datasets?

3. The paper inserts lightweight adapter modules into the architecture of pre-trained language models like PubMedBERT and BioLinkBERT. Explain in detail how these adapter modules work and what advantages they provide over fine-tuning the entire model.  

4. The paper shows performance improvements from using OntoChem as an external knowledge source. What type of entities and relations exist in OntoChem that make it a good knowledge source for biomedical NLP tasks? How does it compare to other popular biomedical knowledge graphs like UMLS?

5. The qualitative analysis provides two examples where the knowledge-enhanced model answers questions correctly unlike the vanilla model. Analyze these examples in depth - what exact knowledge might have been used from OntoChem or UMLS to arrive at the correct predictions?  

6. The paper utilizes an entity prediction pre-training task for the adapter modules instead of masked language modeling. Explain why this choice was made and how the entity prediction task works in detail. What are other possible pre-training objectives for adapters?

7. Explain the AdapterFusion layers used in the paper and how they combine adapter knowledge into the final model. How do they dynamically assign mixture weights to different adapters? What are the benefits of this approach?

8. The paper shows different performance gains across the datasets - significant gains in QA but smaller gains in NLI and document classification. Analyze the possible reasons why external knowledge has different utility across text understanding tasks.

9. The approach keeps the weights of the original PLMs frozen and only trains the small adapter networks. Explain the advantages of this strategy compared to conventional fine-tuning. How does it alleviate catastrophic forgetting? 

10. The paper utilizes a Pfeiffer adapter architecture instead of the Houlsby architecture. Compare and contrast these two adapter configurations and explain what factors influenced the choice in this work.
