# [Diversifying Knowledge Enhancement of Biomedical Language Models using   Adapter Modules and Knowledge Graphs](https://arxiv.org/abs/2312.13881)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT have led to great advances in NLP but still struggle with domain-specific language and lack structured knowledge. This is especially problematic in complex domains like biomedicine. 
- Methods exist to inject knowledge graphs into PLMs but often require full fine-tuning which is compute-intensive. Lightweight approaches like adapters are promising but under explored.

Methodology:  
- Use adapter modules inserted into PubMedBERT and BioLinkBERT to inject biomedical knowledge from UMLS and OntoChem knowledge graphs.
- Partition graphs into smaller subgraphs, train an adapter module for each using masked entity prediction. 
- Fuse adapters together with AdapterFusion to get full knowledge representation.
- Test on 4 downstream biomedical NLP tasks: document classification, question answering, textual entailment.

Results:
- Knowledge-enhanced models boost performance in several cases, especially on the question answering datasets PubMedQA (up to +7%) and BioASQ7b (+3%).
- Computationally lightweight approach - only adapter parameters trained, approx 1-2% of full PLM size.  
- Analysis shows models can correctly answer questions by relying on injected factual knowledge about entities.
- Demonstrate OntoChem is a viable alternative knowledge source to UMLS.

Contributions:
- Novel way to inject biomedical knowledge graphs into PLMs using adapters. Computationally efficient.
- Thorough experiments on multiple models, tasks and knowledge sources. New SOTA on BioASQ7b. 
- In-depth analysis of resulting knowledge integration and prediction capabilities.
- First use of OntoChem knowledge graph to enhance NLP models.
