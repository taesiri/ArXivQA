# [Loose LIPS Sink Ships: Asking Questions in Battleship with   Language-Informed Program Sampling](https://arxiv.org/abs/2402.19471)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies how people efficiently generate informative questions in grounded environments given cognitive constraints. Specifically, it looks at question asking in the context of the Battleship game, where a player is presented with a partially revealed board and asked to pose a question that would help reveal the locations of hidden ships. The key challenges are 1) grounding the questions in the state of the world (board), 2) translating questions from natural language into a symbolic language of thought to allow computation of utility, and 3) doing efficient search over the large space of possible questions subject to resource constraints. 

Proposed Solution:
The paper proposes a Language-Informed Program Sampling (LIPS) model that uses large language models (LLMs) to generate candidate questions in natural language, translate them into symbolic programs, and evaluate their expected information gain. Specifically:

1) The model samples k candidate questions from an LLM distribution conditioned on the board state. Two LLM architectures are explored: CodeLlama and GPT.

2) It translates the questions into programs in a domain specific language using another LLM.

3) The programs are executed against the board to compute the Expected Information Gain (EIG), which measures how many candidate boards are eliminated by the question on average.

4) The question with the highest EIG is selected after sampling k questions. By varying k, the amount of mental computation can be controlled.

Main Contributions:

- Shows that with just k=5 samples, LLM-based LIPS models can achieve human-level performance in predicting informative Battleship questions.

- Demonstrates how Bayesian models of active learning can be applied to natural language inputs and outputs by using LLMs to capture human priors.

- Highlights limitations of LLMs in grounding and coherence, even for state-of-the-art multimodal models like GPT-4V that fail to outperform non-visual baselines.

- Provides both a strong cognitive model for informative question asking and analysis of the strengths and weaknesses of modern LLMs as components in such models.
