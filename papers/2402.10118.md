# [Reusing Softmax Hardware Unit for GELU Computation in Transformers](https://arxiv.org/abs/2402.10118)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformers networks rely heavily on matrix multiplications and non-linear activation functions like softmax and GELU. These functions are commonly accelerated via specialized hardware units.
- However, current hardware implementations compute each function separately, lacking hardware reuse across functions. This leads to inefficiencies in area and power consumption.

Proposed Solution: 
- The paper shows how to map the computation of GELU to a softmax operator. This allows reusing efficient softmax hardware units to also compute GELU in parallel.
- A state-of-the-art softmax unit is incrementally modified to operate in two modes - normal mode for softmax, and GELU mode to compute multiple GELU outcomes in parallel by operating on multiple two-element sub-vectors.
- The modifications include adding capability to return multiple local maxima, multiple parallel logarithm units, and some multiplexing logic. This dual-mode unit incurs only ~10% area/power overhead.

- The dual-mode softmax unit is combined with multipliers and adders to form an overall GELU-Softmax computation unit that can produce multiple GELU outcomes in parallel by reusing the softmax hardware.

Main Contributions:
- Mathematical transformation to compute GELU via a two-element softmax operator
- Dual-mode softmax hardware design with 10% overhead that can operate on full vector or sub-vectors
- Combined hardware unit for parallel GELU and softmax computation, reusing softmax hardware
- 6.1% average area savings and 11.9% power savings with no loss of accuracy versus separate GELU and softmax units

In summary, the paper enables efficient hardware reuse across critical Transformer functions through a dual-mode unit, unlocking benefits in area, power and scalability.


## Summarize the paper in one sentence.

 This paper proposes a method to reuse existing softmax hardware units to also compute the GELU activation function in transformers, achieving similar accuracy as separate units while reducing hardware area and power consumption.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a combined GELU-softmax hardware unit that can reuse existing softmax hardware to also compute the GELU activation function. Specifically:

- The paper shows mathematically how GELU can be computed using a two-element softmax operation and some additional multiplication and additions. 

- It proposes modifications to an existing softmax hardware unit to make it configurable to operate on either a full input vector (normal softmax mode) or on multiple two-element sub-vectors (GELU mode). This allows the inherent parallelism and efficiency of the softmax unit to be leveraged for GELU.

- The combined GELU-softmax unit is shown to provide similar accuracy to separate GELU and softmax units in representative NLP applications, while reducing overall hardware area and power consumption. Savings of 3.8-8.4% for area and 10.7-13.2% for power are reported.

So in summary, the main contribution is an efficient dual-purpose hardware unit for softmax and GELU computation, enabling hardware reuse and savings in area and power for transformer implementations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's abstract and introduction, some of the key terms and concepts associated with this paper include:

- Transformers - The paper discusses improving the performance of transformers, which are a type of deep learning model used for natural language processing and computer vision tasks.

- GELU (Gaussian Error Linear Unit) - This is a non-linear activation function used within the feed-forward layers of transformers. The paper examines reusing softmax hardware to also compute GELU. 

- Softmax - Another activation function commonly used in transformers. The paper proposes reusing efficient softmax hardware units to also compute GELU in a hardware-efficient way.

- Hardware reuse/hardware efficiency - A key focus of the paper is enabling hardware reuse between the softmax and GELU computation units rather than having separate dedicated hardware for each function. This improves efficiency.

- Vectorization/parallelism - The paper aims to leverage the inherent parallel/vectorized nature of softmax units to also compute GELU in a parallel fashion.

- Approximation - The paper uses an approximation of GELU based on the tanh function, which enables formulating GELU computation via a two-element softmax.

- Natural language processing - Transformers and models like BERT are commonly used for NLP tasks. The paper evaluates accuracy for NLP applications.

In summary, the key themes are hardware reuse between softmax & GELU units, leveraging parallelism/vectorization, and approximating GELU via softmax to improve efficiency for transformer implementations.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper mentions modifying the adder tree in the softmax module to return partial sums. Can you elaborate on the specific changes made to the adder tree to enable this? 

2. You utilize piecewise linear approximations for the exponentiation operations. What was the process you followed to derive the PWL coefficients? How many segments did you use and what was the maximum error?

3. The paper states that division is performed in the logarithmic domain. Can you explain in more detail the architecture used for the logarithm and inverse logarithm conversions? 

4. What modifications were made to the maximum value identification unit to enable identifying maximums per pair in GELU mode? Did this require changes to the comparison tree?

5. In the results, you compare the proposed design against an implementation using separate i-GELU units. Could you have reused the polynomial computations of i-GELU instead of the extra multipliers/adders for $k$?

6. Fig. 3 shows multipliers between the softmax and the inputs $z_i$. Would pruning these multipliers using subword techniques be possible to further reduce area?

7. You focused on optimizing hardware complexity. Did you also analyze the impact on throughput/latency? If so, what tradeoffs exist?

8. Did you explore approximations for the computation of $k$ instead of using exact multipliers/adders? Could a LUT or PWL help further reduce area?

9. For future work, do you plan to support lower precisions (<16 bits) for the inputs? Would the PWL approximations need adjustment to maintain accuracy?

10. The technique maps well to AVX-512 vector instructions. Have you quantized potential performance speedups compared to current software implementations?
