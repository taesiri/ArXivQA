# [Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier](https://arxiv.org/abs/2403.09036)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Data often follows a long-tailed distribution where there are significantly more samples for head classes compared to tail classes. 
- This causes models trained on such data to be biased towards predicting head classes.
- The authors identify two causes of this bias: 
   1) Imbalanced ratio of positive and negative gradients (gradient ratio) for tail classes, causing them to be misclassified. 
   2) Imbalanced negative gradients from different classes, causing tail classes to be confused with one another.

Proposed Solution:
- Propose a Gradient-Aware Logit Adjustment (GALA) loss function that introduces two terms to balance gradients:
   1) Adjusts overall negative gradients to ensure an appropriate gradient ratio.
   2) Balances negative gradients from different classes.
- This balances the norms of classifier weights, gradient ratios, negative gradients, and similarity of tail class vectors to their class samples.

- Additionally propose a Prediction Re-balancing strategy to directly re-balance prediction probabilities to mitigate any remaining bias. Normalizes the predictions over classes.

Main Contributions:
- Identify how imbalanced accumulated gradients bias classifiers in long-tail learning.
- Propose GALA loss to regulate classifier weight gradients and balance optimization. Show theoretically it balances gradient ratios and negative distributions.
- Propose prediction re-balance strategy to mitigate remaining prediction bias regardless of its source.
- Achieve new state-of-the-art results on CIFAR100-LT, ImageNet-LT, Places-LT and iNaturalist datasets, surpassing previous methods by large margins.

In summary, the paper provides valuable insights into how imbalanced gradients undermine long-tail learning, and introduces two techniques to address the issues that achieve superior performance over prior art.
