# [A comparison of RL-based and PID controllers for 6-DOF swimming robots:   hybrid underwater object tracking](https://arxiv.org/abs/2401.16618)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Controlling 6DOF swimming robots for tasks like underwater object tracking is challenging due to complex dynamics and lack of models. Traditionally separate PID controllers are used but they have limitations in handling interactions between variables.

- RL controllers offer more flexibility but sample complexity and safety are concerns for real-world application.

Proposed Solution:
- Use a centralized Deep Q Network (DQN) to control pitch and yaw rates while retaining a PID for linear velocity. This combines strengths of both approaches.

- Formulate tracking task into separate vision and control modules for efficiency. Vision uses YOLOv7 and Kalman filter.  

- Design reward to keep target centered in image plane. Use double DQN with separate heads for pitch and yaw actions.  

- Systematically train RL agent starting with PID control, then gradual shift to DQN for safe exploration. Employ spiral search for target recovery.

Contributions:
- Feasibility study of replacing decentralized PIDs with centralized RL agent for 6DOF swimming robot control.

- Training framework to safely explore state-action space and enable online improvement of RL agent.

- Vision and control modularity with bounding box data exchange allows easy vision system swapping.

- Experiments in realistic simulator validate centralized RL over PIDs. RL agent adapts to changes and collaborates with vision system.

In summary, the key novelty is the centralized DQN-based control approach and associated training methodology tailored for 6DOF swimming robots. This is validated through simulated experiments to showcase improved performance over traditional PID control.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper explores using a centralized deep Q network controller in place of separate PID controllers for controlling the yaw/pitch rates of a 6DOF swimming robot for underwater object tracking, proposing a framework to safely train the RL agent leveraging existing controllers and showing through simulations that the learned policy outperforms PID control and can collaborate with the vision system.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. An examination of the feasibility of using a centralized RL controller in place of the conventional, segregated PID controllers for a 6DOF swimming robot.

2. The implementation of an online improvement loop that can effectively handle distribution shifts, uncertainties, and variations in system parameters.

3. The use of a single camera to directly generate pitch and yaw rates, thereby enabling the target view to remain centered without the need for additional state estimation.

4. A general framework for optimizing the training of an RL-based controller via the use of an existing classical control approach, simplifying the exploration of the state-action space.

So in summary, the main contribution is exploring and demonstrating the potential of using a centralized RL controller instead of separate PID controllers for improved performance and adaptability in controlling a complex 6DOF swimming robot for tasks like underwater object tracking. The paper also proposes a framework to safely and efficiently train such an RL agent.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords or key terms associated with this paper are:

- 6DOF swimming robot: The paper focuses on controlling a six degree-of-freedom (6DOF) swimming robot with multiple legs/flippers for maneuvering.

- Centralized controller: Instead of using separate PID controllers, the paper proposes using a single, centralized deep Q-network (DQN) to control the robot's pitch and yaw rates.

- Deep Q network (DQN): A reinforcement learning technique that is used to train the centralized controller to generate pitch and yaw commands.

- Underwater object tracking: The application domain is underwater tracking of objects like divers using the swimming robot.

- Online learning: The framework enables online improvement and adaptation of the DQN controller in real-time.

- Reward function: A key component of reinforcement learning that guides the model's learning - the paper defines a specific reward related to keeping the target centered in camera images.

- Simulation environment: The AquaSim simulator is used to evaluate the controllers in a 3D underwater setting. 

- Performance comparison: Compares the DQN to traditional PID controllers in metrics like long-term tracking ability.

Some other relevant terms include Markov decision process, state/action spaces, policy optimization, sample efficiency, and robust control.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using a centralized Deep Q Network (DQN) controller instead of separate PID controllers. What are the key advantages and disadvantages of this approach? How does it account for the complex dynamics and action spaces of the 6DOF swimming robot?

2. The paper breaks down the underwater tracking task into separate vision and control modules. What is the rationale behind this? How does it facilitate online learning and reduce computational costs? What are the potential drawbacks? 

3. The paper argues that the time delay in the 6DOF robot violates the Markov property for standard DQN. How is this issue addressed? Why is considering the delay time and history of states/actions important?

4. What is the proposed framework for safely training the RL agent using existing classical controllers? Why is this preferred over approaches like behavioural cloning or learning only from demonstration? What are the step-by-step procedures?

5. The reward function is designed to keep the target centered in the image plane. How does this facilitate adaptation to different objects and vision systems? What other factors could be incorporated into the reward to further improve performance?

6. What simulator is used for controlled evaluation? What are some of its key capabilities in replicating robot dynamics and physical interactions? What are its limitations in terms of realism?

7. What metrics are used to evaluate the performance of the RL controller versus the PID controllers? Why are both transient and long-term metrics considered important? How do the results demonstrate the superiority of the RL approach?

8. How does the second study demonstrate the capability of the RL agent to handle model uncertainty and adapt to changes in robot parameters? Why do PID controllers struggle in comparison?

9. How is the final study used to showcase the collaboration between the RL controller and the vision module? Why would reward based on detection confidence be useful?

10. What avenues are identified in the conclusion for further enhancing the performance of the centralized RL controller? What are some ways the proposed approach could be extended or improved?
