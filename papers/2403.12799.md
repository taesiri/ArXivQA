# [Investigating Text Shortening Strategy in BERT: Truncation vs   Summarization](https://arxiv.org/abs/2403.12799)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer-based models like BERT have a limitation on the maximum length of text input they can process due to their parallelism. Typically texts longer than the limit are truncated, but it's unclear how well this performs.
- There is no consensus on which parts of a long text are most important to keep when shortening. Summarization may be able to extract the most important parts regardless of position.

Method:
- Compared 3 summarization strategies (extractive, abstractive, automatic abstractive) to 7 truncation strategies for text classification on the IndoSum dataset using DistilBERT. 
- Filtered dataset to only include documents between 210-460 tokens so truncation variations did not intersect.
- New stratified 5-fold cross validation splits generated.  
- Macro-averaged F1-score used as main evaluation metric.

Results:
- Extractive summarization outperformed most truncation variations, only losing to taking first 70 tokens strategy.  
- Taking beginning of documents works best, indicating important info is in opening sentences for this news dataset.
- Performance gap between top strategies and full-text is less than 0.01 F1.
- Automatic abstractive summarization performed poorly, possibly from overly short summaries degrading performance.

Main Contributions:
- First investigation of text summarization as a shortening strategy compared to truncation for Transformer models in any language. 
- First benchmark of shortening strategies for Indonesian long text classification.
- Showed feasibility and potential of summarization for text shortening in classification tasks.
- Demonstrated importance of location of key information in documents and difficulties when this location is unknown/varies.


## Summarize the paper in one sentence.

 This paper investigates and compares various text shortening strategies like truncation and summarization for text classification using BERT on a dataset of Indonesian news articles, finding that extractive summarization performs competitively to truncation by taking the first part of documents.


## What is the main contribution of this paper?

 Based on the content of the paper, the main contribution is:

This study investigates the effectiveness of text summarization as an alternative shortening strategy compared to truncation methods for text classification tasks using Transformer-based models. Specifically, it compares various truncation strategies to extractive and abstractive summarization methods on the IndoSum dataset using a DistilBERT model. The key findings are:

1) Extractive summarization outperforms most truncation variations, losing only to taking the first 70 tokens of the document. This shows summarization is a feasible and competitive shortening strategy. 

2) The performance of extractive summarization is close to just taking the full document without any shortening. This suggests that with further improvements to summarization methods, they could replace truncation entirely.

3) Taking the first parts of documents works best for this news dataset where important information tends to appear early on. Summarization may be more generally applicable for datasets where key information appears throughout documents.

So in summary, the main contribution is demonstrating that summarization, especially extractive methods, are effective shortening alternatives to truncation for Transformer text classification.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Long text
- Document classification 
- Summarization
- Shortening strategy
- BERT
- Transformer-based models
- Text truncation
- Extractive summarization
- Abstractive summarization
- IndoSum dataset
- Indonesian news articles
- DistilBERT

The paper investigates different text shortening strategies like truncation and summarization for classifying long Indonesian news documents using BERT-based models. It compares the performance of different variations of truncation and summarization methods on the IndoSum dataset. The key goals are to study if summarization can be an effective alternative to truncation for handling long texts, and how close these methods can get to the performance of using the full text.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper compares truncation strategies and summarization strategies for text classification. What were the specific truncation and summarization strategies tested? Why were those specific strategies chosen?

2. The paper uses the IndoSum dataset. What are the key characteristics and statistics of this dataset? Why was IndoSum suitable for evaluating text shortening strategies?

3. The paper filters the IndoSum dataset to only include documents between 210-460 tokens. What was the rationale behind choosing this token length range? How does filtering impact analysis of the results?

4. The paper tests an automatic abstractive summarization method. How was this summarization model trained? What were the key hyperparameters used? How might improvements to this model impact overall performance?

5. The best performing strategy was taking the first 70 tokens. The paper hypothesizes this is because important information tends to be at the beginning for news articles. How could you test whether performance generalizes to other document types?  

6. The extractive summarization strategy performs nearly as well as taking the full text. What factors could be limiting its performance compared to the full text? How could the summarization strategy be improved?

7. The paper uses macro-averaged F1 as the evaluation metric. What are the pros and cons of this metric choice? How sensitive are conclusions to the choice of evaluation metric?

8. Statistical significance testing between strategies was not feasible due to computational constraints. What are some approaches that could be used for significance testing? How important is significance testing to the conclusions?

9. The paper uses a DistilBERT model. How might performance differ using a BERT or RoBERTa model? What adjustments would need to be made during fine-tuning?

10. The paper focuses on a single text classification task. How might the relative performance of strategies differ for other downstream tasks like sequence labeling, QA, entailment? Why might there be differences?
