# [Benchmarking and Building Long-Context Retrieval Models with LoCo and   M2-BERT](https://arxiv.org/abs/2402.07440)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating retrieval models do not effectively require long context reasoning, allowing models to perform well just using short contexts. 
- State-of-the-art retrieval models rely on Transformer architectures that scale quadratically with sequence length, making them inefficient for long documents.  
- Pretraining and finetuning strategies used by current models are not optimized for handling both short queries and long documents.

Proposed Solution:
- Introduce LoCoV1, a new benchmark with 12 tasks across domains like law, medicine, and finance that require reasoning over long document contexts. 
- Propose M2-BERT, an efficient 80M parameter retrieval encoder based on the Monarch Mixer architecture that can process sequences up to 32k tokens.  
- Use a mixture of short and long sequences during pretraining to handle both queries and documents.
- Finetune with orthogonal projection loss that aligns embeddings independent of batch size.

Main Contributions:
- LoCoV1 benchmark to evaluate long document retrieval.
- M2-BERT retrieval encoder that achieves SOTA on LoCoV1 while being 3-676x more efficient that Transformer models.
- Strategies for pretraining and finetuning that equip models to reason over both short queries and long documents.
- Analysis showing M2-BERT outperforms models 5-90x its size, and beats chunking approaches.
- Model checkpoints and benchmark to facilitate research into long document retrieval.
