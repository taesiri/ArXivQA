# Encouraging Divergent Thinking in Large Language Models through   Multi-Agent Debate

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper aims to address is: How can we mitigate the "Degeneration-of-Thought" (DoT) problem in large language models to enable them to generate novel thoughts through self-reflection when their initial stance may be incorrect?The key points are:- The paper identifies and defines a new issue called the Degeneration-of-Thought (DoT) problem in self-reflection for large language models (LLMs). - DoT refers to the scenario where once an LLM establishes confidence in its answers, it struggles to generate new thoughts later through self-reflection even if its initial stance was wrong.- To address DoT, the paper proposes a Multi-Agent Debate (MAD) framework to encourage divergent thinking in LLMs. - In MAD, multiple agents express arguments in a "tit for tat" state while a judge oversees the debate to reach a solution.- Experiments on challenging datasets show MAD helps mitigate DoT and improves performance over baseline methods.So in summary, the key research question is how to address the newly defined DoT problem in LLMs to improve their ability to generate novel thoughts through self-reflection when needed. The proposed MAD framework is presented as a solution.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes and defines the Degeneration-of-Thought (DoT) problem in self-reflection, where LLMs stick to incorrect initial answers and fail to generate novel thoughts through later self-reflection. 2. It addresses the DoT problem by proposing a Multi-Agent Debate (MAD) framework, where multiple agents debate in a "tit for tat" state to encourage divergent thinking and exploration of different reasoning chains.3. It demonstrates the effectiveness of MAD on two challenging tasks - commonsense machine translation and counter-intuitive arithmetic reasoning. Experiments show MAD helps weaker models like GPT-3.5 outperform stronger models like GPT-4.4. It provides extensive analysis on MAD, suggesting an adaptive break strategy and a modest level of disagreement are needed for good performance. It also finds LLMs may exhibit bias and unfairness as a debate judge when using different models for agents.5. The proposed MAD framework is shown to be an effective technique to alleviate the identified DoT problem in LLMs and improve performance on complex reasoning tasks requiring deeper contemplation. The multi-agent debate encourages divergent thinking compared to self-reflection approaches.In summary, the core contribution is identifying the DoT problem in LLMs, and addressing it by proposing a novel MAD framework that outperforms existing methods and explores different chains of reasoning through debate between multiple agents.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Multi-Agent Debate (MAD) framework to address the Degeneration-of-Thought (DoT) problem in self-reflection, where agents engage in "tit for tat" debates managed by a judge to encourage divergent thinking and obtain better solutions on complex reasoning tasks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in the field of improving reasoning abilities of large language models:- The paper focuses on addressing the "Degeneration of Thought" (DoT) problem specifically in self-reflection methods for LLMs. This problem and analysis of how self-reflection can fail due to biases or resistance to change is a novel contribution not explored in detail in prior work. - The proposed Multi-Agent Debate (MAD) framework is unique in leveraging debate between multiple agents to encourage divergent thinking. Other methods like self-consistency or chain-of-thought rely on single model self-generation. Using multiple models to evaluate and correct each other is an innovative approach.- The MAD framework builds on prior work on generative agents and debates, but tailors the approach specifically to handle the identified DoT problem and complex reasoning tasks. The goals and motivations are more targeted compared to general debate agents.- The paper demonstrates strong empirical results, with MAD + GPT-3.5 outperforming GPT-4 on a machine translation task. This shows the promise of MAD as a technique to boost reasoning for existing models. - The analysis provides useful insights, like the importance of adaptive debate length and a balanced "tit-for-tat" dynamic. This helps inform design of effective debate systems.Overall, the identification of the DoT problem, proposal of MAD to address it, and thorough empirical validation on challenging tasks helps advance the understanding and techniques for improving reasoning in LLMs. The multi-agent debate approach appears to be a promising direction not fully explored in prior literature.
