# [TARGET: Federated Class-Continual Learning via Exemplar-Free   Distillation](https://arxiv.org/abs/2303.06937)

## What is the central research question or hypothesis that this paper addresses?

 From my reading, the key research focus of this paper is federated class-continual learning, which aims to enable continually learning new classes in a federated learning setting while mitigating catastrophic forgetting of old classes. 

Specifically, the paper proposes a new method called TARGET (federatTed clAss-continual leaRninG via Exemplar-free disTillation) to address the catastrophic forgetting problem in federated class-continual learning scenarios. The central hypothesis is that leveraging global information from the federated learning process can help alleviate catastrophic forgetting without requiring storing additional datasets or previous task data.

To test this hypothesis, the paper first analyzes the impact of non-IID (non-independent and identically distributed) data on exacerbating catastrophic forgetting in federated learning. It then proposes the TARGET framework which utilizes knowledge distillation from the global model trained on previous tasks to transfer knowledge to the model for the current task, along with training a generator to produce synthetic data capturing the global data distribution. This allows leveraging global information to mitigate catastrophic forgetting without violating privacy by storing real data.

Through extensive experiments on benchmark datasets, the paper demonstrates that TARGET outperforms existing federated class-continual learning methods by achieving higher average accuracy across tasks and lower forgetting of previous tasks. The results validate their hypothesis that using global information can effectively alleviate catastrophic forgetting in this setting.

In summary, the central research contribution is proposing and evaluating the TARGET framework to enable privacy-preserving federated class-continual learning by mitigating catastrophic forgetting through utilizing global model knowledge and synthesized global data.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

1. The authors propose a new method called TARGET (federatTed clAss-continual leaRninG via Exemplar-free disTillation) for alleviating catastrophic forgetting in the setting of Federated Class-Continual Learning (FCCL). 

2. They demonstrate that non-IID (non-independent and identically distributed) data exacerbates the catastrophic forgetting problem in FCCL.

3. Their proposed method leverages global information from the previously trained global model to transfer knowledge to the model being trained on the current task, without requiring storage of extra datasets or data from previous tasks. This makes it applicable to privacy-sensitive scenarios.

4. They introduce a generator that is trained to produce synthetic data simulating the global data distribution. This synthetic data is used along with the real data for local model updates.

5. Extensive experiments show that their method outperforms existing FCCL methods in terms of mitigating catastrophic forgetting, especially in cases of extreme non-IID data distributions.

In summary, the key novelty of this work seems to be the proposal of an exemplar-free knowledge distillation approach to alleviating catastrophic forgetting in federated continual learning settings, while preserving data privacy. The use of synthetic global data and transfer learning from previous global models are the main technical contributions.
