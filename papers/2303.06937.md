# [TARGET: Federated Class-Continual Learning via Exemplar-Free   Distillation](https://arxiv.org/abs/2303.06937)

## What is the central research question or hypothesis that this paper addresses?

 From my reading, the key research focus of this paper is federated class-continual learning, which aims to enable continually learning new classes in a federated learning setting while mitigating catastrophic forgetting of old classes. 

Specifically, the paper proposes a new method called TARGET (federatTed clAss-continual leaRninG via Exemplar-free disTillation) to address the catastrophic forgetting problem in federated class-continual learning scenarios. The central hypothesis is that leveraging global information from the federated learning process can help alleviate catastrophic forgetting without requiring storing additional datasets or previous task data.

To test this hypothesis, the paper first analyzes the impact of non-IID (non-independent and identically distributed) data on exacerbating catastrophic forgetting in federated learning. It then proposes the TARGET framework which utilizes knowledge distillation from the global model trained on previous tasks to transfer knowledge to the model for the current task, along with training a generator to produce synthetic data capturing the global data distribution. This allows leveraging global information to mitigate catastrophic forgetting without violating privacy by storing real data.

Through extensive experiments on benchmark datasets, the paper demonstrates that TARGET outperforms existing federated class-continual learning methods by achieving higher average accuracy across tasks and lower forgetting of previous tasks. The results validate their hypothesis that using global information can effectively alleviate catastrophic forgetting in this setting.

In summary, the central research contribution is proposing and evaluating the TARGET framework to enable privacy-preserving federated class-continual learning by mitigating catastrophic forgetting through utilizing global model knowledge and synthesized global data.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

1. The authors propose a new method called TARGET (federatTed clAss-continual leaRninG via Exemplar-free disTillation) for alleviating catastrophic forgetting in the setting of Federated Class-Continual Learning (FCCL). 

2. They demonstrate that non-IID (non-independent and identically distributed) data exacerbates the catastrophic forgetting problem in FCCL.

3. Their proposed method leverages global information from the previously trained global model to transfer knowledge to the model being trained on the current task, without requiring storage of extra datasets or data from previous tasks. This makes it applicable to privacy-sensitive scenarios.

4. They introduce a generator that is trained to produce synthetic data simulating the global data distribution. This synthetic data is used along with the real data for local model updates.

5. Extensive experiments show that their method outperforms existing FCCL methods in terms of mitigating catastrophic forgetting, especially in cases of extreme non-IID data distributions.

In summary, the key novelty of this work seems to be the proposal of an exemplar-free knowledge distillation approach to alleviating catastrophic forgetting in federated continual learning settings, while preserving data privacy. The use of synthetic global data and transfer learning from previous global models are the main technical contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new method called TARGET for federated class-continual learning that mitigates catastrophic forgetting when learning a sequence of classification tasks with new classes by using knowledge distillation and synthesizing data to simulate the global data distribution, without requiring storage of previous task data.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on federated class-continual learning relates to other research in the same field:

- It tackles the relatively new and underexplored problem of class-continual learning in the federated learning setting, where new classes are incrementally added over time. Most prior federated learning research assumes static classes.

- The paper demonstrates that non-IID data exacerbates catastrophic forgetting in the federated continual learning setting. This analysis of the relationship between data heterogeneity and forgetting provides new insights.

- The proposed TARGET method utilizes global model distillation and data generation techniques to mitigate catastrophic forgetting without needing to store exemplars or old task data, unlike other federated continual learning approaches. This makes it applicable to privacy-sensitive scenarios.

- By not relying on stored data, the work relates to recent exemplar-free continual learning methods like DeepDream and Always Be Dreaming. However, those methods were not designed for federated learning with non-IID data distributions.

- The paper introduces techniques like boundary-aware loss and batch normalization loss for improving the quality of generated pseudo-data for old tasks. This data generation strategy is tailored for federated continual learning.

- Extensive experiments on image classification datasets demonstrate the efficacy of TARGET, achieving state-of-the-art results compared to existing federated continual learning baselines.

In summary, this paper advances federated continual learning research by tackling new challenges related to class-incremental learning, analyzing data heterogeneity issues, and proposing a novel data-free distillation approach suitable for non-IID federated settings. The data generation and model distillation techniques differentiate this work from prior art.
