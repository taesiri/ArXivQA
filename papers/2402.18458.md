# [Meta-Task Prompting Elicits Embedding from Large Language Models](https://arxiv.org/abs/2402.18458)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing methods for generating sentence embeddings from large language models (LLMs) have limitations. Methods that prompt LLMs to summarize the sentence into one word tend to produce simplistic embeddings focused on stop words. Methods requiring task-specific fine-tuning are computationally expensive and produce embeddings tailored to one downstream use-case. 

Proposed Solution:
This paper proposes MetaEOL, a novel unsupervised approach to extract high-quality and versatile sentence embeddings from LLMs without fine-tuning. MetaEOL is based on meta-task prompting, where the LLM is prompted with multiple carefully designed templates that elicit embeddings for diverse usage scenarios (e.g. text classification, sentiment analysis). These meta-task prompts guide the LLM to consider multiple aspects when embedding the sentence.

The resulting meta-task embeddings are simply averaged to produce a final multi-faceted sentence embedding. Extensive experiments show these averaged embeddings achieve strong performance on semantic textual similarity tasks and transfer learning benchmarks without any training, outperforming prior prompt-based and contrastive methods.

Main Contributions:
- Introduces meta-task prompting for unsupervised extraction of high-quality sentence embeddings from LLMs 
- Shows averaging complementary meta-task embeddings produces versatile general-purpose sentence representations
- Empirically demonstrates strong performance of meta-task prompted embeddings on STS and downstream tasks without fine-tuning
- Reveals meta-task prompting is more effective than duplicate prompting and task-specific prompting
- Suggests a potential scaling law whereby optimal layer for extraction changes with LLM size

In summary, this paper presents a prompting-based approach to effectively elicit multi-faceted sentence embeddings from LLMs, achieving strong generalizable performance without costly training. The meta-task prompting paradigm offers a promising direction for versatile unsupervised sentence representation.
