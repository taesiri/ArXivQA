# [Self-Supervised Learning for Contextualized Extractive Summarization](https://arxiv.org/abs/1906.04466)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve extractive summarization by introducing auxiliary pre-training tasks that learn to capture document-level context in a self-supervised fashion?The key points are:- Existing summarization models are usually trained end-to-end from scratch without explicitly modeling document context. This makes it challenging to learn how to leverage document context.- The authors propose using self-supervised pre-training tasks to help the model learn document-level contextual information. - They introduce three pre-training tasks: Mask, Replace, and Switch. These require predicting masked sentences, detecting replaced sentences, and detecting switched sentence positions.- Through solving these pre-training tasks on unlabeled documents, the model is forced to learn document structure and context. - This learned representation is then transferred to improve performance on extractive summarization.So in summary, the main research question is whether self-supervised pre-training on document-level tasks can help improve extractive summarization by teaching the model to better leverage document context. The auxiliary pre-training tasks are proposed as a method to achieve this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors propose to use self-supervision to learn contextualized sentence representations for extractive summarization, without requiring any human labels.2. They introduce three self-supervised pre-training tasks - Mask, Replace, and Switch - that require the model to learn document-level context. 3. Through experiments on the CNN/DM dataset, they show that models pretrained on these tasks achieve better performance and faster convergence compared to training from scratch. The Switch task in particular achieves new state-of-the-art results using a simple hierarchical model.4. The results demonstrate that self-supervised pretraining allows the model to learn useful document-level representations, making it more sample efficient and able to generalize better for the summarization task.In summary, the key contribution is using self-supervision for learning representations that encode document context, which helps improve performance on extractive summarization with less labeled data. The proposed pretraining tasks are shown to be effective for this purpose.
