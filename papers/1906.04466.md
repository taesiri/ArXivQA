# [Self-Supervised Learning for Contextualized Extractive Summarization](https://arxiv.org/abs/1906.04466)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve extractive summarization by introducing auxiliary pre-training tasks that learn to capture document-level context in a self-supervised fashion?The key points are:- Existing summarization models are usually trained end-to-end from scratch without explicitly modeling document context. This makes it challenging to learn how to leverage document context.- The authors propose using self-supervised pre-training tasks to help the model learn document-level contextual information. - They introduce three pre-training tasks: Mask, Replace, and Switch. These require predicting masked sentences, detecting replaced sentences, and detecting switched sentence positions.- Through solving these pre-training tasks on unlabeled documents, the model is forced to learn document structure and context. - This learned representation is then transferred to improve performance on extractive summarization.So in summary, the main research question is whether self-supervised pre-training on document-level tasks can help improve extractive summarization by teaching the model to better leverage document context. The auxiliary pre-training tasks are proposed as a method to achieve this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors propose to use self-supervision to learn contextualized sentence representations for extractive summarization, without requiring any human labels.2. They introduce three self-supervised pre-training tasks - Mask, Replace, and Switch - that require the model to learn document-level context. 3. Through experiments on the CNN/DM dataset, they show that models pretrained on these tasks achieve better performance and faster convergence compared to training from scratch. The Switch task in particular achieves new state-of-the-art results using a simple hierarchical model.4. The results demonstrate that self-supervised pretraining allows the model to learn useful document-level representations, making it more sample efficient and able to generalize better for the summarization task.In summary, the key contribution is using self-supervision for learning representations that encode document context, which helps improve performance on extractive summarization with less labeled data. The proposed pretraining tasks are shown to be effective for this purpose.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using self-supervised learning to pretrain a neural network on unlabeled documents to learn contextualized sentence representations that capture document structure, and shows this pretraining improves performance on extractive summarization.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in extractive text summarization:- The paper focuses on using self-supervised learning to learn sentence representations that incorporate document-level context. This is a novel approach compared to most prior work that uses supervised learning with human annotations. - The proposed self-supervised tasks of Mask, Replace, and Switch are designed to specifically teach the model about document structure and context. This is different from other self-supervised methods like BERT that operate on words or sentence pairs.- The paper shows strong empirical results, with the Switch task outperforming previous state-of-the-art models like NeuSum on the CNN/DM dataset. This demonstrates the effectiveness of self-supervised pretraining for this task.- The model architecture is relatively simple, using an LSTM encoder with self-attention. It does not have complex task-specific modules like some other recent summarization papers. This shows the power of self-supervision to boost even simple models.- The paper focuses solely on extractive summarization. Many recent works have explored abstractive summarization instead which generates summaries rather than extracting sentences. This paper brings a new perspective focused on better representing sentences through self-supervision.Overall, the key novelties of this paper compared to prior work seem to be the use of self-supervision for document modeling and the simple yet effective model architecture. The results demonstrate self-supervised pretraining is a promising approach for extractive summarization that merits further exploration.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Investigate combining different architectures for the basic model, such as replacing the self-attention module with an LSTM. They mention leaving this for future work in Section 2.1.- Try more sophisticated pre-training tasks beyond the three proposed in this work. The authors suggest there is opportunity for designing additional pre-training objectives to learn useful document representations.- Explore how the size of pre-training data affects performance. The authors tried augmenting the CNN/DM dataset but did not see improvements, likely because the augmented dataset was still not large enough. They suggest trying larger pre-training datasets in the future.- Analyze how well pre-trained the model needs to be to yield downstream improvements. The authors pre-trained models until convergence, but suggest investigating if shorter pre-training could still be beneficial.- Apply the pre-training approaches to other document-level tasks besides summarization. The authors propose the pre-training objectives to learn document context, which could aid other tasks.- Investigate combining the model architectures and pre-training techniques with more recent advances like BERT. The authors' techniques provide avenues to incorporate document-level context into pretrained language models.In summary, the main future directions are exploring architectural variants, additional pre-training tasks, larger datasets, less pre-training, application to other tasks, and integration with more recent methods like BERT.
