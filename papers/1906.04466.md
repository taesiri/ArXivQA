# [Self-Supervised Learning for Contextualized Extractive Summarization](https://arxiv.org/abs/1906.04466)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve extractive summarization by introducing auxiliary pre-training tasks that learn to capture document-level context in a self-supervised fashion?The key points are:- Existing summarization models are usually trained end-to-end from scratch without explicitly modeling document context. This makes it challenging to learn how to leverage document context.- The authors propose using self-supervised pre-training tasks to help the model learn document-level contextual information. - They introduce three pre-training tasks: Mask, Replace, and Switch. These require predicting masked sentences, detecting replaced sentences, and detecting switched sentence positions.- Through solving these pre-training tasks on unlabeled documents, the model is forced to learn document structure and context. - This learned representation is then transferred to improve performance on extractive summarization.So in summary, the main research question is whether self-supervised pre-training on document-level tasks can help improve extractive summarization by teaching the model to better leverage document context. The auxiliary pre-training tasks are proposed as a method to achieve this.
