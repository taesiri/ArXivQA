# [ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models](https://arxiv.org/abs/2307.00398)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how to efficiently estimate uncertainty for the embeddings of pre-trained frozen large-scale vision-language models. 

The key hypotheses are:

1. The point estimate embeddings from large frozen VLMs can serve as a good estimate of the mean embedding. Additional parameters of the embedding distribution can be learned efficiently in a post-hoc manner.

2. Learning the additional parameters with objectives enforcing intra-modal and cross-modal alignment will allow capturing ambiguities and uncertainties within and across modalities. 

3. The estimated uncertainties will be well-calibrated and useful for downstream tasks like active learning and model selection.

In summary, the central hypothesis is that the uncertainty for frozen VLMs can be estimated post-hoc in an efficient and calibrated manner via intra/cross-modal alignment objectives. This allows retaining the benefits of pre-training while also quantifying uncertainty.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing ProbVLM, a post-hoc probabilistic adapter that efficiently estimates probability distributions for the embeddings of frozen, pre-trained vision-language models (VLMs) like CLIP. 

The key ideas are:

- Leveraging existing frozen VLMs that provide high-quality deterministic point estimate embeddings, and estimating the remaining parameters to model the distribution. This avoids needing to retrain large models from scratch.

- Using a combination of intra-modal and cross-modal alignment losses to train ProbVLM to capture ambiguity and uncertainty present within and across modalities.

- Demonstrating that ProbVLM provides well-calibrated uncertainty estimates on tasks like cross-modal retrieval, and that the uncertainty can be useful for applications like active learning and model selection.

- Presenting a technique to visualize the predicted embedding distributions using latent diffusion models like Stable Diffusion.

In summary, the main contribution is an efficient way to convert frozen deterministic VLMs into probabilistic models that quantify uncertainty, without needing large datasets or computing resources. The uncertainty estimates are shown to be useful in downstream applications.
