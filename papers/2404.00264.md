# [DiLM: Distilling Dataset into Language Model for Text-level Dataset   Distillation](https://arxiv.org/abs/2404.00264)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing text dataset distillation methods create synthetic samples as sequences of word embeddings instead of actual text. This makes the distilled datasets only applicable to models with the same word embedding weights used during distillation. Also, embedding-level synthetic samples are uninterpretable to humans. 

Proposed Solution: 
The paper proposes a novel text dataset distillation method called "Distilling dataset into Language Model (DiLM)". Instead of directly optimizing synthetic text samples, DiLM trains a language model to generate informative text samples by minimizing the gradient matching loss. To enable backpropagation of the loss through the discrete text, it uses a weighted loss based on generation probabilities.

Main Contributions:
- First study to distill a text dataset into a text-level synthetic dataset applicable to models independent of word embeddings
- Presents DiLM which uses a language model as a surrogate optimization target and bypasses non-differentiable text via loss weighting 
- Experiments show DiLM outperforms baselines in training same and different models, and also as few-shot prompts for large language models
- Provides interpretable text samples representing original dataset

In summary, the paper proposes DiLM to address limitations of existing text dataset distillation methods. By optimizing a language model to generate text samples, DiLM produces superior and model-agnostic synthetic datasets. The method also allows interpreting the knowledge captured from the original dataset.
