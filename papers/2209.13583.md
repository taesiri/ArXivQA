# [Learning State-Aware Visual Representations from Audible Interactions](https://arxiv.org/abs/2209.13583)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we learn meaningful representations from interaction-rich, multi-modal streams of egocentric data in a self-supervised manner, i.e. without relying on human annotated labels?The authors specifically aim to address two key challenges in learning representations from untrimmed egocentric videos of daily activities:1) Identifying the right moments when interactions actually occur, since untrimmed videos contain long periods without interactions. 2) Learning representations that are sensitive to changes in the environment caused by interactions, rather than invariant representations. To address these challenges, the authors propose a self-supervised algorithm called RepLAI that leverages audio signals to identify moments of interaction and uses a novel loss function to associate audio with visual state changes during interactions.So in summary, the central research question is about developing a self-supervised approach to learn useful representations from untrimmed, multi-modal egocentric video containing interactions, by focusing on moments of interaction and changes in visual state. The authors validate their approach on two egocentric datasets and demonstrate improvements on downstream tasks like action recognition, anticipation and state change classification.
