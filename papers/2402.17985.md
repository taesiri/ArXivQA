# [FlattenQuant: Breaking Through the Inference Compute-bound for Large   Language Models with Per-tensor Quantization](https://arxiv.org/abs/2402.17985)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Large language models (LLMs) have very high inference latency and GPU memory usage, restricting their deployment. This is due to the intensive matrix calculations and large parameter sizes.
- Existing quantization methods like GPTQ mitigate the memory bottleneck but still rely on FP16 computation, so they face inference speed issues for large batch sizes or sequence lengths. This compute-bound challenge needs to be addressed.

Proposed Solution - FlattenQuant:
- Identifies outlier channels in activation tensors and expands them by adding extra channels to accommodate the large values. This significantly reduces the tensor's maximum value.
- Flattens the large values in the tensor through this channel expansion while preserving information. This enables accurate low-bit per-tensor quantization.
- Achieves INT4 quantization for nearly 50% of linear layers in LLMs, with INT8 for the remaining. Replaces FP16 matrix multiplication with more efficient INT4 and INT8 computation.

Main Contributions:
- Analyzes the need for quantization schemes that can overcome compute-bound challenges during LLM inference.
- Proposes FlattenQuant to reduce tensor maximum values and enable per-tensor low-bit quantization, overcoming issues with prior fine-grained schemes.
- Achieves up to 2x speedup and 2.3x memory reduction with minimal accuracy loss. About 50% of layers use 4-bit quantization to address compute-bound.

In summary, FlattenQuant enables efficient per-tensor low-bit quantization of LLMs by expanding outlier channels and flattening tensors. This allows low-bit matrix multiplication that significantly accelerates compute-bound inference.
