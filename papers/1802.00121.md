# Interpreting CNNs via Decision Trees

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we explain predictions made by a pre-trained convolutional neural network (CNN) in a quantitative and semantic manner? Specifically, the authors aim to develop a method to:- Bridge the gap between middle-layer CNN features and semantic concepts like object parts, in order to provide linguistic explanations of the knowledge encoded in the CNN. - Quantitatively analyze the rationale behind each CNN prediction by clarifying which parts/filters are activated and how much they contribute to the final prediction score.The key hypothesis is that learning a decision tree on top of a revised CNN with disentangled filters can help decompose high-level CNN features into elementary concepts of object parts and provide semantic and quantitative explanations for CNN predictions.In summary, the central goal is to open the black box of CNNs by explaining their predictions at a semantic and quantitative level using decision trees, without needing additional supervision like part annotations during training.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to learn a decision tree that can explain predictions made by a pre-trained convolutional neural network (CNN) at a semantic level. Specifically:- The decision tree decomposes neural activations in high conv-layers of the CNN into elementary concepts of object parts. This bridges the gap between CNN representations and semantic concepts. - The decision tree tells which object parts activate which filters in the CNN to make a prediction, and quantifies how much each part contributes to the prediction score. This provides semantic and quantitative rationales to explain each CNN prediction.- The decision tree summarizes different decision modes (rationales) of the CNN in a hierarchy. High-level nodes represent common decision modes shared by many images, while low-level nodes encode fine-grained modes. - When making a prediction, the CNN determines a parse tree from the decision tree to explain the prediction at different abstraction levels. The parse tree provides explanations from generic to fine-grained.In summary, the key contribution is using the decision tree to explain predictions of a pre-trained CNN in a semantic and quantitative way, by decomposing CNN representations into interpretable concepts of object parts. This helps open the black box of CNNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes learning a decision tree to semantically and quantitatively explain the predictions made by a convolutional neural network by decomposing the network's feature representations into elementary concepts corresponding to object parts.
