# Interpreting CNNs via Decision Trees

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we explain predictions made by a pre-trained convolutional neural network (CNN) in a quantitative and semantic manner? Specifically, the authors aim to develop a method to:- Bridge the gap between middle-layer CNN features and semantic concepts like object parts, in order to provide linguistic explanations of the knowledge encoded in the CNN. - Quantitatively analyze the rationale behind each CNN prediction by clarifying which parts/filters are activated and how much they contribute to the final prediction score.The key hypothesis is that learning a decision tree on top of a revised CNN with disentangled filters can help decompose high-level CNN features into elementary concepts of object parts and provide semantic and quantitative explanations for CNN predictions.In summary, the central goal is to open the black box of CNNs by explaining their predictions at a semantic and quantitative level using decision trees, without needing additional supervision like part annotations during training.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to learn a decision tree that can explain predictions made by a pre-trained convolutional neural network (CNN) at a semantic level. Specifically:- The decision tree decomposes neural activations in high conv-layers of the CNN into elementary concepts of object parts. This bridges the gap between CNN representations and semantic concepts. - The decision tree tells which object parts activate which filters in the CNN to make a prediction, and quantifies how much each part contributes to the prediction score. This provides semantic and quantitative rationales to explain each CNN prediction.- The decision tree summarizes different decision modes (rationales) of the CNN in a hierarchy. High-level nodes represent common decision modes shared by many images, while low-level nodes encode fine-grained modes. - When making a prediction, the CNN determines a parse tree from the decision tree to explain the prediction at different abstraction levels. The parse tree provides explanations from generic to fine-grained.In summary, the key contribution is using the decision tree to explain predictions of a pre-trained CNN in a semantic and quantitative way, by decomposing CNN representations into interpretable concepts of object parts. This helps open the black box of CNNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes learning a decision tree to semantically and quantitatively explain the predictions made by a convolutional neural network by decomposing the network's feature representations into elementary concepts corresponding to object parts.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on interpreting CNNs via decision trees compares to other related work:- Focuses on semantic-level explanations of CNN predictions rather than just pixel-level visualizations. The goal is to explain which object parts a CNN uses to make each prediction. This provides a more intuitive understanding than gradient-based visualization methods like grad-CAM.- Quantitatively measures the contribution of each object part/filter to the final prediction. Previous work has mostly focused on qualitative visualizations. This paper computes numerical importance scores for parts.- Summarizes the rationale behind predictions into reusable "decision modes" represented as nodes in a decision tree. This provides a structured way to understand the typical reasoning processes of a CNN. Other methods like LIME and SHAP explain individual predictions.- Does not require strong supervision to train the decision tree. Many prior methods explaining CNNs rely on labeled parts or textual rationales. This paper disentangles filters without extra annotations.- Evaluates the decision tree explanations more rigorously than prior work, using both accuracy metrics and information loss metrics. This provides fuller analysis of the explanatory power.Overall, the key novelties are providing semantic explanations that quantify part contributions and generalize across images via decision modes and the tree structure. The paper pushes towards better mimicking how humans interpret model predictions.
