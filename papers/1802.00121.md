# [Interpreting CNNs via Decision Trees](https://arxiv.org/abs/1802.00121)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we explain predictions made by a pre-trained convolutional neural network (CNN) in a quantitative and semantic manner? Specifically, the authors aim to develop a method to:- Bridge the gap between middle-layer CNN features and semantic concepts like object parts, in order to provide linguistic explanations of the knowledge encoded in the CNN. - Quantitatively analyze the rationale behind each CNN prediction by clarifying which parts/filters are activated and how much they contribute to the final prediction score.The key hypothesis is that learning a decision tree on top of a revised CNN with disentangled filters can help decompose high-level CNN features into elementary concepts of object parts and provide semantic and quantitative explanations for CNN predictions.In summary, the central goal is to open the black box of CNNs by explaining their predictions at a semantic and quantitative level using decision trees, without needing additional supervision like part annotations during training.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to learn a decision tree that can explain predictions made by a pre-trained convolutional neural network (CNN) at a semantic level. Specifically:- The decision tree decomposes neural activations in high conv-layers of the CNN into elementary concepts of object parts. This bridges the gap between CNN representations and semantic concepts. - The decision tree tells which object parts activate which filters in the CNN to make a prediction, and quantifies how much each part contributes to the prediction score. This provides semantic and quantitative rationales to explain each CNN prediction.- The decision tree summarizes different decision modes (rationales) of the CNN in a hierarchy. High-level nodes represent common decision modes shared by many images, while low-level nodes encode fine-grained modes. - When making a prediction, the CNN determines a parse tree from the decision tree to explain the prediction at different abstraction levels. The parse tree provides explanations from generic to fine-grained.In summary, the key contribution is using the decision tree to explain predictions of a pre-trained CNN in a semantic and quantitative way, by decomposing CNN representations into interpretable concepts of object parts. This helps open the black box of CNNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes learning a decision tree to semantically and quantitatively explain the predictions made by a convolutional neural network by decomposing the network's feature representations into elementary concepts corresponding to object parts.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on interpreting CNNs via decision trees compares to other related work:- Focuses on semantic-level explanations of CNN predictions rather than just pixel-level visualizations. The goal is to explain which object parts a CNN uses to make each prediction. This provides a more intuitive understanding than gradient-based visualization methods like grad-CAM.- Quantitatively measures the contribution of each object part/filter to the final prediction. Previous work has mostly focused on qualitative visualizations. This paper computes numerical importance scores for parts.- Summarizes the rationale behind predictions into reusable "decision modes" represented as nodes in a decision tree. This provides a structured way to understand the typical reasoning processes of a CNN. Other methods like LIME and SHAP explain individual predictions.- Does not require strong supervision to train the decision tree. Many prior methods explaining CNNs rely on labeled parts or textual rationales. This paper disentangles filters without extra annotations.- Evaluates the decision tree explanations more rigorously than prior work, using both accuracy metrics and information loss metrics. This provides fuller analysis of the explanatory power.Overall, the key novelties are providing semantic explanations that quantify part contributions and generalize across images via decision modes and the tree structure. The paper pushes towards better mimicking how humans interpret model predictions.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing methods to explain why a CNN learns the knowledge it does during end-to-end learning. The authors state that explaining the learning process of CNNs is still an open challenge. - Applying the proposed method to interpret CNNs for different tasks beyond classification, such as detection, segmentation, etc. The authors focused on classification CNNs in this work to simplify the problem.- Exploring ways to improve the disentanglement of filters during CNN learning without extra supervision. The filter loss used in this work is an unsupervised method but may not perfectly disentangle part representations. - Studying how to handle CNN architectures with skip connections like residual networks. The filter loss has limitations with skip connections. New methods may need to be developed.- Validating the approach on a broader range of CNN models and datasets. The authors demonstrated results on a few benchmark CNNs and datasets but further evaluation is needed.- Comparing with more baseline methods quantitatively. The authors made qualitative comparisons but could not do extensive quantitative comparisons. - Developing new metrics to evaluate the fidelity of decision trees to CNNs. The accuracy metrics used in this work are preliminary.In summary, the authors point to several areas of future work to better explain CNN representations and predictions through decision trees or other interpretable models. Advancing the interpretability of CNNs remains an open and challenging problem.


## Summarize the paper in one paragraph.

The paper proposes a method to explain predictions made by a pre-trained convolutional neural network (CNN) using a decision tree. The key ideas are:1) Modify the CNN architecture to disentangle representations in the top convolutional layer so that each filter represents a semantic part of the object. This is done without part annotations by optimizing a filter loss function. 2) For a given input image, compute the contribution of each filter in the top convolutional layer to the final CNN prediction score. This provides an image-specific rationale for the prediction. 3) Build a decision tree that hierarchically clusters the image-specific rationales into common decision modes shared by groups of images. Each node in the tree represents a decision mode that quantitatively explains how different parts contribute to the prediction. 4) For a new test image, trace a parse tree path in the decision tree to explain the CNN prediction at different semantic levels, from coarse generic modes to fine-grained image-specific modes. Experiments on classification datasets demonstrate the approach can provide semantic explanations for CNN predictions.In summary, the key novelty is learning a decision tree to decompose the CNN's internal representations and predictions into semantically meaningful concepts of object parts and their contributions. This provides human-interpretable explanations beyond pixel-level analysis.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a method to interpret and explain predictions made by convolutional neural networks (CNNs) using decision trees. The key idea is to learn a decision tree that captures the different "decision modes" used by the CNN to make predictions. Each node in the decision tree represents a decision mode, which describes a common rationale for predictions shared across a group of images. Specifically, each decision mode identifies which parts of the object contribute to the prediction and how much. To enable this interpretation, the authors first modify the CNN architecture to disentangle the filters in the final convolutional layer so each filter represents a semantic part of the object. Then, for each image, they compute the contribution of each filter to the final prediction. The decision tree is learned by merging images that use similar filters with similar contributions into common nodes. At test time, the decision tree can explain a prediction by tracing the active node and seeing which parts contributed to the score. Experiments on animal classification tasks demonstrate the approach can provide semantic, quantitative explanations for CNN predictions.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a method to learn a decision tree that semantically and quantitatively explains predictions made by a pre-trained convolutional neural network (CNN). The key steps are:1) Modify a pre-trained CNN to make filters in the top convolutional layer represent different object parts. This is done by adding a filter loss to each filter during fine-tuning to encourage disentangled representations. 2) For an input image, compute the contribution of each filter in the top convolutional layer to the final CNN prediction score. This gives a quantitative rationale for the prediction. 3) Summarize the quantitative rationales for predictions on all training images into a hierarchy of decision modes, from coarse (shared by many images) to fine-grained (image-specific). The hierarchy of decision modes is represented as a decision tree. 4) For a new test image, infer a parse tree from the decision tree to explain the CNN prediction at different semantic levels. The parse tree decomposes the prediction score into contributions from different object parts.In this way, the decision tree provides semantic and quantitative explanations for predictions made by the pre-trained CNN. It bridges the gap between CNN representations and human-interpretable concepts.
