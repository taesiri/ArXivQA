# [CAST: Clustering Self-Attention using Surrogate Tokens for Efficient   Transformers](https://arxiv.org/abs/2402.04239)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformers have shown great success in many NLP tasks but do not scale well to long input sequences due to their quadratic memory and computational complexity. This limits their applicability for tasks with long sequences such as image classification. 

Proposed Solution:
- The paper proposes Cluster Attention with Surrogate Tokens (CAST), a more efficient Transformer architecture for long sequence modeling. 

- Key idea is to cluster the input sequence into groups based on a learned similarity score, perform self-attention within each cluster, and share information between clusters using surrogate tokens. This reduces the sequence length for self-attention.

- Two clustering mechanisms are proposed - Top-K and Single Assignment Top-K clustering. The first allows tokens to be assigned to multiple clusters while the second assigns each token to only one cluster.

Main Contributions:
- Introduces a clustering-based attention mechanism to improve efficiency of Transformers for long sequences. Reduces memory and computations compared to standard self-attention.

- Achieves state-of-the-art accuracy among efficient Transformers on image classification using CIFAR-10. Also shows strong performance on text tasks from the Long Range Arena benchmark.

- Visual analysis reveals the clusters capture meaningful visual concepts, indicating the model learns useful representations. 

- Performs extensive ablations to validate design choices regarding the clustering mechanism, surrogate tokens, and combination approach.

In summary, the paper presents CAST, a novel clustered attention approach to improve Transformer efficiency on long sequences while retaining accuracy. Key innovation is the introduction of input clustering and surrogate tokens for communication between clusters.
