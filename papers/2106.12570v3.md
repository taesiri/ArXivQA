# [Learning Multimodal VAEs through Mutual Supervision](https://arxiv.org/abs/2106.12570v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we develop an effective multimodal variational autoencoder (VAE) model that can learn from partially observed data, where some modalities may be entirely missing? 

The key hypotheses appear to be:

1) An implicit regularization approach based on mutual supervision between modalities can allow a multimodal VAE to learn effectively from partial observations, without needing explicit combinations of representations.

2) This mutual supervision approach, termed MEME, will outperform prior multimodal VAE methods in both fully and partially observed settings. 

3) The representations learned by MEME will better capture the relatedness or semantic similarity between multimodal data instances.

The authors propose a novel formulation of multimodal VAEs by viewing them through a semi-supervised lens, with modalities mutually supervising each other via the VAE's evidence lower bound objective. This avoids explicit combinations of representations while enabling learning from partial observations. Experiments on image-image and image-text datasets aim to validate the hypotheses that MEME outperforms baselines on standard metrics and also learns representations that better represent multimodal relatedness.

In summary, the central research question is how to develop a multimodal VAE that handles partial observations, which is addressed through a mutual supervision approach without explicit representation combinations. The key hypotheses concern MEME's superior performance over existing methods.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution appears to be the proposal of a new method for training multimodal variational autoencoders (VAEs). Specifically, the key ideas are:

- Viewing the combination of information between modalities in a multimodal VAE as a form of "mutual supervision" between the modalities. This allows leveraging techniques from semi-supervised VAEs. 

- Avoiding explicit combination of the latent representations from different modalities, unlike many prior multimodal VAE methods which use things like product of experts. Instead, the proposed method uses the VAE framework's own regularization via the KL divergence as an implicit way to combine information.

- Formulating the model in a way that naturally allows handling missing modalities during training. Many previous multimodal VAEs can't easily deal with missing data at training time.

- Showing strong performance on standard multimodal VAE metrics like cross-coherence and latent classification accuracy, outperforming previous baselines. This is demonstrated on image-image and image-text datasets.

- Analyzing the learned representations and showing the proposed approach better captures semantic similarity or "relatedness" between modalities compared to baselines.

So in summary, the key contribution is a novel way of training multimodal VAEs through mutual supervision that avoids explicit combination of representations, handles missing data, and learns higher quality joint representations than prior approaches. The idea of mutual supervision and using VAE regularization for implicit combination seems to be the most novel aspect.
