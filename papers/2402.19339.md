# [Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision   Transformers for High-Level Image Classification](https://arxiv.org/abs/2402.19339)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Abstract concept (AC) image classification is a complex challenge that requires integrating sensory-perceptual and semantic knowledge to interpret images at a high level. 
- Existing computer vision models like CNNs and ViTs rely on sensory-perceptual understanding from training data but lack explicit semantic knowledge.

Proposed Solution:
- Introduce a "situated perceptual knowledge" paradigm that extracts perceptual semantics from images and integrates them into a Knowledge Graph (KG) along with commonsense knowledge and abstract concepts.
- Compute KG embeddings (KGEs) on this graph to generate new image representations for AC classification. 
- Experiment with combining absolute and relative KGEs with ViT embeddings using fusion techniques like concatenation and Hadamard product.

Key Contributions:
- Created the ARTstract KG, a large resource with detailed situated annotation data for 14k+ cultural images labeled with abstract concepts.
- Showed that relative KGEs significantly improve performance over absolute KGEs for AC classification.
- Demonstrated state-of-the-art results by fusing relative KGEs and ViTs using concatenation and Hadamard product.
- Revealed through analysis that ViTs focus on pixel-level features while KGEs better capture high-level semantics. 
- Proposed that hybrid approaches like using the Hadamard product on relative representations can provide complementary spatial and semantic understanding.

In summary, this paper introduced an interpretable situated perceptual knowledge paradigm for AC image classification that outperforms existing techniques. It highlighted the synergy between deep vision and knowledge-based methods and showed the promise of neuro-symbolic approaches for complex visual understanding tasks.
