# [Effectiveness of self-supervised pre-training for speech recognition](https://arxiv.org/abs/1911.03912)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:What are effective self-supervised pre-training methods for speech recognition, and how does quantizing/discretizing the inputs compare to directly learning representations from continuous speech inputs?The key hypotheses tested are:1) Learning discrete representations of speech through vector quantization, followed by pre-training a BERT model over these discrete units, will be more effective than directly learning representations from continuous speech inputs.2) Pre-training BERT models with self-supervision will improve performance in low-resource speech recognition scenarios with limited labeled data. 3) Two-step pre-training with discrete unit discovery followed by BERT pre-training provides complementary benefits compared to single-step pre-training methods like wav2vec.The paper systematically compares different self-supervised pre-training approaches, with and without explicit quantization of the speech inputs, across varying amounts of labeled data. The goal is to demonstrate the effectiveness of pre-training, particularly discrete unit discovery, for low-resource speech recognition.


## What is the main contribution of this paper?

The main contributions of this paper are:- They compare different self-supervised representation learning approaches for speech recognition, including methods that explicitly quantize the speech signal versus methods that learn representations directly from the continuous signal. - They show that first quantizing the speech into discrete units with vq-wav2vec, and then pre-training a BERT model over these discrete units ("Discrete BERT") works better than learning representations directly from the continuous speech ("Continuous BERT").- They demonstrate that directly fine-tuning the pre-trained BERT models with a CTC loss for speech recognition gives better results than using the BERT features in tandem with a separate downstream ASR model as in previous work.- Their Discrete BERT model pre-trained on unlabeled Librispeech and fine-tuned on just 1 hour of labeled data outperforms the previous state-of-the-art result using 100 hours of labeled data.- With only 10 minutes of labeled data, their approach achieves 16.3/25.2 WER on the Librispeech test-clean/test-other benchmarks, showing the effectiveness of pre-training for low-resource speech recognition.In summary, they systematically compare different self-supervised pre-training methods and show strong results by first quantizing with vq-wav2vec, then pre-training BERT on the discrete units, and directly fine-tuning the BERT model, outperforming previous approaches especially in low labeled data settings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper compares self-supervised speech representation learning methods, finding that first discovering discrete units of the audio data with vq-wav2vec and then pre-training a BERT model over these units leads to the best performance; this approach outperforms directly learning representations from continuous speech features.


## How does this paper compare to other research in the same field?

This paper compares several different self-supervised representation learning approaches for speech recognition. The key findings of the paper are:- They compare methods that explicitly quantize/discretize the audio data (vq-wav2vec, k-means clustering) versus methods that learn directly on continuous inputs (wav2vec, raw MFCC/FBANK features). The quantization methods work better, showing that discrete unit discovery is an important first step before pre-training representations.- Pre-training representation models (Discrete BERT and Continuous BERT) leads to much better results compared to just using the features without pre-training, especially when labeled data is limited. This demonstrates the value of self-supervised pre-training.- Their best model, which quantizes audio with vq-wav2vec and then pre-trains a BERT model over the discrete units, achieves state-of-the-art results compared to prior work on Librispeech with only 10 hours of labeled data. It nearly matches performance of a prior model trained on 100x more labeled data.Some key comparisons to related work:- They outperform the CPC-based model of Kawakami et al. (2019) which also pre-trains representations on Librispeech but uses more labeled data (96h vs 1h/10min) to train the ASR model.- Their model surpasses the previous state-of-the-art result on Librispeech test-other using 100h labeled data, while using only 10h labeled data.- They show gains over directly fine-tuning the wav2vec model, demonstrating the value of stacking multiple levels of representation learning.- Their work follows the paradigm of other recent self-supervised speech models like wav2vec, vq-wav2vec, and CPC, but introduces the idea of discretizing the representations before BERT pre-training which proves very effective.Overall, this paper provides a strong empirical comparison of different self-supervised approaches and shows quantization + BERT pre-training is a simple but very effective recipe for learning powerful speech representations, especially in low resource settings. The results clearly advance the state-of-the-art in this area.
