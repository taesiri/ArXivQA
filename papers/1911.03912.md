# [Effectiveness of self-supervised pre-training for speech recognition](https://arxiv.org/abs/1911.03912)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:What are effective self-supervised pre-training methods for speech recognition, and how does quantizing/discretizing the inputs compare to directly learning representations from continuous speech inputs?The key hypotheses tested are:1) Learning discrete representations of speech through vector quantization, followed by pre-training a BERT model over these discrete units, will be more effective than directly learning representations from continuous speech inputs.2) Pre-training BERT models with self-supervision will improve performance in low-resource speech recognition scenarios with limited labeled data. 3) Two-step pre-training with discrete unit discovery followed by BERT pre-training provides complementary benefits compared to single-step pre-training methods like wav2vec.The paper systematically compares different self-supervised pre-training approaches, with and without explicit quantization of the speech inputs, across varying amounts of labeled data. The goal is to demonstrate the effectiveness of pre-training, particularly discrete unit discovery, for low-resource speech recognition.
