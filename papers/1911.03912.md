# [Effectiveness of self-supervised pre-training for speech recognition](https://arxiv.org/abs/1911.03912)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:What are effective self-supervised pre-training methods for speech recognition, and how does quantizing/discretizing the inputs compare to directly learning representations from continuous speech inputs?The key hypotheses tested are:1) Learning discrete representations of speech through vector quantization, followed by pre-training a BERT model over these discrete units, will be more effective than directly learning representations from continuous speech inputs.2) Pre-training BERT models with self-supervision will improve performance in low-resource speech recognition scenarios with limited labeled data. 3) Two-step pre-training with discrete unit discovery followed by BERT pre-training provides complementary benefits compared to single-step pre-training methods like wav2vec.The paper systematically compares different self-supervised pre-training approaches, with and without explicit quantization of the speech inputs, across varying amounts of labeled data. The goal is to demonstrate the effectiveness of pre-training, particularly discrete unit discovery, for low-resource speech recognition.


## What is the main contribution of this paper?

The main contributions of this paper are:- They compare different self-supervised representation learning approaches for speech recognition, including methods that explicitly quantize the speech signal versus methods that learn representations directly from the continuous signal. - They show that first quantizing the speech into discrete units with vq-wav2vec, and then pre-training a BERT model over these discrete units ("Discrete BERT") works better than learning representations directly from the continuous speech ("Continuous BERT").- They demonstrate that directly fine-tuning the pre-trained BERT models with a CTC loss for speech recognition gives better results than using the BERT features in tandem with a separate downstream ASR model as in previous work.- Their Discrete BERT model pre-trained on unlabeled Librispeech and fine-tuned on just 1 hour of labeled data outperforms the previous state-of-the-art result using 100 hours of labeled data.- With only 10 minutes of labeled data, their approach achieves 16.3/25.2 WER on the Librispeech test-clean/test-other benchmarks, showing the effectiveness of pre-training for low-resource speech recognition.In summary, they systematically compare different self-supervised pre-training methods and show strong results by first quantizing with vq-wav2vec, then pre-training BERT on the discrete units, and directly fine-tuning the BERT model, outperforming previous approaches especially in low labeled data settings.
