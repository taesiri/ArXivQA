# [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/abs/2305.10429)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we optimize the mixture proportions of domains (e.g. Wikipedia, books, web text) in a pretraining dataset to improve the efficiency and performance of training large language models?Specifically, the paper proposes and evaluates DoReMi, an algorithm that uses a small proxy model to optimize the domain weights (mixture proportions) of a pretraining dataset. The optimized weights are then used to resample the dataset and train a much larger language model. The key hypotheses are:1) Optimizing the domain weights without knowledge of downstream tasks can improve the efficiency and performance of language model pretraining.2) A small proxy model can be used to effectively optimize the domain weights for a much larger language model.3) Optimizing for the worst-case excess loss across domains with distributionally robust optimization leads to domain weights that improve perplexity and downstream performance across domains.The experiments aim to validate these hypotheses by evaluating DoReMi optimized weights compared to default or uniform weights on datasets like the Pile and GLaM. The results generally support the hypotheses, showing improvements in perplexity, downstream accuracy, and training efficiency.


## What is the main contribution of this paper?

The main contribution of this paper is proposing DoReMi, an algorithm for optimizing the mixture proportions (domain weights) of different data sources in a language modeling dataset. The key ideas are:- Use a small "proxy" language model trained with distributionally robust optimization (DRO) to find good domain weights, without needing downstream task data.- The DRO training minimizes the worst-case excess loss over domains. This allows finding a weighting that improves performance across all domains. - The optimized weights from the proxy model transfer to improve training of a much larger full-sized language model (e.g. 8B parameters).- Experiments show DoReMi improves perplexity on all domains of The Pile dataset. It also significantly improves downstream task accuracy compared to using The Pile's default heuristic weights.- On the GLaM dataset, DoReMi finds weights comparable to those tuned on downstream tasks, without using any downstream data.In summary, the main contribution is an efficient method to automatically find better domain mixtures for language model pretraining, which improves perplexity and downstream performance. The key idea is using a robust optimization-based proxy model to find domain weights that transfer to large full-sized models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes DoReMi, an algorithm that optimizes the mixture proportions of domains in a pretraining dataset for language models by training a small proxy model with group distributionally robust optimization to minimize worst-case excess loss over domains, and shows this improves perplexity and downstream task performance when used to resample a dataset for training much larger language models.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares and contributes to related work on optimizing data mixtures for pretraining language models:- The core novelty is using distributionally robust optimization (DRO) to automatically learn optimal mixture weights over text domains for pretraining, without needing any downstream tasks. This is a more general and efficient approach compared to methods like GLaM and PaLM that required searching over mixtures and tuning them on downstream performance.- It adapts prior work on DRO for language modeling by Oren et al. and Sagawa et al. that aimed to train a robust model. Instead, this work focuses on using DRO to robustly optimize the data distribution/mixture weights.- The proposed DoReMi algorithm innovates on prior DRO-LM methods by using an online Group DRO optimizer that allows end-to-end tuning of mixture weights. It also uses a reference model of the same scale as the proxy model rather than a simple bigram model.- The paper demonstrates strong empirical gains over heuristic baselines by optimizing mixtures for much larger main models (e.g. 8B parameters), with minimal extra compute for training the small proxy models. Even without downstream tasks, DoReMi matches performance of mixtures tuned on downstream data.- Overall, this represents an important advance in being able to automatically determine optimal data mixtures for pretraining giant language models efficiently. The analysis also surfaces insights about which domains are upweighted or downweighted by the algorithm.- In terms of limitations, the approach still requires training proxy models, and performance relies on how well the DRO training transfers across model scales. The qualitative analysis suggests multiple local optima may exist.- The work fits into a broader recent focus on optimizing the data/training for large language models, rather than just model scale or compute. This is a promising direction for enabling more efficient and performant LMs.
