# [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/abs/2305.10429)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we optimize the mixture proportions of domains (e.g. Wikipedia, books, web text) in a pretraining dataset to improve the efficiency and performance of training large language models?Specifically, the paper proposes and evaluates DoReMi, an algorithm that uses a small proxy model to optimize the domain weights (mixture proportions) of a pretraining dataset. The optimized weights are then used to resample the dataset and train a much larger language model. The key hypotheses are:1) Optimizing the domain weights without knowledge of downstream tasks can improve the efficiency and performance of language model pretraining.2) A small proxy model can be used to effectively optimize the domain weights for a much larger language model.3) Optimizing for the worst-case excess loss across domains with distributionally robust optimization leads to domain weights that improve perplexity and downstream performance across domains.The experiments aim to validate these hypotheses by evaluating DoReMi optimized weights compared to default or uniform weights on datasets like the Pile and GLaM. The results generally support the hypotheses, showing improvements in perplexity, downstream accuracy, and training efficiency.


## What is the main contribution of this paper?

The main contribution of this paper is proposing DoReMi, an algorithm for optimizing the mixture proportions (domain weights) of different data sources in a language modeling dataset. The key ideas are:- Use a small "proxy" language model trained with distributionally robust optimization (DRO) to find good domain weights, without needing downstream task data.- The DRO training minimizes the worst-case excess loss over domains. This allows finding a weighting that improves performance across all domains. - The optimized weights from the proxy model transfer to improve training of a much larger full-sized language model (e.g. 8B parameters).- Experiments show DoReMi improves perplexity on all domains of The Pile dataset. It also significantly improves downstream task accuracy compared to using The Pile's default heuristic weights.- On the GLaM dataset, DoReMi finds weights comparable to those tuned on downstream tasks, without using any downstream data.In summary, the main contribution is an efficient method to automatically find better domain mixtures for language model pretraining, which improves perplexity and downstream performance. The key idea is using a robust optimization-based proxy model to find domain weights that transfer to large full-sized models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes DoReMi, an algorithm that optimizes the mixture proportions of domains in a pretraining dataset for language models by training a small proxy model with group distributionally robust optimization to minimize worst-case excess loss over domains, and shows this improves perplexity and downstream task performance when used to resample a dataset for training much larger language models.
