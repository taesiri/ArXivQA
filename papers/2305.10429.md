# [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/abs/2305.10429)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we optimize the mixture proportions of domains (e.g. Wikipedia, books, web text) in a pretraining dataset to improve the efficiency and performance of training large language models?Specifically, the paper proposes and evaluates DoReMi, an algorithm that uses a small proxy model to optimize the domain weights (mixture proportions) of a pretraining dataset. The optimized weights are then used to resample the dataset and train a much larger language model. The key hypotheses are:1) Optimizing the domain weights without knowledge of downstream tasks can improve the efficiency and performance of language model pretraining.2) A small proxy model can be used to effectively optimize the domain weights for a much larger language model.3) Optimizing for the worst-case excess loss across domains with distributionally robust optimization leads to domain weights that improve perplexity and downstream performance across domains.The experiments aim to validate these hypotheses by evaluating DoReMi optimized weights compared to default or uniform weights on datasets like the Pile and GLaM. The results generally support the hypotheses, showing improvements in perplexity, downstream accuracy, and training efficiency.
