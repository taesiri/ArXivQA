# [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/abs/2305.10429)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: 

How can we optimize the mixture proportions of domains (e.g. Wikipedia, books, web text) in a pretraining dataset to improve the efficiency and performance of training large language models?

Specifically, the paper proposes and evaluates DoReMi, an algorithm that uses a small proxy model to optimize the domain weights (mixture proportions) of a pretraining dataset. The optimized weights are then used to resample the dataset and train a much larger language model. 

The key hypotheses are:

1) Optimizing the domain weights without knowledge of downstream tasks can improve the efficiency and performance of language model pretraining.

2) A small proxy model can be used to effectively optimize the domain weights for a much larger language model.

3) Optimizing for the worst-case excess loss across domains with distributionally robust optimization leads to domain weights that improve perplexity and downstream performance across domains.

The experiments aim to validate these hypotheses by evaluating DoReMi optimized weights compared to default or uniform weights on datasets like the Pile and GLaM. The results generally support the hypotheses, showing improvements in perplexity, downstream accuracy, and training efficiency.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing DoReMi, an algorithm for optimizing the mixture proportions (domain weights) of different data sources in a language modeling dataset. The key ideas are:

- Use a small "proxy" language model trained with distributionally robust optimization (DRO) to find good domain weights, without needing downstream task data.

- The DRO training minimizes the worst-case excess loss over domains. This allows finding a weighting that improves performance across all domains. 

- The optimized weights from the proxy model transfer to improve training of a much larger full-sized language model (e.g. 8B parameters).

- Experiments show DoReMi improves perplexity on all domains of The Pile dataset. It also significantly improves downstream task accuracy compared to using The Pile's default heuristic weights.

- On the GLaM dataset, DoReMi finds weights comparable to those tuned on downstream tasks, without using any downstream data.

In summary, the main contribution is an efficient method to automatically find better domain mixtures for language model pretraining, which improves perplexity and downstream performance. The key idea is using a robust optimization-based proxy model to find domain weights that transfer to large full-sized models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes DoReMi, an algorithm that optimizes the mixture proportions of domains in a pretraining dataset for language models by training a small proxy model with group distributionally robust optimization to minimize worst-case excess loss over domains, and shows this improves perplexity and downstream task performance when used to resample a dataset for training much larger language models.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares and contributes to related work on optimizing data mixtures for pretraining language models:

- The core novelty is using distributionally robust optimization (DRO) to automatically learn optimal mixture weights over text domains for pretraining, without needing any downstream tasks. This is a more general and efficient approach compared to methods like GLaM and PaLM that required searching over mixtures and tuning them on downstream performance.

- It adapts prior work on DRO for language modeling by Oren et al. and Sagawa et al. that aimed to train a robust model. Instead, this work focuses on using DRO to robustly optimize the data distribution/mixture weights.

- The proposed DoReMi algorithm innovates on prior DRO-LM methods by using an online Group DRO optimizer that allows end-to-end tuning of mixture weights. It also uses a reference model of the same scale as the proxy model rather than a simple bigram model.

- The paper demonstrates strong empirical gains over heuristic baselines by optimizing mixtures for much larger main models (e.g. 8B parameters), with minimal extra compute for training the small proxy models. Even without downstream tasks, DoReMi matches performance of mixtures tuned on downstream data.

- Overall, this represents an important advance in being able to automatically determine optimal data mixtures for pretraining giant language models efficiently. The analysis also surfaces insights about which domains are upweighted or downweighted by the algorithm.

- In terms of limitations, the approach still requires training proxy models, and performance relies on how well the DRO training transfers across model scales. The qualitative analysis suggests multiple local optima may exist.

- The work fits into a broader recent focus on optimizing the data/training for large language models, rather than just model scale or compute. This is a promising direction for enabling more efficient and performant LMs.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions in the discussion section:

- Investigating dependence of the mixture weights found by DoReMi on the training algorithm. While DoReMi runs the training algorithm to obtain the reference/proxy models, the authors believe the benefits should transfer across different model architectures, training objectives, etc.

- Using extrapolation to save compute in DoReMi by stopping the algorithm early and extrapolating the mixture weights. The weights seem to stabilize quickly during training.

- Varying the reference model in DoReMi, such as using different sizes or specialized references tuned for a particular application area.

- Defining domains at a more fine-grained level beyond just data source/provenance. Using finer domains could lead to bigger gains from optimizing mixture weights.

- Studying the broader impacts of DoReMi-style training, which aims for good worst-case performance across domains, on fairness and mitigating biases in language models.

- Adapting the group DRO optimizer used in DoReMi to be resampling-based instead of reweighting-based, which could improve performance for larger proxy models.

Overall, the authors suggest directions related to understanding what factors affect the optimized weights, using DoReMi more efficiently, applying it to finer-grained domains, and investigating societal impacts. Improving the group DRO optimizer is also mentioned as a potential research direction.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes DoReMi, a method for optimizing the mixture proportions of different domains (e.g. Wikipedia, books, web text) in a language modeling dataset. It first trains a small proxy model using group distributionally robust optimization (Group DRO) to produce domain weights (mixture proportions) without knowledge of downstream tasks. Group DRO minimizes the worst-case excess loss over domains, which incentivizes assigning higher weight to domains where the proxy model still has headroom to improve over the reference model. The domain weights from the proxy model are then used to resample the training data, and a much larger main model is trained on this reweighted dataset. Experiments show that DoReMi improves perplexity on all domains of the Pile dataset even when downweighting some domains. It also improves downstream task accuracy by 6.5% over the baseline Pile dataset and achieves the baseline accuracy 2.6x faster when training an 8B parameter model. Overall, DoReMi provides an automatic way to optimize mixture proportions for language modeling datasets without relying on heuristics or downstream tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new algorithm called Domain Reweighting with Minimax Optimization (DoReMi) for optimizing the mixture proportions of domains in a pretraining dataset to improve language model performance. DoReMi first trains a small proxy model using group distributionally robust optimization (Group DRO) over the domains to produce optimized domain weights (mixture proportions) without using any downstream tasks. It uses the losses of the proxy model relative to a reference model to determine which domains are more difficult for the current proxy model. The weights are updated to upweight more difficult domains. The averaged weights over training are returned. These weights define a new training distribution that resamples the original dataset. A much larger main model is then trained on data sampled from the new distribution. 

Experiments show that weights optimized by DoReMi on a 280M proxy model improve perplexity on all domains of The Pile dataset when used to train an 8B main model, even for downweighted domains. This is because easy and noisy domains can be downweighted to focus sampling on medium-difficulty domains, improving overall perplexity through positive transfer. On The Pile, DoReMi also improves average downstream accuracy on generative tasks by 6.5% and reaches baseline accuracy 2.6x faster than training on the default Pile mixtures. On the GLAM dataset, DoReMi finds weights that perform comparably to those tuned on downstream tasks. The method provides an automatic way to improve language model training efficiency and performance.


## Summarize the main method used in the paper in one paragraph.

 Here is a summary of the main method in one paragraph:

The paper proposes Domain Reweighting with Minimax Optimization (DoReMi), an algorithm that optimizes the data weighting (mixture proportions) across domains for training language models (LMs) more efficiently. DoReMi first trains a small reference LM on some initial data weighting. It then trains a small proxy LM using group distributionally robust optimization (Group DRO) over the domains, which produces optimized data weights for each domain that minimize the worst-case excess loss across domains compared to the reference LM. DoReMi takes these weights averaged over the Group DRO training trajectory, and uses them to resample the dataset. A much larger LM can then be trained on this resampled dataset. The key aspects are training small models to determine data weights for domains, and using the group DRO framework to optimize the weights by minimizing worst-case excess loss across domains.


## What problem or question is the paper addressing?

 The paper addresses the problem of how to efficiently optimize the mixture proportions (domain weights) of different data sources (e.g. Wikipedia, books, web text) in a pretraining dataset to improve the performance of large language models trained on that data. Specifically, it aims to find optimal domain weights without needing to train many large models on different weightings or tuning the weights based on downstream tasks.

The key questions it tries to answer are:

- How can we efficiently find good domain weights for a pretraining dataset to improve language model performance, without needing to train many large models to test different weightings?

- Can we find domain weights that improve perplexity and downstream task performance across all domains, without trading off performance on some domains?

- Can domain weights found with small proxy models transfer gains to much larger language models? 

- How do the optimized domain weights compare to heuristic baseline weights or weights tuned on downstream tasks?

So in summary, it tackles the problem of efficiently finding domain weights for pretraining data that can improve language model training, using only small proxy models and without needing downstream task data. The core questions are around how to do this optimization effectively and how the optimized weights transfer and compare to other weighting schemes.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Language model pretraining
- Mixture proportions of domains
- Domain reweighting
- Group distributionally robust optimization (Group DRO)
- Proxy model
- Downstream task performance
- The Pile dataset
- Perplexity
- Few-shot learning

The paper proposes an algorithm called DoReMi (Domain Reweighting with Minimax Optimization) to optimize the mixture proportions (weights) of different domains in a text dataset to improve language model pretraining. 

Key ideas include:

- Using a small proxy model trained with Group DRO to optimize domain weights without needing downstream tasks. 

- The proxy model minimizes worst-case excess loss over domains to determine weights.

- DoReMi weights are used to resample the training data for a much larger main model.

- Experiments show DoReMi reduces perplexity on all domains of The Pile dataset and improves few-shot downstream accuracy, even outperforming weights tuned on downstream tasks.

So in summary, the core focus is on automatic data reweighting for more efficient language model pretraining, enabled by a novel application of Group DRO. Key terms revolve around language modeling, robust optimization, domain reweighting, pretraining efficiency, and perplexity/downstream performance.
