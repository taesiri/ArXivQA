# [Kernel KMeans clustering splits for end-to-end unsupervised decision   trees](https://arxiv.org/abs/2402.12232)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Decision trees are interpretable models useful for clustering and classification. However, most methods for training unsupervised decision trees first cluster the data using another algorithm like K-Means, then learn a tree to explain those clusters. This couples the tree structure to the clusters found by K-Means.

- Kernel K-Means can find clusters in nonlinear spaces using kernels, but suffers from empty clusters and provides no explanation.

Proposed Solution:
- The paper proposes an end-to-end method called Kauri to greedily optimize a kernel K-Means clustering objective directly, removing the need for pre-clustering. 

- They rephrase the kernel K-Means objective without explicit centroids, instead using kernel alignments between samples. This allows greedy optimization through a decision tree structure by proposing splits and evaluating gains.

- Four types of splits are considered: creating new clusters, assigning samples to existing clusters, creating two new clusters, or reallocating samples between existing clusters. Explicit update rules are derived for efficiently evaluating gains.

Main Contributions:

- First end-to-end kernelised clustering tree, not requiring pre-clustering. Allows finding nonlinear clusters and explanations.

- Achieves performance on par with K-Means+tree baselines on benchmark datasets, often with simpler tree structures. Handles issues with empty clusters in kernel K-Means.

- Demonstrates flexibility in using non-linear kernels, where alternatives requiring centroids fail. Shows improved clustering and explanation quality over baselines.

- Provides an efficient implementation and update rules to greedily evaluate quality of proposed splits in the tree.

In summary, the paper presents a novel end-to-end trainable tree method for nonlinear unsupervised clustering that optimizes a kernel K-Means objective directly in a greedy fashion, removing dependence on pre-clustering while allowing flexibility with kernels.
