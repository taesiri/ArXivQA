# [Learning Structure-Aware Representations of Dependent Types](https://arxiv.org/abs/2402.02104)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- There is a lack of machine learning resources and results for the Agda proof assistant compared to other assistants like Coq and Lean. Agda has innovative features and an actively developing ecosystem, so it could benefit from ML automation.
- Prior ML approaches for theorem proving rely primarily on sequence models operating on surface string representations. This fails to faithfully capture the precise inductive structure and variable binding that is critical in formal proofs.

Proposed Solution:
- Introduce the first ML dataset for Agda by implementing an extraction tool called \agdaToTrain that produces JSON representations of Agda program-proofs. The dataset has very high resolution detail on proof states and type structure.
- Propose a novel neural architecture that can faithfully represent dependent type expressions based on their structure rather than surface strings. Handles variables, bindings and references in a way that is invariant to renaming/substitution.
- Evaluate the architecture on a premise selection task using the new dataset. Premise selection involves identifying which available lemmas are relevant for proving a given goal.

Main Contributions:
- Release an extensive and elaborate dataset of Agda program-proofs extracted from the standard library. Enables various ML applications on Agda.
- Design a neural architecture that can represent dependent types and bindings faithfully. Overcomes limitations of prior sequence or tree encoders.
- Achieve strong initial results on premise selection, demonstrating the dataset's utility and the benefits of structurally sound representations.
- The approach is universal and could be applied to other languages with dependent types. The model is also very compact compared to large language models.

In summary, the paper introduces the first ML dataset for Agda to stimulate research at the intersection of machine learning and proof assistants. It also proposes a way to represent formal objects like proofs in a structurally faithful way for improved learning. Initial results on premise selection confirm the soundness of the approach.
