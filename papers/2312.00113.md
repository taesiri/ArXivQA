# [Event-based Continuous Color Video Decompression from Single Frames](https://arxiv.org/abs/2312.00113)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
- Capturing high-quality, temporally continuous videos is challenging due to hardware limitations of traditional cameras like limited bandwidth, dynamic range constraints, and global shutter causing temporal aliasing and discontinuous motions. 

- Existing solutions like video interpolation suffer from relying on sparse input frames making them susceptible to lighting changes or large motions between frames.

Proposed Solution:
- The paper proposes a new task called "event-based continuous color video decompression" which uses a single RGB image combined with a stream of events from an event camera to generate a temporally continuous, high frame rate video.

- The key idea is that the event camera provides compressed encoded information about changes in the scene at very high temporal resolution that can supplement a single RGB image to recreate in-between RGB frames at any desired time.

Main Contributions:

- A continuous trajectory field module that estimates nonlinear trajectories for each pixel parameterized by a learned motion basis that allows querying motions at arbitrary times

- A K-plane based neural synthesis module that encodes spatio-temporal features into three separate planes to enable efficient decoding into RGB frames

- A multi-scale feature fusion network that merges information from the above two modules to generate the final RGB frames

- A new dataset collected using a novel beam-splitter camera setup for synchronized capture of RGB frames and events

- Quantitative evaluation showing the approach outperforms image-based and event-based baselines in video reconstruction. Qualitative demonstration of using the reconstructed video for downstream tasks like 3D reconstruction and tag detection.

The key novelty is in exploiting events for long-term motion modeling and neural synthesis of in-between RGB frames to achieve temporally continuous video from only a single input RGB image.
