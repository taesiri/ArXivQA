# [(Beyond) Reasonable Doubt: Challenges that Public Defenders Face in   Scrutinizing AI in Court](https://arxiv.org/abs/2403.13004)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies the challenges that public defenders in the U.S. face when scrutinizing and contesting the reliability of computational forensic software (CFS) used by the government to convict and incarcerate people. CFS refers to automated decision systems like facial recognition, gunshot detection, and probabilistic genotyping tools. Making such systems contestable is important for accountability, but there may be practical barriers. 

The paper focuses specifically on public defenders, who represent indigent defendants and have a duty to rigorously scrutinize evidence used against their clients. The research question is: What challenges do public defenders face when scrutinizing the government's use of automated decision systems like facial recognition and probabilistic genotyping software in the criminal legal system?

Proposed Solution and Methods:
The authors conducted 17 semi-structured interviews with public defenders and others working with them on technology issues in criminal cases. They asked about experiences with and challenges in assessing/contesting CFS reliability.

Key Findings:
Public defenders faced 3 main challenges:

1) Navigating CFS developers/users - Actions limited defenders' ability to assess reliability, e.g. insufficient testing/documentation, withdrawing evidence when scrutinized.  

2) Overcoming judges/jurors' non-critical perceptions of CFS - Their prior beliefs, deference to prosecutors, and difficulties understanding technology led to uncritical views of CFS.

3) Gathering CFS expertise - Defenders relied heavily on outside experts to help scrutinize CFS, but faced difficulties finding available experts willing to work with the defense. Building internal expertise is costly and infeasible for most offices.

Implications:
More work is needed to ensure performance evaluations effectively support contestability. This includes:
1) Considering external factors that constrain opportunities for scrutiny
2) Contextualizing evaluation design in real-world use cases 
3) Understanding how factors like deliberation, presentation complexity and prior beliefs shape interpretations of performance.

In summary, the paper reveals a range of technical, social and institutional barriers that public defenders face in assessing CFS reliability, and highlights areas for future work to better position performance evaluations to support contestability in practice.


## Summarize the paper in one sentence.

 This paper investigates the challenges public defenders face when assessing and contesting the reliability of computational forensic software used by the government to convict and incarcerate, finding they encounter technical, social, and institutional barriers stemming from developers, users, judges, and jurors that constrain efforts to scrutinize these automated decision systems.


## What is the main contribution of this paper?

 This paper's main contribution is providing insight into the challenges that public defenders face when assessing and contesting the reliability of computational forensic software (CFS) used by the government to convict and incarcerate people in the criminal justice system. 

Through interviews with 17 public defenders and experts who work with them, the authors identify three primary challenges defenders face in scrutinizing CFS:

1) Navigating policies and practices of CFS developers and users that constrain defenders' ability to gather information to assess reliability. For example, claims of trade secrecy, restrictive NDAs, and ties between prosecutors and forensic labs.

2) Overcoming judges' and jurors' non-critical perceptions of CFS driven by their prior beliefs about technology, perceptions of CFS users (e.g. law enforcement), and difficulties understanding technical concepts.

3) Gathering relevant CFS expertise from scientists, technologists, and other lawyers, which is hindered by limited availability of independent experts and insufficient funding to hire experts or build in-house expertise.

Based on these findings, the paper discusses implications for ensuring performance evaluations are effective for public defenders seeking to contest automated decision systems like CFS. This includes examining barriers outside of evaluation design that constrain scrutiny, contextualizing evaluation design in real-world applications, and understanding factors shaping interpretation of performance results.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Computational forensic software (CFS): Automated decision systems used by the government to convict and incarcerate, such as facial recognition systems, gunshot detection systems, probabilistic genotyping software, etc. A key focus of the paper.

- Public defenders: Lawyers who represent indigent criminal defendants who cannot afford their own attorney. The paper studies the challenges public defenders face in assessing and contesting CFS. 

- Contestability: The concept of automated decision systems being open to scrutiny, disagreement, and challenges. Assessing CFS contestability is a goal discussed in the paper.

- Performance evaluations: Studies evaluating the reliability and effectiveness of algorithmic systems. The paper discusses their role in supporting contestability of CFS.

- Facial recognition systems: One type of CFS studied as a case example.

- Probabilistic genotyping software: Another type of CFS studied, used to interpret complex DNA evidence. 

- Likelihood ratios: A statistic output by probabilistic genotyping software and interpreted by courts.

- Algorithmic accountability, algorithmic fairness, responsible AI: Broader fields studying ethical issues with algorithmic systems that this work builds on.

In summary, the key ideas have to do with studying how public defenders assess and contest the government's use of computational forensic software, and the challenges they face in leveraging performance evaluations to support contestability.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper relies heavily on semi-structured interviews with public defenders to understand their perspectives and experiences. What are some limitations of this methodological approach, and how might complementing the interviews with other methods provide additional insights?

2. The authors use purposive and convenience sampling to recruit interview participants. How might this sampling strategy introduce biases into the findings? How could the authors assess the extent to which their findings generalize? 

3. The authors acknowledge that public defenders' perspectives should not be interpreted as proxies for those of defendants. What methods could the authors use to better center the experiences of defendants while being sensitive to power imbalances and defendants' vulnerable position?

4. The authors analyze their interview data using an inductive, bottom-up approach. What alternative analytic approaches could they have taken, and what tradeoffs might those entail? For example, how could taking a more theory-driven approach change the findings?

5. The authors use storyboards in some of their interviews. What are the strengths and limitations of using speculative storyboards as interview prompts with domain experts? How could the visual nature of storyboards introduce biases?

6. This study focuses specifically on computational forensic software. What new insights or challenges might emerge by broadening the scope to study a wider range of automated decision systems used in the criminal justice system?

7. The authors acknowledge their lack of professional experience in the criminal justice system. How might their positionality shape the research process, from developing the research questions to analyzing and making sense of the data? 

8. The authors connect their findings to ideas around contestability and performance evaluations. To what extent do you think their findings actually speak to those theoretical constructs? Are there alternative theoretical lenses that could provide additional explanatory power?

9. This study examines challenges that public defenders face in one specific context â€“ the U.S. criminal justice system. To what extent might the findings transfer or generalize to other high-stakes settings where automated decisions systems are used, such as healthcare and social services? What cross-context comparisons might be worthwhile to pursue?

10. The authors offer recommendations for how to shape performance evaluations to better support contestability in practice. Which of those recommendations seem most feasible to implement in the short-term? Over the longer-term? And what might be some barriers to adopting those recommendations?
