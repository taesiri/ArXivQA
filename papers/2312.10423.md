# [Stochastic Bayesian Optimization with Unknown Continuous Context   Distribution via Kernel Density Estimation](https://arxiv.org/abs/2312.10423)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers the problem of Bayesian optimization (BO) of an objective function with unknown continuous context distributions. Specifically, the goal is to maximize the stochastic optimization (SO) objective $\mathbb{E}_{\bm{c} \sim p(\bm{c})}[f(\bm{x},\bm{c})]$ over decision variables $\bm{x} \in \mathcal{X}$, where $f$ is a black-box function and the distribution $p$ of context variables $\bm{c}$ is unknown and continuous. Existing works either ignore the context distribution, assume it is known, or discretize the continuous context space which can be inefficient. 

Proposed Solution: 
The paper proposes two BO algorithms - SBO-KDE and DRBO-KDE - that employ kernel density estimation (KDE) to estimate the unknown continuous context distribution. 

SBO-KDE uses KDE to obtain an estimate $\hat{p}$ of $p$, and optimizes an acquisition function based on the UCB criterion that takes the expectation under $\hat{p}$. To optimize the acquisition function, it uses Monte Carlo sampling and sample average approximation.

DRBO-KDE also uses KDE, but optimizes a distributionally robust acquisition function that considers the worst-case expectation over a distribution ball around $\hat{p}$ to account for errors in density estimation. It transforms this inner robust optimization problem into a two-dimensional stochastic program.

Main Contributions:
- First BO methods to directly optimize SO objectives with unknown continuous context distributions, without needing discretization.
- Provide sub-linear Bayesian cumulative regret bounds for both SBO-KDE and DRBO-KDE.
- Empirically demonstrate the effectiveness of the algorithms on synthetic functions and real-world optimization tasks.
- SBO-KDE is simpler and more suitable when the true distribution is not too complicated. DRBO-KDE has better robustness by accounting for distribution approximation errors using distributionally robust optimization.


## Summarize the paper in one sentence.

 This paper proposes two Bayesian optimization algorithms, SBO-KDE and DRBO-KDE, to optimize the expectation of black-box functions over unknown continuous context distributions by using kernel density estimation to learn the context distribution online.


## What is the main contribution of this paper?

 This paper proposes two Bayesian optimization algorithms, SBO-KDE and DRBO-KDE, for stochastic optimization problems where the distribution of the continuous context variable is unknown. 

The key ideas are:

1) SBO-KDE uses kernel density estimation (KDE) to estimate the unknown context distribution and directly optimizes the stochastic optimization (SO) objective under the estimated distribution.

2) Considering that the estimated distribution may have high error for complicated true distributions, DRBO-KDE optimizes a distributionally robust objective by taking the worst-case expectation over a distribution set centered at the estimated distribution.

The main contributions are:

- Proposing two algorithms that use KDE for optimizing problems with unknown continuous context distributions, avoiding drawbacks of discretization.

- Providing theoretical analysis to show that both algorithms have sub-linear Bayesian cumulative regret for the SO objective. 

- Conducting experiments on synthetic and real-world problems to demonstrate the effectiveness of the proposed algorithms over existing methods.

In summary, the main contribution is developing two sample-efficient Bayesian optimization algorithms with theoretical guarantees that can directly optimize problems with unknown continuous context distributions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Bayesian optimization (BO)
- Sample efficiency
- Contextual optimization 
- Stochastic optimization (SO) 
- Distributionally robust Bayesian optimization (DRBO)
- Kernel density estimation (KDE)
- Probability density function (PDF)
- Mean integrated square error (MISE)
- Bayesian cumulative regret (BCR)
- Acquisition function
- Upper confidence bound (UCB)
- Sample average approximation (SAA)

The paper proposes two Bayesian optimization algorithms, SBO-KDE and DRBO-KDE, to optimize the expectation of black-box functions over unknown continuous context distributions. Both algorithms employ kernel density estimation to learn the context distribution and optimize acquisition functions based on that. SBO-KDE directly optimizes the stochastic optimization objective while DRBO-KDE considers a distributionally robust objective. Theoretical analysis shows both algorithms have sub-linear Bayesian cumulative regret. Experiments on synthetic and real-world problems demonstrate their effectiveness over other baselines. So the core focus is on stochastic/contextual Bayesian optimization and learning unknown distributions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper proposes two algorithms, SBO-KDE and DRBO-KDE, that employ kernel density estimation to estimate the unknown continuous context distribution. What are the key differences between these two algorithms and why was DRBO-KDE proposed in addition to the simpler SBO-KDE?

2. Kernel density estimation is used to estimate the unknown context distribution. What are the theoretical guarantees provided in the paper regarding the quality of this density estimation, as measured by mean integrated squared error? How do these guarantees depend on the number of observed context samples?

3. Both SBO-KDE and DRBO-KDE rely on sample average approximation (SAA) to optimize the acquisition function. What theoretical results are provided in the paper regarding the quality of the acquisition function optimization? What conditions need to be satisfied?

4. For DRBO-KDE, how is the inner convex optimization problem formed to optimize the distributionally robust acquisition function? What transformation technique is used and what is the complexity of this optimization?

5. What are the differences between the Bayesian cumulative regret bounds provided for SBO-KDE versus DRBO-KDE? How do these bounds scale with the context dimension and number of iterations? When would DRBO-KDE potentially have better regret than SBO-KDE?

6. The experiments compare against several baselines including DRBO-MMD which also considers distributional robustness but relies on discretization. What are the limitations of DRBO-MMD that motivated the proposed approaches? When does DRBO-MMD perform competitively?

7. For the complicated Hartmann function, there is no ground truth stochastic objective to compare regret against. What metric is used instead and why does this make sense? How do the algorithms compare in this setting?

8. What differences are observed between the synthetic and real-world experiments? When do SBO-KDE and DRBO-KDE differ in performance and why?

9. The context dimensionality is limited in the experiments. What experiment is provided in the appendix to analyze performance for higher dimensional context? What is the outcome?

10. What analysis is provided in the appendix regarding the total variation discrepancy between the estimated and true context distributions? How does this help motivate the need for and analyze the benefits of DRBO-KDE?
