# [Stochastic Bayesian Optimization with Unknown Continuous Context   Distribution via Kernel Density Estimation](https://arxiv.org/abs/2312.10423)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers the problem of Bayesian optimization (BO) of an objective function with unknown continuous context distributions. Specifically, the goal is to maximize the stochastic optimization (SO) objective $\mathbb{E}_{\bm{c} \sim p(\bm{c})}[f(\bm{x},\bm{c})]$ over decision variables $\bm{x} \in \mathcal{X}$, where $f$ is a black-box function and the distribution $p$ of context variables $\bm{c}$ is unknown and continuous. Existing works either ignore the context distribution, assume it is known, or discretize the continuous context space which can be inefficient. 

Proposed Solution: 
The paper proposes two BO algorithms - SBO-KDE and DRBO-KDE - that employ kernel density estimation (KDE) to estimate the unknown continuous context distribution. 

SBO-KDE uses KDE to obtain an estimate $\hat{p}$ of $p$, and optimizes an acquisition function based on the UCB criterion that takes the expectation under $\hat{p}$. To optimize the acquisition function, it uses Monte Carlo sampling and sample average approximation.

DRBO-KDE also uses KDE, but optimizes a distributionally robust acquisition function that considers the worst-case expectation over a distribution ball around $\hat{p}$ to account for errors in density estimation. It transforms this inner robust optimization problem into a two-dimensional stochastic program.

Main Contributions:
- First BO methods to directly optimize SO objectives with unknown continuous context distributions, without needing discretization.
- Provide sub-linear Bayesian cumulative regret bounds for both SBO-KDE and DRBO-KDE.
- Empirically demonstrate the effectiveness of the algorithms on synthetic functions and real-world optimization tasks.
- SBO-KDE is simpler and more suitable when the true distribution is not too complicated. DRBO-KDE has better robustness by accounting for distribution approximation errors using distributionally robust optimization.
