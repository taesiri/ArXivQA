# [Self-supervised learning of visual features through embedding images   into text topic spaces](https://arxiv.org/abs/1705.08631)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we learn useful visual features in a self-supervised manner by training a CNN to predict the semantic context that images illustrate, using freely available multi-modal (image + text) data?The key hypotheses appear to be:1) The semantic topics discovered from text in illustrated articles can provide supervision signals for training a CNN, despite being noisy. 2) By training a CNN to predict which semantic topic an image most likely illustrates, the network will learn visual features that capture semantic relationships and are useful for visual tasks like classification and retrieval.3) The self-supervised features learned this way, without human annotations, can compete with or exceed other unsupervised and self-supervised methods on benchmark tasks.So in summary, the central research question is whether semantic topics from text can provide a supervisory signal for self-supervised visual feature learning from multi-modal data, to learn features that capture semantic relationships and work well for vision tasks. The key hypotheses are that the noisy semantic supervision can still be effective, and the learned features will be useful compared to other unsupervised methods.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a method for self-supervised learning of visual features by leveraging the correlation between images and text in illustrated articles. Specifically:- The paper proposes training a CNN to predict the semantic text topics that an image illustrates, using LDA topic modeling on the articles' text. - This allows the CNN to learn useful visual features in a self-supervised manner, without requiring manually annotated image datasets.- The learned features are evaluated on image classification, object detection, and multi-modal retrieval tasks, showing competitive performance compared to other self-supervised and unsupervised methods.- The model can naturally perform multi-modal retrieval, searching images by text queries or vice versa, without needing any extra annotation or learning.In summary, the key contribution is using the co-occurrence of images and text topics in illustrated articles as a supervisory signal for self-supervised learning of visual features in a CNN, which is shown to work well despite the noise in matching images to text topics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a self-supervised method for training a CNN to predict the semantic context of images by leveraging freely available multi-modal data of illustrated articles and modeling text with LDA topic modeling.
