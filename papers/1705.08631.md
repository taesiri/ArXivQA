# [Self-supervised learning of visual features through embedding images   into text topic spaces](https://arxiv.org/abs/1705.08631)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we learn useful visual features in a self-supervised manner by training a CNN to predict the semantic context that images illustrate, using freely available multi-modal (image + text) data?The key hypotheses appear to be:1) The semantic topics discovered from text in illustrated articles can provide supervision signals for training a CNN, despite being noisy. 2) By training a CNN to predict which semantic topic an image most likely illustrates, the network will learn visual features that capture semantic relationships and are useful for visual tasks like classification and retrieval.3) The self-supervised features learned this way, without human annotations, can compete with or exceed other unsupervised and self-supervised methods on benchmark tasks.So in summary, the central research question is whether semantic topics from text can provide a supervisory signal for self-supervised visual feature learning from multi-modal data, to learn features that capture semantic relationships and work well for vision tasks. The key hypotheses are that the noisy semantic supervision can still be effective, and the learned features will be useful compared to other unsupervised methods.
