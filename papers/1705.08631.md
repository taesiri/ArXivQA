# [Self-supervised learning of visual features through embedding images   into text topic spaces](https://arxiv.org/abs/1705.08631)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we learn useful visual features in a self-supervised manner by training a CNN to predict the semantic context that images illustrate, using freely available multi-modal (image + text) data?The key hypotheses appear to be:1) The semantic topics discovered from text in illustrated articles can provide supervision signals for training a CNN, despite being noisy. 2) By training a CNN to predict which semantic topic an image most likely illustrates, the network will learn visual features that capture semantic relationships and are useful for visual tasks like classification and retrieval.3) The self-supervised features learned this way, without human annotations, can compete with or exceed other unsupervised and self-supervised methods on benchmark tasks.So in summary, the central research question is whether semantic topics from text can provide a supervisory signal for self-supervised visual feature learning from multi-modal data, to learn features that capture semantic relationships and work well for vision tasks. The key hypotheses are that the noisy semantic supervision can still be effective, and the learned features will be useful compared to other unsupervised methods.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a method for self-supervised learning of visual features by leveraging the correlation between images and text in illustrated articles. Specifically:- The paper proposes training a CNN to predict the semantic text topics that an image illustrates, using LDA topic modeling on the articles' text. - This allows the CNN to learn useful visual features in a self-supervised manner, without requiring manually annotated image datasets.- The learned features are evaluated on image classification, object detection, and multi-modal retrieval tasks, showing competitive performance compared to other self-supervised and unsupervised methods.- The model can naturally perform multi-modal retrieval, searching images by text queries or vice versa, without needing any extra annotation or learning.In summary, the key contribution is using the co-occurrence of images and text topics in illustrated articles as a supervisory signal for self-supervised learning of visual features in a CNN, which is shown to work well despite the noise in matching images to text topics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a self-supervised method for training a CNN to predict the semantic context of images by leveraging freely available multi-modal data of illustrated articles and modeling text with LDA topic modeling.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in self-supervised learning of visual features:- The main novelty is using textual semantic information as the supervisory signal for training a CNN to predict image topic distributions. Prior self-supervised methods have used other signals like ego-motion, audio, etc. but not text. Leveraging text is a natural fit since text is commonly used to annotate images.- The results demonstrate state-of-the-art performance compared to other self-supervised methods on standard benchmarks like PASCAL classification and detection. The learned features even outperform some unsupervised methods like k-means clustering.- The idea is related to prior work on multimodal topic modeling for annotation and retrieval, but differs in using it for self-supervised CNN pre-training. So it explores a new application of topic models.- An appealing aspect is the ability to naturally do bi-directional image-text retrieval since images and text are embedded in a common semantic space. This is a novel capability compared to other self-supervised approaches.- A limitation is performance remains below fully supervised pre-training, which is expected. The method likely learns more generic features due to noise in matching images with article text. Using cleaner image captions improves performance.- The work reinforces the viability of self-supervised learning as an alternative to costly annotation for pre-training visual features. It expands the modalities that can be used for self-supervision in a novel way by exploiting image-text data.In summary, the key novelty is the use of freely available text as supervision signal, with results that are state-of-the-art for self-supervised methods and competitive with some unsupervised techniques. The work expands the scope of self-supervised learning.
