# [Backward Learning for Goal-Conditioned Policies](https://arxiv.org/abs/2312.05044)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method for learning goal-conditioned policies without rewards by using a backwards world model and graph search. The key idea is to first learn a latent dynamics model that can simulate state transitions going backwards in time. This is done by training an encoder-decoder model on random forward rollouts. Once trained, the backwards world model is used to generate goal-oriented trajectories by starting at a desired goal state and going backwards. To improve these sequences, they construct a directed graph representing state transitions and use Dijkstra's algorithm to find the shortest paths to the goal. By only keeping backward transitions that reduce the shortest path estimate, they filter the data to create a dataset of useful demonstrations for imitation learning. They show this approach can learn policies that successfully navigate a complex maze environment and reliably reach multiple goal locations purely by backwards planning and imitation, without any rewards. The method demonstrates that goal-conditioned policies can be acquired without the typical hit-and-miss exploration, instead ensuring by construction that the generated trajectories are goal-reaching.
