# [RUST: Latent Neural Scene Representations from Unposed Imagery](https://arxiv.org/abs/2211.14306)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn 3D scene representations from 2D images without requiring any camera pose information, neither for training nor for inference?The key hypothesis is that it should be possible to train "truly pose-free models from RGB images alone without requiring any ground truth pose information."The paper proposes RUST, a novel method for learning latent neural scene representations and generating novel views of complex 3D scenes using only unposed 2D RGB images. The main insight is that during training, a "Pose Estimator" module can peek at the target view and learn an implicit latent pose embedding, which can then be used by the decoder for novel view synthesis without needing explicit pose information.So in summary, the paper aims to show that high-quality neural scene representations can be learned without reliance on pose supervision, by having the model infer its own notion of camera poses. This could enable scaling such models to large uncurated photo collections where pose information is unavailable.


## What is the main contribution of this paper?

The main contribution of this paper is proposing RUST, a novel method for learning latent neural scene representations and generating novel views of 3D scenes from 2D RGB images alone, without requiring any camera pose information. Specifically, the key contributions are:- RUST can be trained end-to-end from unposed 2D RGB images to produce latent 3D scene representations. This is achieved through a novel Pose Estimator module that peeks at the target view and estimates an implicit latent pose embedding used by the decoder for novel view synthesis.- RUST strongly outperforms prior methods when camera poses are noisy or unavailable, while matching their performance when accurate pose information is provided.- The learned latent pose space is shown to have meaningful structure that allows for intuitive camera transformations like changing viewpoint height, distance, and rotation.- RUST demonstrates competitive novel view synthesis on complex synthetic and real-world datasets compared to state-of-the-art methods that have access to ground truth poses.- The usefulness of the learned scene representations is demonstrated through explicit pose readout and semantic segmentation tasks.Overall, the key insight is that implicit latent pose spaces can be learned in a self-supervised manner from unposed imagery alone. This enables training neural scene representations at scale without the need for pose information, unlocking new applications for such models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes RUST, a novel neural scene representation method that can be trained without camera poses to perform high-quality novel view synthesis by using a Pose Estimator module that peeks at the target view to infer an implicit latent pose for the decoder.
