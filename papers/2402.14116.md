# [FanOutQA: Multi-Hop, Multi-Document Question Answering for Large   Language Models](https://arxiv.org/abs/2402.14116)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of resources to evaluate complex reasoning abilities of large language models (LLMs), especially for "fan-out" questions. These are multi-hop, multi-document questions that require gathering information about multiple entities from different documents.

Proposed Solution:
- The authors present FanOutQA, a new dataset consisting of over 1,000 high-quality fan-out questions over Wikipedia, along with human-written decompositions and answers.

- They formulate 3 challenge settings to benchmark LLMs: closed-book (no access to documents), open-book (must retrieve relevant Wikipedia articles), and evidence-provided (given relevant articles).

- 7 LLMs were evaluated, including GPT-4, LLaMA 2.0, Claude 2.1, and others. Human performance was also evaluated.

Key Results:
- Closed-book was difficult for models (accuracy < 50%), showing lack of general knowledge needed.

- In open-book, models struggled with long contexts from retrieved articles. Human accuracy was 68.5%.

- Evidence-provided performance correlated strongly with context length, indicating the dataset properly tests long-range reasoning.

Main Contributions:
- A new high-quality dataset for complex reasoning over multiple documents covering multiple entities

- Formulation of 3 distinct challenge settings tapping into different LLM capabilities

- Benchmarking of SOTA models to establish baseline performance levels on this new challenging task, plus comparison to human performance

- Open-sourced dataset and tools to allow further research

Overall the paper makes a solid contribution in creating a valuable resource for testing and advancing complex reasoning abilities of large language models. The analysis clearly demonstrates current weaknesses and opportunities for progress.
