# [Towards On-device Learning on the Edge: Ways to Select Neurons to Update   under a Budget Constraint](https://arxiv.org/abs/2312.05282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- On-device training is critical for enabling continuous model improvement and adaptation in applications like mobile devices. However, the prohibitive computational and memory costs of backpropagation pose major obstacles. 
- Prior work has shown promise in static neuron selection strategies to reduce costs, but these have limitations in terms of generalizability across tasks and lack dynamism during training.

Proposed Solution:
- The paper proposes a dynamic neuron selection strategy called "Velocity" to select which parts of a model to update during on-device training under tight memory budgets.
- Velocity adapts the existing NEq strategy to work under budget constraints by prioritizing neuron updates based on a velocity metric tracking output changes and parameter sizes.
- A random selection baseline is also introduced for comparison.

Contributions:
- Velocity is compared to the prior static Sparse Update (SU) method and shows competitive or better accuracy in most cases when fine-tuning pretrained models.
- Additional experiments reveal Velocity also outperforms the random selection baseline in a majority of tested configurations.
- The findings demonstrate the potential for superior sub-networks found through dynamic approaches compared to static selection.
- This motivates further research into specialized algorithms for efficient, constrained on-device update selection.
- The feasibility of effective on-device learning without full backpropagation is reinforced, although managing training efficiency metrics like FLOPs usage remains an open challenge.

In summary, the paper makes a case for dynamic neuron update selections during efficient on-device training, challenging the notion that static selections are sufficient. The results open exciting new research avenues around developing optimized selection strategies for this emerging domain.
