# [Towards On-device Learning on the Edge: Ways to Select Neurons to Update   under a Budget Constraint](https://arxiv.org/abs/2312.05282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- On-device training is critical for enabling continuous model improvement and adaptation in applications like mobile devices. However, the prohibitive computational and memory costs of backpropagation pose major obstacles. 
- Prior work has shown promise in static neuron selection strategies to reduce costs, but these have limitations in terms of generalizability across tasks and lack dynamism during training.

Proposed Solution:
- The paper proposes a dynamic neuron selection strategy called "Velocity" to select which parts of a model to update during on-device training under tight memory budgets.
- Velocity adapts the existing NEq strategy to work under budget constraints by prioritizing neuron updates based on a velocity metric tracking output changes and parameter sizes.
- A random selection baseline is also introduced for comparison.

Contributions:
- Velocity is compared to the prior static Sparse Update (SU) method and shows competitive or better accuracy in most cases when fine-tuning pretrained models.
- Additional experiments reveal Velocity also outperforms the random selection baseline in a majority of tested configurations.
- The findings demonstrate the potential for superior sub-networks found through dynamic approaches compared to static selection.
- This motivates further research into specialized algorithms for efficient, constrained on-device update selection.
- The feasibility of effective on-device learning without full backpropagation is reinforced, although managing training efficiency metrics like FLOPs usage remains an open challenge.

In summary, the paper makes a case for dynamic neuron update selections during efficient on-device training, challenging the notion that static selections are sufficient. The results open exciting new research avenues around developing optimized selection strategies for this emerging domain.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper proposes and compares static and dynamic neuron selection strategies for on-device learning under extreme memory budgets, finding that a velocity-based dynamic selection approach can often outperform static selection and random selection.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Modifying a dynamic neuron update selection strategy called NEq to work under extreme memory budgets. This is described in Section 3.2.

2. Comparing the static neuron selection strategy from prior work (Sparse Update) to their proposed dynamic approach, and showing that the dynamic approach performs better on average (Section 4.1). 

3. Introducing a random dynamic neuron selection baseline and showing that their proposed velocity-based selection strategy outperforms this baseline in most cases (Section 4.2). This suggests the potential for designing new algorithms specifically for parameter update selection.

In summary, the main contribution is proposing and evaluating a dynamic neuron selection strategy for on-device learning under extreme memory constraints, which outperforms prior static selection methods as well as a random selection baseline. This highlights the potential benefit of dynamic selection strategies in this setting.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- On-device learning
- Extreme memory and computation constraints
- Backpropagation cost
- Static neuron update selection
- Dynamic neuron update selection
- Velocity-based selection
- Sparse Update (SU)
- Neurons at Equilibrium (NEq)
- Sub-network selection
- Budget constraints
- MobileNetV2
- ResNet

The main focus of the paper is on exploring static versus dynamic neuron update selection strategies for efficient on-device learning under tight memory and computation budgets. It compares an existing static selection method called Sparse Update (SU) to a proposed dynamic selection approach based on neuron equilibrium velocities (inspired by NEq). Experiments are conducted on image classification datasets using MobileNetV2 and ResNet models under varying levels of parameter update budgets. The key findings are that the dynamic velocity-based approach can outperform static SU selection in many cases, and also beats a random neuron selection baseline. This points to the potential for designing new dynamic neuron update selection algorithms for improved on-device learning under constraints.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes adapting the NEq neuron selection strategy to work under extreme memory budgets. What modifications were made to the original NEq algorithm to accommodate the budget constraints? How is the subset of neurons to update selected?

2. The paper introduces a velocity-based neuron selection method. What is the intuition behind using the neurons' velocity to determine which ones to update? How exactly is velocity defined and calculated in this context?

3. How does the proposed dynamic neuron selection strategy differ philosophically from the static Sparse Update (SU) scheme of Lin et al.? What are the potential advantages of a dynamic selection approach?

4. The paper finds the proposed velocity-based selection generally outperforms random selection. Why might selecting neurons in a principled way based on velocity lead to better performance compared to random selection? What factors might explain when random does marginally better?  

5. For the first epoch, the paper uses the SU scheme or random initialization since velocity is undefined. Does the choice of first epoch selection strategy impact final accuracy? Does this suggest velocity encodes useful information beyond what SU or random provide?

6. The gap between the low-budget methods and fully updating the network is surprisingly small. What factors might explain why updating a subset of neurons can reach close to baseline accuracy? When does low-budget updating actually outperform the baseline?

7. The paper mentions controlling FLOPs savings is less straightforward with a velocity-based selection than random. How could velocity-based selection be adapted to better account for expected training cost and energy consumption?

8. The proposed method selects neurons greedily based on velocity magnitude. How might reweighting velocity by the number of parameters in each neuron impact which neurons are selected? What is the motivation behind this reweighting?

9. The paper focuses on extremely constrained memory budgets. How suitable would the proposed velocity-based selection approach be for less extreme budget scenarios? Would the relative benefit compared to random selection diminish?

10. The paper demonstrates promising results on image classification tasks. What considerations would be necessary to apply the method to other domains like NLP? Would velocity remain an effective selection criteria across modalities?
