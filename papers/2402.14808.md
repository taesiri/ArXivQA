# [RelayAttention for Efficient Large Language Model Serving with Long   System Prompts](https://arxiv.org/abs/2402.14808)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are being widely used to provide services like chatbots, assistants, etc, where a long system prompt is concatenated with the user input. 
- However, long system prompts degrade throughput and latency of LLM services as the cost of generating each token grows with sequence length due to causal attention.
- Specifically, there is redundant memory access - the cached key-values (KVs) of the reusable system prompt are loaded from DRAM to on-chip SRAM multiple times, once per request in a batch.

Proposed Solution: 
- The paper proposes RelayAttention, a novel attention algorithm to eliminate this redundant memory access and accelerate LLM inference.
- It mathematically reformulates causal attention as a convex combination of system attention (over shared system prompt) and context attention (over context per request).
- This allows transforming multiple matrix-vector multiplications into matrix-matrix multiplications when computing attention over the system prompt for a batch. 
- Hence the system KVs are read from DRAM only once per batch, eliminating redundancy.

Main Contributions:
- Identified the redundant memory access issue with long system prompts in LLM services, which is a previously unexplored bottleneck.
- Proposed RelayAttention which provides speedup by reducing this redundancy while maintaining generation quality as it computes exact causal attention.
- Integrated into vLLM system and achieved up to 2.2x speedup in sustainable request rate for chatbot workload with Llama2-7B model. Gains increased for longer system prompts.
- Showed consistent gains over multiple popular LLMs and GPUs, for both batch processing and interactive serving scenarios.
