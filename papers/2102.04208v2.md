# [Contrastive Embeddings for Neural Architectures](https://arxiv.org/abs/2102.04208v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we develop neural architecture embeddings that are independent of the parametrization of the search space and capture intrinsic properties of the architectures?

The authors propose using contrastive learning on the extended projected data Jacobian matrices (EPDJMs) of architectures at initialization to learn such embeddings. The key hypotheses appear to be:

- The EPDJMs of architectures contain intrinsic information about their properties and potential performance. 

- Contrastive learning can discover useful embeddings from the EPDJMs that are predictive of architecture performance and independent of parametrization.

- These contrastive embeddings can enable effective architecture search and transfer learning across search spaces.

The paper then presents experiments validating these hypotheses by showing the embeddings can predict performance, enable competent architecture search, and transfer between different search spaces.

In summary, the central research question is how to develop architecture embeddings independent of parametrization that capture intrinsic properties. The key hypotheses are that EPDJMs contain useful intrinsic information, and contrastive learning can produce effective embeddings from them. The experiments aim to validate these hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a method to learn embeddings for neural network architectures using contrastive learning on the architectures' data Jacobians at initialization. The key ideas and contributions are:

- Using contrastive learning on the data Jacobians of architectures at initialization to learn an embedding space that is predictive of architecture performance, without relying on the architectures' parametrizations.

- Showing that the learned embeddings enable traditional black-box optimization algorithms to achieve strong performance on neural architecture search benchmarks like NAS-Bench-201.

- Demonstrating that the embedding space is unified across different search spaces, which allows for transfer learning between the search spaces (e.g. from NATS-Bench size to topology).

- Analyzing how the embeddings evolve during architecture training, suggesting they capture meaningful properties related to final performance.

In summary, the main contribution is developing contrastive architecture embeddings that are parametrization-invariant, predictive, and allow for transfer learning and analysis during training. This provides a new direction for neural architecture search focused on learning good embeddings rather than specialized search algorithms.
