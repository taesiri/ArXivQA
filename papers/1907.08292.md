# [Compositional Deep Learning](https://arxiv.org/abs/1907.08292)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research contributions of this paper appear to be:1. Providing a categorical framework for reasoning about compositionality in deep learning architectures. The paper translates concepts like neural network architectures, datasets, parameter-function maps, etc into the language of category theory. 2. Using this categorical framework to show that optimization can be done in the functor space between two fixed categories, rather than just in the function space between two sets. This allows for structured learning of concepts and relationships using gradient descent.3. Outlining a connection between this categorical deep learning formulation and categorical databases/data migration. There are similarities in how they specify categorical schemas, functor categories of instances, and notions of data integrity/path equivalences. 4. Using the category theory perspective to conceive a novel neural network architecture for learning to insert/delete objects in images using unpaired data. The "product task" setup imposes high-level path equivalence relations that provide a useful training signal.5. Testing this neural network architecture on 3 datasets - circles, CelebA faces, and StyleGAN latent vectors. The experiments demonstrate promising results for the approach, especially on the first two real image datasets.So in summary, the main hypotheses appear to be:- Category theory provides a useful framework for compositionality in deep learning.- Optimization can be done in functor spaces guided by categorical notions. - Path equivalences are a useful inductive bias for tasks like object insertion/deletion.The experiments then provide evidence to support the viability of these hypotheses.


## What is the main contribution of this paper?

Based on my reading of the paper, here is a summary of the main contribution:The paper lays out the beginnings of a formal compositional framework for reasoning about modern neural network architectures using category theory. The key contributions are:- Generalizing the construction of "Para" from prior work to allow compositional reasoning about parametrized neural network components. - Formalizing neural network architectures, datasets, embeddings, etc. categorically, showing that optimization can be done in the functor space between categories rather than just function spaces.- Outlining a correspondence between this categorical deep learning formulation and categorical databases.- Using the framework to conceive a novel neural network architecture for learning to insert and delete objects in images with unpaired data. Promising experimental results are demonstrated on three datasets.In summary, the paper provides a principled categorical perspective on neural network components and training. This allows complex neural network systems to be reasoned about compositionally. The framework is applied to develop a new architecture and task for semantic image manipulation using unpaired data. Overall, it aims to bridge category theory and deep learning in a practically useful way.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of compositional deep learning:- The use of category theory to formulate deep learning architectures and training procedures is novel. Most prior work in this area does not use category theory and is focused more on empirical results. This paper provides a new perspective and theoretical framework.- Translating concepts like neural network architectures, datasets, parameter-function maps, etc into categorical language is an interesting approach not taken by other papers. This allows viewing optimization in the functor space between categories rather than function spaces.- The comparison to categorical databases highlights an intriguing connection that I'm not aware has been made before. Using category theory to bridge deep learning and databases is a unique direction.- Defining a schema via a graph and equivalence relations to capture network structure seems more flexible than approaches in other papers that focus on specific architectures like CNNs. This is more general.- Optimization using adversarial and path equivalence losses generalizes concepts from GANs and CycleGAN in a novel theoretical setting.- The product task for learning to insert/delete objects is an interesting application that demonstrates the utility of the categorical framework. I'm not familiar with other papers approaching this task in the same way.Overall, the categorical perspective and formalism appear to be distinctive compared to related work. Making connections between deep learning, databases, and category theory is a contribution unique to this paper. The abstractions also enable flexibility in modeling architectures and defining tasks. If the theoretical ideas could be combined with strong empirical results, this line of research could potentially provide important new tools for deep learning.
