# [Selfie: Self-supervised Pretraining for Image Embedding](https://arxiv.org/abs/1906.02940)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that self-supervised pretraining techniques like masked language modeling can be generalized to continuous data like images to learn useful representations, especially when labeled data is scarce. Specifically, the authors propose a pretraining technique called GROVER that masks out patches of an image and tries to select the correct patch from "distractor" patches sampled from the same image. This allows the model to learn about the context and content of images in an unsupervised way. The main research question is whether this technique can improve performance on downstream supervised image classification tasks compared to training on just the labeled data, particularly when the amount of labeled data is small. The results across CIFAR-10, ImageNet 32x32, and ImageNet 224x224 show consistent improvements from the GROVER pretraining, especially when only 5-20% of the labeled data is used.So in summary, the central hypothesis is that masking-based pretraining like GROVER can learn useful visual representations to boost performance on downstream tasks, analogous to what has been shown in NLP with masked language modeling. The authors provide evidence for this hypothesis through gains on few-shot image classification.
