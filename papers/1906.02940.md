# [Selfie: Self-supervised Pretraining for Image Embedding](https://arxiv.org/abs/1906.02940)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that self-supervised pretraining techniques like masked language modeling can be generalized to continuous data like images to learn useful representations, especially when labeled data is scarce. Specifically, the authors propose a pretraining technique called GROVER that masks out patches of an image and tries to select the correct patch from "distractor" patches sampled from the same image. This allows the model to learn about the context and content of images in an unsupervised way. The main research question is whether this technique can improve performance on downstream supervised image classification tasks compared to training on just the labeled data, particularly when the amount of labeled data is small. The results across CIFAR-10, ImageNet 32x32, and ImageNet 224x224 show consistent improvements from the GROVER pretraining, especially when only 5-20% of the labeled data is used.So in summary, the central hypothesis is that masking-based pretraining like GROVER can learn useful visual representations to boost performance on downstream tasks, analogous to what has been shown in NLP with masked language modeling. The authors provide evidence for this hypothesis through gains on few-shot image classification.


## What is the main contribution of this paper?

This paper introduces a self-supervised pretraining technique called GROVER (Generalized Reconstruction Objective for Visual Embedding Representations) that generalizes the concept of masked language modeling from BERT to continuous image data. The key contributions are:- Proposes a novel pretraining method that masks out random patches in an image and tries to predict the correct patch from "distractor" patches sampled from the same image. This converts the reconstruction problem into a classification task. - Uses a convolutional feature extractor followed by a Transformer attention module to encode unmasked patches. The decoder then takes this representation and predicts the masked patches based on their positions.- Achieves significant gains in accuracy when finetuning ResNet-50 on downstream tasks with limited labeled data, especially when there is at least 10x more unlabeled data. For example, on ImageNet with 5% labeled data, pretrained ResNet-50 improves top-1 accuracy by 11.1% over baseline.- Demonstrates that pretraining stabilizes ResNet-50 training, reducing variance across runs. Also shows promise for a hybrid convolution-attention architecture.- Provides comprehensive experiments validating the benefits of unsupervised pretraining on image classification across multiple datasets (CIFAR-10, ImageNet 32x32, ImageNet 224x224) and amounts of labeled data.So in summary, the key innovation is a new masked reconstruction pretext task for images that translates well to improved performance on downstream tasks through finetuning. The results also highlight the potential of unsupervised pretraining to improve data efficiency and stability for CNNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a self-supervised pretraining technique called GROVER that generalizes masked language modeling from BERT to images by masking out image patches and learning to classify the correct patch among distractors.


## How does this paper compare to other research in the same field?

Here are some key points about how this paper compares to other related work in unsupervised representation learning for images:- The proposed Grover method generalizes masked language modeling approaches like BERT to the image domain. This is a novel approach in the field of unsupervised image representation learning. - Most prior work has focused on pretext tasks like predicting image rotations or solving jigsaw puzzles. Grover's masked patch prediction task is different and shows strong performance.- On ImageNet, Grover achieves significantly higher accuracy than prior methods like context autoencoders and contrastive predictive coding. The authors report over 11% better accuracy on a small labeled ImageNet subset.- Concurrent work has shown benefits of pretraining on small labeled sets using related contrastive losses. But those use much larger ResNet models. Grover shows strong gains with ResNet-50.- For semi-supervised learning, consistency training methods currently achieve better accuracy than Grover. But the authors suggest combining Grover pretraining with consistency training could further improve results.- An analysis shows Grover benefits most when there is at least 10x more unlabeled data vs labeled data. Gains diminish as labeled set size increases.- The hybrid convolutional and attention architecture also looks promising for representation learning compared to pure convolutional networks.Overall, Grover demonstrates a new approach to self-supervised representation learning that achieves state-of-the-art ImageNet accuracy and shows particular promise when labeled data is limited. The analysis provides insights into how pretraining benefits change with unlabeled vs labeled data amounts.
