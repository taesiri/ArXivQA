# [Selfie: Self-supervised Pretraining for Image Embedding](https://arxiv.org/abs/1906.02940)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that self-supervised pretraining techniques like masked language modeling can be generalized to continuous data like images to learn useful representations, especially when labeled data is scarce. Specifically, the authors propose a pretraining technique called GROVER that masks out patches of an image and tries to select the correct patch from "distractor" patches sampled from the same image. This allows the model to learn about the context and content of images in an unsupervised way. The main research question is whether this technique can improve performance on downstream supervised image classification tasks compared to training on just the labeled data, particularly when the amount of labeled data is small. The results across CIFAR-10, ImageNet 32x32, and ImageNet 224x224 show consistent improvements from the GROVER pretraining, especially when only 5-20% of the labeled data is used.So in summary, the central hypothesis is that masking-based pretraining like GROVER can learn useful visual representations to boost performance on downstream tasks, analogous to what has been shown in NLP with masked language modeling. The authors provide evidence for this hypothesis through gains on few-shot image classification.


## What is the main contribution of this paper?

This paper introduces a self-supervised pretraining technique called GROVER (Generalized Reconstruction Objective for Visual Embedding Representations) that generalizes the concept of masked language modeling from BERT to continuous image data. The key contributions are:- Proposes a novel pretraining method that masks out random patches in an image and tries to predict the correct patch from "distractor" patches sampled from the same image. This converts the reconstruction problem into a classification task. - Uses a convolutional feature extractor followed by a Transformer attention module to encode unmasked patches. The decoder then takes this representation and predicts the masked patches based on their positions.- Achieves significant gains in accuracy when finetuning ResNet-50 on downstream tasks with limited labeled data, especially when there is at least 10x more unlabeled data. For example, on ImageNet with 5% labeled data, pretrained ResNet-50 improves top-1 accuracy by 11.1% over baseline.- Demonstrates that pretraining stabilizes ResNet-50 training, reducing variance across runs. Also shows promise for a hybrid convolution-attention architecture.- Provides comprehensive experiments validating the benefits of unsupervised pretraining on image classification across multiple datasets (CIFAR-10, ImageNet 32x32, ImageNet 224x224) and amounts of labeled data.So in summary, the key innovation is a new masked reconstruction pretext task for images that translates well to improved performance on downstream tasks through finetuning. The results also highlight the potential of unsupervised pretraining to improve data efficiency and stability for CNNs.
