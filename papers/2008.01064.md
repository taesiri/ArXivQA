# [Predicting What You Already Know Helps: Provable Self-Supervised   Learning](https://arxiv.org/abs/2008.01064)

## What is the central research question or hypothesis that this paper addresses?

This paper provides theoretical analysis of self-supervised learning (SSL) for learning useful representations from unlabeled data that can help on downstream supervised tasks. The central question it addresses is:Under what assumptions on the joint data distribution does solving a pretext self-supervised task like image reconstruction lead to learning representations that are useful for downstream tasks? Specifically, it studies the setting where the input data X consists of two "views" X1 and X2 (e.g. two augmented images of the same underlying example), and the pretext task is to reconstruct X2 from X1. The key hypothesis it makes is that if X1 and X2 are approximately conditionally independent given some underlying label Y, then solving this pretext task will learn a representation of X1 that captures information about Y. This leads to good performance on downstream tasks that depend on Y.The main contributions are:- Formalizing the notion of approximate conditional independence (ACI) and relating it to the cross-covariance operator.- Showing that under ACI, the optimal representation function to reconstruct X2 from X1 is able to predict Y from X1 with low approximation error.- Deriving generalization bounds on downstream tasks that depend on how well the pretext task is solved, the amount of downstream data, the degree of ACI, and more. - Extending the analysis to other self-supervised objectives like nonlinear CCA and contrastive learning.- Empirical verification of the theory on both simulated and real datasets.In summary, it provides a theoretical framework based on approximate conditional independence to reason about when and why common SSL techniques can learn useful representations for downstream tasks. The key insight is that reconstructing parts of the input can extract useful information if they are conditionally independent given semantic labels.
