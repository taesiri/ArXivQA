# [Predicting What You Already Know Helps: Provable Self-Supervised   Learning](https://arxiv.org/abs/2008.01064)

## What is the central research question or hypothesis that this paper addresses?

This paper provides theoretical analysis of self-supervised learning (SSL) for learning useful representations from unlabeled data that can help on downstream supervised tasks. The central question it addresses is:Under what assumptions on the joint data distribution does solving a pretext self-supervised task like image reconstruction lead to learning representations that are useful for downstream tasks? Specifically, it studies the setting where the input data X consists of two "views" X1 and X2 (e.g. two augmented images of the same underlying example), and the pretext task is to reconstruct X2 from X1. The key hypothesis it makes is that if X1 and X2 are approximately conditionally independent given some underlying label Y, then solving this pretext task will learn a representation of X1 that captures information about Y. This leads to good performance on downstream tasks that depend on Y.The main contributions are:- Formalizing the notion of approximate conditional independence (ACI) and relating it to the cross-covariance operator.- Showing that under ACI, the optimal representation function to reconstruct X2 from X1 is able to predict Y from X1 with low approximation error.- Deriving generalization bounds on downstream tasks that depend on how well the pretext task is solved, the amount of downstream data, the degree of ACI, and more. - Extending the analysis to other self-supervised objectives like nonlinear CCA and contrastive learning.- Empirical verification of the theory on both simulated and real datasets.In summary, it provides a theoretical framework based on approximate conditional independence to reason about when and why common SSL techniques can learn useful representations for downstream tasks. The key insight is that reconstructing parts of the input can extract useful information if they are conditionally independent given semantic labels.


## What is the main contribution of this paper?

This paper presents theoretical results on the effectiveness of self-supervised learning (SSL) for improving performance on downstream tasks. The key contributions are:- It formally analyzes reconstruction-based SSL under an approximate conditional independence (ACI) assumption between the pretext task data and downstream task data distributions. - It shows that under ACI, the learned representations can help reduce the sample complexity of supervised training on the downstream task compared to training directly on the raw input data.- It provides generalization bounds on the downstream task excess risk that depend on the degree of ACI as measured by a conditional covariance operator norm. The bounds also account for approximation error and finite sample effects.- It connects the SSL objective to problems like non-linear CCA and shows similar theoretical guarantees apply in those settings.- It instantiates the framework for topic modeling tasks and shows only O(k) samples are needed for downstream prediction when k is the number of topics.- It compares the ACI assumption to multi-view redundancy assumptions made in prior contrastive learning theory work and shows guarantees can still be obtained.- It supports the theoretical findings with experiments on simulated and real data showing learning representations helps over just using raw input data, especially when ACI is enforced.In summary, this paper provides one of the first theoretical analyses of reconstruction-based SSL methods under generic conditions like ACI. The analysis gives insight into when and why SSL can help improve downstream performance in bothgenerative and discriminative scenarios.
