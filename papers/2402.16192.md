# [Defending Large Language Models against Jailbreak Attacks via Semantic   Smoothing](https://arxiv.org/abs/2402.16192)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Aligned large language models (LLMs) can still be manipulated by "jailbreaking" attacks to generate harmful content. These attacks bypass LLM safeguards through carefully crafted inputs.
- Existing defenses against jailbreaking attacks have limitations - they rely on heuristics, hurt nominal performance of LLMs, or are susceptible to adaptive attacks. 

Proposed Solution:  
- The paper introduces a new defense called SemanticSmooth that smooths an input prompt by transforming it semantically in multiple ways and aggregating the LLM's responses. 
- It uses 7 semantics-preserving transformations like paraphrasing, translation, summarization etc. that maintain the core meaning.
- An input-dependent policy network adaptively selects suitable transformations for each input prompt.

Key Contributions:
- SemanticSmooth achieves state-of-the-art robustness against prominent jailbreaking attacks like GCG, PAIR and AutoDAN across multiple LLM models.
- It maintains strong nominal performance on instruction-following tasks, offering the best trade-off compared to other defenses.
- Analysis shows the policy network intuitively selects more meaning-altering transformations for attack prompts.  
- Transformations also help explain nonsensical strings generated by GCG attack.

In summary, the paper presents SemanticSmooth as an effective general defense against current jailbreaking attacks on LLMs, with favorable robustness-performance trade-offs.
