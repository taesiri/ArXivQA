# [Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image   Captioning](https://arxiv.org/abs/2312.08865)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes SynTIC, a novel text-only image captioning method that leverages synthetic image-text pairs to improve cross-modal alignment and unify training and inference. A text-to-image model generates synthetic images corresponding to training texts. The CLIP features of these synthetic images are optimized via a contrastive loss to resemble features of real images. Moreover, the optimized synthetic features are projected into the CLIP text embedding space by exploiting textual semantics, enhancing alignment and complementing image details. Additionally, object tags detected in the synthetic images provide auxiliary signals to the caption decoder. For inference, real-world images are encoded and projected in the same manner for decoding. Experiments show SynTIC outperforms state-of-the-art methods on in-domain, cross-domain, and zero-shot benchmark datasets. The use of synthetic pairs bridges modality gaps, enriches feature semantics, and unifies training/inference to achieve superior text-only image captioning.
