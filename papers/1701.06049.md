# [Interactive Learning from Policy-Dependent Human Feedback](https://arxiv.org/abs/1701.06049)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can reinforcement learning agents learn effectively from human feedback that depends on the agent's current policy, rather than just depending on the quality of the agent's action?

The key hypotheses appear to be:

1) Human feedback depends not just on the quality of an agent's action, but also on how that action compares to the agent's overall policy. The paper presents an empirical study supporting this hypothesis.

2) Existing algorithms that assume policy-independent feedback may fail or learn suboptimally when trained with more natural, policy-dependent feedback from humans. The paper illustrates this through comparison studies. 

3) Modeling and learning from policy-dependent feedback enables more effective human training strategies like diminishing returns, differential feedback, and policy shaping. The proposed COACH algorithm is based on this hypothesis.

4) Algorithms based on modeling policy-dependent feedback like COACH can successfully learn complex behaviors interactively from humans, even in challenging robotics domains. The robotics case study provides support for this hypothesis.

In summary, the main research question is how agents can learn from policy-dependent human feedback, with hypotheses concerning how this type of feedback differs from prior assumptions, the limitations of existing methods, the benefits of properly modeling policy-dependent feedback, and the feasibility of methods like COACH that do so. The empirical studies, comparisons, and robotics evaluations aim to support these hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting an algorithm called COACH (Convergent Actor-Critic by Humans) for interactive reinforcement learning from human feedback. The key ideas are:

- Empirically showing that human feedback depends on the agent's current policy, not just the quality of the action. This is in contrast to assumptions made by prior algorithms.

- Proposing to model human feedback as related to the advantage function, which captures properties like rewarding improvement and diminishing returns. 

- Deriving an actor-critic style update rule based on the policy gradient that provably converges when the human gives advantage-based feedback.

- Introducing the COACH algorithm based on this update rule, with additional mechanisms like eligibility traces and delayed feedback to enable real-time learning.

- Demonstrating COACH learns effectively on a simple gridworld with different simulated feedback strategies.

- Showing COACH can learn complex behaviors on a physical Turtlebot robot under 33ms control cycles based on noisy image features. It succeeds on behaviors where a prior algorithm (TAMER) failed.

So in summary, the main contribution is introducing and providing empirical validation for the COACH algorithm for interactive learning based on a new model of human feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents empirical evidence that human trainers give feedback to reinforcement learning agents in a way that depends on the agent's current policy, and proposes a new algorithm called COACH that converges when trained with this type of policy-dependent feedback, enabling more effective interactive training of agents compared to prior methods.


## How does this paper compare to other research in the same field?

 The paper presents some interesting empirical findings related to how human trainers give feedback to reinforcement learning agents, and proposes a novel algorithm called COACH for learning from this type of feedback. Here are some key ways this paper relates to other work in human-centered reinforcement learning:

- It provides evidence that human feedback is often policy-dependent, contrasting with assumptions made by prior algorithms like TAMER, SABL, and Policy Shaping. This finding builds on observations in prior work that trainers taper feedback over time.

- It proposes modeling human feedback as related to the advantage function, which captures properties like diminishing returns and rewarding improvement. This connects to ideas from behavior analysis that successful animal training utilizes these strategies. 

- It introduces the COACH algorithm, which leverages the relationship between the advantage function and the actor-critic TD error to update policies based on human feedback interpreted this way. This is a novel learning algorithm for human training.

- It demonstrates COACH can work on a real robot, highlighting issues like fast decision cycles and noisy perception that are important considerations beyond simple gridworlds. The robot learned multiple behaviors in under 2 minutes each.

- It provides an empirical comparison between COACH and TAMER, suggesting COACH may be more robust to things like sparse feedback and support more advanced training strategies like composition and luring.

Overall, the paper makes both empirical and algorithmic contributions to understanding and improving how reinforcement learning agents can learn interactively from human trainers. The proposed COACH algorithm and findings related to policy-dependent feedback seem like worthwhile directions for further study in human-centered RL research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions the authors suggest are:

- Combining COACH with learning from demonstration and environmental rewards. Since COACH is built on the actor-critic paradigm, the authors note it should be straightforward to incorporate learning from both demonstrations and environmental rewards. This would allow the agent to be trained in multiple ways.

- Investigating how people model the agent's current policy when giving feedback, and how this model differs from the agent's actual policy. The authors argue that since people give policy-dependent feedback, understanding in more detail how they form a model of the agent's policy could lead to further performance gains. 

- Extending the approach to learn more complex policies and representations. The paper demonstrated COACH on a physical robot, but with fairly simple policies and hand-crafted features. Scaling up to learn more complex neural network policies directly from raw sensory inputs like images is noted as an important direction.

- Exploring different training strategies enabled by the COACH update rule. The authors suggest the COACH framework may enable investigating other useful training strategies from animal training or human education based on its policy-dependent update rule.

- Incorporating human gestures and other non-numeric signals into the learning process. The current work focuses on numeric feedback signals. Expanding the types of human feedback the agent can interpret is noted as another interesting direction.

In summary, the main future directions highlighted are combining COACH with other learning approaches, scaling it up to more complex domains, exploring new training strategies it enables, and expanding the types of human feedback that can be incorporated. The key goal is advancing human-in-the-loop reinforcement learning for real-world robotics problems.


## Summarize the paper in one paragraph.

 The paper presents empirical results showing that human trainers give feedback to reinforcement learning agents that is dependent on the agent's current policy, rather than being independent as assumed by many existing algorithms. Based on this, the authors propose COACH, a convergent actor-critic algorithm for human-agent interaction that models policy-dependent feedback using the advantage function. COACH is shown to learn effectively on a simple grid world using simulated policy-dependent feedback. It is also demonstrated on a physical TurtleBot robot learning multiple behaviors from a human trainer using only image features, including successfully leveraging compositional training strategies. Compared to prior methods like TAMER, COACH better handles the natural feedback people give, enabling more sophisticated interactive training.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper investigates the problem of interactively learning behaviors from human trainers providing positive and negative feedback. The authors present empirical results showing that whether a human trainer gives positive or negative feedback for a decision is influenced by the learner's current policy, contradicting the common assumption that feedback depends only on the action and is independent of the policy. Based on this insight that feedback is policy-dependent, the authors introduce an algorithm called Convergent Actor-Critic by Humans (COACH) that is able to learn from policy-dependent feedback and converges to a local optimum. COACH is based on modeling human feedback as the advantage function from actor-critic algorithms. The authors compare COACH to other approaches in a simple domain with simulated feedback. They then validate that COACH can scale to complex problems by training five different behaviors on a physical TurtleBot robot operating from camera images, demonstrating COACH's ability to learn from advanced training strategies. The results show COACH succeeding where a comparison algorithm failed.

In summary, the key contributions are: 1) Empirical evidence that human feedback is policy-dependent, not independent as commonly assumed 2) The COACH algorithm for learning from policy-dependent feedback that converges to a local optimum 3) Demonstration of COACH learning complex behaviors on a physical robot, benefiting from policy-dependent training strategies. The authors argue policy-dependent feedback enables useful training strategies and that modeling it is important for human-centered reinforcement learning.


## Summarize the main method used in the paper in one paragraph.

 The paper presents Convergent Actor-Critic by Humans (COACH), a new algorithm for interactive reinforcement learning from human trainers. COACH is based on the insight that the advantage function used in actor-critic methods provides a good model of human feedback. The advantage function captures important properties of human feedback like diminishing returns, rewarding improvement, and giving 0-valued feedback semantic meaning. 

The key idea is that the temporal difference error used to update the actor in actor-critic methods is an unbiased estimate of the advantage function. Therefore, the actor can be directly updated according to human feedback interpreted as an approximation of the advantage function. This results in an interactive learning algorithm that converges to a local optimum when trained with policy-dependent human feedback.

The authors provide empirical evidence that human feedback is policy-dependent, compare COACH to other algorithms in a simple domain, and demonstrate its effectiveness in training behaviors on a physical Turtlebot robot. The results show COACH can scale to complex tasks and benefit from advanced human training techniques in ways prior algorithms could not.


## What problem or question is the paper addressing?

 The paper is addressing the problem of learning complex behaviors for robots and agents from human trainers using simple positive and negative feedback signals. The key questions it investigates are:

1. Is human feedback dependent on the learner's current policy or independent? 

2. How can an algorithm learn effectively from policy-dependent human feedback?

3. Can such an algorithm scale to training complex robot behaviors in real-time from a human trainer?

The paper argues that human feedback is naturally policy-dependent, meaning the same action may receive different feedback depending on the learner's overall behavior. It shows empirically that people give positive feedback for improving behavior compared to the learner's existing policy. Based on this insight, the paper introduces an algorithm called COACH that can learn effectively from policy-dependent feedback. COACH is demonstrated to learn a variety of robot behaviors interactively in real-time from only a human trainer's simple feedback.

In summary, the key contribution is an interactive learning algorithm that can leverage natural policy-dependent human feedback to train complex robot behaviors, validated on a physical robot platform. The empirical demonstration that human feedback depends on learner policy is also an important insight for human-robot interaction.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms are:

- Human-centered reinforcement learning (HCRL) 
- Policy-dependent feedback
- Advantage function 
- Convergent Actor-Critic by Humans (COACH) algorithm
- Interactive learning 
- Robot training
- Diminishing returns
- Differential feedback
- Policy shaping

The paper presents empirical evidence that human trainers give feedback to reinforcement learning agents that depends on the agent's current policy, rather than just the quality of the action. This insight motivated the development of the COACH algorithm that can learn effectively from policy-dependent feedback. COACH is evaluated in simulation and on a robotics platform for training various behaviors. Key concepts include using the advantage function to model human feedback, eligibility traces to handle delayed feedback, and mechanisms to enable variable magnitude feedback. Overall, the paper makes contributions in interactive learning for robots/agents by modeling and leveraging properties of how humans naturally provide training feedback.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main problem addressed in the paper?

2. What key assumption do existing human-centered reinforcement learning (HCRL) algorithms make about human feedback? 

3. What empirical evidence does the paper present to show that this assumption is incorrect?

4. What is the main insight behind the proposed COACH algorithm?

5. How does COACH update the policy based on human feedback? What key function does it leverage?

6. How is real-time COACH adapted to work with variable magnitude feedback and handle issues like feedback delay?

7. What simple gridworld experiments were conducted to analyze COACH? What key results emerged?

8. How was COACH evaluated on a real robotics platform (TurtleBot)? What behaviors were trained? 

9. How did COACH compare to other algorithms like TAMER in the robotics experiments? What were the key differences in performance?

10. What are some promising future research directions identified based on the COACH algorithm?
