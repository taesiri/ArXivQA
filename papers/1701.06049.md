# Interactive Learning from Policy-Dependent Human Feedback

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can reinforcement learning agents learn effectively from human feedback that depends on the agent's current policy, rather than just depending on the quality of the agent's action?The key hypotheses appear to be:1) Human feedback depends not just on the quality of an agent's action, but also on how that action compares to the agent's overall policy. The paper presents an empirical study supporting this hypothesis.2) Existing algorithms that assume policy-independent feedback may fail or learn suboptimally when trained with more natural, policy-dependent feedback from humans. The paper illustrates this through comparison studies. 3) Modeling and learning from policy-dependent feedback enables more effective human training strategies like diminishing returns, differential feedback, and policy shaping. The proposed COACH algorithm is based on this hypothesis.4) Algorithms based on modeling policy-dependent feedback like COACH can successfully learn complex behaviors interactively from humans, even in challenging robotics domains. The robotics case study provides support for this hypothesis.In summary, the main research question is how agents can learn from policy-dependent human feedback, with hypotheses concerning how this type of feedback differs from prior assumptions, the limitations of existing methods, the benefits of properly modeling policy-dependent feedback, and the feasibility of methods like COACH that do so. The empirical studies, comparisons, and robotics evaluations aim to support these hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper is presenting an algorithm called COACH (Convergent Actor-Critic by Humans) for interactive reinforcement learning from human feedback. The key ideas are:- Empirically showing that human feedback depends on the agent's current policy, not just the quality of the action. This is in contrast to assumptions made by prior algorithms.- Proposing to model human feedback as related to the advantage function, which captures properties like rewarding improvement and diminishing returns. - Deriving an actor-critic style update rule based on the policy gradient that provably converges when the human gives advantage-based feedback.- Introducing the COACH algorithm based on this update rule, with additional mechanisms like eligibility traces and delayed feedback to enable real-time learning.- Demonstrating COACH learns effectively on a simple gridworld with different simulated feedback strategies.- Showing COACH can learn complex behaviors on a physical Turtlebot robot under 33ms control cycles based on noisy image features. It succeeds on behaviors where a prior algorithm (TAMER) failed.So in summary, the main contribution is introducing and providing empirical validation for the COACH algorithm for interactive learning based on a new model of human feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents empirical evidence that human trainers give feedback to reinforcement learning agents in a way that depends on the agent's current policy, and proposes a new algorithm called COACH that converges when trained with this type of policy-dependent feedback, enabling more effective interactive training of agents compared to prior methods.


## How does this paper compare to other research in the same field?

The paper presents some interesting empirical findings related to how human trainers give feedback to reinforcement learning agents, and proposes a novel algorithm called COACH for learning from this type of feedback. Here are some key ways this paper relates to other work in human-centered reinforcement learning:- It provides evidence that human feedback is often policy-dependent, contrasting with assumptions made by prior algorithms like TAMER, SABL, and Policy Shaping. This finding builds on observations in prior work that trainers taper feedback over time.- It proposes modeling human feedback as related to the advantage function, which captures properties like diminishing returns and rewarding improvement. This connects to ideas from behavior analysis that successful animal training utilizes these strategies. - It introduces the COACH algorithm, which leverages the relationship between the advantage function and the actor-critic TD error to update policies based on human feedback interpreted this way. This is a novel learning algorithm for human training.- It demonstrates COACH can work on a real robot, highlighting issues like fast decision cycles and noisy perception that are important considerations beyond simple gridworlds. The robot learned multiple behaviors in under 2 minutes each.- It provides an empirical comparison between COACH and TAMER, suggesting COACH may be more robust to things like sparse feedback and support more advanced training strategies like composition and luring.Overall, the paper makes both empirical and algorithmic contributions to understanding and improving how reinforcement learning agents can learn interactively from human trainers. The proposed COACH algorithm and findings related to policy-dependent feedback seem like worthwhile directions for further study in human-centered RL research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest are:- Combining COACH with learning from demonstration and environmental rewards. Since COACH is built on the actor-critic paradigm, the authors note it should be straightforward to incorporate learning from both demonstrations and environmental rewards. This would allow the agent to be trained in multiple ways.- Investigating how people model the agent's current policy when giving feedback, and how this model differs from the agent's actual policy. The authors argue that since people give policy-dependent feedback, understanding in more detail how they form a model of the agent's policy could lead to further performance gains. - Extending the approach to learn more complex policies and representations. The paper demonstrated COACH on a physical robot, but with fairly simple policies and hand-crafted features. Scaling up to learn more complex neural network policies directly from raw sensory inputs like images is noted as an important direction.- Exploring different training strategies enabled by the COACH update rule. The authors suggest the COACH framework may enable investigating other useful training strategies from animal training or human education based on its policy-dependent update rule.- Incorporating human gestures and other non-numeric signals into the learning process. The current work focuses on numeric feedback signals. Expanding the types of human feedback the agent can interpret is noted as another interesting direction.In summary, the main future directions highlighted are combining COACH with other learning approaches, scaling it up to more complex domains, exploring new training strategies it enables, and expanding the types of human feedback that can be incorporated. The key goal is advancing human-in-the-loop reinforcement learning for real-world robotics problems.
