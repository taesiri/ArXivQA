# [Chain-of-Instructions: Compositional Instruction Tuning on Large   Language Models](https://arxiv.org/abs/2402.11532)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing large language models (LLMs) excel at single instruction tasks but struggle to follow complex, multi-step instructions. 
- There is a lack of compositional instruction datasets to train models on sequential, chained instructions.

Proposed Solution: 
- Introduce the concept of "Chain-of-Instructions" (CoI) - a series of instructions where the output of one step becomes the input to the next.
- Automatically construct a CoI dataset by:
   1) Summarizing lengthy single instructions
   2) Checking if instruction outputs can feed into inputs of subsequent instructions 
   3) Generate full CoI examples with chained instructions
- Fine-tune LLMs (Alpaca, Mistral) on CoI datasets

Main Contributions:
- Formulation of Chain-of-Instructions (CoI) tasks with formal definition
- Pipeline to automatically construct CoI dataset with minimal supervision 
- New CoI dataset created from existing NLP instructions
- Methodology for CoI tuning - fine-tuning models on CoI data
- Experiments showing CoI-tuned models outperform baselines on:
   - Compositional instruction test sets
   - Generalization - unseen single instructions
   - Downstream task - multilingual summarization

In summary, the paper introduces the concept and task formulation of Chain-of-Instructions (CoI) to assess and improve LLM's capabilities on complex, multi-step instructions. It provides an automatic pipeline to construct CoI datasets and presents experiments demonstrating performance gains from CoI-tuning on compositional, unseen, and downstream tasks.
