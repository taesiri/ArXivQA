# [Swin-Free: Achieving Better Cross-Window Attention and Efficiency with   Size-varying Window](https://arxiv.org/abs/2306.13776)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the efficiency and accuracy of Swin Transformer models for computer vision tasks. Specifically, the authors aim to mitigate the high runtime cost and data movement overhead caused by the shifted window mechanism in Swin Transformers. Their proposed approach is to use size-varying windows across stages instead of shifted windows to achieve cross-window connections while avoiding the extra data shuffling operations. The key hypothesis is that this design change will lead to faster runtime with better accuracy compared to Swin Transformer models.The paper introduces Swin-Free, a modified Swin Transformer architecture that removes shifted windows and instead varies the size of local attention windows across stages. The authors hypothesize and demonstrate that:- Avoiding shifted windows reduces memory copy overhead, thereby improving runtime efficiency. - Varying window sizes provides better cross-window modeling than shifted windows, leading to accuracy improvements.- Larger window sizes combined with fewer windows per stage improve GPU utilization for faster matrix multiplications. - Additional tweaks like BatchNorm/ReLU layers and reduced model depth can further enhance latency without sacrificing accuracy.In summary, the central hypothesis is that Swin-Free will achieve superior efficiency and accuracy compared to Swin Transformers through its use of size-varying windows and other optimizations. The experiments validate these claims and analyze the impact of different design choices.
