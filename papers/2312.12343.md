# [Avoiding Data Contamination in Language Model Evaluation: Dynamic Test   Construction with Latest Materials](https://arxiv.org/abs/2312.12343)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Data contamination in evaluations is becoming more common as language models are pretrained on massive corpora. This leads to inflated metrics that do not accurately reflect model capabilities.  
- Existing benchmarks use online data and have high overlap with training corpora. Decontaminating them is extremely difficult and not possible for closed models.

Proposed Solution - LatestEval:
- Automatically create uncontaminated reading comprehension benchmarks using the most recent texts from the web. 
- Collect latest texts from arXiv, BBC and GitHub as passages.
- Extract key info like terminology definitions and summaries as answers. 
- Generate questions targeting the extracted info using templates and GPT-4.
- Remove explicit answers from passages to require reasoning rather than copy-pasting.

Main Contributions:
- First analysis of real Human-AI conversations to determine benchmark scope 
- Novel pipeline to leverage latest materials to avoid contamination
- New perplexity-based method to quantify contamination
- Experiments show models exhibit negligible memorization on LatestEval versus high memorization on existing benchmarks
- Enables fine-grained assessment on different comprehension abilities

In summary, LatestEval provides a dynamic benchmark that mitigates data contamination by only using recent texts, enabling more accurate evaluation of language model capabilities on reading comprehension.
