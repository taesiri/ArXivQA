# [Policy Smoothing for Provably Robust Reinforcement Learning](https://arxiv.org/abs/2106.11420v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question it addresses is:

How can we make reinforcement learning policies provably robust against adversarial attacks?

In particular, it focuses on using randomized smoothing techniques to provide certified robustness guarantees for RL policies against norm-bounded adversarial perturbations of the policy's inputs. 

The key challenges it aims to address are:

- Adapting existing randomized smoothing techniques (like those developed for static tasks like image classification) to the more complex dynamic and adaptive setting of reinforcement learning. 

- Providing guarantees on the total episodic reward rather than just robustness of the policy at each intermediate time step.

- Handling the adaptive nature of RL adversaries that can modify their attack strategy over time based on observations of the victim agent's behavior.

To summarize, the central hypothesis is that adding random noise to the policy's inputs at each time step can make it certifiably robust to adversarial perturbations of bounded norm, and they develop new theoretical results on randomized smoothing to formally prove this in the dynamic RL setting.
