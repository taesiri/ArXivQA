# [Detecting Concrete Visual Tokens for Multimodal Machine Translation](https://arxiv.org/abs/2403.03075)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multimodal machine translation (MMT) aims to translate text from one language to another using image context to help resolve ambiguities. However, many sentences may not have useful visual context.
- Prior work has tried masking random tokens or visually depictable entities, but has had mixed results. There is a need for better methods to select relevant tokens to mask to force reliance on visual context.

Proposed Solution:
- Introduce new methods to detect concrete (visually and contextually relevant) tokens using NLP and object detection techniques:
  - NLTK technique: Parse nouns and noun phrases and determine if they map to concrete or abstract concepts
  - MDETR technique: Use object detection model on whole sentence to detect tokens
  - Joint technique: Verify NLTK detections using MDETR on each token
- Deterministic token selection techniques: longest tokens, shortest tokens, all tokens
- Create synthetic MMT datasets by masking selected tokens in aligned image-sentence pairs
- Train MMT model (GRAM architecture) on these datasets 

Key Contributions:
- Concrete token detection techniques that combine NLP and object detection
- Joint verification to link linguistic and visual concreteness  
- Deterministic token selection techniques 
- Method to create synthetic datasets with masked concrete tokens
- Improved performance over baseline:
  - Up to 0.67 CoMMuTE score (vs 0.61 baseline)
  - Up to 46.2 BLEU score (vs 45.0 baseline)
  - Increased visual context usage during translation

In summary, the key innovation is the joint NLP/detection technique to select relevant tokens to mask based on both linguistic and visual concreteness. Masking these tokens creates better reliance on visual context and improves multimodal translation performance.
