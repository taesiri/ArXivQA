# [MetaViewer: Towards A Unified Multi-View Representation](https://arxiv.org/abs/2303.06329)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to learn an optimal unified multi-view representation from heterogeneous data sources. Specifically, the paper proposes a novel framework called MetaViewer that learns to fuse multi-view features and filter out view-private redundant information in a meta-learning paradigm.

The key hypotheses are:

1. Learning the unified representation in a "uniform-to-specific" manner by observing the reconstruction process from the unified representation back to the original views is more effective than the common "specific-to-uniform" approaches.

2. Modeling the view-specific reconstruction in the inner loop of meta-learning allows identifying and separating view-private information from the shared representation.

3. Meta-learning an optimal fusion function over multiple views provides a more data-driven way to aggregate features compared to manual fusion rules.

4. The resulting meta-learned unified representation will contain richer shared information and less redundant view-specific noise, thus benefiting downstream tasks like clustering and classification.

In summary, the core research question is how to leverage meta-learning to obtain an optimal shared representation from multiple views by learning to fuse features and filter noise in a bi-level optimization framework. The key hypotheses focus on the advantages of a uniform-to-specific modeling approach compared to prior work.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a novel meta-learning based framework for multi-view representation learning, called MetaViewer. 

2. Instead of the conventional specific-to-uniform pipeline, MetaViewer learns the unified representation in a uniform-to-specific manner.

3. It uses a bi-level optimization process to meta-learn an optimal fusion scheme for combining multi-view information. The outer level updates the meta-learner and inner level trains view-specific base learners.

4. The inner level reconstruction modeling explicitly captures view-private information and helps filter it from the unified representation. 

5. Extensive experiments validate the effectiveness of MetaViewer, showing it outperforms existing methods on clustering and classification tasks using benchmark multi-view datasets.

In summary, the key novelty is the uniform-to-specific meta-learning approach to learn an optimal fusion scheme while filtering view-private information for multi-view representation. This is in contrast to most prior works that follow a specific-to-uniform pipeline with predefined fusion schemes.
