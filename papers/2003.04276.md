# [How to Train Your Super-Net: An Analysis of Training Heuristics in   Weight-Sharing NAS](https://arxiv.org/abs/2003.04276)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, this paper does not appear to have a single central research question or hypothesis. Instead, it seems to provide a systematic evaluation of the heuristics and hyperparameters commonly used in weight-sharing neural architecture search (NAS) algorithms. The key points I gathered are:- The authors benchmark and analyze the impact of various choices made in designing and training the shared-weight backbone network (super-net) used in weight-sharing NAS methods. - They aim to understand how factors like training heuristics and hyperparameters influence the correlation between super-net and stand-alone performance.- Their goal is to provide a controlled comparison of these techniques across benchmark datasets to reveal which factors strongly impact performance vs. which have marginal effects.- They introduce a modified Kendall-Tau metric called "sparse Kendall-Tau" to better evaluate super-net quality compared to commonly-used super-net accuracy.- Their analysis identifies crucial factors for super-net design, allows them to improve state-of-the-art on one benchmark with a simple random search approach, and provides a strong baseline for future NAS research.So in summary, this paper provides a comprehensive benchmarking study rather than testing a specific hypothesis. The key research question seems to be understanding how various heuristics and factors in super-net design affect the performance of weight-sharing NAS methods.


## What is the main contribution of this paper?

Based on my reading of the abstract, the main contribution of this paper is a systematic evaluation of the heuristics and hyperparameters commonly used by weight-sharing NAS algorithms. The authors benchmark different choices for the weight-sharing training protocol P_ws and mapping function f_ws across three benchmark datasets. Their analysis reveals which factors have a strong vs marginal influence on performance, uncovers issues with some commonly-used heuristics, and shows that different search spaces have varying amenability to weight sharing. The key findings are:- Certain hyperparameters like learning rate have a strong influence on final performance, reducing discrepancies between algorithms.- Some commonly-used heuristics like weight-sharing batchnorm negatively impact performance. - Metrics like super-net accuracy have low correlation with final stand-alone performance. The authors propose a modified Kendall-Tau metric that better evaluates super-net quality.- Some believed-important factors like number of channels have little measured impact. - The analysis allows the authors to improve super-net design and achieve state-of-the-art search results with a simple random search algorithm.In summary, this is the first paper to systematically analyze super-net design choices, uncovering crucial vs non-important factors. It provides guidelines for training high-quality super-nets and sets a strong baseline for further research. The code and models are released to serve as a unified WS-NAS framework.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper provides a systematic analysis of the heuristics and hyperparameters commonly used when training the shared-weight backbone network (super-net) in weight-sharing neural architecture search, finding that some commonly-used heuristics negatively impact the correlation between super-net and stand-alone performance while certain hyperparameters and architectural choices have a strong influence.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in neural architecture search and weight sharing:- It provides a systematic evaluation of various heuristics and hyperparameters commonly used when training weight-sharing NAS models (the "super-net"). Most prior work focuses on developing new NAS algorithms but does not analyze the impact of super-net training details.- The paper benchmarks and compares results across three popular search spaces (NASBench-101, NASBench-201, DARTS-NDS). Looking across multiple spaces provides more robust and generalizable insights.- It proposes a new metric, sparse Kendall-Tau, to evaluate super-net quality by correlating sampled architectures with ground truth performance. This provides an alternative to relying only on final stand-alone accuracy.- The analysis shows commonly used techniques like path dropout and dynamic convolutions can negatively impact search results. Some factors believed important like batch size have little effect. This challenges assumptions in prior NAS research.- By carefully controlling key factors like hyperparameters, the paper achieves state-of-the-art results with a simple weight-sharing random search. This establishes a strong baseline for further NAS research. Overall, the systematic benchmarking approach and insights on super-net training are novel contributions compared to prior NAS literature focused on developing new search algorithms. The paper helps advance understanding of what really matters for effective weight-sharing NAS.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Continued analysis and benchmarking of weight-sharing NAS heuristics and hyperparameters. The authors suggest more thorough exploration and reporting of these implementation details to enable fair comparisons between NAS algorithms.- Developing better metrics for evaluating super-net quality, beyond just super-net accuracy. The authors propose their sparse Kendall-Tau metric as an improved alternative, but suggest there is room for other metrics as well.- Studying the trainability and amenability of different search spaces to weight sharing. The analysis here focused on three benchmark search spaces, but looking at more diverse spaces could provide further insights.- Reducing the computational and environmental costs of NAS methods. The authors acknowledge the high resource usage of NAS and suggest exploring ways to further increase efficiency and reduce impacts.- Incorporating the guidelines and baseline provided into future work. The authors hope their systematic analysis will encourage improved reporting of details and adoption of their training recommendations to advance the field.- Continuing to move from heuristic NAS approaches to more principled and theory-driven methods. The authors' analysis helps identify important factors, providing a foundation to build more rigorous NAS approaches.Overall, the paper emphasizes the need for more controlled, reproducible analysis in NAS research to truly understand the core factors driving performance. The authors' work provides an initial set of best practices and insights that can guide follow-up studies to further advance the foundations and efficacy of neural architecture search.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a systematic evaluation of the heuristics and hyperparameters that are commonly used when training the shared-weight backbone network (super-net) in weight-sharing neural architecture search (NAS) methods. The authors benchmark different choices for the weight-sharing training protocol and mapping function on three NAS benchmark datasets - NASBench-101, NASBench-201, and DARTS-NDS. Their analysis shows that certain training heuristics negatively impact the correlation between super-net and stand-alone performance, and reveals the significant influence of particular hyperparameters and architectural choices. The commonly-used super-net accuracy metric has low correlation with final stand-alone performance, while their proposed sparse Kendall-Tau metric shows higher correlation. Their experiments uncover the most important factors for super-net training and allow them to construct a strong baseline using weight-sharing random search that achieves state-of-the-art results. The work provides a controlled evaluation and guidelines for training high-quality super-nets.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper provides a systematic evaluation of the heuristics and hyperparameters frequently used in weight-sharing neural architecture search (NAS) algorithms. Weight-sharing NAS allows all possible architectures to share weights, making NAS more computationally tractable. However, existing methods rely on diverse heuristics for designing and training the shared-weight backbone network (super-net). The authors benchmark different choices for the weight-sharing training protocol and mapping function on three NAS benchmark datasets. The analysis reveals that certain commonly-used super-net training heuristics negatively impact the correlation between super-net and stand-alone performance. It highlights the strong influence of specific hyperparameters like batch normalization and learning rate. The results demonstrate that some search spaces are more amenable to weight sharing than others. The proposed sparse Kendall-Tau metric better correlates with final stand-alone performance versus super-net accuracy. Following the guidelines from the analysis allows the construction of a strong baseline using simple random search that is competitive with state-of-the-art NAS algorithms. Overall, the work provides the first systematic study of super-net design factors, setting a reproducible baseline for fair comparisons between weight-sharing NAS methods.
