# [Regularization by denoising: Bayesian model and Langevin-within-split   Gibbs sampling](https://arxiv.org/abs/2402.12292)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper is interested in Bayesian image inversion, where the goal is to estimate an image x from noisy/degraded measurements y. This is an ill-posed inverse problem common in image processing tasks like deblurring, inpainting, super-resolution etc.

- Most existing methods rely on hand-designed prior distributions to regularize the problem. Recently, data-driven regularization methods like Plug-and-Play (PnP) and Regularization by Denoising (RED) have gained popularity as they avoid manual design of priors. 

- However, these methods have been formulated only in an optimization framework to get point estimates of x. The paper aims to formulate RED in a probabilistic setting to enable Bayesian inference.

Main Contributions:

1) Defines a new probability distribution called the RED prior by using the RED regularization potential. Shows that this is a proper prior distribution under mild assumptions.

2) Combines the RED prior with the likelihood to define the RED posterior distribution. As this posterior is non-standard, designs a dedicated MCMC algorithm called Langevin-within-Split Gibbs Sampler (LwSGS) to sample from it.

3) The key idea in LwSGS is to leverage data augmentation to split the RED potential term from the likelihood term into two conditionals. One conditional is sampled using Langevin Monte Carlo while the other uses algorithms like CG or FFT. 

4) Provides theoretical analysis of LwSGS - shows existence of unique stationary distribution, establishes bias bounds and convergence guarantees.

5) Demonstrates the proposed Bayesian RED framework and LwSGS algorithm on tasks like image deblurring, inpainting and super-resolution. Shows it competes well with state-of-the-art optimization and sampling based methods.

In summary, the paper proposes a novel way to formulate the powerful RED regularization technique in a Bayesian setting to enable probabilistic inference. The designed LwSGS algorithm can efficiently sample from the resulting non-standard posterior distribution.
