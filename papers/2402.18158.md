# [Evaluating Quantized Large Language Models](https://arxiv.org/abs/2402.18158)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have high memory and computational costs. Post-training quantization (PTQ) is an effective technique to reduce these costs, but choosing suitable quantization methods for diverse models and tasks is challenging. 

- Prior works either focus narrowly on language modeling tasks or do not evaluate important abilities like dialog, long-context processing, and trustworthiness after quantization. There lacks a systematic understanding of quantization's effects across models, tasks, tensor types, and methods.

Methodology:
- The paper evaluates 11 LLM families (125M-180B parameters) on 5 task types: basic NLP, emergent abilities, trustworthiness, dialog, and long-context processing (16k tokens).

- Three tensor types are quantized: weights, activations, and key-value caches. Quantization bitwidths range from 2-bit to 8-bit. Advanced quantization methods like SmoothQuant are also tested.

Key Findings:
- Larger models better tolerate weight and key-value cache quantization but are more sensitive to activation quantization due to higher outlier rates.  

- Increasing model size via mixture-of-experts does not improve quantization tolerance compared to similarly sized dense models.

- Most models sustain good performance under W4, W4A8 and KV4 quantization for most NLP tasks. But mathematical reasoning and self-calibration abilities prefer W8+ bitwidths.

- Dialog models tolerate weight and key-value cache quantization well but collapse under W4A4. Long context tasks are very sensitive to key-value cache quantization.  

- Advanced quantization methods struggle to recover extremely low bitwidth cases but help for moderate quantization.

Recommendations and Future Work: 
- Employ dynamic quantization for activations and study mix-precision schemes tailored to different tensor statistics.

- Investigate quantization-aware training and operator-level optimizations like LLM-INT8 to push efficiency further. Extend evaluations to training and sparse models.
