# [Parallel Structures in Pre-training Data Yield In-Context Learning](https://arxiv.org/abs/2402.12530)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (LMs) exhibit surprising in-context learning (ICL) abilities - they can adapt to new tasks given just a few examples in the prompt without any parameter updates. However, it is unclear where this ability comes from since pre-training data is quite different from the structured in-context prompts.

- Prior work has studied synthetic data or coarse properties of pre-training data that lead to ICL. But the specific structures in real pre-training text that are important for ICL are not well understood.

Proposed Solution:
- This paper hypothesizes that "parallel structures" in pre-training text are crucial for LMs to acquire ICL abilities. Parallel structures are defined as pairs of phrases in a context window that follow a similar template/distribution.

- An algorithm is proposed to detect parallel structures by checking if training on one phrase improves prediction of the other phrase. The effect of parallel structures on ICL is measured by ablating them from pre-training data.

Key Results:
- Ablating parallel structures reduces ICL accuracy by 51% while ablating random tokens only reduces accuracy by 2%, showing parallel structures are essential for ICL. This effect persists as model size increases.

- Parallel structures are more important than n-gram repetitions and long-range dependency for ICL. Analysis shows detected parallel structures exhibit diverse linguistic patterns and span long distances.

Main Contributions:
- Identified that parallel structures in pre-training text are crucial for language models to acquire in-context learning ability.

- Proposed a method to detect parallel structures and verified their effect on ICL through ablation studies on real pre-training data at scale.

- Analysis revealed characteristics of parallel structures that explain properties of in-context learning, shedding light on this mysterious ability of large language models.
