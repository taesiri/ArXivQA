# MAT: Mask-Aware Transformer for Large Hole Image Inpainting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we develop an effective transformer-based model for large hole image inpainting that can directly process high-resolution images?The key points are:- Existing inpainting methods using attention or transformers are typically limited to low-resolution outputs due to complexity issues. This leads to coarse image structures and compromised image quality for large-scale masks. - The paper proposes a novel transformer architecture called MAT (Mask-Aware Transformer) that unifies the merits of transformers and convolutions to efficiently process high-resolution images.- The model carefully designs each component to guarantee high fidelity and diversity of recovered images for large holes, including a customized transformer block, a multi-head contextual attention module, and a style manipulation module.- Extensive experiments show MAT achieves state-of-the-art performance on multiple datasets and supports high-quality pluralistic image completion.In summary, the core research focus is developing an efficient transformer that can directly generate high-resolution results for large hole image inpainting and outperforms existing methods. The key contribution lies in the meticulous transformer design to achieve this goal.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It develops a novel inpainting transformer called MAT (Mask-Aware Transformer) that is capable of directly generating high-resolution completed images for large mask inpainting. This is the first transformer-based inpainting method that can process high-resolution images.2. It carefully designs each component of the MAT framework to enable efficient long-range modeling and high-fidelity image generation, including:- A customized transformer block without layer normalization and residual learning to make training more stable. - A multi-head contextual attention module to selectively aggregate information from valid tokens indicated by a dynamic mask.- A style manipulation module to produce diverse outputs.3. Extensive experiments show MAT achieves state-of-the-art performance on Places and CelebA-HQ datasets for large hole image inpainting. It also enables high-quality pluralistic image completion.In summary, the key contribution is developing an efficient transformer that unifies the merits of convolutions and attention to directly process high-resolution images and fill large holes with both high fidelity and diversity. The careful designs of transformer components are also vital for the final performance.
