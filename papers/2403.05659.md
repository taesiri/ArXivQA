# [Audio-Synchronized Visual Animation](https://arxiv.org/abs/2403.05659)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing visual generation methods can generate high-quality videos from text prompts, but effectively controlling object dynamics over time remains challenging. 
- Prior work on audio-to-visual generation has been limited to either semantic control without temporal synchronization, or very constrained scenarios like talking faces.
- There is a lack of datasets and methods for audio-synchronized video generation that can animate object dynamics in diverse scenarios.

Proposed Solution:
- Introduce the task of Audio-Synchronized Visual Animation (ASVA), which animates a static image into a video with object motions synchronized to an input audio clip.
- Construct a dataset called AVSync15 by carefully curating videos from VGGSound through multiple steps of automatic filtering and manual verification. AVSync15 has 15 categories with 100 examples each that exhibit strong audio-visual synchronization.
- Propose a model called Audio-Video Synchronized Diffusion that builds on latent diffusion models by incorporating: (1) frozen time-aware audio embeddings from ImageBind, (2) first-frame conditioned temporal convolutions and attentions, (3) additional temporal self-attention layers.
- Train the model on AVSync15 and show it generates better quality and more synchronized motions compared to prior arts through automated metrics and human evaluation.

Main Contributions:
- Formulation of the ASVA task and creation of the AVSync15 dataset to facilitate research into controlled and synchronized video generation.
- Audio-Video Synchronized Diffusion model that effectively utilizes time-dependent audio embeddings to guide temporally coherent object animations.
- Quantitative experiments demonstrating state-of-the-art performance, and qualitative demonstrations of controllable generation by animating target objects based on arbitrary input sounds.
