# [Artist-Friendly Relightable and Animatable Neural Heads](https://arxiv.org/abs/2312.03420)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new method for creating animatable and relightable neural avatars of human heads. The method builds on the Mixture of Volumetric Primitives (MVP) architecture, which represents a deformable head model using a collection of geometric primitives with appearance properties. The key contribution is an illumination-modulated appearance branch that conditions the primitive appearance not just on view direction but also on per-primitive local light directions. This allows relighting with near-field effects. The model is trained on multi-view video of an actor performing facial expressions, captured under one-light-at-a-time illumination from an inexpensive LED setup. Compared to prior work like TRAvatar, this method achieves higher quality rendering of faces under novel lighting conditions and viewpoints, even generalizing to new expressions. Key results demonstrate photorealistic rendering of captured or artist-created performances with control over scene illumination using environment maps. Limitations include motion blur and inability to extrapolate beyond the span of training expressions. The proposed relightable MVP representation leads to an animatable neural head model with unprecedented artistic control.


## Summarize the paper in one sentence.

 This paper proposes a new architecture for creating animatable and relightable neural avatars of human heads by extending the Mixture of Volumetric Primitives with per-primitive lighting and view directions.


## What is the main contribution of this paper?

 The main contribution of this paper is a new architecture for creating animatable and relightable neural avatars of human heads. Specifically:

- They build on top of the Mixture of Volumetric Primitives (MVP) architecture and extend it to support relighting by conditioning the appearance branch on per-primitive light and view directions. This allows rendering with near-field illumination and viewpoints.

- They train the network end-to-end on one-light-at-a-time (OLAT) sequences captured with an inexpensive hardware setup consisting of only 10 cameras and 32 LED light bars.

- Their model supports novel view and light estimation and also generalizes to unseen expressions and performances. It allows playback of captured performances as well as driving the avatar using other sources like blendshape animation or monocular face tracking.

- They demonstrate high quality rendering of dynamic neural avatars that can be artistically controlled in terms of scene illumination, camera viewpoint and facial expressions.

In summary, the key innovation is a new neural architecture and training process for creating animatable and relightable neural head avatars with practical capture requirements.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Relightable neural avatars - The paper focuses on creating digital avatars that can be relit under different lighting conditions.

- Animatable neural avatars - The paper also focuses on creating avatars that are animatable, i.e. they can perform different facial expressions and motions. 

- Mixture of Volumetric Primitives (MVP) - The method builds on this representation for animatable avatars.

- One light at a time (OLAT) - The training data consists of performances captured under OLAT lighting.

- Local light directions - The appearance branch is conditioned on per-primitive local light directions to enable nearfield lighting effects. 

- Local view directions - Similarly, per-primitive local view directions allow for nearfield camera views and effects like dolly zooms.

- Facial performance capture - The avatars are trained on captured facial performances of subjects.

- Neural rendering - The overall approach falls under neural rendering, where a neural network is used to render novel views of a scene.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1) The paper proposes a new architecture for creating animatable and relightable neural avatars. Can you explain in detail the modifications made to the MVP architecture to enable relighting capabilities? How is the appearance branch conditioned on lighting? 

2) The method uses a per-primitive lighting model rather than a global distant lighting model. What is the motivation behind this? How does it allow for near-field lighting effects?

3) The training data uses a sparse set of LED bars compared to dense light stage setups. How is the paper still able to achieve high quality relighting? What is the lighting sequence used during capture?

4) Can you explain the mesh encoder used in the method and how it differs from the one used in the original MVP formulation? Why was this change made?

5) What are the practical applications of having animatable and relightable neural avatars? In what scenarios would this representation be preferred over traditional graphics pipelines?

6) The method supports novel expressions and performances at test time. What are the different ways these novel animations can be generated? What are the limitations?

7) How does the method perform lighting interpolation and extrapolation? Can you analyze the relighting quality both quantitatively and qualitatively when going beyond the training light directions?

8) What components of the method could be improved to achieve real-time performance for rendering? Would any quality tradeoffs need to be made?

9) The paper demonstrates near-field lighting effects. How is this achieved technically in terms of network architecture and conditioning? Why is it an important practical feature?

10) The method still has limitations in terms of facial coverage, motion blur, and extreme expressions. Can you suggest ways these could be addressed in future work? What changes would need to be made?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Creating photo-realistic and controllable digital avatars of real people has many applications, but is challenging. Recent neural rendering approaches like Neural Radiance Fields (NeRFs) allow novel view synthesis of static heads from images, but don't support motion or relighting. Other methods have extended NeRFs for animatable avatars, but still lack controllable relighting. The goal is to create neural avatar representations that are both animatable and relightable.

Proposed Solution: 
The paper proposes a new architecture called Relightable MVP (RelMVP) that builds on top of the Mixture of Volumetric Primitives (MVP) representation for animatable avatars. The key ideas are:

1) Capture multi-view videos of an actor under one-light-at-a-time (OLAT) illumination instead of constant lighting.

2) Extend the MVP appearance branch to take per-primitive local view and light directions as input instead of global vectors. This allows modeling of near-field effects.

3) Compute local view and light directions per primitive by transforming primitive centers into world space using the geometry branch output. 

4) Condition the RGB prediction on local view/light directions, but predict opacity independently.

5) Train end-to-end on OLAT image sequences, facial landmarks and calibration.


Main Contributions:

- A new RelMVP architecture for animatable and relightable neural avatar creation from multi-view OLAT videos.

- Modeling of near-field illumination and camera effects like dolly-zoom by using per-primitive local view/light directions.

- High visual quality neural rendering of faces for novel expressions, views and lighting without needing an expensive light stage setup.

- Quantitative and qualitative evaluation showing improved performance over baseline methods.

The proposed RelMVP representation advances neural avatars towards more flexible artistic control over motion and illumination. Limitations include slow rendering and lack of hair/eyelid animation.
