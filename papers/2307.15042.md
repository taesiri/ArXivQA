# [TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis](https://arxiv.org/abs/2307.15042)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to adapt diffusion models to synthesize arbitrarily long, high quality motion sequences for character animation. 

The key ideas and contributions are:

- Proposing a novel way to entangle the temporal axis of the motion sequence with the time axis of the diffusion process. This enables continuous generation of motion frames during diffusion.

- Introducing a motion buffer that maintains an auto-regressive context containing progressively noised future frames. Denoising this buffer produces clean frames while maintaining the buffer structure.

- Enabling controllability during generation through guided inputs and trajectory control by modifying the motion buffer.

- Demonstrating the ability to generate diverse, non-repetitive motions for an arbitrary duration without common failure modes like motion collapse.

The overall hypothesis is that by adapting the diffusion framework to operate along the temporal dimension of motion, it can achieve long-term synthesis not possible with prior diffusion or RNN based approaches. The method is evaluated on tasks like guided motion synthesis and trajectory control.

In summary, the key innovation is a temporally-entangled diffusion mechanism that harnesses the power of diffusion models for continuous long-term motion generation with control.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing TEDi, a novel adaptation of diffusion models for long-term motion synthesis. The key ideas are:

- Extending the diffusion framework to support temporally-varying noise injection during the diffusion process. This allows entangling the temporal axis of the motion with the diffusion time axis.

- Using a motion buffer that contains increasingly noised future frames. During each diffusion step, the model denoises the buffer, pops out a clean frame, and appends a new noisy frame. This enables continuous generation of motion frames.

- Allowing guided generation by replacing frames in the buffer with motion guides. This enables performing desired motions at specified times during the overall generated sequence.

- Demonstrating that TEDi can generate diverse, non-repetitive, and high quality motions for arbitrarily long durations. This is a significant improvement over prior RNN and VAE based methods.

Overall, the main contribution is proposing a novel way to adapt diffusion models to motion synthesis in order to achieve high-quality, controllable, and infinite length motion generation. The key ideas of temporally-entangled diffusion and the motion buffer are innovative.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes Temporally-Entangled Diffusion (TEDi), a new framework that adapts diffusion models for long-term motion synthesis by entangling the temporal axis of the motion with the diffusion time axis, enabling continuous generation of arbitrary length motion sequences in an autoregressive manner.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in motion generation with diffusion models:

- This paper introduces a novel way to generate long-term motion sequences with diffusion models by entangling the temporal axis of the motion with the diffusion process time axis. Other recent works like Motion Diffusion (MDM) and animation via cascaded diffusion models (ACDM) have applied diffusion models to motion, but generate fixed-length outputs and do not have a natural mechanism for long-term generation.

- The key innovation of this paper is using a "motion buffer" that maintains future frames at increasing noise levels, and recursively denoises, pops, and pushes frames. This enables continuous generation of new frames in an autoregressive manner. Other diffusion model papers have not explored this idea of a stationary diffusion process for sequence generation.

- For conditional generation and control, this paper shows guiding by modifying the motion buffer, which enables smoother transitions compared to naive conditioning. Other papers like MDM and ACDM have not demonstrated extensive conditioning for motion.

- This paper focuses on character motion, while other concurrent work like Motion Diffuse has applied diffusion to human motion capture data. The method should be applicable to other motion modalities.

- The paper demonstrates very long and diverse generated motions (up to 1000 frames = 30 seconds) without obvious degeneration. Quantitative comparisons show the method avoids collapse better than RNN baselines. The method produces strong results on complex motions like dancing.

- The approach is not constrained to a specific model architecture, and uses a simple 1D U-Net model. Other papers have explored more complex models like transformer-based models.

In summary, this paper makes valuable contributions in adapting diffusion models to sequence generation tasks, and demonstrates a promising approach for controllable long-term motion synthesis. The core idea of entangling axes could inspire other sequential generation tasks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Exploring how to achieve even lower latency during inference by leveraging ideas like DDIM to skip ahead during the denoising process instead of requiring every frame to go through the full diffusion process. 

- Investigating long-term text-conditioned motion generation, where the model could generate long sequences conditioned on natural language prompts or descriptions.

- Combining high-level control from textual or other conditional inputs with low-level user guidance during long-term generation. This could allow balancing user control with model capabilities.

- Applying the idea of entangling the temporal axis with the diffusion time axis to other types of sequential data beyond just motion, such as audio, video, or even patches within images. The autoregressive diffusion approach may be useful across modalities.

- Improving the interactive controllability and ability to plan for future motions during long-term generation, by manipulating or conditioning on the motion buffer.

- Addressing current limitations such as the latency in generating a clean frame from pure noise, which still requires going through the full diffusion process chain.

In summary, key future directions focus on improving controllability, generalizing the approach to other data modalities, achieving lower latency, and exploring conditional generation through text or other inputs. The core idea of entangling the temporal axis with diffusion could have broad applications.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes TEDi, a novel adaptation of denoising diffusion probabilistic models (DDPMs) for generating long-term motion sequences. TEDi entangles the temporal axis of the motion sequence with the diffusion time axis, enabling continuous synthesis of arbitrarily long motion streams. During training, TEDi adds temporally varying noise to clean motions. At inference, TEDi maintains a motion buffer of progressively noised future frames and recursively denoises them. At each step, it pops off a clean frame from the start of the buffer, shifts the remaining frames, and appends new noise to the end. This recursive mechanism generates clean frames indefinitely without stitching artifacts. TEDi also enables controllable generation by modifying the motion buffer with motion guides or trajectories. Experiments demonstrate TEDi's ability to generate varied, non-repetitive motions superior to baselines. The entangled diffusion offers a new paradigm for synthesizing long sequential data like motion.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes TEDi, a new method for long-term motion synthesis using diffusion models. Diffusion models have achieved high quality results for image synthesis by gradually adding noise to data during training, and learning to reverse the process during inference. The key idea in TEDi is to entangle the temporal axis of the motion sequence with the diffusion time axis. This is done by training the model to denoise motion sequences where noise levels vary across frames. At inference time, TEDi maintains a buffer of noisy future frames, and recursively denoises them to generate clean frames. Specifically, in each step the model denoises the buffer, outputs the first clean frame, pops it from the buffer, and pushes a new noisy frame to the end. This allows arbitrarily long motion sequences to be generated. TEDi also enables control over the generated motions through guided generation, where desired motion clips can replace parts of the buffer to influence the generated frames. Experiments demonstrate TEDi's ability to generate diverse, non-repetitive motions over long time horizons. It also shows higher quality compared to prior RNN and diffusion baselines.

Overall, the key contributions are 1) entangling the temporal axis with diffusion time axis for long-term generation 2) a buffer-based inference scheme that recursively denoises futures frames 3) control through guided generation by modifying the buffer 4) higher quality long-term motion compared to RNN and diffusion baselines. The method opens possibilities for controlling long character animations and motion sequences in other domains.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method in the paper:

The paper proposes Temporally-Entangled Diffusion (TEDi), a new diffusion-based framework for long-term motion synthesis. TEDi entangles the temporal axis of the motion sequence with the diffusion time axis, enabling the iterative denoising of a motion buffer to produce clean motion frames. During training, clean motion sequences are noised with temporally-varying levels. At inference, a motion primer initializes the buffer with progressively noised frames. TEDi then denoises the buffer, pops off the clean frame, appends a new noisy frame, and repeats. This recursively generates an arbitrarily long motion stream. A key advantage is the ability to guide generation by directly modifying the motion buffer with desired future motions. Experiments demonstrate TEDi's effectiveness for long-term synthesis and control compared to prior RNN and diffusion baselines. The entangled temporal-diffusion axes enable continuous, non-collapsing motion generation in an intuitive diffusion framework.


## What problem or question is the paper addressing?

 The main problem this paper is addressing is how to generate long-term, high-quality motion sequences for character animation using diffusion models. Specifically, it focuses on two key challenges:

1. Existing diffusion models for motion synthesis like DDPM generate fixed-length outputs, which makes long-term generation difficult as simply stitching together short sequences can cause artifacts. 

2. Diffusion models typically require hundreds of denoising steps on the full sequence before producing any clean outputs, limiting interactive control during inference.

To address these issues, the paper proposes a new framework called TEDi that entangles the temporal axis of the motion sequence with the diffusion time axis. This allows TEDi to continuously generate new frames during each diffusion step in an autoregressive manner, enabling arbitrary length synthesis. It also maintains a buffer of future frames that enable more direct interactive control over the generated motions.

So in summary, the key problem is enabling high-quality, controllable long-term motion generation using diffusion models in a way that avoids the stitching artifacts or lack of interactivity in prior works. TEDi offers a new formulation to achieve this.


## What are the keywords or key terms associated with this paper?

 Based on reading the paper, some of the key terms and concepts are:

- Denoising Diffusion Probabilistic Models (DDPM): The paper proposes adapting this framework for motion synthesis by entangling the temporal axis of motion with the diffusion time axis. DDPM models have shown success in image synthesis.

- Temporally-Entangled Diffusion (TEDi): The name of the proposed method that entangles the temporal axis of motion sequences with the diffusion time axis to enable long-term motion synthesis. 

- Motion buffer: A buffer that contains noisy future motion frames that are progressively denoised. This is a key component of the proposed TEDi framework.

- Diffusion time axis vs. temporal axis: The diffusion time axis refers to the noise schedule in DDPM. Entangling this with the temporal axis of motion is a key idea in TEDi.

- Long-term motion synthesis: A goal and application of the proposed method. TEDi aims to address limitations of prior works for long-term motion generation.

- Autoregressive generation: TEDi generates motion frames autoregressively by denoising the motion buffer. This allows arbitrary length motion synthesis.

- Guided generation: TEDi supports guiding motion generation by inserting motion guides into the buffer. This allows control over the generated motions.

- Motion representation: The paper describes representing motion using joint rotations, root displacements, foot contacts, etc. 

- Recursive inference: The proposed recursive inference procedure for long-term generation with a stationary diffusion time axis.

In summary, the key ideas are entangling the temporal and diffusion axes, the motion buffer, and autoregressive recursive generation for long-term motion synthesis.
