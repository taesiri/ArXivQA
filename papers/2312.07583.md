# [Classification with Partially Private Features](https://arxiv.org/abs/2312.07583)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper considers the problem of differentially private classification when some features of an individual are sensitive/private while other features and the label are public. The authors argue that existing differentially private classification algorithms either add noise to individual data items or to the output, without distinguishing between public and private features. This leads to more noise than necessary. The key question studied is: how to design a differentially private classifier that improves accuracy over methods that assume all features/label as private by exploiting public features?

Proposed Solution:
The paper proposes a novel modification of the AdaBoost algorithm called Boosting with Random Classifiers (BRC). The key ideas are:

1) BRC maintains two sets of weights - public weights for public features and private weights for private features. Public classifiers are learned using public weights. Private classifiers are randomly generated linear classifiers, without using private weights.

2) In each AdaBoost iteration, BRC generates a public classifier and a random private classifier. It picks the one farther from 50% accuracy and updates weights accordingly. Only the accuracy computation of the private classifier is made differentially private.  

3) This avoids adding noise to private weights while preserving privacy. Empirically it adds much less noise compared to methods that perturb both classifier accuracy and weights.

Contributions:

1) BRC provides an effective differentially private classifier when some features are public, significantly outperforming baselines like public-feature-only classifiers or private classifiers on all features.

2) BRC also extends naturally to the classical setting of all features being private, providing comparable or better accuracy than differentially private logistic regression. The privacy analysis is simpler.

3) Theoretically, the paper shows boosting works even with random weak classifiers, an extreme case of AdaBoost's ability to generalize despite overfitting.

4) Empirically, BRC outperforms previous differentially private boosting methods and converges at nearly the same rate as non-private AdaBoost. Experiments are on real datasets with naturally split public and private features.

In summary, the paper introduces a novel yet simple adaption of AdaBoost to differentially private classification that exploits public features, with strong theoretical justification and empirical performance.
