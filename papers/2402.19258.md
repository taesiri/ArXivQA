# [MaskFi: Unsupervised Learning of WiFi and Vision Representations for   Multimodal Human Activity Recognition](https://arxiv.org/abs/2402.19258)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Human activity recognition (HAR) plays an important role in many applications like healthcare, security monitoring, etc. Vision-based methods suffer from poor robustness in adverse illumination conditions. Though WiFi-based methods are robust, they require massive labeled data which is cumbersome to collect. 
- Existing WiFi-vision multimodal HAR relies on large labeled datasets and cannot handle new environments well. They use simple feature concatenation unable to deeply explore cross-modal correlations.

Proposed Solution:
- The paper proposes MaskFi, a self-supervised multimodal HAR framework requiring only unlabeled WiFi-vision data for model pretraining and few labeled samples for finetuning.
- It proposes a novel pretraining task - Masked wIfi-vIsion Modeling (MI2M) to learn cross-modal features by predicting masked sections in the joint representation. 
- The framework encodes WiFi and video frames into tokens. After masking some tokens, an encoder-decoder model is trained to reconstruct original tokens, learning useful features for downstream tasks.

Main Contributions:
- First work incorporating self-supervised learning for WiFi-vision multimodal HAR to achieve labor-efficient system.
- Novel MI2M pretraining by masking and reconstructing joint representations to learn cross-modal correlations.
- Experiments on two datasets show state-of-the-art performance for activity recognition and identification, demonstrating effectiveness in real-world scenarios.
- The method shows stronger robustness in dark environments compared to single modality methods.
- With large dataset pretraining, the framework can quickly adapt to new environments with 1.74% performance drop, showing good transferability.
