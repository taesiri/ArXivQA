# [BERMo: What can BERT learn from ELMo?](https://arxiv.org/abs/2110.15802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can modifying the BERT architecture by adding skip connections like in ELMo lead to improvements in performance and efficiency? In particular, the paper seems focused on investigating whether combining features from different layers of BERT using a linear weighting scheme similar to ELMo can:1) Improve performance on semantic NLP tasks compared to standard BERT.2) Enable more stable and efficient compression of BERT via pruning. The authors hypothesize that adding these ELMo-style skip connections will allow BERT to create richer, more complex feature representations by combining features from different depths. They further hypothesize this will improve performance on semantic tasks and also help optimization and compression by improving gradient flow during training.The experiments then test these hypotheses by evaluating the modified "BERMo" model on semantic probing tasks, pruning tasks, and other NLP datasets. The results generally confirm their hypotheses, showing improvements in accuracy on semantic tasks, faster convergence during pruning, and better stability when compressing smaller datasets compared to standard BERT.In summary, the central research question is whether adding ELMo-style skip connections to BERT can improve performance and enable more efficient compression, which the experiments generally validate. Let me know if I missed anything or you would like me to clarify or expand on any part of the summary!
