# [BERMo: What can BERT learn from ELMo?](https://arxiv.org/abs/2110.15802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can modifying the BERT architecture by adding skip connections like in ELMo lead to improvements in performance and efficiency? In particular, the paper seems focused on investigating whether combining features from different layers of BERT using a linear weighting scheme similar to ELMo can:1) Improve performance on semantic NLP tasks compared to standard BERT.2) Enable more stable and efficient compression of BERT via pruning. The authors hypothesize that adding these ELMo-style skip connections will allow BERT to create richer, more complex feature representations by combining features from different depths. They further hypothesize this will improve performance on semantic tasks and also help optimization and compression by improving gradient flow during training.The experiments then test these hypotheses by evaluating the modified "BERMo" model on semantic probing tasks, pruning tasks, and other NLP datasets. The results generally confirm their hypotheses, showing improvements in accuracy on semantic tasks, faster convergence during pruning, and better stability when compressing smaller datasets compared to standard BERT.In summary, the central research question is whether adding ELMo-style skip connections to BERT can improve performance and enable more efficient compression, which the experiments generally validate. Let me know if I missed anything or you would like me to clarify or expand on any part of the summary!


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing BERMo, an architectural modification to BERT that combines features from different layers using a linear combination scheme similar to ELMo. This is done to obtain a richer feature representation.2. Evaluating BERMo on the probing tasks from the SentEval dataset and showing it performs better than baseline BERT on average, especially on semantic tasks. 3. Demonstrating BERMo is more stable and converges faster than baseline BERT when subject to pruning techniques, especially for smaller datasets like SST-2 where BERT often diverges.4. Showing BERMo achieves comparable or marginally better results than BERT when combined with knowledge distillation.5. Overall, proposing and evaluating an architectural modification to BERT to improve performance, stability, and efficiency when pruning, while having a negligible parameter overhead. The linear combination scheme enables better gradient flow and feature representation.In summary, the main contribution seems to be proposing and demonstrating the benefits of the BERMo architecture, which modifies BERT to combine features across layers like ELMo, for improved standalone performance, pruning stability, convergence speed, and efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes BERMo, a modification to the BERT architecture that combines features from different layers using a linear combination scheme like ELMo. This improves performance on semantic tasks, enables more stable and faster training and compression, and allows for higher pruning rates with comparable accuracy. The key idea is that combining features from different layers results in a richer representation that improves BERT's capabilities.


## How does this paper compare to other research in the same field?

Based on my review, here are some thoughts on how this paper compares to other research in the field:- The approach of combining features from different layers of BERT using a linear weighting scheme is similar to prior work like ELMo and methods that compute scalar mixing weights for BERT layers. However, the goal here is different - to improve model stability and enable more aggressive pruning rather than just interpreting what the layers capture.- Using skip connections between layers has been explored in other architectures like ResNets, but applying it to Transformer models/BERT is novel. The impact on model convergence during pruning specifically has not been shown before. - Most prior BERT compression techniques rely solely on pruning, knowledge distillation, quantization etc. Modifying the architecture itself to enable better compression is a fairly new direction.- For distillation, TinyBERT is a well known compressed variant of BERT based on knowledge distillation. This paper shows BERMo can match or slightly exceed baseline BERT performance under distillation, contributing a new compressed model variant.- Many recent BERT compression papers have focused on optimizing accuracy after compression. Analyzing convergence speed, training stability, and parameter efficiency in compression as done here adds useful dimensions for comparison.- Overall, the modifications seem simple but the paper convincingly demonstrates their impact on compression capability. The comparisons to baseline BERT are thorough.In summary, the approach stands out in leveraging architectural changes to improve model compression. The analysis offers new insights into model stability and efficiency during compressed training that were not studied in detail before for BERT models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring more complex skip connection architectures with more parameters in order to try to improve upon the baseline performance. The authors used a simple weighted linear combination scheme in this work, but suggest exploring more complex connections.- Mixing features at each layer rather than just the last layer. The authors suggest that summarizing features at each layer could potentially benefit the model further, though this would require pretraining from scratch since it changes the activations. - Applying the proposed approach to other Transformer-based models beyond BERT. The authors note their methodology is agnostic to the choice of backbone network, so it could be extended to other architectures like GPT.- Testing whether the faster convergence and stability trends hold during pretraining. The authors focused on the fine-tuning phase in this work, but suggest extensively testing if similar benefits are seen during pretraining as well.- Exploring modifications like gradually increasing the number of skip connections during training. The authors suggest this could help ease optimization.- Studying the locality of the features combined through the skip connections to better understand what is being combined.In summary, the main future directions focus on exploring more complex and varied skip connection architectures, applying the approach to other models and pretraining, and analyzing the features being combined to better understand how the skip connections are improving performance. The authors frame this work as a starting point for further exploration of skip connections in Transformer models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper: The paper proposes BERMo, an architectural modification to BERT that generates complex feature maps by linearly combining the features from different layers of the network. This is inspired by the approach used in ELMo where features from different BiLSTM layers are combined. BERMo adds a small number of learnable scalar parameters, one per layer, to weigh the contributions from each layer. Experiments on probing tasks show improvements on semantic tasks compared to baseline BERT, indicating the combined features better capture semantic information. BERMo also demonstrates improved stability and faster convergence during pruning showing its potential for model compression. Overall, BERMo provides a simple way to enrich BERT's features and improve performance on semantic tasks while enabling more efficient model compression.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes BERMo, an architectural modification to BERT that combines features from different layers to create richer representations. BERMo uses a linear combination scheme like ELMo to mix the scaled internal representations from various network depths. This improves gradient flow for downstream tasks since every layer connects directly to the loss, and increases the model's representational power since features from all layers contribute. BERMo has negligible overhead with just one scalar per layer. Experiments on SentEval probing tasks show BERMo improves accuracy especially on semantic tasks, with a 2.67% average boost. BERMo also enables more stable pruning on small datasets where BERT diverges, and faster convergence, supporting 1.67x and 1.15x higher pruning rates on MNLI and QQP. For penalty-based pruning, BERMo achieves better parameter efficiency on QQP. Results are comparable to BERT for knowledge distillation, with minor improvements on SQuAD.In summary, BERMo modifies BERT to combine features across layers like ELMo. This improves accuracy on semantic tasks, model stability and convergence for pruning, parameter efficiency on some datasets, and matches BERT performance for knowledge distillation. The architectural change enables better gradient flow and more complex representations without much overhead. So BERMo represents a promising way to enhance BERT with negligible costs, especially for compressed models.
