# [Abstraction via exemplars? A representational case study on lexical   category inference in BERT](https://arxiv.org/abs/2312.03708)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a debate between "abstractionist" and "exemplar" accounts of how humans acquire linguistic categories and generalize to novel expressions. Abstractionists argue we form abstract rules, while exemplar theories posit we generalize based on stored instances.

- Neural language models like BERT have unprecedented success in language tasks, even though they differ from both accounts - they store a summarized form of training exemplars guided by their training objective. Can their success be viewed through the lens of "abstraction via exemplars"?

Method:
- The paper adapts experiments from Kim & Schutze (2021) testing if BERT can infer lexical categories (noun, verb, etc) for novel tokens from a single exemplar exposure.

- They track the movement of novel token embeddings during training and test performance when novel tokens are initialized to be close to regions populated by known category exemplars.

Results: 
- Novel token embeddings move closer to regions of known category exemplars after training.

- When novel tokens are initialized closer to exemplar regions, BERT shows strong generalization performance to new inputs without any training.

Conclusion:
- The analyses provide evidence that representational movement towards exemplar regions enables abstraction-like generalization in BERT. 

- This supports the view that abstraction can emerge in systems that encode training exemplars, without explicit abstractionist machinery. The accounts need not contradict.
