# [PointNorm: Dual Normalization is All You Need for Point Cloud Analysis](https://arxiv.org/abs/2207.06324)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is how to create an effective and efficient 3D point cloud analysis framework that addresses the irregularity of point clouds while eliminating the need for complex local and global feature extractors. The key hypotheses appear to be:1) Normalizing the grouped points and sampled points to each other after sampling-grouping operations can help address the irregularity of point clouds and make learning easier for subsequent layers. This is done through the proposed DualNorm module.2) Using local mean and global standard deviation in the normalization allows leveraging both local and global features while maintaining efficiency. 3) The overall framework, PointNorm, can achieve excellent accuracy and efficiency on point cloud analysis tasks like classification, part segmentation, and semantic segmentation without relying on sophisticated feature extraction modules.So in summary, the central goal is developing a simple yet accurate and efficient framework for point cloud analysis, with the key ideas being the DualNorm module for addressing irregularity and the use of local mean and global standard deviation for feature learning. The effectiveness of this approach is evaluated through experiments on various analysis tasks.
