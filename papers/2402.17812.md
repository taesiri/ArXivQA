# [DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping   Backward Propagation](https://arxiv.org/abs/2402.17812)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Training deep neural networks like large language models (LLMs) is computationally expensive, requiring substantial computations for both forward propagation and backward propagation (backpropagation).
- Techniques like parameter-efficient fine-tuning (PEFT) reduce memory for gradients and parameters but do not decrease computations in forward and backward propagation. 
- Layer dropping techniques reduce computations by randomly skipping layers but hurt accuracy as they modify model outputs during forward propagation.

Proposed Solution:
- The paper proposes DropBP, a novel technique that randomly drops layers during backward propagation only, avoiding modifying model outputs.
- DropBP calculates a sensitivity score for each layer indicating its impact on training. Layers are dropped during backpropagation based on sensitivity to stabilize training.  
- DropBP can accelerate both full fine-tuning and PEFT methods like LoRA and QLoRA that rely on backpropagation.

Main Contributions:
- DropBP maintains accuracy comparable to baseline fine-tuning while reducing training time. It accelerates convergence by 1.5x to the same loss level.
- DropBP is implemented as an easy-to-use PyTorch extension with simple APIs to integrate into existing training code.
- When used with QLoRA fine-tuning of LLaMA2-70B, DropBP reduces training time by 44%, increases max sequence length trainable on 1 GPU by 6.2x, with negligible accuracy drop.
- For full fine-tuning LLaMA2-7B, DropBP reduces training time by 57% and increases max sequence length by 7.1x with comparable accuracy.

In summary, the paper makes significant contributions in accelerating fine-tuning of large neural models by proposing DropBP that strategically drops computation in backpropagation only. The method can seamlessly integrate into existing training pipelines.
