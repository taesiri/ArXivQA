# [Generalizing Few-Shot NAS with Gradient Matching](https://arxiv.org/abs/2203.15207v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the accuracy of neural architecture search methods that use weight-sharing techniques?

The key hypothesis appears to be:

By carefully partitioning the weight-sharing supernet into multiple sub-supernets to reduce the level of coupling, we can obtain more accurate performance estimations of the architectures and improve the search results.

Specifically, the paper proposes using a gradient matching score to determine which architectures should share weights vs be separated into different sub-supernets. This allows creating a partition that is optimized to reduce the harmful effects of weight sharing. The paper hypothesizes and shows that this approach can outperform prior weight-sharing NAS methods like ENAS and DARTS.

In summary, the main research question is how to improve weight-sharing NAS, and the key hypothesis is that using an optimized partition of the supernet based on gradient matching can achieve this goal. The experiments aim to validate if the proposed gradient-based partitioning method actually improves accuracy over other weight-sharing techniques.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight sharing. 

The key ideas are:

- Weight sharing in one-shot NAS methods like DARTS leads to inaccurate performance estimation of child models due to coupled optimization. This degrades the search results.

- To address this issue, GM-NAS reduces the level of weight sharing by partitioning the one-shot supernet into multiple sub-networks. This is similar to prior work like Few-Shot NAS.

- However, GM-NAS proposes a more principled way to determine which child models should share weights vs. be separated, based on their training dynamics (gradients). Child models with more dissimilar gradients are separated.

- This allows GM-NAS to generalize few-shot NAS to have flexible branching factors. With lower branching, it can split more edges given a fixed budget.

- Empirically, GM-NAS outperforms few-shot NAS significantly across various search spaces, datasets, and base NAS algorithms like DARTS, SNAS, ProxylessNAS etc.

In summary, the key contribution is a method to partition weight-sharing supernets during NAS using gradient similarity as a splitting criterion, which results in more accurate architecture search.
