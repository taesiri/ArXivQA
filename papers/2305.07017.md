# [An Inverse Scaling Law for CLIP Training](https://arxiv.org/abs/2305.07017)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether there exists some kind of "inverse scaling law" for training contrastive language-image models like CLIP, where larger encoder architectures allow for shorter sequence lengths during training while maintaining strong performance. Specifically, the authors investigate how reducing the length of image patches/tokens and text tokens during pre-training affects the final performance of CLIP models with different sized encoders. They test different strategies for reducing image and text sequence lengths, like truncation, masking, and resizing. The key finding is that larger CLIP encoders are more robust to reductions in sequence length during pre-training, allowing the use of fewer image patches and text tokens while still achieving competitive performance after fine-tuning. For example, the authors show that a large CLIP model can be trained with only 17 image tokens while a small model requires 101 tokens, for a similar performance drop compared to using full sequences.In summary, the central hypothesis is that an "inverse scaling law" exists where larger CLIP models enable shorter sequence lengths during pre-training with minimal impact on final performance. This allows for more efficient training of large CLIP models using fewer compute resources.


## What is the main contribution of this paper?

The main contribution of this paper seems to be presenting evidence for an "inverse scaling law" for training contrastive language-image models like CLIP. The key findings are:- Larger image and text encoders allow using shorter image and text token sequences during training while maintaining competitive performance compared to models trained with full token lengths. - For image tokens, strategies like resizing that retain more semantic information allow shorter token lengths to be used compared to masking strategies. - For text tokens, syntactic masking that prioritizes retaining certain parts of speech allows shorter token lengths.- Based on these findings, the authors introduce an efficient CLIP training framework called CLIPA that can train CLIP-scale models with much less computation than prior work. For example, they train a CLIPA-L/16 model in 3 days on 8 A100 GPUs that achieves 67.8% ImageNet accuracy, compared to OpenCLIP-B/16 which requires over 16x more GPU hours for a similar accuracy.In summary, the main contribution is identifying this inverse scaling law that enables more efficient training of large CLIP models, making it more accessible for academics and others with limited compute resources. The key idea is that bigger models need less input signal, enabling shortcuts like using lower resolution images and truncated text.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper finds that there is an inverse scaling law for CLIP training, whereby larger image/text encoders enable effective training with fewer image/text tokens.
