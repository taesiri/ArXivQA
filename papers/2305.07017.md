# [An Inverse Scaling Law for CLIP Training](https://arxiv.org/abs/2305.07017)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether there exists some kind of "inverse scaling law" for training contrastive language-image models like CLIP, where larger encoder architectures allow for shorter sequence lengths during training while maintaining strong performance. Specifically, the authors investigate how reducing the length of image patches/tokens and text tokens during pre-training affects the final performance of CLIP models with different sized encoders. They test different strategies for reducing image and text sequence lengths, like truncation, masking, and resizing. The key finding is that larger CLIP encoders are more robust to reductions in sequence length during pre-training, allowing the use of fewer image patches and text tokens while still achieving competitive performance after fine-tuning. For example, the authors show that a large CLIP model can be trained with only 17 image tokens while a small model requires 101 tokens, for a similar performance drop compared to using full sequences.In summary, the central hypothesis is that an "inverse scaling law" exists where larger CLIP models enable shorter sequence lengths during pre-training with minimal impact on final performance. This allows for more efficient training of large CLIP models using fewer compute resources.
