# [ComplexityNet: Increasing LLM Inference Efficiency by Learning Task   Complexity](https://arxiv.org/abs/2312.11511)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT-4 have significantly advanced AI capabilities but also have very high computational costs that scale with model size. This makes them inaccessible and inefficient, especially for simpler tasks that don't require the most advanced model.

- There lacks a systematic approach to automatically select the optimal LLM for a given task based on its complexity. Current platforms rely on manual selection, leading to inefficiency.

Proposed Solution:
- The authors propose ComplexityNet, a small LLM fine-tuned to predict the complexity of a task prompt. Complexity is defined as the simplest LLM that can successfully accomplish the task.

- They assign complexity labels to prompts in a Python coding dataset based on success rates of Code Llama 7B, GPT-3.5 and GPT-4 on solving them. A mapping then assigns these labels to one of the three models.

- They fine-tune DaVinci-002 multiple times on this labeled dataset to create ComplexityNet models that take a prompt and output a complexity score.

Main Contributions:
- Created first complexity labels for Python coding tasks based on LLM success rates

- Fine-tuned ComplexityNet achieves 79% accuracy on complexity inference, significantly higher than 34% for base model

- Enables over 90% cost reduction in inferences while maintaining 86.7% accuracy by using appropriate LLM selected by ComplexityNet

- Demonstrates finetuning smaller models to categorize task complexity is a promising direction to balance accuracy and efficiency for LLM applications

In summary, the paper presents ComplexityNet that can accurately assess task complexity for efficient LLM usage, helping address growing computational demands. The framework is generalizable to other datasets and tasks as well.
