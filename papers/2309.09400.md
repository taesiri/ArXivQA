# [CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large   Language Models in 167 Languages](https://arxiv.org/abs/2309.09400)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the key goal of this paper is to introduce CulturaX, a new large-scale and high-quality multilingual dataset for training large language models (LLMs) across 167 languages.  The authors argue that the lack of transparency around training datasets for state-of-the-art LLMs has hindered research into issues like hallucination and bias. Existing multilingual datasets are also inadequate in scale or cleaning rigor. To address this, the authors create CulturaX by merging and extensively cleaning two web-scraped multilingual corpora - mC4 and OSCAR. After careful data cleaning and deduplication, CulturaX contains 6.3 trillion tokens for 167 languages. Over half the data is non-English. The central hypothesis is that CulturaX, as a substantial and transparent multilingual dataset, will facilitate research into and development of LLMs in multiple languages. By releasing it publicly, the authors aim to promote democratization and investigation of multilingual LLMs.In summary, the key research question is: Can creating and openly releasing a large-scale, cleaned multilingual dataset advance research into and applications of multilingual LLMs? CulturaX represents their attempt to accomplish this goal.
