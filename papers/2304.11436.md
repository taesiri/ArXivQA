# [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://arxiv.org/abs/2304.11436)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It investigates the privacy risks of Federated Learning with Model Distillation (FedMD), which is a collaborative learning approach where clients share output logits on public datasets instead of model parameters or gradients. 

- The paper reveals that FedMD is vulnerable to a malicious attack called Paired-Logits Inversion (PLI) attack, where the attacker can reconstruct private images from the shared public logits. 

- The authors propose a novel PLI attack method that exploits the "confidence gap" between the server and client models' predictions on public data to estimate the logits for private data.

- Through this confidence gap, the attack is able to decouple private information from public knowledge and directly recover private images in a gradient-free manner.

- Experiments on facial recognition tasks demonstrate that the PLI attack can successfully reconstruct private face images on various datasets under FedMD and its variants with high attack success rate.

In summary, the key hypothesis is that FedMD is not as privacy-preserving as previously thought, and private images can be reconstructed from public logits through the proposed PLI attack. The paper reveals a new privacy risk in FedMD frameworks.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

1. Revealing a previously unknown privacy vulnerability in federated learning with model distillation (FedMD) and related schemes. The authors show that a malicious server can reconstruct private training images using only publicly available model outputs (logits). 

2. Proposing a novel "paired logits inversion" (PLI) attack that exploits confidence gaps between server and client models to reconstruct private data without access to gradients. The attack trains an inversion network on public data and optimizes logits to reconstruct private images.

3. Providing thorough quantitative and qualitative evaluation of the proposed attack on facial recognition tasks. Experiments on multiple datasets demonstrate successful reconstruction of private images under FedMD and related schemes with high accuracy.

In summary, this paper identifies and demonstrates a serious privacy threat in federated learning frameworks that distill public knowledge like FedMD. The authors design a new gradient-free attack that can reliably recover private images using only public outputs, highlighting the need for improved defense mechanisms. The proposed PLI attack and analysis of privacy risks under distillation-based federated learning are the main contributions.
