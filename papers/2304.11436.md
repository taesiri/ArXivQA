# [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://arxiv.org/abs/2304.11436)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It investigates the privacy risks of Federated Learning with Model Distillation (FedMD), which is a collaborative learning approach where clients share output logits on public datasets instead of model parameters or gradients. 

- The paper reveals that FedMD is vulnerable to a malicious attack called Paired-Logits Inversion (PLI) attack, where the attacker can reconstruct private images from the shared public logits. 

- The authors propose a novel PLI attack method that exploits the "confidence gap" between the server and client models' predictions on public data to estimate the logits for private data.

- Through this confidence gap, the attack is able to decouple private information from public knowledge and directly recover private images in a gradient-free manner.

- Experiments on facial recognition tasks demonstrate that the PLI attack can successfully reconstruct private face images on various datasets under FedMD and its variants with high attack success rate.

In summary, the key hypothesis is that FedMD is not as privacy-preserving as previously thought, and private images can be reconstructed from public logits through the proposed PLI attack. The paper reveals a new privacy risk in FedMD frameworks.
