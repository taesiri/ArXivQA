# [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://arxiv.org/abs/2304.11436)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It investigates the privacy risks of Federated Learning with Model Distillation (FedMD), which is a collaborative learning approach where clients share output logits on public datasets instead of model parameters or gradients. 

- The paper reveals that FedMD is vulnerable to a malicious attack called Paired-Logits Inversion (PLI) attack, where the attacker can reconstruct private images from the shared public logits. 

- The authors propose a novel PLI attack method that exploits the "confidence gap" between the server and client models' predictions on public data to estimate the logits for private data.

- Through this confidence gap, the attack is able to decouple private information from public knowledge and directly recover private images in a gradient-free manner.

- Experiments on facial recognition tasks demonstrate that the PLI attack can successfully reconstruct private face images on various datasets under FedMD and its variants with high attack success rate.

In summary, the key hypothesis is that FedMD is not as privacy-preserving as previously thought, and private images can be reconstructed from public logits through the proposed PLI attack. The paper reveals a new privacy risk in FedMD frameworks.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

1. Revealing a previously unknown privacy vulnerability in federated learning with model distillation (FedMD) and related schemes. The authors show that a malicious server can reconstruct private training images using only publicly available model outputs (logits). 

2. Proposing a novel "paired logits inversion" (PLI) attack that exploits confidence gaps between server and client models to reconstruct private data without access to gradients. The attack trains an inversion network on public data and optimizes logits to reconstruct private images.

3. Providing thorough quantitative and qualitative evaluation of the proposed attack on facial recognition tasks. Experiments on multiple datasets demonstrate successful reconstruction of private images under FedMD and related schemes with high accuracy.

In summary, this paper identifies and demonstrates a serious privacy threat in federated learning frameworks that distill public knowledge like FedMD. The authors design a new gradient-free attack that can reliably recover private images using only public outputs, highlighting the need for improved defense mechanisms. The proposed PLI attack and analysis of privacy risks under distillation-based federated learning are the main contributions.


## How does this paper compare to other research in the same field?

 This paper presents a new attack against federated learning with model distillation (FedMD) and its variants. The key contributions are:

1. It reveals a previously unknown privacy vulnerability in FedMD-like schemes. Prior work considered FedMD to be secure against model inversion attacks since it only shares output logits on public datasets instead of model parameters or gradients which are more susceptible to inversion attacks. However, this paper shows that carefully designed attacks can still reconstruct private data from the public logits.

2. It proposes a novel "paired logits inversion" (PLI) attack that is gradient-free and exploits the confidence gap between server and client models. Most prior inversion attacks rely on gradients or model parameters, so they cannot work under FedMD's threat model. In contrast, PLI is the first attack tailored for FedMD that recovers private data using only public logits.

3. It provides comprehensive experiments on facial recognition tasks to demonstrate the efficacy of PLI, showing it can reconstruct private face images with high success rate under various FedMD schemes. Both quantitative metrics and visual results are presented.

Some key differences from related work:

- Compared to gradient-based inversion attacks like DLG and iDLG, PLI is gradient-free and tailored for the FedMD setting.

- Unlike TBI which also uses an inversion network, PLI exploits the confidence gap between server and clients instead of relying on availability of private data.

- PLI meets the requirements of being knowledge-decoupling and gradient-free, unlike prior arts like MI-FACE, GAN attacks, etc.

In summary, this is the first study investigating and demonstrating the privacy risks of FedMD under malicious attacks. The novel PLI attack is designed specifically to breach FedMD's defense of only sharing public logits. The comprehensive results and analyses reveal previously unknown vulnerabilities in FedMD's privacy guarantees.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new Paired-Logits Inversion (PLI) attack that can successfully reconstruct private face images from only the public logits shared under federated learning with model distillation (FedMD), revealing previously unknown privacy risks even when gradients or model parameters are not exposed.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Investigating protections against the proposed PLI attack, such as differential privacy or homomorphic encryption. The authors mention these techniques in the conclusion as potential defenses against their attack. Further research could explore how to best apply them to FedMD frameworks.

- Adaptively choosing the hyperparameters γ and τ in the PLI attack based on estimating the quality of the prior data. The authors note this as a way to potentially improve attack performance.

- Extending the PLI attack to other learning tasks beyond image classification, such as natural language processing. The general attack methodology could likely be applied to other domains.

- Exploring other kinds of prior estimation beyond GANs and autoencoders. The authors used these techniques to generate priors in their experiments, but other methods could be studied as well.

- Analyzing the information leakage differences between gradients and logits more thoroughly, both theoretically and empirically. The authors provide some initial analysis but suggest more work could be done here.

- Designing more rigorous privacy accounting techniques for FedMD frameworks to better understand the privacy guarantees. The authors' work reveals vulnerabilities, but more precise accounting could help guide defense mechanisms.

- Developing and testing concrete defense methods like differential privacy and homomorphic encryption within FedMD systems. The authors propose these defenses but leave experimental validation for future work.

In summary, the core suggested directions are 1) defenses against the attack, 2) extensions of the attack to new domains and priors, and 3) deeper understanding of information leakage in FedMD. Advancing knowledge in these areas could help make FedMD more robust against privacy risks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel Paired-Logits Inversion (PLI) attack that can reconstruct private training data from the output logits of models trained under Federated Learning with Model Distillation (FedMD) and its variants. FedMD is a federated learning paradigm where only output logits from public datasets are shared between clients and server, instead of model parameters or gradients that may leak private information. The authors found that despite only sharing public logits, FedMD is still vulnerable to serious privacy threats. They observe a "confidence gap" between server and client models' predictions, where client models trained on private data are more confident in their predictions. The proposed PLI attack exploits this gap by training an inversion neural network on paired server-client logits from public data, then optimizes an analytical solution to estimate server-client logits for private data based on maximizing the confidence gap. Finally, the estimated logits are fed into the trained inversion network to reconstruct private images. Experiments on facial recognition datasets show the attack successfully recovers private faces under FedMD and its variants with high accuracy, demonstrating inherent privacy risks exist even when sharing public logits only.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new attack against Federated Learning with Model Distillation (FedMD) and its variants, which can reconstruct private training images from the output logits on public data. FedMD is a distributed learning framework where clients train on private and public data, and send output logits on the public dataset to the server for aggregation and distribution. It is considered safer than directly sharing gradients or parameters. However, the authors found FedMD is still vulnerable to privacy attacks. They designed a novel Paired-Logits Inversion (PLI) attack, where the attacker trains an inversion neural network using the confidence gap between server and client logits on public data. By optimizing this confidence gap, the attacker can estimate logits for private data, which are then fed into the inversion network to reconstruct private images. 

Experiments on facial recognition datasets show the proposed attack successfully recovers facial images under FedMD and similar schemes. For example, on LFW dataset, attack accuracy reaches 99.5% under FedMD with 10 clients. The reconstructed images accurately reflect distinct facial features of the private data. This reveals a previously unknown privacy risk in FedMD. The authors also provide thorough analysis on controlling the trade-off between attack accuracy and image quality. Overall, this work demonstrates sharing public logits still leaks substantial private information, urging new designs for secure federated learning algorithms.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a novel Paired-Logits Inversion (PLI) attack against federated learning frameworks that rely on sharing logits on public datasets, such as FedMD and its variants. The key ideas are:

The attacker trains an inversion neural network to reconstruct the original images from the paired logits (server model predictions and client model predictions on public data). This exploits the confidence gap between the server and client models - client models trained on private data are more confident, while server models only trained on public data are less confident.  

To estimate logits for private data, the attacker optimizes the paired logits to maximize a metric that captures this confidence gap. This results in optimal logits for private data that amplify the gap. The estimated logits are fed into the trained inversion network to reconstruct private images.

To improve image quality, the attacker also trains a prior estimation model using public data from a different domain (e.g. blurred images). This prior model regularizes the inversion network training and prevents unrealistic reconstructions.

Experiments on facial recognition datasets show the attack can successfully reconstruct private face images from public logits alone, outperforming prior arts. The method is gradient-free and fully decouples private data from public knowledge. Overall, the attack reveals vulnerabilities in sharing logits for FedMD-like frameworks, previously thought to be safe.


## What problem or question is the paper addressing?

 The paper is addressing the privacy risks in Federated Learning with Model Distillation (FedMD). Specifically, it investigates whether sharing output logits on public datasets in FedMD can still leak private information. 

The key questions the paper tries to answer are:

1) Can a malicious server in FedMD reconstruct a client's private data using only the output logits on public datasets? 

2) How can the server launch such an attack effectively?

3) How successful are such reconstruction attacks on breaching privacy in FedMD and its variants?

4) Compared to standard gradient-based attacks in federated learning, how severe is the privacy threat of the proposed logit-based attack?

In summary, the paper examines if FedMD and similar distillation-based federated learning schemes are truly secure against privacy attacks as commonly believed, by designing a new logit-inversion attack method and evaluating it extensively.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Federated Learning with Model Distillation (FedMD): A collaborative learning paradigm where clients send output logits on public datasets instead of model parameters to a central server. 

- Knowledge decoupling: The concept of decoupling private information from learned knowledge on public data only, and directly recovering private data.

- Paired-Logits Inversion (PLI) attack: The novel attack proposed in this paper that exploits the confidence gap between server and client logits on public data to reconstruct private data.

- Confidence gap: The observation that a client model trained on private data produces more confident predictions than a server model without access to that data. PLI exploits this gap.

- Gradient-free attack: PLI attack does not require access to gradients, making it suitable against FedMD schemes.

- Inversion neural network: A neural network trained by the adversary on public data to take paired logits as input and reconstruct the original images. 

- Prior estimation: Using insensitive public images to generate a prior regularization that prevents unrealistic reconstructed images.

- Knowledge decoupling and gradient-free principles: Key requirements for attacks against FedMD that PLI satisfies.

In summary, the key focus is on designing a novel inversion attack called PLI that can breach FedMD schemes by exploiting the confidence gap between server and client models on public data, without requiring model gradients.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the PLI attack exploit the confidence gap between the server and client models? Why is this gap useful for reconstructing private images?

2. Why is knowledge decoupling an important principle for the PLI attack? How does the attack ensure that private information is decoupled from public knowledge? 

3. Explain the analytical solution for obtaining the optimal input logits in Eq. 5. Why is it important to optimize the logits for the private data separately from the public data?

4. What is the purpose of using a prior estimation? How does the quality of the prior affect the image reconstruction and the choice of hyperparameters?

5. What is the motivation behind using paired logits as input to the inversion network? How does this differ from prior inversion attacks?

6. Walk through the steps of how the PLI attack recovers a private image of a target class. What are the key components and how do they fit together?

7. How does temperature tau control the tradeoff between image quality and attack accuracy? Provide examples of how different tau values affect the reconstructed images.

8. Compare and contrast the information leakage of gradients versus logits. Why can gradients potentially reveal more private information?

9. What are the limitations of the PLI attack? Are there any constraints on the type of datasets or models it can be applied to?

10. How might the PLI attack be extended to other collaborative learning frameworks besides FedMD? Would the core principles of the attack still apply?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new privacy attack called Paired-Logits Inversion (PLI) that can reconstruct private training data in Federated Learning with Model Distillation (FedMD) frameworks. FedMD is considered more secure than standard federated learning since it only shares logits of predictions on public data rather than model parameters or gradients. However, the authors show it is still vulnerable to privacy attacks. They observe a "confidence gap" between server and client models, where the client's predictions are more confident on private data. PLI exploits this by training an inversion neural network on paired server-client logits from public data. It then optimizes the confidence gap to estimate logits for private data, reconstructing private images by feeding estimated logits into the trained inversion network. Experiments on facial recognition datasets validate that PLI successfully reconstructs private images for FedMD and variants, using public logits only. This reveals previously unknown privacy risks in FedMD-like schemes. The proposed attack principles of being gradient-free and decoupling public knowledge could inspire more robust privacy protections for federated learning.


## Summarize the paper in one sentence.

 This paper proposes a novel Paired-Logits Inversion attack that can successfully reconstruct private images from the shared output logits on public datasets under FedMD and its variant federated learning frameworks.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points in this paper:

This paper reveals a privacy vulnerability in Federated Learning with Model Distillation (FedMD) schemes, where clients share output logits computed on public data instead of model parameters or gradients. The authors propose a novel Paired-Logits Inversion (PLI) attack where a malicious server trains an inversion network using paired logits from an auxiliary server model and client models on public data. By exploiting the confidence gap between the server and client logits, the attack optimizes simulated logits to reconstruct private data with the trained inversion network. Experiments on facial recognition benchmarks demonstrate successful reconstruction of private images across different FedMD schemes, using public logits only. The results indicate sharing output logits still poses privacy risks, and more work is needed to secure federated learning frameworks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a novel Paired-Logits Inversion (PLI) attack against FedMD frameworks. How is the proposed attack fundamentally different from prior inversion attacks like MI-FACE, TBI, DLG, etc. in terms of being knowledge-decoupled and gradient-free?

2. The core idea behind PLI attack is exploiting the confidence gap between the server and client models. Elaborate on how this confidence gap manifests and how it is mathematically formulated in Eq. 5 in the paper.  

3. The paper claims logit inversion is more challenging than gradient inversion. Can you explain the intuition behind this claim and how Proposition 1 theoretically proves this?

4. Explain the complete training process of the inversion neural network in PLI attack. What is the role of the public dataset $D_0$, auxiliary domain dataset $D_a$ and prior estimation in training this network?

5. How does the analytical solution to the confidence gap optimization problem result in optimal logits that maximize the similarity metric Q? Walk through the mathematical derivations in Appendix A.

6. Discuss the effect of the hyperparameters - temperature τ and prior weight γ on controlling the tradeoff between attack accuracy and image quality in PLI attack.

7. The paper evaluates PLI attack on three different federated learning frameworks - FedMD, DS-FL and FedGEMS. Compare and contrast the attack results across these frameworks. What inferences can you draw?

8. How does the proposed attack deal with selecting the best reconstructed image when there are multiple clients? Explain the motivation behind using Eq. 12 as the selection criterion. 

9. Analyze the limitations of the PLI attack method. In what ways can the attack strategy be further improved?

10. The paper demonstrates attacking facial recognition tasks. Discuss how you may extend the PLI attack strategy to other domains like text, speech, medical imaging, etc. What challenges do you foresee?
