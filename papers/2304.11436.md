# [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://arxiv.org/abs/2304.11436)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It investigates the privacy risks of Federated Learning with Model Distillation (FedMD), which is a collaborative learning approach where clients share output logits on public datasets instead of model parameters or gradients. 

- The paper reveals that FedMD is vulnerable to a malicious attack called Paired-Logits Inversion (PLI) attack, where the attacker can reconstruct private images from the shared public logits. 

- The authors propose a novel PLI attack method that exploits the "confidence gap" between the server and client models' predictions on public data to estimate the logits for private data.

- Through this confidence gap, the attack is able to decouple private information from public knowledge and directly recover private images in a gradient-free manner.

- Experiments on facial recognition tasks demonstrate that the PLI attack can successfully reconstruct private face images on various datasets under FedMD and its variants with high attack success rate.

In summary, the key hypothesis is that FedMD is not as privacy-preserving as previously thought, and private images can be reconstructed from public logits through the proposed PLI attack. The paper reveals a new privacy risk in FedMD frameworks.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

1. Revealing a previously unknown privacy vulnerability in federated learning with model distillation (FedMD) and related schemes. The authors show that a malicious server can reconstruct private training images using only publicly available model outputs (logits). 

2. Proposing a novel "paired logits inversion" (PLI) attack that exploits confidence gaps between server and client models to reconstruct private data without access to gradients. The attack trains an inversion network on public data and optimizes logits to reconstruct private images.

3. Providing thorough quantitative and qualitative evaluation of the proposed attack on facial recognition tasks. Experiments on multiple datasets demonstrate successful reconstruction of private images under FedMD and related schemes with high accuracy.

In summary, this paper identifies and demonstrates a serious privacy threat in federated learning frameworks that distill public knowledge like FedMD. The authors design a new gradient-free attack that can reliably recover private images using only public outputs, highlighting the need for improved defense mechanisms. The proposed PLI attack and analysis of privacy risks under distillation-based federated learning are the main contributions.


## How does this paper compare to other research in the same field?

 This paper presents a new attack against federated learning with model distillation (FedMD) and its variants. The key contributions are:

1. It reveals a previously unknown privacy vulnerability in FedMD-like schemes. Prior work considered FedMD to be secure against model inversion attacks since it only shares output logits on public datasets instead of model parameters or gradients which are more susceptible to inversion attacks. However, this paper shows that carefully designed attacks can still reconstruct private data from the public logits.

2. It proposes a novel "paired logits inversion" (PLI) attack that is gradient-free and exploits the confidence gap between server and client models. Most prior inversion attacks rely on gradients or model parameters, so they cannot work under FedMD's threat model. In contrast, PLI is the first attack tailored for FedMD that recovers private data using only public logits.

3. It provides comprehensive experiments on facial recognition tasks to demonstrate the efficacy of PLI, showing it can reconstruct private face images with high success rate under various FedMD schemes. Both quantitative metrics and visual results are presented.

Some key differences from related work:

- Compared to gradient-based inversion attacks like DLG and iDLG, PLI is gradient-free and tailored for the FedMD setting.

- Unlike TBI which also uses an inversion network, PLI exploits the confidence gap between server and clients instead of relying on availability of private data.

- PLI meets the requirements of being knowledge-decoupling and gradient-free, unlike prior arts like MI-FACE, GAN attacks, etc.

In summary, this is the first study investigating and demonstrating the privacy risks of FedMD under malicious attacks. The novel PLI attack is designed specifically to breach FedMD's defense of only sharing public logits. The comprehensive results and analyses reveal previously unknown vulnerabilities in FedMD's privacy guarantees.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new Paired-Logits Inversion (PLI) attack that can successfully reconstruct private face images from only the public logits shared under federated learning with model distillation (FedMD), revealing previously unknown privacy risks even when gradients or model parameters are not exposed.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Investigating protections against the proposed PLI attack, such as differential privacy or homomorphic encryption. The authors mention these techniques in the conclusion as potential defenses against their attack. Further research could explore how to best apply them to FedMD frameworks.

- Adaptively choosing the hyperparameters γ and τ in the PLI attack based on estimating the quality of the prior data. The authors note this as a way to potentially improve attack performance.

- Extending the PLI attack to other learning tasks beyond image classification, such as natural language processing. The general attack methodology could likely be applied to other domains.

- Exploring other kinds of prior estimation beyond GANs and autoencoders. The authors used these techniques to generate priors in their experiments, but other methods could be studied as well.

- Analyzing the information leakage differences between gradients and logits more thoroughly, both theoretically and empirically. The authors provide some initial analysis but suggest more work could be done here.

- Designing more rigorous privacy accounting techniques for FedMD frameworks to better understand the privacy guarantees. The authors' work reveals vulnerabilities, but more precise accounting could help guide defense mechanisms.

- Developing and testing concrete defense methods like differential privacy and homomorphic encryption within FedMD systems. The authors propose these defenses but leave experimental validation for future work.

In summary, the core suggested directions are 1) defenses against the attack, 2) extensions of the attack to new domains and priors, and 3) deeper understanding of information leakage in FedMD. Advancing knowledge in these areas could help make FedMD more robust against privacy risks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel Paired-Logits Inversion (PLI) attack that can reconstruct private training data from the output logits of models trained under Federated Learning with Model Distillation (FedMD) and its variants. FedMD is a federated learning paradigm where only output logits from public datasets are shared between clients and server, instead of model parameters or gradients that may leak private information. The authors found that despite only sharing public logits, FedMD is still vulnerable to serious privacy threats. They observe a "confidence gap" between server and client models' predictions, where client models trained on private data are more confident in their predictions. The proposed PLI attack exploits this gap by training an inversion neural network on paired server-client logits from public data, then optimizes an analytical solution to estimate server-client logits for private data based on maximizing the confidence gap. Finally, the estimated logits are fed into the trained inversion network to reconstruct private images. Experiments on facial recognition datasets show the attack successfully recovers private faces under FedMD and its variants with high accuracy, demonstrating inherent privacy risks exist even when sharing public logits only.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new attack against Federated Learning with Model Distillation (FedMD) and its variants, which can reconstruct private training images from the output logits on public data. FedMD is a distributed learning framework where clients train on private and public data, and send output logits on the public dataset to the server for aggregation and distribution. It is considered safer than directly sharing gradients or parameters. However, the authors found FedMD is still vulnerable to privacy attacks. They designed a novel Paired-Logits Inversion (PLI) attack, where the attacker trains an inversion neural network using the confidence gap between server and client logits on public data. By optimizing this confidence gap, the attacker can estimate logits for private data, which are then fed into the inversion network to reconstruct private images. 

Experiments on facial recognition datasets show the proposed attack successfully recovers facial images under FedMD and similar schemes. For example, on LFW dataset, attack accuracy reaches 99.5% under FedMD with 10 clients. The reconstructed images accurately reflect distinct facial features of the private data. This reveals a previously unknown privacy risk in FedMD. The authors also provide thorough analysis on controlling the trade-off between attack accuracy and image quality. Overall, this work demonstrates sharing public logits still leaks substantial private information, urging new designs for secure federated learning algorithms.
