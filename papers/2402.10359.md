# [Can we soft prompt LLMs for graph learning tasks?](https://arxiv.org/abs/2402.10359)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper introduces GraphPrompter, a novel framework to enhance the ability of Large Language Models (LLMs) to interpret graph-structured data through soft prompting strategies. 

Problem: Directly applying LLMs to graph data is challenging due to the discrepancy between graph and text modalities. Existing methods that map graphs to text may lose crucial structural information. GraphPrompter aims to address this limitation.

Proposed Solution: GraphPrompter consists of two components - a Graph Neural Network (GNN) and an LLM. The GNN encodes a node's local neighborhood to generate a node embedding capturing topological features. This embedding is projected and concatenated with the node's text embedding from the LLM to create a soft prompt. The LLM then makes predictions based on this fused representation.  

Key Insights: 
- GNNs effectively aggregate neighborhood information into node embeddings.
- LLMs interpret text embeddings using their vast pre-trained knowledge.
- Fusing both sources of knowledge (graph structure and text semantics) enhances the LLM's graph comprehension.

Main Contributions:
- First work investigating whether LLMs can understand graph learning tasks via soft prompting
- Introduction of GraphPrompter, a novel framework to align graph and text knowledge to prompt LLMs
- Extensive experiments demonstrating superior performance over baselines on node classification and link prediction across benchmark datasets

The paper shows GraphPrompter can successfully prompt LLMs for graph learning. This enables new opportunities to apply LLMs for complex data beyond just text.
