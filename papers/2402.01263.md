# [A Differentiable Partially Observable Generalized Linear Model with   Forward-Backward Message Passing](https://arxiv.org/abs/2402.01263)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement:
The partially observable generalized linear model (POGLM) is a powerful probabilistic model for inferring neural connectivity and interactions between observed (visible) neurons and unobserved (hidden) neurons using neural spike train data recorded from the visible neurons. However, existing variational inference methods for learning POGLM have two main limitations:

(1) The discrete Poisson distribution used to model hidden spike counts makes it difficult to apply the low-variance pathwise gradient estimator when optimizing the variational objective. Only the high-variance score function estimator can be used.

(2) The design of the variational distribution (which approximates the intractable posterior distribution over hidden spikes) lacks expressiveness and efficiency. Specifically, existing methods fail to account for the influence of future visible spikes on current hidden spikes.

Proposed Solutions:

(1) A differentiable POGLM is proposed where the discrete Poisson distribution over hidden spikes is relaxed to a categorical distribution or continuous distributions like exponential that enable pathwise gradients.

(2) A new forward-backward message passing scheme is used for the variational distribution that captures dependencies of hidden spikes on both past and future visible spikes. This results in a more accurate posterior approximation.

Main Contributions:

- Formulation of a differentiable POGLM using relaxations like Gumbel-softmax that facilitate low-variance pathwise gradient estimation.

- Introduction of a forward-backward message passing sampling scheme for the variational distribution to better capture hidden-visible dependencies.

- Extensive experiments on synthetic and real neural data demonstrating superior performance of the proposed differentiable POGLM with forward-backward variational distribution over existing methods.

- Analysis showing the differentiable POGLM yields more interpretable recovered model parameters, which is significant for neuroscience applications.

Overall, the paper presents major advances in developing an enhanced variational inference framework for POGLMs that enables more effective analysis of neural connectivity from limited observable spike train data.
