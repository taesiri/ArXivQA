# [Shape-aware Text-driven Layered Video Editing](https://arxiv.org/abs/2301.13173)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract, the key research focus of this paper is on enabling shape-aware, text-driven video editing to overcome limitations of existing video editing methods. Specifically:

- Existing methods using layered video representation can consistently edit object appearance, but are limited to fixed UV mappings so cannot edit object shapes. 

- The paper presents a new method to achieve consistent video editing with changes to both object shape and appearance, guided by text prompts.

The central hypothesis seems to be that by propagating deformation fields between input and edited keyframes to all frames, and using guidance from pre-trained text-conditioned diffusion models, their method can achieve shape-aware video editing with temporal consistency.

The key research questions/goals addressed are:

- How to enable shape changes in addition to appearance editing for consistent video editing?

- How to propagate target shape edits from a keyframe to all frames while preserving original object motions?

- How to handle unseen regions and shape distortions when propagating edits using a single edited keyframe?

In summary, the central research focus is on achieving shape-aware, text-guided video editing in a temporally consistent manner, which existing methods fail to accomplish. The core hypothesis is that deformation propagation and diffusion model guidance can help overcome current limitations.
