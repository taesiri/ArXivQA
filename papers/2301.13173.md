# [Shape-aware Text-driven Layered Video Editing](https://arxiv.org/abs/2301.13173)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract, the key research focus of this paper is on enabling shape-aware, text-driven video editing to overcome limitations of existing video editing methods. Specifically:

- Existing methods using layered video representation can consistently edit object appearance, but are limited to fixed UV mappings so cannot edit object shapes. 

- The paper presents a new method to achieve consistent video editing with changes to both object shape and appearance, guided by text prompts.

The central hypothesis seems to be that by propagating deformation fields between input and edited keyframes to all frames, and using guidance from pre-trained text-conditioned diffusion models, their method can achieve shape-aware video editing with temporal consistency.

The key research questions/goals addressed are:

- How to enable shape changes in addition to appearance editing for consistent video editing?

- How to propagate target shape edits from a keyframe to all frames while preserving original object motions?

- How to handle unseen regions and shape distortions when propagating edits using a single edited keyframe?

In summary, the central research focus is on achieving shape-aware, text-guided video editing in a temporally consistent manner, which existing methods fail to accomplish. The core hypothesis is that deformation propagation and diffusion model guidance can help overcome current limitations.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is presenting a shape-aware, text-driven video editing method that can handle both appearance and shape changes in videos. 

The key points are:

- Existing video editing methods like Neural Layered Atlas (NLA) and Text2LIVE can only edit the appearance of objects in videos, but cannot change the object shapes due to using a fixed UV mapping field. 

- This paper proposes a deformation formulation to transform the UV mapping field according to the edited shape. Specifically, it estimates a dense semantic correspondence between the input and edited keyframes to capture the shape deformation. This deformation is then propagated to all frames through the UV mapping.

- To refine distortions and complete unseen regions, the edited atlas texture and deformation parameters are further optimized using a pre-trained text-conditioned diffusion model. 

- Through this shape-aware deformation formulation and atlas optimization, the method can achieve consistent video editing with both appearance and shape changes corresponding to the text prompt.

- The experiments demonstrate that the proposed approach outperforms existing methods and baselines in editing videos with shape changes, while maintaining temporal consistency.

In summary, the main contribution is enabling shape-aware, text-driven video editing by presenting techniques to deform the UV mapping and optimize the atlas guided by semantic correspondence and diffusion models. This significantly improves upon prior arts that are restricted to appearance editing only.
