# [Regularity as Intrinsic Reward for Free Play](https://arxiv.org/abs/2312.01473)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Standard intrinsic rewards in model-based reinforcement learning, such as novelty seeking, tend to favor chaotic dynamics like flipping and throwing objects. This leads to an exploration strategy that overlooks important regularities like symmetries, stability, and general structure. However, regularity is a crucial inductive bias, as it allows efficient interaction with the world and is actively sought out by humans, especially in play behavior. The paper argues that incorporating a regularity-seeking objective alongside novelty can guide exploration towards more diverse and meaningful behavior.

Method:
The paper proposes Regularity as Intrinsic Reward (RaIR). Regularity is quantified as low entropy or high redundancy in the description of a scene. This is operationalized by mapping the state to a multiset of symbols and computing the entropy over the empirical distribution of symbols. Both a direct version using absolute properties and a relational version between entity pairs are described. The method is incorporated into free play by combining RaIR with ensemble disagreement as novelty signal. 

Contributions:
- Formalizes regularity as entropy minimization over state descriptions, enables optimization via planning
- Studies emerging patterns when optimizing RaIR with ground truth models
- Injects regularity objective into free play alongside novelty seeking 
- Achieves autonomous construction of towers and symmetric patterns
- Demonstrates improved exploration and downstream performance on assembly tasks like stacking

The method is evaluated in a 2D shape world and a robotic block manipulation environment. When optimizing RaIR with ground truth models, various symmetric patterns emerge like stacks, grids, and triangles. In free play, the combination of RaIR and novelty produces more interactions between objects, lifts, and alignments. After learning, the method substantially outperforms baselines in zero-shot generalization to assembly tasks while retaining performance in chaotic behaviors.


## Summarize the paper in one sentence.

 This paper proposes using regularity (low entropy states with high redundancy) as an intrinsic reward signal to guide reinforcement learning agents towards structured behaviors during free play, and shows it can lead to improved performance on downstream assembly tasks.


## What is the main contribution of this paper?

 This paper proposes using regularity as an intrinsic reward signal for reinforcement learning agents. Specifically, it introduces Regularity as Intrinsic Reward (RaIR), which quantifies regularity by measuring the redundancy or compressibility of the state description. The key ideas are:

1) RaIR operationalizes the concept of regularity from developmental psychology as an intrinsic reward for RL agents. This helps guide exploration towards more structured and symmetric states during free play.

2) RaIR is incorporated into model-based RL, using an ensemble of graph neural networks as world models. It is combined with ensemble disagreement to balance regularity-seeking and novelty-seeking.

3) Experiments in a 2D grid world and a robotic block manipulation environment demonstrate the emergence of diverse regular patterns when optimizing RaIR. Using RaIR to augment free play leads to improved performance on downstream assembly tasks like stacking.

4) The formulation of RaIR is general and can be computed between object states or any symbols describing the environment state. Different variants exhibit invariance to certain symmetries and can target different types of regularities.

In summary, the main contribution is the proposal and evaluation of RaIR as a way to bias exploration towards more structured behaviors, taking inspiration from child development. This improves sample efficiency and performance on related downstream tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, some of the key terms and concepts associated with this paper include:

- Regularity as intrinsic reward (RaIR)
- Symmetry
- Free play
- Child development
- Model-based reinforcement learning
- Exploration
- Curiosity
- Information gain
- Ensemble disagreement
- Structured world models
- Graph neural networks
- Zero-shot generalization
- Downstream tasks
- Assembly tasks (e.g. stacking)

The main focus of the paper seems to be using regularity as a novel intrinsic reward signal to guide reinforcement learning agents to explore orderly and symmetric states and behaviors. This is motivated by child development research showing children's preference for order and patterns during free play. The method is implemented using model-based RL with graph network models, and tested on synthetic environments as well as a robotic block stacking domain. A key result is that combining regularity objectives with novelty-seeking intrinsic rewards leads to improved performance on downstream assembly tasks solved in a zero-shot manner after the free play phase.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using regularity as an intrinsic reward signal to guide reinforcement learning agents towards structured exploration. However, quantifying "regularity" itself seems non-trivial. How exactly is regularity quantified in this work and what design choices were made? For example, how is the mapping $\Phi$ from states to symbols defined?

2. The paper shows emergent patterns from optimizing their proposed regularity metric in both a 2D grid world and a robotic block manipulation environment. However, what prevents the method from just repeatedly constructing the same "optimal" regular structure over and over? Does the combination with an ensemble disagreement based novelty metric help drive continued exploration?

3. The paper combines the proposed regularity metric with ensemble disagreement to balance regularity-seeking and novelty-seeking behaviors during free play. However, how sensitive is performance to the weighting term λ between these two objectives as shown in Equation 4? Is there a principled way to set this hyperparameter?

4. For the relational formulation of the proposed regularity metric, what informed the choice of using pairwise relations between entities/objects? How does considering higher-order relationships affect the types of regularities discovered during exploration?

5. The paper shows qualitative results that their method can recreate existing regular structures in a scene by simply optimizing the proposed regularity reward. Could the same approach also work for mimicking more complex temporally extended patterns of behavior just by observing them?

6. The paper evaluates the proposed approach on robotic block manipulation tasks and shows improved performance on construction tasks requiring stability. However, how does the approach compare to other methods on less structured tasks like throwing or rolling blocks? Does combining regularity and novelty seeking lead to a balanced skill set?

7. The paper argues that optimizing for regularity guides exploration in RL towards structured behaviors that are common in child’s play. However, the environments considered are still relatively simple. How well would the approach scale to more complex real world environments? Would the same inductive bias still be as useful?

8. The paper computes regularity based on state representations that are object/entity centric. How does performance depend on the quality of the underlying state representation? Would the method work as well with only raw pixel observations instead of clean object states?

9. The paper combines regularity seeking with novelty seeking, arguing that neither pure regularity nor pure novelty are enough. However, are there other complementary intrinsic rewards that could be combined with regularity seeking to shape exploration?

10. The results show that regularity shaped exploration during a free play phase leads to better generalization on downstream construction tasks. However, does the improved performance indicate that the model has learned more broadly generalizable knowledge about stability and structures or has it just specialized to the particular construction tasks evaluated?
