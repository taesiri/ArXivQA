# Toward Grounded Social Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can robots perform grounded social reasoning to act in a socially appropriate manner in real-world environments, when they lack full information about the state of objects in the scene?The key hypotheses appear to be:1) Large language models (LLMs) have the capacity for social reasoning, but they need to be grounded in perception of the real environment.2) Passively querying vision-language models (VLMs) is often insufficient, as real-world scenes may be cluttered or occluded.3) Robots can actively gather missing information by asking clarifying questions about the scene and obtaining new viewpoints/images of objects through embodied perception. 4) This active information gathering will significantly improve the grounded social reasoning capabilities of robots equipped with LLMs and VLMs, allowing them to make better socially appropriate decisions about manipulating real-world objects and scenes.The central goal seems to be developing and evaluating a framework that enables robots to actively perceive the environment in order to gather the information needed to make socially appropriate decisions, as humans are able to do through commonsense reasoning. The key insight is that both active questioning and active perception are needed to obtain the necessary contextual details.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Proposing a framework to enable robots to perform "grounded social reasoning" by combining large language models (LLMs) and vision-language models (VLMs). The key insight is that robots need to actively gather information from the environment in order to make socially appropriate decisions, rather than just passively querying the LLM and VLM models.2. Releasing a new dataset called MessySurfaces that contains images of 70 real-world surfaces/desks that need to be cleaned. The dataset has over 300 objects annotated with multiple choice questions about the appropriate way to clean each object. This serves as a benchmark for evaluating grounded social reasoning.3. Demonstrating their approach on the MessySurfaces benchmark dataset as well as with robot experiments on real surfaces. They show improvements over baselines that do not employ active perception.4. Analyzing the different components of their framework - generating good follow up questions, choosing informative close-up angles, answering questions using the VLM. This provides an in-depth evaluation.5. Showing preliminary experiments that incorporating personal preferences on top of their framework can further improve performance.In summary, the key contribution appears to be proposing and evaluating a method that combines LLMs and VLMs in a novel way to enable robots to actively gather information and perform grounded social reasoning, with minimal human intervention. The release of the MessySurfaces dataset also provides a way to benchmark progress in this area.
