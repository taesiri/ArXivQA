# [Space-Time Correspondence as a Contrastive Random Walk](https://arxiv.org/abs/2006.14613)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is how to learn visual representations for establishing temporal correspondence in videos in a self-supervised manner, without requiring manually annotated ground truth data. Specifically, the paper proposes a method to learn features such that temporal correspondences are represented by strong connections in a space-time graph constructed from the video. The nodes in this graph are image patches sampled from each frame, and edges represent affinities between patches in adjacent frames based on feature similarity. The main idea is to learn the feature representation by training the model to perform random walks on this graph that follow paths of high visual similarity, using only the raw video data itself as supervision.The key hypotheses are:- Temporal correspondence in video can be modeled as path finding on a space-time similarity graph, where transition probabilities of a random walk depend on learned feature similarity.- The feature representation can be learned in a self-supervised manner by using cycle consistency, where the objective is for walks on a "palindrome" sequence to return to their starting node. This provides implicit supervision for intermediate correspondences along the walk.- Modeling correspondence as a soft attention random walk allows considering many possible paths to handle ambiguity, and provides a dense learning signal from all patches in the video.In summary, the main research question is self-supervised learning of representations for temporal correspondence, which is addressed through the idea of modeling video as a graph and supervising representation learning through consistency of random walks.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a self-supervised approach for learning a visual representation from raw video for spatial-temporal correspondence. The key ideas are:- Representing video as a space-time graph, where nodes are image patches and edges connect nodes in neighboring frames based on visual similarity. - Formulating correspondence as performing a random walk on this graph. The transition probabilities of the random walk are determined by the similarity between nodes under a learned representation.- Learning the representation by fitting the random walk probabilities, so that there is high probability of returning to the start node when walking along the graph constructed from a "palindrome" video sequence.- Using this cycle consistency objective to provide implicit supervision for intermediate correspondences in the walk, without needing ground truth labels. - Showing this simple approach, which allows jointly learning from all patches in the video in a self-supervised manner, leads to a representation that outperforms prior self-supervised methods on propagation tasks for objects, parts and keypoints.In summary, the key contribution is a simple yet effective graph-based framework for learning visual correspondence through self-supervised random walks on video, using cycle consistency as the supervisory signal. The representation learned with this approach transfers well to various correspondence tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a self-supervised approach for learning a visual representation for correspondence in videos by modeling video as a space-time graph and learning to predict links between nodes with a contrastive random walk objective.
