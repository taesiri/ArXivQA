# [Neural Volumetric Memory for Visual Locomotion Control](https://arxiv.org/abs/2304.01201)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:- How can we enable legged robots to traverse complex terrains using only visual observations from an onboard camera? The paper focuses on the problem of locomotion using only vision, without other sensors providing detailed terrain information.- Can introducing explicit geometric priors and structure into the learning process lead to better visuomotor policies for locomotion? The authors propose a neural volumetric memory architecture that aggregates observations while accounting for camera pose changes, with the goal of encouraging equivariance and leading to better policies.- Does representing the surrounding scene geometry explicitly as a structured 3D volumetric memory provide benefits over simpler baselines like CNN feature extraction from images? The paper compares the proposed method against baselines without explicit geometric modeling.- Can a policy trained purely in simulation using the proposed architecture transfer successfully to a physical robot in complex real-world environments? The authors demonstrate sim-to-real transfer by deploying the trained policy on a physical quadrupedal robot.So in summary, the key hypotheses appear to revolve around using explicit geometric modeling and volumetric scene representations to enable visuomotor control for legged locomotion, with a focus on achieving sim-to-real transfer. The comparisons against baselines aim to validate the benefits of the proposed neural volumetric memory architecture.


## What is the main contribution of this paper?

This paper proposes a novel neural volumetric memory (NVM) architecture for legged locomotion control using a forward-facing depth camera. The key ideas are:- NVM takes a sequence of depth images as input and aggregates them into a 3D feature volume representing the surrounding scene. - It explicitly models the SE(3) equivariance of the 3D world by estimating relative camera poses between frames and transforming/aligning the 3D features accordingly before aggregating. This allows integrating observations from different viewpoints into a coherent scene representation.- The model is trained with a two-stage pipeline. First a teacher policy is trained using privileged information like elevation maps. Then the NVM module and a visual policy are trained to mimic the teacher using only images, via behavior cloning and an additional self-supervised view reconstruction loss.- Experiments in simulated and real-world environments with stairs, gaps, and obstacles demonstrate that explicitly modeling 3D structure and equivariance enables superior visual locomotion compared to baselines. The method generalizes successfully from simulation to the real world.In summary, the key contribution is proposing the neural volumetric memory architecture that leverages 3D geometry and equivariance principles for learning visuomotor policies that succeed on complex terrains using only monocular images. The results highlight the importance of 3D reasoning for visual locomotion.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a neural volumetric memory (NVM) architecture that aggregates sequential visual observations from a depth camera into an equivariant 3D feature volume to enable a legged robot to traverse complex environments like stairs and gaps; experiments in simulation and on a real robot show NVM outperforms baselines without explicit 3D structure modeling.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this CVPR 2023 paper compares to other related research:- The main contribution is proposing a novel neural volumetric memory (NVM) architecture for legged robot locomotion that better captures the 3D structure of the environment from ego-centric camera views. This builds on prior work in visuomotor control for legged robots by incorporating more explicit 3D reasoning.- Most prior visuomotor control methods for legged robots use some form of frame stacking or recurrent networks over 2D image features. This paper argues these approaches don't adequately capture the 3D structure and equivariance of the environment. The NVM module is designed to address this.- The idea of aggregating 3D feature volumes and using estimated camera pose transformations seems inspired by recent self-supervised 3D scene representation learning methods in computer vision like NeRF and neural volumes. The application to robot locomotion control is novel.- The two-stage training procedure of first training an elevation map policy then distilling to a visual policy is similar to other recent visuomotor learning papers. The main novelty is in the proposed NVM architecture used in the second stage.- Compared to blind locomotion policies, the inclusion of vision substantially expands the capabilities and robustness of the robot, especially for precise foothold and terrain reasoning. This is demonstrated through comparisons in complex simulated environments.- The real-world robot experiments help validate the effectiveness of NVM for sim-to-real transfer. Performance gains over baseline vision-based methods are shown across diverse terrain types.Overall, the paper makes nice connections between 3D scene understanding in computer vision and visuomotor control for legged robots. The proposed volumetric memory offers improvements over prior image-based methods by better handling 3D structure. The results demonstrate more capable real-world terrain locomotion.
