# [No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based   Language Models](https://arxiv.org/abs/2312.09494)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models":

Problem:
- Skimming-based language models dynamically drop unimportant tokens in input sequences to reduce computation cost and energy consumption. However, the paper reveals for the first time that such models may be vulnerable to Denial-of-Service (DoS) attacks. 
- Specifically, adversarial inputs could be crafted to increase the ratio of remaining tokens, thus increasing computation complexity and deteriorating efficiency. This poses serious challenges for deploying skimming models on real-time services or resource-constrained edge devices.

Proposed Solution:
- The paper proposes "No-Skim", a general framework to evaluate the efficiency robustness of skimming models by generating adversarial inputs.
- The framework has 3 main steps: (1) Identify the most important word using gradient or mask-based ranking. (2) Generate candidate perturbations at word/character level. (3) Search for the best perturbation that maximizes remaining token ratio.
- The framework is modular and works under white-box, gray-box and black-box settings by using different techniques to approximate or increase the remaining token ratio.

Main Contributions:
- First work to systematically study potential efficiency vulnerability of skimming language models. 
- Propose an effective and general evaluation framework "No-Skim" to generate adversarial inputs that increase computation complexity.
- Modular design makes the framework adaptable to different practical scenarios and knowledge levels about the target model.
- Extensive evaluations on BERT and RoBERTa with state-of-the-art skimming scheme show the framework can increase computation cost by 145% in the worst case.


## Summarize the paper in one sentence.

 This paper proposes No-Skim, a framework to evaluate the efficiency robustness of skimming-based language models by generating adversarial inputs that increase the models' computation cost.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Proposing "No Skim", the first general framework to evaluate the efficiency robustness of skimming-based language models. 

2) The framework generates adversarial inputs that increase the computation complexity and pose serious challenges to deploying skimming models.

3) The framework is modular and extensible to different components, making it adaptable to evaluations under different levels of knowledge and access. 

4) Conducting extensive evaluations on state-of-the-art skimming schemes like Transkimmer with BERT and RoBERTa on GLUE. The framework increased computation cost by up to 145% in the worst case.

So in summary, the main contribution is proposing a general evaluation framework called "No Skim" to test the efficiency robustness of skimming-based language models by generating adversarial inputs. The framework is the first of its kind, extensible, and proved effective in increasing computation costs substantially during evaluations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Skimming-based language models - The paper focuses on evaluating the efficiency robustness of skimming-based language models, which dynamically drop unimportant tokens to reduce computation.

- Efficiency robustness - The paper proposes a framework called "No Skim" to evaluate the efficiency robustness of skimming models against adversarial attacks.

- Adversarial attacks - The goal is to generate adversarial inputs that increase the remaining token ratio to deteriorate the efficiency of skimming models.

- Remaining token ratio - A key metric that measures the computation complexity and serves as the attack objective to maximize. 

- Modular evaluation framework - The "No Skim" framework is modular and extensible to different scenarios based on the knowledge and access to the target skimming model.

- White-box, gray-box, black-box evaluation - The framework supports evaluation under different levels of access to the target skimming model.

- Gradient-based importance score - A technique used under white-box setting to identify the most efficient word to perturb. 

- Inference time approximation - A side channel approach under black-box setting to estimate remaining token ratio.

In summary, the key focus is on evaluating and demonstrating potential efficiency vulnerabilities of skimming-based language models using adversarial attacks under different practical scenarios.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a general framework called "No Skim" to evaluate the efficiency robustness of skimming-based language models. Can you elaborate on the key components and steps involved in this framework? What makes it generalizable?

2. One main contribution mentioned is the modularity of the evaluation framework to support different plug-in components. Can you discuss some examples of the plug-in components that could be used at different steps like word importance ranking, candidate set generation, etc?  

3. The paper categorizes the evaluation scenarios into white-box, gray-box and black-box access. Can you explain the differences in assumptions made across these scenarios and how the methodology adapts to them?

4. For the word importance ranking step, both gradient-based and mask-based approaches are discussed. What are the relative advantages and disadvantages of these two approaches? When would you use one over the other?

5. The candidate set generation uses both word-level and character-level perturbations. What is the intuition behind these two types of perturbations? Can you discuss scenarios where one would be preferred over the other?

6. In the best candidate searching step, direct efficiency indicators like remaining token ratio are used in white-box and gray-box scenarios. However, black-box uses a side-channel approach with token-level inference time. Elaborate on why this was required and how it approximates the remaining token ratio.

7. The results show higher vulnerability in BERT compared to RoBERTa. What intrinsic differences between these two model architectures could explain this? How can this inform future designs?

8. The evaluation results use metrics like Average Remaining Ratio (ARR) and Cumulative Remaining Ratio (CRR) to measure efficiency degradation. Can you explain what these metrics represent and their relevance?

9. The paper discusses potential challenges this attack surface introduces when deploying skimming models in real-time services and edge devices. Can you expand on the specific issues and how the increasing computation complexity impacts them?

10. The paper focuses only on evaluating efficiency robustness of skimming models. Can you discuss other potential vulnerabilities in these models? What new attack strategies could be explored as future work?
