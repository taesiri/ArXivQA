# [Picture that Sketch: Photorealistic Image Generation from Abstract   Sketches](https://arxiv.org/abs/2303.11162)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How can we generate photorealistic images from abstract human sketches? 

The key points about the research question are:

- The goal is to generate photorealistic images from sketches. So the task is sketch-to-image generation.

- The sketches are "abstract human sketches" rather than clean line drawings or edge maps. So the sketches are hand-drawn by amateurs and can be very abstract. 

- Existing methods fail to generate good results from such abstract sketches. The paper aims to address this limitation.

In summary, the main research question is how to bridge the abstraction gap and generate photorealistic images even from highly abstract free-hand sketches made by amateurs. The paper proposes a new model architecture and training process to achieve this.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a novel method for photorealistic image generation from abstract freehand sketches. The key ideas and innovations are:

- A decoupled encoder-decoder training paradigm, where the decoder is a pre-trained frozen StyleGAN that ensures photorealistic outputs. The encoder is trained separately to map sketches to StyleGAN's latent space.

- An autoregressive sketch mapper that handles the varying abstraction levels in sketches by predicting only a few latent codes for abstract sketches and more codes for detailed sketches. 

- A fine-grained discriminative loss based on a pre-trained sketch-photo retrieval model that captures the fine details in the sketch better than just reconstruction loss.

- A partial-aware sketch augmentation strategy to make the model robust to incomplete and sparse sketches.

Overall, the main contribution is developing an end-to-end model tailored for abstract sketch to photorealistic image generation by handling challenges like abstraction, sparsity, deformations in sketches through innovative designs like the autoregressive mapper, fine-grained loss etc. The results showcase the model's superior performance compared to previous arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes a novel autoregressive sketch-to-photo generation model that leverages a pre-trained StyleGAN to produce photorealistic images from abstract free-hand sketches, enabling fine-grained control over the output by predicting latent vectors in a sequential manner based on the input sketch detail.
