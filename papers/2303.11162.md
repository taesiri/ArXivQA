# [Picture that Sketch: Photorealistic Image Generation from Abstract   Sketches](https://arxiv.org/abs/2303.11162)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How can we generate photorealistic images from abstract human sketches? 

The key points about the research question are:

- The goal is to generate photorealistic images from sketches. So the task is sketch-to-image generation.

- The sketches are "abstract human sketches" rather than clean line drawings or edge maps. So the sketches are hand-drawn by amateurs and can be very abstract. 

- Existing methods fail to generate good results from such abstract sketches. The paper aims to address this limitation.

In summary, the main research question is how to bridge the abstraction gap and generate photorealistic images even from highly abstract free-hand sketches made by amateurs. The paper proposes a new model architecture and training process to achieve this.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a novel method for photorealistic image generation from abstract freehand sketches. The key ideas and innovations are:

- A decoupled encoder-decoder training paradigm, where the decoder is a pre-trained frozen StyleGAN that ensures photorealistic outputs. The encoder is trained separately to map sketches to StyleGAN's latent space.

- An autoregressive sketch mapper that handles the varying abstraction levels in sketches by predicting only a few latent codes for abstract sketches and more codes for detailed sketches. 

- A fine-grained discriminative loss based on a pre-trained sketch-photo retrieval model that captures the fine details in the sketch better than just reconstruction loss.

- A partial-aware sketch augmentation strategy to make the model robust to incomplete and sparse sketches.

Overall, the main contribution is developing an end-to-end model tailored for abstract sketch to photorealistic image generation by handling challenges like abstraction, sparsity, deformations in sketches through innovative designs like the autoregressive mapper, fine-grained loss etc. The results showcase the model's superior performance compared to previous arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes a novel autoregressive sketch-to-photo generation model that leverages a pre-trained StyleGAN to produce photorealistic images from abstract free-hand sketches, enabling fine-grained control over the output by predicting latent vectors in a sequential manner based on the input sketch detail.


## How does this paper compare to other research in the same field?

 Here are some key comparisons between this paper and other related research on photorealistic image generation from sketches:

- Unlike many prior works that use edge maps or other pseudo-sketches, this paper focuses on generating photorealistic images from freehand abstract human sketches. This is a more challenging problem due to the greater abstraction gap between sketches and photos. 

- The proposed method differs from typical image-to-image translation approaches like pix2pix by decoupling the training of the generator (pre-trained StyleGAN) and encoder/sketch mapper. This avoids the common issue of deformed outputs caused by pixel alignment assumptions.

- The autoregressive sketch mapper and incorporation of a fine-grained discriminative loss are novel techniques aimed at better capturing the semantic intent from abstract sketches compared to prior generative models.

- Evaluations demonstrate superior performance over recent state-of-the-art methods like pix2pix, CycleGAN, MUNIT in terms of standard metrics and human opinion scores. The method also shows good generalization to unseen sketch datasets.

- Downstream applications like fine-grained sketch-based image retrieval and precise semantic editing showcase unique capabilities enabled by this photorealistic sketch-to-image generation approach.

Overall, the key innovations seem to be in tackling abstraction differences, avoiding pixel alignment assumptions, and better capturing semantic intent from sketches. The decoupled training strategy and new loss terms aid in producing photorealistic rather than deformed outputs. The quantitative and qualitative results verify improved generation quality and sketch mapping abilities compared to related works.
