# [Scaling Laws for Fine-Grained Mixture of Experts](https://arxiv.org/abs/2402.07871)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training large language models (LLMs) is computationally expensive, requiring millions of GPU hours. Mixture-of-Experts (MoE) models have emerged as a way to reduce this cost, but some recent work has questioned whether MoE models remain efficient at very large scale. 

- Prior work studied MoE scaling while keeping training duration and expert size constant. This may underestimate MoE efficiency since longer training and adjusting expert size could further improve performance.

Methods & Contributions:

1) Introduce "granularity" hyperparameter G to control expert size in MoE models, with higher G meaning smaller experts. Show both theoretically and empirically that higher G improves efficiency.

2) Derive new scaling laws for fine-grained MoE incorporating variable training duration, number of parameters, and granularity G. Allows calculating optimal settings for a given compute budget.

3) Demonstrate with optimal hyperparameters, MoE models always outperform traditional Transformers in terms of efficiency, with compute savings exceeding 40x at scale. Contradicts prior claims that MoE efficiency drops at large scale.

4) Find that setting expert size equal to feedforward size (standard practice) is almost never optimal. Recommend using higher granularity G for better efficiency.

5) Provide practical guidance for configuring optimally efficient MoE architectures for a given model size and computational budget.

Implications:
- With proper configuration, MoE models consistently surpass dense Transformers in efficiency across all model sizes considered. Fine-grained MoE is an effective strategy for training giant yet efficient language models.
