# [Cross-Domain Image Conversion by CycleDM](https://arxiv.org/abs/2403.02919)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the problem of converting between machine-printed character images (font images) and handwritten character images. Despite representing the same characters (e.g. 'A'), printed and handwritten characters have significant stylistic differences in shape variations, ornamentations, stroke widths, etc. Thus, converting between these two domains is a challenging image-to-image translation task. 

The task is motivated by:
1) Tackling a difficult domain shift problem.
2) Generating handwriting-style fonts.  
3) Developing a new OCR paradigm by first transforming handwritten input.
4) Understanding whether character classes exhibit core similarities across printed and handwritten forms.

Existing Approaches: 
Supervised approaches like pix2pix require paired images between domains. CycleGAN can learn unpaired relationships but generates lower quality results. SDEdit uses diffusion models, but makes a poor assumption that noisy source images resemble target images.

Proposed Solution:
The paper proposes CycleDM - a novel unpaired image conversion model incorporating diffusion models with cycle-consistency from CycleGAN. 

Key Points:
- Leverages pretrained diffusion models for two domains and frozen weights.  
- Introduces additional conversion modules $F_t$ and $G_t$ to convert between noisy images $X_t$ and $X'_t$ from the two diffusion model spaces.
- $F_t$ and $G_t$ trained for cycle-consistency without correspondences between $X_t$ and $X'_t$. 
- Allows starting the diffusion process in target domain from an explicitly converted image, unlike SDEdit.

Main Contributions:
1) Propose CycleDM for higher quality unpaired image-to-image conversion using diffusion models.
2) Apply model specifically for machine printed vs handwritten character conversion. 
3) Experiments show quantitative and qualitative improvements over SDEdit and CycleGAN.

Let me know if you need any clarification or have additional questions on the summary!
