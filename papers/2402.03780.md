# [Exposing propaganda: an analysis of stylistic cues comparing human   annotations and machine classification](https://arxiv.org/abs/2402.03780)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Propaganda is an informational weapon used to influence opinions, especially in times of conflict. With the rise of the internet and social media, propagandists have new platforms to spread biased and misleading information masked as news articles.  

- Detecting and analyzing propaganda is challenging. This paper investigates the language and stylistic features that characterize propaganda in order to better understand how it differs from regular news content.

Methods & Contributions:

- The authors introduce the PPN (Propagandist Pseudo-News) dataset - a multisource, multilingual dataset of news articles from propaganda websites related to the Ukraine conflict.

- They conducted an annotation experiment where propagandist and regular news articles were mixed and presented to human annotators, who labeled them using 11 distinct categories related to propaganda techniques and news quality. Annotators reliably distinguished propagandist from regular articles.  

- Using the annotated articles, the authors compare human detection of propaganda based on the annotation study with automatic propaganda classification methods:
   - The VAGO system analyzed articles for subjectivity and vagueness markers, finding correlations with human annotations.  
   - Machine learning models including BERT, a syntax-based model, and an XGBoost model were trained to classify propagandist vs regular articles. The models performed very well, with accuracies near 100% for the neural models. Explainability techniques highlighted informative features learned by the models related to writing style.

- Key findings show propagandist articles differ in dimensions like exaggeration, lack of details, and inadequate sourcing. Subtler syntactic and lexical cues also assist in distinguishing the language of propaganda from regular news.

In summary, the paper makes available a valuable multilingual propaganda dataset, and provides analysis comparing human and automatic classification of propaganda based on stylistic and linguistic signals characteristic of biased information.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper introduces a new multilingual dataset of propaganda articles, conducts a human annotation experiment to analyze how propaganda differs from regular news based on various stylistic dimensions, and compares those results to automatic classification using models that rely on lexical, syntactic and semantic cues to detect propaganda.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The introduction of the PPN (Propagandist Pseudo-News) dataset, a new multisource, multilingual, multimodal dataset of news articles from propaganda sources related to the Russia-Ukraine conflict.

2) An annotation experiment conducted on a subset of the French PPN data mixed with regular French press articles. Human annotators were able to reliably discriminate between the two types of press using 11 distinct labels.

3) Analysis of the textual and stylistic differences between the propaganda and regular press using different NLP techniques:
- The VAGO tool to analyze discourse vagueness and subjectivity
- Machine learning models (Roberta, CATS, XGBoost) to identify propaganda based on text
- Explainability methods to understand the cues used by the models

4) A comparison between the stylistic cues identified by human annotators vs. those used by the machine learning models to detect propaganda.

In summary, the key contribution is the new PPN dataset and analyses to better understand how humans and machines can identify propaganda based on linguistic and stylistic properties. The paper introduces new data, experiments, and insights into propaganda detection.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Propaganda detection
- Stylistic cues
- Vagueness
- Subjectivity
- Annotation experiment
- Propaganda techniques
- Multilingual dataset
- Discourse analysis
- Machine learning models
- RoBERTa
- CamemBERT
- CATS
- XGBoost
- SHAP explainability
- Inter-annotator agreement
- News verification

The paper introduces a new multilingual propaganda dataset called PPN (Propagandist Pseudo-News) and conducts experiments using both human annotations and machine learning models to analyze stylistic cues that differentiate propaganda from regular news articles. Key aspects examined include vagueness, subjectivity, exaggeration, use of sources, etc. Various NLP techniques and models like RoBERTa, CamemBERT, CATS, XGBoost, SHAP are leveraged to detect propaganda and explain the predictions. The annotation experiment also looks at inter-annotator agreement on different labels. Overall, the paper focuses on propaganda detection, discourse analysis, and news verification using both human and automated approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper introduces the PPN dataset for propaganda detection. What are some of the key characteristics of this dataset in terms of diversity of sources, topics, and languages? How was the dataset compiled?

2. The paper conducts an annotation experiment to understand how human annotators perceive propaganda. Can you describe the methodology of this experiment, including the articles selected, the labels used, and the analysis of inter-annotator agreement? 

3. The VAGO analyzer is used to measure discourse vagueness and subjectivity in the articles. How are the VAGO scores of vagueness, opinion, and relative detail calculated? What correlations were found between VAGO scores and annotation labels?

4. Several machine learning models are trained for automatic propaganda detection. Can you outline the models, including RoBERTa, CamemBERT, CATS, XGBoost, and TF-IDF? What features and parameters were used for training? 

5. What were the key results of the machine learning models? How did they compare to each other and to the human annotations in detecting propaganda? What are some potential limitations or biases identified?

6. The paper analyzes the explainability of the CATS and XGBoost models. What propaganda markers and stylistic cues were identified from the salient sentences and features highlighted by these models?

7. The analysis of the XGBoost model reveals several key syntactic features for propaganda detection involving punctuation. Can you describe these punctuation-based differences found between regular and propaganda articles?

8. The high performance of the TF-IDF model points to clear lexical distinctions between regular and propaganda articles. What were some of the differential terms identified from the TF-IDF analysis?

9. The paper focuses specifically on topic-based propaganda detection related to the Ukraine invasion. How might the models and analysis transfer, or not transfer, to propaganda detection in general or for other topics?

10. What are some ways discussed in the paper for further improving explainability of propaganda detection, either through improvements to tools like VAGO or additional annotation labels that could be introduced?
