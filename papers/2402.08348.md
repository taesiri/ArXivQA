# [Visually Dehallucinative Instruction Generation](https://arxiv.org/abs/2402.08348)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generative language models can hallucinate unintended content when generating image-text data, reducing accuracy. 
- Existing VQA datasets have limitations in scale, image-alignness of text, and sentence-level answers.
- There is a need for methods that can generate visually-grounded instructions with sentence-level answers, while avoiding hallucination.

Proposed Method (CAP2QA):
- Presents a scalable framework to generate image-aligned question-answer pairs using GPT, guided by detailed prompts and filtering.
- Leverages existing image-caption datasets (COCO) to ensure visual grounding.  
- Explicit rules in prompts constrain the text to image contents and reduce hallucination risk.
- Produces the CAP2QA-COCO dataset with 873K+ questions anchored to 123K COCO images.


Main Contributions:
- Image-aligned instruction generation method to improve visual grounding of text.
- Significant reduction in visual hallucination, improved object recognition accuracy. 
- Enables complex, sentence-level answers compared to prior VQA datasets.
- Releases large-scale CAP2QA-COCO dataset for aligned visual reasoning.
- Consistent improvements demonstrated over models tuned on prior instruction dataset.

In summary, the paper presents an effective framework to generate visually-grounded question-answering data at scale using GPT, with strong empirical results on improving visual recognition capabilities. The CAP2QA-COCO dataset facilitates complex visual reasoning while avoiding hallucination issues faced by generative models.
