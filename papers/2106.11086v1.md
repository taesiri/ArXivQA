# [Analytically Tractable Bayesian Deep Q-Learning](https://arxiv.org/abs/2106.11086v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question is: 

Can deep reinforcement learning algorithms based on analytically tractable Bayesian inference match or exceed the performance of standard deep reinforcement learning algorithms based on gradient backpropagation?

The authors propose a method called "tractable approximate Gaussian inference" (TAGI) for Bayesian deep learning that allows analytically inferring the weights and biases of a neural network without relying on gradient-based optimization. They adapt this TAGI method to temporal difference Q-learning frameworks like deep Q-networks (DQN).

The central hypothesis is that this TAGI-based deep Q-learning approach can achieve comparable or better performance to standard backpropagation-trained deep Q-networks on reinforcement learning benchmarks, while using fewer hyperparameters and avoiding reliance on gradients.

The experiments compare TAGI-DQN to standard DQN with backpropagation on off-policy (experience replay) and on-policy (n-step Q-learning) algorithms. The results show TAGI-DQN can match or exceed DQN performance on some games, supporting the hypothesis that analytical Bayesian inference can work well for deep reinforcement learning.

In summary, the key research question is whether analytically tractable Bayesian inference can effectively replace backpropagation for training deep neural networks in challenging reinforcement learning problems. The experiments provide an initial proof-of-concept that the answer may be yes.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

- Proposes a method called Tractable Approximate Gaussian Inference (TAGI) for Bayesian deep learning that allows analytical inference of the weights and biases of a neural network. This is different from most other Bayesian deep learning methods that rely on gradient-based optimization or sampling.

- Shows how to adapt the temporal difference Q-learning framework commonly used in reinforcement learning to make it compatible with TAGI. This results in an approach called TAGI-DQN.

- Demonstrates that TAGI-DQN can achieve performance comparable to standard deep Q-learning networks trained with backpropagation on reinforcement learning benchmarks like Atari games. 

- Shows that TAGI-DQN requires fewer hyperparameters and no target network compared to standard DQN.

- Provides the first demonstration that an analytically tractable inference approach for Bayesian neural networks can scale to large and complex reinforcement learning problems like Atari games.

In summary, the main contribution is proposing TAGI-DQN, a tractable Bayesian approach to deep Q-learning that can scale to challenging reinforcement learning problems while requiring less tuning and resources than standard backpropagation-trained methods. The paper shows the promise of analytical Bayesian methods in deep reinforcement learning.
