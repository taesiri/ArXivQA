# [CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo](https://arxiv.org/abs/2305.10320)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to improve the cost aggregation step in learning-based multi-view stereo (MVS) methods using Transformers. The key hypothesis is that applying Transformers to cost aggregation can help explore global correspondences and refine ambiguous matching points effectively, overcoming limitations of CNNs like limited receptive fields.Specifically, the paper proposes CostFormer, a novel Transformer-based cost aggregation network, with two main components:1) Residual Depth-Aware Cost Transformer (RDACT) to aggregate long-range features on the cost volume via self-attention along depth and spatial dimensions.2) Residual Regression Transformer (RRT) to enhance spatial attention before depth regression.The overall hypothesis is that plugging CostFormer into existing MVS networks can improve their cost volumes and 3D reconstruction performance. Experiments on standard MVS datasets validate this hypothesis, showing improved accuracy and completeness with CostFormer.In summary, the paper explores using Transformers to improve cost aggregation in MVS, proposing an efficient CostFormer module that brings long-range contextual reasoning to this core MVS step.


## What is the main contribution of this paper?

The main contribution of this paper is proposing CostFormer, a novel cost aggregation transformer to improve multi-view stereo reconstruction. Specifically:- It proposes Residual Depth-Aware Cost Transformer (RDACT) to aggregate long-range features on the cost volume via self-attention along both depth and spatial dimensions. This helps explore global correspondences and refine ambiguous matching points.- It proposes Residual Regression Transformer (RRT) between cost aggregation and depth regression to enhance spatial attention. This helps maintain accuracy while improving completeness.- CostFormer with RDACT and RRT is a universal plug-in that can be integrated with existing MVS networks to boost performance. - Experiments on DTU, Tanks & Temples, ETH3D, and BlendedMVS datasets demonstrate that CostFormer brings consistent benefits in terms of reconstruction quality, while maintaining efficiency.In summary, the key contribution is using transformer architectures in a novel way for cost aggregation in MVS, which helps overcome limitations of previous CNN-based approaches. The proposed CostFormer is shown to improve performance across different MVS methods and datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes CostFormer, a novel cost aggregation transformer module that can be plugged into existing multi-view stereo networks to improve their performance on depth estimation by enhancing cost volume regularization through long-range spatial and depth attention.
