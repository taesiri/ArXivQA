# [Subnetwork-to-go: Elastic Neural Network with Dynamic Training and   Customizable Inference](https://arxiv.org/abs/2312.03464)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method to train a large neural network that can be flexibly adapted to different model size and complexity constraints during inference without retraining or finetuning. The authors introduce a dynamic-width model design inspired by dynamic neural networks and mixture-of-experts models, where the width corresponds to the number of parallel sub-modules and an input-dependent gating mechanism is used to combine the outputs. Both dynamic depth and width are supported during training, where losses are calculated on the output of the full network and random subnetworks. For inference, subnetworks with arbitrary depth and width can be extracted to match hardware requirements. Experiments on music source separation validate that subnetworks extracted using this method improve over stand-alone training, while training the single overparameterized network drastically reduces total training time. The model supports inference-time configuration ranging widely in complexity, enabling versatile deployment. Overall, this work provides an effective and efficient way to obtain high-performance specialized subnetworks from a single trained large network.
