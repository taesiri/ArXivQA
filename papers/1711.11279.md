# [Interpretability Beyond Feature Attribution: Quantitative Testing with   Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: How can we interpret the internal state and predictions of deep neural networks in terms of high-level, human-understandable concepts? The key ideas and contributions of the paper are:- Introducing Concept Activation Vectors (CAVs) as a way to represent human concepts in the activation space of a neural network layer. CAVs are learned by training a linear classifier to distinguish between activations produced by examples of a concept vs random examples. - Proposing Testing with CAVs (TCAV), which uses directional derivatives along a CAV to quantify how sensitive a prediction is to a particular concept. This allows interpreting model predictions globally in terms of conceptual importance.- Demonstrating TCAV on real image classifiers and a medical application, gaining insights about model biases, confirming intuitions, and revealing model errors.- Comparing TCAV to saliency maps on a controlled dataset and showing TCAV can recover known ground truth while saliency maps fail.- Validating that CAVs align with concepts via image sorting and activation maximization.So in summary, the main hypothesis is that TCAV can be used to interpret neural network predictions in terms of human concepts not known during training. The results support this claim and highlight the benefits over just using saliency methods or input features.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing the concept of Concept Activation Vectors (CAVs) as a method for interpreting deep neural network models. Specifically:- CAVs are vectors that represent concepts of interest (e.g. stripes, dots, gender) in the activation space of a neural network layer. They are learned by training a linear classifier on the activations produced by examples of a concept versus random examples. - CAVs allow computing directional derivatives of a model's predictions with respect to the concept, quantifying the model's sensitivity to that concept. This leads to the proposed Testing with CAV (TCAV) method.- TCAV scores measure the influence of a concept on a class prediction by computing the directional derivatives for inputs of that class. The scores indicate how important the concept is for the model's predictions globally across a whole class.- The authors show TCAV can be used to gain insights into image classifiers and reveal biases without needing to retrain models. They also apply it to a medical imaging application for interpretability.In summary, the key contribution is introducing CAVs and the TCAV method to quantify model sensitivity to user-defined concepts globally across classes, enabling interpretability and analysis without retraining or model access. The approach allows non-experts to test hypotheses and gain insights into models using examples to define concepts of interest.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper introduces Concept Activation Vectors (CAVs) as a way to interpret deep neural network models by attributing predictions to user-defined concepts, and proposes Testing with CAVs (TCAV) to quantify a model's sensitivity to those concepts without needing to retrain or modify the model.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on interpretability of deep learning models:- It focuses on developing a post-hoc interpretability method (TCAV) that can be applied to any trained neural network model without needing to modify or retrain the model. Many other interpretability methods require changing the model architecture or training procedure.- The key idea is to quantify the model's sensitivity to user-defined concepts using directional derivatives along "Concept Activation Vectors" (CAVs). This allows testing hypotheses about which concepts a model uses for particular predictions. Other methods like saliency maps highlight important input features but don't allow testing concepts directly.- TCAV generates global explanations about model behavior on a whole class, whereas saliency maps and other gradient-based methods produce local explanations about individual predictions. The global nature of TCAV scores makes the method more robust.- The concepts tested by TCAV do not need to be known at training time. Users can define new concepts through sets of example images. This makes it customizable for analyzing models for biases related to human-defined concepts like gender or race that were not explicitly part of the training data.- TCAV is model-agnostic and tested on CNN image classifiers, but the approach could likely be extended to other data types like text, audio, etc.- The paper presents thorough experiments demonstrating that TCAV scores align with approximated ground truth and reveal biases and insights in real image classifiers. It also compares to saliency maps via human subject experiments.In summary, this paper introduces a novel approach to interpretability that is model-agnostic, post-hoc, global, and customizable without needing model access or training data. The approach is demonstrated through insightful experiments on real classifiers.


## What future research directions do the authors suggest?

 The authors suggest several potential future research directions:- Applying TCAV to other types of data beyond images, such as audio, video, sequences, etc. They suggest this could yield new insights.- Using TCAV for applications other than interpretability, such as identifying adversarial examples. - Developing methods to automatically identify concepts and generate CAVs without requiring user-provided example sets. This could help analyze models that show super-human performance.- Concept attribution may help humans improve their own abilities, if applied to models that show super-human performance on certain tasks.- There are opportunities to build on the concept attribution approach in general. For example, developing methods to automatically identify important concepts, going beyond linear approximations, integrating concept attribution into model training, etc.So in summary, the main future directions suggested are: applying TCAV to new data types and tasks, automating concept identification, using concept attribution to improve human abilities, and building on the concept attribution approach in various ways. The key opportunities identified involve extending concept attribution to new domains, automating parts of the process, and using it to improve models and human understanding.


## Summarize the paper in one paragraph.

 The paper proposes a method called Testing with Concept Activation Vectors (TCAV) for interpreting deep learning models by attributing their predictions to user-defined concepts. The key ideas are:1) Defining concepts using sets of example images chosen by the user, not limited to the model's training data labels. 2) Learning a Concept Activation Vector (CAV) for each concept by training a linear classifier to separate the activations of the concept's examples from random examples. The CAV is the normal to the hyperplane classifier.3) Using the CAV and directional derivatives to quantify a prediction's sensitivity to a concept via the TCAV score. This measures the model's directional sensitivity to the concept across inputs.4) Conducting hypothesis testing by retraining CAVs to eliminate spurious correlations. Experiments apply TCAV to gain insights into image classifiers, reveal biases, and identify learned concepts across layers. It is also applied to a medical application, where TCAV provided interpretation for model errors. Overall, TCAV is proposed as a general approach for model-agnostic interpretability using human-defined concepts.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:The paper introduces the Concept Activation Vector (CAV) method for interpreting deep neural network models. CAVs provide a way to quantify the importance of user-defined concepts to a model's predictions. The key idea is to view the high-dimensional internal state of a neural network as an aid rather than an obstacle. CAVs are vectors that represent directions in the activation space that correspond to specific concepts defined by example images. The main contribution is a testing methodology called Testing with CAV (TCAV) that uses CAVs and directional derivatives to quantify a model's sensitivity to a concept for a specific class. For example, TCAV can quantify how sensitive the prediction of "zebra" is to the "striped" concept. Experiments show that TCAV can provide insights into widely used image classification models, reveal biases, and identify model errors in a medical application. The paper argues that TCAV meets important goals such as accessibility, customization, plug-in readiness, and global quantification. Overall, CAVs and TCAV aim to create more human-interpretable explanations of model decisions grounded in natural concepts.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper introduces Concept Activation Vectors (CAVs) as a way to interpret deep neural network models by attributing predictions to human-friendly concepts. The key idea is that a CAV represents the direction in a model's internal activation space that corresponds to a particular concept defined by a set of examples. CAVs are computed by training a linear classifier to distinguish between a concept's activation patterns and random activation patterns. The CAV is the normal vector to the decision boundary. These CAVs can then be used to compute directional derivatives of the model's predictions with respect to the concept, which quantifies the model's sensitivity to that concept. The paper introduces Testing with Concept Activation Vectors (TCAV), which uses CAVs and directional derivatives to interpret an entire class of predictions in terms of human-defined concepts. TCAV does not require retraining or modifying the original model. Experiments demonstrate how TCAV can reveal insights and biases in widely used image classification models and a medical application.


## What problem or question is the paper addressing?

 The paper is addressing the challenge of interpreting and understanding the behavior of deep learning models, especially image classification models. Some key problems and questions it aims to tackle:- Deep learning models operate on low-level features like pixels, not high-level human-interpretable concepts. This makes their internal representations and decision making opaque and hard to understand. - Existing interpretation methods like saliency maps are limited - they provide local, per-input explanations and don't allow global quantification of a concept's importance. They also don't allow customization to explore user-defined concepts.- Can we develop interpretation methods that are accessible to non-experts, allow flexible customization, provide global quantifications, and don't require retraining models?The main contribution of the paper is introducing Concept Activation Vectors (CAVs) and Testing with CAV (TCAV), which aim to provide human-friendly, globally-quantitative interpretations of models in terms of user-defined concepts, without needing to modify or retrain the model.In summary, the key problem is the lack of interpretability of complex deep learning models, especially in terms of high-level concepts. And the paper introduces TCAV using CAVs as a potential solution method for this problem.
