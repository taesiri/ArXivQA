# Interpretability Beyond Feature Attribution: Quantitative Testing with   Concept Activation Vectors (TCAV)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we interpret the internal state and predictions of deep neural networks in terms of high-level, human-understandable concepts? The key ideas and contributions of the paper are:- Introducing Concept Activation Vectors (CAVs) as a way to represent human concepts in the activation space of a neural network layer. CAVs are learned by training a linear classifier to distinguish between activations produced by examples of a concept vs random examples. - Proposing Testing with CAVs (TCAV), which uses directional derivatives along a CAV to quantify how sensitive a prediction is to a particular concept. This allows interpreting model predictions globally in terms of conceptual importance.- Demonstrating TCAV on real image classifiers and a medical application, gaining insights about model biases, confirming intuitions, and revealing model errors.- Comparing TCAV to saliency maps on a controlled dataset and showing TCAV can recover known ground truth while saliency maps fail.- Validating that CAVs align with concepts via image sorting and activation maximization.So in summary, the main hypothesis is that TCAV can be used to interpret neural network predictions in terms of human concepts not known during training. The results support this claim and highlight the benefits over just using saliency methods or input features.


## What is the main contribution of this paper?

The main contribution of this paper is introducing the concept of Concept Activation Vectors (CAVs) as a method for interpreting deep neural network models. Specifically:- CAVs are vectors that represent concepts of interest (e.g. stripes, dots, gender) in the activation space of a neural network layer. They are learned by training a linear classifier on the activations produced by examples of a concept versus random examples. - CAVs allow computing directional derivatives of a model's predictions with respect to the concept, quantifying the model's sensitivity to that concept. This leads to the proposed Testing with CAV (TCAV) method.- TCAV scores measure the influence of a concept on a class prediction by computing the directional derivatives for inputs of that class. The scores indicate how important the concept is for the model's predictions globally across a whole class.- The authors show TCAV can be used to gain insights into image classifiers and reveal biases without needing to retrain models. They also apply it to a medical imaging application for interpretability.In summary, the key contribution is introducing CAVs and the TCAV method to quantify model sensitivity to user-defined concepts globally across classes, enabling interpretability and analysis without retraining or model access. The approach allows non-experts to test hypotheses and gain insights into models using examples to define concepts of interest.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces Concept Activation Vectors (CAVs) as a way to interpret deep neural network models by attributing predictions to user-defined concepts, and proposes Testing with CAVs (TCAV) to quantify a model's sensitivity to those concepts without needing to retrain or modify the model.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on interpretability of deep learning models:- It focuses on developing a post-hoc interpretability method (TCAV) that can be applied to any trained neural network model without needing to modify or retrain the model. Many other interpretability methods require changing the model architecture or training procedure.- The key idea is to quantify the model's sensitivity to user-defined concepts using directional derivatives along "Concept Activation Vectors" (CAVs). This allows testing hypotheses about which concepts a model uses for particular predictions. Other methods like saliency maps highlight important input features but don't allow testing concepts directly.- TCAV generates global explanations about model behavior on a whole class, whereas saliency maps and other gradient-based methods produce local explanations about individual predictions. The global nature of TCAV scores makes the method more robust.- The concepts tested by TCAV do not need to be known at training time. Users can define new concepts through sets of example images. This makes it customizable for analyzing models for biases related to human-defined concepts like gender or race that were not explicitly part of the training data.- TCAV is model-agnostic and tested on CNN image classifiers, but the approach could likely be extended to other data types like text, audio, etc.- The paper presents thorough experiments demonstrating that TCAV scores align with approximated ground truth and reveal biases and insights in real image classifiers. It also compares to saliency maps via human subject experiments.In summary, this paper introduces a novel approach to interpretability that is model-agnostic, post-hoc, global, and customizable without needing model access or training data. The approach is demonstrated through insightful experiments on real classifiers.
