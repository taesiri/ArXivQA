# Interpretability Beyond Feature Attribution: Quantitative Testing with   Concept Activation Vectors (TCAV)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we interpret the internal state and predictions of deep neural networks in terms of high-level, human-understandable concepts? The key ideas and contributions of the paper are:- Introducing Concept Activation Vectors (CAVs) as a way to represent human concepts in the activation space of a neural network layer. CAVs are learned by training a linear classifier to distinguish between activations produced by examples of a concept vs random examples. - Proposing Testing with CAVs (TCAV), which uses directional derivatives along a CAV to quantify how sensitive a prediction is to a particular concept. This allows interpreting model predictions globally in terms of conceptual importance.- Demonstrating TCAV on real image classifiers and a medical application, gaining insights about model biases, confirming intuitions, and revealing model errors.- Comparing TCAV to saliency maps on a controlled dataset and showing TCAV can recover known ground truth while saliency maps fail.- Validating that CAVs align with concepts via image sorting and activation maximization.So in summary, the main hypothesis is that TCAV can be used to interpret neural network predictions in terms of human concepts not known during training. The results support this claim and highlight the benefits over just using saliency methods or input features.


## What is the main contribution of this paper?

The main contribution of this paper is introducing the concept of Concept Activation Vectors (CAVs) as a method for interpreting deep neural network models. Specifically:- CAVs are vectors that represent concepts of interest (e.g. stripes, dots, gender) in the activation space of a neural network layer. They are learned by training a linear classifier on the activations produced by examples of a concept versus random examples. - CAVs allow computing directional derivatives of a model's predictions with respect to the concept, quantifying the model's sensitivity to that concept. This leads to the proposed Testing with CAV (TCAV) method.- TCAV scores measure the influence of a concept on a class prediction by computing the directional derivatives for inputs of that class. The scores indicate how important the concept is for the model's predictions globally across a whole class.- The authors show TCAV can be used to gain insights into image classifiers and reveal biases without needing to retrain models. They also apply it to a medical imaging application for interpretability.In summary, the key contribution is introducing CAVs and the TCAV method to quantify model sensitivity to user-defined concepts globally across classes, enabling interpretability and analysis without retraining or model access. The approach allows non-experts to test hypotheses and gain insights into models using examples to define concepts of interest.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces Concept Activation Vectors (CAVs) as a way to interpret deep neural network models by attributing predictions to user-defined concepts, and proposes Testing with CAVs (TCAV) to quantify a model's sensitivity to those concepts without needing to retrain or modify the model.
