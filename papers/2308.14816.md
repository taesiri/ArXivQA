# [CLNeRF: Continual Learning Meets NeRF](https://arxiv.org/abs/2308.14816)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is:How can we design an effective continual learning system for Neural Radiance Fields (NeRFs) that can efficiently adapt to new scene data over time without catastrophic forgetting or requiring large amounts of storage of past data?The key points are:- NeRFs are useful for novel view synthesis but assume all training data is available upfront. In many real applications, new data arrives sequentially over time.- Naively retraining NeRFs on new data leads to catastrophic forgetting of past data. Storing all past data is expensive. - This paper proposes CLNeRF, a continual learning system for NeRFs that combines generative replay with advanced NeRF architectures like NGP.- CLNeRF allows efficient adaptation to new data over time without forgetting past data, while requiring minimal storage of historical images.- Experiments show CLNeRF performs on par with models trained on all data jointly, outperforming other continual learning techniques.So in summary, the main research contribution is an effective and low-storage continual learning system to allow NeRFs to be adapted efficiently as new data arrives over time.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new dataset called World Across Time (WAT) for benchmarking continual learning methods for neural radiance fields (NeRF). WAT contains scenes with natural appearance and geometry changes over time, providing a more realistic setup compared to existing NeRF datasets where scenes are static. 2. It proposes a continual learning system called CLNeRF that can efficiently adapt a single NeRF model as new images of a changing scene arrive over time. CLNeRF combines generative replay and the Instant Neural Graphics Primitives (NGP) architecture to enable fast model updates and prevent catastrophic forgetting without storing historical images.3. It introduces trainable appearance and geometry embeddings into NGP so a single compact model can handle complex scene changes over time. 4. It provides an extensive experimental evaluation demonstrating CLNeRF's superiority over other continual learning baselines on standard datasets and the proposed WAT dataset. CLNeRF performs on par with a model trained on all data at once while requiring minimal storage and memory overhead to incorporate new data continually.In summary, the key contribution is an effective continual learning system CLNeRF tailored for NeRF that can efficiently adapt to changing scenes over time while preventing catastrophic forgetting. The proposed dataset WAT enables more realistic benchmarking.
