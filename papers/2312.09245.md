# [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral   Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes DriveMLM, a novel autonomous driving framework that integrates large language models (LLMs) to perform closed-loop driving in realistic simulators. A key innovation is aligning the linguistic outputs of the LLM with the decision states of the behavioral planning module in Apollo to enable vehicle control. Specifically, they design a multi-modal LLM planner that takes in sensor inputs like images and LiDAR along with driving rules and user commands, and outputs driving decisions and natural language explanations. To train this model, they collect and annotate a large-scale dataset from CARLA simulator with decision states and explanations. Experiments demonstrate superior performance over Apollo, achieving 76.1 driving score on CARLA Town05 Long benchmark. The model also has strong generalizability, even performing reasonably on real-world nuScenes images without training on them. Key advantages are the model's interpretability, adaptability to new user commands, and incorporation of world knowledge to handle corner cases. Overall, this work serves as an effective baseline for integrating LLMs with autonomous driving systems.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving":

Problem:
- Traditional rule-based and data-driven autonomous driving (AD) systems have limitations in handling corner cases due to constraints from expert knowledge or training data diversity. 
- Recently, large language models (LLMs) have shown potential as AD planners due to their world knowledge and reasoning capability, but existing LLM-based AD systems cannot perform closed-loop driving in realistic environments. This is because LLM outputs are linguistic and cannot directly control vehicles.

Proposed Solution:
- Propose DriveMLM, an LLM-based AD framework that can perform closed-loop driving by aligning LLM outputs with the decision states of the behavioral planning module in Apollo.
- Design a multi-modal LLM (MLLM) planner that takes inputs from cameras, LiDAR, traffic rules, system messages, user instructions, and outputs aligned decisions and explanations.
- Develop an efficient data engine to collect 280 hours of driving data with decision state and explanation annotations for model training.

Main Contributions:
- First LLM-based AD system that bridges the gap between linguistic outputs of LLM and executable decisions for vehicle control via decision state alignment.
- Tailor multi-modal inputs and decode aligned decisions and explanations with an MLLM planner.  
- Collect comprehensive annotated dataset encompassing various driving scenarios, decision states and explanations.
- Achieve state-of-the-art driving performance on CARLA simulator, surpassing Apollo by 4.7 points in driving score. Demonstrate generalizability to real-world datasets.

In summary, this paper pioneeringly equips AD systems with reasoning capability of LLMs by aligning decisions with behavioral planning states, and builds an LLM-based AD system that can perform closed-loop driving in realistic environments.
