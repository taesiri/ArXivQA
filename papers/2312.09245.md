# [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral   Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes DriveMLM, a novel autonomous driving framework that integrates large language models (LLMs) to perform closed-loop driving in realistic simulators. A key innovation is aligning the linguistic outputs of the LLM with the decision states of the behavioral planning module in Apollo to enable vehicle control. Specifically, they design a multi-modal LLM planner that takes in sensor inputs like images and LiDAR along with driving rules and user commands, and outputs driving decisions and natural language explanations. To train this model, they collect and annotate a large-scale dataset from CARLA simulator with decision states and explanations. Experiments demonstrate superior performance over Apollo, achieving 76.1 driving score on CARLA Town05 Long benchmark. The model also has strong generalizability, even performing reasonably on real-world nuScenes images without training on them. Key advantages are the model's interpretability, adaptability to new user commands, and incorporation of world knowledge to handle corner cases. Overall, this work serves as an effective baseline for integrating LLMs with autonomous driving systems.
