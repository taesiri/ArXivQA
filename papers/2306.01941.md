# [AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap](https://arxiv.org/abs/2306.01941)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it does not appear to be making a single central claim or testing a specific hypothesis. Rather, it seems to be more of a perspective/opinion piece that lays out a roadmap and open questions for future research on transparency approaches for large language models (LLMs). Some of the key points and themes I picked up are:- LLMs introduce unique challenges for transparency (e.g. complex capabilities, massive scale, proprietary nature) compared to past AI systems that transparency research has focused on.- There are lessons learned from past human-centered AI and HCI research on transparency that are relevant, like taking a goal-oriented perspective, supporting appropriate trust, and considering people's mental models.- Existing transparency approaches like model reporting, evaluation, explanations, and uncertainty communication can serve as a starting point but may need to be rethought for LLMs. The paper discusses open questions around how to adapt these approaches.- Overall, there is a need for more research taking a human-centered approach to transparency for LLMs, considering the needs of diverse new stakeholders and usage contexts. The paper aims to lay out a roadmap for this research.So in summary, I would say the paper is intended to synthesize past work, articulate the new challenges introduced by LLMs, and outline open questions and opportunities to guide future human-centered research on transparency for LLMs. It does not make a single central claim.


## What is the main contribution of this paper?

The main contribution of this paper is to provide a roadmap for human-centered research on AI transparency in the era of large language models (LLMs). Specifically, the paper:- Reflects on the unique challenges LLMs pose for transparency, including their complex capabilities, massive scale, proprietary nature, and other factors. - Synthesizes lessons learned from prior HCI and responsible AI research taking a human-centered perspective on transparency, such as the importance of supporting people's goals, building appropriate trust and mental models, and effective communication.- Explores the applicability of common transparency approaches like model reporting, evaluation, explanations, and uncertainty communication to LLMs, while raising open questions specific to LLMs and their stakeholders.- Calls for research attending to both the technology characteristics of LLMs and the human stakeholders interacting with or impacted by them in order to develop effective transparency.In summary, the paper provides a useful framework and starting point to guide future human-centered research on transparency as LLMs continue to be developed and deployed. It highlights the need to rethink transparency approaches for this new class of models while building on insights from prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper lays out a human-centered research roadmap for transparency approaches to support responsible development and use of large language models, reflecting on the unique challenges posed by LLMs, lessons from prior HCI/FATE research, and open questions around applying common transparency techniques like model reporting, evaluation, explanation, and uncertainty communication.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on AI transparency:- It takes a human-centered perspective on transparency, focusing on the needs of diverse stakeholders and how they process and interact with transparency information. Much prior work focuses more narrowly on developing technical approaches without considering human factors.- It provides a comprehensive overview across transparency techniques like model reporting, evaluation, explanations, and uncertainty communication. Many papers focus on just one technique in isolation. - It explores open questions and future directions for transparency research specifically for large language models (LLMs). Most existing work predates the rise of LLMs or focuses on other types of models.- It synthesizes lessons from the HCI and FATE literature on human mental models, appropriate trust, control, and effective communication. Many technical papers do not connect to this related work.- It considers transparency needs across the LLM ecosystem, including base models, adapted models, and LLM-powered applications. Most work looks at transparency for isolated models.- It takes a critical perspective, considering challenges like proprietary models and organizational incentives that may limit transparency. Much work presumes unfettered access to models.Overall, this paper provides a broad, human-centered perspective on transparency for LLMs, synthesizing a wide range of prior work. It lays out a research agenda considering the unique aspects of LLMs and connects technical approaches to human needs and impact. This provides useful guidance for the field at a critical juncture as LLMs become more widespread.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the paper:- Investigating the new transparency goals that arise for stakeholders interacting with LLMs, such as supporting ideation, model adaptation, prompting, and discovering risky model behaviors. - Studying which approaches to transparency can best help people develop appropriate levels of trust in LLMs. This includes conceptualizing and measuring trust, and empirically evaluating the impact of different transparency approaches.- Exploring how transparency approaches can contribute to better control mechanisms for LLMs, enabling stakeholders to steer LLM behavior.- Unpacking people's mental models of LLMs and supporting the formation of more accurate and complete mental models through transparency.- Distilling principles for effectively communicating necessary information about LLMs during natural language interactions.- Developing new evaluation techniques and communication patterns to evaluate LLMs in a transparent manner, considering their unpredictable capabilities.- Pursuing more faithful explanations for LLMs, potentially by articulating different types of explanations and their uses.- Defining useful notions of uncertainty for LLMs based on human needs and studying effective ways to communicate uncertainty.- Considering transparency around temporal changes in LLMs and their downstream effects.- Exploring the role of external auditors, regulators, and the general public in achieving transparency for LLMs.The authors highlight the need for a human-centered perspective on LLM transparency and encourage research attending to the technology, stakeholders, and broader socio-organizational context.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper discusses transparency approaches for large language models (LLMs). It first reflects on the unique challenges LLMs pose for transparency, including their complex and uncertain capabilities, massive scale, proprietary nature, use in novel applications, expanded stakeholders, flawed public perception, and organizational pressures. It then synthesizes lessons learned from human-computer interaction (HCI) and responsible AI research taking a human-centered perspective, including the importance of considering transparency as goal-oriented, aiming for appropriate trust, accounting for mental models, thoughtful communication choices, and joint design of transparency and control. Next, it explores the applicability of common transparency approaches like model reporting, evaluation, explanations, and uncertainty communication for LLMs, discussing open questions around what information is useful, faithful explanation methods, and appropriate uncertainty metrics. It concludes by calling for research accounting for the technology, human, and socio-organizational aspects of LLMs and their applications. Overall, the paper lays out a roadmap for human-centered transparency research on LLMs.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper lays out a research roadmap for achieving transparency for large language models (LLMs). It first reflects on the unique challenges LLMs pose for transparency, including their complex and uncertain capabilities, massive scale, proprietary nature, and expanded ecosystem of applications and stakeholders. It then synthesizes lessons learned from prior human-centered AI research, emphasizing the importance of supporting people's goals, building appropriate trust, considering mental models, thoughtful communication, and linking transparency to control. The paper then explores how existing transparency approaches like model reporting, evaluation, explanations, and uncertainty communication may translate to LLMs, raising open questions around each. For example, it discusses the need to rethink what constitutes a good explanation for LLMs and how to effectively communicate the notions of uncertainty that matter to stakeholders. It concludes by noting that human-centered research must investigate the needs of the diverse new stakeholders interacting with LLMs and LLM-infused applications, in light of LLMs' unique capabilities and limitations, in order to develop tailored transparency approaches that allow different groups to use these models responsibly.In summary, this paper provides a useful roadmap for the research community by surveying the landscape of transparency approaches for LLMs. It highlights open challenges and questions in applying existing techniques to LLMs due to their scale, complexity and proprietary nature. It also emphasizes taking a goal-driven, human-centered perspective in investigating transparency needs for the new ecosystem of LLM stakeholders and applications. The paper lays groundwork for developing effective transparency approaches for responsible LLM development and deployment.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper provides a roadmap for human-centered research on AI transparency in the era of large language models (LLMs). The authors first reflect on the unique challenges introduced by LLMs, including their complex capabilities, massive scale, proprietary nature, and expanded ecosystem. They then synthesize lessons learned from prior HCI and responsible AI research taking a human-centered perspective on transparency, emphasizing the importance of considering people's goals, mental models, and needs around trust, control, and communication. The authors review common transparency approaches like model reporting, evaluation, explanations, and uncertainty communication, and explore their applicability to LLMs, calling out open questions around adapting these techniques and developing new ones suited to the LLM context. Overall, the paper takes a reflective, human-centered approach to charting a research agenda on transparency for LLMs based on the technology landscape and stakeholder needs.
