# Do Transformers Really Perform Bad for Graph Representation?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can Transformer architectures perform well for graph representation learning, despite not having achieved competitive performance on popular leaderboards compared to mainstream GNN variants?The authors aim to address this question by presenting Graphormer, a Transformer-based model for graph representation learning. Their key insight is that it is necessary to effectively encode the structural information of graphs into Transformer models to make them work well. To summarize, the main hypothesis is that with proper incorporation of graph structural information, Transformer architectures can perform excellent graph representation learning, contrary to prior results suggesting Transformers may not be suitable for this task. The authors support this via the proposed Graphormer model and experiments demonstrating state-of-the-art performance on graph learning benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing Graphormer, a Transformer model adapted for graph representation learning. The key modifications include centrality encoding, spatial encoding, and edge encoding to incorporate graph structure information into the Transformer architecture. 2. Demonstrating state-of-the-art performance of Graphormer on a variety of graph learning benchmark datasets, including the recent OGB Large-Scale Challenge for quantum chemistry regression.3. Providing theoretical analysis on the expressive power of Graphormer, showing it can cover several popular GNN models as special cases. The strong capacity enables the great empirical performance.4. Conducting ablation studies to validate the effectiveness of the proposed encodings for capturing graph structure. In summary, the key contribution is developing Graphormer by carefully designing structural encodings to adapt the powerful Transformer architecture for graph representation learning. Both theoretical analysis and extensive experiments demonstrate Graphormer's effectiveness. The work provides new state-of-the-art results across various graph learning tasks and sheds light on how to effectively utilize Transformer architectures for modeling graph data.
