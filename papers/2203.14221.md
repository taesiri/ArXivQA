# [How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?](https://arxiv.org/abs/2203.14221)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How sensitive are current video self-supervised learning methods to the standard benchmarks, and do they generalize beyond the canonical evaluation settings?The authors identify that most video self-supervised learning methods are evaluated on very similar datasets to the pre-training data (e.g. Kinetics-400, UCF-101, HMDB-51). While this has shown the effectiveness of self-supervised learning, it does not provide insight into how well these methods generalize. To address this, the authors systematically evaluate generalization across four factors:1. Downstream domain - How well do methods transfer when there is a domain shift from pre-training data?2. Downstream samples - How sensitive are methods to the amount of labeled data available for finetuning? 3. Downstream actions - Can methods recognize more fine-grained, semantically similar actions?4. Downstream tasks - Are self-supervised features useful beyond action recognition?Through experiments over 500+ settings on 7 datasets, 9 methods and 6 tasks, the authors demonstrate current benchmarks are insufficient to measure generalization. They also find supervised pre-training outperforms self-supervised methods, especially with large domain shifts and limited labeled data. Based on this, they propose the SEVERE benchmark to evaluate generalizability of future self-supervised video methods.
