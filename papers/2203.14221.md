# [How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?](https://arxiv.org/abs/2203.14221)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How sensitive are current video self-supervised learning methods to the standard benchmarks, and do they generalize beyond the canonical evaluation settings?The authors identify that most video self-supervised learning methods are evaluated on very similar datasets to the pre-training data (e.g. Kinetics-400, UCF-101, HMDB-51). While this has shown the effectiveness of self-supervised learning, it does not provide insight into how well these methods generalize. To address this, the authors systematically evaluate generalization across four factors:1. Downstream domain - How well do methods transfer when there is a domain shift from pre-training data?2. Downstream samples - How sensitive are methods to the amount of labeled data available for finetuning? 3. Downstream actions - Can methods recognize more fine-grained, semantically similar actions?4. Downstream tasks - Are self-supervised features useful beyond action recognition?Through experiments over 500+ settings on 7 datasets, 9 methods and 6 tasks, the authors demonstrate current benchmarks are insufficient to measure generalization. They also find supervised pre-training outperforms self-supervised methods, especially with large domain shifts and limited labeled data. Based on this, they propose the SEVERE benchmark to evaluate generalizability of future self-supervised video methods.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It identifies the problem of benchmark-sensitivity in video self-supervised learning, and examines this sensitivity along four factors - downstream domain, samples, actions, and tasks. 2. It performs an extensive evaluation spanning over 500 experiments with 9 video self-supervised methods across 7 datasets and 6 tasks. The key findings are:- Current benchmarks in video self-supervised learning are not good indicators of generalization along the four sensitivity factors. - Self-supervised methods lag significantly behind vanilla supervised pre-training, especially when domain shift is large and number of downstream samples is low.3. It proposes the SEVERE-benchmark, a subset of the experiments, to evaluate generalizability of video self-supervised methods along the four factors.4. It provides an analysis of the results to distill insights about:- No single self-supervised method consistently generalizes the best.- Supervised pre-training dominates self-supervised methods when domain and task shift together. - Methods encouraging temporal distinctiveness generalize better across factors.- Pretext tasks can be effective for domain and action generalization.5. It makes recommendations for future video self-supervised learning research based on the observations.In summary, the main contribution is a comprehensive benchmarking study to evaluate generalization of video self-supervised learning methods beyond current benchmarks, leading to insights about their limitations and recommendations for future work. The SEVERE-benchmark is proposed as a more thorough testbed for generalizability.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, the key points of the paper are:- The paper evaluates the generalization capability of current video self-supervised learning methods by testing them across different downstream factors like domain, samples, actions and tasks. - Through an extensive set of over 500 experiments on 7 datasets involving 9 methods, the paper shows that the standard benchmarks used currently are not good indicators of generalization for video self-supervised learning.- The paper finds supervised pre-training outperforms self-supervised methods considerably when domain shift is large and available downstream samples are few. - From the analysis, the paper proposes the SEVERE benchmark, a subset of their experiments, to evaluate the generalizability of video self-supervised learning methods in future works.In one sentence, I would summarize it as: The paper performs a large-scale study to demonstrate the benchmark sensitivity in current video self-supervised learning evaluations and proposes the SEVERE benchmark for more rigorous testing of generalization in future methods.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research in video self-supervised learning:- This paper focuses on evaluating the generalization capability of existing video self-supervised learning methods, whereas most prior works propose new methods and evaluate on standard benchmarks like UCF101 and HMDB51. - The paper thoroughly investigates generalization across four factors - downstream domain, samples, actions, and tasks. This is a more comprehensive analysis of generalization compared to prior works that typically just add results on one extra dataset.- The scale of this study is much larger than prior works with over 500 experiments across 7 datasets, 9 methods, and 6 tasks. - The paper takes inspiration from benchmarking studies in image self-supervised learning, but notes that separate analysis is needed for video due to distinct tasks like exploiting temporal structure.- The paper provides concrete recommendations for improving video self-supervised learning based on analysis. Most prior works only focus on proposing new methods.- A key contribution is the proposed SEVERE benchmark for systematically evaluating generalization capability of future video self-supervised methods.In summary, this paper provides the most extensive analysis of generalization for video self-supervised learning done so far. It moves beyond just proposing new methods to critically evaluating existing methods, distilling insights, and laying out a path forward with the SEVERE benchmark. The large-scale study fills an important gap in understanding these rapidly developing methods.
