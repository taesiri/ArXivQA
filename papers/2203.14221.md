# [How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?](https://arxiv.org/abs/2203.14221)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How sensitive are current video self-supervised learning methods to the standard benchmarks, and do they generalize beyond the canonical evaluation settings?The authors identify that most video self-supervised learning methods are evaluated on very similar datasets to the pre-training data (e.g. Kinetics-400, UCF-101, HMDB-51). While this has shown the effectiveness of self-supervised learning, it does not provide insight into how well these methods generalize. To address this, the authors systematically evaluate generalization across four factors:1. Downstream domain - How well do methods transfer when there is a domain shift from pre-training data?2. Downstream samples - How sensitive are methods to the amount of labeled data available for finetuning? 3. Downstream actions - Can methods recognize more fine-grained, semantically similar actions?4. Downstream tasks - Are self-supervised features useful beyond action recognition?Through experiments over 500+ settings on 7 datasets, 9 methods and 6 tasks, the authors demonstrate current benchmarks are insufficient to measure generalization. They also find supervised pre-training outperforms self-supervised methods, especially with large domain shifts and limited labeled data. Based on this, they propose the SEVERE benchmark to evaluate generalizability of future self-supervised video methods.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It identifies the problem of benchmark-sensitivity in video self-supervised learning, and examines this sensitivity along four factors - downstream domain, samples, actions, and tasks. 2. It performs an extensive evaluation spanning over 500 experiments with 9 video self-supervised methods across 7 datasets and 6 tasks. The key findings are:- Current benchmarks in video self-supervised learning are not good indicators of generalization along the four sensitivity factors. - Self-supervised methods lag significantly behind vanilla supervised pre-training, especially when domain shift is large and number of downstream samples is low.3. It proposes the SEVERE-benchmark, a subset of the experiments, to evaluate generalizability of video self-supervised methods along the four factors.4. It provides an analysis of the results to distill insights about:- No single self-supervised method consistently generalizes the best.- Supervised pre-training dominates self-supervised methods when domain and task shift together. - Methods encouraging temporal distinctiveness generalize better across factors.- Pretext tasks can be effective for domain and action generalization.5. It makes recommendations for future video self-supervised learning research based on the observations.In summary, the main contribution is a comprehensive benchmarking study to evaluate generalization of video self-supervised learning methods beyond current benchmarks, leading to insights about their limitations and recommendations for future work. The SEVERE-benchmark is proposed as a more thorough testbed for generalizability.
