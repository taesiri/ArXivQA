# [Task Indicating Transformer for Task-conditional Dense Predictions](https://arxiv.org/abs/2403.00327)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-task learning for dense prediction tasks like semantic segmentation and depth estimation is important but challenging. 
- Existing task-conditional methods have limitations in learning shared and task-specific representations due to CNN architectures having limited receptive fields.
- Lack of multi-scale feature interaction in the decoding stage also limits performance.

Proposed Solution:
- Propose Task Indicating Transformer (TIT), a lightweight task-conditional framework that uses transformers to capture long-range dependencies.
- Introduce Mix Task Adapter module that incorporates a Task Indicating Matrix to enhance modeling of inter- and intra-task features. Achieves parameter efficiency via matrix decomposition.
- Propose Task Gate Decoder module to enable adaptive multi-scale feature refinement guided by task embeddings and gating mechanism.

Main Contributions:
- Mix Task Adapter module for efficient feature adaptation and joint learning of task-specific and shared representations.
- Task Gate Decoder module for improved multi-scale feature interaction conditioned on task information.
- Experiments on NYUD-v2 and PASCAL-Context show state-of-the-art performance compared to prior task-conditional approaches. Ablations validate contributions of proposed modules.

In summary, the paper tackles limitations of prior work by proposing novel transformer-based task-conditional modules for enhanced modeling, adaptation and decoding for multi-task dense predictions. Extensive experiments demonstrate effectiveness.
