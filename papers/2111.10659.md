# [Are Vision Transformers Robust to Patch Perturbations?](https://arxiv.org/abs/2111.10659)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How robust are vision transformers compared to convolutional neural networks when individual input image patches are perturbed with either natural corruptions or adversarial perturbations?The key hypotheses seem to be:1) Vision transformers will be more robust to natural patch corruptions compared to CNNs.2) Vision transformers will be more vulnerable to adversarial patch perturbations compared to CNNs. The paper investigates these hypotheses through empirical evaluations on vision transformer models like DeiT and CNN models like ResNet. The goal is to understand how the unique patch-based architecture and attention mechanism of vision transformers affect robustness to different types of patch-wise perturbations.In summary, the central research question is about understanding the robustness of vision transformers to patch-level perturbations by comparing to CNNs, with a focus on how architectural differences like attention lead to different robustness properties.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Finding: Based on a fair comparison, the authors discover that Vision Transformers (ViT) are more robust to natural patch corruption than Convolutional Neural Networks (CNNs like ResNet), but more vulnerable to adversarial patch perturbations. 2. Understanding: The authors reveal that the self-attention mechanism of ViT can effectively ignore natural corrupted patches to maintain correct predictions, but can also be easily fooled by adversarial patches to make mistakes. 3. Improvement: Inspired by their analysis, the authors propose Smoothed Attention to improve the robustness of ViT against adversarial patch attacks by discouraging attention to focus too strongly on any single patch.In summary, the key contributions are carefully evaluating the robustness of ViT versus CNNs to different types of patch perturbations, analyzing the role of the self-attention mechanism in ViT's robustness properties, and proposing a method to improve ViT's robustness against adversarial patches based on these insights. Thecombination of empirical findings, analysis to develop understanding, and an improvement method based on that understanding make up the main contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper investigates the robustness of vision transformers (ViTs) compared to convolutional neural networks (CNNs) when image patches are naturally corrupted or adversarially perturbed, finding that ViTs are more robust to natural corruptions but more vulnerable to adversarial patches due to differences in their attention mechanisms.


## How does this paper compare to other research in the same field?

This paper investigates the robustness of vision transformers (ViTs) to patch-wise perturbations, compared to convolutional neural networks (CNNs). The key findings and how they relate to previous work are:1. ViTs are more robust to natural patch corruption than CNNs. This aligns with previous work showing ViTs are more robust to common corruptions overall. However, this paper looks specifically at patch-level corruptions.2. ViTs are more vulnerable to adversarial patch attacks than CNNs. Prior work has explored adversarial robustness of ViTs vs CNNs with image-level perturbations, with mixed results. This paper provides evidence ViTs are weaker against patch attacks. 3. The self-attention mechanism in ViTs allows ignoring natural corruption but also makes models susceptible to adversarial patches. This provides a novel analysis and explanation for the robustness differences, enabled by visualizing attention.4. A simple method of smoothing attention by temperature scaling improves adversarial robustness of ViTs. This validates the attention analysis and provides a way to enhance ViT robustness based on interpretability.Overall, this paper provides new insights into ViT robustness through targeted patch-level experiments and attention analysis. The findings on natural vs adversarial robustness tradeoffs, the role of attention manipulation, and a way to improve adversarial robustness specifically advance understanding in this area. The patch-based perspective aligns well with the ViT architecture and allows direct comparison to CNNs.
