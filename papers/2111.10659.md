# [Are Vision Transformers Robust to Patch Perturbations?](https://arxiv.org/abs/2111.10659)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How robust are vision transformers compared to convolutional neural networks when individual input image patches are perturbed with either natural corruptions or adversarial perturbations?The key hypotheses seem to be:1) Vision transformers will be more robust to natural patch corruptions compared to CNNs.2) Vision transformers will be more vulnerable to adversarial patch perturbations compared to CNNs. The paper investigates these hypotheses through empirical evaluations on vision transformer models like DeiT and CNN models like ResNet. The goal is to understand how the unique patch-based architecture and attention mechanism of vision transformers affect robustness to different types of patch-wise perturbations.In summary, the central research question is about understanding the robustness of vision transformers to patch-level perturbations by comparing to CNNs, with a focus on how architectural differences like attention lead to different robustness properties.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Finding: Based on a fair comparison, the authors discover that Vision Transformers (ViT) are more robust to natural patch corruption than Convolutional Neural Networks (CNNs like ResNet), but more vulnerable to adversarial patch perturbations. 2. Understanding: The authors reveal that the self-attention mechanism of ViT can effectively ignore natural corrupted patches to maintain correct predictions, but can also be easily fooled by adversarial patches to make mistakes. 3. Improvement: Inspired by their analysis, the authors propose Smoothed Attention to improve the robustness of ViT against adversarial patch attacks by discouraging attention to focus too strongly on any single patch.In summary, the key contributions are carefully evaluating the robustness of ViT versus CNNs to different types of patch perturbations, analyzing the role of the self-attention mechanism in ViT's robustness properties, and proposing a method to improve ViT's robustness against adversarial patches based on these insights. Thecombination of empirical findings, analysis to develop understanding, and an improvement method based on that understanding make up the main contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper investigates the robustness of vision transformers (ViTs) compared to convolutional neural networks (CNNs) when image patches are naturally corrupted or adversarially perturbed, finding that ViTs are more robust to natural corruptions but more vulnerable to adversarial patches due to differences in their attention mechanisms.


## How does this paper compare to other research in the same field?

This paper investigates the robustness of vision transformers (ViTs) to patch-wise perturbations, compared to convolutional neural networks (CNNs). The key findings and how they relate to previous work are:1. ViTs are more robust to natural patch corruption than CNNs. This aligns with previous work showing ViTs are more robust to common corruptions overall. However, this paper looks specifically at patch-level corruptions.2. ViTs are more vulnerable to adversarial patch attacks than CNNs. Prior work has explored adversarial robustness of ViTs vs CNNs with image-level perturbations, with mixed results. This paper provides evidence ViTs are weaker against patch attacks. 3. The self-attention mechanism in ViTs allows ignoring natural corruption but also makes models susceptible to adversarial patches. This provides a novel analysis and explanation for the robustness differences, enabled by visualizing attention.4. A simple method of smoothing attention by temperature scaling improves adversarial robustness of ViTs. This validates the attention analysis and provides a way to enhance ViT robustness based on interpretability.Overall, this paper provides new insights into ViT robustness through targeted patch-level experiments and attention analysis. The findings on natural vs adversarial robustness tradeoffs, the role of attention manipulation, and a way to improve adversarial robustness specifically advance understanding in this area. The patch-based perspective aligns well with the ViT architecture and allows direct comparison to CNNs.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest are:- Exploring the robustness of different ViT variants and hybrid architectures to patch perturbations. The authors evaluated standard ViT, CNNs like ResNet, and a hybrid LeViT model, but suggest examining other emerging ViT architectures as well.- Studying how modifications to the self-attention mechanism in ViTs could improve robustness to adversarial patches. The authors propose smoothed attention as one method, but other techniques could be explored.- Considering different types of adversarial patch attacks, like imperceptible or targeted attacks. The authors evaluated some variations, but more could be tested.- Extending the understanding and improvement of ViT robustness to other patch-based inputs beyond images, such as video or point clouds. - Developing more standardized methodology and models for evaluating and comparing robustness of different architectures to patch perturbations. The authors emphasize the need for fair comparisons.- Exploring the theoretical connections between ViT architectures and their robustness properties related to patches and attention mechanisms.- Studying how patch perturbations interact with other forms of adversarial attacks on full images, not just patches.So in summary, the authors highlight the need for further work in: testing new models and attacks, improving techniques like attention mechanisms, generalization beyond images, standardized evaluation, theoretical analysis, and connections to other adversarial attack research areas. Their work provides a foundation, but opens up many avenues for future exploration.
