# [Differentiable Architecture Search with Random Features](https://arxiv.org/abs/2208.08835)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we alleviate the performance collapse problem in differentiable architecture search (DARTS) methods?

Specifically, the authors investigate the causes of the performance collapse issue in DARTS, where the search tends to favor skip connections too strongly, leading to sub-optimal architectures. 

Their main hypothesis is that the expressive power of the DARTS supernet is over-powerful, and by restricting it using random features (only training batch norm), they can avoid the skip connection dominance issue.

To test this, they propose RF-DARTS which trains the DARTS supernet with random conv features and only optimized batch norm. They analyze why this allows fairer competition between operations and avoids skip connection dominance.

Through experiments on CIFAR and ImageNet datasets, they demonstrate RF-DARTS can effectively solve the performance collapse issue and achieve state-of-the-art NAS performance.

In summary, the central research question is how to solve the performance collapse problem in DARTS, and the key hypothesis is that limiting supernet expressiveness with random features can achieve this.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Investigating the expressive power of the supernet in DARTS (Differentiable Architecture Search). The authors find that only training the BatchNorm (BN) layers in the supernet, rather than all the convolution and BN layers, achieves the best architecture search performance. This challenges the conventional DARTS paradigm.

2. Proposing a new DARTS variant called RF-DARTS (DARTS with Random Features) which freezes the convolution weights and only optimizes the BN affine weights during architecture search.

3. Providing analysis on how random features help resolve the performance collapse issue in DARTS by diminishing the role of skip connections as an auxiliary connection in the supernet.

4. Evaluating RF-DARTS extensively on CIFAR and ImageNet across different search spaces. RF-DARTS achieves state-of-the-art ImageNet top-1 error of 24.0% when transferred from CIFAR. RF-PCDARTS searched directly on ImageNet achieves 23.9% error.

5. Demonstrating the robustness of RF-DARTS across 3 datasets and 4 challenging search spaces. The results consistently show the effectiveness of RF-DARTS.

In summary, the key contribution is introducing random features to DARTS (only training BN in supernet) and showing its effectiveness in boosting architecture search performance. The theoretical analysis and comprehensive experiments provide insights into the expressive power of DARTS supernet and how to resolve performance collapse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new extension of the DARTS neural architecture search method called RF-DARTS, which trains the supernet using only batch normalization parameters while keeping the convolutional weights fixed, and shows this approach helps avoid performance collapse issues caused by skip connections dominating in the original DARTS method.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in neural architecture search (NAS):

- This paper proposes RF-DARTS, which trains the supernet using only Batchnorm (BN) weights while keeping convolution weights random/fixed. Most prior NAS methods like DARTS train both BN and convolution weights in the supernet. 

- The motivation is to alleviate the issue of skip connections dominating in DARTS. By training only BN weights, RF-DARTS reduces the role of skip connections in stabilizing supernet training. This is a novel approach compared to methods like DARTS- which modify the supernet architecture.

- The paper shows RF-DARTS achieves state-of-the-art NAS performance on CIFAR and ImageNet, outperforming many recent works. This demonstrates the effectiveness of this simple approach.

- RF-DARTS follows the general DARTS framework, but makes a conceptual change in how the supernet is trained. Other works like Single-Path NAS, TE-NAS take more significant deviations from DARTS.

- The analysis on how training only BN enables stable optimization is an interesting theoretical contribution. The experiments ablating supernet training schemes are also insightful.

- Overall, this paper makes both algorithmic and conceptual contributions. The idea of training only BN weights for NAS is simple yet powerful, and provides new perspective on the role of supernet optimization in DARTS. The strong empirical results validate this approach.

In summary, this paper proposes a novel training technique for DARTS-based NAS methods by only updating BN weights. This conceptually simple change provides benefits in terms of training stability and search performance. The approach and analysis stand out from prior efforts in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Further exploring the expressive power and capabilities of random features for neural architecture search. The authors show promising results using random features in RF-DARTS, but suggest there is more room to analyze the effectiveness and advantages of random features for NAS.

- Developing new NAS paradigms that go beyond the limitations of DARTS. The authors mention their counter-intuitive findings with RF-DARTS reveal issues with existing NAS methods like DARTS. They suggest breaking out of the constraints of DARTS in the future to propose entirely new NAS approaches.

- Scaling up RF-DARTS and applying it to larger datasets and more complex tasks. The authors demonstrate RF-DARTS mainly on CIFAR and ImageNet datasets for image classification. Applying and validating their approach on more challenging and diverse datasets could be an impactful next step.

- Theoretical analysis to explain why random features work well for NAS. While the authors provide some empirical analysis, they suggest formal theoretical analysis to understand the working mechanism of random features in RF-DARTS could be valuable future work.

- Addressing the expressive power gap between search and evaluation in RF-DARTS. The authors point out this power gap introduced by random features as an issue to address in future work.

- Continued ablation studies of key components of NAS methods. The analysis of supernet expressive power in this work provides insights into DARTS. Performing more ablations could further delve into the core elements of NAS.

In summary, the key opportunities highlighted are developing new paradigms beyond DARTS, scaling RF-DARTS to broader applications, theoretical understanding of random features in NAS, closing the search-eval expressiveness gap, and further ablation studies of NAS building blocks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new variant of differentiable architecture search called RF-DARTS, which trains the DARTS supernet with random features by only optimizing the affine parameters of BatchNorm while keeping the weights of convolutions fixed. This is motivated by an analysis showing that random features can avoid the gradient vanishing problem for deep models, thereby removing the need for skip connections to act as an auxiliary connection in addition to being a candidate operation. As a result, RF-DARTS reduces the tendency of DARTS to collapse to just selecting skip connections, improving search performance. Experiments across various datasets and search spaces show RF-DARTS achieves state-of-the-art performance, such as 24.0% top-1 error on ImageNet, while also demonstrating robustness. The results suggest the expressive power of the DARTS supernet is overpowered, and that random features are well suited for the differentiable architecture search paradigm.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper proposes a new method called Differentiable Architecture Search with Random Features (RF-DARTS) to improve the Differentiable Architecture Search (DARTS) approach for neural architecture search. The key idea is to modify the training of the DARTS supernet by only optimizing the batch normalization layers while keeping the weights of the convolutional layers fixed (as "random features"). 

The authors argue that standard DARTS suffers from "performance collapse" issues where skip connections tend to dominate, due to their dual role in providing useful gradients and being candidate operations. By using random features, the gradient vanishing problem is avoided so skip connections are no longer needed to stabilize training. This allows for a fairer comparison between operations during architecture search. Experiments across various datasets and search spaces demonstrate RF-DARTS achieves state-of-the-art performance, surpassing previous DARTS variants. For example, on CIFAR-10, RF-DARTS achieves 94.36% test accuracy, close to the optimal 94.37% in NAS-Bench-201. The robustness of RF-DARTS is also verified. Overall, the counter-intuitive findings suggest the expressive power of the DARTS supernet is overpowered, and that random features are "just perfect" for DARTS.
