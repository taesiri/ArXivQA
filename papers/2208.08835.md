# [Differentiable Architecture Search with Random Features](https://arxiv.org/abs/2208.08835)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we alleviate the performance collapse problem in differentiable architecture search (DARTS) methods?

Specifically, the authors investigate the causes of the performance collapse issue in DARTS, where the search tends to favor skip connections too strongly, leading to sub-optimal architectures. 

Their main hypothesis is that the expressive power of the DARTS supernet is over-powerful, and by restricting it using random features (only training batch norm), they can avoid the skip connection dominance issue.

To test this, they propose RF-DARTS which trains the DARTS supernet with random conv features and only optimized batch norm. They analyze why this allows fairer competition between operations and avoids skip connection dominance.

Through experiments on CIFAR and ImageNet datasets, they demonstrate RF-DARTS can effectively solve the performance collapse issue and achieve state-of-the-art NAS performance.

In summary, the central research question is how to solve the performance collapse problem in DARTS, and the key hypothesis is that limiting supernet expressiveness with random features can achieve this.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Investigating the expressive power of the supernet in DARTS (Differentiable Architecture Search). The authors find that only training the BatchNorm (BN) layers in the supernet, rather than all the convolution and BN layers, achieves the best architecture search performance. This challenges the conventional DARTS paradigm.

2. Proposing a new DARTS variant called RF-DARTS (DARTS with Random Features) which freezes the convolution weights and only optimizes the BN affine weights during architecture search.

3. Providing analysis on how random features help resolve the performance collapse issue in DARTS by diminishing the role of skip connections as an auxiliary connection in the supernet.

4. Evaluating RF-DARTS extensively on CIFAR and ImageNet across different search spaces. RF-DARTS achieves state-of-the-art ImageNet top-1 error of 24.0% when transferred from CIFAR. RF-PCDARTS searched directly on ImageNet achieves 23.9% error.

5. Demonstrating the robustness of RF-DARTS across 3 datasets and 4 challenging search spaces. The results consistently show the effectiveness of RF-DARTS.

In summary, the key contribution is introducing random features to DARTS (only training BN in supernet) and showing its effectiveness in boosting architecture search performance. The theoretical analysis and comprehensive experiments provide insights into the expressive power of DARTS supernet and how to resolve performance collapse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new extension of the DARTS neural architecture search method called RF-DARTS, which trains the supernet using only batch normalization parameters while keeping the convolutional weights fixed, and shows this approach helps avoid performance collapse issues caused by skip connections dominating in the original DARTS method.
