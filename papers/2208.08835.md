# [Differentiable Architecture Search with Random Features](https://arxiv.org/abs/2208.08835)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we alleviate the performance collapse problem in differentiable architecture search (DARTS) methods?

Specifically, the authors investigate the causes of the performance collapse issue in DARTS, where the search tends to favor skip connections too strongly, leading to sub-optimal architectures. 

Their main hypothesis is that the expressive power of the DARTS supernet is over-powerful, and by restricting it using random features (only training batch norm), they can avoid the skip connection dominance issue.

To test this, they propose RF-DARTS which trains the DARTS supernet with random conv features and only optimized batch norm. They analyze why this allows fairer competition between operations and avoids skip connection dominance.

Through experiments on CIFAR and ImageNet datasets, they demonstrate RF-DARTS can effectively solve the performance collapse issue and achieve state-of-the-art NAS performance.

In summary, the central research question is how to solve the performance collapse problem in DARTS, and the key hypothesis is that limiting supernet expressiveness with random features can achieve this.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Investigating the expressive power of the supernet in DARTS (Differentiable Architecture Search). The authors find that only training the BatchNorm (BN) layers in the supernet, rather than all the convolution and BN layers, achieves the best architecture search performance. This challenges the conventional DARTS paradigm.

2. Proposing a new DARTS variant called RF-DARTS (DARTS with Random Features) which freezes the convolution weights and only optimizes the BN affine weights during architecture search.

3. Providing analysis on how random features help resolve the performance collapse issue in DARTS by diminishing the role of skip connections as an auxiliary connection in the supernet.

4. Evaluating RF-DARTS extensively on CIFAR and ImageNet across different search spaces. RF-DARTS achieves state-of-the-art ImageNet top-1 error of 24.0% when transferred from CIFAR. RF-PCDARTS searched directly on ImageNet achieves 23.9% error.

5. Demonstrating the robustness of RF-DARTS across 3 datasets and 4 challenging search spaces. The results consistently show the effectiveness of RF-DARTS.

In summary, the key contribution is introducing random features to DARTS (only training BN in supernet) and showing its effectiveness in boosting architecture search performance. The theoretical analysis and comprehensive experiments provide insights into the expressive power of DARTS supernet and how to resolve performance collapse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new extension of the DARTS neural architecture search method called RF-DARTS, which trains the supernet using only batch normalization parameters while keeping the convolutional weights fixed, and shows this approach helps avoid performance collapse issues caused by skip connections dominating in the original DARTS method.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in neural architecture search (NAS):

- This paper proposes RF-DARTS, which trains the supernet using only Batchnorm (BN) weights while keeping convolution weights random/fixed. Most prior NAS methods like DARTS train both BN and convolution weights in the supernet. 

- The motivation is to alleviate the issue of skip connections dominating in DARTS. By training only BN weights, RF-DARTS reduces the role of skip connections in stabilizing supernet training. This is a novel approach compared to methods like DARTS- which modify the supernet architecture.

- The paper shows RF-DARTS achieves state-of-the-art NAS performance on CIFAR and ImageNet, outperforming many recent works. This demonstrates the effectiveness of this simple approach.

- RF-DARTS follows the general DARTS framework, but makes a conceptual change in how the supernet is trained. Other works like Single-Path NAS, TE-NAS take more significant deviations from DARTS.

- The analysis on how training only BN enables stable optimization is an interesting theoretical contribution. The experiments ablating supernet training schemes are also insightful.

- Overall, this paper makes both algorithmic and conceptual contributions. The idea of training only BN weights for NAS is simple yet powerful, and provides new perspective on the role of supernet optimization in DARTS. The strong empirical results validate this approach.

In summary, this paper proposes a novel training technique for DARTS-based NAS methods by only updating BN weights. This conceptually simple change provides benefits in terms of training stability and search performance. The approach and analysis stand out from prior efforts in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Further exploring the expressive power and capabilities of random features for neural architecture search. The authors show promising results using random features in RF-DARTS, but suggest there is more room to analyze the effectiveness and advantages of random features for NAS.

- Developing new NAS paradigms that go beyond the limitations of DARTS. The authors mention their counter-intuitive findings with RF-DARTS reveal issues with existing NAS methods like DARTS. They suggest breaking out of the constraints of DARTS in the future to propose entirely new NAS approaches.

- Scaling up RF-DARTS and applying it to larger datasets and more complex tasks. The authors demonstrate RF-DARTS mainly on CIFAR and ImageNet datasets for image classification. Applying and validating their approach on more challenging and diverse datasets could be an impactful next step.

- Theoretical analysis to explain why random features work well for NAS. While the authors provide some empirical analysis, they suggest formal theoretical analysis to understand the working mechanism of random features in RF-DARTS could be valuable future work.

- Addressing the expressive power gap between search and evaluation in RF-DARTS. The authors point out this power gap introduced by random features as an issue to address in future work.

- Continued ablation studies of key components of NAS methods. The analysis of supernet expressive power in this work provides insights into DARTS. Performing more ablations could further delve into the core elements of NAS.

In summary, the key opportunities highlighted are developing new paradigms beyond DARTS, scaling RF-DARTS to broader applications, theoretical understanding of random features in NAS, closing the search-eval expressiveness gap, and further ablation studies of NAS building blocks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new variant of differentiable architecture search called RF-DARTS, which trains the DARTS supernet with random features by only optimizing the affine parameters of BatchNorm while keeping the weights of convolutions fixed. This is motivated by an analysis showing that random features can avoid the gradient vanishing problem for deep models, thereby removing the need for skip connections to act as an auxiliary connection in addition to being a candidate operation. As a result, RF-DARTS reduces the tendency of DARTS to collapse to just selecting skip connections, improving search performance. Experiments across various datasets and search spaces show RF-DARTS achieves state-of-the-art performance, such as 24.0% top-1 error on ImageNet, while also demonstrating robustness. The results suggest the expressive power of the DARTS supernet is overpowered, and that random features are well suited for the differentiable architecture search paradigm.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper proposes a new method called Differentiable Architecture Search with Random Features (RF-DARTS) to improve the Differentiable Architecture Search (DARTS) approach for neural architecture search. The key idea is to modify the training of the DARTS supernet by only optimizing the batch normalization layers while keeping the weights of the convolutional layers fixed (as "random features"). 

The authors argue that standard DARTS suffers from "performance collapse" issues where skip connections tend to dominate, due to their dual role in providing useful gradients and being candidate operations. By using random features, the gradient vanishing problem is avoided so skip connections are no longer needed to stabilize training. This allows for a fairer comparison between operations during architecture search. Experiments across various datasets and search spaces demonstrate RF-DARTS achieves state-of-the-art performance, surpassing previous DARTS variants. For example, on CIFAR-10, RF-DARTS achieves 94.36% test accuracy, close to the optimal 94.37% in NAS-Bench-201. The robustness of RF-DARTS is also verified. Overall, the counter-intuitive findings suggest the expressive power of the DARTS supernet is overpowered, and that random features are "just perfect" for DARTS.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a differentiable architecture search method called RF-DARTS, which is an extension of the DARTS algorithm. The key idea is to only train the batch normalization (BN) layers in the supernet during architecture search while keeping the weights of the convolution layers fixed (as "random features"). Compared to standard DARTS which trains both BN and convolution layers, RF-DARTS avoids the issue of skip connections dominating during architecture search, leading to more effective operation selection. By analyzing the variance of gradients, the authors show that training only BN is sufficient to avoid gradient vanishing issues in deep supernets. Empirically, RF-DARTS achieves state-of-the-art performance on CIFAR and ImageNet benchmarks while also demonstrating robustness across different search spaces. The simplicity yet surprising effectiveness of only training BN highlights the need to rethink conventions in differentiable NAS methods.


## What problem or question is the paper addressing?

 The paper is addressing the issue of performance collapse in differentiable architecture search (DARTS). Specifically, it investigates the reasons why DARTS tends to converge to suboptimal architectures that heavily rely on skip connections, resulting in poor performance when evaluated. The key question is how to alleviate this performance collapse issue in DARTS.

The main contributions of the paper are:

1. Investigating the expressive power of different learnable modules (convolution, BatchNorm) in the DARTS supernet. The authors find that only training BatchNorm can actually achieve better search performance than jointly training convolution and BatchNorm like in standard DARTS.

2. Theoretically analyzing why training only BatchNorm (random features) can help resolve the performance collapse issue - by diminishing the role of skip connections as an auxiliary connection for stabilizing training, making the search focus more on the actual operation selection.

3. Proposing RF-DARTS which trains the DARTS supernet with only BatchNorm, and showing through experiments that it achieves significantly better performance across various datasets and search spaces.

4. Proposing RF-PCDARTS which applies the BatchNorm-only training to PC-DARTS and achieves state-of-the-art results on ImageNet.

In summary, the key focus is understanding and alleviating the performance collapse problem in differentiable NAS methods like DARTS, by investigating the supernet training procedure. The proposed RF-DARTS provides a simple yet effective solution.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Differentiable Architecture Search (DARTS): A neural architecture search method that relaxes the search space to be continuous and optimizes the architecture parameters with gradient descent. 

- Performance Collapse: A problem suffered by DARTS where the searched architecture tends to choose skip connections over other operations, leading to suboptimal performance.

- Supernet: The over-parameterized network used by DARTS to encode all candidate architectures during architecture search.

- Random Features: Fixing the weights of convolutional layers in the supernet and only training Batch Normalization, which helps avoid performance collapse. 

- Expressive Power: The representation capacity of different trainable modules (convolutions, BN) in the DARTS supernet.

- Skip Connections: Operations in the DARTS search space that act as both shortcuts to help supernet training and as candidate operations.

- Batch Normalization: A technique to normalize layer inputs that has affine parameters that can be trained in the DARTS supernet.

- Robustness: Testing the method across different datasets and search spaces to show its generalizability.

In summary, the key ideas are using random features to avoid performance collapse in DARTS, analyzing the expressive power of different supernet modules, and showing the robustness of the approach across datasets and search spaces.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing this paper:

1. What is the key problem this paper aims to solve? 

2. What is the proposed method RF-DARTS and how does it work?

3. What are the main contributions of this paper?

4. What are the key components and assumptions of the original DARTS method? How does RF-DARTS differ?

5. What analysis does the paper provide on the role of skip connections in DARTS and how RF-DARTS addresses this? 

6. What experiments were conducted to evaluate RF-DARTS? What datasets were used?

7. What were the main experimental results? How did RF-DARTS compare to other methods?

8. What are the limitations or potential negative societal impacts identified by the authors? 

9. What conclusions or future work do the authors suggest based on this research?

10. How does this research fit into the broader landscape of work on neural architecture search and differentiable methods? Does it open up new research directions?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a new extension of DARTS called RF-DARTS that trains the supernet with only batch normalization layers and keeps the convolutional weights fixed. Why does this approach result in better performance compared to standard DARTS? What is the intuition behind this?

2. The authors claim RF-DARTS alleviates the performance collapse problem in DARTS due to the unfair advantage of skip connections. How specifically does training only batch normalization weights and fixing convolutional weights remove this unfair advantage? 

3. The authors provide a theoretical analysis of how training only BN helps avoid the gradient vanishing problem. Can you walk through this analysis step-by-step and explain the key insights? How does fixing convolutional weights lead to more stable gradient propagation?

4. Fig. 1 shows an unintuitive result - supernet training with only BN outperforms joint training of BN and conv layers. What implications does this have on the role of supernet expressive power in architecture search?

5. How exactly does random feature optimization in RF-DARTS dilute the role of skip connections? Does this stem primarily from removing the auxiliary connection role or some other effect?

6. The authors claim RF-DARTS makes "fairer operation selection" by diminishing the role of skip connections. What specifically is "unfair" about skip connections in DARTS and how does RF-DARTS ameliorate this?

7. Are there any potential downsides or limitations to the RF-DARTS approach? For instance, does fixing convolutional weights limit the search space or final architecture performance in any way?

8. The comparison between RF-DARTS and BN-NAS highlights key differences in motivation and applicability. Can you summarize the key differences between these two methods? When is each one preferable?

9. How broadly applicable is the RF-DARTS approach? Could it be extended to other NAS methods beyond DARTS or does it rely on specific properties of the DARTS formulation?

10. The method trains supernet with mismatched capacity between search and evaluation. Does this optimization gap remain a concern? How could RF-DARTS be modified or improved to better align search and evaluation?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes RF-DARTS, an improved differentiable architecture search method that alleviates the issue of performance collapse in vanilla DARTS. The authors first investigate the expressive power of DARTS supernet by examining different combinations of trainable modules, finding that optimizing only BatchNorm (BN) layers surprisingly leads to the best architecture search results. They then theoretically analyze how random features provided by fixed convolutional weights and scaled BN layers can mitigate the dominance of skip connections for gradient flow in DARTS, enabling fairer operation selection. Based on these analyses, RF-DARTS incorporates random features into DARTS and PC-DARTS. Experiments across datasets and search spaces demonstrate RF-DARTS achieves state-of-the-art performance, including 94.36% test accuracy on CIFAR-10 and 24.0% top-1 error on ImageNet when transferred from CIFAR-10. The analyses and results provide new insights into the working mechanism of DARTS and the efficacy of random features for differentiable NAS.


## Summarize the paper in one sentence.

 The paper proposes RF-DARTS, which trains the supernet with random features by only optimizing BatchNorm parameters, to solve the performance collapse issue in differentiable architecture search.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes a new version of differentiable architecture search (DARTS) called RF-DARTS which uses random features to alleviate the performance collapse problem in vanilla DARTS. The key idea is to only train the BatchNorm parameters in the DARTS supernet while freezing the convolution weights. Through experiments and analysis, the authors find that this leads to better architecture search compared to jointly training all weights like in DARTS. They argue random features dilute the role of skip connections as an auxiliary shortcut for optimizing the supernet, allowing the search algorithm to focus on fairer operation selection. RF-DARTS obtains state-of-the-art results on CIFAR and ImageNet benchmarks. The expressive power of supernet in DARTS is analyzed and it is shown that only training BatchNorm provides just enough representational power for effective architecture search. Overall, this work provides new insights into the working mechanism of DARTS using random features.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. This paper proposes RF-DARTS, which trains the DARTS supernet with only batch normalization layers while keeping the weights of convolution layers random (untrained). Why does this approach help alleviate the issue of skip connections dominating during architecture search?

2. The paper finds that training a DARTS supernet with only batch normalization ("Option C") leads to better architecture search performance than jointly training batchnorm and convolutions ("Option B") or only training convolutions ("Option A"). What does this suggest about the expressive power needed in the supernet during architecture search?

3. How does fixing the convolution weights to random initialization and only training batchnorm help avoid the issue of vanishing gradients in very deep neural networks? Explain the theoretical analysis provided in Section 3.2.

4. How exactly does using random convolution weights help diminish the role of skip connections in stabilizing supernet training, as discussed in Section 3.3? 

5. The paper introduces RF-DARTS and RF-PCDARTS. How do these methods differ from prior DARTS variants, and what are the key advantages of using random features?

6. Explain the differences between RF-DARTS and the concurrent work BN-NAS in terms of motivation, approach, and applicability to different search spaces.

7. Analyze the architecture parameters visualized in Figure 4. What do they suggest about the role skip connections play in RF-DARTS compared to vanilla DARTS?

8. Why does directly training on the target dataset lead to better architecture search performance compared to using a proxy dataset, as evidenced by the NAS-Bench-201 experiments?

9. How does the performance of RF-DARTS across the RobustDARTS S1-S4 spaces demonstrate its robustness compared to prior DARTS variants?

10. What are the practical implications of the RF-DARTS and RF-PCDARTS results on CIFAR and ImageNet? How do they advance the state-of-the-art in architecture search?
