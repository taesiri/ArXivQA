# [Differentiable Architecture Search with Random Features](https://arxiv.org/abs/2208.08835)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we alleviate the performance collapse problem in differentiable architecture search (DARTS) methods?

Specifically, the authors investigate the causes of the performance collapse issue in DARTS, where the search tends to favor skip connections too strongly, leading to sub-optimal architectures. 

Their main hypothesis is that the expressive power of the DARTS supernet is over-powerful, and by restricting it using random features (only training batch norm), they can avoid the skip connection dominance issue.

To test this, they propose RF-DARTS which trains the DARTS supernet with random conv features and only optimized batch norm. They analyze why this allows fairer competition between operations and avoids skip connection dominance.

Through experiments on CIFAR and ImageNet datasets, they demonstrate RF-DARTS can effectively solve the performance collapse issue and achieve state-of-the-art NAS performance.

In summary, the central research question is how to solve the performance collapse problem in DARTS, and the key hypothesis is that limiting supernet expressiveness with random features can achieve this.
