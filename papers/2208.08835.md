# [Differentiable Architecture Search with Random Features](https://arxiv.org/abs/2208.08835)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we alleviate the performance collapse problem in differentiable architecture search (DARTS) methods?

Specifically, the authors investigate the causes of the performance collapse issue in DARTS, where the search tends to favor skip connections too strongly, leading to sub-optimal architectures. 

Their main hypothesis is that the expressive power of the DARTS supernet is over-powerful, and by restricting it using random features (only training batch norm), they can avoid the skip connection dominance issue.

To test this, they propose RF-DARTS which trains the DARTS supernet with random conv features and only optimized batch norm. They analyze why this allows fairer competition between operations and avoids skip connection dominance.

Through experiments on CIFAR and ImageNet datasets, they demonstrate RF-DARTS can effectively solve the performance collapse issue and achieve state-of-the-art NAS performance.

In summary, the central research question is how to solve the performance collapse problem in DARTS, and the key hypothesis is that limiting supernet expressiveness with random features can achieve this.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Investigating the expressive power of the supernet in DARTS (Differentiable Architecture Search). The authors find that only training the BatchNorm (BN) layers in the supernet, rather than all the convolution and BN layers, achieves the best architecture search performance. This challenges the conventional DARTS paradigm.

2. Proposing a new DARTS variant called RF-DARTS (DARTS with Random Features) which freezes the convolution weights and only optimizes the BN affine weights during architecture search.

3. Providing analysis on how random features help resolve the performance collapse issue in DARTS by diminishing the role of skip connections as an auxiliary connection in the supernet.

4. Evaluating RF-DARTS extensively on CIFAR and ImageNet across different search spaces. RF-DARTS achieves state-of-the-art ImageNet top-1 error of 24.0% when transferred from CIFAR. RF-PCDARTS searched directly on ImageNet achieves 23.9% error.

5. Demonstrating the robustness of RF-DARTS across 3 datasets and 4 challenging search spaces. The results consistently show the effectiveness of RF-DARTS.

In summary, the key contribution is introducing random features to DARTS (only training BN in supernet) and showing its effectiveness in boosting architecture search performance. The theoretical analysis and comprehensive experiments provide insights into the expressive power of DARTS supernet and how to resolve performance collapse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new extension of the DARTS neural architecture search method called RF-DARTS, which trains the supernet using only batch normalization parameters while keeping the convolutional weights fixed, and shows this approach helps avoid performance collapse issues caused by skip connections dominating in the original DARTS method.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in neural architecture search (NAS):

- This paper proposes RF-DARTS, which trains the supernet using only Batchnorm (BN) weights while keeping convolution weights random/fixed. Most prior NAS methods like DARTS train both BN and convolution weights in the supernet. 

- The motivation is to alleviate the issue of skip connections dominating in DARTS. By training only BN weights, RF-DARTS reduces the role of skip connections in stabilizing supernet training. This is a novel approach compared to methods like DARTS- which modify the supernet architecture.

- The paper shows RF-DARTS achieves state-of-the-art NAS performance on CIFAR and ImageNet, outperforming many recent works. This demonstrates the effectiveness of this simple approach.

- RF-DARTS follows the general DARTS framework, but makes a conceptual change in how the supernet is trained. Other works like Single-Path NAS, TE-NAS take more significant deviations from DARTS.

- The analysis on how training only BN enables stable optimization is an interesting theoretical contribution. The experiments ablating supernet training schemes are also insightful.

- Overall, this paper makes both algorithmic and conceptual contributions. The idea of training only BN weights for NAS is simple yet powerful, and provides new perspective on the role of supernet optimization in DARTS. The strong empirical results validate this approach.

In summary, this paper proposes a novel training technique for DARTS-based NAS methods by only updating BN weights. This conceptually simple change provides benefits in terms of training stability and search performance. The approach and analysis stand out from prior efforts in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Further exploring the expressive power and capabilities of random features for neural architecture search. The authors show promising results using random features in RF-DARTS, but suggest there is more room to analyze the effectiveness and advantages of random features for NAS.

- Developing new NAS paradigms that go beyond the limitations of DARTS. The authors mention their counter-intuitive findings with RF-DARTS reveal issues with existing NAS methods like DARTS. They suggest breaking out of the constraints of DARTS in the future to propose entirely new NAS approaches.

- Scaling up RF-DARTS and applying it to larger datasets and more complex tasks. The authors demonstrate RF-DARTS mainly on CIFAR and ImageNet datasets for image classification. Applying and validating their approach on more challenging and diverse datasets could be an impactful next step.

- Theoretical analysis to explain why random features work well for NAS. While the authors provide some empirical analysis, they suggest formal theoretical analysis to understand the working mechanism of random features in RF-DARTS could be valuable future work.

- Addressing the expressive power gap between search and evaluation in RF-DARTS. The authors point out this power gap introduced by random features as an issue to address in future work.

- Continued ablation studies of key components of NAS methods. The analysis of supernet expressive power in this work provides insights into DARTS. Performing more ablations could further delve into the core elements of NAS.

In summary, the key opportunities highlighted are developing new paradigms beyond DARTS, scaling RF-DARTS to broader applications, theoretical understanding of random features in NAS, closing the search-eval expressiveness gap, and further ablation studies of NAS building blocks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new variant of differentiable architecture search called RF-DARTS, which trains the DARTS supernet with random features by only optimizing the affine parameters of BatchNorm while keeping the weights of convolutions fixed. This is motivated by an analysis showing that random features can avoid the gradient vanishing problem for deep models, thereby removing the need for skip connections to act as an auxiliary connection in addition to being a candidate operation. As a result, RF-DARTS reduces the tendency of DARTS to collapse to just selecting skip connections, improving search performance. Experiments across various datasets and search spaces show RF-DARTS achieves state-of-the-art performance, such as 24.0% top-1 error on ImageNet, while also demonstrating robustness. The results suggest the expressive power of the DARTS supernet is overpowered, and that random features are well suited for the differentiable architecture search paradigm.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper proposes a new method called Differentiable Architecture Search with Random Features (RF-DARTS) to improve the Differentiable Architecture Search (DARTS) approach for neural architecture search. The key idea is to modify the training of the DARTS supernet by only optimizing the batch normalization layers while keeping the weights of the convolutional layers fixed (as "random features"). 

The authors argue that standard DARTS suffers from "performance collapse" issues where skip connections tend to dominate, due to their dual role in providing useful gradients and being candidate operations. By using random features, the gradient vanishing problem is avoided so skip connections are no longer needed to stabilize training. This allows for a fairer comparison between operations during architecture search. Experiments across various datasets and search spaces demonstrate RF-DARTS achieves state-of-the-art performance, surpassing previous DARTS variants. For example, on CIFAR-10, RF-DARTS achieves 94.36% test accuracy, close to the optimal 94.37% in NAS-Bench-201. The robustness of RF-DARTS is also verified. Overall, the counter-intuitive findings suggest the expressive power of the DARTS supernet is overpowered, and that random features are "just perfect" for DARTS.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a differentiable architecture search method called RF-DARTS, which is an extension of the DARTS algorithm. The key idea is to only train the batch normalization (BN) layers in the supernet during architecture search while keeping the weights of the convolution layers fixed (as "random features"). Compared to standard DARTS which trains both BN and convolution layers, RF-DARTS avoids the issue of skip connections dominating during architecture search, leading to more effective operation selection. By analyzing the variance of gradients, the authors show that training only BN is sufficient to avoid gradient vanishing issues in deep supernets. Empirically, RF-DARTS achieves state-of-the-art performance on CIFAR and ImageNet benchmarks while also demonstrating robustness across different search spaces. The simplicity yet surprising effectiveness of only training BN highlights the need to rethink conventions in differentiable NAS methods.


## What problem or question is the paper addressing?

 The paper is addressing the issue of performance collapse in differentiable architecture search (DARTS). Specifically, it investigates the reasons why DARTS tends to converge to suboptimal architectures that heavily rely on skip connections, resulting in poor performance when evaluated. The key question is how to alleviate this performance collapse issue in DARTS.

The main contributions of the paper are:

1. Investigating the expressive power of different learnable modules (convolution, BatchNorm) in the DARTS supernet. The authors find that only training BatchNorm can actually achieve better search performance than jointly training convolution and BatchNorm like in standard DARTS.

2. Theoretically analyzing why training only BatchNorm (random features) can help resolve the performance collapse issue - by diminishing the role of skip connections as an auxiliary connection for stabilizing training, making the search focus more on the actual operation selection.

3. Proposing RF-DARTS which trains the DARTS supernet with only BatchNorm, and showing through experiments that it achieves significantly better performance across various datasets and search spaces.

4. Proposing RF-PCDARTS which applies the BatchNorm-only training to PC-DARTS and achieves state-of-the-art results on ImageNet.

In summary, the key focus is understanding and alleviating the performance collapse problem in differentiable NAS methods like DARTS, by investigating the supernet training procedure. The proposed RF-DARTS provides a simple yet effective solution.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Differentiable Architecture Search (DARTS): A neural architecture search method that relaxes the search space to be continuous and optimizes the architecture parameters with gradient descent. 

- Performance Collapse: A problem suffered by DARTS where the searched architecture tends to choose skip connections over other operations, leading to suboptimal performance.

- Supernet: The over-parameterized network used by DARTS to encode all candidate architectures during architecture search.

- Random Features: Fixing the weights of convolutional layers in the supernet and only training Batch Normalization, which helps avoid performance collapse. 

- Expressive Power: The representation capacity of different trainable modules (convolutions, BN) in the DARTS supernet.

- Skip Connections: Operations in the DARTS search space that act as both shortcuts to help supernet training and as candidate operations.

- Batch Normalization: A technique to normalize layer inputs that has affine parameters that can be trained in the DARTS supernet.

- Robustness: Testing the method across different datasets and search spaces to show its generalizability.

In summary, the key ideas are using random features to avoid performance collapse in DARTS, analyzing the expressive power of different supernet modules, and showing the robustness of the approach across datasets and search spaces.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing this paper:

1. What is the key problem this paper aims to solve? 

2. What is the proposed method RF-DARTS and how does it work?

3. What are the main contributions of this paper?

4. What are the key components and assumptions of the original DARTS method? How does RF-DARTS differ?

5. What analysis does the paper provide on the role of skip connections in DARTS and how RF-DARTS addresses this? 

6. What experiments were conducted to evaluate RF-DARTS? What datasets were used?

7. What were the main experimental results? How did RF-DARTS compare to other methods?

8. What are the limitations or potential negative societal impacts identified by the authors? 

9. What conclusions or future work do the authors suggest based on this research?

10. How does this research fit into the broader landscape of work on neural architecture search and differentiable methods? Does it open up new research directions?
