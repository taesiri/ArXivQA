# [When Parameter-efficient Tuning Meets General-purpose Vision-language   Models](https://arxiv.org/abs/2312.12458)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses two main limitations of existing works on instruction tuning of large pre-trained models: 1) The high training costs and heavy computing resource dependence due to full model fine-tuning, and 2) The lack of semantic information in instructions, which hinders effective multimodal alignment.

Proposed Solution: 
The paper proposes PETAL, a novel approach for parameter-efficient tuning of general-purpose vision-language models. PETAL has two main components:

1. Efficient Fine-Tuning Framework: PETAL establishes an efficient framework based on dynamic mode approximation, which significantly reduces training costs and reliance on heavy computing by only requiring 0.5% of the total parameters.

2. Enhanced Instruction Semantics: PETAL enhances instruction semantics in two ways - introducing adaptive instruction mixture-of-experts (MOEs) to extract features from multiple perspectives, and fortifying the linkage between parameter-efficient tuning and mutual information using a scored-based information bottleneck.

By combining efficient learning with enhanced instructions, PETAL achieves efficient fine-tuning of large multimodal models while maintaining high performance.

Main Contributions:
1. Novel efficient tuning framework using dynamic mode approximation for efficiency.  

2. Adaptive instruction MOEs and scored-based mutual information loss to boost instruction semantics and alignment of modalities.

3. Demonstrated superiority over previous methods, with substantial performance gains on five benchmark datasets, while only using 0.5% of the trainable parameters.

In summary, the paper makes pivotal contributions in enhancing the accessibility and utility of powerful pre-trained models for a wider range of applications through innovative techniques for efficient tuning and enhanced instruction semantics.
