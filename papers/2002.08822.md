# [Automatic Shortcut Removal for Self-Supervised Representation Learning](https://arxiv.org/abs/2002.08822)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we automatically remove "shortcut" features from images used for self-supervised pretraining in order to improve the representations learned by neural networks? The key ideas and approach are:- Self-supervised pretraining tasks like predicting image rotations often rely on "shortcut" features like watermarks or chromatic aberrations rather than learning higher-level semantic features. This limits their usefulness for transfer learning.- The authors propose training an adversarial "lens" network to perturb images before feeding them to the feature extraction network. The lens is trained to minimally modify images in a way that reduces performance on the pretraining task. - This forces the feature extraction network to rely less on shortcut features and learn more robust representations. Combining representations from original and lensed images helps transferability.- Experiments across multiple pretraining tasks and datasets show improved transfer learning performance with the adversarial lens, confirming that it helps remove shortcut features.In summary, the central hypothesis is that an adversarially trained lens can automatically remove shortcuts relied upon in self-supervised pretraining, improving the learned representations. The method and experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The key contributions of this paper appear to be:- Proposing a general framework for automatically removing shortcut features in self-supervised visual representation learning. This involves training an adversarial "lens" network to modify images in a way that makes it harder for the feature extraction network to solve the pretext task.- Demonstrating that this automatic shortcut removal improves representation quality across a variety of common pretext tasks (rotation prediction, exemplar, relative patch location, jigsaw) and datasets (ImageNet, YouTube-8M frames). The method can replace hand-engineered data augmentations designed to remove shortcuts.- Using the lens to visualize and compare shortcut features across different pretext tasks and datasets. This analysis provides insights into the types of shortcuts that are specific to tasks and datasets. For example, the analysis reveals that rotation prediction relies heavily on text and logos, while exemplar uses average image color.- Showing that the lens can help compensate for the lower quality of web-scraped datasets like YouTube-8M compared to curated image datasets. The lens removes dataset-specific biases like over-representation of certain classes.- Providing evidence that removing shortcuts encourages learning of more abstract, semantic representations. For example, networks trained with the lens are more likely to make shape-based rather than texture-based decisions.In summary, the main contribution is a general framework for automatically identifying and removing shortcuts in self-supervised learning. This improves representation learning, provides insights into shortcuts, and helps exploit web-scale data.


## How does this paper compare to other research in the same field?

This paper presents an approach for removing shortcut features in self-supervised learning to improve the quality of learned visual representations. Here is a comparison to related work in this field:- Self-supervised learning for visual representations has become very popular in recent years. Many papers have proposed different "pretext tasks" like predicting image rotations, solving jigsaw puzzles, colorization, etc. A key challenge is that networks can find shortcuts like chromatic aberrations instead of learning high-level features.- Several papers have identified specific shortcuts and proposed specialized data augmentation techniques to mitigate them, like color dropping, spatial jittering, etc. This paper proposes a more general framework to automatically identify and remove shortcuts.- The idea of using adversarial training to make a pretext task harder was explored before in limited contexts, for example to avoid image artifacts. This paper generalizes adversarial shortcut removal to arbitrary tasks. - The lens network is conceptually related to image-to-image translation networks like CycleGAN, but applied to remove shortcuts here. Using a lightweight network makes this method efficient.- Analyzing the modifications made by the lens provides insights into the shortcut features learned by different self-supervised tasks, which was not done before.- The extensive experiments on multiple datasets and tasks demonstrate the broad applicability of this framework for improving self-supervision.In summary, this paper makes contributions in automatically removing shortcuts, providing interpretability, and showing consistent gains across diverse self-supervised learning methods. The lens framework seems widely applicable for improving representation learning.
