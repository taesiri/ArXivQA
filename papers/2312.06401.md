# [Compound Text-Guided Prompt Tuning via Image-Adaptive Cues](https://arxiv.org/abs/2312.06401)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel prompt tuning framework called Compound Text-Guided Prompt Tuning (TGP-T) for adapting vision-language models (VLMs) like CLIP to downstream tasks. TGP-T introduces text supervision to guide the optimization of prompts, enabling two key benefits: 1) releasing reliance on pre-defined category names during inference, allowing more flexible prompt generation, and 2) reducing the number of textual inputs to just two instead of one per category, significantly decreasing GPU memory consumption. Specifically, compound text supervisions consisting of category-wise and content-wise descriptions are highly effective by providing inter-class separability and capturing intra-class variations, respectively. Additionally, a lightweight Bonder module aligns the generated prompts with visual features to better harness the VLM. Extensive experiments on few-shot recognition and domain generalization datasets demonstrate TGP-T's superior performance with lower training costs, reducing GPU usage by 93% and achieving a 2.5% increase in 16-shot ImageNet accuracy. The compound text supervision and efficient design of TGP-T facilitate prompt tuning for large-scale datasets across various scenarios.
