# Contrastive Decoding: Open-ended Text Generation as Optimization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be:By simply contrasting two frozen language models of different sizes, it is possible to decode higher quality text than from the larger LM alone, without any additional training.The key ideas are:- Larger LMs exhibit less undesirable behaviors like repetition and incoherence compared to smaller LMs.- By maximizing the difference in log probabilities between a large "expert" LM and a small "amateur" LM, the text generated will amplify the desirable qualities of the expert LM while reducing the amateur tendencies. - This contrastive decoding approach requires no extra training, just two pre-trained LMs. It outperforms common decoding methods like nucleus sampling in coherence while maintaining fluency and diversity.So in summary, the central hypothesis is that contrastive decoding, by exploiting the differences between large and small LMs, can generate better text than simply decoding from the large LM alone. This is tested without any model training, only using pre-trained LMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new decoding method called contrastive decoding (CD) for open-ended text generation. The key idea is to optimize a contrastive objective that maximizes the difference between the log-likelihood of a large "expert" LM and a small "amateur" LM. - Introducing an adaptive plausibility constraint to complement the contrastive objective. This avoids false positives and false negatives.- Showing that CD produces higher quality text than standard decoding methods like nucleus sampling, in terms of coherence and fluency.- Demonstrating that CD works well across different model scales (e.g. GPT-2 1.5B, OPT-13B) and domains (Wikipedia, news, stories).- Providing analysis to show that the contrastive objective appropriately scores good continuations higher and repetitive/incoherent text lower compared to likelihood.- Giving a pragmatic interpretation of CD through theories of cooperative communication.- Performing ablations to validate design choices like search vs sampling, choice of amateur LM, and the importance of the plausibility constraint.In summary, the main contribution is proposing contrastive decoding as a way to improve open-ended text generation by exploiting contrasts between large and small LMs, without any model re-training. The method is shown to work well empirically across models and domains.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a summary of how it compares to other related research:- The paper proposes a new decoding approach called contrastive decoding for open-ended text generation. This approach contrasts two off-the-shelf language models of different sizes - a large "expert" LM and a smaller "amateur" LM - in order to produce higher quality and more coherent text.- The idea of using contrastive objectives for text generation has been explored before, but most prior work requires additional training or fine-tuning of the models. A key contribution of this paper is showing that simply contrasting two frozen, off-the-shelf LMs can improve generation without any training.- The most similar prior work is DExperts (Liu et al. 2021), which also exploited contrasts between models. However, DExperts focused on controlled generation and required training an "anti-expert" model. This paper focuses on open-ended generation and uses off-the-shelf amateur LMs, making the approach simpler.- Other related works have used model contrasts for goals like improving diversity or faithfulness (Li et al. 2016, Maynez et al. 2020). However, those also require training or fine-tuning models. This work is novel in using frozen LMs for decoding.- Compared to other decoding methods like nucleus sampling, top-k sampling, or beam search, this paper shows both automatic and human evaluations demonstrating that contrastive decoding improves coherence without sacrificing fluency or diversity.In summary, the key novelties of this paper compared to prior art seem to be 1) exploiting contrasts in frozen LMs without training, and 2) demonstrating improvements in open-ended generation quality, especially coherence. The simplicity and strong empirical results are major advantages over related works.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring contrastive decoding in other text generation settings like summarization, translation, etc. The authors mention that contrastive decoding is designed for open-ended generation and may not directly apply to more goal-driven tasks. Further research could explore adapting contrastive decoding for these other tasks.- Using other methods besides model scale to create the amateur-expert contrast. The authors suggest things like using different checkpoints of the same model or models trained on different data. More research could explore other ways to construct the amateur and expert models. - Expanding beyond frozen models to do some light adaptation/training of the amateur model to better mimic expert failures. The authors experiment with a small amount of tuning the amateur, but more extensive training could be tried.- Studying other pragmatic/communicative objectives beyond the likelihood difference that could improve text quality. The authors provide a nice interpretation of contrastive decoding in terms of pragmatics, and suggest room for exploring other related objectives.- Applying contrastive decoding-style ideas to other modalities like images, audio, etc. The overall principle of contrasting models could extend to other domains.So in summary, the main directions are exploring contrastive decoding in other tasks and settings, using other amateur-expert construction methods, doing more training of the amateur, developing new related objectives, and extending the ideas to other data modalities. The authors position contrastive decoding as a general framework applicable in many settings.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new decoding approach called contrastive decoding (CD) for open-ended text generation. CD takes an off-the-shelf large language model (expert LM) and small language model (amateur LM) without any additional training. It formulates a novel contrastive objective that rewards fluent text favored by the expert LM while penalizing repetitive or incoherent text preferred by the amateur LM. This objective is optimized via beam search, subject to a plausibility constraint that restricts the search space to tokens with sufficiently high probability under the expert LM. Experiments across news, Wikipedia, and story domains show CD significantly outperforms strong baselines like nucleus sampling and typical decoding in coherence while maintaining fluency and diversity. The benefits are shown across model scales and requires minimal overhead since the amateur LM can be quite small. CD provides a simple but effective way to decode higher quality text from existing LMs.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new decoding approach called contrastive decoding (CD) for improving open-ended text generation from large language models. The key idea is to contrast the probabilities assigned by a large "expert" LM (e.g. GPT-3) versus a smaller "amateur" LM (e.g. GPT-2). Specifically, CD searches for text that maximizes the difference in log probabilities between the expert and amateur LM, subject to a plausibility constraint that restricts the search to tokens with sufficiently high probability under the expert. CD exploits the observation that undesirable generation behaviors like repetition and incoherence are more prevalent in smaller LMs compared to larger ones. By maximizing the probability difference, CD essentially removes the amateur tendencies from the expert LM. Experiments across three domains (Wikipedia, news, stories) show CD significantly improves coherence over baselines like nucleus sampling, while maintaining fluency. CD also works well across model scales and requires no extra training. The paper provides both automatic and human evals, ablation studies, and shows CD's connection to concepts like mutual information.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new decoding approach called contrastive decoding (CD) for open-ended text generation. CD takes an off-the-shelf large language model (called the expert, e.g. GPT-3) and off-the-shelf small language model (called the amateur, e.g. GPT-2). It searches for text that maximizes the difference between the expert and amateur log probabilities (the contrastive objective), subject to a plausibility constraint that restricts the search to only plausible tokens under the expert LM. Specifically, CD computes a token-level score as the log probability ratio between the expert and amateur if the token is above a certain probability threshold under the expert, otherwise the token is assigned a very low score. CD then uses beam search to find the sequence with the highest sum of token-level scores. The key intuition is that the failures of large LMs like repetition and incoherence are even more pronounced in smaller LMs, so contrasting them will favor text with the good properties of the large LM but not the bad properties of the small LM. The plausibility constraint avoids implausible text. The main novelty is generating high quality text by simply contrasting two frozen LMs without any training.
