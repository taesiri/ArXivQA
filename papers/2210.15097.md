# Contrastive Decoding: Open-ended Text Generation as Optimization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be:By simply contrasting two frozen language models of different sizes, it is possible to decode higher quality text than from the larger LM alone, without any additional training.The key ideas are:- Larger LMs exhibit less undesirable behaviors like repetition and incoherence compared to smaller LMs.- By maximizing the difference in log probabilities between a large "expert" LM and a small "amateur" LM, the text generated will amplify the desirable qualities of the expert LM while reducing the amateur tendencies. - This contrastive decoding approach requires no extra training, just two pre-trained LMs. It outperforms common decoding methods like nucleus sampling in coherence while maintaining fluency and diversity.So in summary, the central hypothesis is that contrastive decoding, by exploiting the differences between large and small LMs, can generate better text than simply decoding from the large LM alone. This is tested without any model training, only using pre-trained LMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new decoding method called contrastive decoding (CD) for open-ended text generation. The key idea is to optimize a contrastive objective that maximizes the difference between the log-likelihood of a large "expert" LM and a small "amateur" LM. - Introducing an adaptive plausibility constraint to complement the contrastive objective. This avoids false positives and false negatives.- Showing that CD produces higher quality text than standard decoding methods like nucleus sampling, in terms of coherence and fluency.- Demonstrating that CD works well across different model scales (e.g. GPT-2 1.5B, OPT-13B) and domains (Wikipedia, news, stories).- Providing analysis to show that the contrastive objective appropriately scores good continuations higher and repetitive/incoherent text lower compared to likelihood.- Giving a pragmatic interpretation of CD through theories of cooperative communication.- Performing ablations to validate design choices like search vs sampling, choice of amateur LM, and the importance of the plausibility constraint.In summary, the main contribution is proposing contrastive decoding as a way to improve open-ended text generation by exploiting contrasts between large and small LMs, without any model re-training. The method is shown to work well empirically across models and domains.
