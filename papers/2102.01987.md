# [Learning Graph Embeddings for Compositional Zero-shot Learning](https://arxiv.org/abs/2102.01987)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to perform compositional zero-shot learning - that is, how to recognize novel compositions of observed visual primitives like objects and their states. The key hypothesis is that exploiting the dependency structure between objects, states, and their compositions within a graph convolutional network framework can allow for better generalization to novel unseen compositions.The paper proposes a new method called Compositional Graph Embedding (CGE) that models these dependencies in an end-to-end framework. The key ideas are:- Constructing a compositional graph to capture dependencies between objects, states, and compositions (both seen and unseen).- Learning a joint compatibility function between images, states, and objects that is globally consistent with respect to the compositional graph structure. - Propagating information between related concepts (e.g. old car, old dog, cute dog) through graph convolutions to support recognizing novel compositions like "old dog".So in summary, the central hypothesis is that modeling the rich dependency structure between visual primitives and compositions allows CGE to generalize better to unseen compositions compared to prior state-of-the-art methods. The experiments aim to demonstrate the superiority of the proposed CGE approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel graph formulation called Compositional Graph Embedding (CGE) for compositional zero-shot learning. Specifically:- They introduce a graph structure that models the dependencies between visual primitives (states and objects) as well as their compositions. This allows propagating knowledge from seen to unseen compositions.- They propose an end-to-end framework that jointly learns image features, compositional classifiers and representations of visual primitives using graph convolutional networks. - By learning a compatibility function between concepts, their model can generalize to unseen compositions without relying on external knowledge bases like WordNet.- They significantly outperform prior art on the challenging generalized compositional zero-shot learning setting on MIT-States, UT-Zappos datasets. - They propose a new benchmark dataset called C-GQA based on GQA with more diverse and cleaner annotations compared to existing datasets.In summary, the key contribution is a novel graph-based approach for compositional zero-shot learning that exploits dependencies between visual concepts and compositions, and jointly learns the feature extractor and classifier end-to-end. This results in state-of-the-art performance without requiring external knowledge bases.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel graph-based approach called Compositional Graph Embedding (CGE) for compositional zero-shot learning that learns dependencies between visual primitives like objects and states as well as their compositions in an end-to-end manner, outperforming prior methods on benchmark datasets.


## How does this paper compare to other research in the same field?

This paper presents a novel method for compositional zero-shot learning (CZSL) by proposing a graph formulation called Compositional Graph Embedding (CGE). Here are some key ways this paper compares to prior work on CZSL:- Most prior work treats each state-object composition independently without considering dependencies between them. This paper argues that exploiting the dependency structure provides strong regularization for generalization. The graph formulation in CGE is designed to model these dependencies.- Many previous methods rely solely on fixed image features from pretrained networks. This paper shows CGE can be trained end-to-end, learning the image features jointly with the compositional classifiers.- CGE does not require any external knowledge graph like WordNet to relate concepts. The compositional graph is constructed from the labels themselves. Other GCN-based ZSL methods rely on WordNet.- The paper shows CGE significantly outperforms prior state-of-the-art methods like TMN, SymNet, and others on standard CZSL benchmarks. The gains are attributed to the graph modeling dependencies and end-to-end training.- The paper also contributes a new and more challenging CZSL benchmark called C-GQA with over 9k concepts, to encourage further research. Most prior benchmarks have limited concepts.In summary, the key novelty of this work is the graph formulation for CZSL that models dependencies between concepts and enables end-to-end training. This leads to considerable improvement over prior art that treats compositions independently and uses fixed image features. The paper makes both algorithmic and benchmark contributions.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Explore richer graph structures that better capture the dependencies between states, objects, and compositions. The graph proposed in this paper is relatively simple, with undirected and unweighted edges based on compositional relationships. More complex graphs could allow learning deeper GCN models and lead to performance improvements.- Develop datasets with more structured compositional relations. The authors note limitations of current CZSL datasets in terms of noisy or insufficiently compositional labels. New datasets with cleaner, richer semantic relations between concepts could further advance research.- Apply the compositional modeling approach to other vision tasks beyond CZSL. The graph formulation could potentially be useful for other problems involving compositionality, like scene graph prediction.- Improve generalization by using the compositional approach for cross-dataset learning. The qualitative results show promise for retrieving images across datasets. A model trained on multiple datasets could learn more robust representations.- Incorporate external knowledge into the graph structure. While their graph does not rely on external knowledge bases, integrating relevant structured knowledge could provide additional benefits.- Address the issue of multiple valid labels for images. The authors note that images often contain multiple valid state-object compositions that are not reflected in the single label. Accounting for this could improve model accuracy.- Learn better visual representations tuned for compositional tasks. Jointly learning the image features and graph embeddings benefits performance, suggesting further gains may be possible with representations tailored to compositional reasoning.In summary, the main future directions are developing richer graph formulations, datasets, and representations to better exploit compositional structures, and applying the approach to new tasks and settings involving compositionality.
