# [Can Small Language Models be Good Reasoners for Sequential   Recommendation?](https://arxiv.org/abs/2403.04260)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Traditional sequential recommendation models rely solely on historical user-item interactions, lacking reasoning capabilities and open-world knowledge. Recently emerged large language models (LLMs) have shown promise for reasoning and generation, but have prohibitively high computational costs. 

Solution - SLIM Framework:
- Proposes a novel Step-by-step Knowledge Distillation framework (SLIM) to transfer reasoning capabilities from a large teacher LLM to a smaller student LLM specialized for recommendations.

- Employs chain-of-thought (CoT) prompting to guide teacher LLM to generate step-by-step recommendation rationales reflecting user preferences. These rationales serve as soft labels for distilling student LLM.

- The distilled student LLM acts as a knowledge generator, offering user preference rationales for integrating with recommendation models. Two approaches are provided for utilizing the rationales: ID-based fusion and ID-agnostic text matching.

Key Contributions:  
- First work exploring knowledge distillation of LLMs tailored for sequential recommendation.

- Proposes SLIM framework enabling sequentiaI recommenders to acquire exceptional reasoning capabilities from LLMs in a resource-efficient manner.

- Provides both ID-based and ID-agnostic approaches for flexibly empowering recommenders with step-by-step rationales from the distilled student LLM.

- Experiments show SLIM significantly improves over state-of-the-art baselines, and analysis demonstrates its ability to generate meaningful reasoning affordably.
