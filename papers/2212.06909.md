# [Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image   Inpainting](https://arxiv.org/abs/2212.06909)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be how to advance and evaluate text-guided image inpainting, which refers to the task of editing images in localized regions based on textual descriptions. 

Specifically, the paper makes two main contributions:

1. It proposes a new model called Imagenator for high-quality text-guided image inpainting. The key innovations include using object detectors to propose better training masks and architectural improvements to enable high-resolution editing.

2. It introduces a new benchmark called EditBench for systematically evaluating text-guided image inpainting models. EditBench has a diverse set of example images, masks, and textual prompts to probe model performance on different attributes, objects, and scenes.

The overarching goal is to both improve text-guided image inpainting models through contributions like Imagenator, and also enable more rigorous evaluation of different models through benchmarks like EditBench. The research questions revolve around how to generate higher fidelity and more controllable image edits based on text prompts, and how to thoroughly assess progress on this task.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. EditBench, a new systematic benchmark for evaluating text-guided image inpainting. EditBench evaluates models across different axes like attributes, objects, and scenes to provide a diagnostic view of model strengths/weaknesses.

2. Imagenator, a new text-guided image editing model finetuned from Imagen. Key aspects of Imagenator include:

- Using an object detector during training to generate masks that encourage reliance on the text prompt. This improved text-image alignment. 

- Architectural changes like new convolution layers to enable high-resolution image editing.

3. A comprehensive human evaluation study on EditBench comparing Imagenator to other models like DALL-E 2 and Stable Diffusion. The study analyzes text-image alignment, image quality, and performance on different attributes/objects/scenes. 

4. An analysis of various automatic evaluation metrics and their correlation with human judgments. The authors find CLIPScore has the highest agreement with human preferences for model selection and image ranking.

In summary, the main contributions are an evaluation benchmark, a new state-of-the-art editing model, extensive human evaluations, and an analysis of automatic metrics - advancing the field of text-guided image editing. The introduction of EditBench and the insights from the human studies are particularly significant.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper introduces a new benchmark dataset and text-guided image editing model for evaluating localized image edits made in response to natural language prompts, and shows the proposed model outperforms prior work like DALL-E 2 and Stable Diffusion.
