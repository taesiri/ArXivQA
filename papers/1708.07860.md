# [Multi-task Self-Supervised Visual Learning](https://arxiv.org/abs/1708.07860)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively combine multiple self-supervised learning tasks to train a single visual representation. The key hypotheses are:- Combining diverse self-supervised tasks will lead to better feature representations compared to using any single task alone, because different tasks will induce complementary features. - Explicitly encouraging the network to separate the features needed for different tasks (via the lasso technique) will further improve the learned representation. - Modifying inputs so tasks receive more similar inputs ("harmonization") will reduce conflicts between tasks and also improve the joint representation.In particular, the paper investigates combining four different self-supervised tasks - relative position, colorization, exemplar learning, and motion segmentation. It hypothesizes that jointly training on these diverse tasks will produce a unified visual representation that outperforms each individual task, and approaches the performance of fully supervised pre-training on ImageNet classification. The lasso technique and input harmonization are proposed as methods to further reduce conflicts between different self-supervised tasks.
