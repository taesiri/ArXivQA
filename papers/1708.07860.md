# [Multi-task Self-Supervised Visual Learning](https://arxiv.org/abs/1708.07860)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively combine multiple self-supervised learning tasks to train a single visual representation. The key hypotheses are:- Combining diverse self-supervised tasks will lead to better feature representations compared to using any single task alone, because different tasks will induce complementary features. - Explicitly encouraging the network to separate the features needed for different tasks (via the lasso technique) will further improve the learned representation. - Modifying inputs so tasks receive more similar inputs ("harmonization") will reduce conflicts between tasks and also improve the joint representation.In particular, the paper investigates combining four different self-supervised tasks - relative position, colorization, exemplar learning, and motion segmentation. It hypothesizes that jointly training on these diverse tasks will produce a unified visual representation that outperforms each individual task, and approaches the performance of fully supervised pre-training on ImageNet classification. The lasso technique and input harmonization are proposed as methods to further reduce conflicts between different self-supervised tasks.


## What is the main contribution of this paper?

The main contribution of this paper is developing methods for combining multiple self-supervised visual learning tasks to train a single visual representation. Specifically:- They provide an apples-to-apples comparison of four different self-supervised tasks (relative position, colorization, exemplar, and motion segmentation) using the ResNet-101 architecture. - They show that combining multiple self-supervised tasks via a multi-task learning framework improves performance over individual tasks alone on ImageNet classification, PASCAL object detection, and NYU depth prediction.- They propose methods to mitigate conflicts between tasks, including "input harmonization" to make inputs more similar across tasks, and lasso regularization to encourage the network to factorize features.- Their best combined self-supervised model nearly matches ImageNet pre-training on PASCAL detection and matches it on NYU depth prediction, demonstrating self-supervised learning can approach fully supervised pre-training.In summary, the key contribution is demonstrating that combining diverse self-supervised tasks is an effective approach for learning visual representations without manual labels, closing the gap with supervised pre-training. The methods for mediated combination of tasks are an additional contribution.
