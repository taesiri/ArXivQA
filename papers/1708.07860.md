# [Multi-task Self-Supervised Visual Learning](https://arxiv.org/abs/1708.07860)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively combine multiple self-supervised learning tasks to train a single visual representation. The key hypotheses are:- Combining diverse self-supervised tasks will lead to better feature representations compared to using any single task alone, because different tasks will induce complementary features. - Explicitly encouraging the network to separate the features needed for different tasks (via the lasso technique) will further improve the learned representation. - Modifying inputs so tasks receive more similar inputs ("harmonization") will reduce conflicts between tasks and also improve the joint representation.In particular, the paper investigates combining four different self-supervised tasks - relative position, colorization, exemplar learning, and motion segmentation. It hypothesizes that jointly training on these diverse tasks will produce a unified visual representation that outperforms each individual task, and approaches the performance of fully supervised pre-training on ImageNet classification. The lasso technique and input harmonization are proposed as methods to further reduce conflicts between different self-supervised tasks.


## What is the main contribution of this paper?

The main contribution of this paper is developing methods for combining multiple self-supervised visual learning tasks to train a single visual representation. Specifically:- They provide an apples-to-apples comparison of four different self-supervised tasks (relative position, colorization, exemplar, and motion segmentation) using the ResNet-101 architecture. - They show that combining multiple self-supervised tasks via a multi-task learning framework improves performance over individual tasks alone on ImageNet classification, PASCAL object detection, and NYU depth prediction.- They propose methods to mitigate conflicts between tasks, including "input harmonization" to make inputs more similar across tasks, and lasso regularization to encourage the network to factorize features.- Their best combined self-supervised model nearly matches ImageNet pre-training on PASCAL detection and matches it on NYU depth prediction, demonstrating self-supervised learning can approach fully supervised pre-training.In summary, the key contribution is demonstrating that combining diverse self-supervised tasks is an effective approach for learning visual representations without manual labels, closing the gap with supervised pre-training. The methods for mediated combination of tasks are an additional contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates methods for combining multiple self-supervised visual learning tasks to train a single deep neural network representation, finding that deeper networks work better, combining diverse tasks improves performance, and the gap to supervised pre-training is nearly closed on detection and fully closed on depth prediction.


## How does this paper compare to other research in the same field?

This paper reports on results from experiments training deep neural networks with multiple self-supervised learning tasks, and evaluating the learned representations on standard computer vision benchmarks. Here is a summary of how it relates to other work in self-supervised learning:- It provides an in-depth study and apples-to-apples comparison of several major self-supervised learning methods on the same deep neural network architecture (ResNet-101). Prior works often used different base architectures, making comparisons difficult. - It shows that combining multiple self-supervised tasks during pre-training leads to better transfer performance compared to individual tasks. This confirms the intuition that diverse pre-training signals are beneficial.- The performance of the multi-task self-supervised model comes very close to supervised pre-training on ImageNet classification for PASCAL object detection, and matches it for NYU depth prediction. This narrows the gap between self-supervised and supervised pre-training.- It proposes methods like input harmonization and lasso regularization to try to learn more unified representations from diverse self-supervised tasks. However, these give minimal improvements over naively combining tasks.- Compared to concurrent work in 2017, this paper achieves stronger performance by using a deeper ResNet architecture and training for longer. But the concepts are similar to earlier papers on combining self-supervised losses.- Later work has built on these ideas to achieve even better performance by using more data, architectural improvements like selective kernels, and new pretext tasks like rotation prediction. But this paper provided an important proof of concept.In summary, this paper significantly advanced self-supervised learning by conducting a rigorous large-scale study, showing the benefits of multi-task learning, and almost matching supervised pre-training. It laid the foundation for subsequent progress in self-supervised representation learning.


## What future research directions do the authors suggest?

The authors suggest the following future research directions in the paper:- Adding more self-supervision tasks: The authors note that new self-supervision tasks have been proposed during the preparation of the paper, such as in Fernando et al. (2016). Adding more diverse tasks could potentially lead to learning more general and useful features.- Adding more evaluation tasks: The authors suggest replacing the depth prediction task with an alternative shape measurement task like surface normal prediction, which may be more informative. Evaluating on more diverse tasks could better probe the generality of the learned features.- Experiments with different network architectures: The authors suggest trying VGG-16, which has less correlation between layers, or even deeper networks like ResNet-152 and DenseNet to analyze the relationship between network depth and self-supervision performance.- Experiments with larger datasets: It is an open question whether using larger self-supervision datasets could improve performance. - Dynamic task weighting: The authors propose exploring methods to dynamically weight the importance of different self-supervision tasks during optimization to improve learning.- Block-level lasso regularization: For the lasso experiments, the authors suggest investigating block-level weightings using group sparsity regularization.- No supervised pre-training: The results show self-supervision may eventually replace or augment supervised pre-training, which is a direction for future work.In summary, the main future directions are: exploring more tasks, architectures, and datasets; dynamically weighting tasks; improved regularization techniques; and reducing reliance on supervised pre-training. The authors aim to further close the gap between self-supervised and fully supervised methods.
