# [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient   Generative Inference](https://arxiv.org/abs/2403.09054)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are constrained by memory bandwidth when handling longer input sequences due to the expanding size of the key-value (\kv) cache. This cache stores computed keys and values to avoid costly recomputation.
- The \kv size can exceed the model size itself for long contexts, exacerbating memory bottlenecks. This significantly increases inference latency and reduces throughput.

Proposed Solution: 
- The paper proposes \keyformer, a novel inference-time method to reduce the \kv size by discarding unnecessary tokens. 
- It identifies a small subset of "key" tokens that receive ~90\% of the total attention. Retaining only these tokens preserves accuracy while shrinking the \kv.
- \keyformer uses a specialized score function with logit regularization to identify key tokens robustly, even with unknown contexts at inference. It employs a temperature-controlled Gumbel softmax distribution for this purpose.

Contributions:
- \keyformer reduces \kv size by up to 50\% while maintaining high accuracy per MLPerf guidelines, unlike prior approaches.
- It improves inference latency by \latencyimprv and boosts throughput by \throughputimprv without model retraining or fine-tuning.
- Evaluations across diverse models (GPT-J, Cerebras-GPT, MPT) and tasks demonstrate \keyformer's generalizability.
- The method conforms with deployed accelerators, enabling easy adoption.

In summary, \keyformer enables efficient and accurate LLM inference for long contexts by exploiting inherent attention sparsity to minimize the \kv overhead. Its robust key token selection technique is the main innovation.
