# [DramaQA: Character-Centered Video Story Understanding with Hierarchical   QA](https://arxiv.org/abs/2005.03356)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that constructing a video question answering dataset with hierarchical question-answer pairs and character-centered annotations can enable more comprehensive video story understanding and evaluation of AI models. Specifically, the paper proposes:

1. Hierarchical QA pairs based on cognitive development stages as an evaluation metric for video story understanding. The QA pairs have 4 levels of difficulty based on memory capacity and logical complexity.

2. Character-centered video annotations such as visual bounding boxes, behaviors, emotions, and coreference-resolved scripts to model local coherence and provide a consistent view of characters. 

3. A multi-level context matching model that utilizes the character-centered annotations to hierarchically understand correlations between video clips, QA pairs, and characters.

The key hypothesis is that the hierarchical QA metric and character-centered annotations will lead to better video QA models and metrics that align with human video story understanding. The paper introduces the DramaQA dataset implementing this approach and provides both quantitative experiments and qualitative examples to demonstrate the utility.


## What is the main contribution of this paper?

 The main contribution of this paper is:

1. Proposing a new video question answering (Video QA) dataset called DramaQA with the following key features:

- Hierarchical QA difficulty levels as an evaluation metric based on cognitive development stages. This allows measuring the video understanding capability more comprehensively.

- Character-centered video annotations such as bounding boxes, behaviors, emotions of main characters, and coreference resolved scripts. This helps to model the story coherence. 

2. A Multi-level Context Matching model that hierarchically understands character-centered video representations to answer questions about the videos.

3. Quantitative experiments showing the proposed model's performance on DramaQA using the hierarchical QA difficulty levels. 

4. A new perspective on video story understanding research through the use of hierarchical QAs and character-centered annotations.

In summary, the key contribution is the DramaQA dataset with hierarchical QAs and character-centered annotations, along with a Multi-level Context Matching model designed for this dataset. The aim is to enable more comprehensive video story understanding and provide a new direction for research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here's a one sentence summary of the paper:

The paper proposes a new video question answering dataset and model focused on hierarchical question difficulty levels and character-centered video annotations for evaluating and improving video story understanding.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in video story understanding:

- Focus on hierarchical question-answering (QA) as an evaluation metric: Many prior video QA datasets do not systematically vary question difficulty or consider cognitive development stages in designing the questions. The hierarchical QA approach in this paper provides a more nuanced benchmark for assessing video story understanding.

- Character-centered video annotations: Other video QA datasets provide some textual descriptions or scripts, but this paper uniquely provides rich visual annotations centered around main characters, including bounding boxes, behaviors, and emotions. This supports modeling the characters and story coherently. 

- Multi-level reasoning model: The proposed model incorporates both low-level context and high-level abstractions guided by the question to reason about the video story. This differs from prior works that generally stay at one level of reasoning.

- Large-scale video QA dataset: With nearly 18K QA pairs, this is one of the largest and most comprehensive video QA datasets to date focused on story understanding. Many prior datasets are smaller or concentrate on shorter clips.

- Real-world drama videos: Many prior video QA datasets use animated or scripted videos. Basing the dataset on Korean TV drama provides more naturalistic and complex stories and interactions.

Overall, the hierarchical QA framework, character-centric annotations, multi-level reasoning model, and drama-based videos seem to push forward the state-of-the-art in modeling and evaluating video story understanding compared to previous benchmark datasets and methods. The innovations in dataset design and modeling approach are significant contributions.


## What future research directions do the authors suggest?

 The authors suggest several future research directions:

- Expand the two criteria of hierarchical QA to deal with longer and more complex video stories, and expand the coverage of the evaluation metric.

- Provide hierarchical character-centered story descriptions, objects, and places annotations. The authors expect these could help handle difficult questions that require common sense beyond simply understanding the video contents. 

- Use the DramaQA dataset for other video-related research like emotion/behavior analysis, coreference resolution, etc. since it contains rich character-centered annotations.

- Improve the model architecture with more sophisticated modules like BERT to further boost performance, since the current model was focused on reflecting characteristics of the dataset rather than maximizing performance.

- Apply insights from the multi-level character-centered modeling approach to tackle other intrinsic challenges in video story understanding research, like handling integrated multimodal data.

In summary, the main future directions are: expanding the dataset's scope and annotations, leveraging the dataset for other video research tasks, improving the model architecture, and applying the approach to address other video understanding challenges. The key theme is enhancing video story understanding via richer annotations and models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a new video question answering dataset called DramaQA for character-centered video story understanding, along with a model named Multi-level Context Matching. DramaQA contains about 18K question-answer pairs over video clips from a Korean TV drama series. The key aspects of DramaQA are: 1) It provides multiple choice questions at 4 difficulty levels based on cognitive criteria like memory capacity and logical complexity, allowing hierarchical evaluation of story understanding. 2) It contains rich character-centered annotations including visual bounding boxes, behaviors, emotions and coreference-resolved scripts to enable modeling of story coherence. 3) The Multi-level Context Matching model uses both low-level and high-level representations to match video and text contents with the questions and answer candidates in a character-focused way. Experiments show the model achieves over 71% accuracy, and the annotations and multi-level architecture provide useful cues. Overall, DramaQA enables more comprehensive video story understanding through its hierarchical QA structure and character-based annotations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

Paragraph 1: This paper presents a new video question answering dataset called DramaQA for understanding video stories. The dataset contains nearly 18,000 QA pairs over video clips from a Korean TV drama series. The questions have hierarchical difficulty levels based on memory capacity and logical complexity to evaluate models' story understanding abilities. The dataset also provides rich character-centered annotations like visual bounding boxes, behaviors, emotions, and coreference resolved scripts to help models focus on key characters. 

Paragraph 2: The authors propose a Multi-level Context Matching model to utilize the character-centered annotations in DramaQA. The model represents the video and text streams at both low and high levels. The low levels capture context related to the annotated characters. The high levels use attention to get query-specific representations. These multi-level representations are matched to the QA context for scoring answer candidates. Experiments show the model effectively leverages the character annotations and difficulty levels. The authors plan to expand the dataset with more character-focused descriptions and annotations. DramaQA offers a new perspective for video story understanding research.


## Summarize the main method used in the paper in one paragraph.

 The main method used in the paper is a hierarchical question answering (QA) approach for video story understanding. The key aspects are:

1) They propose a new video QA dataset called DramaQA, which contains QA pairs from video clips of a Korean TV drama. The QAs have 4 levels of difficulty based on memory capacity and logical complexity. DramaQA also provides character-centered annotations like bounding boxes, behaviors, emotions, and coreference resolved scripts. 

2) They introduce a Multi-level Context Matching model to answer the QAs by reasoning over the video context. It has two streams for visual and textual modalities, each with low-level and high-level representations. The low-level encodes the raw context, while the high-level attends to character-relevant clips using the question. Context matching layers create QA-aware sequences. 

3) The model fuses the multi-level representations from both modalities to select the best answer. Experiments show the importance of using both difficulty-based QAs and character-centered context for hierarchical video story understanding.


## What problem or question is the paper addressing?

 The paper is addressing the problem of video story understanding through question answering. The main issues it aims to tackle are:

1) Existing video QA datasets lack careful consideration of question difficulty and true "understanding" of video stories. Questions are often biased and lack variance in difficulty levels. 

2) Previous datasets don't provide character-centered annotations to help model local story coherence. Information about "who did what" is important for story understanding but often missing.

3) There is a need for hierarchical QA as an evaluation metric that aligns with cognitive development stages. This can better measure true understanding.

4) Multimodal video stories are complex. Models need to effectively integrate information across modalities and abstraction levels.

To address these issues, the paper introduces DramaQA, a new video QA dataset and model:

- DramaQA contains 17,983 QA pairs over short "shot" clips and longer "scene" clips. Questions have 4 difficulty levels based on memory and logical complexity.

- It provides rich character-centric annotations - bounding boxes, behaviors, emotions, coreference resolved scripts.

- The Multi-Level Context Matching model utilizes these annotations and has both low-level and high-level representations to hierarchically understand correlations between video, QAs, and characters.

In summary, the paper tackles video story understanding through a new hierarchical QA dataset with character-centric annotations and a model designed to leverage them for coherent multimodal reasoning.
