# [3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion   Process](https://arxiv.org/abs/2303.10406)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is developing a generalized and unified 3D shape generation prior model that can serve as an efficient backbone for various downstream 3D shape generation tasks. 

The main hypothesis is that by combining a vector quantized variational autoencoder (VQ-VAE) to encode local part-based shape information and a discrete diffusion generator to model structural dependencies, the proposed model called 3DQD can generate high-fidelity, diverse 3D shapes while aligning well across different data modalities.

In particular, the paper aims to address the limitations of previous approaches which are often task-specific and fail to capture diverse shape samples or perform well on cross-modality generation. The proposed 3DQD model is designed to possess the following capabilities:

- High-fidelity shape generation via precisely capturing fine-grained local shape details with the VQ-VAE.

- Diverse shapes sampling by modeling inherent structural dependencies with the discrete diffusion generator and introducing randomness. 

- Good cross-modality alignment ability via the discrete part-based representation that eliminates intrinsic structural bias across modalities.

- Generalized backbone for multiple downstream tasks without much adaptation needed.

In essence, the central hypothesis is that by combining part-based quantization and discrete diffusion, the proposed unified 3DQD model can achieve superior performance on various 3D shape generation tasks in terms of quality, diversity, and flexibility. The experiments verify this hypothesis by benchmarking against state-of-the-art methods on tasks like unconditional shape generation, shape completion, and cross-modal generation.
