# [3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion   Process](https://arxiv.org/abs/2303.10406)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is developing a generalized and unified 3D shape generation prior model that can serve as an efficient backbone for various downstream 3D shape generation tasks. 

The main hypothesis is that by combining a vector quantized variational autoencoder (VQ-VAE) to encode local part-based shape information and a discrete diffusion generator to model structural dependencies, the proposed model called 3DQD can generate high-fidelity, diverse 3D shapes while aligning well across different data modalities.

In particular, the paper aims to address the limitations of previous approaches which are often task-specific and fail to capture diverse shape samples or perform well on cross-modality generation. The proposed 3DQD model is designed to possess the following capabilities:

- High-fidelity shape generation via precisely capturing fine-grained local shape details with the VQ-VAE.

- Diverse shapes sampling by modeling inherent structural dependencies with the discrete diffusion generator and introducing randomness. 

- Good cross-modality alignment ability via the discrete part-based representation that eliminates intrinsic structural bias across modalities.

- Generalized backbone for multiple downstream tasks without much adaptation needed.

In essence, the central hypothesis is that by combining part-based quantization and discrete diffusion, the proposed unified 3DQD model can achieve superior performance on various 3D shape generation tasks in terms of quality, diversity, and flexibility. The experiments verify this hypothesis by benchmarking against state-of-the-art methods on tasks like unconditional shape generation, shape completion, and cross-modal generation.


## What is the main contribution of this paper?

 This paper proposes a generalized 3D shape generation prior model called 3DQD, which can serve as an efficient backbone for various 3D shape generation tasks like unconditional shape generation, shape completion from partial inputs, and cross-modal shape generation from images/text. The key contributions are:

- It uses a Vector Quantized Variational Autoencoder (VQ-VAE) to encode shapes into discrete local geometry tokens, which provides a compact and consistent representation across tasks. 

- It models the joint distribution of these shape tokens using a discrete diffusion process, which iteratively samples in the time domain to capture complex shape structures. This allows generating high-fidelity and diverse results.

- It introduces a Multi-Frequency Fusion Module to suppress high-frequency noise and encourage smoother surfaces in the generated shapes. 

- Experiments show the model achieves state-of-the-art performance on unconditional shape generation, shape completion, and text/image-guided shape generation tasks.

- The model serves as an efficient general purpose 3D shape prior that can be applied to multiple downstream tasks with little or no tuning, unlike previous specialized models.

In summary, the key contribution is proposing an expressive and unified 3D shape generation prior model based on part-discretization and diffusion, which achieves strong performance across diverse shape generation tasks. The generality and efficiency of the model are its main advantages.
