# [DVAE++: Discrete Variational Autoencoders with Overlapping   Transformations](https://arxiv.org/abs/1802.04920)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method for training generative models with discrete latent variables. The key ideas are:

1. A new class of "overlapping transformations" is introduced to smooth discrete variables. These transformations rely on mixtures of overlapping distributions that converge to discrete distributions.

2. The transformations enable training of models with directed priors (like VAEs) as well as undirected priors (like Boltzmann machines). New variational bounds are derived for training with Boltzmann machine priors. 

3. The transformations allow gradients to be estimated without special handling, enabling the use of automatic differentiation libraries.

4. A new VAE architecture called DVAE++ is developed that combines global discrete variables (to capture discontinuous properties) and hierarchical continuous variables (to capture local dependencies). The global variables have a Boltzmann machine prior.

5. Experiments show DVAE++ generates high quality samples and outperforms previous techniques for training discrete latent variable models, achieving state-of-the-art results on several benchmarks.

So in summary, the central hypothesis is that overlapping transformations can be used to effectively train deep generative models with discrete latent variables, capturing both global discrete properties and local dependencies. The effectiveness of this approach is demonstrated through the DVAE++ model and experiments.


## What is the main contribution of this paper?

 The paper proposes a new class of smoothing transformations for training discrete latent variable models. The key contributions are:

1. It introduces overlapping transformations that use a mixture of two overlapping distributions to relax discrete variables. This allows gradients to flow through discrete nodes during training.

2. It shows how overlapping transformations can be applied to train both directed and undirected (Boltzmann machine) priors over discrete latent variables. 

3. For undirected priors, it derives a new variational lower bound that enables end-to-end training without any special manual handling of gradients.

4. It develops DVAE++, a new deep generative model architecture with discrete global variables and convolutional local variables. DVAE++ incorporates overlapping transformations and achieves state-of-the-art performance on several image datasets.

5. Experiments demonstrate that overlapping transformations outperform previous continuous relaxations like Gumbel-Softmax and the original smoothing method used in discrete variational autoencoders. 

In summary, the main contribution is proposing overlapping transformations as a new way to incorporate discrete variables in variational autoencoders and showing this enables training highly expressive models like DVAE++. The theoretical derivation of the bounds and demonstration of strong empirical performance are also key results.
