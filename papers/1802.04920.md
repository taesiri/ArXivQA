# [DVAE++: Discrete Variational Autoencoders with Overlapping   Transformations](https://arxiv.org/abs/1802.04920)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method for training generative models with discrete latent variables. The key ideas are:

1. A new class of "overlapping transformations" is introduced to smooth discrete variables. These transformations rely on mixtures of overlapping distributions that converge to discrete distributions.

2. The transformations enable training of models with directed priors (like VAEs) as well as undirected priors (like Boltzmann machines). New variational bounds are derived for training with Boltzmann machine priors. 

3. The transformations allow gradients to be estimated without special handling, enabling the use of automatic differentiation libraries.

4. A new VAE architecture called DVAE++ is developed that combines global discrete variables (to capture discontinuous properties) and hierarchical continuous variables (to capture local dependencies). The global variables have a Boltzmann machine prior.

5. Experiments show DVAE++ generates high quality samples and outperforms previous techniques for training discrete latent variable models, achieving state-of-the-art results on several benchmarks.

So in summary, the central hypothesis is that overlapping transformations can be used to effectively train deep generative models with discrete latent variables, capturing both global discrete properties and local dependencies. The effectiveness of this approach is demonstrated through the DVAE++ model and experiments.


## What is the main contribution of this paper?

 The paper proposes a new class of smoothing transformations for training discrete latent variable models. The key contributions are:

1. It introduces overlapping transformations that use a mixture of two overlapping distributions to relax discrete variables. This allows gradients to flow through discrete nodes during training.

2. It shows how overlapping transformations can be applied to train both directed and undirected (Boltzmann machine) priors over discrete latent variables. 

3. For undirected priors, it derives a new variational lower bound that enables end-to-end training without any special manual handling of gradients.

4. It develops DVAE++, a new deep generative model architecture with discrete global variables and convolutional local variables. DVAE++ incorporates overlapping transformations and achieves state-of-the-art performance on several image datasets.

5. Experiments demonstrate that overlapping transformations outperform previous continuous relaxations like Gumbel-Softmax and the original smoothing method used in discrete variational autoencoders. 

In summary, the main contribution is proposing overlapping transformations as a new way to incorporate discrete variables in variational autoencoders and showing this enables training highly expressive models like DVAE++. The theoretical derivation of the bounds and demonstration of strong empirical performance are also key results.


## How does this paper compare to other research in the same field?

 Here are some ways this paper compares to other research in discrete variational autoencoders and generative modeling:

- It proposes a new class of smoothing transformations for binary latent variables based on overlapping distributions. This is a novel approach compared to previous smoothing methods like the Gumbel-Softmax trick or spike-and-exponential distributions. 

- It shows these overlapping transformations can be used to train both directed and undirected (Boltzmann machine) priors over discrete latent variables. Prior work like Concrete/Gumbel-Softmax focused only on directed graphical models, while DVAE used spike-and-exponential for undirected priors. 

- The paper presents a new bound for training VAEs with Boltzmann machine priors that avoids manually coding gradients. This is an improvement over the original DVAE which required modifying gradients during training.

- DVAE++ integrates discrete latent variables with a hierarchy of convolutional continuous variables. This architecture builds upon recent advances in convolutional VAEs, adding discrete variables to capture global structure.

- Experiments show DVAE++ achieves state-of-the-art or comparable results to other VAEs on benchmark image datasets. The discrete variables are shown to capture meaningful semantic factors.

- The approach is compared extensively to other methods for training discrete variable models like Gumbel-Softmax, DVAE, NVIL, MuProp, etc. This provides insight into the relative merits of overlapping transformations.

Overall, this paper makes significant contributions in combining discrete and continuous latent variables in VAEs, proposing a novel smoothing method with theoretical justification, and achieving strong empirical performance on generative modeling tasks. It advances the state-of-the-art in variational autoencoders with discrete variables.


## What future research directions do the authors suggest?

 The authors suggest several potential future research directions:

- Developing more advanced inference algorithms and architectures for discrete VAEs. They note there is still room for improvement in the inference model, for example by using normalizing flows or richer autoregressive structures.

- Exploring different choices of smoothing distributions in overlapping transformations beyond the exponential mixture used in this work. Other choices may have better properties. 

- Applying discrete VAEs to broader application areas like semi-supervised learning, disentanglement, interpretable models, etc. Discrete latent variables may help capture explanatory factors. 

- Extending discrete VAEs to sequence modeling. The RBM prior could capture long-range dependencies while local latent variables model local smoothness.

- Combining discrete VAEs with more powerful autoregressive decoders like PixelCNN to achieve state-of-the-art densities. Currently PixelCNN is used with continuous latent variables.

- Developing more sophisticated priors over discrete variables, beyond the RBM. For example using graph neural networks or transformer networks to capture complex dependencies.

- Theoretical analysis of convergence and approximation error properties of different continuous relaxations for discrete variables.

- New methods to reduce the KL penalty between the inference model and complex priors over discrete variables.

So in summary, they highlight opportunities to improve both the inference model, decoder model, and discrete prior in discrete VAEs, and suggest applying them to a wider range of tasks. They also note the need for more theoretical analysis of different relaxations.
