# [Hyperparameter Tuning with Renyi Differential Privacy](https://arxiv.org/abs/2110.03620v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we properly account for the privacy cost of hyperparameter tuning when training machine learning models with differential privacy? 

In particular, the paper investigates whether simply setting hyperparameters based on non-private training can leak private information about individuals in the training data. It then aims to develop techniques to tune hyperparameters privately while maintaining good utility.

The key hypotheses appear to be:

1) Naively tuning hyperparameters without differential privacy can reveal private information about training data, especially in cases where outliers skew the optimal hyperparameter settings.

2) Repeatedly running a differentially private training procedure with random hyperparameter settings, then outputting the best model, can allow private hyperparameter tuning. The privacy cost grows much slower than naive composition methods would suggest if the number of repetitions is randomized appropriately.

3) There is a tradeoff between privacy and utility based on the distribution chosen for the number of random hyperparameter trials. Concentrated distributions like Poisson give the best intermediate privacy/utility tradeoff.

In summary, the central research question is how to properly account for privacy costs during hyperparameter tuning, and the hypothesis is that randomized repeated trials with output privacy amplification can enable private tuning with good utility guarantees.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It shows that hyperparameter tuning can leak private information if done naively without differential privacy protections. The paper gives an illustrative example with SVM training where the optimal regularization hyperparameter alpha changes noticeably in the presence of a few outlier data points. This could enable membership inference attacks.

- It provides tools to perform differentially private hyperparameter tuning. In particular, it analyzes repeating the training procedure a random number of times (according to various distributions like Poisson or truncated negative binomial) and outputting the best model. This avoids the privacy pitfalls of naive tuning. 

- It proves privacy guarantees for this private hyperparameter tuning approach under Renyi differential privacy. The privacy loss grows only logarithmically with the expected number of repetitions, as opposed to the linear growth with naive composition.

- It empirically evaluates the proposed private tuning algorithms on a CNN trained on MNIST. This demonstrates the tradeoffs between privacy and utility for the different distributions considered.

Overall, the key insight is that tuning hyperparameters privately is important, and this can be achieved with minimal loss of utility by repeating training a random number of times rather than a fixed budget. The paper provides a rigorous analysis of this approach.
