# [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper proposes a new family of parameter-efficient finetuning methods called Representation Finetuning (ReFT). 

Motivation:
- Existing parameter-efficient finetuning (PEFT) methods like adapters and LoRA work by updating a small number of model weights. However, much research has shown that representations encode rich semantic information.
- So the paper hypothesizes that directly editing representations could be a more powerful and efficient way to adapt models compared to only updating weights.

Proposed Method:
- ReFT methods directly learn to edit the internal representations of a pretrained frozen language model to adapt it to new tasks. 
- A key instance called LoReFT (Low-rank Linear Subspace ReFT) is introduced which intervenes on representations using a low-rank projection that spans a linear subspace. This builds on prior work using such subspace interventions for interpretability.
- LoReFT has far fewer parameters than weight-update PEFT methods like LoRA. But it is a drop-in replacement method.

Experiments and Results:
- Extensive experiments compare LoReFT against SOTA PEFTs like prefix tuning, adapters and LoRA on LMs of varying scales. 
- Tasks covered include commonsense reasoning, arithmetic reasoning, instruction-following and GLUE language understanding datasets.
- LoReFT consistently matches or beats all PEFT baselines while using 10-50x fewer parameters. It delivers the best efficiency-performance tradeoff.

Conclusion:
- The paper shows for the first time that directly editing representations is a promising direction for efficient tuning. The results motivate more research on ReFT methods.
