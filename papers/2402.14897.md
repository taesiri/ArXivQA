# [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Recently, \citet{lanham2023measuring} proposed a metric to measure the faithfulness of chain-of-thought (CoT) explanations generated by large language models (LLMs). On a set of MCQ tasks, they found that model faithfulness exhibits a scaling then inverse-scaling trend, decreasing after models reach 13B parameters.

- This raises questions around whether this trend generalizes across LLM families or is unique to the proprietary models they evaluated. The paper examines:
  - If inverse scaling in faithfulness occurs once models become sufficiently capable, across families
  - If the drop in faithfulness happens at a similar model size of 13B parameters
  - If the optimally faithful model size depends on task difficulty

Methodology
- The authors replicate the experimental setup from \citet{lanham2023measuring} using the unfaithfulness metric, CoT prompting methods, and 8 MCQ benchmarks.

- They evaluate models from 3 openly available families: Llama 2, FLAN-T5+UL2, Pythia DPO, covering up to 70B parameters.

Key Findings
- Under certain conditions, inverse scaling trends are observed, but the onset of unfaithfulness happens before 13B parameters.

- The metric has high correlation (R^2=0.91) with accuracy for capable models, questioning its validity.  

- The trends are very sensitive to answer choice orderings. Shuffling order between CoT/no-CoT conditions can reduce scores by up to 73 percentage points.

Conclusions
- The inverse scaling trend occurs across families but does not reliably start at 13B parameters.

- The faithfulness metric correlates strongly with accuracy, reducing it to a simple accuracy measurement.

- The high sensitivity to implementation details raises doubts about the suitability of this metric to measure CoT faithfulness. The drop in scores likely stems from other confounding factors versus genuine reasoning unfaithfulness.


## Summarize the paper in one sentence.

 This paper evaluates whether the scaling trends for chain-of-thought faithfulness reported in prior work generalize across language models, finding that while similar patterns can be reproduced, the faithfulness metric correlates strongly with task accuracy, raising doubts about its validity.


## What is the main contribution of this paper?

 The main contribution of this paper is evaluating whether the scaling trends for chain-of-thought (CoT) faithfulness reported by Lanham et al. (2023) generalize across different language model families. Specifically, the authors replicate Lanham et al.'s experimental setup using Llama 2, FLAN-T5, and Pythia DPO models. They are able to reproduce the scaling then inverse scaling relationship between model size and CoT faithfulness under certain conditions. However, they find that simply changing the order of multiple choice answers can reduce the faithfulness metric by up to 73 percentage points. They also discover a very high correlation (R^2 = 0.91) between the faithfulness metric and accuracy. Based on these findings, the authors question the validity of the metric proposed by Lanham et al. for evaluating CoT faithfulness, and caution against using the rate at which answers change with CoTs as a proxy.

The key contribution is evaluating and critiquing the generalizability of Lanham et al.'s faithfulness metric across model families, identifying its sensitivity to ordering effects and correlation with accuracy. This sheds doubts on its validity for assessing reasoning faithfulness.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and concepts related to this work include:

- Chain-of-thought (CoT) faithfulness - The extent to which a model's generated reasoning chain aligns with its actual computations. The paper examines metrics proposed for measuring this.

- Scaling laws - How a model's performance changes with its number of parameters. The paper looks at inverse scaling where performance decreases after reaching an optimal model size. 

- Unfaithfulness metric - The rate at which a model changes its prediction between normal prompting and chain-of-thought prompting. The paper evaluates this metric across models.

- Multiple choice benchmarks - The paper examines model performance on several multiple choice QA datasets like LogiQA, HellaSwag, TruthfulQA etc.

- Model families - Groups of related models with differing number of parameters. The paper looks at model families like Llama 2, FLAN, and Pythia. 

- Accuracy correlation - The observed correlation between model accuracy and the unfaithfulness metric brings into question what the latter actually measures.

So in summary, key terms cover chain-of-thought faithfulness, scaling laws trends, the specific unfaithfulness metric, the models and datasets used, and critiques of the metric. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper argues that the metric for chain-of-thought (CoT) faithfulness proposed by Lanham et al. is too simplistic and reductive. Do you agree or disagree with this assessment? Why? What additional complexity needs to be captured in a CoT faithfulness metric?

2. The paper shows the CoT faithfulness metric is highly correlated with accuracy. Does this indicate the metric is not actually measuring faithfulness and is instead just another accuracy proxy? What would need to change about the metric for it to more directly measure faithfulness? 

3. The paper demonstrates the CoT faithfulness metric is highly sensitive to factors like answer choice order shuffling. How could the metric be made more robust to these types of changes that should theoretically not impact faithfulness assessments?

4. The authors hypothesize that larger models may be able to capture CoT reasoning within their parameters, allowing them to produce the correct prediction even without explicit CoT generation at test time. How could this hypothesis be tested? What experiments could determine if reasoning chains are embedded within model parameters?

5. Do you think the inverse scaling trends observed, especially by Lanham et al., are indicative of genuine drops in CoT faithfulness or might other factors be at play? How could the role of accuracy and capability be better isolated?

6. Lanham et al. claim CoT faithfulness peaks at a model size of 13 billion parameters, but this paper showed peak faithfulness depends greatly on model family and even task condition. What explains this discrepancy? Is 13 billion parameters a consistent peak across families?

7. The paper evaluates model families pretrained in different ways (RLHF, DPO, standard self-supervised). Could differences in pretraining objectives impact CoT faithfulness? How could this be measured and what impacts might be observed? 

8. What other model architectures besides decoder-only models could be evaluated using the CoT faithfulness metric? Would results differ for encoder-decoder models? What about models with explicit reasoning components like a knowledge graph?

9. The paper focuses on evaluating faithfulness of CoTs for multiple choice QA. Would using CoTs for open-ended QA or even non-QA tasks potentially show different faithfulness scaling trends? How could CoT prompting be adapted to other tasks?

10. Do you think the CoT faithfulness metric proposed by Lanham et al. can be adapted to more reliably measure faithfulness? Or should wholly different metrics be explored that do not exhibit the same sensitivities observed? What properties would a more robust metric have?
