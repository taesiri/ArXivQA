# [Social Biases through the Text-to-Image Generation Lens](https://arxiv.org/abs/2304.06034)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What is the extent of representational biases related to gender, race, age, and geographical location in images generated by large vision and language models like DALLE-2 and Stable Diffusion?The authors take a multi-dimensional approach to quantifying common social biases reflected in images generated by these models for various prompts related to occupations, personality traits, everyday situations, and simply "person". Through automated analysis and human evaluation, the study aims to uncover how different groups are represented in the generated images compared to real-world distributions, and whether techniques like prompt engineering can help mitigate such biases. The key aspects examined are:- Representation of different genders, races, ages for occupational prompts compared to US labor statistics- Associations of certain personality traits with specific groups - Representation of different countries in everyday situation prompts- Effectiveness of prompt expansion (e.g. "female doctor", "black engineer") in improving diversity of generationsSo in summary, the central question is examining the extent of biases in current text-to-image models through a multi-faceted analysis, in order to inform efforts towards building more fair and inclusive generative models.


## What is the main contribution of this paper?

The main contribution of this paper is a multi-dimensional analysis of gender, race, age, and geographical biases in text-to-image generative models, specifically DALLE-v2 and Stable Diffusion. The key aspects of the analysis are:- Studying representation biases in generated images for neutral prompts describing occupations, personality traits, everyday situations, and simply "person". This analyzes model bias without interference from prompt crafting.- Analyzing the effectiveness of mitigation techniques like prompt expansion for occupations by adding gender, race, and age specificity. This studies whether diversity can be increased through prompting.- Measuring discrepancies in image quality for gendered occupation prompts to see if prompt expansion introduces other biases.- Analyzing geographical representation in everyday situations across countries to see which ones are under/over-represented in default generations. - Employing both human evaluation through crowdsourcing and automated evaluation using computer vision models.The main findings are:- Both models exhibit and exacerbate occupational gender and race biases compared to labor statistics.- Prompt expansion increases diversity but does not fully mitigate biases and can introduce quality discrepancies.- Certain countries are constantly under-represented across everyday situations.Overall, this is a comprehensive study across multiple bias dimensions and subjects using both human and automated evaluation. The results provide an extensive view into the types and severity of biases in text-to-image models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper systematically analyzes and quantifies social biases in text-to-image generation models across dimensions like gender, race, age, and geography, finding both models exhibit significant biases that are sometimes worse than web image search results.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is how I would summarize it in relation to other research in the field:The paper presents a systematic study evaluating gender, racial, age, and geographical biases in two major text-to-image generative AI models - DALL-E 2 and Stable Diffusion. It examines these biases across four different topics: images of people, occupations, personality traits, and everyday situations. This kind of comprehensive evaluation of AI bias goes beyond most prior work, which has tended to focus on evaluating bias in one model and along one or two dimensions like gender and race. For example, the Cho et al. (2022) study looked primarily at gender and racial bias in occupations for Stable Diffusion and miniDALL-E. Bianchi et al. (2022) also focused heavily on intersectional gender and race biases in Stable Diffusion. The scope of this paper's analysis across multiple models, topics, and dimensions of bias represents an advance in understanding AI biases. The comparison to real world labor statistics is also novel, providing a reference point for occupations that previous studies lacked.Additionally, this paper thoroughly investigates prompt engineering strategies to mitigate bias and finds limitations, whereas prior work had not evaluated this in depth. The analysis of resulting image quality differences is also new.In summary, this paper significantly expands the investigation of biases in text-to-image models compared to prior work by evaluating more models, along more dimensions of bias, across more topics, and delving deeper into prompt-based mitigation strategies and effects. The findings reveal important representational issues for researchers and practitioners working with these generative models.
