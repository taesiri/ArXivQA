# [Learning to Compose SuperWeights for Neural Parameter Allocation Search](https://arxiv.org/abs/2312.01274)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Prior work in neural parameter allocation search (NPAS) automates parameter sharing to generate efficient neural networks given a fixed parameter budget. However, it has two major limitations: 
1) There is a disconnect between the search and training steps. During search, shared template weights are warped to measure similarity between layers of different sizes. But during training, no warping occurs which reduces performance.  
2) Similarity between shared parameters is measured by comparing the parameter values themselves. However, this does not account for conflicting gradients where layers try to update the shared weights in opposite directions.

Proposed Solution - SuperWeight Networks (SWN):
1) Introduce SuperWeights - groups of parameters that act as feature detectors and are reusable across layers. SuperWeights are created to be large enough to represent any layer but small enough to enable efficient sharing.

2) Layer weights are generated by concatenating SuperWeights together. This avoids the need to warp layers during search. 

3) Identify SuperWeight Clustering: Group together layers that can share the same SuperWeights effectively. This provides a coarse sharing policy.

4) Refine sharing policy: Allow both hard parameter sharing (same coefficients used to combine templates into SuperWeights) and soft parameter sharing (unique coefficients). This enables specialization.  

5) Instead of comparing parameter values, use gradient information to identify layers where sharing will be effective. Layers with conflicting gradients hurt optimization.

Main Contributions:
1) Compose reusable SuperWeights across diverse architectures for automated parameter sharing.

2) Introduce gradient-based analysis for superior sharing strategies over prior work.

3) Demonstrate SWN enables efficient ensembling, outperforming fully-parameterized ensembles and supporting anytime prediction.

The key ideas are:
(i) SuperWeights avoid warping during search 
(ii) Gradient analysis identifies better sharing
(iii) Hierarchical construction enables diverse architectures
(iv) Improves ensemble efficiency and anytime prediction
