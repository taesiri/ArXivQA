# [Generating Unsupervised Abstractive Explanations for Rumour Verification](https://arxiv.org/abs/2401.12713)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper addresses the task of rumour verification in social media, which involves assessing the veracity (true, false or unverified) of claims spreading on platforms like Twitter. Prior work has focused on just predicting a veracity label. However, recent studies show that for automated fact-checking systems to be trusted, model predictions need to be accompanied by explanations. 

Proposed Solution:
The paper proposes a novel unsupervised framework to generate free-text, abstractive explanations that justify the veracity predictions of a rumour verification model. It has three main components:

1) A graph-based neural network model is trained to predict rumour veracity using conversations from the PHEME dataset. The model incorporates structural and stance-related cues.

2) Post-hoc explanation methods like Integrated Gradients and Shapley Values are used to score the importance of individual posts with respect to the model's prediction.

3) The most important posts are fed to a template-guided summarization model to generate an explanatory abstractive summary, which aggregates opinions in the thread.

Main Contributions:

- First work to introduce free-text explanations for rumour verification grounded in model importance scores.

- Demonstrates that abstractive summaries are more informative (75%) than extractive explanations (34%) when evaluated by a large language model (LLM) powered evaluator.

- Compares gradient-based and game-theoretic explanation algorithms and shows the former is faster and more performant. 

- Introduces a new LLM-based evaluation protocol to assess informativeness and faithfulness of explanations at scale. Achieves good human-LLM agreement.

The paper concludes by discussing limitations of the conversational summarization and LLM-assessment approaches. Potential future directions are outlined such as end-to-end training and incorporation of external evidence.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces an unsupervised framework for generating abstractive explanations using template-guided summarization to justify model predictions for the task of rumor verification on social media.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing an unsupervised framework for generating abstractive explanations using template-guided summarization for the task of rumor verification. Specifically, the key contributions are:

1) Proposing a novel unsupervised method to generate faithful abstractive explanations for a rumor verification model's predictions. This is the first work to introduce free-text explanations for this task.

2) Investigating the use of a gradient-based algorithm (Integrated Gradients) and a game-theoretical algorithm (Shapley Values) to provide post-hoc explainability for a graph-based hierarchical rumor verification model.

3) Evaluating several explanation baselines by using them as input to a few-shot trained large language model (LLM) to assess informativeness. The results show that abstractive explanations are informative in 75% of cases compared to only 34% for the highest ranked post.

4) Providing both human and LLM-based evaluation of the generated explanatory summaries, showing that LLMs can achieve sufficient agreement with humans to allow scaling up the evaluation.

In summary, the main contribution is proposing a novel unsupervised framework to generate abstractive explanations for rumor verification using important posts identified by a model explainer and template-guided summarization. The quality of the explanations is evaluated automatically using LLMs and with human annotators.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Rumour verification - The main task that is the focus of the paper, which involves assessing the veracity of claims spread on social media

- Explainability - A core concept explored in generating model-centric explanations to justify rumour verification predictions

- Abstractive summarization - Used to produce free-text explanations from important posts in the conversation threads

- Post-hoc explainability methods - Specifically gradient-based (Integrated Gradients) and game theory-based (Shapley Values) algorithms used to score important posts

- Template-guided summarization - BART model trained on summarizing topical Twitter groups is used to generate fluent explanatory summaries 

- Faithfulness - Explanations are evaluated on how well they reflect the predicted rumour veracity to assess fidelity to the model

- Informativeness - The ability of an explanation to provide enough evidence for deducing a claim's veracity

- Large language models (LLMs) - Used as evaluators to assess informativeness and faithfulness by comparing LLM-suggested veracity labels to model predictions


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces an unsupervised framework for generating abstractive explanations for rumour verification. What are the key components of this framework and how do they work together? Discuss in detail.

2. The paper trains a rumour verification model based on graph neural networks and incorporates stance information. Explain the architecture, training process, and performance results of this model. How is stance information integrated and why is it important?

3. The paper uses two post-hoc explainability methods - Integrated Gradients and Shapley Values. Compare and contrast these methods. What are their relative advantages and disadvantages in terms of performance and computational complexity? 

4. The paper generates explanations via template-guided summarisation using a BART model. Explain how the summarisation process works. Why is a template-guided approach suitable for generating explanations in this context?

5. The paper evaluates explanation quality using a large language model (LLM) as an evaluator. Discuss the motivation, setup, and findings of the LLM-based evaluation experiments in detail. What are the limitations?

6. The paper conducts human evaluation studies to validate the LLM-based evaluation approach. Summarize the setup, metrics, and key results of these human experiments. How do the LLMs compare to human judgement?  

7. The paper experiments with multiple LLMs as evaluators - ChatGPT, ChatGPT 0613, and GPT-4. Compare their relative strengths and weaknesses for evaluation based on the error analysis. How do temporal snapshots and model choice impact performance?

8. What is the definition of an "informative" explanation in this paper? How is informativeness evaluated? Why is it an appropriate metric for assessing explanation quality?

9. The paper introduces a novel unsupervised approach to generating explanations. How is it different from prior supervised and extractive methods? What are the advantages of an unsupervised abstractive approach?

10. What are some limitations of the methods proposed in the paper and how can they be addressed in future work? Discuss limitations related to conversation thread summarisation, human evaluation, and incorporating external evidence.
