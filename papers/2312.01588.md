# [ActiveClean: Generating Line-Level Vulnerability Data via Active   Learning](https://arxiv.org/abs/2312.01588)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper presents ActiveClean, a scalable tool that leverages active machine learning to automatically generate line-level vulnerability data from commits. The key idea is to train models to distinguish vulnerability-relevant lines from irrelevant ones like refactoring based on designed syntactic and semantic features. Through an active learning approach, ActiveClean reaches 70% F1 score using just 400 manually labeled data. When applied to clean the FFmpeg dataset, it reduced 50% of the lines as irrelevant and enabled the state-of-the-art line-level vulnerability detector LineVul to identify 18 more functions and 70 more lines as vulnerable, improving its Top 10 accuracy from 66% to 73%. The cleaned dataset also helped detect 29 incorrect vulnerability labels. Overall, ActiveClean demonstrates an effective way to automatically produce cleaner line-level vulnerability data to facilitate building better vulnerability detectors.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "ActiveClean: Generating Line-Level Vulnerability Data via Active Learning":

Problem:
- Deep learning based vulnerability detection tools rely on large volumes of high-quality training data. 
- Most existing datasets only provide function-level labels indicating if a function is vulnerable. 
- For vulnerability detection to be useful, we need line-level data indicating the specific lines responsible for the vulnerability. 
- Manually cleaning commits to generate line-level data is expensive and not scalable.

Proposed Solution:
- The paper proposes ActiveClean, a tool to automatically generate line-level vulnerability data from commits using active learning. 
- It extracts syntactic and semantic features from commit lines to train a model to classify each line as vulnerability-relevant or irrelevant.
- An active learning framework with a "query by committee" approach is used to efficiently train the model with minimal labeled data.

Main Contributions:
- An automatic technique and scalable tool (ActiveClean) to clean patches and generate line-level vulnerability data.
- New feature engineering and application of active learning for this problem.  
- Evaluation on Java and C datasets with over 4.3K commits and 119K lines showing 70-74% F1 score.
- Line-level vulnerability dataset with labels for 5K functions in FFmpeg. 
- Demonstration that vulnerability detection model (LineVul) improves from 66% to 73% top 10 accuracy when trained on ActiveClean dataset compared to original dataset.

The key idea is to use active learning to train a model to distinguish vulnerability-relevant vs irrelevant lines in commits based on code features. This helps automatically clean commits to generate useful line-level vulnerability data. The paper demonstrates the effectiveness of this approach.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents ActiveClean, a scalable tool that uses active learning to automatically generate line-level vulnerability data from commits by training a model to distinguish vulnerability-relevant lines from irrelevant lines based on designed semantic and syntactic features.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. An automatic technique and scalable tool called ActiveClean that can clean the buggy commits and generate line-level vulnerability data. 

2. FFmpeg vulnerability dataset with line-level labels, consisting of 5K vulnerable functions with 9K labels.

3. Feature engineering and an application of active learning as a novel method for this problem.

4. A systematic evaluation to show the approach can improve the quality of the dataset and improve deep learning models for vulnerability detection. For example, using the cleaned FFmpeg dataset, the vulnerability detection model LineVul detected 18 more vulnerable functions and 70 more vulnerable lines compared to using the original dataset.

So in summary, the main contribution is proposing ActiveClean, a novel approach based on active learning and feature engineering, to automatically generate line-level vulnerability data from commits. The paper also provides a cleaned dataset for FFmpeg and shows that their approach can help improve vulnerability detection models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, here are some of the key terms and concepts associated with this paper:

- Active learning - The paper proposes an active learning approach called "ActiveClean" to generate line-level vulnerability data. This is a key technique used in the paper.

- Line-level vulnerability data - The paper aims to automatically generate line-level labels indicating which lines in a code commit are relevant to a vulnerability. This is the key output produced by the proposed approach. 

- Feature engineering - The paper extracts syntactic and semantic features from code commits and contexts to train machine learning models to identify vulnerability-relevant lines. The feature engineering is a key aspect.

- Query-by-committee - The active learning approach uses a "query-by-committee" framework with a committee of machine learning models to select the most informative data points to label.

- Effectiveness - The paper evaluates the effectiveness of the approach in reducing irrelevant lines and producing accurate line-level vulnerability labels.

- Efficiency - The paper also evaluates how efficiently the active learning approach can learn from a small number of labeled examples.

- Applications - The paper demonstrates applications of the line-level data for improving vulnerability detection models and identifying incorrect function-level labels.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the motivation behind developing a technique to automatically generate line-level vulnerability data from commits? Why is line-level data important?

2. Explain the overall framework of the ActiveClean approach. What are the inputs and outputs? How does it leverage active learning to train the model? 

3. What are the different categories of features extracted from the commit lines to train the ActiveClean model? Explain at least 3 features from each category and the intuition behind them.  

4. How does ActiveClean apply the query-by-committee strategy for active learning? What is the committee composed of and how does it select the data to be manually labeled?

5. What are the datasets used to evaluate ActiveClean and why were they chosen? Discuss the experimental setup, including model selection, training, and evaluation metrics used.  

6. Analyze the results reported in Table 2. What do they indicate about the effectiveness of ActiveClean in reducing bug-irrelevant lines and commits?

7. Compare the efficiency of ActiveClean versus the baseline methods in terms of F1 score achieved with additional training data. What can you conclude?

8. What do the ablation studies show regarding the impact of different model combinations and active learning versus random selection strategies?

9. How was ActiveClean applied to improve line-level vulnerability detection using the LineVul model? Analyze the improvements obtained on metrics like F1 score, Top 10 accuracy etc.

10. What is the implication of ActiveClean being able to detect incorrect function-level vulnerability labels? Discuss with examples from the paper.
