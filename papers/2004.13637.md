# Recipes for building an open-domain chatbot

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: what are the key ingredients for building an engaging, humanlike open-domain chatbot? The authors explore different model architectures, training objectives, datasets, and decoding strategies to identify factors that contribute to improved performance on this challenging task.The key hypotheses tested in the paper are:1) Blending skills from different dialogue datasets (personality, empathy, knowledge) into a single model leads to better conversational ability compared to training on a single skill. 2) Careful choice of decoding algorithm is critical - models with the same perplexity can give very different results depending on the generation strategy used. In particular, controlling the minimum response length is important to balance between dull/unengaging and incoherent responses.3) Scaling up model size (number of parameters) and training data gives improvements, but other ingredients like skill blending and decoding choices are also very important.Through extensive experiments, the authors provide recipes to build chatbots that outperform prior work like Meena in human evaluations of engagingness and humanness. The paper thus highlights model architecture, training objectives, datasets, and decoding as key ingredients in building open-domain chatbots.


## What is the main contribution of this paper?

The main contribution of this paper is developing recipes for building open-domain chatbots that perform well on engagingness and humanness in human evaluations. The key takeaways are:1) Blending skills: Large improvements can be made by fine-tuning models on datasets that emphasize particular conversational skills like personality, knowledge, and empathy. The authors show that using the Blended Skill Talk (BST) dataset gives significant gains compared to just using Reddit for pre-training.2) Generation strategies: The choice of decoding algorithm is critical. The authors find that controlling the minimum length of responses with beam search gives much better results than unconstrained beam search. This prevents dull and unengaging short responses.The paper shows that large Transformer models fine-tuned on BST with optimized decoding outperform existing chatbots like Meena on engagingness and humanness metrics in human evaluations. The best models are made publicly available.The authors also analyze limitations and failure cases of their models, showing there are still issues around repetition, factual correctness, and deeper understanding that need to be addressed in future work. But overall the paper provides good recipes for building chatbots that are more engaging and humanlike.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes recipes for building engaging and humanlike open-domain chatbots through large-scale pre-training and fine-tuning on blended conversational skills data, and using careful decoding strategies like minimum beam length; they release 90M, 2.7B and 9.4B parameter models outperforming Meena on human metrics, while analyzing remaining limitations.
