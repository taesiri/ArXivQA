# [Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An   Adversarial Perspective](https://arxiv.org/abs/2402.18607)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates the potential privacy and fairness risks in the emerging data sharing paradigm involving pre-trained diffusion models. Specifically, it considers a system where one party (the model sharer) trains a diffusion model on private data and provides another party (the model receiver) black-box access to the model to generate synthetic data for downstream tasks. The paper explores whether there are privacy and security vulnerabilities for the two parties in this setting.

Proposed Solutions:
The paper proposes two types of attacks - a fairness poisoning attack performed by the sharer and a property inference attack performed by the receiver.

The fairness poisoning attack manipulates the distribution of sensitive features (e.g. gender) in the training data to inject bias into the receiver's downstream classifier, degrading its fairness while maintaining accuracy. This is formulated as an optimization problem to maximize dependence between sensitive features and labels in the poisoned data, subject to a data utility constraint. A greedy algorithm is proposed to construct the poisoned dataset.

The property inference attack estimates the proportion of target properties (e.g. age group) in the sharer's private training data by sampling from the diffusion model and analyzing the distribution. Two variants are proposed - one uses an auxiliary dataset to train a property discriminator, while the other uses CLIP. Theoretical error bounds are derived using Hoeffding's inequality.

Main Contributions:

- First study to explore potential privacy and fairness attacks in sharing pre-trained diffusion models.

- Novel fairness poisoning attack that degrades model fairness while preserving accuracy by manipulating training data distribution. Formulated as an information-theoretic optimization problem.  

- Practical black-box property inference attack to accurately estimate proportions of sensitive properties in private training data. Established theoretical error bounds.

- Extensive experiments validate attack effectiveness and generality across datasets and model types. Comparative analysis provides insights into suitable defenses.

- Attacks highlight need for robust auditing and privacy protection protocols to enable secure and ethical sharing of diffusion models.
