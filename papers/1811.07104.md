# On Hallucinating Context and Background Pixels from a Face Mask using   Multi-scale GANs

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to hallucinate realistic context and background pixels from only a face mask input using a multi-scale generative adversarial network (GAN) model. The key hypothesis is that a cascaded network of GAN blocks, with each block operating at a different resolution and guided by the previous block, can generate high quality and identity-preserving synthetic context and background for a face mask. The model aims to produce realistic full face images starting only from the provided face region, without requiring any additional user input or annotation.In summary, the main research questions and hypotheses are:- Can a GAN model directly generate realistic context and background pixels from only a face mask, without any user guidance? - Will a cascaded multi-resolution approach, with each GAN block predicting pixels at a certain scale while being guided by lower resolution predictions, produce better results compared to a single resolution model?- Can such a model generate identity-preserving and realistic synthetic face images for varied datasets not seen during training?- How does this approach compare with existing facial inpainting and face swapping methods in terms of image realism and identity preservation?- Can the model act as a data augmentation technique by generating supplemental training images with diverse context/background for existing datasets?The paper presents experiments to validate these hypotheses and demonstrates the model's ability to produce high quality synthetic face images starting from varied face masks across different datasets.
