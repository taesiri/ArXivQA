# [Robust Mixture-of-Expert Training for Convolutional Neural Networks](https://arxiv.org/abs/2308.10110)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How to make convolutional neural networks (CNNs) more robust against adversarial attacks when integrated with sparsely-gated mixture of experts (MoE)?Specifically, the paper investigates whether the standard adversarial training approach used for vanilla CNNs remains effective for CNNs integrated with sparse MoE modules. The key hypotheses seem to be:1) Adversarial training for MoE-CNNs is non-trivial compared to vanilla CNNs due to the added complexity of the sparsely-gated routing system. 2) Improving the robustness of the routers alone is not sufficient to robustify the overall MoE-CNN model. The routing decisions need to be robustified in conjunction with the expert CNN pathways.3) A coupled training approach is needed that enables the routers and expert pathways to co-adapt and make concerted efforts towards improving robustness of the overall MoE-CNN model.So in summary, the main research question is how to achieve robustness for MoE-integrated CNNs, with the central hypothesis being that this requires a coupled adversarial training approach tailored for the router-expert structure of MoE-CNNs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new adversarial training framework called AdvMoE for robustifying Mixture of Experts (MoE) convolutional neural networks (CNNs). - Dissecting the robustness of MoE-CNNs into two dimensions - robustness of the routers (gating functions for expert selection) and robustness of the experts (subnetworks of the CNN backbone). Analyzing how these two components interact during adversarial training.- Developing a router-expert alternating adversarial training approach based on bi-level optimization to make routers and experts robustify each other in an adaptive fashion. This is in contrast to conventional adversarial training that optimizes the entire model jointly.- Conducting extensive experiments on 4 CNN architectures over 4 benchmark datasets to demonstrate that the proposed AdvMoE method can improve adversarial robustness over both conventionally trained dense CNNs and MoE-CNNs. AdvMoE also retains the efficiency benefits of sparse MoE models.In summary, the key innovation seems to be proposing a new adversarial training protocol tailored for MoE-CNNs by decomposing the model into routers and experts and alternately training them to achieve better robustness than standard adversarial training methods. The router-expert interaction analysis and bi-level optimization framework for robustness appear to be the main technical novelties.
