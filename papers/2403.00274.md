# [CustomListener: Text-guided Responsive Interaction for User-friendly   Listening Head Generation](https://arxiv.org/abs/2403.00274)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
Existing listening head generation methods can generate diverse and fine-grained listener motions based on simple emotional labels (e.g. positive, neutral, negative). However, they lack explicit control over the motions and can't generate realistic listener agents that have customizable attributes like identity, personality, habits. Simple labels are not enough for accurate control.

Proposed Solution - CustomListener Framework
The paper proposes the CustomListener framework that enables free-form text-guided listening head generation, allowing explicit control over motions. Key components:

1. Static to Dynamic Portrait (SDP) Module: Transforms static portrait tokens from text prior into dynamic tokens that incorporate time-dependent info about motion rhythm and completion. Allows motions to fluctuate with speaker's semantics and movements to achieve speaker-listener coordination.

2. Past-Guided Generation (PGG) Module: Generates motion prior based on similarity between current and past portrait tokens. Uses diffusion model with fixed noise at segment transitions. Ensures motion coherence between segments and maintains listener's customizable attributes/habits reflected in past segments.

Main Contributions:

- Enables freely controllable listening head generation guided by text priors instead of simple labels
- SDP module to transform static portrait into dynamic one for speaker-listener coordination 
- PGG module to achieve coherent motions and maintain listener's customized attributes/habits
- Constructed text-annotated listening head datasets based on ViCo and RealTalk
- Experiments show state-of-the-art performance in controllability, interactivity and realism

The key novelty is the ability to guide motions via free-form text and explicitly control listener behaviors, while ensuring coordination with speaker and coherence between segments.
