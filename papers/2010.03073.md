# [Beyond [CLS] through Ranking by Generation](https://arxiv.org/abs/2010.03073)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the conclusion section, the main research question addressed in this paper is whether large pretrained neural language models can be effective for ranking tasks in information retrieval, specifically for answer selection/passage ranking. The authors propose a generative approach using models like GPT2 and BART fine-tuned for question generation conditioned on passage content. Their main hypothesis seems to be that this generative approach of "ranking by generation" can match or exceed the effectiveness of state-of-the-art discriminative ranking models based on semantic similarity. The robust experimental results presented on four datasets appear to confirm this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Proposing a new generative approach for information retrieval based on fine-tuning large pretrained neural language models like GPT2 and BART. 2. Demonstrating that these generative models can be very effective for ranking tasks like answer selection/passage ranking when trained with the right objectives.3. Showing that using unlikelihood losses to leverage negative examples improves the performance of the generative ranking models.4. Providing robust experimental results on multiple standard datasets that show the proposed generative ranking approaches are as effective as state-of-the-art semantic similarity based discriminative models for answer selection.In summary, the key contribution is revisiting generative models for IR using modern large pretrained language models, and showing they can achieve state-of-the-art effectiveness when trained properly. The proposed generative framework offers an alternative to discriminative ranking models based on semantic similarity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a new generative approach for information retrieval based on fine-tuning large pretrained language models like GPT2 and BART on the task of query generation conditioned on document content, and shows this is competitive with state-of-the-art discriminative ranking models on several answer selection datasets.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in generative models for information retrieval:- This paper focuses specifically on using large pretrained neural language models like GPT-2 and BART for passage ranking. Much prior work on generative models for IR used traditional language modeling approaches with separate models trained per document. - The authors propose novel unlikelihood and ranking losses to leverage negative examples when fine-tuning the generative models. This is different from prior work that mainly used maximum likelihood training.- They demonstrate strong empirical results on multiple standard passage ranking datasets. Their proposed methods are competitive or superior to state-of-the-art discriminative BERT models. - Contemporaneous work by Nogueira et al. (2020) also explores generative ranking but uses generation of keywords rather than full question generation likelihood. - This work helps revive interest in generative ranking approaches, which had fallen out of favor compared to discriminative methods in recent years with the rise of large pretrained models like BERT.- The proposed techniques for leveraging negative examples via unlikelihood training could be applicable more broadly when fine-tuning generative models for discriminative tasks.Overall, this paper makes important contributions in adapting modern neural language models for generative ranking, proposing effective training techniques, and achieving strong empirical results. It represents an advance over prior generative IR techniques and state-of-the-art discriminative methods.
