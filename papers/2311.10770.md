# [Exponentially Faster Language Modelling](https://arxiv.org/abs/2311.10770)

## Summarize the paper in one sentence.

 The paper presents FastBERT, a BERT variant that replaces feedforward layers with fast feedforward networks to achieve exponential speedup in inference while retaining similar downstream performance.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper presents FastBERT, a variant of BERT that replaces the feedforward layers with fast feedforward networks (FFNs). FFNs organize neurons in a binary tree structure and only execute one branch of the tree per input, allowing exponential acceleration. FastBERT-1x11 uses only 0.3% of its neurons during inference but achieves comparable performance to BERT-base on GLUE tasks. The authors implement conditional matrix multiplication to simulate FFNs and achieve up to 78x speedup on CPU and 39x on GPU over standard feedforward implementations. The results demonstrate the potential for conditional neural execution to greatly accelerate large language models like BERT while retaining accuracy. The authors call for native implementations of conditional matrix multiplication in deep learning frameworks to fully deliver on this speedup potential.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper presents FastBERT, a variant of the BERT architecture that replaces dense feedforward network layers with fast feedforward networks (FFNs) in order to demonstrate the potential for conditional neural execution to accelerate inference in large language models. FastBERT selectively activates only a tiny fraction of neurons (e.g. 0.3% for FastBERT-1x11) during each inference pass by organizing neurons in balanced binary tree structures and only executing one branch of each tree based on the input. Experiments show FastBERT models perform on par with standard BERT models on GLUE benchmarks while using exponentially fewer neurons. The core operation enabling this acceleration is conditional matrix multiplication (CMM), which the authors argue has great promise but currently lacks efficient implementations in deep learning frameworks. Using high-level CPU and GPU code, they demonstrate CMM can already achieve up to 78x speedup over optimized dense matrix multiplication. The authors argue CMM primitives in device programming interfaces could fully unlock the 341x theoretical speedup. Overall, FastBERT provides convincing evidence that conditional neural execution could drastically accelerate inference in large language models with minimal performance trade-offs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper demonstrates that large language models like BERT only need to use an exponential fraction of their parameters during inference, as shown through a modified BERT model called FastBERT that selectively uses just 0.3% of its neurons yet performs comparably on downstream tasks.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can large language models like BERT perform well on downstream NLP tasks while only using an exponentially small fraction of their parameters during inference?

The authors hypothesize that language models only need to engage an exponential fraction of their neurons/parameters for individual inferences in order to achieve good performance. They test this hypothesis through the development of FastBERT, a variant of BERT that uses fast feedforward networks to selectively activate a tiny fraction of neurons during inference. The goal is to show that FastBERT can perform on par with BERT while being exponentially faster due to this sparse activation.

In summary, the key hypothesis is that language models are highly overparameterized and the vast majority of their parameters are redundant for any given inference. The paper aims to demonstrate this via FastBERT and analyses of its performance compared to baseline BERT models.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal and demonstration of FastBERT, a variant of the BERT language model architecture that uses fast feedforward networks (FFNs) instead of regular feedforward networks in its intermediate layers. The key properties of FastBERT are:

- It uses an exponentially smaller fraction of neurons during inference compared to regular BERT, while achieving comparable performance on downstream tasks. Specifically, FastBERT-1x11 uses only 0.3% of its neurons during inference, compared to 100% for regular BERT.

- This exponential reduction in active neurons is achieved by organizing the FFN neurons in a binary tree structure and only executing one branch of the tree per inference based on the input. This conditional execution is implemented via conditional matrix multiplication.

- A naive CPU implementation of FastBERT's conditional matrix multiplication already achieves a 78x speedup over optimized dense matrix multiplication for feedforward networks. This demonstrates the potential for large efficiency gains from conditional neural execution. 

- FastBERT performs on par with BERT on a range of GLUE benchmark tasks, showing that the exponential neuron reduction does not negatively impact model quality.

In summary, the paper proposes FastBERT as a proof of concept for achieving exponentially faster inference in language models through conditional neural execution, while retaining model performance. The results highlight the potential efficiency gains from conditional execution and help motivate further work on optimized implementations.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on efficient language modeling:

- The main contribution is introducing FastBERT, a variant of BERT that uses fast feedforward networks to selectively activate only a small fraction of neurons during inference. This is a novel way to improve efficiency over other methods like knowledge distillation or pruning that have been more thoroughly explored.

- The paper shows FastBERT can match performance of BERT-base while using only 0.3% of neurons, demonstrating the big potential for efficiency gains from conditional execution. This speedup is much greater than achieved by other approaches like distillation alone.

- FastBERT relies on a new operation, conditional matrix multiplication (CMM), to implement the fast feedforward networks. The paper acknowledges that efficient implementation of CMM remains an open challenge. Other methods like pruning and quantization tend to leverage more standard operations.

- The paper focuses narrowly on transformer feedforward layers as a target for efficiency gains, unlike other works that look at all layers or other model architectures. The gains from fast feedforward could likely combine with improvements in attention. 

- The empirical evaluations are limited to GLUE tasks with a relatively small BERT-base sized model. More extensive evaluations on larger models and datasets would help demonstrate scalability.

Overall, FastBERT introduces a novel direction for efficient inference based on conditional execution. While promising, efficient implementation of the core CMM operation and more extensive evaluations remain open challenges compared to other established methods like distillation and pruning. The paper makes a conceptual contribution highlighting the potential for conditional execution, but substantial engineering work is still needed to fully deliver on those speedups.


## What future research directions do the authors suggest?

 The authors suggest the following future research directions:

- Implementation of primitives for conditional neural execution as part of device programming interfaces like Intel MKL, NVIDIA cuBLAS, etc. This would allow for efficient implementation of conditional matrix multiplication and unlock the full potential of fast feedforward networks.

- Exploration of different tree-structuring approaches for fast feedforward networks beyond binary trees. This could lead to even faster models that use an even smaller fraction of parameters during inference.

- Applying conditional neural execution more broadly beyond language modeling, e.g. in computer vision. The techniques could bring similar benefits in other domains with large feedforward networks.

- Investigating techniques to reduce the performance degradation on some downstream tasks when using very deep/narrow fast feedforward networks. This could help close the remaining small performance gap compared to baseline models.

- Developing methods to efficiently train very deep fast feedforward networks. The paper relies on a simple training procedure, but more advanced techniques could enable training even larger fast models.

In summary, the main suggestion is to work on enabling efficient conditional neural execution through low-level library support and exploring applications of the technique across domains. There are also open research questions around model training and structuring as well as closing small performance gaps.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- FastBERT - The name of the modified BERT model presented in the paper that uses fast feedforward networks instead of regular feedforward networks. 

- Conditional neural execution - The concept of only executing a small fraction of neurons in a neural network for each input, by organizing the neurons in a conditional manner. This is the key idea behind FastBERT.

- Fast feedforward networks (FFFs) - The neural network architecture proposed that organizes neurons in a binary tree structure and only executes one branch of the tree per input. This allows exponential speedups. 

- Conditional matrix multiplication (CMM) - The core mathematical operation for performing inference in FFFs that only computes dot products for a subset of weight matrix columns based on the input.

- Acceleration, speedup - Key goals of FastBERT and FFFs in general. The paper demonstrates and quantifies the potential for acceleration over standard feedforward networks. 

- Language modeling - The application domain that FastBERT targets. Showing the potential benefits of conditional execution for large language models like BERT.

- BERT, Transformer - Standard neural network architectures that the paper bases FastBERT on and compares against.

- Downstream performance - How well FastBERT transfers to end tasks like GLUE. Shows it can match baseline BERT.

- Implementation, hardware - Discusses prospects for efficiently implementing CMM on CPUs and GPUs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper claims that language models only need an exponential fraction of their neurons for individual inferences. What evidence does the paper provide to support this claim? How convincing is this evidence?

2. The core idea of the paper is to replace feedforward networks with fast feedforward networks (FFFs) that selectively activate only a subset of neurons. What are the key architectural differences between feedforward networks and FFFs that enable this sparse activation? 

3. The paper presents FastBERT as a proof-of-concept model. How is FastBERT constructed? What modifications were made to the original BERT architecture? How well does FastBERT perform on downstream tasks compared to BERT?

4. The paper claims significant inference speedups from using FFFs instead of feedforward networks. What is the theoretical speedup discussed in the paper? How was the speedup evaluation performed? What were the actual speedups achieved on CPU and GPU?

5. Conditional matrix multiplication (CMM) is central to enabling sparse activation in FFFs. How does CMM differ from dense matrix multiplication? What makes an efficient implementation of CMM challenging? 

6. The paper provides some high-level CPU code to implement CMM. What are the limitations of this implementation? What further optimizations could be made to unlock the full potential of CMM?

7. The paper argues that FFFs are compatible with existing CPU and GPU hardware. What modifications would be needed at the hardware or software level to fully optimize FFF execution? 

8. What future engineering efforts does the paper suggest could help realize the full speedup potential of FFFs? Are these efforts reasonable and feasible?

9. How robust is the FastBERT model presented? Does it achieve consistent speedups and performance across different tasks, data sets, and hardware?

10. What are the broader implications of the paper for efficient inference in large language models? Could the approach be extended to models other than BERT? What limitations need to be addressed?
