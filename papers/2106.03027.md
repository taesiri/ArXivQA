# [Model Zoo: A Growing "Brain" That Learns Continually](https://arxiv.org/abs/2106.03027)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we develop an effective continual learning algorithm that leverages data from past tasks to improve performance on future tasks, while avoiding catastrophic forgetting of past tasks?

The key hypothesis proposed in the paper is that splitting the learning capacity of the model across multiple smaller models trained on sets of related tasks can help mitigate the problem of catastrophic forgetting in continual learning. 

Specifically, the paper argues that:

1) Training a single model on multiple tasks can be detrimental if the tasks compete for the model's limited capacity. Tasks that are dissimilar may interfere with each other when trained jointly.

2) It is beneficial to train smaller models on sets of related/synergistic tasks. This avoids negative interference between dissimilar tasks.

3) An ensemble approach that grows the model capacity over time by adding smaller models trained on related tasks can leverage synergies between tasks while avoiding catastrophic forgetting.

The paper provides theoretical analysis to characterize task relatedness and competition. It also proposes a continual learning algorithm called Model Zoo that implements the idea of growing model capacity and training models on related tasks. Experiments demonstrate that Model Zoo outperforms existing continual learning methods on several benchmarks.

In summary, the central hypothesis is that explicitly accounting for task relatedness and splitting model capacity can enable effective continual learning that avoids catastrophic forgetting. Model Zoo is proposed as an instantiation of this approach.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. A theoretical analysis characterizing when training a single model on multiple tasks can be beneficial versus detrimental due to task competition. The key idea is defining a notion of relatedness between tasks. 

2. The development of a continual learning algorithm called Model Zoo that grows an ensemble of models, with each model trained on the current task and a subset of past tasks. This is inspired by boosting and helps mitigate task competition.

3. An empirical evaluation showing that Model Zoo outperforms existing continual learning methods on several benchmarks, with large gains in some cases. The method demonstrates strong forward and backward transfer.

4. The finding that a simple baseline method of training an isolated model per task also outperforms many existing continual learning methods. This suggests existing methods are not sufficiently leveraging data from multiple tasks.

In summary, the main contribution appears to be the proposed Model Zoo continual learning algorithm and analysis showing it mitigates task competition more effectively than prior methods. The paper also provides useful theoretical and empirical insights into continual learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a continual learning method called Model Zoo that grows an ensemble of small models, each trained on the current task and a subset of past tasks, in order to effectively leverage synergies between tasks while avoiding detrimental competition between dissimilar tasks.


## How does this paper compare to other research in the same field?

 This paper presents a new continual learning algorithm called Model Zoo that grows an ensemble of small models, with each model trained on a subset of tasks. The key ideas and findings are:

- It provides a theoretical analysis of when and why training a single model on multiple tasks can be detrimental due to task competition. This motivates their approach of splitting capacity across models.

- Model Zoo is inspired by boosting and selects subsets of tasks in each round based on which tasks have high training loss currently. This allows it to incrementally increase capacity on the difficult tasks.

- Empirically, Model Zoo achieves state-of-the-art results on several continual learning benchmarks, outperforming existing methods by large margins in some cases.

- Surprisingly, a basic baseline of training separate models per task also outperforms many existing algorithms, indicating they do not sufficiently leverage data from multiple tasks. 

This represents an advance over prior continual learning methods that typically use a single model with replay, architectural modifications, or regularization. The analysis of task competition and results demonstrating the shortcomings of existing methods are novel contributions. The capacity splitting idea is loosely related to prior work on modular and growing networks, but the application to continual learning and the boosting-based task selection are new. Overall, this provides important insights into effectively combining data from multiple tasks in a sequential setting.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Developing better methods to quantify and estimate the relatedness/competition between tasks. The transfer exponent defined in the paper is one way to do this, but more investigation is needed. Discovering good proxies for task relatedness that can be used by continual learning algorithms like Model Zoo could lead to improvements.

- Exploring different model architectures and training schemes tailored for continual learning. The authors propose a simple ensemble approach with Model Zoo, but more sophisticated architectures that allow capacity growth while limiting forgetting may be beneficial. 

- Scaling continual learning methods like Model Zoo to more complex datasets and tasks. The benchmarks used in the paper are relatively small image classification datasets. Testing on larger, more realistic datasets and tasks would be an important next step.

- Reducing the need for full replay of previous tasks. Model Zoo relies on replaying data from previous tasks, but reducing this replay while maintaining performance could better match constrained, continual learning settings.

- Investigating theoretical guarantees for continual learning methods, building on the analysis in the paper. The authors provide some theoretical results on task relatedness, but more work is needed to understand convergence and generalization of continual learners.

- Comparing to and combining ideas from related fields like transfer learning, meta-learning, and multi-task learning that could inform continual learning.

- Developing better evaluation protocols and metrics for continual learning that accurately measure capabilities like forward/backward transfer.

In summary, key directions are developing better ways to measure and exploit task relatedness, scaling continual learning methods, reducing replay dependence, establishing theory, and improving evaluation. Advances in these areas could significantly improve continual learning for real-world applications.


## Summarize the paper in one paragraph.

 The paper theoretically analyzes how training a single model on multiple tasks can be beneficial or detrimental depending on the relatedness of the tasks. It shows that tasks that are strongly related, with simple transformations between inputs/outputs, benefit from joint training. However, dissimilar tasks that compete for model capacity can deteriorate accuracy. Based on this analysis, the paper develops a continual learning algorithm called Model Zoo that grows an ensemble of small models, each trained on the current task and a few related past tasks. This allows splitting capacity across related tasks. Model Zoo selects past tasks using a boosting-inspired mechanism. Empirical results demonstrate Model Zoo outperforms existing continual learning methods by significant margins across various benchmarks. Surprisingly, even a basic method that trains a separate small model per task outperforms most existing methods, indicating they fail to properly leverage multiple tasks. Overall, the paper provides important theoretical and empirical evidence that effectively handling task relatedness is key to performing well in continual learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. The authors use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates their method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. 

The authors demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. They comprehensively evaluate Model Zoo on existing task-incremental continual learning benchmarks and show comparisons with existing methods. Model Zoo obtains better accuracy than existing methods on the evaluated benchmarks. Improvement in average per-task accuracy is quite large in some cases, e.g., 30% for Split-miniImagenet. The authors also show that Model Zoo demonstrates strong forward and backward transfer. They find that even a simple isolated learner that trains a small model on tasks from each episode significantly outperforms most existing continual learning methods, indicating room for improvement.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a continual learning method called Model Zoo that grows an ensemble of small multi-task models, with each model trained on the current task and a subset of past tasks. At each episode, a new model is added that focuses on the current task and past tasks that had high training loss. This is inspired by boosting methods like AdaBoost which add new weak learners focused on misclassified examples. Model Zoo automatically splits model capacity across related tasks to mitigate task competition. At test time, predictions are made by averaging outputs from all models trained on that task. Empirical results demonstrate Model Zoo outperforms existing continual learning methods by effectively leveraging data from multiple tasks without catastrophic forgetting. The capacity splitting mechanism enables both forward and backward transfer.


## What problem or question is the paper addressing?

 The paper presents a theoretical analysis and new algorithm for continual learning, where a model learns new tasks sequentially over time without forgetting past tasks. The key points are:

1. It provides a theoretical analysis showing when training a single model on multiple tasks can be beneficial versus detrimental due to competition between tasks. The analysis defines a notion of task relatedness and shows training on unrelated tasks that "compete" for model capacity can hurt performance.

2. Motivated by this analysis, the paper proposes a new continual learning algorithm called Model Zoo that grows an ensemble of small models, each trained on the current task and a subset of past tasks. This splits model capacity to mitigate negative transfer between competing tasks. 

3. The paper comprehensively evaluates Model Zoo on benchmark datasets and shows it outperforms existing continual learning methods by significant margins (e.g. 30% on Split-miniImagenet). It also demonstrates strong forward and backward transfer.

4. Surprisingly, the paper finds even a basic method that trains a small model per task independently outperforms most existing methods. This suggests existing methods fail to sufficiently leverage data from multiple tasks.

In summary, the key focus is developing a theoretical understanding of when training on multiple tasks helps or hurts, and using this to design a new continual learning algorithm that effectively leverages synergistic tasks while avoiding negative transfer between competing tasks. A critical finding is that common benchmarks may not properly evaluate continual learning methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Continual learning - The paper focuses on continual learning, where models are trained sequentially on new tasks while retaining performance on old ones. Avoiding catastrophic forgetting is a key challenge.

- Task relatedness - The paper analyzes how performance depends on the relatedness or similarity between tasks. Related tasks can be synergistic and improve learning, while unrelated tasks can compete. 

- Model Zoo - The proposed method which trains an ensemble of small multi-task models, each trained on new and old related tasks. It automatically determines related tasks.

- Capacity splitting - Key idea of splitting model capacity across related tasks to mitigate competition and leverage synergies.

- Forward transfer - Performance on a new task, measures if old tasks provide benefit. 

- Backward transfer - Retained performance on old tasks, measures if learning new tasks causes forgetting.

- Task competition - Learning new unrelated tasks can hurt old task performance by competing for limited model capacity.

- Statistical learning theory - Theoretical analysis drawing on VC dimension and generalization bounds to study multi-task learning.

- Transfer exponent - New theoretical construct to quantify task relatedness and characterize competition.

So in summary, key terms revolve around continual learning, task relationships, splitting capacity, measuring transfer, and theoretical analysis. The Model Zoo method and analysis of task competition seem to be the main novel contributions.
