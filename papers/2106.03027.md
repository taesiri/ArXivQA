# [Model Zoo: A Growing "Brain" That Learns Continually](https://arxiv.org/abs/2106.03027)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an effective continual learning algorithm that leverages data from past tasks to improve performance on future tasks, while avoiding catastrophic forgetting of past tasks?The key hypothesis proposed in the paper is that splitting the learning capacity of the model across multiple smaller models trained on sets of related tasks can help mitigate the problem of catastrophic forgetting in continual learning. Specifically, the paper argues that:1) Training a single model on multiple tasks can be detrimental if the tasks compete for the model's limited capacity. Tasks that are dissimilar may interfere with each other when trained jointly.2) It is beneficial to train smaller models on sets of related/synergistic tasks. This avoids negative interference between dissimilar tasks.3) An ensemble approach that grows the model capacity over time by adding smaller models trained on related tasks can leverage synergies between tasks while avoiding catastrophic forgetting.The paper provides theoretical analysis to characterize task relatedness and competition. It also proposes a continual learning algorithm called Model Zoo that implements the idea of growing model capacity and training models on related tasks. Experiments demonstrate that Model Zoo outperforms existing continual learning methods on several benchmarks.In summary, the central hypothesis is that explicitly accounting for task relatedness and splitting model capacity can enable effective continual learning that avoids catastrophic forgetting. Model Zoo is proposed as an instantiation of this approach.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. A theoretical analysis characterizing when training a single model on multiple tasks can be beneficial versus detrimental due to task competition. The key idea is defining a notion of relatedness between tasks. 2. The development of a continual learning algorithm called Model Zoo that grows an ensemble of models, with each model trained on the current task and a subset of past tasks. This is inspired by boosting and helps mitigate task competition.3. An empirical evaluation showing that Model Zoo outperforms existing continual learning methods on several benchmarks, with large gains in some cases. The method demonstrates strong forward and backward transfer.4. The finding that a simple baseline method of training an isolated model per task also outperforms many existing continual learning methods. This suggests existing methods are not sufficiently leveraging data from multiple tasks.In summary, the main contribution appears to be the proposed Model Zoo continual learning algorithm and analysis showing it mitigates task competition more effectively than prior methods. The paper also provides useful theoretical and empirical insights into continual learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a continual learning method called Model Zoo that grows an ensemble of small models, each trained on the current task and a subset of past tasks, in order to effectively leverage synergies between tasks while avoiding detrimental competition between dissimilar tasks.


## How does this paper compare to other research in the same field?

This paper presents a new continual learning algorithm called Model Zoo that grows an ensemble of small models, with each model trained on a subset of tasks. The key ideas and findings are:- It provides a theoretical analysis of when and why training a single model on multiple tasks can be detrimental due to task competition. This motivates their approach of splitting capacity across models.- Model Zoo is inspired by boosting and selects subsets of tasks in each round based on which tasks have high training loss currently. This allows it to incrementally increase capacity on the difficult tasks.- Empirically, Model Zoo achieves state-of-the-art results on several continual learning benchmarks, outperforming existing methods by large margins in some cases.- Surprisingly, a basic baseline of training separate models per task also outperforms many existing algorithms, indicating they do not sufficiently leverage data from multiple tasks. This represents an advance over prior continual learning methods that typically use a single model with replay, architectural modifications, or regularization. The analysis of task competition and results demonstrating the shortcomings of existing methods are novel contributions. The capacity splitting idea is loosely related to prior work on modular and growing networks, but the application to continual learning and the boosting-based task selection are new. Overall, this provides important insights into effectively combining data from multiple tasks in a sequential setting.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing better methods to quantify and estimate the relatedness/competition between tasks. The transfer exponent defined in the paper is one way to do this, but more investigation is needed. Discovering good proxies for task relatedness that can be used by continual learning algorithms like Model Zoo could lead to improvements.- Exploring different model architectures and training schemes tailored for continual learning. The authors propose a simple ensemble approach with Model Zoo, but more sophisticated architectures that allow capacity growth while limiting forgetting may be beneficial. - Scaling continual learning methods like Model Zoo to more complex datasets and tasks. The benchmarks used in the paper are relatively small image classification datasets. Testing on larger, more realistic datasets and tasks would be an important next step.- Reducing the need for full replay of previous tasks. Model Zoo relies on replaying data from previous tasks, but reducing this replay while maintaining performance could better match constrained, continual learning settings.- Investigating theoretical guarantees for continual learning methods, building on the analysis in the paper. The authors provide some theoretical results on task relatedness, but more work is needed to understand convergence and generalization of continual learners.- Comparing to and combining ideas from related fields like transfer learning, meta-learning, and multi-task learning that could inform continual learning.- Developing better evaluation protocols and metrics for continual learning that accurately measure capabilities like forward/backward transfer.In summary, key directions are developing better ways to measure and exploit task relatedness, scaling continual learning methods, reducing replay dependence, establishing theory, and improving evaluation. Advances in these areas could significantly improve continual learning for real-world applications.
