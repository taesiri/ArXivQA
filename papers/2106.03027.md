# [Model Zoo: A Growing "Brain" That Learns Continually](https://arxiv.org/abs/2106.03027)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an effective continual learning algorithm that leverages data from past tasks to improve performance on future tasks, while avoiding catastrophic forgetting of past tasks?The key hypothesis proposed in the paper is that splitting the learning capacity of the model across multiple smaller models trained on sets of related tasks can help mitigate the problem of catastrophic forgetting in continual learning. Specifically, the paper argues that:1) Training a single model on multiple tasks can be detrimental if the tasks compete for the model's limited capacity. Tasks that are dissimilar may interfere with each other when trained jointly.2) It is beneficial to train smaller models on sets of related/synergistic tasks. This avoids negative interference between dissimilar tasks.3) An ensemble approach that grows the model capacity over time by adding smaller models trained on related tasks can leverage synergies between tasks while avoiding catastrophic forgetting.The paper provides theoretical analysis to characterize task relatedness and competition. It also proposes a continual learning algorithm called Model Zoo that implements the idea of growing model capacity and training models on related tasks. Experiments demonstrate that Model Zoo outperforms existing continual learning methods on several benchmarks.In summary, the central hypothesis is that explicitly accounting for task relatedness and splitting model capacity can enable effective continual learning that avoids catastrophic forgetting. Model Zoo is proposed as an instantiation of this approach.
