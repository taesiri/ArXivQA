# [FIANCEE: Faster Inference of Adversarial Networks via Conditional Early   Exits](https://arxiv.org/abs/2304.10306)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we speed up inference and reduce computational cost for generative adversarial networks (GANs) by dynamically routing inputs through early exits conditioned on input complexity?

The key hypothesis appears to be that:

By adding lightweight conditional early exit branches at intermediate layers of a GAN generator, adaptively choosing exit points based on input complexity predictions, and using a small guiding example database, we can significantly reduce computations while maintaining consistent output quality for generative image synthesis tasks.

In summary, the paper proposes a method to implement dynamic early exits in GANs in order to reduce redundant computations and accelerate inference, especially for easier instances, while upholding a minimum quality threshold. The core ideas are:

- Add multiple early exit branches of varying depth to tap intermediate layer outputs 

- Use a predictor network to estimate output quality and route inputs to earliest sufficient exit 

- Retrieve similar patches from a database to guide synthesis and improve early exit quality

- Evaluate approach on semantic image synthesis and neural avatar cross-reenactment tasks

The key hypothesis is that by combining these techniques, major compute savings can be achieved on common generative models without excessively sacrificing output quality.
