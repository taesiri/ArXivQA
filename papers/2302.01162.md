# [Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using   Pixel-aligned Reconstruction Priors](https://arxiv.org/abs/2302.01162)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we generate high-quality and diverse 3D textured human models without requiring a large dataset of 3D ground truth data? 

The key ideas and contributions in addressing this question are:

- Leveraging priors from 2D human image generators (StyleGAN-Human) and 3D human reconstructor (PIFu) to provide inductive bias and supervision for training the 3D generator. This avoids the need for a large 3D ground truth dataset.

- Proposing a novel prior induction mechanism to map the latent space of StyleGAN-Human to shape and texture feature volumes compatible with PIFu's feature space. The outputs provide supervisory signals for training the 3D generator.

- Designing tailored losses (latent prior loss, adversarial loss, normal/depth losses) applied to generated 3D volumes and intermediate features to enable effective and efficient training.

- A refinement module to further improve the quality of generated textures by performing image-to-image translation and back-projecting refined textures to the 3D model.

- Demonstrating state-of-the-art performance in generating high quality and diverse 3D human models compared to previous methods. The model also supports applications like shape interpolation, retexturing, and inversion.

In summary, the key novelty and contribution is utilizing 2D/3D priors to overcome the lack of 3D training data, and designing an effective 3D generative framework capitalizing on these priors. The results significantly advance 3D human generation quality and diversity while requiring minimal 3D supervision.


## What is the main contribution of this paper?

 The key contributions of this paper appear to be:

- Proposing a new 3D human generation framework called Get3DHuman that can generate high-quality and diverse 3D textured human models. 

- Leveraging priors from 2D human generators (StyleGAN-Human) and 3D reconstructors (PIFu) to facilitate training without requiring a large corpus of 3D ground truth data. They extract shape and texture features from StyleGAN-Human outputs using a PIFu-like reconstructor as supervision.

- Introducing specially-designed losses including a latent prior loss, an adversarial feature volume loss, and intermediate losses on normal/depth maps for effective training.

- Demonstrating high-quality 3D human generation results that outperform state-of-the-art methods quantitatively and qualitatively. The framework also supports applications like shape interpolation, re-texturing, and inversion from images.

In summary, the key contribution is proposing a novel way to generate 3D textured human models by incorporating 2D and 3D priors to address the limitation of 3D ground truth data. The tailored losses and training strategy also help achieve improved quality and diversity compared to prior arts.
