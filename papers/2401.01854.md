# [Multilingual Instruction Tuning With Just a Pinch of Multilinguality](https://arxiv.org/abs/2401.01854)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- As large language models (LLMs) are adopted globally, their ability to follow instructions in multiple languages is crucial. However, creating high-quality multilingual instruction tuning datasets is challenging.

Proposed Solution: 
- Investigate cross-lingual transfer, where models acquire capabilities in a language by being tuned on another language.
- Use machine translation to create multilingual variants of existing English instruction tuning datasets.  
- Conduct controlled experiments on multilingual instruction tuning using a modern multilingual pretrained LLM (PaLM 2).

Key Contributions:
- Show that monolingual instruction tuning transfers non-negligible capabilities to other languages, especially English, Italian and Spanish.
- Adding just 40 evenly distributed multilingual examples (1% of dataset) significantly improves multilingual performance, without hurting English. This even helps unseen languages.
- Increasing number of languages from 1 to 2, 3 or 4 improves cross-lingual generalization to unseen languages.
- For some languages, multilingual tuning achieves better performance than monolingual tuning with the same number of examples.  
- No strong correlation found between cross-lingual transfer and language similarity or pretraining data proportions.

Main Conclusion:
- Cross-lingual transfer enables building capable multilingual instruction-following models even with minimal multilingual data. Just a small set of examples in a few languages can significantly enhance generalization across languages unseen during tuning.


## Summarize the paper in one sentence.

 This paper investigates how multilingual data during instruction tuning of large language models affects their ability to follow instructions across languages, finding that even a small amount of multilingual examples significantly improves cross-lingual generalization.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that only a small amount of multilingual data (as few as 40 examples distributed across languages) during instruction tuning of a multilingual large language model can significantly improve its ability to follow instructions across languages. Specifically, the paper shows that:

- Monolingual instruction tuning transfers non-negligible instruction following abilities to other languages in a zero-shot manner.

- Replacing just 1% (40 examples) of an English instruction tuning set with multilingual examples boosts performance in those languages and even languages not represented in the tuning set. 

- Increasing the number of languages in the tuning set from 1 to just 2 or 3 improves cross-lingual generalization to new unseen languages.

Overall, the key insight is that a little language diversity goes a long way for building multilingual instruction-following models, paving the path for more globally usable assistants.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Instruction tuning - Finetuning large language models on instruction-response pairs to improve their ability to follow instructions. This is a key aspect of building general-purpose LLMs.

- Cross-lingual transfer - The phenomenon where an LLM acquires abilities in some language after being finetuned on another language. Enables expanding model capabilities to more languages. 

- Multilingual instruction tuning - Finetuning models on instruction-response pairs from multiple languages. The paper investigates how much multilinguality is needed for good cross-lingual transfer.

- Language diversity - Having multiple languages represented in the instruction tuning data. Even a small amount is shown to improve multilingual performance.

- Generalization - The ability of models tuned on certain languages to have improved performance on unseen languages. Language diversity helps cross-lingual generalization.

- Low resource languages - Languages with small amounts of available training data. The paper shows you can get multilingual instruction-following with very little data per language.

- Language similarity - Linguistic similarity between language pairs. The paper finds it does not strongly correlate with cross-lingual transfer levels.

- Pretraining data per language - The amount of data per language the model saw during pretraining. Also doesn't strongly correlate with cross-lingual transfer.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What motivated the authors to explore cross-lingual transfer for building massively multilingual instruction-following language models? What gap were they trying to address?

2. Why did the authors use open-ended instructions rather than task-specific prompts for instruction tuning? What are the advantages and disadvantages of this approach? 

3. The authors used machine translation to create multilingual variants of their datasets. What are some potential issues with using translated data instead of naturally sourced prompts? How might this impact the conclusions?

4. In the monolingual instruction tuning experiments, what factors might explain why certain languages like English, Italian and Spanish transferred better on average than languages like Hindi?  

5. When evaluating the tradeoff between English and multilingual examples, what might account for the finding that a small number of multilingual examples helps even unseen languages? What mechanisms enable this cross-lingual generalization?

6. The paper finds that having just 2-4 languages in the tuning set improves cross-lingual generalization compared to monolingual tuning. Why does language diversity help even when the total number of examples is controlled? 

7. For the language similarity analysis, what other dimensions of similarity beyond script, family, and intelligibility may impact cross-lingual transfer (e.g. morphology)? Why did similarity not clearly correlate with transfer?

8. The paper analyzes the correlation between pretraining data amount and cross-lingual transfer. What other pretraining factors could potentially impact transfer abilities for a language (e.g. data diversity)? 

9. The authors use a specific pretrained model, PALM-2. How might the trends discovered here compare if replicated with other multilingual models? What model properties are important?

10. What are some key next steps to scale up the development of massively multilingual instruction-following models with minimal data requirements per language? What data strategies seem most promising?
