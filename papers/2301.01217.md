# [Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples](https://arxiv.org/abs/2301.01217)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How to generate effective "unlearnable examples" (UEs) to protect private visual data under a more realistic label-agnostic setting, where hackers/attackers may exploit the protected data differently from the original labeling?Specifically, the paper argues that existing UE generation methods rely on a strong assumption of label-consistency, where the hacker will use the same labels as the original dataset. This is unrealistic as hackers may relabel or use the data differently. To address this, the paper proposes a new method called "Unlearnable Clusters" that generates UEs by disrupting the inherent clustering of data, without relying on the class labels. This makes the UEs more robust to different labeling strategies by hackers. The key ideas include:- Using cluster-wise perturbations rather than class-wise.- Leveraging Vision-Language models like CLIP as the surrogate model to improve transferability.- Showing UCs are effective on diverse datasets against different models and training schemes.In summary, the main hypothesis is that exploiting inherent data clusters instead of labels can generate more robust UEs under the label-agnostic setting. The proposed Unlearnable Clusters method is evaluated to verify this hypothesis.


## What is the main contribution of this paper?

This paper proposes a new method called "Unlearnable Clusters" (UCs) to generate unlearnable examples that are robust under a "label-agnostic" setting. The key ideas and contributions are:- It identifies a limitation with existing unlearnable example (UE) generation methods - they rely on a "label-consistency" assumption where the hacker/attacker uses the same labels as the data protector. - It proposes a more realistic "label-agnostic" setting where the hacker may use different labels or different number of classes. Existing UE methods fail under this setting.- It analyzes the working mechanism of existing UE methods and finds they disrupt the uniformity and discrepancy of the deep representation.- Based on this analysis, it proposes the UC method to craft unlearnable noise in a cluster-wise manner, without relying on labels, to disrupt uniformity and discrepancy.- It leverages vision-language pretrained models like CLIP as the surrogate model to generate more transferable UCs.- It evaluates UC extensively on different datasets, target models, training schemes, and even against commercial ML platforms, showing superiority over existing UE baselines.In summary, the main contribution is proposing the label-agnostic setting for UE generation and the UC technique to craft label-agnostic UEs by exploiting clusters in the representation space. The use of CLIP and the comprehensive experiments also showcase its effectiveness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called Unlearnable Clusters (UCs) to generate unlearnable examples that can prevent machine learning models from exploiting private images in a label-agnostic setting, where the hacker's use of labels may differ from the protector's.
