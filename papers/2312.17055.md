# [Improving In-context Learning via Bidirectional Alignment](https://arxiv.org/abs/2312.17055)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) show impressive few-shot learning capabilities via in-context learning (ICL), but their large scale leads to high computational demands and deployment challenges. 
- Existing methods transfer capabilities of larger models to smaller ones by aligning output distributions, but pay little attention to the input demonstrations which influence ICL performance.

Proposed Solution:
- The paper proposes Bidirectional Alignment (BiA) to improve ICL abilities of smaller models by aligning both input preferences and output distributions with those of larger models.

- BiA incorporates a novel ranking loss to align preferences of smaller and larger models for different demonstration subsets, in addition to the token-level KL divergence loss.

- The key idea is that for effective distillation, the smaller model should not only match the output distribution, but also the relative preferences of the larger model for different inputs.

Main Contributions:
- First work to consider aligning smaller and larger models from an input preference perspective for improving ICL.

- Proposal of Bidirectional Alignment framework with ranking loss for input preference alignment and token-level distribution alignment.

- Experiments on diverse tasks show BiA consistently outperforms previous distillation baselines, demonstrating the efficacy of leveraging input preferences.

In summary, the key novelty is incorporating input preference alignment through a ranking loss for knowledge distillation to improve in-context learning abilities. The consistent gains over strong baselines highlight the importance of modeling input preferences alongside output distributions.
