# [SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference](https://arxiv.org/abs/2312.01597)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a new self-attention mechanism called Correlative Self-Attention (CSA) to enhance the capability of CLIP models for semantic segmentation tasks. The authors identify that vanilla CLIP struggles in dense prediction because its self-attention learns spatial-invariant features, preventing correct localization. In contrast, CSA computes attention by token correlations to produce spatial-covariant features. Specifically, CSA reuses CLIP's projection matrices in a simple dot-product attention, encouraging tokens to attend to positions with similar semantics. This requires no extra parameters nor fine-tuning. The proposed SCLIP model then replaces the self-attention in CLIP's last encoder layer with CSA for a training-free adaptation. Experiments on eight segmentation benchmarks show SCLIP significantly outperforms prior arts, achieving 38.2% average mIoU. It also produces high-quality masks qualitatively. Ablations verify that CSA with reused CLIP projections outperforms other localization schemes. The effectiveness of minimal SCLIP modification validates the transferability of CLIP's pretraining for dense prediction and its potential as a visual foundation model. In summary, this work provides an important step towards adapting self-supervised vision-language models to complex recognition tasks simply through architectural innovations.
