# [A Deep Look into Neural Ranking Models for Information Retrieval](https://arxiv.org/abs/1903.06902)

## What is the central research question or hypothesis that this paper addresses?

This paper provides a comprehensive survey and analysis of neural ranking models for information retrieval. The key aspects it covers are:- Formulating a unified framework for neural ranking models that abstracts the key components (representation functions, interaction functions, evaluation functions). - Reviewing and categorizing major neural ranking model architectures based on their assumptions and design principles, including symmetric vs. asymmetric, representation-focused vs. interaction-focused, and single-granularity vs. multi-granularity.- Analyzing different learning objectives like pointwise, pairwise, listwise, and multi-task, as well as training strategies like supervised, semi-supervised, and weak supervision. - Empirically comparing different neural ranking models on benchmark ad-hoc retrieval and QA datasets to gain insights about their effectiveness.- Discussing trending topics and promising future directions like indexing schemes, knowledge integration, visualized matching, contextual ranking, and model explanation.So in summary, the central hypothesis is that by systematically formulating, categorizing, and empirically analyzing neural ranking models along these different dimensions, we can gain a deeper understanding of their assumptions, principles, capabilities, and limitations to guide future research and development. The paper aims to provide a comprehensive reference and synthesis of existing work on this increasingly important topic.


## What is the main contribution of this paper?

Here are the key contributions of the paper:- It provides a unified formulation of neural ranking models, abstracting them into representation functions, interaction functions, and evaluation functions. This allows existing models to be analyzed and compared within a common framework.- It categorizes neural ranking models along several dimensions: symmetric vs. asymmetric architecture, representation-focused vs. interaction-focused, and single-granularity vs. multi-granularity. This provides a systematic way to understand the design principles and assumptions behind different models.- It surveys a wide range of existing neural ranking models from the literature and discusses their connections to the task characteristics. This gives insight into why certain models are more suitable for certain tasks like ad-hoc retrieval vs QA.- It empirically compares neural ranking models on benchmark ad-hoc retrieval and QA datasets. The results reveal trends like the shift from symmetric to asymmetric models over time.- It discusses several trending topics in neural ranking research like indexing, knowledge integration, visualization, and context modeling. This highlights important open problems and future directions.Overall, the paper provides a comprehensive review and analysis of neural ranking models for text retrieval. The unified formulation, categorization, literature survey, empirical comparison, and discussion of open challenges give significant insight into this rapidly developing field. The paper helps guide future research by learning from past work and identifying promising ideas.\section{Model Architecture}Based on the above unified formulation, here we review existing neural ranking model architectures to better understand their basic assumptions and design principles.\subsection{Symmetric vs. Asymmetric Architectures}\label{sec:symm_nonsymm}Starting from different underlying assumptions over the input texts $s$ and $t$, two major architectures emerge in neural ranking models, namely symmetric architecture and asymmetric architecture.\textbf{Symmetric Architecture}: The inputs $s$ and $t$ are assumed to be homogeneous, so that symmetric network structure could be applied over the inputs.  Note here symmetric structure means that the inputs $s$ and $t$ can exchange their positions in the input layer without affecting the final output. Specifically, there are two representative symmetric structures, namely siamese networks and symmetric interaction networks.\textit{Siamese networks} literally imply symmetric structure in the network architecture. Representative models include DSSM~\cite{Huang2013}, CLSM~\cite{Shen2014} and LSTM-RNN~\cite{Palangi2016}. For example, DSSM represents two input texts with a unified process including the letter-trigram  mapping followed by the multi-layer perceptron (MLP) transformation, i.e., function $\phi$ is the same as function $\psi$. After that a cosine similarity function is applied to evaluate the similarity between the two representations, i.e., function $g$ is symmetric. Similarly, CLSM~\cite{Shen2014} replaces the representation functions $\psi$ and $\phi$ by two identical convolutional neural networks (CNNs) in order to capture the local word order information. LSTM-RNN~\cite{Palangi2016} replaces $\psi$ and $\phi$ by two identical long short-term memory (LSTM) networks in order to capture the long-term dependence between words.\textit{Symmetric interaction networks}, as shown by the name, employ a symmetric interaction function to represent the inputs. Representative models include DeepMatch~\cite{Lu2013}, Arc-II~\cite{Hu2014}, MatchPyramid~\cite{Pang2016} and Match-SRNN~\cite{Wan2016MatchSRNN}. For example, Arc-II defines an interaction function $\eta$ over $s$ and $t$ by computing similarity (i.e., weighted sum) between every n-gram pair from $s$ and $t$, which is symmetric in nature. After that, several convolutional and max-pooling layers are leveraged to obtain the final relevance score, which is also symmetric over $s$ and $t$. MatchPyramid defines a symmetric interaction function $\eta$ between every word pair from $s$ and $t$ to capture fine-grained interaction signals. It then leverages a symmetric evaluation function $g$, i.e., several 2D CNNs and a dynamic pooling layer, to produce the relevance score.%leaves complex interaction signal recognition to the function $g$, which is composed by several 2D CNNs and a dynamic pooling layer. A similar process can be found in DeepMatch and Match-SRNN.Symmetric architectures, with the underlying homogeneous assumption, can fit well with the CQA and AC tasks, where $s$ and $t$ usually have similar lengths and similar forms (i.e., both are natural language sentences). They may sometimes work for the ad-hoc retrieval or QA tasks if one only uses document titles/snippets \cite{Huang2013} or short answer sentences \cite{Yang2016aNMM} to reduce the heterogeneity between the two inputs.\textbf{Asymmetric Architecture:} The inputs $s$ and $t$ are assumed to be heterogeneous, so that asymmetric network structures should be applied over the inputs. Note here asymmetric structure means if we change the position of the inputs $s$ and $t$ in the input layer, we will obtain totally different output. Asymmetric architectures have been introduced mainly in the ad-hoc retrieval task \cite{Huang2013, Pang2017}, due to the inherent heterogeneity between the query and the document as discussed in Section \ref{sec:task_ad_hoc}. Such structures may also work for the QA task where answer passages are ranked against natural language questions \cite{Dai2018Conv-KNRM}. Here we take the ad-hoc retrieval scenario as an example to analyze the asymmetric architecture. We find there are three major strategies used in the asymmetric architecture to handle the heterogeneity between the query and the document, namely query split, document split, and joint split.\begin{figure}    \begin{minipage}[t]{0.3\linewidth}        \centerline{        \includegraphics[width=0.8\linewidth]{figures/asymmetric_qcut.pdf}}        \centerline{\footnotesize{(a) Query Split}}    \end{minipage}    \begin{minipage}[t]{0.3\linewidth}        \centerline{        \includegraphics[width=0.97\linewidth]{figures/asymmetric_dcut.pdf}}        \centerline{\footnotesize{(b) Document Split}}    \end{minipage}    \begin{minipage}[t]{0.3\linewidth}        \centerline{        \includegraphics[width=0.65\linewidth]{figures/asymmetric_qattn.pdf}}        \centerline{\footnotesize{(c) One-way Attention}}    \end{minipage}    \caption{Three types of Asymmetric Architecture.}    \label{fig:asymmetric_arch}\end{figure}\begin{itemize}\item \textit{Query split} is based on the assumption that most queries in ad-hoc retrieval are keyword based, so that we can split the query into terms to match against the document, as illustrated in Figure \ref{fig:asymmetric_arch}(a). A typical model based on this strategy is DRMM~\cite{Guo2016DRMM}. DRMM splits the query into terms and defines the interaction function $\eta$ as the matching histogram mapping between each query term and the document. The evaluation function $g$ consists of two parts, i.e., a feed-forward network for term-level relevance computation and a gating network for score aggregation. Obviously such a process is asymmetric with respect to the query and the document.%The evaluation function $g$ consists of two parts. One is the feed forward network which produces the matching score for each query term based on the histogram signals, and the other is the term gating network which learns the term importance for the matching score aggregation. K-NRM~\cite{Xiong2017K-NRM} also belongs to this type of approach. It introduces a kernel pooling function to approximate matching histogram mapping to enable end-to-end learning.\item \textit{Document split} is based on the assumption that a long document could be partially relevant to a query under the scope hypothesis \cite{robertson1976relevance}, so that we split the document to capture fine-grained interaction signals rather than treat it as a whole, as depicted in Figure \ref{fig:asymmetric_arch}(b). A representative model based on this strategy is HiNT~\cite{Fan2018}. In HiNT, the document is first split into passages using a sliding window. The interaction function $\eta$ is defined as the cosine similarity and exact matching between the query and each passage. The evaluation function $g$ includes the local matching layers and global decision layers.  %A similar process  as Match-SRNN is then used to produce the local relevance representations over the query and each passage. The final relevance score is evaluated by accumulating the local signals in different ways. \item \textit{Joint split}, by its name, uses both assumptions of query split and document split. A typical model based on this strategy is DeepRank~\cite{Pang2017}. Specifically, DeepRank splits the document into  term-centric contexts with respect to each query term. It then defines the interaction function $\eta$ between the query and term-centric contexts in several ways. The evaluation function $g$ includes three parts, i.e., term-level computation, term-level aggregation, and global aggregation.%The final relevance score is evaluated via a two-step aggregation strategy, i.e., the term level aggregation and the global aggregation based on the term gating network similar to DRMM. Similarly, PACRR~\cite{Hui2017} takes the query as a set of terms and splits the document using the sliding window as well as the first-k term window.\end{itemize}In addition, in neural ranking models applied for QA, there is another popular strategy leading the asymmetric architecture. We name it \textit{one-way attention mechanism} which typically leverages the question representation to obtain the attention over candidate answer words in order the enhance the answer representation, as illustrated in Figure \ref{fig:asymmetric_arch}(c). For example, IARNN~\cite{IARNN} and CompAgg~\cite{Wang2017ICLR}  get the attentive answer representation sequence that weighted by the question sentence representation.\subsection{Representation-focused vs. Interaction-focused Architectures}\label{sec:rep_inter}Based on different assumptions over the features (extracted by the representation function $\phi, \psi$ or the interaction function $\eta$) for relevance evaluation, we can divide the existing neural ranking models into another two categories of architectures, namely representation-focused architecture and interaction-focused architecture, as illustrated in Figure \ref{fig:rep_inter}. Besides these two basic categories, some neural ranking models adopt a hybrid way to enjoy the merits of both architectures in learning relevance features.%In order to take advantages of both architectures, researchers turn to combine them as a hybrid architecture, which resort to a direct combination or an attention mechanism. %However, since it is a natural extension and there is limited work in this direction \cite{Mitra2017DUET,IARNN}, we will not discuss it separately.%Next, we focus on the different between representation function $\phi, \psi$ and interaction function $\eta$. Another two major architectures designed for these two aspects, namely representation architecture and interaction architecture.\begin{figure}    \begin{minipage}[t]{0.5\linewidth}        \centerline{        \includegraphics[width=0.4\linewidth]{figures/representation.pdf}}        \centerline{\footnotesize{(a) Representation-focused}}    \end{minipage}    \begin{minipage}[t]{0.5\linewidth}        \centerline{        \includegraphics[width=0.38\linewidth]{figures/interaction.pdf}}        \centerline{\footnotesize{(b) Interaction-focused}}    \end{minipage}    \caption{Representation-focused and Interaction-focused Architectures.}    \label{fig:rep_inter}\end{figure}\textbf{Representation-focused Architecture}: The underlying assumption of this type of architecture is that relevance depends on compositional meaning of the input texts. Therefore, models in this category usually define complex representation functions $\phi$ and $\psi$ (i.e., deep neural networks), but no interaction function $\eta$, to obtain high-level representations of the inputs $s$ and $t$, and uses some simple evaluation function $g$ (e.g. cosine function or MLP) to produce the final relevance score. Different deep network structures have been applied for $\phi$ and $\psi$, including fully-connected networks, convolutional networks and recurrent networks.\begin{itemize}\item To our best knowledge, DSSM \cite{Huang2013} is the only one that uses the fully-connected network for the functions $\phi$ and $\psi$, which has been described in Section~\ref{sec:symm_nonsymm}.\item Convolutional networks have been used for $\phi$ and $\psi$ in Arc-I~\cite{Hu2014}, CNTN~\cite{Qiu2015} and  CLSM~\cite{Shen2014}. Take Arc-I as an example, stacked 1D convolutional layers and max pooling layers are applied on the input texts $s$ and $t$ to produce their high-level representations respectively. Arc-I then concatenates the two representations and applies an MLP as the evaluation function $g$. The main difference between CNTN and Arc-I is the function $g$, where the neural tensor layer is used instead of the MLP. The description on CLSM could be found in Section~\ref{sec:symm_nonsymm}.\item Recurrent networks have been used for $\phi$ and $\psi$ in LSTM-RNN~\cite{Palangi2016} and MV-LSTM~\cite{Wan2016MV-LSTM}. LSTM-RNN uses a one-directional LSTM as $\phi$ and $\psi$ to encode the input texts, which has been described in Section~\ref{sec:symm_nonsymm}. MV-LSTM employs a bi-directional LSTM instead to encode the input texts. Then, the top-k strong matching signals between the two high-level representations are fed to an MLP to generate the relevance score. \end{itemize}By evaluating relevance based on high-level representations of each input text, representation-focused architecture better fits tasks with the global matching requirement \cite{Guo2016DRMM}. This architecture is also more suitable for tasks with short input texts (since it is often difficult to obtain good high-level representations of long texts). Tasks with these characteristics include CQA and AC as shown in Section~\ref{sec:application}. Moreover, models in this category are efficient for online computation, since one can pre-calculate representations of the texts offline once $\phi$ and $\psi$ have been learned.%The representation function $\phi$ and $\psi$ are designed to be a complex function, in order to capture the high level representation of the inputs $s$ and $t$. It is useful for the tasks require global semantics, such as question answering task or automatic conversation task, described in Section 2. Models in representation architecture are always efficient for online evaluation, since it can pre-calculate text representation offline using $\phi$ and $psi$. Based on the network structures, it can be categorized into three, fully-connected based, convolutional based and recurrent based.% assumption, the semantic comparison could be based on the high level representation of the two texts, the task require global semantics, the texts are usually short, advantage: online efficient.% Arc-I, DSSM, CLSM, CNTN ...\textbf{Interaction-focused Architecture}: The underlying assumption of this type of architecture is that relevance is in essence about the relation between the input texts, so it would be more effective to directly learn from interactions rather than from individual representations. Models in this category thus define the interaction function $\eta$ rather than the representation functions $\phi$ and $\psi$, and use some complex evaluation function $g$ (i.e., deep neural networks) to abstract the interaction and produce the relevance score. Different interaction functions have been proposed in literature, which could be divided into two categories, namely non-parametric interaction functions and parametric interaction functions.%directly learn interaction is more effective since the task is about matching. Therefore, the models are constructed only based on the interaction function $\eta$. % assumption, directly learn interaction is more effective since the task is about matching, the task usually requires detailed signals, texts are usually long, disadvantage: online non-efficient.% Arc-II, MatchPyramid, DRMM, DeepRank, aNMM...\begin{itemize}\item \textit{Non-parametric interaction functions} are functions that reflect the closeness or distance between inputs without learnable parameters. In this category, some are defined over each pair of input word vectors, such as binary indicator function~\cite{Pang2016, Pang2017}, cosine similarity function~\cite{Pang2016, Yang2016aNMM, Pang2017}, dot-product function~\cite{Pang2016, Pang2017, Fan2018} and radial-basis function \cite{Pang2016}. %The binary indicator function tests whether the two vectors are identical, the cosine similarity function considers the vector direction, while the dot-product further uses the vector length. The others are defined between a word vector and a set of word vectors, e.g. the matching histogram mapping in DRMM~\cite{Guo2016DRMM} and the kernel pooling layer in K-NRM~\cite{Xiong2017K-NRM}.  %designed to measure the distance between words or phrases in two texts $s$ and $t$. It is can be a binary function, which return 1 for identical words, 0 otherwise. It is first introduced in MatchPyramid. Some vector distance functions are defined on word embeddings, such as cosine similarity, dot product and euclidean distance. They are widely used in MatchPyramid and MatchSRNN~\cite{}. Cosine similarity only considers the direction of the vector, while dot product uses vector length to reflect word importance. However, these interaction functions are applied without any parameters to learn. \item \textit{Parametric interaction functions} are adopted to learn the similarity/distance function from data. For example, Arc-II \cite{Hu2014} uses 1D convolutional layer for the interaction bwteen two phrases. Match-SRNN~\cite{Wan2016MatchSRNN} introduces the neural tensor layer to model complex interactions between input words. Some BERT-based model \cite{yang2019simple} takes attention as the interaction function to learn the interaction vector (i.e., [CLS


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper provides a comprehensive survey of neural ranking models for information retrieval, analyzing their architectures, learning objectives, and training strategies, as well as comparing models through empirical evaluations on benchmark tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this survey paper on neural ranking models compares to other research in the information retrieval field:- Scope: This paper provides a broad overview of neural ranking models, covering major applications, model architectures, learning objectives, and training strategies. Many existing surveys focus on specific aspects like applications or model structures. The comprehensive scope is useful for gaining a holistic view of this research area.- Empirical analysis: The paper includes a comparison of neural models on benchmark ad-hoc retrieval and QA datasets. Analyzing results across papers is difficult, so the authors tried to collect comparable published results for fair evaluation. This empirical analysis provides insights on how different model architectures perform. - Novel frameworks: The paper proposes new frameworks to categorize neural ranking models based on symmetry, representation/interaction, and granularity. These frameworks provide different lenses to understand model assumptions and design choices.- Trend analysis: Several promising directions are highlighted like knowledge integration, visual models, and context modeling. Identifying trends is useful for guiding future research.- Unified formulation: A generalized formulation of ranking functions is provided to establish a common notation and terminology for describing different models. This facilitates better understanding of model components.Overall, the comprehensive scope, empirical analysis, novel frameworks, trend analysis, and unified formulation help position this survey as a thorough reference that synthesizes key developments and insights in neural ranking research. The broad treatment of the field in a structured way distinguishes it from other more specialized surveys.
