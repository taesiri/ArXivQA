# [Transformers are Multi-State RNNs](https://arxiv.org/abs/2401.06104)

## Summarize the paper in one sentence.

 The paper proposes viewing transformers as a form of multi-state RNNs with an infinite number of states, and shows they can be converted to finite multi-state RNNs via attention-based token selection while maintaining strong performance.


## What is the main contribution of this paper?

 The main contribution of this paper is formally defining transformers as a type of multi-state recurrent neural network (RNN) with an infinite number of states. The paper then shows that transformers can be converted to finite multi-state RNNs by limiting the number of tokens processed at each step, through a proposed method called TOVA that selects which tokens to keep based on their attention scores. The paper demonstrates that TOVA allows reducing the context size to 1/8th - 1/4th of the original with minimal performance degradation on several long-range tasks, shedding light on the behavior of transformer models and providing a way to dramatically reduce their memory requirements.


## What are the keywords or key terms associated with this paper?

 Based on the content of the paper, some of the key terms and concepts include:

- Transformers - The dominant neural network architecture in NLP. The paper analyzes decoder-only transformers.

- Recurrent neural networks (RNNs) - An older architecture that transformers replaced. The paper shows connections between transformers and a variant of RNNs.  

- Multi-state RNNs (MSRNNs) - A proposed RNN variant with a state matrix instead of vector. The paper frames transformers as a form of infinite MSRNNs.

- Finite MSRNNs - Converting pretrained transformers to MSRNNs with fixed, limited state size, done by token omission policies.

- Token Omission Via Attention (TOVA) - A proposed token omission policy that determines state importance based on attention scores.

- Long range tasks - Tasks requiring reasoning over long text sequences, used to evaluate different methods. Examples: language modeling, summarization, QA.

- Memory footprint - Amount of memory required during inference. Reducing transformer memory footprint is a motivation for using finite MSRNNs.

The key ideas are framing transformers as a form of multi-state RNNs, proposing the TOVA method to convert them to low memory finite MSRNNs, and showing strong performance of TOVA on various long range tasks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the proposed \method policy for selecting which tokens to keep in memory fundamentally differ from prior approaches like the sliding window or highest aggregated attention policies? What advantages does it offer?

2. The paper shows that pretrained transformers often behave empirically like finite multi-state RNNs, requiring only a fraction of the full context length to maintain strong performance. What does this finding imply about the nature of the representations learned by LLMs during pretraining?

3. Could the proposed view of transformers as multi-state RNNs inspire novel architectures that blend aspects of transformers and RNNs? What challenges might need to be overcome in designing such hybrid models?  

4. How sensitive is the performance of \method to the choice of layer for computing the attention scores used to select tokens for omission? Does performance degrade substantially if lower layers are used instead of the last layer?

5. What role does the implicit weak recency bias in \method play? How does selectively dropping older tokens impact model performance compared to a sliding window?

6. The analysis shows certain token types like punctuation and possessive nouns are kept for longer by \method. What underlying properties of these tokens make them more important for the model to retain?

7. How well does \method transfer to new domains compared to the full context model? Are there situations where its omission selections become substantially less optimal?

8. Could \method be used to expand the context capacity of models beyond their original pretrained length by only caching the most relevant prior tokens? What difficulties need to be addressed?

9. For very long sequence lengths, could an iterative version of \method be devised that makes omission decisions over multiple passes to better identify globally rather than just locally optimal tokens?

10. What modifications would be needed to apply \method to encoder-decoder transformers? Would separate omission policies be required for the encoder and decoder states?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points in the paper:

Problem:
- Transformers are considered conceptually different from RNNs. Transformers have direct access to all tokens while RNNs maintain a recurring state that depends only on previous inputs.
- However, decoder transformers generate tokens auto-regressively, so each token depends on cached computations from previous tokens, similar to RNN state.  

Proposed Solution:
- Formally define transformers as a type of multi-state RNN (MSRNN), which is an RNN variant with a state matrix instead of vector. 
- Show transformers are a special case of MSRNNs with an infinite number of states, as the cached context grows indefinitely with more tokens.
- Introduce methods to convert pretrained transformers to finite MSRNNs by limiting number of cached tokens using different cache compression policies.

Key Contributions:  
- New perspective of transformers as infinite MSRNNs and option to convert them to finite MSRNNs.
- Propose TOVA - a new cache compression policy that simply drops tokens based on attention scores.
- Experimentally show TOVA matches full models using only 1/4 to 1/8 of the context on language modeling and long-range tasks.
- Analysis shows transformers often act as finite MSRNNs in practice rather than utilizing infinite capacity.
- potential to greatly reduce memory requirements of decoding transformers with minimal performance loss.

In summary, the paper offers a new RNN-based view of transformers and introduces an attention-based method to successfully limit context size that outperforms prior approaches. The results provide new insights about transformer behavior and offer practical benefits for reduced memory.
