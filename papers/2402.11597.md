# [Multi-Task Inference: Can Large Language Models Follow Multiple   Instructions at Once?](https://arxiv.org/abs/2402.11597)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) are typically only prompted to follow a single instruction per inference call. It is unclear if LLMs can also handle multiple instructions simultaneously, which the authors refer to as Multi-Task Inference (MTI).  
- There is a lack of comprehensive evaluation benchmarks to assess the MTI capabilities of LLMs across diverse tasks.

Proposed Solution:
- The authors introduce the MTI Bench, a new benchmark consisting of 5,000 instances across 25 tasks. Each task has 2-3 sub-tasks.
- The benchmark is divided into a Multi-Step subset (tasks with sequential dependency) and a Multi-Part subset (independent tasks).
- They compare MTI against Single-Task Inference (doing tasks sequentially) and Batch Prompting (batching instances of the same task).

Key Findings:
- Surprisingly, larger LLMs like Llama-70B and GPT-4 perform up to 7.3% and 12.4% better on MTI compared to Single-Task Inference.
- MTI provides a 1.46x speedup over Single-Task Inference by reducing the number of inference calls.
- Analyses suggest MTI allows models to leverage clues from later sub-tasks to improve performance on earlier sub-tasks.

Main Contributions:
- Introduction of MTI Bench, the first comprehensive benchmark for evaluating the MTI capabilities of LLMs.
- Demonstration that MTI can improve performance and reduce inference time compared to conventional prompting.
- Analysis providing insights into why MTI works better than expected for large LLMs.

In summary, the paper shows that large LLMs exhibit an emergent capability to handle multiple instructions simultaneously using the new MTI evaluation benchmark. This highlights opportunities to improve prompting schemes to better leverage model capabilities.
