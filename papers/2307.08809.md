# [Local or Global: Selective Knowledge Assimilation for Federated Learning   with Limited Labels](https://arxiv.org/abs/2307.08809)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively leverage unlabeled data in federated learning (FL) when clients have limited labeled data and there is data heterogeneity across clients. 

Specifically, the paper examines how clients can selectively utilize both their local model trained on limited labeled data and the global model aggregated from other clients to pseudo-label their unlabeled data. The key hypotheses are:

1) Neither the local model nor global model alone may be optimal for pseudo-labeling a client's unlabeled data due to limited generalization of the local model from few labels and heterogeneity between the client's data distribution and global data distribution.

2) Selectively choosing either the local or global model for pseudo-labeling based on confidence scores can outperform using only one model.

3) Further utilizing both models' knowledge via a global-local consistency regularization when they predict the same pseudo-label can improve performance compared to discarding the non-selected model.

The paper aims to demonstrate these hypotheses through experiments in both cross-device and cross-silo federated learning settings with varying amounts of label scarcity and data heterogeneity. The proposed method FedLabel is shown to outperform existing semi-supervised federated learning baselines in these challenging scenarios.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new semi-supervised federated learning (SSFL) method called FedLabel that allows clients to selectively leverage their unlabeled data using either their local model or the global model. The key ideas and contributions are:

- FedLabel adaptively chooses either the local model or global model to pseudo-label the client's unlabeled data based on the confidence score of each model's predictions. This accounts for cases where the local model may not generalize well to unlabeled data due to limited labels, while the global model may not work well due to data heterogeneity across clients.

- It introduces a novel global-local consistency regularization term that minimizes the divergence between the local and global models' outputs when they predict the same pseudo-label. This allows assimilating knowledge from both models when applicable. 

- The method does not require additional expert models computed by the server, communication of extra parameters, server-labeled data, or any fully-labeled clients. It leverages unlabeled data using just the local and global models available in standard federated learning.

- Experiments demonstrate FedLabel achieves 8-24% higher test accuracy compared to other SSFL baselines, and even exceeds fully-supervised federated learning with 100% labels using just 5-20% labels.

- The robustness of FedLabel to both limited labeled data/class distribution mismatch and high data heterogeneity between clients is shown experimentally. It outperforms baselines by an even wider margin in high label scarcity and heterogeneity settings.

In summary, the key contribution is a selective knowledge assimilation approach for SSFL that adaptively leverages local and global models to pseudo-label unlabeled data in an effective yet simple manner, without restrictive assumptions on availability of extra labeled data/models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the key points from the paper:

The paper proposes FedLabel, a novel semi-supervised federated learning method where clients selectively leverage unlabeled data by choosing between pseudo-labels from either the local model or global model based on confidence scores, and further assimilate knowledge from both models via a global-local consistency loss when they agree on the pseudo-label.


## How does this paper compare to other research in the same field?

 Here is a brief comparison of the main contributions of this paper to other related work in semi-supervised federated learning (SSFL):

- The paper proposes a new SSFL method called FedLabel that allows clients to selectively use either their local model or the global model for pseudo-labeling unlabeled data. This is a novel approach compared to prior SSFL methods that use predefined experts like other clients' models or splice local and global models.

- FedLabel does not require any additional communication of parameters or computation of experts beyond the standard local and global models. Other SSFL methods like FedMatch and FedTriNet impose extra communication/computation costs.

- The paper shows FedLabel is robust to both limited labeled data (e.g. class distribution mismatch) and high inter-client data heterogeneity. Many prior SSFL methods do not jointly handle both challenges.

- FedLabel achieves strong performance with just two experts - local and global models. Other methods rely on finding multiple expert models, which can be costly and lead to lower performance when expert quality is poor.

- The paper empirically demonstrates FedLabel outperforms other SSFL methods by 8-24% and even exceeds fully supervised FL with 100% labeled data using just 5-20% labels. This is a significant improvement over prior art.

- Unlike some other methods, FedLabel does not assume the server has any labeled data or require fully labeled clients. The setting is more practical for real-world SSFL.

In summary, FedLabel proposes a lightweight yet effective approach for SSFL that adaptively assimilates local and global knowledge. The method achieves new state-of-the-art results under practical low-label and high-heterogeneity conditions. The innovations over prior work include selective expert usage, robustness, and performance gains using just two standard models.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more advanced and efficient algorithms for semi-supervised federated learning. The authors propose FedLabel as an initial algorithm but note there is room for improvement, such as handling label noise and dynamic participation of clients.

- Evaluating FedLabel and other SSFL algorithms on more complex models like CNNs and NLP models. The current experiments use relatively simple models like ResNets, so testing on more advanced architectures would be an important next step.

- Exploring personalization in SSFL, where models are fine-tuned on each client's unlabeled data to improve performance on their local dataset. 

- Studying the theoretical properties of SSFL algorithms like convergence guarantees, privacy, and communication efficiency. Formal analysis is lacking in this emerging area.

- Applying SSFL to real-world collaborative learning applications, beyond the image classification tasks studied so far. Testing the algorithms on practical federated datasets with label scarcity would be impactful.

- Comparing SSFL to other techniques like transfer learning and domain adaptation for handling statistical heterogeneity in federated settings.

- Developing SSFL techniques that are robust to noisy/adversarial data, relaxed client participation assumptions, and systems issues like stragglers and fault tolerance.

In summary, the authors lay out several interesting open problems related to developing better SSFL algorithms, theoretical understanding, and real-world validation that can guide future work in this area. Advancing SSFL would make federated learning more practical in scenarios with limited labeled data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new federated learning method called FedLabel that aims to tackle the problem of limited label availability in realistic federated learning settings. Many existing federated learning algorithms assume clients have fully labeled local datasets, but in practice labeling data is expensive and clients may only have a small subset of their data labeled. This can lead to poor model performance as the limited labeled data may not generalize well to the full unlabeled dataset due to class distribution mismatch. FedLabel allows clients to selectively leverage knowledge from both their local model trained on limited labeled data, as well as the global model aggregated from other clients, in order to pseudo-label their unlabeled data for semi-supervised learning. It chooses between the local and global model for pseudo-labeling based on confidence scores, and uses a novel global-local consistency regularization term when both models predict the same label to assimilate knowledge from both. Experiments on cross-device and cross-silo scenarios show FedLabel significantly outperforms standard federated learning and other semi-supervised federated learning baselines, especially in settings with high label scarcity and data heterogeneity across clients. The method does not require additional model communication or computation compared to vanilla federated learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new method called FedLabel for semi-supervised federated learning when clients have limited labeled data and a large amount of unlabeled data. FedLabel allows clients to selectively leverage knowledge from either their local model trained on the limited labeled data or the global model aggregated from other clients, in order to pseudo-label their unlabeled data. It does this by having each client choose between the local and global model's predictions based on the confidence scores, and pseudo-labeling the unlabeled data using the chosen model's predictions. FedLabel further utilizes both local and global knowledge through a consistency regularization term when both models predict the same pseudo-label, which aligns the predictions of the two models. 

Experiments on cross-device and cross-silo settings demonstrate that FedLabel significantly outperforms existing semi-supervised federated learning methods, especially in cases of high data heterogeneity and label scarcity. For example, with only 5-20% labeled data per client, FedLabel achieves 8-24% higher test accuracy than other baselines on datasets like CIFAR10 and OrganAMNIST. It also matches or exceeds the performance of fully-supervised federated learning using 100% labels, with just 5-20% labeled data. The key advantages of FedLabel are that it selectively leverages local and global knowledge without needing additional expert models or communication overhead, making it robust and practical for real federated learning scenarios with limited labels.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new semi-supervised federated learning (SSFL) method called FedLabel that enables clients to leverage their unlabeled data by selectively choosing between the local client model or the global model for pseudo-labeling, based on each model's confidence score on the unlabeled data. The key ideas are: (1) Using only the two naturally occurring experts in federated learning - the local and global models - for pseudo-labeling, without requiring any additional models or communication. (2) Adaptively selecting either the local or global model for each unlabeled sample based on the model confidence. (3) Using a global-local consistency regularization term that minimizes the divergence between the local and global models' outputs when they predict the same label, allowing the method to assimilate knowledge from both models. The paper shows this approach is robust to limited labeled data and data heterogeneity across clients, outperforming standard federated learning and other SSFL methods especially in settings with high label scarcity and heterogeneity.


## What problem or question is the paper addressing?

 The paper is addressing the problem of semi-supervised federated learning with limited client labels in heterogeneous settings. In particular, the key issues it aims to tackle are:

- Clients have only a small amount of labeled data, while most of their local data is unlabeled. This leads to poor generalization of the local models to the unlabeled data.

- There is heterogeneity in the data distributions across clients. So the global model may also not generalize well to a client's unlabeled data. 

- Existing semi-supervised federated learning (SSFL) methods have limitations such as requiring additional expert models, extra communication, restrictive assumptions like the server having labels, etc.

To address these issues, the paper proposes a new SSFL method called FedLabel that:

- Selectively chooses either the local or global model for pseudo-labeling client unlabeled data based on model confidence scores.

- Uses a novel global-local consistency regularization to assimilate knowledge from both local and global models when they agree on the pseudo-label.

- Does not require additional expert models, extra communication, server labels, or fully labeled clients.

- Is robust to limited labeled data generalizability and heterogeneity across clients.

So in summary, the key question is how to effectively leverage unlabeled data in federated learning when clients have scarce labels and heterogeneous data distributions, which this paper aims to address with the proposed FedLabel method.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Federated learning (FL): A distributed machine learning approach that enables training models across multiple decentralized devices or servers holding local data samples, without exchanging their data.

- Semi-supervised learning (SSL): A technique to leverage unlabeled data along with labeled data during training to improve model performance. 

- Limited labels/label scarcity: Real-world federated learning scenarios often have limited labeled data available due to the expense and effort of human labeling. This can degrade model performance.

- Data heterogeneity: The distribution of data samples varies across different clients in federated learning. This makes it challenging to have a global model that works well for all clients.

- Class distribution mismatch: With limited labeled data, the class distribution in the labeled set can differ significantly from that in the unlabeled data, reducing generalization. 

- Local model: The model trained locally on a client using its labeled data.

- Global model: The model trained collaboratively across clients via federated learning.

- Pseudo-labeling: Using model predictions on unlabeled data as proxy labels to train in a semi-supervised manner.

- Confidence-based selection: Choosing between local vs global model for pseudo-labeling based on their prediction confidence. 

- Global-local consistency: Regularizing to minimize divergence between local and global model outputs when their pseudo-labels agree.

In summary, the key focus is on semi-supervised federated learning with limited labels, leveraging unlabeled data intelligently despite data heterogeneity.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are some potential questions to ask to create a comprehensive summary of the paper:

1. What is the main research problem or goal that the paper aims to address? 

2. What are the key limitations or gaps in previous work related to this problem? How does this paper try to address them?

3. What are the key assumptions or settings considered in the paper (e.g. federated learning framework, amount of labeled/unlabeled data, heterogeneity of data)?

4. What is the proposed method or approach in the paper? What are its key components, steps or algorithms? 

5. What are the main theoretical contributions or analyses provided regarding the proposed method?

6. How is the proposed method evaluated empirically? What tasks, datasets, and baselines are used?

7. What are the main results from the empirical evaluations? How does the proposed method compare to baselines quantitatively?

8. What are the key benefits or advantages of the proposed method over existing approaches based on the results?

9. What are the limitations or potential drawbacks of the proposed method based on the empirical analyses or discussion?

10. What directions for future work are suggested by the authors based on this research?

11. What are the key takeaways or conclusions from this work? What are the broader impacts or implications?

12. Does the paper provide enough details and empirical analyses to properly evaluate the claims and contributions? Are there any gaps?

13. Does the framing of the problem, proposed method, and results align well? Is the narrative logical and complete?

14. Are there any interesting related works that could provide additional context or comparisons?

15. What are the key figures or tables that capture the essential results from the paper?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes selectively choosing between the local and global model for pseudo-labeling unlabeled data based on confidence scores. How exactly is the confidence score calculated for each model? What impact does the choice of confidence metric have on model selection and overall performance?

2. For the global-local consistency regularization term, the paper uses KL divergence between the non-selected model and the training model outputs. What is the motivation behind using KL divergence versus other divergence measures? How does the choice of divergence metric impact optimization and final model convergence?

3. When applying the global-local consistency regularization term, the paper weighs it by the confidence score ratio between the non-selected and selected models. What is the intuition behind modulating the regularization weight based on relative confidence scores? How does this weighting impact optimization?

4. The paper shows the global-local consistency term improves performance when included. However, what are the potential downsides? Could enforcing agreement between the local and global models in some cases degrade performance? When might the consistency term be unhelpful or even detrimental? 

5. For the strong data augmentation, the paper uses RandAugment with specific parameters. How were these RandAugment hyperparameters selected? What impact does augment strength have on pseudo-label quality and final model performance?

6. The paper evaluates performance over a range of label percentages, from 5% to 50%. How does the relative importance of the global versus local models change as more labeled data becomes available? When does preference shift from global to local or vice versa?

7. How does the performance of FedLabel vary with different degrees of data heterogeneity across clients? Under what conditions does the global model become more or less useful compared to the local models?

8. The paper mentions robustness to label noise as future work. What modifications would be needed to make FedLabel robust to noisy or incorrect labels in the limited labeled data?

9. For the confidence thresholding, is there an optimal threshold value that generalizes across tasks and datasets? Or does the threshold need to be tuned on a per dataset basis?

10. How does the communication cost of FedLabel compare to other semi-supervised federated learning methods? Are there opportunities to further reduce communication via approaches like federated distillation?
