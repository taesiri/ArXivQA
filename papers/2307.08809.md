# [Local or Global: Selective Knowledge Assimilation for Federated Learning   with Limited Labels](https://arxiv.org/abs/2307.08809)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively leverage unlabeled data in federated learning (FL) when clients have limited labeled data and there is data heterogeneity across clients. 

Specifically, the paper examines how clients can selectively utilize both their local model trained on limited labeled data and the global model aggregated from other clients to pseudo-label their unlabeled data. The key hypotheses are:

1) Neither the local model nor global model alone may be optimal for pseudo-labeling a client's unlabeled data due to limited generalization of the local model from few labels and heterogeneity between the client's data distribution and global data distribution.

2) Selectively choosing either the local or global model for pseudo-labeling based on confidence scores can outperform using only one model.

3) Further utilizing both models' knowledge via a global-local consistency regularization when they predict the same pseudo-label can improve performance compared to discarding the non-selected model.

The paper aims to demonstrate these hypotheses through experiments in both cross-device and cross-silo federated learning settings with varying amounts of label scarcity and data heterogeneity. The proposed method FedLabel is shown to outperform existing semi-supervised federated learning baselines in these challenging scenarios.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new semi-supervised federated learning (SSFL) method called FedLabel that allows clients to selectively leverage their unlabeled data using either their local model or the global model. The key ideas and contributions are:

- FedLabel adaptively chooses either the local model or global model to pseudo-label the client's unlabeled data based on the confidence score of each model's predictions. This accounts for cases where the local model may not generalize well to unlabeled data due to limited labels, while the global model may not work well due to data heterogeneity across clients.

- It introduces a novel global-local consistency regularization term that minimizes the divergence between the local and global models' outputs when they predict the same pseudo-label. This allows assimilating knowledge from both models when applicable. 

- The method does not require additional expert models computed by the server, communication of extra parameters, server-labeled data, or any fully-labeled clients. It leverages unlabeled data using just the local and global models available in standard federated learning.

- Experiments demonstrate FedLabel achieves 8-24% higher test accuracy compared to other SSFL baselines, and even exceeds fully-supervised federated learning with 100% labels using just 5-20% labels.

- The robustness of FedLabel to both limited labeled data/class distribution mismatch and high data heterogeneity between clients is shown experimentally. It outperforms baselines by an even wider margin in high label scarcity and heterogeneity settings.

In summary, the key contribution is a selective knowledge assimilation approach for SSFL that adaptively leverages local and global models to pseudo-label unlabeled data in an effective yet simple manner, without restrictive assumptions on availability of extra labeled data/models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the key points from the paper:

The paper proposes FedLabel, a novel semi-supervised federated learning method where clients selectively leverage unlabeled data by choosing between pseudo-labels from either the local model or global model based on confidence scores, and further assimilate knowledge from both models via a global-local consistency loss when they agree on the pseudo-label.


## How does this paper compare to other research in the same field?

 Here is a brief comparison of the main contributions of this paper to other related work in semi-supervised federated learning (SSFL):

- The paper proposes a new SSFL method called FedLabel that allows clients to selectively use either their local model or the global model for pseudo-labeling unlabeled data. This is a novel approach compared to prior SSFL methods that use predefined experts like other clients' models or splice local and global models.

- FedLabel does not require any additional communication of parameters or computation of experts beyond the standard local and global models. Other SSFL methods like FedMatch and FedTriNet impose extra communication/computation costs.

- The paper shows FedLabel is robust to both limited labeled data (e.g. class distribution mismatch) and high inter-client data heterogeneity. Many prior SSFL methods do not jointly handle both challenges.

- FedLabel achieves strong performance with just two experts - local and global models. Other methods rely on finding multiple expert models, which can be costly and lead to lower performance when expert quality is poor.

- The paper empirically demonstrates FedLabel outperforms other SSFL methods by 8-24% and even exceeds fully supervised FL with 100% labeled data using just 5-20% labels. This is a significant improvement over prior art.

- Unlike some other methods, FedLabel does not assume the server has any labeled data or require fully labeled clients. The setting is more practical for real-world SSFL.

In summary, FedLabel proposes a lightweight yet effective approach for SSFL that adaptively assimilates local and global knowledge. The method achieves new state-of-the-art results under practical low-label and high-heterogeneity conditions. The innovations over prior work include selective expert usage, robustness, and performance gains using just two standard models.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more advanced and efficient algorithms for semi-supervised federated learning. The authors propose FedLabel as an initial algorithm but note there is room for improvement, such as handling label noise and dynamic participation of clients.

- Evaluating FedLabel and other SSFL algorithms on more complex models like CNNs and NLP models. The current experiments use relatively simple models like ResNets, so testing on more advanced architectures would be an important next step.

- Exploring personalization in SSFL, where models are fine-tuned on each client's unlabeled data to improve performance on their local dataset. 

- Studying the theoretical properties of SSFL algorithms like convergence guarantees, privacy, and communication efficiency. Formal analysis is lacking in this emerging area.

- Applying SSFL to real-world collaborative learning applications, beyond the image classification tasks studied so far. Testing the algorithms on practical federated datasets with label scarcity would be impactful.

- Comparing SSFL to other techniques like transfer learning and domain adaptation for handling statistical heterogeneity in federated settings.

- Developing SSFL techniques that are robust to noisy/adversarial data, relaxed client participation assumptions, and systems issues like stragglers and fault tolerance.

In summary, the authors lay out several interesting open problems related to developing better SSFL algorithms, theoretical understanding, and real-world validation that can guide future work in this area. Advancing SSFL would make federated learning more practical in scenarios with limited labeled data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new federated learning method called FedLabel that aims to tackle the problem of limited label availability in realistic federated learning settings. Many existing federated learning algorithms assume clients have fully labeled local datasets, but in practice labeling data is expensive and clients may only have a small subset of their data labeled. This can lead to poor model performance as the limited labeled data may not generalize well to the full unlabeled dataset due to class distribution mismatch. FedLabel allows clients to selectively leverage knowledge from both their local model trained on limited labeled data, as well as the global model aggregated from other clients, in order to pseudo-label their unlabeled data for semi-supervised learning. It chooses between the local and global model for pseudo-labeling based on confidence scores, and uses a novel global-local consistency regularization term when both models predict the same label to assimilate knowledge from both. Experiments on cross-device and cross-silo scenarios show FedLabel significantly outperforms standard federated learning and other semi-supervised federated learning baselines, especially in settings with high label scarcity and data heterogeneity across clients. The method does not require additional model communication or computation compared to vanilla federated learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new method called FedLabel for semi-supervised federated learning when clients have limited labeled data and a large amount of unlabeled data. FedLabel allows clients to selectively leverage knowledge from either their local model trained on the limited labeled data or the global model aggregated from other clients, in order to pseudo-label their unlabeled data. It does this by having each client choose between the local and global model's predictions based on the confidence scores, and pseudo-labeling the unlabeled data using the chosen model's predictions. FedLabel further utilizes both local and global knowledge through a consistency regularization term when both models predict the same pseudo-label, which aligns the predictions of the two models. 

Experiments on cross-device and cross-silo settings demonstrate that FedLabel significantly outperforms existing semi-supervised federated learning methods, especially in cases of high data heterogeneity and label scarcity. For example, with only 5-20% labeled data per client, FedLabel achieves 8-24% higher test accuracy than other baselines on datasets like CIFAR10 and OrganAMNIST. It also matches or exceeds the performance of fully-supervised federated learning using 100% labels, with just 5-20% labeled data. The key advantages of FedLabel are that it selectively leverages local and global knowledge without needing additional expert models or communication overhead, making it robust and practical for real federated learning scenarios with limited labels.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new semi-supervised federated learning (SSFL) method called FedLabel that enables clients to leverage their unlabeled data by selectively choosing between the local client model or the global model for pseudo-labeling, based on each model's confidence score on the unlabeled data. The key ideas are: (1) Using only the two naturally occurring experts in federated learning - the local and global models - for pseudo-labeling, without requiring any additional models or communication. (2) Adaptively selecting either the local or global model for each unlabeled sample based on the model confidence. (3) Using a global-local consistency regularization term that minimizes the divergence between the local and global models' outputs when they predict the same label, allowing the method to assimilate knowledge from both models. The paper shows this approach is robust to limited labeled data and data heterogeneity across clients, outperforming standard federated learning and other SSFL methods especially in settings with high label scarcity and heterogeneity.
