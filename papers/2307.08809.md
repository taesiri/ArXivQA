# [Local or Global: Selective Knowledge Assimilation for Federated Learning   with Limited Labels](https://arxiv.org/abs/2307.08809)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively leverage unlabeled data in federated learning (FL) when clients have limited labeled data and there is data heterogeneity across clients. 

Specifically, the paper examines how clients can selectively utilize both their local model trained on limited labeled data and the global model aggregated from other clients to pseudo-label their unlabeled data. The key hypotheses are:

1) Neither the local model nor global model alone may be optimal for pseudo-labeling a client's unlabeled data due to limited generalization of the local model from few labels and heterogeneity between the client's data distribution and global data distribution.

2) Selectively choosing either the local or global model for pseudo-labeling based on confidence scores can outperform using only one model.

3) Further utilizing both models' knowledge via a global-local consistency regularization when they predict the same pseudo-label can improve performance compared to discarding the non-selected model.

The paper aims to demonstrate these hypotheses through experiments in both cross-device and cross-silo federated learning settings with varying amounts of label scarcity and data heterogeneity. The proposed method FedLabel is shown to outperform existing semi-supervised federated learning baselines in these challenging scenarios.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new semi-supervised federated learning (SSFL) method called FedLabel that allows clients to selectively leverage their unlabeled data using either their local model or the global model. The key ideas and contributions are:

- FedLabel adaptively chooses either the local model or global model to pseudo-label the client's unlabeled data based on the confidence score of each model's predictions. This accounts for cases where the local model may not generalize well to unlabeled data due to limited labels, while the global model may not work well due to data heterogeneity across clients.

- It introduces a novel global-local consistency regularization term that minimizes the divergence between the local and global models' outputs when they predict the same pseudo-label. This allows assimilating knowledge from both models when applicable. 

- The method does not require additional expert models computed by the server, communication of extra parameters, server-labeled data, or any fully-labeled clients. It leverages unlabeled data using just the local and global models available in standard federated learning.

- Experiments demonstrate FedLabel achieves 8-24% higher test accuracy compared to other SSFL baselines, and even exceeds fully-supervised federated learning with 100% labels using just 5-20% labels.

- The robustness of FedLabel to both limited labeled data/class distribution mismatch and high data heterogeneity between clients is shown experimentally. It outperforms baselines by an even wider margin in high label scarcity and heterogeneity settings.

In summary, the key contribution is a selective knowledge assimilation approach for SSFL that adaptively leverages local and global models to pseudo-label unlabeled data in an effective yet simple manner, without restrictive assumptions on availability of extra labeled data/models.
