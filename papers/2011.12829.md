# [All You Need is a Good Functional Prior for Bayesian Deep Learning](https://arxiv.org/abs/2011.12829)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we impose more interpretable, Gaussian process-like functional priors on Bayesian neural networks, and what effect does this have on the predictive performance of the models?

The key points are:

- Specifying sensible priors over the many parameters of modern neural nets is challenging. The induced functional prior (distribution of functions obtained by sampling the parameters) is hard to characterize. 

- In contrast, Gaussian process priors offer a more intuitive way to define distributions over functions in terms of properties like smoothness and lengthscale.

- The authors propose matching the functional prior of a neural net to a target GP prior by minimizing the Wasserstein distance between them. This is done by optimizing the parameters of the neural net prior.

- They show that using GP-induced priors instead of standard parameter priors (like i.i.d. Gaussians) leads to improved predictive performance across a range of neural network architectures and datasets when doing full Bayesian inference.

So in summary, the central hypothesis is that using more interpretable functional priors from GPs can improve Bayesian neural nets, and they provide evidence for this via the Wasserstein matching procedure and experiments. The overall goal is advancing the usability of Bayesian deep learning.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is developing a method to impose interpretable functional priors on Bayesian neural networks (BNNs) by matching the prior distribution over functions induced by the BNN to a target Gaussian process (GP) prior. 

Specifically, the key points are:

- Reasoning about functional priors is more intuitive than specifying priors over weights for BNNs, but directly imposing GP priors on BNNs is analytically intractable due to their nonlinear nature.

- The authors propose minimizing the Wasserstein distance between the BNN and GP functional priors by optimizing the parameters of the BNN prior. This provides an efficient way to match the BNN prior to a desired GP prior.

- They demonstrate that using common GP priors like the RBF kernel as targets leads to improved performance over standard parameter priors across various BNN models and tasks.

- The benefits are shown through extensive experiments including Bayesian CNNs and regression/classification tasks. The GP-induced priors give systematic improvements in generalization and robustness compared to standard weight priors and competitive baselines.

In summary, the main contribution is a practical framework to impose interpretable GP functional priors on BNNs by optimizing the Wasserstein distance between priors. This allows translating the intuitions and properties of GP priors to BNNs, leading to performance improvements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my review of the paper, here is a one sentence summary:

The paper proposes a novel framework to impose Gaussian process priors on Bayesian neural networks by minimizing the Wasserstein distance between the distribution of neural network functions and a target Gaussian process prior.

In slightly more detail:

The paper argues that reasoning directly in the space of functions is easier than specifying priors over the weights of neural networks. Gaussian processes offer a principled way to define priors over functions. The authors propose to "tune" the priors over neural network weights to match a target Gaussian process prior by minimizing the Wasserstein distance between the two induced distributions over functions. This is done by optimizing the Wasserstein distance with respect to the neural network prior parameters using only samples from the two distributions. The authors show this allows imposing properties like smoothness from Gaussian process priors onto neural network models. Experiments demonstrate systematic improvements in predictive performance over common choices of priors using this technique.


## How does this paper compare to other research in the same field?

 Here is a high-level summary of how this paper on imposing Gaussian process priors on Bayesian neural networks relates to other research in Bayesian deep learning:

- The key idea of reasoning about priors in function space rather than weight space aligns with several other recent works such as functional variational inference for BNNs and related methods. However, the proposed approach of matching the prior distributions using Wasserstein distance is novel.

- The paper argues that Gaussian processes offer a useful framework for defining flexible functional priors in a more interpretable way compared to standard weight-space priors like Gaussians. Using GPs to inform BNN priors has been explored before, but matching via Wasserstein distance is a new strategy.

- The extensive experiments benchmarking MCMC posterior inference demonstrate systematic benefits of the proposed priors across many architectures and datasets. This helps provide empirical evidence that functional priors are important for Bayesian neural nets.

- The work fits into a broader recent focus on improving priors and posterior inference for BNNs. It offers a practical way to define good priors based on GPs without needing marginal likelihood approximations.

- The proposed methodology is complementary to other advances like new MCMC samplers and normalizing flow priors. It could be integrated with these methods for further improvements.

In summary, this paper makes both methodological and empirical contributions to an active area of research on priors and inference for Bayesian deep learning. The Wasserstein distance approach to matching functional priors is novel, and the experiments confirm the usefulness of this strategy across a range of models and problems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Further exploration of functional priors for Bayesian neural networks. The authors argue that reasoning in terms of functional priors is easier and more intuitive than reasoning about priors over network weights. They propose Gaussian processes as a way to define sensible functional priors, but suggest there may be other useful ways to specify functional priors as well.

- Investigation of normalizing flows for modeling priors. The authors used normalizing flows to represent flexible prior distributions over network weights. They note that normalizing flows have not been widely explored for modeling priors, and suggest this as an interesting direction for future work. However, they note computational complexity as a challenge.

- Reducing the complexity of optimizing the Wasserstein distance between the neural network and target functional priors. The authors used Wasserstein distance minimization to fit the neural network priors to target functional priors. But they note the computational expense of this procedure and suggest investigating ways to improve efficiency.

- Applying the methodology to unsupervised/latent variable models. The authors focused on supervised learning problems but suggest their method of imposing functional priors could also be useful for unsupervised learning models like variational autoencoders.

- Exploring different parametrizations of neural network weight priors. The authors mainly used Gaussian or normalizing flow distributions over weights, but suggest trying other flexible distributions.

- Evaluating the impact of functional priors in other application domains. The experiments focused on regression and image classification. The authors suggest evaluating whether functional priors can also improve performance in other domains like natural language processing.

In summary, the main future directions are: exploring other types of functional priors, improving efficiency of the weight prior fitting, applying the method to unsupervised learning, trying other weight prior representations beyond Gaussians/normalizing flows, and evaluating performance on a wider range of applications. The key goal is to further develop and scale up the overall concept of directly imposing interpretable functional priors on neural networks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a novel framework to impose interpretable functional priors on Bayesian neural networks (BNNs). Currently, priors over BNN parameters do not necessarily reflect desirable properties on the induced distribution over functions. Instead, the authors argue for directly specifying priors over the BNN functional outputs. Gaussian processes offer a principled way to define function priors, so the authors propose matching the BNN functional prior to a target Gaussian process prior by minimizing their Wasserstein distance. They estimate this distance using samples, avoiding analytical intractabilities. Through extensive experiments, they demonstrate that coupling these Gaussian process-induced priors with scalable Markov chain Monte Carlo sampling substantially improves predictive performance over common parameter priors and recent approximate Bayesian deep learning methods on a diverse range of tasks. The proposed framework is a considerable advance towards enabling full Bayesian treatment of neural networks with meaningful functional priors.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a novel method to impose interpretable functional priors on Bayesian neural networks (BNNs). BNNs are typically specified by defining a prior distribution over their weights and biases. However, due to the nonlinearity of neural networks, the effect of this parameter prior on the distribution of functions that can be represented is not obvious. In contrast, Gaussian processes (GPs) offer a framework to directly specify a prior over functions in terms of a kernel function. The authors argue that it is more intuitive to reason about priors directly in function space. They propose to optimize the parameter prior of a BNN to minimize the Wasserstein distance between its induced functional prior and a target GP prior. This is formulated as a minimax optimization problem, where the Wasserstein distance is estimated using samples from the BNN and GP priors. The inner maximization adjusts a Lipschitz critic to discriminate between the two priors. The outer minimization tunes the BNN prior to fool the critic. The authors demonstrate improved performance of BNNs with GP-matched priors on a range of regression and classification tasks. They also show benefits on tasks requiring good uncertainty estimates like active learning and out-of-distribution detection.

In summary, this paper introduces a practical methodology to impose interpretable Gaussian process priors on Bayesian neural networks. Matching the functional prior of a BNN to a GP target is framed as minimization of the Wasserstein distance between the two stochastic processes. The induced priors lead to improved predictive performance and uncertainty estimates compared to common predefined priors over weights. The proposed framework offers a way to enjoy the modeling flexibility of neural networks while also leveraging the advantages of GP priors.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel approach for imposing functional priors on Bayesian neural networks (BNNs). They argue that directly specifying priors over the weights of BNNs makes it difficult to control and interpret their effect on the induced distribution over functions. Instead, they use Gaussian processes (GPs) to define target functional priors which have more interpretable properties. They then optimize the parameters of priors over BNN weights so as to minimize the Wasserstein distance between the induced functional prior and the target GP prior. This is done by alternately updating a Lipschitz function that estimates the Wasserstein distance and updating the BNN prior parameters via gradient descent on the estimated Wasserstein distance. Using this method, they are able to impose GP priors with properties like smoothness and lengthscale onto BNNs. They demonstrate that this approach systematically improves predictive performance over other priors across a range of models and datasets when combined with Markov chain Monte Carlo posterior inference.


## What problem or question is the paper addressing?

 Based on my reading, the key points and contributions of the paper are:

- The paper argues that specifying appropriate prior distributions for Bayesian neural networks (BNNs) is challenging, especially for modern deep neural networks with thousands or millions of parameters. The choice of prior affects the induced "functional prior", which is the distribution over functions obtained by sampling network parameters from the prior. However, the effect of the parameter priors on the functional prior is difficult to characterize for nonlinear deep networks.

- The paper proposes to reason directly in the space of functional priors rather than parameter priors. Specifically, the authors use Gaussian processes (GPs) to define interpretable functional priors which have desirable properties like smoothness and lengthscales. 

- The core method involves optimizing the Wasserstein distance between the BNN functional prior and a target GP prior. By minimizing this distance, the BNN priors over weights and biases can be tuned to reflect the target functional prior. This avoids the difficulties of working directly with the intractable KL divergence between functional priors.

- Through experiments, the paper shows that using the proposed GP-tuned priors leads to improved predictive performance compared to common default priors like Gaussian or hierarchical priors. Gains are shown for BNNs applied to UCI regression/classification, CNNs for CIFAR-10, and in settings like covariate shift and active learning.

- The method provides a way to impose structured functional priors on BNNs to improve performance. This is seen as a significant step towards enabling full Bayesian treatment of complex neural network models.

In summary, the key contribution is a practical framework to impose interpretable functional priors on BNNs by optimizing the Wasserstein distance to a target GP prior over functions. This is shown to improve performance across a range of models and applications.


## What are the keywords or key terms associated with this paper?

 Based on skimming through the paper, some of the key terms and keywords that seem most relevant are:

- Bayesian neural networks (BNNs) - The paper focuses on Bayesian methods for training neural networks, treating the weights as random variables with prior distributions rather than fixed parameters.

- Prior distributions - A major theme is how to specify good prior distributions over the weights of neural networks. This controls the inductive biases.

- Functional priors - The paper argues for reasoning about priors over the space of functions that a neural network can represent, rather than just over its weights.

- Gaussian processes (GPs) - GPs are proposed as a way to define sensible functional priors for neural networks. The paper matches BNN priors to GP priors.

- Wasserstein distance - A distance metric between probability distributions. Used to match the functional prior of a BNN to a target GP prior.

- Markov chain Monte Carlo (MCMC) - Used to generate posterior samples over neural network weights by sampling from the prior and likelihood.

- Uncertainty - Bayesian neural networks can provide well-calibrated estimates of predictive uncertainty. The choice of prior affects this.

- Convolutional neural networks (CNNs) - The methods are applied to CNNs for image classification, not just standard MLPs.

So in summary, the key focus is on using GP-inspired functional priors for BNNs to improve uncertainty estimates and predictive performance across various architectures and datasets. The Wasserstein distance is used to map the GP priors over to the BNN priors.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing the paper:

1. What is the main research question or problem being investigated in the paper?

2. What methods/approach does the paper propose to address this research question? 

3. What are the key assumptions or components of the proposed approach?

4. What datasets were used to evaluate the proposed approach? What were the experimental setup and evaluation metrics?

5. What were the main results reported in the paper? Were there any interesting or surprising findings?

6. How does the proposed approach compare to other existing methods on this problem? What are its advantages and limitations?

7. Do the authors identify any potential broader impacts or limitations of the work?

8. Does the paper suggest any promising directions for future work?

9. Are the claims made in the paper supported by sufficient evidence and analysis? Are there any potential flaws in the methodology or conclusions?

10. Does the paper make a novel contribution to the field? Why might this work be important and influential?

To create a good summary, I would try to extract key information to answer these questions after carefully reading through the full paper. The goal is to concisely capture the core ideas, contributions, and limitations of the work.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes matching the functional prior of a Bayesian neural network (BNN) to a Gaussian process (GP) prior by minimizing the Wasserstein distance between them. Why is the Wasserstein distance used instead of other divergence measures like KL divergence? What are the benefits and limitations of this choice?

2. The paper uses a neural network to parameterize the Lipschitz function for estimating the Wasserstein distance. What is the rationale behind this design choice? How sensitive is the overall method to the architecture and training of this neural network?

3. The paper recommends using a hierarchical GP as the target functional prior instead of tuning the GP hyperparameters via marginal likelihood. What is the motivation behind this? In what cases would tuning the GP prior be preferred over using a hierarchical prior?

4. The measurement set used to estimate the Wasserstein distance is constructed from a combination of training inputs and randomly sampled points. How does the choice of measurement set impact the resulting BNN prior? Should any considerations be made in selecting the measurement set?

5. The paper alternates between optimizing the Lipschitz neural network and the BNN prior parameters in separate steps. Could these be optimized jointly instead? What are the tradeoffs between these two optimization strategies?

6. Normalizing flows are used to increase the flexibility of the BNN prior parameterization. What are the benefits and limitations of using normalizing flows compared to simpler Gaussian or hierarchical priors?

7. The paper focuses on using GPs as the target functional prior. Could other stochastic processes like Gaussian Markov random fields also be used? What considerations should be made in selecting an appropriate target functional prior?

8. The method is applied to both fully-connected and convolutional neural networks. Are there any differences in how well it works for these two architectures? Are any modifications needed to apply it to other types of neural network architectures?

9. The experiments show improved performance over common Bayesian neural network priors. Is the performance gain solely due to using a better functional prior, or does the inference procedure also play a role? How much improvement is gained from each component?

10. The method requires optimizing a Wasserstein distance which can be computationally expensive. Are there ways to improve the computational efficiency of the overall approach? Could approximations of the Wasserstein distance be used instead?
