# [GraphNAS: Graph Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1904.09981)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to automatically search for optimal graph neural network architectures using reinforcement learning. Specifically, the paper proposes an algorithm called GraphNAS that can efficiently search the space of graph neural network architectures and find architectures that achieve state-of-the-art performance on node classification tasks.The key ideas and contributions are:- Proposes using reinforcement learning, specifically a recurrent neural network controller, to generate graph neural network architectures. The controller is trained to maximize the validation accuracy of generated architectures.- Designs a search space of components for graph neural networks, including sampling functions, aggregation functions, etc. - Introduces a parameter sharing scheme during training to improve efficiency. Child models can share weights instead of being trained from scratch each time.- Evaluates GraphNAS on transductive and inductive node classification tasks, showing it can find architectures that match or exceed state-of-the-art performance.- Demonstrates the effectiveness of the proposed search algorithm compared to alternatives like random search, NAS without weight sharing, and ENAS without training child models.In summary, the key hypothesis is that reinforcement learning can automate and improve the design of graph neural network architectures. The results on several benchmark datasets support this hypothesis and show the promise of this approach.


## What is the main contribution of this paper?

 The main contributions of this paper are:- Proposes GraphNAS, a novel method to automatically search for graph neural network architectures using reinforcement learning. - Designs a search space covering sampling functions, aggregation functions and gated functions for graph neural networks.- Presents an efficient search algorithm based on policy gradient and parameter sharing to speed up the search. - Validates GraphNAS on node classification tasks for both transductive and inductive learning settings. Achieves state-of-the-art or competitive results compared to human-designed architectures.- Shows the effectiveness of the proposed parameter sharing and search strategies compared to other methods like random search, NAS-style search, etc.In summary, this paper proposes a novel graph neural architecture search method called GraphNAS that can automatically design high-performance graph neural network architectures for node classification tasks. The key ideas are the designed search space, efficient search algorithm and validation of the method on multiple datasets in transductive and inductive settings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a graph neural architecture search method called GraphNAS that uses reinforcement learning to automatically design high-performance graph neural network architectures for node classification tasks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on graph neural architecture search:- This paper proposes GraphNAS, a novel method for automatically searching architectures of graph neural networks (GNNs) using reinforcement learning. Other papers have explored neural architecture search for CNNs and RNNs, but applying it to GNNs is still relatively new.- The paper identifies some unique challenges in applying architecture search to GNNs, like designing the search space and training efficiently. It proposes solutions like a parameterized search space and weight sharing during training.- For the search algorithm, GraphNAS uses policy gradient to train a recurrent network controller, similar to the original NAS paper. Some other papers have explored evolution-based or gradient-based search.- The experiments compare GraphNAS to state-of-the-art GNN models on both transductive and inductive node classification tasks. GraphNAS achieves competitive or better performance, demonstrating it can find architectures rivaling human-designed models. - The paper ablates different components like weight sharing and compares to other search strategies like random search. This provides insight into what makes GraphNAS effective.- Overall, this paper makes a solid contribution in exploring and advancing neural architecture search specifically for graph neural networks. The solutions for challenges unique to GNNs are novel. The empirical results demonstrate the promise of automated architecture search in this domain. It paves the way for future work on GNN architecture search.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Expanding the search space of GraphNAS to include more components like different training hyperparameters, regularization techniques, etc. The authors mention that currently GraphNAS focuses mainly on searching over different neighborhood sampling, correlation measurement, and aggregation functions.- Evaluating GraphNAS on a wider range of graph-based tasks beyond just node classification, such as link prediction, community detection, etc. - Developing more advanced reinforcement learning algorithms and training strategies to make the neural architecture search process more efficient. The authors note that the search space for graph neural networks is very large, so improving the efficiency of the search algorithm is an important direction.- Experimenting with different controller architectures beyond just LSTM, likeTransformer or graph networks, to generate the neural network descriptions. - Exploring how the architectures found by GraphNAS generalize to different graphs outside of the training distribution. Evaluating the transferability of learned architectures.- Comparing GraphNAS with more neural architecture search baselines and analyzing the differences in performance.- Developing inductive learning versions of GraphNAS that do not require access to the full graph structure and features during training.- Trying to theoretically understand why certain graph neural architectures work better than others for different tasks and datasets.In summary, the main future directions are expanding the GraphNAS search space, evaluating on more tasks, improving search efficiency, studying transferability, comparing with more methods, and developing inductive learning versions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper proposes GraphNAS, a graph neural architecture search method using reinforcement learning. GraphNAS uses a recurrent network controller to generate variable-length architecture descriptions of graph neural networks. The controller is trained with policy gradient to maximize the validation accuracy of the generated architectures. GraphNAS addresses challenges like designing the search space, efficiency of the search algorithm, and evaluation in transductive and inductive settings. The search space covers sampling, aggregation, gating functions etc. Efficiency is improved by a new parameter sharing scheme. Experiments on node classification tasks demonstrate that GraphNAS can achieve state-of-the-art performance on citation networks and protein-protein interaction networks, designing architectures that rival human-designed models. Comparisons show the parameter sharing and search strategies in GraphNAS are effective.


## Summarize the paper in two paragraphs.

 Here are two paragraph summaries of the key points from the paper:The paper proposes a novel graph neural architecture search method called GraphNAS that uses reinforcement learning to automatically search for optimal graph neural network architectures. GraphNAS uses a recurrent neural network controller to generate variable-length string descriptions of graph neural architectures. The controller is trained with policy gradient reinforcement learning to maximize the expected accuracy of the generated architectures on a validation dataset. To make the search more efficient, GraphNAS introduces a parameters sharing technique where child models generated during the search process can share weights. Experiments on node classification tasks demonstrate that GraphNAS can design novel graph network architectures that match or exceed state-of-the-art human-designed networks in terms of accuracy on citation networks and protein-protein interaction networks.The key contributions of the paper are: 1) Studying graph neural architecture search using reinforcement learning for the first time. 2) Introducing the GraphNAS method which uses a recurrent controller and policy gradient training to efficiently search the graph architecture space. 3) Proposing a parameters sharing technique to accelerate the search process. 4) Validating GraphNAS on node classification tasks and showing it can design architectures that rival human-designed networks. The results demonstrate the promise of using reinforcement learning for automated neural architecture search in the graph domain. Overall, the GraphNAS framework provides an important step toward automating and improving graph neural network design.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:This paper proposes GraphNAS, a neural architecture search method that uses reinforcement learning to automatically design graph neural network architectures. GraphNAS uses a recurrent network controller to generate variable-length strings describing graph neural architectures. The controller is trained with policy gradient to maximize the expected accuracy of the generated architectures on a validation set. To make the search more efficient, GraphNAS shares parameters between child models and uses a new update strategy when training child models. After training the controller, the best architecture is derived by sampling models from the trained controller and selecting the one with the highest accuracy on the validation set after retraining from scratch. Experiments on citation network and protein-protein interaction datasets demonstrate that GraphNAS can design graph neural architectures that achieve state-of-the-art performance for node classification tasks in transductive and inductive settings.


## What problem or question is the paper addressing?

 This paper is proposing a new method called GraphNAS for automatic neural architecture search for graph neural networks (GNNs). The key problems/questions it aims to address are:- How to design an effective search space for GNN architectures. Unlike CNNs that work on grid-like data, GNNs work on graph data which is irregular. So the search space design is more challenging.- How to develop an efficient reinforcement learning based search algorithm to find good architectures in the large search space. Training convergence is slower for GNNs compared to CNNs.- How to evaluate the performance of GraphNAS in both transductive (labeled and unlabeled data in same graph) and inductive (train and test graphs are different) learning settings for tasks like node classification.In summary, the main problems are developing an appropriate search space, efficient search algorithm, and comprehensive evaluation for graph neural architecture search using reinforcement learning. The goal is to automate and improve GNN design compared to manual heuristics.
