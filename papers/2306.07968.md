# [arXiVeri: Automatic table verification with GPT](https://arxiv.org/abs/2306.07968)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we automatically verify the accuracy of numerical data contained in tables in academic papers by cross-referencing the cited sources?The key hypothesis seems to be:By leveraging large language models, it is possible to develop methods for automatic table verification that can accurately match tables across documents and identify corresponding cells containing the same data.To summarize, the main goal of the paper is to introduce a new task called Automatic Table Verification (AutoTV) that aims to verify numerical table data by comparing it to referenced sources. The central hypothesis is that large language models can be applied to this problem to perform table matching across documents and identify matching cells, despite challenges like varying data presentation. The paper proposes a benchmark dataset, metrics, and baselines to facilitate research on this task.Does this summarize the core research question and hypothesis accurately? Let me know if you need any clarification on my interpretation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing the new task of automatic table verification (AutoTV) for scientific documents. Specifically:- They introduce AutoTV, which aims to verify the accuracy of numerical data in tables by cross-referencing cited sources. This helps ensure fidelity when researchers manually copy data between papers.- To support research on AutoTV, they create a new benchmark dataset called arXiVeri. It contains 3.8K target-source cell pairs and 158 target-source table pairs extracted from arXiv papers. - They propose evaluation metrics for two key sub-tasks: table matching (identifying the source table corresponding to a target table) and cell matching (identifying rows/cols of equivalent cells between tables).- They provide baseline methods for AutoTV using large language models like GPT-4. The results demonstrate the complexity of AutoTV, even for state-of-the-art models.In summary, the key novelty is introducing the AutoTV task to ensure scientific data integrity, along with a benchmark, evaluation framework, and baselines to stimulate further research progress. The paper highlights the need for better automated tools to verify manually copied data in academia.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a new task called Automatic Table Verification (AutoTV) which aims to verify the accuracy of numerical data in tables by cross-referencing cited sources, proposes a new benchmark dataset called arXiVeri to stimulate research in this area, and provides simple baselines using large language models which reveal the complexity of the task.
