# [Improving Generalization of Adversarial Training via Robust Critical   Fine-Tuning](https://arxiv.org/abs/2308.02533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we improve the generalization performance and out-of-distribution robustness of adversarially trained deep neural network models without compromising their adversarial robustness?In particular, the paper investigates whether adversarially trained models exhibit "redundant capacity for robustness", analogous to the redundant capacity for generalization observed in standard trained models. The key hypothesis is that by identifying and fine-tuning the "non-robust-critical" modules of an adversarially trained model (i.e. modules that are redundant for robustness), it may be possible to enhance generalization and OOD robustness while maintaining adversarial robustness. To test this hypothesis, the paper introduces a new metric called "module robust criticality" (MRC) to quantify the robustness contribution of each module. The module with the lowest MRC is deemed "non-robust-critical" and fine-tuned using clean data. The fine-tuned weights are interpolated with the original adversarially trained weights to find the best tradeoff between standard accuracy, OOD robustness and adversarial robustness.In summary, the central question is whether redundant capacity for robustness exists in adversarially trained models, and whether exploiting this via fine-tuning "non-robust-critical" modules can simultaneously improve generalization and OOD robustness without compromising adversarial robustness. The concept of module robust criticality and the proposed RiFT method aim to address this question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new concept called Module Robust Criticality (MRC) to quantify the robustness contribution of each module in a neural network. The module with the lowest MRC value is considered non-robust-critical, meaning it has redundant capacity for robustness. 2. It proposes a method called Robustness Critical Fine-Tuning (RiFT) to exploit the redundant robustness capacity in adversarially trained models to improve their generalization performance. RiFT fine-tunes the non-robust-critical module identified by MRC on clean examples, and then interpolates between the fine-tuned weights and original adversarially trained weights to find the optimal balance between accuracy and robustness.3. Through experiments on various models and datasets, it demonstrates that RiFT can significantly enhance both the standard accuracy and out-of-distribution robustness of adversarially trained models by around 1-2%, while maintaining or slightly improving their adversarial robustness. 4. The results provide several insights, such as the existence of redundant robustness capacity in adversarially trained models, the potential for both accuracy and robustness to be improved simultaneously, and the promise of fine-tuning to further enhance out-of-distribution robustness.In summary, the key contribution is the proposal of MRC to identify redundant capacity in robust models, and RiFT to exploit this redundancy to enhance generalization without compromising robustness. The empirical results demonstrate the effectiveness of this approach across different settings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method called Module Robust Criticality to identify redundant modules in adversarially trained neural networks, and leverages this to fine-tune the models in a way that improves generalization performance without compromising robustness.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to related work in adversarial robustness and generalization:- This paper proposes a novel post-processing method called RiFT (Robust Critical Fine-Tuning) to improve the generalization of adversarially trained models without compromising robustness. Most prior work has focused on modifying the adversarial training procedure itself to address the generalization gap, while this work takes a different approach.- The concept of module robust criticality (MRC) introduced in this paper is new. While some previous works have studied module criticality for generalization, the authors extend this to focus specifically on robustness. Identifying non-robust critical modules with low MRC enables selective fine-tuning.- The paper shows RiFT is compatible with various adversarial training methods like TRADES, MART, AWP, etc. and can further enhance their generalization. This is a useful finding, as prior adversarial training enhancements have mainly focused within the training loop.- The results demonstrate RiFT improves in-distribution generalization and out-of-distribution robustness while preserving or slightly improving adversarial robustness across datasets and architectures. This sheds new light on the relationship between these properties.- The paper provides insights like fine-tuning adversarially trained models can further boost OOD robustness, in contrast to some prior work showing fine-tuning degrades OOD performance. The existence of non-robust critical modules also suggests current adversarial training is not fully utilizing model capacity.Overall, this paper introduces a novel perspective of exploiting redundant robustness capacity after adversarial training to improve generalization. The proposed RiFT approach and analysis around module criticality help advance understanding in this area. The results also highlight open questions around more efficient training and utilizing capacity that could motivate future work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing more efficient adversarial training approaches that fully utilize the redundant capacity in adversarially trained models. The authors found that current adversarial training regimes do not fully exploit the capacity of neural networks, as evidenced by the existence of non-robust critical modules. This motivates designing improved training methods that can better leverage this unused capacity.- Further theoretical and empirical analysis on fine-tuning of adversarially trained models. The authors propose fine-tuning adversarially trained models as a promising direction for enhancing out-of-distribution robustness and improving generalization. More research is needed to advance the understanding of this technique. - Investigating the relationship between model capacity, task complexity, and the gains provided by the proposed RiFT approach. The authors observe patterns connecting these factors to the generalization improvements, indicating potential strategies could emerge from studying them further.- Broadening the definition of module robust criticality to encompass the effects of simultaneous worst-case perturbations across multiple modules. The current MRC definition considers modules individually, making it difficult to account for interactions.- Exploring other ways to identify and leverage redundant capacity beyond fine-tuning a single non-robust critical module. The paper shows fine-tuning multiple modules did not improve results, but other approaches may prove effective.In summary, the authors highlight the potential of exploiting redundant robustness capacity and emphasize that techniques like fine-tuning adversarially trained models warrant greater exploration, both empirically and theoretically. Their findings reveal promising research directions for improving generalization and robustness of deep networks.


## Summarize the paper in one paragraph.

The paper proposes a method called Module Robust Criticality Characterization for evaluating the importance of each module/layer in a neural network to the model's adversarial robustness. It calculates the worst-case increase in robust loss when perturbing the weights of each module under a constrained perturbation. Modules with low increases in robust loss are deemed non-robust-critical, indicating they have redundant capacity for robustness. Based on this analysis, the paper proposes a fine-tuning method called Robust Critical Fine-Tuning (RiFT) to leverage this redundant capacity to improve generalization without compromising robustness. It first calculates the module robust criticality of each module. It then fine-tunes only the weights of the module deemed most non-robust-critical on clean examples. Finally, it interpolates between the fine-tuned weights and original adversarially trained weights to find the optimal weights that improve generalization while maintaining robustness. Experiments on CIFAR and Tiny ImageNet datasets with ResNet and WideResNet models show RiFT can improve generalization and out-of-distribution robustness by around 1-2% while preserving or slightly improving adversarial robustness. The analysis also reveals insights into the relationship between generalization, robustness and model capacity.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a method called Module Robust Criticality Characterization to identify modules in neural networks that are not critical for robustness against adversarial examples. The key idea is to perturb the weights of each module and measure the change in robustness loss - modules that have little effect on robustness when perturbed are deemed "non-robust-critical". The paper then uses this analysis to propose a fine-tuning method called Robustness Critical Fine-Tuning (RiFT). RiFT freezes all modules except the least robust-critical one, and fine-tunes only that module on clean examples to improve standard accuracy. It then interpolates between the adversarially trained weights and fine-tuned weights to find the best balance between accuracy and robustness. The experiments show RiFT improves standard and out-of-distribution accuracy by around 1-2% on CIFAR and Tiny ImageNet datasets while preserving the adversarial robustness. The results indicate adversarially trained models have redundant capacity that can be utilized through selective fine-tuning. The paper provides insights into the relationship between robustness, accuracy and model capacity. It suggests fine-tuning adversarially trained models could be a promising way to further enhance robustness and accuracy. The proposed module criticality measure could also inspire more efficient adversarial training methods.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a method called Robust Critical Fine-Tuning (RiFT) to improve the generalization of adversarially trained deep neural networks without compromising adversarial robustness. The key idea is to leverage the redundant capacity for robustness in adversarially trained models by fine-tuning the parameters of the module that is least critical for robustness, referred to as the non-robust-critical module. The method has three main steps:1. Characterize the module robust criticality (MRC) of each module, which measures the maximum increase in robust loss when perturbing the module's weights. The module with the lowest MRC is identified as the non-robust-critical module.2. Fine-tune only the non-robust-critical module parameters on clean examples to exploit its redundant capacity. 3. Interpolate between the adversarially trained weights and fine-tuned weights to find the optimal point that improves generalization while preserving robustness.By fine-tuning the non-critical module for robustness, RiFT is able to enhance model generalization and out-of-distribution robustness without compromising the adversarial robustness achieved by adversarial training. Experiments show consistent improvements across models and datasets.
