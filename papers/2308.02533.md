# [Improving Generalization of Adversarial Training via Robust Critical   Fine-Tuning](https://arxiv.org/abs/2308.02533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we improve the generalization performance and out-of-distribution robustness of adversarially trained deep neural network models without compromising their adversarial robustness?In particular, the paper investigates whether adversarially trained models exhibit "redundant capacity for robustness", analogous to the redundant capacity for generalization observed in standard trained models. The key hypothesis is that by identifying and fine-tuning the "non-robust-critical" modules of an adversarially trained model (i.e. modules that are redundant for robustness), it may be possible to enhance generalization and OOD robustness while maintaining adversarial robustness. To test this hypothesis, the paper introduces a new metric called "module robust criticality" (MRC) to quantify the robustness contribution of each module. The module with the lowest MRC is deemed "non-robust-critical" and fine-tuned using clean data. The fine-tuned weights are interpolated with the original adversarially trained weights to find the best tradeoff between standard accuracy, OOD robustness and adversarial robustness.In summary, the central question is whether redundant capacity for robustness exists in adversarially trained models, and whether exploiting this via fine-tuning "non-robust-critical" modules can simultaneously improve generalization and OOD robustness without compromising adversarial robustness. The concept of module robust criticality and the proposed RiFT method aim to address this question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new concept called Module Robust Criticality (MRC) to quantify the robustness contribution of each module in a neural network. The module with the lowest MRC value is considered non-robust-critical, meaning it has redundant capacity for robustness. 2. It proposes a method called Robustness Critical Fine-Tuning (RiFT) to exploit the redundant robustness capacity in adversarially trained models to improve their generalization performance. RiFT fine-tunes the non-robust-critical module identified by MRC on clean examples, and then interpolates between the fine-tuned weights and original adversarially trained weights to find the optimal balance between accuracy and robustness.3. Through experiments on various models and datasets, it demonstrates that RiFT can significantly enhance both the standard accuracy and out-of-distribution robustness of adversarially trained models by around 1-2%, while maintaining or slightly improving their adversarial robustness. 4. The results provide several insights, such as the existence of redundant robustness capacity in adversarially trained models, the potential for both accuracy and robustness to be improved simultaneously, and the promise of fine-tuning to further enhance out-of-distribution robustness.In summary, the key contribution is the proposal of MRC to identify redundant capacity in robust models, and RiFT to exploit this redundancy to enhance generalization without compromising robustness. The empirical results demonstrate the effectiveness of this approach across different settings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method called Module Robust Criticality to identify redundant modules in adversarially trained neural networks, and leverages this to fine-tune the models in a way that improves generalization performance without compromising robustness.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to related work in adversarial robustness and generalization:- This paper proposes a novel post-processing method called RiFT (Robust Critical Fine-Tuning) to improve the generalization of adversarially trained models without compromising robustness. Most prior work has focused on modifying the adversarial training procedure itself to address the generalization gap, while this work takes a different approach.- The concept of module robust criticality (MRC) introduced in this paper is new. While some previous works have studied module criticality for generalization, the authors extend this to focus specifically on robustness. Identifying non-robust critical modules with low MRC enables selective fine-tuning.- The paper shows RiFT is compatible with various adversarial training methods like TRADES, MART, AWP, etc. and can further enhance their generalization. This is a useful finding, as prior adversarial training enhancements have mainly focused within the training loop.- The results demonstrate RiFT improves in-distribution generalization and out-of-distribution robustness while preserving or slightly improving adversarial robustness across datasets and architectures. This sheds new light on the relationship between these properties.- The paper provides insights like fine-tuning adversarially trained models can further boost OOD robustness, in contrast to some prior work showing fine-tuning degrades OOD performance. The existence of non-robust critical modules also suggests current adversarial training is not fully utilizing model capacity.Overall, this paper introduces a novel perspective of exploiting redundant robustness capacity after adversarial training to improve generalization. The proposed RiFT approach and analysis around module criticality help advance understanding in this area. The results also highlight open questions around more efficient training and utilizing capacity that could motivate future work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing more efficient adversarial training approaches that fully utilize the redundant capacity in adversarially trained models. The authors found that current adversarial training regimes do not fully exploit the capacity of neural networks, as evidenced by the existence of non-robust critical modules. This motivates designing improved training methods that can better leverage this unused capacity.- Further theoretical and empirical analysis on fine-tuning of adversarially trained models. The authors propose fine-tuning adversarially trained models as a promising direction for enhancing out-of-distribution robustness and improving generalization. More research is needed to advance the understanding of this technique. - Investigating the relationship between model capacity, task complexity, and the gains provided by the proposed RiFT approach. The authors observe patterns connecting these factors to the generalization improvements, indicating potential strategies could emerge from studying them further.- Broadening the definition of module robust criticality to encompass the effects of simultaneous worst-case perturbations across multiple modules. The current MRC definition considers modules individually, making it difficult to account for interactions.- Exploring other ways to identify and leverage redundant capacity beyond fine-tuning a single non-robust critical module. The paper shows fine-tuning multiple modules did not improve results, but other approaches may prove effective.In summary, the authors highlight the potential of exploiting redundant robustness capacity and emphasize that techniques like fine-tuning adversarially trained models warrant greater exploration, both empirically and theoretically. Their findings reveal promising research directions for improving generalization and robustness of deep networks.
