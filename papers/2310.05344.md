# [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to   RLHF](https://arxiv.org/abs/2310.05344)

## What is the main contribution of this paper?

 Based on my reading, the key contributions of this paper seem to be:

- Introducing SteerLM, a new approach for aligning large language models (LLMs) to human preferences through supervised fine-tuning. SteerLM trains an attribute prediction model on datasets with annotated attributes like quality, humor, and toxicity. 

- Leveraging the attribute prediction model to annotate diverse datasets and use them to do attribute-conditioned supervised fine-tuning. This allows learning from both high and low quality responses, similar to RLHF approaches.

- Showcasing the flexibility of SteerLM models to steer generation towards different attributes like humor or toxicity during inference without retraining. This makes the models adaptable to different use cases.

- Demonstrating state-of-the-art performance of SteerLM models on the Vicuna benchmark, outperforming strong baselines like ChatGPT-3.5. The results are validated through both automatic evaluation with GPT-4 and human evaluation.

- Providing an effective yet simple alternative to complex RLHF methods for aligning LLMs, with the goal of enabling better and customizable AI assistants.

In summary, the main contribution appears to be the proposal and evaluation of SteerLM as a new supervised learning based approach for aligning LLMs to human preferences in a customizable way, demonstrating strong performance compared to existing methods.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the same field of large language model alignment:

- The key contribution is proposing SteerLM as an alternative alignment approach to RLHF (reinforcement learning from human feedback). RLHF is considered the current state-of-the-art for aligning LLMs, but has limitations in training complexity and controlling model values at inference time. 

- The paper compares against other recent works that use supervised fine-tuning (SFT) alone for alignment, such as Dettmer et al. and Kopf et al. It shows SteerLM outperforms SFT baselines, likely because it better incorporates reward signal information through using predicted attribute labels during training.

- Most similar works have focused on SFT or RLHF in isolation. A few like Bai et al. and Ouyang et al. combine SFT and RLHF, but SteerLM proposes a novel alternative that only uses SFT while achieving comparable results. 

- Unique aspects are the attribute prediction model to enable scoring diverse datasets, the attribute conditioned SFT approach, and the inference-time steering capability. These allow better alignment while retaining user control.

- Compared to attribute grounded generation works, this paper handles open-domain dialog with multiple steerable attributes rather than single-attribute grounding in narrow tasks.

- Evaluation is quite extensive, with both automatic (GPT-4) and human evaluations on the Vicuna benchmark. Results show SteerLM performs better than RLHF and other baselines.

In summary, this paper introduces a simple yet effective approach for LLM alignment that rivals RLHF results using only SFT. The ability to steer generations is also novel and addresses limitations in current alignment techniques. The strong empirical results validate SteerLM as a promising new technique in this field.


## What future research directions do the authors suggest?

 After reviewing the paper, some of the key future research directions suggested by the authors include:

- Exploring different and larger base models for SteerLM besides the 43B and 13B models evaluated in the paper. The authors note that model size has an effect on performance, so trying SteerLM on larger models could further improve results.

- Testing SteerLM on multilingual benchmarks and prompts. The current evaluations are done only on English, so evaluating the approach on other languages would be useful.

- Investigating the efficacy of SteerLM on more complex conversational tasks beyond single-turn prompts. The current benchmarks focus on single exchanges, but SteerLM may be applicable to multi-turn conversations as well.

- Enhancing the flexibility of steering by giving users the ability to combine and tune the influence of different attributes rather than setting them independently. This could allow smoother and more nuanced control.

- Developing techniques to make training more parameter-efficient compared to fine-tuning the full models, to reduce compute costs. Methods like adapter tuning could help here.

- Exploring different training schemes, like iteratively growing the training dataset to further improve sample efficiency and quality.

- Comparing to and combining SteerLM with other alignment techniques like reinforcement learning and prompts/demonstrations. Hybrid approaches could be fruitful.

- Developing better automatic evaluation metrics, as relying just on model-based assessments like GPT-3 can be biased. More human studies could help too.

So in summary, the authors propose several worthwhile directions around model scaling, multilinguality, conversational modeling, flexible steering, efficiency, training schemes, architecture combinations, and evaluation improvements. Testing SteerLM in broader and more rigorous settings seems like a key next step.
