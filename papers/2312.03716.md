# [Co-guiding for Multi-intent Spoken Language Understanding](https://arxiv.org/abs/2312.03716)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper tackles the task of multi-intent spoken language understanding (SLU), which includes multiple intent detection and slot filling. Existing methods suffer from three issues: (1) They only model the unidirectional guidance from intent prediction to slot filling, while ignoring the guidance from slot to intent. (2) They use homogeneous graphs to model intent-slot interactions, causing node and edge ambiguity. (3) They ignore the subtle semantic differences among the instances of the two tasks.

Proposed Solution: 
This paper proposes a two-stage Co-guiding Net to achieve mutual guidances between multiple intent detection and slot filling. In stage 1, initial labels for both tasks are predicted. In stage 2, the initial predictions are leveraged to guide each other via two proposed heterogeneous graph attention networks (HGATs) over two designed heterogeneous semantics-label graphs. This allows modeling of node-specific semantics-label interactions.

To further capture single-task and dual-task semantics contrastive relations, the paper proposes Co-guiding-SCL Net. It performs: (1) Single-task supervised contrastive learning in stage 1; (2) Co-guiding supervised contrastive learning in stage 2, which integrates dual-task correlations by jointly considering both tasks' labels as supervision signal.

Main Contributions:
(1) Proposes the first model to achieve mutual guidances between multiple intent detection and slot filling.
(2) Designs heterogeneous semantics-label graphs and heterogeneous graph attention networks to model semantics-label interactions.
(3) Proposes single-task and co-guiding supervised contrastive learning to capture single-task and dual-task semantics contrastive relations.

Experiments show the model achieves new state-of-the-art results on public multi-intent SLU datasets, significantly outperforming previous models. The model also achieves strong improvements on zero-shot cross-lingual multi-intent SLU.
