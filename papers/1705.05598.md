# Learning how to explain neural networks: PatternNet and   PatternAttribution

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that existing methods for explaining neural network decisions (such as DeConvNet, Guided BackProp and Layer-wise Relevance Propagation) do not produce theoretically correct explanations even for simple linear models. The key points are:- The authors analyze how linear models work, showing that the weights of a linear model are primarily determined by canceling out "distractor" components in the input, not by detecting the "signal" component. - They argue that visualizing the weights alone does not reveal the input patterns that are truly relevant for the model's output. Identifying these relevant patterns requires learning from both the model and the data distribution.- They propose new methods called PatternNet and PatternAttribution that learn to estimate the relevant input patterns and use these to produce improved explanations for neural networks.- Experiments show PatternNet and PatternAttribution give superior results compared to existing methods like DeConvNet and Layer-wise Relevance Propagation, even on simple linear models where ground truth is known.So in summary, the central hypothesis is that current explanation methods are flawed even for linear models, and the authors propose novel learned explanation techniques that perform better.


## What is the main contribution of this paper?

The main contribution of this paper is proposing two new methods, PatternNet and PatternAttribution, for explaining and interpreting neural network models. The key ideas are:- Analyzing explanation methods for neural networks in the context of linear models. The authors show that popular approaches like DeConvNet, GuidedBackProp, and Layer-wise Relevance Propagation (LRP) do not provide correct explanations even for simple linear models. - Proposing a new quality measure for evaluating how well an explanation method captures the true "signal" versus "distractor" components in the input data.- Deriving a new signal estimator called PatternAttribution that optimizes this quality measure. It learns to estimate the signal and distractor components. - Using PatternAttribution to propose two new explanation methods:1) PatternNet - Projects the estimated signal back to the input space to visualize what input patterns caused a neuron's activation.2) PatternAttribution - Computes pixel-wise relevance scores by element-wise multiplying the signal estimate with the weight vector. Improves upon LRP.- Showing qualitatively and quantitatively that PatternNet and PatternAttribution produce improved visualizations and heatmaps compared to prior methods.In summary, the main contribution is developing a theoretical framework for evaluating explanation methods on neural networks, and using it to derive two new methods that produce superior explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper analyzes explanation methods for deep neural networks and finds that popular approaches like DeConvNet, GuidedBackProp and LRP fail even for simple linear models; the authors propose new methods called PatternNet and PatternAttribution that provide improved explanations by estimating the true signal and distractor components in the data.
