# Learning how to explain neural networks: PatternNet and   PatternAttribution

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that existing methods for explaining neural network decisions (such as DeConvNet, Guided BackProp and Layer-wise Relevance Propagation) do not produce theoretically correct explanations even for simple linear models. The key points are:- The authors analyze how linear models work, showing that the weights of a linear model are primarily determined by canceling out "distractor" components in the input, not by detecting the "signal" component. - They argue that visualizing the weights alone does not reveal the input patterns that are truly relevant for the model's output. Identifying these relevant patterns requires learning from both the model and the data distribution.- They propose new methods called PatternNet and PatternAttribution that learn to estimate the relevant input patterns and use these to produce improved explanations for neural networks.- Experiments show PatternNet and PatternAttribution give superior results compared to existing methods like DeConvNet and Layer-wise Relevance Propagation, even on simple linear models where ground truth is known.So in summary, the central hypothesis is that current explanation methods are flawed even for linear models, and the authors propose novel learned explanation techniques that perform better.


## What is the main contribution of this paper?

The main contribution of this paper is proposing two new methods, PatternNet and PatternAttribution, for explaining and interpreting neural network models. The key ideas are:- Analyzing explanation methods for neural networks in the context of linear models. The authors show that popular approaches like DeConvNet, GuidedBackProp, and Layer-wise Relevance Propagation (LRP) do not provide correct explanations even for simple linear models. - Proposing a new quality measure for evaluating how well an explanation method captures the true "signal" versus "distractor" components in the input data.- Deriving a new signal estimator called PatternAttribution that optimizes this quality measure. It learns to estimate the signal and distractor components. - Using PatternAttribution to propose two new explanation methods:1) PatternNet - Projects the estimated signal back to the input space to visualize what input patterns caused a neuron's activation.2) PatternAttribution - Computes pixel-wise relevance scores by element-wise multiplying the signal estimate with the weight vector. Improves upon LRP.- Showing qualitatively and quantitatively that PatternNet and PatternAttribution produce improved visualizations and heatmaps compared to prior methods.In summary, the main contribution is developing a theoretical framework for evaluating explanation methods on neural networks, and using it to derive two new methods that produce superior explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper analyzes explanation methods for deep neural networks and finds that popular approaches like DeConvNet, GuidedBackProp and LRP fail even for simple linear models; the authors propose new methods called PatternNet and PatternAttribution that provide improved explanations by estimating the true signal and distractor components in the data.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on explaining and visualizing deep neural networks:- It starts with a fundamental analysis of linear models, showing that the weights do not necessarily reveal the relevant input patterns. This highlights flaws in existing methods like DeConvNet, Guided Backprop, and LRP that become obvious even in simple linear settings. - The paper proposes a novel quality criterion for evaluating signal estimators, based on how much output-correlated information remains in the residuals after removing the estimated signal. This provides a principled way to optimize and compare explanation methods.- It introduces two new methods, PatternNet and PatternAttribution, that learn optimized signal estimators for each layer based on this criterion. These are shown to outperform previous approaches both quantitatively and qualitatively.- The proposed methods are among the first to provide theoretically sound explanations even for simple linear models. Previous popular methods like DeConvNet, Guided Backprop, LRP, etc. fail at this basic task.- Experiments highlight flaws in prevailing assumptions that gradient-based visualizations reveal the input signal. The analysis and proposed techniques generalize recent work on interpreting linear models to deep neural networks. - The approach shares some similarities with concurrent work by Zintgraf et al. and Dabkowski et al. in learning to explain models. But it offers a broader theoretical framework applicable beyond image models.Overall, this paper makes fundamental connections between deep nets and linear models for interpretability. The proposed methods offer improved visualization and attribution with a strong theoretical grounding lacking in many previous techniques. The qualitative and quantitative experiments demonstrate clear benefits over existing approaches in identifying the input signals that produce activations.
