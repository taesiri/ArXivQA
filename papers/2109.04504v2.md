# [Bootstrapped Meta-Learning](https://arxiv.org/abs/2109.04504v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper investigates is how to improve the efficiency and effectiveness of meta-learning algorithms. Specifically, it proposes a new meta-learning algorithm called Bootstrapped Meta-Gradients (BMG) that aims to address two key challenges in meta-learning:

1. Mitigating short-horizon bias: Standard meta-learning algorithms like MAML optimize performance after a small number of gradient steps K. This can lead to myopic optimization and poor performance when the learner must continue learning after K steps. BMG aims to extend the effective optimization horizon by using a "target bootstrap" that simulates taking additional steps after K.

2. Controlling curvature: The meta-objective in standard meta-learning has the same curvature as the base learning problem. If the base problem is ill-conditioned, meta-optimization will also be ill-conditioned. BMG aims to improve the geometry of meta-optimization by matching the base learner to a target in a chosen metric space. 

The key ideas in BMG are:

- Generate a target for the base learner by simulating additional update steps after K steps. This infuses information about future learning dynamics into the meta-objective.

- Match the base learner to this target under a chosen metric like KL divergence rather than directly optimizing the base objective. This allows controlling the curvature of meta-optimization.

- Use the mismatch between the base learner and target to update meta-parameters and improve the update rule.

So in summary, the central hypothesis is that bootstrapping targets and matching in chosen metric spaces will allow BMG to extend the optimization horizon and better condition the meta-learning problem compared to standard approaches. The paper aims to demonstrate these advantages empirically and theoretically.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution seems to be proposing a new meta-learning algorithm called Bootstrapped Meta-Gradients (BMG). The key ideas behind BMG are:

- Using a "target bootstrap" to generate future targets for the meta-learner, rather than directly optimizing the meta-learner's objective. This helps address the short-horizon bias in standard meta-learning. 

- Optimizing the meta-learner by minimizing the distance/divergence to the bootstrapped target under some metric. This allows controlling the geometry of the meta-optimization problem.

- The bootstrapping mechanism allows extending the effective meta-learning horizon without needing to backpropagate through all the update steps.

So in summary, BMG introduces a new form of meta-learning objective based on bootstrapped targets and distance minimization that can improve performance, efficiency, and enable new capabilities compared to standard meta-gradients. The authors demonstrate benefits of BMG on RL benchmark tasks as well as few-shot image classification.
