# [Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning":

Problem:
- Fine-tuning large pre-trained language models (LLMs) on downstream tasks requires updating all the parameters, which is computationally expensive. 
- Low-rank adaptation (LoRA) reduces this cost by approximating the weight updates with low-rank matrices. However, the low-rank approximation causes a performance gap compared to full fine-tuning.

Proposed Solution:
- The paper proposes Mini-Ensemble LoRA (MELoRA), which freezes the original pre-trained weights and trains multiple small LoRA modules in parallel.
- Each mini LoRA module captures a different low-rank update. By concatenating their outputs, MELoRA achieves a higher overall rank without additional parameters.
- Theoretically, the rank of MELoRA is the sum of the ranks of each mini LoRA. This provides more expressive power using the same number of parameters.

Main Contributions:
- Proposes MELoRA method to achieve higher and flexible rank adaptations using fewer trainable parameters than regular LoRA.
- Provides theoretical analysis to show that MELoRA attains higher rank and lower complexity than LoRA.
- Empirically demonstrates that MELoRA outperforms LoRA on multiple NLP datasets, using 8x fewer parameters on NLU tasks and 36x fewer on instruction following.
- Shows the effect of key hyperparameters like number and rank of mini LoRAs, revealing optimal configurations for different datasets.

In summary, the paper presents MELoRA as an improved parameter-efficient fine-tuning approach over LoRA, enabling better performance with fewer trainable parameters. Theoretical and empirical analyses confirm the benefits of the proposed method.
