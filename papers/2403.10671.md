# [Hessian-Free Laplace in Bayesian Deep Learning](https://arxiv.org/abs/2403.10671)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Bayesian neural networks place a prior on model weights and allow quantifying uncertainty, but computing the full posterior distribution is intractable for large models. 
- The Laplace approximation uses a Gaussian centered at the maximum a posteriori (MAP) estimate to approximate the posterior. However, calculating and inverting the Hessian matrix, which captures local curvature, is a major bottleneck.

Proposed Solution:
- The paper proposes Hessian-Free Laplace (HFL) to estimate the predictive variance of the Laplace approximation without explicitly computing the Hessian. 
- HFL requires two point estimates - the standard MAP estimate and the prediction-regularized MAP estimate where the prediction at a query point is added as a regularizer to the objective. 
- Under common assumptions, the scaled difference in predictions from these two estimates recovers the Laplace predictive variance. This allows uncertainty quantification without the Hessian.

Key Contributions:
- Develop the theoretical basis connecting regularization and uncertainty through the implicit function theorem.
- Introduce Hessian-Free Laplace which avoids expensive Hessian calculations by using prediction regularization.
- Propose ways to amortize HFL through pre-training on evaluation sets.
- Empirically demonstrate strong performance compared to approximations using the exact or approximate Hessian on a range of uncertainty tasks.
- Show that HFL can be used for parameter uncertainty and model introspection like finding redundant parameters.

In summary, the paper makes Bayesian deep learning more scalable by developing Hessian-free Laplace based on regularization. This provides accurate and calibrated uncertainties without expensive curvature calculations.
