# [Character Generation through Self-Supervised Vectorization](https://arxiv.org/abs/2208.02012)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a self-supervised drawing agent that can generate and parse characters by operating on stroke-level vector representations, using only raster images as supervision during training?Some key points about the research question:- The paper focuses on stroke-based image generation and parsing, where images are built up sequentially stroke-by-stroke rather than generated all at once. This allows the model to leverage the compositional structure of the images.- The model operates directly on vector (stroke) representations rather than pixel representations. This avoids issues like blurriness that affect pixel-based models.- Crucially, they aim to train the model using only raster images as supervision. Previous stroke-based models rely on full stroke-level supervision during training, which is costly to obtain. Their method is "self-supervised" in that it learns the vectorization purely from raster images.- They aim to develop an agent that can handle multiple generation tasks (unconditional, conditioned on exemplar or type) as well as parsing/reconstruction. Showing one model architecture can successfully tackle all these tasks is an important contribution.So in summary, the key hypothesis is that a self-supervised stroke-based drawing agent can match the performance of supervised approaches on generation and parsing tasks, using only raster images during training. The paper then supports this hypothesis through experiments on different datasets and tasks.
