# [Character Generation through Self-Supervised Vectorization](https://arxiv.org/abs/2208.02012)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a self-supervised drawing agent that can generate and parse characters by operating on stroke-level vector representations, using only raster images as supervision during training?Some key points about the research question:- The paper focuses on stroke-based image generation and parsing, where images are built up sequentially stroke-by-stroke rather than generated all at once. This allows the model to leverage the compositional structure of the images.- The model operates directly on vector (stroke) representations rather than pixel representations. This avoids issues like blurriness that affect pixel-based models.- Crucially, they aim to train the model using only raster images as supervision. Previous stroke-based models rely on full stroke-level supervision during training, which is costly to obtain. Their method is "self-supervised" in that it learns the vectorization purely from raster images.- They aim to develop an agent that can handle multiple generation tasks (unconditional, conditioned on exemplar or type) as well as parsing/reconstruction. Showing one model architecture can successfully tackle all these tasks is an important contribution.So in summary, the key hypothesis is that a self-supervised stroke-based drawing agent can match the performance of supervised approaches on generation and parsing tasks, using only raster images during training. The paper then supports this hypothesis through experiments on different datasets and tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a drawing agent that operates on the stroke-level representation of images for character generation and parsing. The key aspects of the agent are:- It takes the current canvas as input at each time step and decides whether to stop drawing or output a stroke program to keep drawing. - It is trained via reinforcement learning, using different reward functions for unconditional character generation vs parsing/reconstruction tasks.- It does not require any vector/stroke-level supervision, only raster images. 2. Demonstrating that the parsing agent can be used for exemplar generation and type-conditioned concept generation without any further training, by exploiting the learned compositionality.3. Achieving unconditional generation, parsing, exemplar generation, and type-conditioned concept generation on MNIST and Omniglot datasets using the proposed approach. The method is the first self-supervised stroke-based approach to accomplish all of these tasks.4. Providing quantitative benchmarks on the exemplar and type-conditioned concept generation tasks using perceptual similarity metrics like LPIPS, which has not been done before.So in summary, the key contribution is proposing a novel self-supervised stroke-based agent that can handle multiple generation tasks, and demonstrating its capabilities on standard datasets while also providing new quantitative evaluations. The main advantage is learning stroke-level representations without needing vector supervision.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a drawing agent that can generate novel handwritten characters through stroke-based vector representations in a completely self-supervised manner, without requiring any vector-level supervision, and shows successful results on unconditional generation, parsing/reconstruction, exemplar generation, and type-conditioned generation tasks.


## How does this paper compare to other research in the same field?

Based on my review, here are some key ways this paper compares to other related research:- The paper presents a novel stroke-based drawing agent for character generation and parsing tasks. Other recent work like SPIRAL, Sketch-RNN, Sketchformer operate on the stroke level as well, but rely on stroke-level supervision during training. This paper's approach is self-supervised and only uses raster images, making it more widely applicable.- For the character parsing (reconstruction) task, the paper shows competitive quantitative results to other self-supervised approaches like SPIRAL and Learning to Paint. However, those methods focused mainly on parsing, while this paper tackles multiple generation tasks in addition to parsing.- For unconditional character generation, the paper shows MNIST samples that seem on par with other GAN methods like WGAN in terms of visual quality. The FID score is reasonable but not state-of-the-art. For Omniglot, the generations demonstrate diversity and conform to alphabet styles, unlike previous models.- This is the first stroke-based method that achieves exemplar generation and type-conditioned concept generation on Omniglot in a self-supervised manner. Prior Bayesian program learning models required stroke supervision for these tasks. Quantitative metrics are provided for the first time.- Compared to raster-based Omniglot models like DRAW, this method better captures the compositional structure of characters and generates interpretable programs. But one-step raster models can generate higher visual fidelity currently.In summary, the key novelty is the self-supervised stroke-based approach, which expands the capability of character generation and parsing models to be applicable to datasets without stroke labels. Both qualitative results and benchmarking metrics are provided demonstrating these capabilities.
