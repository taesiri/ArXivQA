# [CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring   Commonsense Reasoning and Long-Tail Knowledge](https://arxiv.org/abs/2403.01395)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing knowledge graph QA (KGQA) datasets have two major limitations: 
   1) No dataset requires commonsense reasoning beyond factual queries
   2) Existing datasets focus on popular entities that large language models (LLMs) can directly answer without needing the knowledge graph (KG)
- Thus there is a need for a KGQA dataset that:
   1) Requires commonsense reasoning 
   2) Focuses on long-tail, obscure entities where LLMs are prone to hallucinating

Proposed Solution:
- The authors create a new KGQA dataset called CR-LT-KGQA with two subtasks - question answering and claim verification
- CR-LT-KGQA is built by extending two commonsense reasoning datasets - StrategyQA and CREAK - to use long-tail entities from Wikidata instead of famous entities
- Queries require commonsense reasoning like temporal reasoning, geographical reasoning etc. along with facts from the KG
- Baselines are established using GPT-3.5 with Chain-of-Thought prompting, evaluated on accuracy, factuality and reasoning faithfulness

Main Contributions:
- First KGQA dataset requiring commonsense reasoning, unlike existing factoid KGQA datasets
- Focuses on long-tail entities where LLMs hallucinate, needing the KG for grounding
- Evaluation shows LLMs struggle on CR-LT-KGQA, demonstrating the challenges posed and the need for novel KGQA methods that can perform complex reasoning
- Overall, CR-LT-KGQA opens up the ability to perform richer KGQA in the era of hallucination-prone LLMs

In summary, the paper identifies limitations of existing KGQA datasets and provides a novel benchmark, CR-LT-KGQA, that requires commonsense reasoning over long-tail entities, posing new challenges for both LLMs and KGQA methods.


## Summarize the paper in one sentence.

 This paper introduces CR-LT-KGQA, a new knowledge graph question answering dataset requiring commonsense reasoning over long-tail entities to address limitations of existing KGQA datasets, with baseline evaluations showing language models struggle on it due to hallucination.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Creating CR-LT-KGQA, a novel dataset requiring commonsense reasoning and long-tail knowledge about entities for two subtasks - question answering and claim verification. This is the first KGQA dataset that goes beyond factoid questions and requires making commonsense inferences.

2. Establishing baseline results using large language models (LLMs) which show a high rate of hallucination on the long-tail entities in CR-LT-KGQA. This demonstrates the challenges posed by the dataset for LLMs and the need for future research to design KGQA methods that can provide factual and attributable commonsense inference for long-tail entities.

So in summary, the main contributions are proposing a new challenging KGQA dataset requiring commonsense reasoning and long-tail knowledge, and benchmarking state-of-the-art LLMs on it to demonstrate the need for future research to handle such complex KGQA queries.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with it are:

- Knowledge graph question answering (KGQA)
- Commonsense reasoning
- Long-tail knowledge
- Wikidata
- Language models (LLMs)
- Hallucination
- Factoid queries
- Chain-of-Thought (CoT) prompting
- FActScore
- Reasoning score

The paper introduces a new KGQA dataset called CR-LT-KGQA that requires commonsense reasoning over long-tail entities from Wikidata. It evaluates the performance of large language models on this dataset and shows they suffer from a high rate of hallucination due to the long-tail nature of the queries. The paper also defines new evaluation metrics like FActScore and reasoning score to specifically measure factuality and reasoning faithfulness. Overall, the key focus is on pushing the boundaries of KGQA to require more complex commonsense reasoning while also targeting obscure, long-tail entities to limit the knowledge already encoded in language models and reduce hallucination.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper describes a two-step process for query selection involving two annotators. What were the roles of the first and second annotators respectively? What criteria did they use to select or validate questions?

2. When replacing entities in the questions with long-tail counterparts, what process was followed to generate and select suitable candidate entities from Wikidata? What SPARQL queries and criteria were used? 

3. When rewriting questions to be more natural, the authors followed a coding scheme for evaluating question naturalness. What were some of the key dimensions of this coding scheme that were considered when rewriting questions?

4. The inference rules annotated for each question express commonsense knowledge as logical axioms. What is the structure of these axioms, including the predicates, functions and operators used? Provide some examples.

5. What are the two types of reasoning steps annotated for each question? How do they depend on each other sequentially? What ground truth evidence is provided for certain types of steps?

6. Two measures, entity popularity and number of reasoning steps, are used to characterize question difficulty. How exactly are these measures quantified based on the given annotations? How do they vary between the question answering and claim verification subsets?

7. A taxonomy of reasoning skills involved in answering the questions is provided. What are some of the key domain-specific and domain-independent skills annotated? Provide some statistical examples characterizing their distribution.  

8. Since existing KGQA methods don't support commonsense reasoning, what evaluation methodology was chosen instead? Why wouldn't methods like semantic parsing or translating to logical forms work?

9. Beyond accuracy, what custom human evaluation metrics were defined to assess the factualness and reasoning fidelity? Precisely define these metrics. 

10. Qualitatively analyze the most frequent error modes of LLMs based on the human evaluations performed. What opportunities exist for future research to address these deficiencies?
