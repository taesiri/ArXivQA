# [Is a prompt and a few samples all you need? Using GPT-4 for data   augmentation in low-resource classification tasks](https://arxiv.org/abs/2304.13861)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether large language models (LLMs) like GPT-4 and ChatGPT can be effectively used to augment small sets of human-generated training data for text classification tasks, particularly in low-resource settings. Specifically, the authors investigate whether synthetic training data generated by LLMs using simple prompts can be used to train models that achieve comparable or better performance than models trained on human-labeled data. They also explore the zero-shot classification capabilities of GPT-4 and ChatGPT on the classification tasks.The key hypothesis seems to be that even using simple prompts, LLMs can generate useful synthetic training data for low-resource classification tasks. The authors test this by augmenting small seed datasets across tasks of varying complexity, training models on the synthetic data, and comparing their performance to human-labeled data.In summary, the central research question is whether LLMs can be leveraged for effective data augmentation in sparse training data scenarios via basic prompting, particularly benefiting rare classes in imbalanced datasets. The hypothesis is that even simple prompts can produce synthetic data sufficient for training performant models.


## What is the main contribution of this paper?

The main contribution of this paper is experimenting with using large language models like GPT-4 and ChatGPT to generate synthetic training data by augmenting small sets of human-labeled examples. The authors test this approach on three NLP classification tasks of varying complexity within computational social science. They find that even with simple prompts, the synthetic training data can be used to train smaller in-house models that achieve good performance, particularly aiding low-resource settings like identifying rare classes. However, human-annotated data still tends to outperform synthetic data, indicating the need for more research into crafting better prompts to generate more diverse and informative synthetic examples. The key points are:- Using GPT-4 and ChatGPT to augment small labeled datasets with synthetic training data via simple prompts.- Testing this on 3 classification tasks with increasing complexity in computational social science.- Finding synthetic data can train models well, especially helping low-resource cases like rare classes.- Human-annotated data still largely outperforms synthetic data, showing need for more complex prompts.- Demonstrating the potential of LLMs for cheap and fast data augmentation in low-resource domains.So in summary, the main contribution is conducting new experiments on using large language models for data augmentation in low-resource settings via prompt engineering, and analyzing the strengths and weaknesses of this approach compared to human-generated data.
