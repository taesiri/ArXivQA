# [Efficient Controllable Multi-Task Architectures](https://arxiv.org/abs/2308.11744)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we design an efficient multi-task learning (MTL) model that can be dynamically controlled at test time to trade off between task performance, task importance, and computational cost without needing to retrain the model?The key points are:- The goal is to develop a flexible MTL model that can be customized at test time for different user needs and resource constraints without requiring retraining. - The model should allow controlling the tradeoff between computational cost, overall MTL performance, and relative importance of different tasks.- This would allow the same model to be deployed in diverse real-world scenarios with varying computational budgets and task priorities without having to design and retrain separate models.- The two main directions explored are: 1) Training strategies to get a strong yet adaptable MTL model, 2) Efficient search methods to sample sub-architectures matching user constraints.- The proposed method, ECMT, aims to address this by training a single MTL "supernet" that can then be adapted into different "subnetworks" matching specified constraints without retraining.So in summary, the central research goal is developing an efficient and flexible MTL modeling approach that can handle diverse deployment needs after training just once. The key novelty is enabling joint control over computational cost, overall MTL accuracy, and relative task importances when extracting subnetworks from the supernet.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to efficiently train controllable multi-task neural network architectures that can satisfy dynamic user preferences on task performance and compute budget, without needing to retrain the model. Specifically:- They propose an efficient training strategy for a multi-task "supernet" that consists of a shared encoder and task-specific decoders, where both modules have slimmable (adjustable) widths. This allows crafting subnetworks with different capacities after training just once.- A key part of the training is a new "configuration invariant knowledge distillation" loss that transfers knowledge from the high-capacity supernet encoder to the lower-capacity subnetwork encoders. This enhances the performance of the subnetworks.- They propose an evolution-based search algorithm to sample subnetwork architectures that match the user's joint preferences on task performance and compute budget. The search algorithm allocates more capacity to the higher priority tasks' decoders, while finding an optimal shared encoder configuration.- Experiments on multi-task datasets show their method provides higher accuracy and controllability compared to prior methods, while requiring much less computational overhead. For example, it increased the controllability metric by 33.5% on NYUDv2 and 55% on PascalContext over the state-of-the-art.In summary, the key novelty is the ability to efficiently train just once but generate customized subnetworks matching diverse user constraints, saving substantial training costs compared to training specialized models each time. The controllability and accuracy results demonstrate the effectiveness of their proposed training strategy and search algorithm.
