# [Efficient Controllable Multi-Task Architectures](https://arxiv.org/abs/2308.11744)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we design an efficient multi-task learning (MTL) model that can be dynamically controlled at test time to trade off between task performance, task importance, and computational cost without needing to retrain the model?The key points are:- The goal is to develop a flexible MTL model that can be customized at test time for different user needs and resource constraints without requiring retraining. - The model should allow controlling the tradeoff between computational cost, overall MTL performance, and relative importance of different tasks.- This would allow the same model to be deployed in diverse real-world scenarios with varying computational budgets and task priorities without having to design and retrain separate models.- The two main directions explored are: 1) Training strategies to get a strong yet adaptable MTL model, 2) Efficient search methods to sample sub-architectures matching user constraints.- The proposed method, ECMT, aims to address this by training a single MTL "supernet" that can then be adapted into different "subnetworks" matching specified constraints without retraining.So in summary, the central research goal is developing an efficient and flexible MTL modeling approach that can handle diverse deployment needs after training just once. The key novelty is enabling joint control over computational cost, overall MTL accuracy, and relative task importances when extracting subnetworks from the supernet.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to efficiently train controllable multi-task neural network architectures that can satisfy dynamic user preferences on task performance and compute budget, without needing to retrain the model. Specifically:- They propose an efficient training strategy for a multi-task "supernet" that consists of a shared encoder and task-specific decoders, where both modules have slimmable (adjustable) widths. This allows crafting subnetworks with different capacities after training just once.- A key part of the training is a new "configuration invariant knowledge distillation" loss that transfers knowledge from the high-capacity supernet encoder to the lower-capacity subnetwork encoders. This enhances the performance of the subnetworks.- They propose an evolution-based search algorithm to sample subnetwork architectures that match the user's joint preferences on task performance and compute budget. The search algorithm allocates more capacity to the higher priority tasks' decoders, while finding an optimal shared encoder configuration.- Experiments on multi-task datasets show their method provides higher accuracy and controllability compared to prior methods, while requiring much less computational overhead. For example, it increased the controllability metric by 33.5% on NYUDv2 and 55% on PascalContext over the state-of-the-art.In summary, the key novelty is the ability to efficiently train just once but generate customized subnetworks matching diverse user constraints, saving substantial training costs compared to training specialized models each time. The controllability and accuracy results demonstrate the effectiveness of their proposed training strategy and search algorithm.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes an efficient method to train a single multi-task neural network that can be customized at test time for diverse user preferences of task performance and computational budget, without needing to retrain models from scratch.


## How does this paper compare to other research in the same field?

Here are some ways this paper compares to other research in controllable multi-task learning:- Novelty: This paper proposes a new method called Efficient Controllable Multi-Task architectures (ECMT) for sampling efficient sub-architectures from a single trained multi-task model based on user preferences. Prior works have focused on either task preference controllability or compute budget controllability separately. ECMT jointly handles both, allowing more flexible deployment.- Approach: ECMT uses a shared encoder with task-specific decoders, where both are slimmable. It controls task importance by varying decoder capacities and computational cost by adjusting encoder capacity. Prior works like CDMA used separate encoder streams and controlled branching points. ECMT shows this is not needed.- Training strategy: A key novelty is the proposed Configuration Invariant Knowledge Distillation (CI-KD) loss that transfers knowledge from the high-capacity parent model encoder to sub-network encoders. This improves performance of sampled sub-networks. - Search method: ECMT translates user preferences to decoder widths using a simple rule and searches the encoder space using an evolution algorithm. It uses an accuracy predictor for quick evaluation. Prior works relied more on complex hypernetworks.- Performance: Experiments on 3 datasets with diverse tasks show ECMT achieves higher controllability than state-of-the-art CDMA, especially for dense prediction tasks. ECMT shows 33.5% higher hypervolume on NYUD-v2 and 55% on PASCAL Context.- Impact: ECMT allows sampling multiple sub-networks from one trained model based on user constraints, saving expensive retraining costs. This is useful for practical MTL deployment.Overall, ECMT advances controllable MTL research by handling joint user constraints with a simpler yet more effective approach compared to prior arts. The training strategy and search method are particularly novel.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring the proposed ECMT method on transformer-based architectures in addition to CNNs. The paper currently only evaluates ECMT on CNN models like MobileNetV2, ResNet, etc. Applying it to transformers could be an interesting direction.- Incorporating task relativity when extracting the sub-networks from the trained multi-task supernetwork. The authors note that task relativity is an important factor in multi-task learning that their current method does not explicitly consider. Accounting for it could potentially improve the performance of the sampled sub-networks.- Evaluating the approach on a wider range of multi-task problems and datasets beyond the ones explored in the paper (NYUDv2, Pascal Context, CIFAR-100). Assessing how well the method generalizes is an important next step.- Exploring other search strategies beyond the evolution-based approach used in the paper to find optimal sub-network configurations matching the user constraints. The search space could likely be navigated more efficiently.- Reducing the performance gap between the sub-networks sampled from the supernetwork and individually trained specialized networks. There is still often a degradation in going from the specialized networks to the sub-networks, so closing this gap further would be useful.- Extending the framework to continually update and improve the supernetwork as new user constraints and preferences are provided over time. The current approach only trains the supernet once. An online learning approach could be interesting.So in summary, the key directions are expanding the model architectures and tasks evaluated, improving the search efficiency, reducing the performance gap to specialized networks, and allowing for continuous re-training and improvement of the supernetwork over time.
