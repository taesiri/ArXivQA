# [Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination](https://arxiv.org/abs/2403.14401)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-modal large language models (MLLMs) demonstrate impressive capabilities in vision-language tasks but suffer from visual hallucination - generating responses that diverge from the provided image. 

- Prior work has focused on flaws in MLLMs like insufficient visual features, modality gap, biased aggregations, or adherence to language priors. 

- This paper provides a new perspective - MLLMs may not be completely oblivious to accurate visual cues amidst hallucination; rather they seem partially "deceived" by their eyes to support both accurate and fabricated content.

Proposed Solution: 
- Key idea is that images with similar semantics/appearance can induce analogous visual hallucinations. 

- Propose Pensieve - a training-free method where during inference, MLLMs are enabled to retrospect relevant reference images and compare them to the test image.

- This retrospect-then-compare paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input.

- For image captioning, retrieve references with similar semantics/appearance. For VQA, retrieve references that align with the question semantics.

- Contrast confidence scores predicted from references to pinpoint accurate visual cues. Adaptively adjust influence of references.

Key Contributions:
- Empirically show MLLMs are not completely blind to accurate visuals amidst hallucinations - they seem partially deceived by visual input

- Introduce novel Pensieve approach to leverage phenomenon of analogous hallucinations among similar images and discern accurate cues

- Demonstrate Pensieve mitigates visual hallucinations, corrects errors, avoids specious responses, identifies nuanced visual details, and enhances specificity of descriptions

- Validate zero-shot effectiveness on image captioning and VQA tasks, outperforming advanced decoding strategies
