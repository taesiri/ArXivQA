# [An Improved Traditional Chinese Evaluation Suite for Foundation Model](https://arxiv.org/abs/2403.01858)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating large language models (LLMs) in Traditional Chinese are limited in scope and coverage. For example, the TMMLU dataset only has 55 subjects and 3,300 questions.
- There are inconsistencies in existing Traditional Chinese benchmarks regarding question formats and lack of development sets for few-shot learning experiments.
- It is important to develop more comprehensive benchmarks to advance LLMs' understanding and applicability for the linguistically and culturally distinct Traditional Chinese context.

Proposed Solution:
- The authors introduce TMMLU+, an enhanced version of TMMLU with over 22,000 multiple-choice questions covering 66 subjects ranging from primary to professional education levels.
- The dataset has a minimum of 110 questions per subject, includes STEM subjects as well as culturally-relevant topics about Taiwan. 
- The data is split into development, validation and test sets. Questions follow a consistent 4-choice multiple choice format.

Experiments & Results:
- Evaluated 24 Chinese LLMs with parameters ranging from 1.8B to 72B on TMMLU+ using zero-shot and few-shot prompting.
- Simplified Chinese models outperformed Traditional Chinese models despite having comparable parameters. 
- The best performing model was Qwen-72B, achieving 64.3%, but still trails human performance at 68.2% on average.
- Analysis showed translation between Simplified & Traditional Chinese impacts performance. Tokenization efficiency exhibits positive correlation with accuracy.

Main Contributions:
- Creation of TMMLU+, a large-scale, improved Traditional Chinese benchmark for evaluating LLMs' language understanding capabilities 
- Extensive comparative evaluation of 24 existing Chinese language models on the new benchmark
- Analysis providing insights into current limitations of LLMs on Traditional Chinese understanding
- Public release of dataset and benchmark source code to advance progress in this area
