# [An Improved Traditional Chinese Evaluation Suite for Foundation Model](https://arxiv.org/abs/2403.01858)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating large language models (LLMs) in Traditional Chinese are limited in scope and coverage. For example, the TMMLU dataset only has 55 subjects and 3,300 questions.
- There are inconsistencies in existing Traditional Chinese benchmarks regarding question formats and lack of development sets for few-shot learning experiments.
- It is important to develop more comprehensive benchmarks to advance LLMs' understanding and applicability for the linguistically and culturally distinct Traditional Chinese context.

Proposed Solution:
- The authors introduce TMMLU+, an enhanced version of TMMLU with over 22,000 multiple-choice questions covering 66 subjects ranging from primary to professional education levels.
- The dataset has a minimum of 110 questions per subject, includes STEM subjects as well as culturally-relevant topics about Taiwan. 
- The data is split into development, validation and test sets. Questions follow a consistent 4-choice multiple choice format.

Experiments & Results:
- Evaluated 24 Chinese LLMs with parameters ranging from 1.8B to 72B on TMMLU+ using zero-shot and few-shot prompting.
- Simplified Chinese models outperformed Traditional Chinese models despite having comparable parameters. 
- The best performing model was Qwen-72B, achieving 64.3%, but still trails human performance at 68.2% on average.
- Analysis showed translation between Simplified & Traditional Chinese impacts performance. Tokenization efficiency exhibits positive correlation with accuracy.

Main Contributions:
- Creation of TMMLU+, a large-scale, improved Traditional Chinese benchmark for evaluating LLMs' language understanding capabilities 
- Extensive comparative evaluation of 24 existing Chinese language models on the new benchmark
- Analysis providing insights into current limitations of LLMs on Traditional Chinese understanding
- Public release of dataset and benchmark source code to advance progress in this area


## Summarize the paper in one sentence.

 This paper introduces TMMLU+, an enhanced Traditional Chinese benchmark for evaluating large language models' understanding across 66 subjects, with analysis showing Simplified Chinese models outperforming Traditional Chinese models and no models yet surpassing average human performance.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Introducing TMMLU+, an enhanced and more comprehensive benchmark dataset for evaluating large language models' understanding of Traditional Chinese. TMMLU+ contains 22,690 multiple-choice questions across 66 subjects, making it 6 times larger than its predecessor TMMLU.

2. Conducting extensive experiments on 24 open-source and closed-source Chinese language models on TMMLU+, analyzing their performance across different domains, prompting strategies, and model architectures. 

3. Finding that Simplified Chinese models generally outperform Traditional Chinese models, and no model yet matches average human performance. The best models were Qwen-72B and GPT-4, scoring 64.3% and 67.5% accuracy respectively.

4. Analyzing various factors impacting model performance, including tokenizer efficiency, error patterns compared to humans, translation between Traditional and Simplified characters, and the effect of chain-of-thought prompting.

5. Publicly releasing the TMMLU+ dataset and corresponding model evaluation benchmarks to facilitate future research.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it include:

- TMMLU+ - The name of the enhanced Traditional Chinese evaluation dataset introduced in the paper
- Multitask language understanding - Evaluating models on diverse tasks across many subjects/domains
- Traditional Chinese - The focus of the dataset is on Traditional Chinese text
- Multiple-choice QA - The dataset format, consisting of questions with multiple choice answer options
- Language models - Various large language models are benchmarked on the dataset 
- Few-shot learning - Examining model performance with only a few examples
- Simplified Chinese - Comparisons are made between Traditional and Simplified Chinese models
- Tokenization - Analysis on impact of tokenizer vocabulary size and efficiency
- Human baseline - Model results are compared against human performance
- Taiwan - Questions related to Taiwanese culture and geography are included

Does this summary cover the key terms and topics that characterize what this paper is about? Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I have composed about the method proposed in this paper:

1. How does TMMLU+ improve upon previous Traditional Chinese evaluation datasets like TMMLU in terms of size, balance, consistency, and scope? What specific limitations does it aim to address?

2. What was the process for collecting, cleaning, formatting, and splitting the 22,690 questions across 66 subjects in TMMLU+? How much effort was involved and what were some key quality assurance steps?  

3. What are the key differences you observed in the performance of Simplified Chinese versus Traditional Chinese language models on TMMLU+? How do you account for these differences based on factors like tokenizer design and efficiency of parameter utilization?

4. You conducted an interesting analysis correlating tokenizer fertility scores and downstream performance on TMMLU+. Can you further elaborate on why a tokenizer's fertility score seems crucial for multilingual benchmarks? What might be the underlying reasons?

5. How exactly did you categorize the questions in TMMLU+ into easy, medium and hard for your analysis of error patterns between humans and AI models? What methodology did you use to compute the thresholds?

6. The CoT prompting method did not lead to improved performance on TMMLU+ as expected - why do you think this occurred? Are CoT prompts incompatible with certain question structures or dataset characteristics?  

7. You have a lot of proprietary models like Claude and Gemini in your evaluations - can you discuss if and how much their undisclosed training methodologies might give them an edge over open-source models?

8. Can you think of ways to expand the human baseline data coverage in TMMLU+? What strategies could help in sourcing more ground truth data across all subjects?

9. How amenable are the TMMLU+ Traditional Chinese questions to automatic conversion into Simplified Chinese using existing tools? What are some linguistic nuances that might be lost in translation?

10. If you had access to a 100B+ parameter Traditional Chinese model, what types of specialized pretraining techniques would you recommend to maximize its performance on datasets like TMMLU+?
