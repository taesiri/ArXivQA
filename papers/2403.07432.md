# [Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for   Scene Flow](https://arxiv.org/abs/2403.07432)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing scene flow methods rely on single RGB or LiDAR modality, which have degraded visual features due to limitations like low dynamic range (RGB) or incomplete structure (LiDAR). This deteriorates motion features.
- Multimodal fusion methods directly fuse RGB and LiDAR features but suffer from modality gap due to their heterogeneous nature, also deteriorating motion features.

Proposed Solution: 
- Introduce event camera as a bridge between RGB and LiDAR since it has complementary nature with both in visual (luminance, structure) and motion (correlation) spaces.
- Propose hierarchical visual-motion fusion framework VisMoFlow to fuse cross-modal knowledge in homogeneous spaces:
  - Visual Luminance Fusion: Fuse relative luminance of event and absolute luminance of RGB for high dynamic range imaging.
  - Visual Structure Fusion: Fuse local boundary of event and global shape of LiDAR for structure integrity.
  - Motion Correlation Fusion: Fuse spatial-dense correlation of RGB, temporal-dense correlation of event and sparse correlation of LiDAR for motion continuity.

Main Contributions:
- First to bring event camera as a bridge between RGB and LiDAR and propose hierarchical fusion in homogeneous spaces for interpretable cross-modal fusion. 
- Discover complementarity of event with RGB and LiDAR in both visual (luminance, structure) and motion (correlation) spaces.
- Achieve state-of-the-art performance on day and night scene flow estimation by progressive fusion from visual space to motion space.
