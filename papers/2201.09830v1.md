# [Learning Graph Augmentations to Learn Graph Representations](https://arxiv.org/abs/2201.09830v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question/hypothesis seems to be:

Can an end-to-end framework be developed to automatically learn graph augmentations and graph representations in a joint manner, without requiring manual tuning or a bi-level optimization? 

The paper proposes a framework called LG2AR that aims to address this question. The key ideas are:

- Using a probabilistic policy module to learn a conditional distribution over possible graph augmentations based on the dataset. This automates the augmentation selection process.

- Using probabilistic augmentation heads to learn distributions over the parameters of each augmentation type, instead of hand-picking them. This allows learning better dataset-specific augmentations. 

- Training the policy, augmentations, and base encoder end-to-end with a contrastive loss, without needing a nested bi-level optimization. 

- Showing that this framework can be used for both node-level and graph-level representation learning tasks.

So in summary, the central hypothesis is that jointly learning the augmentations and representations in this automated, end-to-end manner will lead to improved graph representation learning, compared to relying on hand-designed or randomly selected augmentations. The results on various benchmarks seem to confirm this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proposing an end-to-end framework called LG2AR for learning graph augmentations and graph representations. Specifically:

- It introduces a probabilistic augmentation selection policy that learns to sample augmentations conditioned on the dataset. This automates the combinatorial augmentation selection process.

- It introduces probabilistic augmentation heads where each head learns a distribution over the parameters of a specific augmentation type. This allows learning better augmentations adapted to each dataset. 

- The policy and augmentation heads are trained end-to-end along with the graph encoder, without needing a separate outer-loop optimization.

- It can be used for both node-level and graph-level representation learning, achieving state-of-the-art results on several benchmarks under both settings.

So in summary, the main contribution is an end-to-end framework to automatically learn graph augmentations and representations tailored to each dataset, replacing hand-designed heuristics and trial-and-error processes. The automation, general applicability to node and graph tasks, and strong empirical results are the key aspects that differentiate it from prior work.
