# [Improving Black-box Robustness with In-Context Rewriting](https://arxiv.org/abs/2402.08225)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Machine learning models often perform well on in-distribution (ID) data seen during training, but struggle to generalize to out-of-distribution (OOD) data. Most techniques for improving OOD robustness require access to model weights or OOD labels. However, in many real-world settings, the model is a black box (e.g. weights are frozen, costly to retrain, abstracted away). Thus, there is a need for techniques to improve OOD robustness of black-box NLP models.  

Proposed Solution:
The paper proposes using a large language model (LLM) to generate augmentations of the test input, which are then fed to the task model to get predictions that are aggregated - a method called LLM test-time augmentation (LLM-TTA). Specifically, two LLM augmentation methods are explored: 1) zero-shot paraphrasing to rephrase the input, and 2) in-context rewriting (ICR) to rewrite the input to be more like the ID data distribution seen by the task model.

Contributions:
- Show LLM-TTA improves OOD accuracy of BERT by 4.3 percentage points on average across sentiment, toxicity and news classification tasks without regressing ID performance. LLM-TTA outperforms conventional augmentation techniques like back-translation, word insertion and substitution.
- Demonstrate LLM-TTA works well across low and high-resource settings by training on subsets of data. ICR tends to work better than paraphrasing.
- Propose selective augmentation based on prediction entropy to reduce expensive LLM calls. Can get most of the gains while augmenting 57.76% fewer examples.
- Analysis shows TTA tends to benefit some classes more than others, and optimal number of augmentations varies per task.
- Provide datasets, models and code for reproducibility.

In summary, the paper shows LLM-TTA is an effective black-box technique for improving OOD robustness of NLP models, with potential for greater robustness gains by generating better augmentations that close the domain gap.


## Summarize the paper in one sentence.

 This paper proposes using large language models to generate augmentations of text inputs at test time, in order to improve the robustness of text classification models to out-of-distribution data, without requiring access to model weights or out-of-distribution labels.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing and evaluating LLM-TTA, a method for using large language models to generate augmentations at test time to improve the robustness of text classification models to out-of-distribution data. Specifically, the key contributions are:

1) Proposing two methods for using LLMs to generate augmentations: zero-shot paraphrasing and in-context rewriting (ICR), where the LLM rewrites inputs to be more like in-distribution examples. 

2) Evaluating LLM-TTA across sentiment analysis, toxicity detection, and news classification tasks and models. The results show that LLM-TTA, especially ICR, improves robustness to OOD data for BERT and T5 models, with gains of 4-7 percentage points on average.

3) Proposing an entropy-based selective augmentation method to reduce the rate of expensive LLM augmentations by only augmenting uncertain inputs. This maintains most of the robustness gains while reducing augmentation rate by 57% on average.

4) Analysis showing LLM-TTA improves robustness across low and high resource settings, affects some classes more than others, and the optimal number of augmentations varies per task.

In summary, the main contribution is presenting LLM-TTA as an effective black-box technique for improving model robustness to OOD data across tasks without modifying the model or requiring new labels. The analysis also provides insights into the approach.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts include:

- Out-of-distribution (OOD) robustness - The paper focuses on improving model performance on unseen or out-of-distribution data samples. This is an important challenge in deploying machine learning models to real-world settings.

- Black-box robustness techniques - The methods explored aim to improve robustness without accessing or retraining the original "task" model. This makes them applicable when the model is a black box. 

- Test-time augmentation (TTA) - A technique that aggregates predictions across augmented versions of the test input. The choice of augmentation function is critical.

- Large language models (LLMs) - LLMs are used to generate augmentations of text inputs. Their strong generative capabilities enable higher quality augmentations compared to traditional approaches. 

- In-Context Rewriting (ICR) - An LLM-TTA approach that rewrites OOD examples to be more like in-distribution examples provided in the prompt context.

- Selective augmentation - A proposed efficiency improvement where only uncertain test inputs are augmented based on prediction entropy.

- Sentiment analysis, toxicity detection, news classification - The text classification tasks and datasets used to benchmark performance.

In summary, the key focus is using LLMs at test time to rewrite or augment OOD text inputs to improve model robustness to distribution shifts, applicable even when the model is a black box.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does LLM-TTA compare to other test-time robustness techniques like test-time adaptation? What are the tradeoffs between these methods in terms of performance gains, compute requirements, and ease of implementation?

2. The paper studies LLM-generated paraphrasing and in-context rewriting as the augmentation functions. What other types of LLM-generated augmentations could be effective for TTA? For example, could conditional generation constrained to preserve specific attributes be useful?

3. For the in-context rewriting augmentation, what is the ideal number and selection of in-distribution examples to provide in the prompt? How does the performance vary based on these prompt design choices? 

4. The paper proposes an entropy-based selective augmentation technique to reduce expensive LLM calls. What other signals or metrics could be effective for determining when an input would likely benefit from augmentation?

5. How well does LLM-TTA transfer across different task modalities? For example, could the approach be effective for document classification or sequence labeling tasks?

6. Could LLM-TTA be improved by dynamically weighting augmentations in the aggregation stage based on confidence scores? What method would be most effective for assigning these weights?

7. The paper studies LLM-TTA in a black-box setting on frozen task model weights. How well does LLM-TTA combine with test-time adaptation methods that update task model weights?

8. For real-world deployment, what strategies could reduce the latency of generating multiple LLM augmentations per input to enable low-latency predictions?

9. The paper focuses evaluation on accuracy metrics. How does LLM-TTA affect other model evaluation metrics like uncertainty calibration and OOD detection performance?

10. What modifications or improvements to LLM-TTA would be required to make the approach effective for long-form text classification tasks? How do the design tradeoffs differ from short-text settings?
