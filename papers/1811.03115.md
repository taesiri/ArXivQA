# [Blockwise Parallel Decoding for Deep Autoregressive Models](https://arxiv.org/abs/1811.03115)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions/hypotheses addressed in this paper are:1) Can we improve the decoding speed of deep autoregressive sequence models by making predictions for multiple future tokens in parallel? 2) Will making parallel predictions followed by backtracking lead to the same final greedy decoding output as standard sequential greedy decoding?3) How much speedup can we get with this blockwise parallel decoding approach before quality degrades too much?4) Can we further improve the decoding speed by using approximate inference techniques during the verification step?5) Will knowledge distillation and fine-tuning help improve the accuracy of the auxiliary prediction models and lead to larger accepted block sizes?The main proposal is blockwise parallel decoding, where multiple future tokens are predicted independently in parallel, then the longest matching prefix is verified and accepted. The hypotheses are that this can speed up decoding iteration count, maintain output quality, and be further improved via techniques like distillation and approximation. The authors perform experiments on machine translation and image super-resolution to validate the approach.In summary, the core focus is on faster decoding for autoregressive models by exploiting parallelism while maintaining output quality as much as possible. The blockwise parallel prediction approach is the key idea that is proposed and evaluated.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a blockwise parallel decoding scheme for deep autoregressive models. Specifically:- They train auxiliary models to make predictions for multiple future output tokens in parallel. - At test time, they use these proposal models to independently predict the next several tokens in parallel.  - They determine the longest prefix of these predictions that would have been generated greedily using a scoring model. - If this prefix is longer than one token, they can skip ahead that many steps instead of decoding token-by-token.This allows them to reduce the number of sequential decoding iterations, thereby accelerating generation for neural autoregressive models with parallel computation capabilities like the Transformer. They show improvements in decoding speed with minimal impact on accuracy for machine translation and image super-resolution tasks.The key ideas are exploiting model parallelism during decoding by predicting multiple future tokens, verifying only the longest valid prefix, and accepting this prefix to skip ahead when possible. This contributes a simple but effective approach for faster decoding in certain types of autoregressive models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes a blockwise parallel decoding scheme for autoregressive models like the Transformer, where multiple future outputs are predicted in parallel then verified against the base model to skip decoding iterations, achieving faster inference speeds with minimal impact on quality.


## How does this paper compare to other research in the same field?

 Here are some key points about how this paper compares to other related work on accelerating inference for autoregressive sequence models:- It proposes a simple and generic algorithmic technique called blockwise parallel decoding that can be applied to many existing model architectures like Transformers. This makes it easy to integrate with prior work. In contrast, some other approaches like non-autoregressive models require more significant architectural changes.- The technique achieves speedups of up to 3-4x in wall clock time with minimal loss in accuracy for machine translation. This is a better tradeoff than prior work like non-autoregressive Transformers, which sacrificed a lot of accuracy for speed. Iterative refinement also achieves comparable speedups but requires more decoding passes.- For image super-resolution, it achieves up to 7x speedup in iterations at a slight cost in perceptual quality. This demonstrates the technique's effectiveness for dense prediction tasks beyond just language.- It explores different training strategies like knowledge distillation and fine-tuning the base model parameters. Distillation in particular helps maintain accuracy at higher speeds.- It studies the effect of approximate decoding techniques like top-k pruning and distance-based matching. These provide additional speedups by relaxing the decoding constraints.- The paper provides both algorithmic insights about block parallel decoding as well as empirical results on strong baselines. The code is open-sourced for easy replication too.Overall, this paper pushes the state of the art in fast inference for autoregressive models by proposing a straightforward yet well-studied approach. The results are strong compared to prior work, and the method can complement many existing techniques. The analysis also sheds light on the tradeoffs between accuracy, speed, and model design.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions suggested by the authors include:- Exploring combinations of blockwise parallel decoding with other techniques like discrete latent variable models. The authors mention this could be a promising direction for further speeding up decoding.- Applying the method to other tasks and architectures beyond machine translation and image super-resolution. The authors frame their approach as being generic and widely applicable.- Investigating different approximation techniques during the verification step. The paper explores top-k selection and distance-based matching, but other heuristics could be possible. - Optimizing the tradeoff between computation cost and accuracy as block size increases. Larger block sizes improve iteration count but slow down per-iteration speed.- Reducing the training memory requirements to enable larger block sizes. The authors had to limit training to a random sample of outputs due to memory constraints.- Experimenting with different distillation setups and training procedures. The paper finds distillation helps, but more work could be done to optimize this.In summary, the main future directions focus on broadening the applicability of the method to other domains, combining it with complementary techniques, finding better approximations to enable larger speedups, and reducing training memory requirements. The authors frame this as an open area with much room for further work.


## Summarize the paper in one paragraph.

 The paper proposes a blockwise parallel decoding method for improving the generation speed of deep autoregressive models. The key idea is to train auxiliary models to make predictions for multiple future tokens in parallel, then verify the longest matching prefix using the base model in parallel, and finally accept this prefix to skip decoding iterations. For machine translation experiments using the Transformer, this method achieves a 3x speedup over greedy decoding with minimal loss in BLEU score. For image super-resolution using the Image Transformer, it obtains a 4x speedup while maintaining similar perceptual quality based on human evaluation. The method can be easily implemented on existing models and combines well with distillation and approximate decoding techniques. Overall, it enables faster parallel decoding for sequential models without architectural changes.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a novel blockwise parallel decoding scheme for improving the inference speed of deep autoregressive sequence models like the Transformer. The key idea is to train auxiliary models to make predictions for multiple future tokens in parallel, then verify the longest prefix of these predictions that would be generated greedily by scoring the sequence in parallel. This allows the decoder to potentially skip over multiple time steps at once during iterative greedy decoding. The authors implement this approach by adding a small modification to the Transformer architecture. They demonstrate its effectiveness on machine translation and image super-resolution tasks. Using distillation and approximate decoding, they achieve real-time speedups of over 3x on translation and 4x on super-resolution with minimal loss in quality.The authors first explain the standard greedy decoding scheme and then introduce their blockwise parallel decoding algorithm. They describe a combined scoring and proposal model implementation based on the Transformer, along with training procedures involving knowledge distillation and fine tuning. Experiments on WMT English-German translation verify that the algorithm can reduce decoding iterations by nearly 2x with no loss in BLEU score. The improvements increase to over 4x slower decoding iterations with approximate decoding, corresponding to a 3x wall-clock speedup. Similar speedups are shown for image super-resolution while maintaining output quality. The approach is compared favorably to related work on non-autoregressive and iterative refinement techniques.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a blockwise parallel decoding method for deep autoregressive models to accelerate generation speed. The key idea is to train auxiliary models to make independent predictions for multiple future tokens in parallel. The longest prefix of these predictions that matches the scoring distribution of the main model is then accepted and appended to the output sequence. This allows the decoder to potentially skip over multiple time steps at once, reducing the number of sequential operations required. The method is applied to Transformer models for machine translation and image super-resolution. By training on distilled data and allowing approximate matching, speedups of over 3x in wall-clock time are demonstrated with minimal impact on output quality. The proposed approach provides a simple way to leverage parallel computation in decoding that is complementary to other techniques for accelerating generation in autoregressive models.


## What problem or question is the paper addressing?

 The paper is addressing the problem of slow autoregressive decoding in sequence generation models like neural machine translation systems. Autoregressive models generate output sequences one token at a time, which limits the speed of decoding. This is an issue for real-time applications.The main question the paper seeks to answer is: how can we accelerate decoding in autoregressive sequence models while maintaining high output quality?The key ideas and contributions are:- Proposing a blockwise parallel decoding algorithm that makes multiple forward predictions then verifies them, allowing the model to skip ahead during decoding. This reduces the number of required decoding iterations.- Implementing this with Transformer models by adding a multi-output layer and training auxiliary models to make predictions for multiple future tokens.- Evaluating the approach on machine translation and image super-resolution. For translation they achieve up to 3x speedup with minimal loss in BLEU. For super-resolution they achieve up to 7x iteration reduction with comparable image quality.- Analyzing the tradeoffs between speed, quality, and model variations like distillation and fine-tuning.In summary, the paper addresses the problem of slow decoding in autoregressive models by developing a parallel decoding algorithm and demonstrating significant speedups on generation tasks. The key innovation is making and verifying multiple predictions in parallel during each step.
