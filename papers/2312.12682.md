# [Mini-GPTs: Efficient Large Language Models through Contextual Pruning](https://arxiv.org/abs/2312.12682)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-4 have immense capabilities for natural language processing but also have impractically large computational demands in terms of cost, latency, emissions, and cloud dependence. 
- There is a need for model optimization techniques to address these challenges while retaining strong performance.

Proposed Solution:  
- The paper introduces a novel approach of developing "Mini-GPTs" - smaller yet efficient versions of existing LLMs - through contextual pruning. 
- Contextual pruning strategically removes less critical weights in the computational architecture of LLMs based on analysis of neuron importance across different domains.

Main Contributions:
- Applies contextual pruning focused on linear, activation and embedding layers to reduce model sizes of pre-trained LLMs like Phi-1.5, Opt-1.3 and Llama-1.3.
- Tests approach on diverse domains (US law, medical, games, translation, economics) and datasets to evaluate model optimization.
- Achieves high reduction in model sizes (up to 40%) while maintaining or improving domain-specific perplexity and accuracy on multiple choice questions.
- Demonstrates the practical viability of contextual pruning for building specialized, efficient LLMs for real-world applications, complementing other optimization techniques.
- Discusses future work of applying this methodology to larger datasets, newer models, and integrating it with other methods like quantization.

In summary, the paper puts forth the novel concept of Mini-GPTs created through contextual pruning of less useful weights in LLMs for specific domains. The experiments underscore the promise of this approach for developing performant yet highly resource-efficient domain-specific LLMs.
