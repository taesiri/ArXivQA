# [IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral   Evolution History](https://arxiv.org/abs/2403.16075)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Traditional imitation learning (IL) methods assume the expert demonstrations come from an experienced, fixed policy. However, in many streaming applications like recommender systems, the expert policy evolves from novice to experienced over time. This evolution history contains valuable information but also contradictions that violate the assumptions of IL methods. There is a need for methods that can effectively learn from the expert's full behavioral evolution without requiring reward feedback.

Proposed Solution:
This paper proposes the Inverse Batched Contextual Bandit (IBCB) framework to address this problem. IBCB works under the batched contextual bandit setting where the expert updates its policy in batches over episodes. It formulates the inverse learning problem as a quadratic programming problem with linear constraints derived from the bandit policy structure and execution history. This allows IBCB to simultaneously recover both the reward and policy parameters of the evolving expert. 

Key Contributions:

- Defines a new problem of imitating expert policies that evolve over time from novice to experienced
- Introduces the IBCB framework for deterministic and randomized batched bandit expert policies 
- Formulates a simple quadratic program to enable sample-efficient learning without reward feedback
- Unified approach that recovers both reward and policy parameters
- Outperforms existing IL methods on synthetic and real-world data
- Robust to out-of-distribution and contradictory expert demonstration data
- Significantly faster training compared to sampling-based inverse RL methods

The key insight is to leverage the bandit policy structure to constrain the inverse learning problem, avoiding inefficient sampling methods. By modeling the full evolution of the expert, IBCB can imitate both novice and experienced policies and generalize effectively. The experimental results validate IBCB's ability to efficiently learn complex streaming expert behaviors.
