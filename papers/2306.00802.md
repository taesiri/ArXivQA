# [Birth of a Transformer: A Memory Viewpoint](https://arxiv.org/abs/2306.00802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How do transformers develop the ability to do in-context learning during training? In particular, how do they balance using global knowledge stored during training vs dynamically adapting to new information provided in the specific context?The key hypotheses explored are:- Transformers store a lot of generic knowledge about language statistics, syntax, etc. in their parameters during training. This acts as a global statistical model.- The transformer architecture, especially mechanisms like the multi-head self-attention, also allows them to rapidly adapt to new information provided in the specific context or prompt. This enables in-context learning. - There is an interplay between these two types of knowledge. The paper tries to understand this balance by studying the training dynamics of transformers in a simplified setup.Specifically, the paper introduces a synthetic dataset where sequences are generated using both generic bigrams (global knowledge) as well as context-specific bigrams (in-context). The hypothesis is that the model will learn global statistics faster, while in-context learning requires developing inductive biases like the "induction head" mechanism. By probing the training dynamics, the paper aims to test this hypothesis and shed light on how both types of knowledge emerge.
