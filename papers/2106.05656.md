# [MST: Masked Self-Supervised Transformer for Visual Representation](https://arxiv.org/abs/2106.05656)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we design an effective self-supervised learning method for visual representation based on Transformer architectures, that captures both global semantics as well as local context?

The key hypotheses explored in this paper are:

1) Using an attention-guided mask strategy instead of random masking can help preserve crucial spatial structures and avoid suppressing the model's ability to recognize objects.

2) Reconstructing the full image instead of just predicting the masked patches can encourage learning useful representations and be more amenable to transfer learning on dense prediction tasks. 

3) Combining the attention-guided masking with full image reconstruction in a novel Masked Self-Supervised Transformer (MST) framework can lead to visual representations that capture both global semantics and local context, outperforming previous methods.

In summary, the central research question is how to develop an improved self-supervised visual representation learning approach using Transformers. The key hypotheses are around using attention-guided masking and full image reconstruction in a novel framework called MST. The experiments aim to validate that MST can outperform previous methods by learning representations that combine global and local information.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new self-supervised learning method for visual representations called Masked Self-supervised Transformer (MST). 

2. It introduces an attention-guided mask strategy to selectively mask patches/tokens based on a multi-head self-attention map from the teacher network. This allows capturing local context while preserving global semantics.

3. It uses a global image decoder to reconstruct the original image from the masked and unmasked tokens. This helps recover spatial information and is beneficial for dense prediction tasks like detection and segmentation.

4. Extensive experiments show MST achieves state-of-the-art performance on ImageNet classification. It also transfers well to downstream tasks like detection on COCO and segmentation on Cityscapes.

In summary, the key contribution is the proposed MST approach combining an attention-guided masking strategy and global image reconstruction to learn useful visual representations in a self-supervised manner. The results demonstrate its effectiveness and versatility for both image classification and dense prediction tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The key points of the paper are:

1) The paper proposes a new self-supervised learning method called Masked Self-supervised Transformer (MST) for visual representation learning. 

2) MST introduces an attention-guided mask strategy to dynamically mask patches based on the self-attention map from the teacher network. This can focus on local context while preserving global semantics.

3) MST uses a global image decoder after masking to recover the original image. This helps retain spatial information and transfer better to dense prediction tasks.

4) Experiments show MST achieves state-of-the-art results on ImageNet classification and also transfers well to object detection and segmentation.

In one sentence: MST is a self-supervised visual representation learning method that uses an attention-guided masking strategy and image reconstruction to capture both local details and global semantics.
