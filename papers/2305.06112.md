# [The Compositional Structure of Bayesian Inference](https://arxiv.org/abs/2305.06112)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to formalize and make functorial the concept of Bayesian inversion of Markov kernels in the abstract setting of Markov categories. The key contributions and findings are:- It introduces the notion of Bayesian lenses, which are pairs of a Markov kernel and an indexed family of "backward" kernels representing Bayesian inverses. These have a similar structure to lenses from database theory.- It shows that Bayesian inversion can be made into a functor from a Markov category to the category of Bayesian lenses, but only up to "almost sure equality" due to the ambiguity in Bayesian inversion at points of zero probability.- It introduces the use of support objects to pick out canonical Bayesian inverses and resolve the ambiguity, leading to an exact functorial inversion using dependent Bayesian lenses.- It provides examples to illustrate the compositional nature of Bayesian inversion, such as inverting state trace kernels of a Markov chain.- It relates the construction to concepts from dependent type theory like the families fibration, and discuss connections to differential programming and probabilistic programming.In summary, the paper gives a categorical semantics for Bayesian inversion of stochastic kernels that makes precise the intuitive notion of "chain rule for Bayesian updating", while relating it to other cybernetic processes like backpropagation. The technical constructions lead towards applications in probabilistic programming and compositional inference.


## What is the main contribution of this paper?

This paper presents a categorical framework for modeling Bayesian inversion and relating it to lens structures. The main contributions are:- It defines Bayesian inversion abstractly in terms of morphisms in a Markov category satisfying an equation relating to Bayes' theorem. This provides a general, axiomatic treatment of Bayesian inversion.- It shows that Bayesian inverses are only defined up to "almost-sure equality", meaning they are not uniquely defined on parts of the domain with zero probability under the prior. This makes functoriality of Bayesian inversion tricky. - To resolve this, it introduces the idea of "support objects" representing the support of a distribution. Bayesian inverses can then be defined between support objects, giving a unique representative.- Bayesian inversion is shown to have a similar compositional structure to backpropagation and reverse-mode automatic differentiation. This is formalized by constructing categories of "Bayesian lenses" where morphisms have both a forward and backward/inverse component. - Bayesian lenses have a similar structure to lens data types from programming. This provides a link between concepts from probability, programming languages, and category theory.- An extension to "dependent Bayesian lenses" is introduced, giving extra structure for relating inverses to priors. This also has links to interpretations of dependent type theory.- An application is shown where Bayesian inversion is used to learn Markov chain transition probabilities from observed state sequences in a compositional way.In summary, the main contribution is presenting Bayesian inversion abstractly in terms of Markov categories and lenses, elucidating its compositional properties and relationships to concepts in programming languages and type theory. The categorical perspective provides a unifying lens on these connections.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper develops a categorical framework for Bayesian inversion of stochastic kernels, showing how Bayesian updates can be composed optically using the notion of lenses from functional programming.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper takes a categorical perspective on Bayesian inversion and compositionality. This is a relatively novel approach compared to more traditional statistical treatments of Bayesian inference. The use of string diagrams and Markov categories provides an intuitive graphical calculus for reasoning about Bayesian updates.- The idea of using lenses/optics for Bayesian inference seems to be new. Lenses have been studied for deterministic state updates, but applying the idea for probabilistic state updating is novel. This provides a way to make Bayesian inversion functorial.- The paper relates Bayesian inversion to other forms of "cybernetic" bidirectional computation like backpropagation and reinforcement learning. Making these connections is innovative compared to papers that study Bayesian methods in isolation.- The construction of the category of "dependent Bayesian lenses" using indexed categories seems original. This allows supporting objects like the distributions themselves to depend on other distributions. This is more general than previous categorical approaches.- The paper discusses representing zero-probability events using dependent types. This connects Bayesian inversion to languages with dependent types for ruling out illegal states. This perspective relating typing and probability seems new.- Overall, the paper brings together ideas from probability, category theory, programming languages, and other fields. Integrating these perspectives to develop new foundations for probabilistic programming and compositional inference differentiates it from more narrow statistical treatments.In summary, the categorical viewpoint, use of lenses, and connections to other fields make the techniques stand out as innovative compared to other research on Bayesian inference and compositionality. The unifying perspective is a strength.
