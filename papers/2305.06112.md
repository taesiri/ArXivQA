# [The Compositional Structure of Bayesian Inference](https://arxiv.org/abs/2305.06112)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how to formalize and make functorial the concept of Bayesian inversion of Markov kernels in the abstract setting of Markov categories. 

The key contributions and findings are:

- It introduces the notion of Bayesian lenses, which are pairs of a Markov kernel and an indexed family of "backward" kernels representing Bayesian inverses. These have a similar structure to lenses from database theory.

- It shows that Bayesian inversion can be made into a functor from a Markov category to the category of Bayesian lenses, but only up to "almost sure equality" due to the ambiguity in Bayesian inversion at points of zero probability.

- It introduces the use of support objects to pick out canonical Bayesian inverses and resolve the ambiguity, leading to an exact functorial inversion using dependent Bayesian lenses.

- It provides examples to illustrate the compositional nature of Bayesian inversion, such as inverting state trace kernels of a Markov chain.

- It relates the construction to concepts from dependent type theory like the families fibration, and discuss connections to differential programming and probabilistic programming.

In summary, the paper gives a categorical semantics for Bayesian inversion of stochastic kernels that makes precise the intuitive notion of "chain rule for Bayesian updating", while relating it to other cybernetic processes like backpropagation. The technical constructions lead towards applications in probabilistic programming and compositional inference.


## What is the main contribution of this paper?

 This paper presents a categorical framework for modeling Bayesian inversion and relating it to lens structures. The main contributions are:

- It defines Bayesian inversion abstractly in terms of morphisms in a Markov category satisfying an equation relating to Bayes' theorem. This provides a general, axiomatic treatment of Bayesian inversion.

- It shows that Bayesian inverses are only defined up to "almost-sure equality", meaning they are not uniquely defined on parts of the domain with zero probability under the prior. This makes functoriality of Bayesian inversion tricky. 

- To resolve this, it introduces the idea of "support objects" representing the support of a distribution. Bayesian inverses can then be defined between support objects, giving a unique representative.

- Bayesian inversion is shown to have a similar compositional structure to backpropagation and reverse-mode automatic differentiation. This is formalized by constructing categories of "Bayesian lenses" where morphisms have both a forward and backward/inverse component. 

- Bayesian lenses have a similar structure to lens data types from programming. This provides a link between concepts from probability, programming languages, and category theory.

- An extension to "dependent Bayesian lenses" is introduced, giving extra structure for relating inverses to priors. This also has links to interpretations of dependent type theory.

- An application is shown where Bayesian inversion is used to learn Markov chain transition probabilities from observed state sequences in a compositional way.

In summary, the main contribution is presenting Bayesian inversion abstractly in terms of Markov categories and lenses, elucidating its compositional properties and relationships to concepts in programming languages and type theory. The categorical perspective provides a unifying lens on these connections.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper develops a categorical framework for Bayesian inversion of stochastic kernels, showing how Bayesian updates can be composed optically using the notion of lenses from functional programming.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related research:

- The paper takes a categorical perspective on Bayesian inversion and compositionality. This is a relatively novel approach compared to more traditional statistical treatments of Bayesian inference. The use of string diagrams and Markov categories provides an intuitive graphical calculus for reasoning about Bayesian updates.

- The idea of using lenses/optics for Bayesian inference seems to be new. Lenses have been studied for deterministic state updates, but applying the idea for probabilistic state updating is novel. This provides a way to make Bayesian inversion functorial.

- The paper relates Bayesian inversion to other forms of "cybernetic" bidirectional computation like backpropagation and reinforcement learning. Making these connections is innovative compared to papers that study Bayesian methods in isolation.

- The construction of the category of "dependent Bayesian lenses" using indexed categories seems original. This allows supporting objects like the distributions themselves to depend on other distributions. This is more general than previous categorical approaches.

- The paper discusses representing zero-probability events using dependent types. This connects Bayesian inversion to languages with dependent types for ruling out illegal states. This perspective relating typing and probability seems new.

- Overall, the paper brings together ideas from probability, category theory, programming languages, and other fields. Integrating these perspectives to develop new foundations for probabilistic programming and compositional inference differentiates it from more narrow statistical treatments.

In summary, the categorical viewpoint, use of lenses, and connections to other fields make the techniques stand out as innovative compared to other research on Bayesian inference and compositionality. The unifying perspective is a strength.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing a new approach to probabilistic programming that incorporates Bayesian and differential updating, building on the use of lenses. They suggest this could lead to improvements in compositionality of inference and the ability to use dependent typing to rule out zero-probability events.

- Exploring the use of approximate Bayesian inverses represented as lenses, with associated loss functions that compose according to the lens structure. This is suggested as a way to develop algorithms for approximate inference that are "correct by construction."

- Connecting the lens structure to other related ideas like backpropagation and predictive coding in neural networks, backward induction in reinforcement learning, etc. The goal would be to build on these relationships to develop intelligent systems that are efficient and well-understood.

- Investigating whether the construction of dependent Bayesian lenses using support objects could be extended beyond Markov categories that have support objects, perhaps using a hybrid approach with the category ProbStoch.

- Further exploring the monoidal structure and how Bayesian inversion interacts with the monoidal product in Markov categories.

- Applying the conceptual framework developed to additional practical examples and case studies to further validate and refine the theoretical ideas.

In summary, the main directions are: applications to probabilistic programming and approximate inference, relating the framework to other paradigms like backpropagation and predictive coding, generalizing the theoretical construction, and further practical investigations. The overall goal is to develop more efficient, correct-by-construction algorithms while also elucidating conceptual connections between different approaches to inference and learning.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper studies Bayesian inversion of Markov kernels in terms of categorical constructs. It introduces the notion of a Bayesian lens, which consists of a kernel paired with an associated "backward" kernel representing its Bayesian inverse. The paper shows how to construct categories of Bayesian lenses where Bayesian inversion becomes functorial. It also introduces dependent Bayesian lenses, which use the notion of support objects to make Bayesian inversion more canonical. The constructions allow representing inverse models compositionally in terms of basic components. An example is provided of using this framework to model inference of Markov chain parameters from observed state sequences. Overall, the categorical viewpoint provides formal underpinnings for the compositional nature of Bayesian inversion and enables new approaches to probabilistic programming and approximate inference.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper examines Bayesian inversion of Markov kernels (stochastic mappings) from a categorical perspective. It defines a Bayesian inverse of a kernel f:X→Y with respect to a prior state p:I→X as a kernel f':Y→X satisfying a certain compositional equation relating f, f', p and the copy and delete maps. The definition only determines f' up to almost sure equality. The authors construct categories of "Bayesian lenses" to represent state-indexed families of Bayesian inverses as single morphisms, embracing the dependency of the inverse on the prior. Bayesian inversion is shown to be almost functorial in these categories. By moving to "dependent Bayesian lenses", allowing support-dependent indexing, and assuming existence of support objects, the construction can be made strictly functorial.

As an application, the paper shows how Bayesian inference problems like learning transition probabilities in a Markov chain can be viewed compositionally, with composite observables built from basic kernels. Their Bayesian inverses are then constructed functorially from inverses of the components. This structural perspective is argued to give additional pedagogical and algorithmic insight compared to traditional derivations. The compatibility with dependent types also suggests connections to using type systems to rule out zero probability events.

In summary, the paper gives a new categorical perspective on Bayesian inversion, constructs categories of lenses where inversion behaves functorially, and argues this supports an improved understanding of compositionality in Bayesian inference algorithms and connections to programming language techniques.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces the concept of Bayesian lenses, which are pairs of a Markov kernel along with an associated "Bayesian inverter" that inverts the kernel according to Bayes' law. These Bayesian lenses are constructed categorically, by first defining an indexed category that represents families of kernels indexed by prior distributions. Taking the Grothendieck construction of this indexed category yields the category of Bayesian lenses, whose morphisms represent the Bayesian inversion operation. The paper shows that Bayesian inversion can be made functorial in this category under certain coherence assumptions. A key result is that Bayesian inverses are unique up to "almost sure" equality, meaning they agree on a subset of full measure. This allows defining Bayesian inversion on support objects to obtain uniqueness. Overall, the categorical framework provides a way to make Bayesian inversion functorial and compositional.


## What problem or question is the paper addressing?

 Based on my reading, it looks like this paper is addressing the problem of making Bayesian inversion of Markov kernels functorial in a category theoretic setting. 

A Markov kernel represents a stochastic or probabilistic process. Bayesian inversion refers to the process of inverting a Markov kernel to perform Bayesian inference - updating a prior belief to a posterior belief after observing data. 

The issue is that Bayesian inversion depends not just on the kernel itself but also the prior distribution. So it's not straightforward to make Bayesian inversion into a functor mapping kernels to their inverses in a category of Markov kernels.

The paper proposes using a category where morphisms are indexed families of kernels, so the entire family of Bayesian inverses can be represented as a single morphism. This allows Bayesian inversion to be made functorial.

The paper also relates this construction to the idea of a "lens" from functional programming, where data structures have forward and inverse views. So Bayesian inversion of kernels is framing as a kind of "Bayesian lens".

Overall, the main contribution seems to be developing category theory tools to make Bayesian inversion of kernels functorial/compositional, relating it to the lens concept, and using this to better understand Bayesian inference processes.
