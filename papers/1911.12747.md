# [ASR is all you need: cross-modal distillation for lip reading](https://arxiv.org/abs/1911.12747)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether it is possible to train a strong lip reading system without requiring human annotated ground truth data. The key idea is to teach a lip reading model by distilling knowledge from a pre-trained automatic speech recognition (ASR) model. Specifically, the main hypotheses are:1. Ground truth transcriptions are not necessary to train an effective lip reading system.2. Arbitrary amounts of unlabeled video data can be leveraged to improve lip reading performance. 3. Distillation can significantly accelerate lip reading training compared to using only the CTC loss.4. This method can achieve state-of-the-art lip reading performance when trained only on publicly available datasets.The authors propose using a cross-modal distillation method that combines CTC loss on ASR-generated transcripts with a frame-wise cross-entropy distillation loss between the ASR and lip reading models. Their experiments support the above hypotheses, showing strong lip reading models can be trained without human annotations and large amounts of unlabeled data boost performance. The distillation approach also speeds up training.


## What is the main contribution of this paper?

The main contributions of this paper are:1. They propose a method to train visual speech recognition (lip reading) models using cross-modal distillation from an automatic speech recognition (ASR) model, without requiring manually annotated ground truth data. 2. They show this method allows leveraging large amounts of unlabeled video data to improve lip reading performance. Pretraining on unlabeled data and then fine-tuning with a small amount of labels improves results.3. They demonstrate the proposed training method with combined CTC and cross-entropy distillation loss significantly accelerates training compared to just using CTC, even when ground truth labels are available.4. They obtain state-of-the-art lip reading performance on the LRS2 and LRS3 benchmarks using only publicly available data, surpassing prior work trained on proprietary datasets. In summary, the key ideas are using knowledge distillation from ASR to train lip reading without manual annotations, enabling use of extra unlabeled data, and achieving strong performance competitive with models trained on much larger proprietary datasets. The method is scalable, fast, and removes the need for ground truth transcriptions.
