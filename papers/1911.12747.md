# [ASR is all you need: cross-modal distillation for lip reading](https://arxiv.org/abs/1911.12747)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether it is possible to train a strong lip reading system without requiring human annotated ground truth data. The key idea is to teach a lip reading model by distilling knowledge from a pre-trained automatic speech recognition (ASR) model. Specifically, the main hypotheses are:1. Ground truth transcriptions are not necessary to train an effective lip reading system.2. Arbitrary amounts of unlabeled video data can be leveraged to improve lip reading performance. 3. Distillation can significantly accelerate lip reading training compared to using only the CTC loss.4. This method can achieve state-of-the-art lip reading performance when trained only on publicly available datasets.The authors propose using a cross-modal distillation method that combines CTC loss on ASR-generated transcripts with a frame-wise cross-entropy distillation loss between the ASR and lip reading models. Their experiments support the above hypotheses, showing strong lip reading models can be trained without human annotations and large amounts of unlabeled data boost performance. The distillation approach also speeds up training.


## What is the main contribution of this paper?

The main contributions of this paper are:1. They propose a method to train visual speech recognition (lip reading) models using cross-modal distillation from an automatic speech recognition (ASR) model, without requiring manually annotated ground truth data. 2. They show this method allows leveraging large amounts of unlabeled video data to improve lip reading performance. Pretraining on unlabeled data and then fine-tuning with a small amount of labels improves results.3. They demonstrate the proposed training method with combined CTC and cross-entropy distillation loss significantly accelerates training compared to just using CTC, even when ground truth labels are available.4. They obtain state-of-the-art lip reading performance on the LRS2 and LRS3 benchmarks using only publicly available data, surpassing prior work trained on proprietary datasets. In summary, the key ideas are using knowledge distillation from ASR to train lip reading without manual annotations, enabling use of extra unlabeled data, and achieving strong performance competitive with models trained on much larger proprietary datasets. The method is scalable, fast, and removes the need for ground truth transcriptions.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in visual speech recognition:- The main contribution of this paper is using knowledge distillation from an ASR model to train a lip reading model, without requiring manual transcriptions. This is a novel approach compared to most prior work in lip reading, which relies on ground truth annotated datasets. - Using distillation for cross-modal transfer is not entirely new - some prior works have explored distilling visual models to audio models or vice versa. However, this paper focuses specifically on distilling an audio ASR model into a visual lip reading model, which is an underexplored direction.- The paper shows competitive or state-of-the-art word error rates on the LRS2 and LRS3 benchmarks compared to prior published methods. However, some recent unpublished works have reported even lower error rates by training on much larger proprietary datasets. This highlights the benefit of larger training data for lip reading.- The proposed model architecture is relatively standard - a 1D convolutional network similar to Jasper. The main novelty is in the training methodology rather than model architecture.- For training, the paper explores combining CTC loss on ground truth or ASR transcriptions with the distillation loss. Using both together is shown to work better than either alone.- The paper demonstrates the benefit of pretraining on unlabeled data like VoxCeleb2 and then fine-tuning on smaller annotated datasets. This is a commonly used technique in other areas of deep learning as well.Overall, I would say the training methodology is the main novel contribution here compared to prior work in visual speech recognition. The results are competitive but some very recent unpublished works have pushed performance further. Exploring how to scale up the distillation approach to even larger datasets could be an interesting direction for future work.


## What future research directions do the authors suggest?

Here are some of the key future research directions suggested by the authors:- Scaling up the approach to even larger unlabeled datasets and model sizes, to match or surpass models trained on very large proprietary labeled datasets. The authors note they only used VoxCeleb2 in this work due to resource constraints, but could utilize larger unlabeled datasets like AVSpeech in the future.- Applying the method to develop lip reading models for many languages that lack annotated visual speech recognition data, by leveraging unlabeled video and a pretrained ASR model in the new target language.- Further investigating difficulties in distilling CTC acoustic models reported in prior work, stemming from misalignment between teacher and student spike timings. The authors did not observe improvements from proposed solutions like sequence-level distillation, so more work is needed.- Exploring whether the student model could surpass the teacher ASR model performance over time by exploiting the extra visual modality, as observed in some prior cross-modal distillation work.- Combining the cross-modal distillation approach with architectural improvements from concurrent work that also achieved state-of-the-art performance, for further gains.- Investigating the use of the method for unsupervised pre-training and then fine-tuning on a domain-shifted target dataset, in addition to the low-resource language scenario.In summary, the main future directions are scaling up the model and data size, extending to new languages, improving the distillation process, combining complementary architectural changes, and exploring additional applications of the cross-modal distillation technique.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a method to train visual speech recognition models without requiring human-annotated ground truth data. The key idea is to use knowledge distillation to transfer knowledge from a pre-trained automatic speech recognition (ASR) model to a student lip reading model. The student model is trained using a combination of connectionist temporal classification (CTC) loss on ASR-generated transcriptions and a frame-level cross-entropy distillation loss that minimizes the divergence between the student and teacher output distributions. Experiments demonstrate that this approach achieves strong lip reading performance without ground truth, speeds up training compared to CTC alone, improves with more unlabelled data, and obtains state-of-the-art results on LRS2 and LRS3 benchmarks when combined with a small amount of labelled data. The method provides a scalable way to exploit large unlabeled video datasets to boost visual speech recognition.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a cross-modal distillation method to train visual speech recognition models without requiring human-annotated ground truth data by combining CTC loss on ASR-generated transcripts with a frame-wise cross-entropy loss between student and teacher models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a method for training visual speech recognition models without requiring manually annotated data. The key idea is to use knowledge distillation to transfer knowledge from a pre-trained automatic speech recognition (ASR) model to the visual speech recognition (VSR) model. Specifically, the ASR model serves as a "teacher" and its output distributions over characters are used to train the VSR "student" model. The student model is trained using a combination of connectionist temporal classification (CTC) loss on the ASR output transcripts and a distillation loss that minimizes the divergence between the student and teacher output distributions. The experiments demonstrate four main findings: (1) Ground truth transcriptions are not necessary to train an effective lip reading system. (2) Large amounts of unlabeled video data can be leveraged to improve performance by pretraining then finetuning. (3) Distillation significantly accelerates training compared to CTC alone. (4) Using the proposed approach achieves state-of-the-art results on the LRS2 and LRS3 benchmarks when training only on publicly available data. Overall, the method provides a scalable way to train VSR models without requiring manual annotations.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a cross-modal distillation method to train a visual speech recognition (VSR) model without requiring human-annotated ground truth data. The method combines Connectionist Temporal Classification (CTC) loss with a frame-wise cross-entropy distillation loss. An Automatic Speech Recognition (ASR) model pre-trained on a large audio corpus serves as the teacher model. Its output transcriptions and frame-level posteriors are used to supervise training of the student VSR model on unlabeled video data through the combined CTC and distillation loss. This allows leveraging large amounts of unlabeled video containing talking heads to improve lip reading performance, without needing ground truth alignments between speech and text. The student model can also be optionally fine-tuned on a small amount of labeled data. The proposed approach achieves state-of-the-art lip reading performance on the LRS2 and LRS3 benchmarks when trained solely on publicly available data.
