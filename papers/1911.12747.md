# [ASR is all you need: cross-modal distillation for lip reading](https://arxiv.org/abs/1911.12747)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether it is possible to train a strong lip reading system without requiring human annotated ground truth data. The key idea is to teach a lip reading model by distilling knowledge from a pre-trained automatic speech recognition (ASR) model. Specifically, the main hypotheses are:1. Ground truth transcriptions are not necessary to train an effective lip reading system.2. Arbitrary amounts of unlabeled video data can be leveraged to improve lip reading performance. 3. Distillation can significantly accelerate lip reading training compared to using only the CTC loss.4. This method can achieve state-of-the-art lip reading performance when trained only on publicly available datasets.The authors propose using a cross-modal distillation method that combines CTC loss on ASR-generated transcripts with a frame-wise cross-entropy distillation loss between the ASR and lip reading models. Their experiments support the above hypotheses, showing strong lip reading models can be trained without human annotations and large amounts of unlabeled data boost performance. The distillation approach also speeds up training.
