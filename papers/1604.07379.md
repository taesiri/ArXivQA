# Context Encoders: Feature Learning by Inpainting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: can convolutional neural networks (CNNs) learn semantic image representations in an unsupervised manner by being trained to fill in large missing regions of images based on the surrounding context? The key hypothesis is that in order to succeed at predicting missing image regions based only on context, the model needs to build a strong semantic understanding of scenes and objects. So the authors propose training "context encoders" - CNNs that take an image with missing regions as input and must fill in those missing regions. They hypothesize this will force the model to learn generally useful semantic features.Some key points:- Context encoders are a type of convolutional autoencoder trained to inpaint missing image regions based on surrounding context.- The authors argue this forces more semantic understanding compared to other unsupervised approaches like denoising autoencoders.- They show context encoders can plausibly fill in missing image regions, especially with a combined reconstruction + adversarial loss.- They demonstrate the learned features transfer well to other tasks like classification, detection, and segmentation.So in summary, the central hypothesis is that the unsupervised task of image inpainting based on context will produce semantically useful feature representations. The paper aims to demonstrate and evaluate this claim.
