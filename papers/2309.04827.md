# [Neurons in Large Language Models: Dead, N-gram, Positional](https://arxiv.org/abs/2309.04827)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper examines is how neurons in large language models evolve with scale, focusing specifically on neurons inside the feedforward networks (FFNs) of OPT models ranging from 125M to 66B parameters. The key hypotheses are:- Many neurons in the early layers are "dead", i.e. never activate on a diverse set of data. Larger models have more of these dead neurons, indicating increased sparsity. - Many alive neurons act as n-gram detectors, activating only for specific tokens or short sequences. Larger models have more of these specialized neurons.- Some n-gram detecting neurons suppress information about their triggering tokens, actively removing that information from the representation rather than just burying it. - Some neurons encode positional information regardless of textual content, conflicting with the view of FFNs as purely key-value memories. Smaller models rely more heavily on these positional neurons.In summary, the central hypothesis is that FFN neuron roles evolve systematically with scale, with larger models exhibiting increased sparsity and specialization. The paper aims to characterize and understand this evolution.


## What is the main contribution of this paper?

The main contribution of this paper is analyzing how neurons in the feedforward layers of large language models behave, in order to better understand the internal workings and evolution of these models. Specifically, the key findings are:- Many neurons in the early layers are "dead", never activating on the data. Larger models have more dead neurons, indicating they are more sparse.- Many non-dead neurons act as detectors for specific tokens or n-grams. Their corresponding feedforward updates promote next token candidates but also explicitly remove information about the current input tokens. - Some neurons encode positional information regardless of textual content. Smaller models have more explicit positional neurons while larger models move away from absolute position encoding. - There are qualitative differences in how neurons behave across model sizes. With more capacity, models have more specialized neurons, operate in more stages, and rely less on absolute position encoding.- Even minor implementation details like LayerNorm ordering affect neuron behavior and interpretability. Overall, this lightweight analysis reveals new insights into how individual neurons behave and how this changes with scale, questioning some common assumptions about the roles of feedforward layers. The main contribution is providing a better understanding of large language models through analyzing individual neurons.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper analyzes neurons in large language models ranging from 125M to 66B parameters and finds that larger models have more "dead" neurons as well as neurons specialized for discrete features like tokens and n-grams, and that some neurons encode positional information independent of content, questioning the view of feedforward layers as purely content-based key-value memories.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to related work on analyzing and interpreting large language models:- The paper focuses specifically on analyzing different roles of individual neurons in the feedforward layers of OPT models. Most prior work has looked at other components like attention or entire transformer blocks. Analyzing individual neurons provides finer-grained insights.- The analysis is done in a very lightweight way using only forward passes on a single GPU. This allows scaling up to very large models like OPT-66B. Most prior work has been limited to smaller models like BERT due to computational constraints. - The findings reveal new types of neurons not previously identified, like "dead" neurons that never activate and neurons that explicitly remove information. Prior work has found things like token and positional detectors, but not these new types.- The paper challenges the common view of feedforward layers as purely key-value memories by showing neurons that violate this assumption. This questions current theories on the role of feedforward layers.- The analysis looks at how interpretations change across model scale, from 125M to 66B parameters. Most prior work analyzes one model size. The scale analysis provides insights into how representations evolve.- The ensemble-like behavior across layers and explicit suppression of input tokens are new findings not highlighted in prior work.Overall, the lightweight methodology, new neuron types discovered, and scale analysis differentiate this work from most prior interpretability research focused on smaller areas of smaller models. The findings advance our theoretical understanding of large language models.


## What future research directions do the authors suggest?

The authors propose several directions for future work based on the findings in this paper:- Further explore mechanisms for explicitly removing information from the residual stream. The finding that some neurons target suppressing their trigger tokens suggests models may have specialized mechanisms for removing information. This could be investigated more thoroughly.- Better understand the roles played by feedforward layers in transformers. The discovery of strong positional neurons indicates feedforward layers are used in ways beyond just matching input patterns to output distributions, questioning the prevalent "key-value memory" view. More research is needed on how these layers work. - Analyze the effects of different modeling choices on interpretability. They found the 350M OPT model behaved differently due to where layer norm was applied, impacting the interpretability of neurons. Systematically exploring such modeling decisions could reveal other "knobs" for controlling interpretability.- Scale analysis to other model families besides OPT. This work focused on the OPT model family - studying other large pretrained language models could reveal different insights.- Look at other units of analysis beyond individual neurons. While they focused on neurons, analyzing other components like attention or computational circuits could reveal new findings.- Apply analysis to broader tasks beyond language modeling. Much of their data was language modeling - studying neurons on more end tasks could uncover different behaviors.Overall, the authors call for more research taking a "lightweight" approach to analyzing large language models in order to better understand their internal representations and mechanics. Their findings reveal we still lack understanding of these powerful models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper analyzes the inner workings of large language models in the OPT family ranging from 125M to 66B parameters. The analysis focuses on the feedforward network (FFN) neurons in these models. The key findings are: 1) In early layers, many neurons are "dead" and never activate. Larger models have more dead neurons indicating greater sparsity. 2) Many alive neurons act as detectors for specific tokens or n-grams. Their FFN updates promote next token candidates but also deliberately suppress the current token, actively removing that information. Larger models have more token detectors. 3) Some neurons encode positional information independent of content, contradicting the view of FFNs solely as key-value memories. Smaller models use explicit position range indicators while larger models are less focused on absolute position. Overall, the paper provides insights into the evolving inner mechanisms of transformers as they scale up, including increased sparsity, more specialized processing, and shifts in positional encoding. The analysis is done efficiently using single GPU processing of neuron activations.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper analyzes the internal workings of a family of large language models called OPT ranging from 125 million to 66 billion parameters. The analysis focuses on the feedforward neurons, specifically when they activate or not. The first main finding is that many neurons in the earlier layers of the models are "dead" - they never activate on a diverse dataset. Larger models have a higher percentage of dead neurons, indicating they are more sparse. The second finding is that many alive neurons act as detectors for specific tokens or n-grams, activating only for those discrete inputs. Interestingly, when activated these neurons not only promote concepts related to the next token but also explicitly remove information about the triggering token. This suggests mechanisms in the models for removing as well as adding information. Finally, some neurons encode positional information independent of the textual input, acting as indicators of certain position ranges. Smaller models rely more on these positional neurons while larger models operate less explicitly. Overall the analysis reveals the models have neurons that are dead, detect discrete inputs, and encode positional information, with larger models being more sparse in various senses.In summary, the paper provides a lightweight analysis of large language models focusing on individual feedforward neurons. It finds these models have dedicated neurons for discrete features like tokens and n-grams, with larger models packing more concepts into individual neurons. The updates from discrete detectors remove as well as add information. Some neurons also encode positional information, contrary to the view of feedforward layers solely matching textual patterns. Larger models rely less on absolute position. The analysis reveals new sparseness, discreteness, concept suppression, and position encoding in the internal components of large language models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper analyzes a family of large language models in a lightweight manner that can be done on a single GPU. Specifically, it focuses on the OPT family of models ranging from 125m to 66b parameters and relies only on whether an FFN neuron is activated or not. The authors look at three main aspects of FFN neurons: 1) They identify "dead" neurons that never activate on a large diverse dataset and find that larger models have a higher proportion of dead neurons. 2) They identify token and n-gram detecting neurons that activate only for specific tokens/n-grams and find that larger models have more of these, with the neurons in each layer detecting different tokens/n-grams than previous layers. The updates from the token detectors both promote related concepts but also explicitly remove information about the triggering token. 3) They identify positional neurons that activate based mainly on position, not content, with smaller models encoding absolute position more explicitly. Overall the lightweight analysis reveals new insights into neurons' roles, like dead neurons, explicit removal of info, and non key-value positional neurons.
