# [HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training](https://arxiv.org/abs/2005.00200)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we develop an effective framework for large-scale video+language representation learning that better captures the temporal alignment and sequential nature of videos compared to existing methods?The key aspects are:- Proposing a hierarchical model architecture that encodes multimodal inputs in a hierarchical fashion to capture both local and global contexts. This is in contrast to existing models that use a flat BERT-like encoder. - Designing new pre-training tasks (VSM and FOM) that encourage temporal alignment of multimodalities and exploit the sequential characteristics of videos.- Using a more diverse video corpus for pre-training, beyond just instructional videos, to learn from richer visual content. - Introducing new challenging retrieval and QA datasets to evaluate the model's video understanding capabilities.The overarching goal is to advance video+language representation learning by better capturing temporal dynamics through novel model architectures and pre-training tasks, training on diverse video data, and evaluating on more comprehensive benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The proposal of a new video+language pre-training framework called HERO (Hierarchical Encoder for Omni-Representation Learning). This includes:- A hierarchical model architecture with a Cross-modal Transformer and a Temporal Transformer to capture local and global context. - Two new pre-training tasks: Video-Subtitle Matching (VSM) and Frame Order Modeling (FOM), in addition to standard Masked LM and Masked Frame Modeling.2. Pre-training on a diverse corpus including both instructional videos (HowTo100M) and TV shows to learn from richer, more complex visual content.3. The introduction of two new challenging datasets for video moment retrieval (How2R) and QA (How2QA) based on HowTo100M videos.4. State-of-the-art results on multiple downstream tasks including retrieval, QA, inference, and captioning across different domains and video types (multi-channel and single-channel).In summary, the key contributions are proposing a new hierarchical Transformer-based model HERO, designing better pre-training objectives, using a more diverse pre-training corpus, collecting new challenging benchmarks, and showing superior performance on various downstream tasks compared to prior work. The main novelty seems to be in model architecture and pre-training design to better exploit temporal alignment and the sequential nature of videos.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a hierarchical Transformer-based model called HERO for large-scale video and language representation learning, which achieves state-of-the-art results on multiple video-and-language understanding tasks by leveraging novel pre-training objectives that explicitly model temporal alignment between modalities.
