# [HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training](https://arxiv.org/abs/2005.00200)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we develop an effective framework for large-scale video+language representation learning that better captures the temporal alignment and sequential nature of videos compared to existing methods?The key aspects are:- Proposing a hierarchical model architecture that encodes multimodal inputs in a hierarchical fashion to capture both local and global contexts. This is in contrast to existing models that use a flat BERT-like encoder. - Designing new pre-training tasks (VSM and FOM) that encourage temporal alignment of multimodalities and exploit the sequential characteristics of videos.- Using a more diverse video corpus for pre-training, beyond just instructional videos, to learn from richer visual content. - Introducing new challenging retrieval and QA datasets to evaluate the model's video understanding capabilities.The overarching goal is to advance video+language representation learning by better capturing temporal dynamics through novel model architectures and pre-training tasks, training on diverse video data, and evaluating on more comprehensive benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The proposal of a new video+language pre-training framework called HERO (Hierarchical Encoder for Omni-Representation Learning). This includes:- A hierarchical model architecture with a Cross-modal Transformer and a Temporal Transformer to capture local and global context. - Two new pre-training tasks: Video-Subtitle Matching (VSM) and Frame Order Modeling (FOM), in addition to standard Masked LM and Masked Frame Modeling.2. Pre-training on a diverse corpus including both instructional videos (HowTo100M) and TV shows to learn from richer, more complex visual content.3. The introduction of two new challenging datasets for video moment retrieval (How2R) and QA (How2QA) based on HowTo100M videos.4. State-of-the-art results on multiple downstream tasks including retrieval, QA, inference, and captioning across different domains and video types (multi-channel and single-channel).In summary, the key contributions are proposing a new hierarchical Transformer-based model HERO, designing better pre-training objectives, using a more diverse pre-training corpus, collecting new challenging benchmarks, and showing superior performance on various downstream tasks compared to prior work. The main novelty seems to be in model architecture and pre-training design to better exploit temporal alignment and the sequential nature of videos.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a hierarchical Transformer-based model called HERO for large-scale video and language representation learning, which achieves state-of-the-art results on multiple video-and-language understanding tasks by leveraging novel pre-training objectives that explicitly model temporal alignment between modalities.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper focuses on video and language representation learning, an area that has gained increasing interest recently but is still relatively new compared to image and text multimodal research. The paper notes that most prior work has focused on static images rather than dynamic video. - The proposed model, Hero, uses a hierarchical encoder structure to better capture temporal alignment between video frames and subtitles. This differs from many existing approaches that use a flat BERT-like encoder or simple concatenation. The results demonstrate the benefits of the hierarchical design.- The pre-training procedure incorporates two new tasks, Video-Subtitle Matching (VSM) and Frame Order Modeling (FOM), that are designed to explicitly model temporal alignment and video sequence order. This is a key novelty compared to prior work that directly adapts image-text pre-training objectives.- For pre-training data, the paper uses both instructional videos (HowTo100M) as well as a new large-scale TV dataset. Using diverse video sources, especially those with complex social interactions, appears to be an advantage over prior work focused just on cooking videos.- The model is evaluated on a broad set of downstream tasks including retrieval, QA, inference, and captioning across both single-channel and multi-channel videos. Showing strong performance on this diverse set of benchmarks demonstrates the versatility of the approach.- The new How2R and How2QA datasets provide more challenging video-and-language understanding benchmarks compared to existing options like YouCook2 and MSR-VTT.Overall, the hierarchical encoder, novel pre-training tasks, diverse video data, and thorough downstream evaluation seem to give Hero advantages over related prior art in video-and-language representation learning. The consistently strong empirical results validate the proposed techniques.
