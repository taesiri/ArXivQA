# COLD Decoding: Energy-based Constrained Text Generation with Langevin   Dynamics

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be how to incorporate constraints into text generation using pretrained language models, without the need for task-specific fine-tuning. The main hypothesis appears to be that constrained text generation can be formulated as sampling from an energy-based model through the use of a flexible energy function and efficient Langevin dynamics sampling. The proposed approach, called COLD (Constrained Decoding with Langevin Dynamics), aims to enable controllable and coherent text generation by specifying constraints via an energy function and then sampling from the induced distribution using gradient-based reasoning. The key ideas seem to be:- Formulating constrained text generation in an energy-based modeling framework to flexibly incorporate different constraints into a unified energy function.- Using continuous approximations and Langevin dynamics for efficient differentiable sampling from the complex energy landscape, overcoming challenges with discrete text.- Applying COLD to pretrained language models without task-specific fine-tuning to steer generation based on constraints.- Demonstrating COLD's flexibility and effectiveness on challenging text generation tasks like lexically-constrained decoding, abductive reasoning, and counterfactual reasoning by specifying appropriate energy functions.So in summary, the main research question appears to be focused on exploring constrained decoding without fine-tuning by formulating it as energy-based sampling, in order to provide control over text generation from pretrained LMs. The key hypothesis seems to be that the proposed COLD approach can effectively handle diverse constraints and generate high quality outputs for varying tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The proposal of a new constrained decoding approach called COLD (Constrained Decoding with Langevin Dynamics) that formulates decoding as sampling from an energy-based model (EBM). COLD allows flexibly composing constraints based on the task using an energy function, then performs sampling via Langevin dynamics.2. COLD provides a unified framework for incorporating hard lexical constraints and soft contextual constraints. It complements prior decoding methods that search for an optimal solution, by providing a sampling approach instead. 3. An empirical evaluation of COLD on three challenging text generation tasks: lexically constrained decoding, abductive reasoning, and counterfactual reasoning. The results provide insights into the strengths of COLD compared to prior approaches involving discrete search or differentiable reasoning.4. The introduction of Langevin dynamics to text-based EBMs for efficient gradient-based sampling, by using a continuous relaxation of text. This helps address longstanding challenges with sampling from EBMs over discrete text.5. Demonstrating that COLD can be applied directly to off-the-shelf language models without task-specific fine-tuning, highlighting its flexibility.In summary, the main contribution appears to be the proposal and empirical evaluation of the COLD framework for flexible constrained text generation using efficient sampling with Langevin dynamics. The results on three tasks highlight its strengths over prior decoding methods involving search or optimization.
