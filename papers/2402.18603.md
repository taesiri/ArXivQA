# [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Symbolic regression aims to identify mathematical expressions that model relationships between variables in data. It is typically framed as a combinatorial optimization problem and solved using genetic programming or reinforcement learning, but these have limitations. 

- Recent work has treated symbolic regression as a translation task using sequence-to-sequence models. However, the numeric data and symbolic expressions are different modalities without clear word-level alignments between them.

- Prior methods using contrastive learning for representation alignment train the alignment and downstream modules separately. This decoupled training strategy is less effective for fusing representations.

Method:
- Proposes to formulate symbolic regression as a multimodal modeling problem with separate encoders for the numeric data and symbolic expressions. 

- Uses a Set Transformer to encode numeric data for permutation invariance and a causal transformer decoder to generate the expression tree traversal sequences.

- Employs an alignment module with contrastive loss to bring matched numeric and symbolic representations closer in the embedding space.

- Jointly optimizes contrastive loss for alignment along with cross-entropy and MSE losses for the downstream task in an end-to-end fashion.

Contributions:
- First to treat symbolic regression as a purely multimodal modeling problem with a single training phase.

- Introduces a novel model architecture tailored for this formulation using set transformer, causal transformer decoder.

- Achieves new state-of-the-art results on over 10 benchmark datasets demonstrating effectiveness.

- Analysis shows joint training of contrastive loss and downstream losses enables better representation fusion.
