# [Neural networks with trainable matrix activation functions](https://arxiv.org/abs/2109.09948v4)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: How can we make neural network activation functions adaptive and trainable instead of pre-specified and fixed? 

The key ideas and contributions of the paper are:

- Proposing a new method to construct matrix activation functions whose entries are generalized from ReLU. The activation functions depend on trainable parameters.

- The matrix activation is realized efficiently using only scalar multiplications and comparisons. 

- Both diagonal and tridiagonal matrix activations are considered, which allows interactions between different neurons.

- The proposed trainable matrix activation functions (TMAF) are incorporated into neural network models. The resulting TMAF neural networks have activation functions that can be adapted during training. 

- Numerical experiments on function approximation and image classification tasks demonstrate the effectiveness and robustness of TMAF neural networks compared to networks with fixed ReLU-type activations.

In summary, the main hypothesis is that making activation functions trainable instead of fixed will improve neural network performance. The paper proposes a way to achieve this via trainable matrix activations, and provides empirical validation on some test problems.
