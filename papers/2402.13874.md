# [$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for   In-Context Learning](https://arxiv.org/abs/2402.13874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- In-context learning (ICL) for large language models (LLMs) relies heavily on appropriate demonstration examples to activate the model's capabilities. However, prior work on example selection predominantly follows a "select then organize" approach which neglects relationships between examples.

Proposed Solution  
- The paper proposes Se2, a novel method that formulates example selection as a sequential problem. Se2 utilizes an LLM's feedback on varying context to capture inter-relationships and sequential information between examples. 

- Se2 employs a sequential-aware model trained on LLM scoring data constructed across different context inputs. Beam search is used during inference to enhance sequence quality and diversity.

Main Contributions
- Proposes a new sequential paradigm for selecting demo examples that models relationships between them.

- Introduces Se2, a sequence-aware method that leverages LLM feedback on changing context to discern example relationships.

- Achieves significant gains over baselines on 23 NLP datasets, highlighting efficacy of modeling sequence information for ideal prompt construction.

- Analysis shows Se2's stability across tasks/models, ability to identify logically related examples, and transferability by scoring with smaller LLM.

In summary, the key innovation is formulating example selection as a sequential problem and using sequential signals from an LLM to discern relationships between examples. This facilitates superior prompt construction and downstream task performance.
