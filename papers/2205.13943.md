# [Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN](https://arxiv.org/abs/2205.13943)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is:

What is the essence of masked image modeling (MIM) as a self-supervised pre-training task, and can it be made compatible with CNN architectures in addition to ViTs? 

The authors hypothesize that the essence of MIM is teaching models to learn better middle-order interactions between image patches, which allows extracting more generalized features. They propose an architecture-agnostic MIM framework called A^2MIM that is compatible with both CNNs and ViTs, in contrast to prior works that insist MIM is incompatible with CNNs. Through extensive experiments, they demonstrate that A^2MIM improves representation learning and transfer capabilities for both CNNs and ViTs on image classification, object detection, and semantic segmentation.

In summary, the key research question is understanding the working mechanism of MIM and proposing an architecture-agnostic framework to enable MIM with CNNs, not just ViTs. The hypothesis is that MIM essentially enhances middle-order patch interactions, which is tested empirically.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It conducts systematic experiments and shows that masked image modeling (MIM) as a pre-training task essentially teaches models to learn better middle-order interactions between image patches. This provides an explanation for why MIM works.

2. It proposes a novel masked image modeling framework called A^2MIM that bridges the gap between convolutional neural networks (CNNs) and transformers for self-supervised pre-training. The key ideas are:

- Masking input images with the RGB mean value instead of a learnable mask token to avoid distorting low-level statistics.

- Adding the mask token to intermediate feature maps instead of the input to enhance middle-order interactions. 

- Introducing a loss term in the Fourier domain to guide the model to learn more informative middle frequencies corresponding to middle-order interactions.

3. It demonstrates through extensive experiments that A^2MIM improves representation learning for both CNNs and transformers on ImageNet classification. It also shows strong transfer learning performance to downstream tasks like object detection and semantic segmentation.

4. The proposed A^2MIM framework is architecture-agnostic and does not rely on designs specialized for transformers. This allows it to work for both CNNs and transformers in a unified manner.

In summary, the key contribution is an analysis of what MIM pre-training does along with an architecture-agnostic framework A^2MIM that enables masked image modeling for both CNNs and transformers. This bridges the gap between the two model families for self-supervised representation learning.
