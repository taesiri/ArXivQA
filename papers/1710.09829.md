# [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we design neural network architectures that can represent visual entities and their spatial relationships more efficiently than convolutional neural networks?

The key hypotheses are:

- Using vector representations ("capsules") for entities can encode instantiation parameters like pose, lighting etc. This is more efficient than convolutional nets for viewpoint invariance.

- Routing by agreement between capsules can segment out visual entities better than convolutional nets, even with heavy overlap. 

- The overall approach of capsules with routing by agreement can achieve state-of-the-art performance on digit recognition tasks while using fewer parameters than convolutional nets.

In summary, the paper proposes capsules and dynamic routing as a more efficient alternative to convolutional nets for certain visual recognition tasks involving viewpoint invariance and highly overlapping entities. The hypotheses are that capsules can represent visual entities more efficiently, and routing by agreement can perform segmentation better.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a capsule model for object recognition that uses vectors rather than scalars for the activities of neural units. The vector represents the various properties of an entity like pose, deformation, texture etc. 

2. Using the length of the activity vector to represent the probability that the entity exists. The orientation of the vector represents the instantiation parameters.

3. Introducing a dynamic routing algorithm between capsules where lower level capsules make predictions for higher level capsules. The coupling coefficients between capsules are iteratively refined based on the agreement between predicted and actual higher level capsule output.

4. Achieving state-of-the-art results on MNIST by using a shallow capsule model trained with margin loss and reconstruction regularization.

5. Showing that capsules can recognize highly overlapping digits much better than convolutional networks on a MultiMNIST dataset. The routing by agreement allows the model to segment the image into individual digits even with extreme overlap.

6. Demonstrating that capsules are more robust to affine transformations than convolutional networks without explicitly training for it.

In summary, the main contribution is proposing capsules and the routing algorithm as a new architecture for neural networks that tries to address some limitations of convolutional networks. The results on benchmark datasets showcase the potential advantages and effectiveness of the capsule model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes capsule networks, a new neural network architecture that uses iterative dynamic routing between layers of capsules to perform complex tasks like recognizing highly overlapping digits more effectively than convolutional neural networks.


## How does this paper compare to other research in the same field?

 This paper on dynamic routing between capsules makes several notable contributions to the field of deep learning for computer vision:

- It proposes a new architecture called CapsuleNet that uses vector outputs called capsules to represent entities like objects or object parts. This is different from standard CNNs that use scalar outputs like class probabilities. 

- It introduces a dynamic routing algorithm between capsules that allows them to iteratively build part-whole relationships by agreement. This replaces the pooling operations used in CNNs.

- It shows that CapsuleNets can achieve state-of-the-art results on MNIST, outperforming regular CNNs with fewer parameters.

- It demonstrates that CapsuleNets are better at generalizing to novel viewpoints and segmenting highly overlapping digits on MultiMNIST.

- The vector capsule outputs appear to encode meaningful instantiation parameters like scale, skew, thickness etc. as shown through reconstruction regularization.

Some key differences from prior work:

- Transforming autoencoders (Hinton 2011) also used capsules but required external specification of transformation matrices. This work shows end-to-end learning of capsules.

- Other attempts at part-whole relationships like spatial transformer networks (Jaderberg 2015) did not model multiple entities and relationships simultaneously like dynamic routing does.

- Prior work on overlapping digits (Ba 2014) used sequential attention on 2D convolutions whereas this segments via part-whole capsules.

- The results on MultiMNIST with 80% overlap outperform prior work with much less overlap like <4% in Ba et al.

So in summary, CapsuleNets advance the state-of-the-art in modeling part-whole relationships and segmenting overlapping objects compared to regular CNNs and other prior approaches. The dynamic routing algorithm is a key contribution. However, at the time, computational demands limited application to larger scale problems.


## What future research directions do the authors suggest?

 The authors suggest several potential future research directions for capsules:

- Optimizing the routing algorithm for GPUs to make training faster. Currently capsules are slower to train than standard convolutional networks due to the routing procedure involving many small matrix multiplies. Implementing these efficiently on GPUs could help scale capsules to larger datasets.

- More extensive hyperparameter tuning and architecture exploration for capsules on datasets like CIFAR and ImageNet. The authors were limited in how much of this they could do due to the slower training, but more of this could help achieve state-of-the-art results. 

- Exploring different capsule architectures beyond the simple models tested in this paper. The authors propose capsules as a promising new approach, but only demonstrate a few small architectures. Trying out more diverse capsule network designs could uncover better ways to take advantage of capsules.

- Developing new techniques to allow capsules to represent multiple instances of the same object class, which they currently cannot handle well. This could improve performance on complex images with repeated objects.

- Further experimenting with the dimensionality of capsules at different layers of the network architecture. The authors suggest capsule dimension may increase up higher levels, but more exploration is needed to determine the optimal dimensions.

- Investigating whether capsules can learn more robust representations on a wider variety of transformations like rotations. The authors tested some basic affine transformations but not 3D rotations.

- Exploring whether capsules can achieve better transfer learning or generalization compared to standard CNNs due to learning more robust representations.

In summary, the main future directions are around optimizing capsules for faster training, trying out more capsule architectures, improving how capsules handle multiple objects, and further testing the generalization abilities of capsules. The authors propose capsules as an promising alternative to CNNs but more research is needed to fully demonstrate their capabilities.


## Summarize the paper in one paragraph.

 This paper proposes a capsule network architecture for visual pattern recognition. Capsules are groups of neurons that represent different properties of an entity like pose, deformation, texture etc. The overall length of the capsule's output vector represents the probability that the entity exists, and the orientation represents the entity's properties. Capsules in one layer make predictions for the properties of capsules in the next layer via transformation matrices. Through an iterative routing process, lower level capsules send their output to higher level capsules whose prediction vectors best agree with the lower level output. This "routing by agreement" allows the model to recognize multiple overlapping objects. 

The proposed CapsNet architecture has two convolutional layers followed by a capsule layer and a fully connected digit capsule layer. On the MNIST dataset, CapsNet achieves state-of-the-art accuracy by modeling the pose of digits and using a separate margin loss per digit capsule. The dynamic routing allows CapsNet to achieve high accuracy onMultiMNIST, a dataset of overlapping MNIST digit pairs. CapsNet also generalizes better to affine transformations than convolutional networks. The dimensionality of the digit capsule output allows different properties like thickness and skew to be independently manipulated. While not yet competitive with deep neural nets on larger datasets, CapsNet shows potential for modeling spatial relationships and segmentation tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes Capsule Networks, a new neural network architecture for image classification. Capsules are groups of neurons that represent different properties of visual entities such as objects or object parts. The overall length of a capsule's output vector represents the probability that the entity exists, while the orientation represents the entity's properties. Capsules in one layer make predictions for the properties of capsules in the next layer via transformation matrices. Through an iterative routing-by-agreement process, capsules decide which predictions to follow based on agreement between the lower level capsule's output and the prediction. 

The authors test Capsule Networks on the MNIST and MultiMNIST datasets. On MNIST, a simple 3-layer Capsule Network achieves state-of-the-art accuracy. On MultiMNIST, where handwritten digits overlap significantly, Capsule Networks outperform convolutional networks at classifying both digits. This demonstrates capsules' ability to correctly assign parts to wholes. The dimensions of the capsule output vectors also capture interpretable variations like scale, skew, stroke thickness, and localization. The results show Capsule Networks have potential as more efficient models that generalize better to novel viewpoints.


## Summarize the main method used in the paper in one paragraph.

 This paper proposes a capsule-based neural network architecture called CapsNet for image classification. The key idea is to represent visual entities as vectors called capsules, where the vector represents the various properties of the entity like pose, deformation, texture etc. The length of the capsule vector represents the probability that the entity exists. 

CapsNet uses a dynamic routing algorithm between capsule layers to decide how lower level capsules should be routed to higher level capsules. The routing is based on the agreement between the lower level capsule's output and the higher level capsule's input. This allows the network to model part-whole relationships and perform segmentation. 

The CapsNet architecture has two convolutional capsule layers followed by a fully connected capsule layer. A reconstruction network is attached to the final layer capsules to regularize the capsules and learn the entity properties. On MNIST and a highly overlapping digits dataset, CapsNet with routing achieves state of the art performance and is able to perform segmentation well. The dimensions of the final capsule layer are shown to encode visual entity properties like thickness, skew etc.


## What problem or question is the paper addressing?

 This paper is addressing the limitations of convolutional neural networks (CNNs) for object recognition, and proposing capsule networks as an alternative approach to overcome those limitations. 

Some key problems/questions the paper discusses:

- CNNs have trouble generalizing to novel viewpoints of objects, due to relying on replicated feature detectors and large training sets covering different views. The paper argues capsules can represent objects in a more viewpoint invariant way using transformation matrices.

- CNNs struggle with segmenting overlapping objects in images. The paper shows capsules can perform segmentation by routing-by-agreement between different capsules detecting parts of objects. 

- What is the right architecture for implementing capsules effectively? The paper explores a simple 3-layer capsule architecture.

- How can capsules represent the properties of visual entities? The paper proposes using the vector output of capsules to encode instantiation parameters like pose, deformation, texture etc.

- How to implement dynamic routing between capsules for part-whole relations? The paper introduces an iterative routing procedure based on agreement between capsule predictions.

- How to represent the existence of visual entities? The paper proposes using the length of the capsule vector to represent existence probability.

So in summary, the key focus is overcoming limitations in CNNs by developing the capsule architecture and routing algorithm as a better approach to viewpoint invariance, segmentation, and representing visual entities.


## What are the keywords or key terms associated with this paper?

 Here are some key points from the paper:

- Capsules - Groups of neurons that represent entities like objects or parts. The activity vector represents instantiation parameters like pose, texture, deformation, etc.

- Routing-by-agreement - A dynamic routing procedure between capsule layers, where lower-level capsules send their output to higher capsules based on agreement between their predictions and the higher capsules' output. This implements "explaining away".

- Activity vector length - The length represents the probability that the entity represented exists. Short vectors get shrunk, long vectors get slightly shrunk via a squashing nonlinearity.

- Affine invariance - Capsules are robust to affine transformations like translation, rotation, scaling, etc. due to encoding spatial relationships with matrices. Better generalization.  

- Convolutional capsule layers - Capsule layers can be convolutional, with capsules making different predictions for different spatial locations.

- Parsing/segmentation - Routing allows parsing the image into objects and assigning parts, segmenting highly overlapping digits.

- Reconstruction - The digit capsule activity vectors are decoded to reconstruct the image, acting as a regularization method.

- MNIST, MultiMNIST - State-of-the-art or competitive results on digit classification and segmenting overlapping digits.

Some key terms: capsules, routing by agreement, activity vectors, transformation matrices, parsing, convolutional capsule layer, reconstruction, digit classification.
