# [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop conditional generative models that directly produce implicit 3D representations like neural radiance fields (NeRFs) and textured meshes?The key ideas and contributions appear to be:- They train a Transformer-based encoder to map 3D assets to implicit neural representation (INR) parameters. The encoder is trained on a large dataset to encode assets as both NeRFs and signed distance / texture fields.- They then train diffusion models on the latent spaces produced by the encoder. This allows sampling novel view-consistent 3D assets conditioned on images or text. - They compare their proposed model ("Shape-E") to Point-E, a diffusion model over point clouds. Shape-E matches or exceeds Point-E in sample quality despite operating in a higher-dimensional output space.- Shape-E also seems to converge faster than Point-E during training. The authors argue that implicit 3D representations like NeRFs and meshes may be easier to directly generate compared to explicit representations like point clouds.So in summary, the main research question is how to build conditional generative models that directly output implicit 3D representations, which could offer benefits over explicit 3D representations. The key ideas are training encoders to map assets to implicit parameters, and then modeling these latent spaces with diffusion models.


## What is the main contribution of this paper?

The main contribution of this paper is introducing Shap-E, a conditional generative model for 3D assets. Shap-E generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. The key aspects are:- Shap-E trains an encoder to deterministically map 3D assets into implicit function parameters. - A conditional diffusion model is then trained on the encoder outputs to generate new assets conditioned on text or images.- When trained on a large dataset, Shap-E can generate complex and diverse 3D assets in seconds conditioned on text prompts.- Compared to Point-E, a generative model over point clouds, Shap-E converges faster and reaches better sample quality despite modeling a higher-dimensional output space with multiple representations.- Shap-E shares architecture, datasets, and conditioning with Point-E, allowing for isolating the effects of the output representation. Surprisingly, the models exhibit similar failure cases when conditioned on images, suggesting behavior depends more on data/architecture than output space.- Qualitatively, Shap-E and Point-E can produce different samples given the same text prompt, indicating modeling differences between explicit and implicit representations.In summary, the main contribution is presenting Shap-E as an implicit conditional generative model for 3D that matches or improves on an explicit model baseline while offering more flexible representations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents Shape-E, a generative model that can generate 3D assets conditioned on text prompts by modeling them as implicit neural representations that can be rendered as textured meshes or neural radiance fields.
