# [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop conditional generative models that directly produce implicit 3D representations like neural radiance fields (NeRFs) and textured meshes?The key ideas and contributions appear to be:- They train a Transformer-based encoder to map 3D assets to implicit neural representation (INR) parameters. The encoder is trained on a large dataset to encode assets as both NeRFs and signed distance / texture fields.- They then train diffusion models on the latent spaces produced by the encoder. This allows sampling novel view-consistent 3D assets conditioned on images or text. - They compare their proposed model ("Shape-E") to Point-E, a diffusion model over point clouds. Shape-E matches or exceeds Point-E in sample quality despite operating in a higher-dimensional output space.- Shape-E also seems to converge faster than Point-E during training. The authors argue that implicit 3D representations like NeRFs and meshes may be easier to directly generate compared to explicit representations like point clouds.So in summary, the main research question is how to build conditional generative models that directly output implicit 3D representations, which could offer benefits over explicit 3D representations. The key ideas are training encoders to map assets to implicit parameters, and then modeling these latent spaces with diffusion models.
