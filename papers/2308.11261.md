# [HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations](https://arxiv.org/abs/2308.11261)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to generate full-body avatar motion in real-time given only sparse input from a head-mounted device (HMD), including scenarios where the hands may only be partially visible or out of view entirely. The key hypothesis is that a neural network model can learn to accurately predict full body motion from limited HMD input in an online fashion, even with incomplete hand observations.The paper proposes HMD-NeMo, a neural network model to generate full body avatar motion from sparse HMD signals. The main capabilities and hypotheses it aims to demonstrate are:- Can accurately predict full body motion including global trajectory from only head and hand positions/orientations. - Can handle scenarios where hands are only partially visible (e.g. due to limited hand tracking camera field-of-view) by filling in plausible motions when hands are missing.- Generates motions in an online, real-time fashion, predicting each pose given only current and past HMD observations.- Achieves state-of-the-art performance on a standard motion capture dataset, demonstrating the approach generalizes well.- Provides intuitive trade-off between motion accuracy and visual plausibility when optimizing pose predictions to match observations.In summary, the main research question is real-time full avatar motion prediction from very sparse HMD inputs, even with incomplete hand observations, which requires making plausible guesses about unobserved body motions. The key hypothesis is that an online neural network model can learn to do this effectively.


## What is the main contribution of this paper?

The main contribution of this paper is proposing HMD-NeMo, a novel approach for generating full-body avatar motion in real-time given sparse input from a head-mounted device (HMD). The key highlights are:- HMD-NeMo is the first unified approach that can generate plausible and accurate full body motion from HMD signals in both motion controller and hand tracking scenarios. Prior work focused only on motion controller scenarios where both hands are fully visible. - It introduces Temporally Adaptable Mask Tokens (TAMTs) to handle missing hand observations in a temporally coherent way. TAMTs use information from previous timesteps to fill in plausible representations when hands go out of view.- The model architecture combines RNNs and self-attention to capture both temporal and spatial relationships between different input signals (head, hands).- Extensive experiments show HMD-NeMo achieves state-of-the-art performance on a large motion capture dataset. It also provides good results even with partial hand visibility.- The paper provides ablation studies analyzing the impact of different components of HMD-NeMo. It also discusses optimization strategies to balance plausibility and accuracy based on application needs.In summary, the key innovation is enabling realistic avatar motion from HMDs even with incomplete hand observations, which removes a major limitation of prior work. The experiments and analyses provide useful insights into generating human motion from sparse on-body sensors.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a neural network method called HMD-NeMo that can generate full-body avatar motions in real time from sparse head and hand tracking data, even when the hands are only partially visible due to limited sensor field of view.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of full body avatar motion generation from sparse inputs:- This paper tackles a very challenging problem of generating full body motion given only head and hand signals from a head-mounted device (HMD). Most prior work relies on more dense observations like images, 2D joints, or marker data. Generating motion from just HMD inputs is more practical but much harder.- A key advance is the proposed method's ability to handle partial hand visibility, which is common with HMD hand tracking. Prior HMD-driven methods assume full visibility of both hands. The temporally adaptable mask tokens allow plausible motion generation even with missing hand observations.- The results significantly outperform recent state-of-the-art methods on the AMASS dataset for both motion controller and hand tracking scenarios. The motion also looks more natural and plausible compared to prior work.- Cross-dataset evaluation demonstrates good generalization ability. The ablation studies provide useful insights into the contributions of the different components like the spatio-temporal encoder, loss functions, etc.- The optimization strategy provides a way to balance motion fidelity and plausibility based on application needs. This is important for deploying such avatar systems in practice.- The model is lightweight and runs in real-time which makes it suitable for interactive avatar animation in mixed reality.Overall, this paper pushes the state-of-the-art for full body motion generation from very sparse HMD inputs. The ability to handle partial hand observations is an important practical contribution. The real-time performance, strong results, and design choices justified by extensive experiments are positives. An interesting direction for future work could be exploring how such avatar motion generation could work on mobile/wearable devices.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring other causes of partial hand visibility beyond hands going out of the FOV, such as hand tracking failures or occlusions. The authors note that their TAMT approach could likely handle these cases as well without modification.- Considering body shape parameters in addition to just pose. The current method focuses only on pose prediction given a default body shape. Incorporating personalized body shape would be an interesting extension.- Optimization of the lower body joints during the post-processing optimization step. Currently only the upper body joints are optimized to match the head and hand observations. Optimizing the lower body as well may improve overall quality. - Combining different loss terms during training to see their synergistic effects. The ablation studies evaluate the loss terms in isolation, but jointly training with various combinations could be explored.- Testing the approach on even more diverse datasets beyond AMASS to analyze generalization.- Reducing optimization time to make it more efficient for real-time usage. There are likely optimizations possible in the implementation to speed it up.- Exploring alternate network architectures and encoder-decoder schemes to improve accuracy and efficiency. The current transformer architecture could likely be improved upon.So in summary, the main suggestions are around enhancing the approach to handle more diverse data and scenarios, improving optimization and efficiency for real-time use, and exploring architectural and algorithmic improvements to the model. Overall the authors propose several worthwhile directions to build on their method and results.
