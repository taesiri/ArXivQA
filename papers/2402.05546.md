# [Offline Actor-Critic Reinforcement Learning Scales to Large Models](https://arxiv.org/abs/2402.05546)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent work has shown that large transformer models trained via supervised behavior cloning (BC) can achieve impressive capabilities on control and robotics tasks. However, BC requires large datasets of expert demonstrations which are expensive and time-consuming to collect. 
- Reinforcement learning (RL) methods can learn from suboptimal data but have not been shown to scale to large models comparable to BC. It is unclear if offline RL objectives exhibit similar scaling laws as supervised learning and if they can be combined with BC to leverage different data quality.

Proposed Solution:
- The paper introduces Perceiver Actor-Critic (PAC), a scalable neural architecture for continuous control. PAC can smoothly interpolate between BC and offline actor-critic RL objectives.
- A simplified offline RL algorithm is proposed that regularizes the policy towards the data distribution. This stabilization allows scaling model sizes up to 1B parameters.
- The architecture uses Perceiver-style cross-attention to enable training on rich sensory inputs while remaining efficient enough for real-time control. Action vectors are used as queries to predict Q-values via cross-attention for improved credit assignment.
- Scaling experiments show that the offline RL objective exhibits power-law scaling behavior analogous to supervised learning, and outperforms BC given sufficient compute.

Contributions:
- First demonstration that offline actor-critic RL objectives can scale to large transformer models without instability issues. Smooth interpolation between BC and RL is possible.
- Introduction of a Perceiver-based architecture for control that can leverage multiple modalities as input and scale to 1B parameters while remaining efficient enough for real-time robot control.
- Scaling analysis establishing similar power-law relations between compute, data and performance for offline RL compared to supervised learning. Offline RL outperforms BC given sufficient compute.
- Experiments on 100+ continuous control tasks demonstrating improved performance over BC baselines. Further improvement is shown by leveraging the built-in Q-function for self-improvement from suboptimal data on a real robot task.

The paper demonstrates that offline RL should be considered a viable alternative to BC for training large control policies, especially when expert demonstrations are not readily available. The proposed method and analysis open up new research aven avenues into scaling RL with large models.
