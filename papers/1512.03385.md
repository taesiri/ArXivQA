# [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that reformulating the representation learning problem as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions, will make it easier to train very deep neural networks. Specifically, the paper hypothesizes that explicitly letting the stacked layers fit a residual mapping rather than directly fit a desired underlying mapping will help address the degradation problem, where accuracy gets saturated and then degrades rapidly as network depth increases.The key idea is that rather than expecting stacked layers to directly approximate the desired mapping, it may be easier to optimize them to learn the residual functions that reconstruct the desired mapping. If identity mappings are optimal, the residual formulation should make it easier for the network to simply drive the residuals to zero rather than try to learn identity mappings from scratch.So in summary, the central hypothesis is that residual learning reformulation will enable successful training of much deeper networks by making it easier to optimize the layers to approximate residual functions rather than unreferenced functions.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a deep residual learning framework to ease the training of very deep convolutional neural networks. Specifically:- It presents a residual learning framework where the stacked layers explicitly fit a residual mapping instead of directly fitting the desired underlying mapping. This reformulation makes it easier to optimize the deep networks. - It provides comprehensive experiments showing that plain networks suffer from degradation as the depth increases, but residual networks can avoid this problem and enjoy accuracy gains from increased depth.- It evaluates deep residual networks with depths up to 152 layers (8x deeper than VGG nets) on the ImageNet dataset. These very deep residual networks advance the state-of-the-art results on ImageNet classification, detection, and localization tasks.- It analyzes the behaviors of residual networks and shows they exhibit smaller response magnitudes compared to their plain counterparts. This supports the hypothesis that residual functions tend to be closer to zero than non-residual functions.- It shows residual learning can be applied to very deep networks with over 1000 layers using CIFAR-10 experiments. The 1202-layer network can still be optimized end-to-end with no optimization difficulties.In summary, the key innovation is the residual learning formulation that eases the training and allows constructing of very deep convolutional networks. This technique led to state-of-the-art results in various vision tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a deep residual learning framework to ease the training of very deep convolutional neural networks by reformulating the layers as learning residual functions with reference to the layer inputs, rather than directly fitting the desired underlying mapping.


## How does this paper compare to other research in the same field?

This paper introduces deep residual networks, a novel neural network architecture that makes use of shortcut connections to facilitate the training of very deep networks. Here are a few key ways it compares to other research in the field of deep neural networks:- It tackles the degradation problem that arises when simply stacking more layers onto a deep network leads to higher training error. The proposed residual connections help with the optimization and allow accurate training of networks over 100 layers deep. This sets it apart from prior "plain" networks that saw degradation with increased depth beyond 10-20 layers.- It shows that residual networks can achieve state-of-the-art accuracy on ImageNet classification with substantially fewer parameters than VGG nets. A 152-layer residual network matches VGG-19 accuracy while having lower complexity. This demonstrates the parameter efficiency of residual networks.- It empirically evaluates very deep residual networks, pushing the boundaries on depth. Prior to this work, the deepest networks with published results had around 30 layers. This paper demonstrates successful training for more than 10x deeper networks.- Residual networks form the foundation of the authors' winning entries in the ILSVRC 2015 and COCO 2015 competition across multiple tracks like classification, detection, localization, etc. This demonstrates the strong transfer learning abilities of these features.- Compared to contemporaneous work like highway networks, residual networks focus more on ease of optimization and do not have gating functions that adaptively change the shortcut connections. The identity shortcuts are parameter-free, making residual networks easier to train at extreme depths.So in summary, this paper makes important contributions in enabling training of vastly deeper networks through residual connections. It shows they achieve excellent accuracy in a parameter-efficient manner, and the features transfer well across tasks. This sparked a lot of follow-up work leveraging residual network architectures.


## What future research directions do the authors suggest?

The authors of the paper suggest several potential future research directions:- Exploring deeper residual networks beyond 152 layers. The authors were able to train networks with over 1000 layers on CIFAR-10, but only experimented with up to 152 layers on ImageNet due to computational constraints. They suggest exploring deeper architectures on larger datasets like ImageNet.- Studying if the degradation problem and optimization difficulties occur in other network architectures besides the "plain" networks examined in the paper. The residual learning framework may also provide improvements for other advanced network architectures.- Investigating other types of nonlinearities besides ReLU. The authors used ReLU activations in their residual networks but suggest exploring other activation functions as well.- Trying other options for shortcut connections besides identity mappings, such as 1x1 convolutions. The identity shortcuts worked well in their experiments but other types of shortcuts may prove useful.- Exploring whether pre-activation helps in residual networks, as it did in plain counterparts. The authors suggest applying batch normalization and activation before the weight layers rather than after.- Developing effective regularization methods for extremely deep networks, to maximize the gains of increased depth. The 1202-layer network did not improve on the 110-layer network on CIFAR-10 likely due to overfitting, indicating the need for better regularization techniques.In summary, the main future directions are developing methods to train even deeper residual networks, studying residual connections in other network architectures, trying new nonlinearity and shortcut types, and improving regularization techniques - with the overarching goal of pushing the capabilities of deep residual learning.


## Summarize the paper in one paragraph.

The paper presents a deep residual learning framework to ease the training of very deep neural networks. The authors propose explicitly reformulating the layers as learning residual functions with reference to the layer inputs, instead of directly learning unreferenced functions as in traditional neural networks. They provide empirical evidence showing residual networks are easier to optimize and can gain substantial accuracy from greatly increased depth. On ImageNet, they evaluate residual nets up to 152 layers deep, which is 8x deeper than VGG nets but with lower complexity. A 152-layer residual net ensemble achieves 3.57% top-5 error on the ImageNet test set and won 1st place in the 2015 ILSVRC image classification challenge. They also analyze residual nets on CIFAR-10 with over 100 layers. The key idea is that residual functions tend to be closer to zero than non-residual functions, making them easier to optimize. Overall, the paper demonstrates the importance of depth for visual representations but shows this requires a residual learning framework to overcome optimization difficulties in very deep networks.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the paper:The paper presents a deep residual learning framework to ease the training of very deep convolutional neural networks. The key idea is to reformulate the layers as learning residual functions with reference to the layer inputs, instead of directly learning unreferenced functions. This is implemented via shortcut connections that perform identity mapping and skip one or more layers. The paper provides comprehensive empirical evidence showing that residual networks are easier to optimize and can gain accuracy from considerably increased depth. For example, on CIFAR-10, a 110-layer residual network is able to be optimized and achieves lower training error than a 20-layer plain network. On ImageNet, the paper develops a 152-layer residual network that achieves better accuracy than shallower VGG nets while being lower in complexity. Ensembles of these deep residual nets obtain state-of-the-art results on ImageNet, COCO, and ImageNet detection/localization tasks.In summary, the paper addresses the degradation problem of deep neural networks via residual learning. By letting layers fit residual mappings with shortcut connections, it is possible to train networks that are substantially deeper than previous models. The developed deep residual nets achieve excellent performance across different visual recognition tasks. The residual learning principle is shown to be generic and is expected to be applicable in other problem domains.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a deep residual learning framework to ease the training of very deep neural networks. The key idea is to reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Specifically, instead of expecting stacked layers to directly fit a desired underlying mapping H(x), the layers are designed to fit a residual mapping F(x):= H(x) - x. The original function thus becomes F(x)+x. The hypothesis is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. This residual learning framework is implemented by using shortcut connections that perform identity mapping and element-wise addition.The paper provides comprehensive empirical evidence showing that deep residual networks are easier to optimize and can gain accuracy from considerably increased depth. On ImageNet, 152-layer deep residual nets are presented which outperform previous networks while having lower complexity. On CIFAR-10, over 100 layer residuals nets are successfully trained, with 1202-layer networks also converging well. The paper further demonstrates excellent generalization performance of deep residual nets on object detection and localization tasks.
