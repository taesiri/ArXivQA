# [The logic of NTQR evaluations of noisy AI agents: Complete postulates   and logically consistent error correlations](https://arxiv.org/abs/2312.05392)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Unsupervised evaluation of noisy AI agents faces an infinite validation chain issue - how to validate the evaluator itself without ground truth labels? 
- Plato's "ship of state" allegory poses a relevant question - how can a group with limited expertise recognize the most qualified expert among them?
- This has parallels to evaluating and selecting the best machine learning models in absence of ground truth.

Proposed Solution:
- Formulate complete algebraic postulates relating statistics of correctness to observed responses for any ensemble of classifiers. 
- These postulates allow deriving logical consistency for any grading scheme based purely on observed responses.
- Finite test size allows for integer/rational solutions, avoiding infinite validation chains.
- Focus on binary classification tests with aligned per-item responses from classifier ensemble.

Key Contributions: 
- Demonstrate postulates for N=1, N=2, and N=3 binary classifiers, highlighting pair correlations.
- Identify flaws in prior algebraic solution for independent classifiers, provide correct solution.  
- Devise algorithm to estimate only logically consistent error correlation between any classifier pair.
- Compare independent evaluator and majority voting, show self-warning for correlation.
- Establish framework and methodology for testing logical consistency of any streaming evaluation.

In summary, the paper makes significant contributions in formally defining complete postulates for unsupervised evaluation of classifiers, enabling safety validations and detecting correlations. The methodology helps increase trust and transparency in monitoring noisy AI systems.
