# [TCNCA: Temporal Convolution Network with Chunked Attention for Scalable   Sequence Processing](https://arxiv.org/abs/2312.05605)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Attention mechanisms in transformers have quadratic computational complexity which limits their ability to process very long sequences. 
- Transformers also underperform on long sequence classification tasks compared to linear recurrent models.
- The recent MEGA model combines linear recurrences with attention in a sub-quadratic architecture, but its core recurrence operation is still a bottleneck.

Proposed Solution:
- Replace the exponential moving average (EMA) recurrence in MEGA with a temporal convolutional network (TCN). TCNs can achieve long receptive fields with shallower networks.  
- This results in the TCNCA model - a TCN combined with chunked attention. TCNCA has linear computational complexity with sequence length.

Main Contributions:
- TCNCA outperforms MEGA on EnWik8 language modeling, achieving lower loss with 1.37x/1.24x faster forward/backward pass during training.
- On long sequence classification tasks in LRA dataset, TCNCA matches MEGA's accuracy while having 1.28x faster inference.
- The dilated convolutions in TCNCA are up to 7.07x/2.86x faster than the parallelized EMA in MEGA for very long sequences. This demonstrates better scalability.
- A simplified version of TCNCA remains competitive with MEGA on associative recall over varying sequence lengths and vocabulary sizes.
- Overall, replacing the EMA in MEGA with a TCN leads to faster training and inference while maintaining accuracy across different tasks involving long sequences.

In summary, the paper demonstrates the potentials of using TCNs instead of recurrences in transformer-based models for improved efficiency and scalability for long sequence modeling without sacrificing too much accuracy.
