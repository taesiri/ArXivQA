# [Multi-task self-supervised learning for Robust Speech Recognition](https://arxiv.org/abs/2001.09239)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop robust speech representations using multi-task self-supervised learning that can generalize well to challenging noisy and reverberant conditions. Specifically, the paper proposes an improved architecture called PASE+ that aims to learn speech features that are invariant to distortions like noise, reverberation, clipping etc. while still capturing linguistically relevant information like phonetic content. The key ideas explored are:- Using an online speech distortion module to contaminate the input with reverberation, noise, clipping etc. during self-supervised training. This acts as a form of data augmentation.- Revising the encoder architecture to better capture speech dynamics using convolutional, recurrent (QRNN) and skip connections. - Refining the set of self-supervised tasks (workers) to include additional regression tasks to estimate more speech features over various contexts as well as binary tasks to capture global sequence-level information.The central hypothesis is that learning representations robust to distortions in a self-supervised manner will generalize better to challenging noisy test scenarios compared to standard hand-crafted speech features like MFCCs. The paper evaluates this on the TIMIT, DIRHA and CHiME-5 datasets which contain noisy and reverberant speech.
