# [Multi-task self-supervised learning for Robust Speech Recognition](https://arxiv.org/abs/2001.09239)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop robust speech representations using multi-task self-supervised learning that can generalize well to challenging noisy and reverberant conditions. Specifically, the paper proposes an improved architecture called PASE+ that aims to learn speech features that are invariant to distortions like noise, reverberation, clipping etc. while still capturing linguistically relevant information like phonetic content. The key ideas explored are:- Using an online speech distortion module to contaminate the input with reverberation, noise, clipping etc. during self-supervised training. This acts as a form of data augmentation.- Revising the encoder architecture to better capture speech dynamics using convolutional, recurrent (QRNN) and skip connections. - Refining the set of self-supervised tasks (workers) to include additional regression tasks to estimate more speech features over various contexts as well as binary tasks to capture global sequence-level information.The central hypothesis is that learning representations robust to distortions in a self-supervised manner will generalize better to challenging noisy test scenarios compared to standard hand-crafted speech features like MFCCs. The paper evaluates this on the TIMIT, DIRHA and CHiME-5 datasets which contain noisy and reverberant speech.


## What is the main contribution of this paper?

The main contributions of this paper are:- The proposal of PASE+, an improved version of PASE (Problem Agnostic Speech Encoder) for robust speech recognition in noisy and reverberant environments. - The development of an online speech distortion module that contaminates the input speech with reverberation, noise, frequency/temporal masking etc. during self-supervised training. This acts as a powerful data augmentation technique.- A revised encoder architecture that combines convolutional neural networks with a quasi-recurrent neural network (QRNN) to better model short and long-term speech dynamics. - Refining the set of self-supervised tasks/workers to encourage better cooperation and learn more robust representations. This includes additional regressors to estimate more acoustic features over various contexts, as well as binary tasks based on local and global information maximization.- Demonstrating that PASE+ significantly outperforms the previous PASE model, as well as standard acoustic features like MFCCs, on challenging datasets like TIMIT, DIRHA and CHiME-5. The features learned by PASE+ in a self-supervised manner are highly transferable to mismatched conditions.So in summary, the key innovation is a self-supervised framework to learn universal robust speech representations by carefully designing the speech encoder architecture and the set of auxiliary tasks, without relying on any manual transcript labels.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes PASE+, an improved multi-task self-supervised learning approach for extracting robust speech representations. The key ideas are contaminating the input speech with distortions, revising the encoder architecture, and refining the self-supervised tasks to encourage better cooperation among workers. The results show PASE+ significantly outperforms the previous PASE model and standard speech features on challenging speech recognition tasks.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in robust speech recognition:- It builds on the authors' previous work on PASE (Problem Agnostic Speech Encoder), which was a novel multi-task self-supervised learning approach for extracting speech representations without manual labels. This paper proposes an improved version called PASE+ with features specifically aimed at improving robustness.- For robustness, PASE+ introduces an online speech distortion module that contaminates the input speech with various types of noise, reverberation, etc. during self-supervised training. This acts as a form of data augmentation. The model must then learn to extract meaningful representations from the distorted signals. - The encoder architecture in PASE+ is revised with additions like skip connections, a QRNN layer for capturing longer context, and increased dimensionality of the representations compared to the original PASE. The self-supervised tasks are also expanded.- Experiments show PASE+ significantly outperforms the previous PASE model and standard hand-crafted features like MFCCs on noisy/reverberant speech recognition tasks. The transferability of the representations to mismatched conditions is also notable.- Compared to other robust feature extraction methods, PASE+ is still novel in its use of multi-task self-supervision and raw waveform input rather than just supervised training on speech features. The incorporation of data augmentation into self-supervision is also a distinguishing factor.- Overall, the results demonstrate self-supervised learned features can surpass hand-engineered ones for robust ASR. The transfer learning abilities are also promising. This represents an impactful direction for robust speech recognition and related domains.
