# [Multi-task self-supervised learning for Robust Speech Recognition](https://arxiv.org/abs/2001.09239)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop robust speech representations using multi-task self-supervised learning that can generalize well to challenging noisy and reverberant conditions. Specifically, the paper proposes an improved architecture called PASE+ that aims to learn speech features that are invariant to distortions like noise, reverberation, clipping etc. while still capturing linguistically relevant information like phonetic content. The key ideas explored are:- Using an online speech distortion module to contaminate the input with reverberation, noise, clipping etc. during self-supervised training. This acts as a form of data augmentation.- Revising the encoder architecture to better capture speech dynamics using convolutional, recurrent (QRNN) and skip connections. - Refining the set of self-supervised tasks (workers) to include additional regression tasks to estimate more speech features over various contexts as well as binary tasks to capture global sequence-level information.The central hypothesis is that learning representations robust to distortions in a self-supervised manner will generalize better to challenging noisy test scenarios compared to standard hand-crafted speech features like MFCCs. The paper evaluates this on the TIMIT, DIRHA and CHiME-5 datasets which contain noisy and reverberant speech.


## What is the main contribution of this paper?

The main contributions of this paper are:- The proposal of PASE+, an improved version of PASE (Problem Agnostic Speech Encoder) for robust speech recognition in noisy and reverberant environments. - The development of an online speech distortion module that contaminates the input speech with reverberation, noise, frequency/temporal masking etc. during self-supervised training. This acts as a powerful data augmentation technique.- A revised encoder architecture that combines convolutional neural networks with a quasi-recurrent neural network (QRNN) to better model short and long-term speech dynamics. - Refining the set of self-supervised tasks/workers to encourage better cooperation and learn more robust representations. This includes additional regressors to estimate more acoustic features over various contexts, as well as binary tasks based on local and global information maximization.- Demonstrating that PASE+ significantly outperforms the previous PASE model, as well as standard acoustic features like MFCCs, on challenging datasets like TIMIT, DIRHA and CHiME-5. The features learned by PASE+ in a self-supervised manner are highly transferable to mismatched conditions.So in summary, the key innovation is a self-supervised framework to learn universal robust speech representations by carefully designing the speech encoder architecture and the set of auxiliary tasks, without relying on any manual transcript labels.
