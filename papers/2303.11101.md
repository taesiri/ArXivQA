# [Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning](https://arxiv.org/abs/2303.11101)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: How can we effectively leverage a large-scale unlabeled open-set to improve self-supervised learning on a fine-grained target dataset?

The key ideas and contributions are:

- Proposes a novel "Open-Set Self-Supervised Learning" (OpenSSL) problem setup, where an unlabeled open-set is available during pretraining along with the fine-grained target dataset. 

- Points out the distribution mismatch issue between the open-set and target dataset in the OpenSSL setup.

- Proposes a "Simple Coreset" (SimCore) sampling algorithm to select a subset from the open-set that is semantically similar to the target dataset.

- Demonstrates through extensive experiments that SimCore significantly improves representation learning performance by sampling an effective coreset from the open-set.

In summary, the core hypothesis is that sampling a semantically similar coreset from the unlabeled open-set can enhance self-supervised pretraining on fine-grained datasets, even when there is a distribution mismatch between the open-set and target data. The SimCore algorithm is proposed to address this hypothesis.
