# [MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision   Transformer with Heterogeneous Attention](https://arxiv.org/abs/2211.13955)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we develop accurate yet efficient Vision Transformers (ViTs) to accelerate ViT inference under the constraints of secure multi-party computation (MPC)?

The key hypothesis is that selectively replacing or simplifying expensive operations in ViT architectures like Softmax can reduce communication overhead and latency during private inference, without compromising model accuracy. 

Specifically, the paper proposes:

- Analyzing different attention mechanisms and identifying efficient yet accurate variants for MPC.

- Developing a heterogeneous attention design space and MPC-aware neural architecture search to find optimized ViT architectures.

- Proposing techniques like knowledge distillation and GeLU optimization to further improve accuracy and efficiency.

Through extensive experiments, the paper shows their proposed MPCViT method outperforms prior ViT architectures and optimizations in accuracy and efficiency for private inference. The central goal is developing privacy-preserving yet performant ViTs for tasks like image classification.


## What is the main contribution of this paper?

 Here is a summary of the main contributions of this paper:

- The paper proposes an MPC-friendly Vision Transformer (ViT) architecture called MPCViT to enable accurate and efficient ViT inference under multi-party computation (MPC). 

- It analyzes different attention mechanisms and identifies Softmax attention as the major latency bottleneck due to expensive operations like exponential, max, and reciprocal in MPC.

- It proposes a heterogeneous attention optimization space with both high-accuracy ReLU Softmax attention and low-latency Scaling attention. 

- It develops an MPC-aware neural architecture search algorithm to explore the heterogeneous attention space for Pareto-optimal tradeoffs between accuracy and latency.

- It further extends MPCViT to jointly optimize Softmax attention and other components like GeLU activation in MLP blocks, named MPCViT+.

- Experiments show MPCViT outperforms prior ViT architectures in accuracy and latency. For example, it achieves 1.9% higher accuracy with 6.2x lower latency compared to baseline ViT on Tiny-ImageNet dataset.

In summary, the key ideas are leveraging NAS to explore mixing expensive but accurate attention with cheap but fast attention, and jointly optimizing multiple components like Softmax and GeLU for efficient private ViT inference. The proposed MPCViT architecture demonstrates superior accuracy and efficiency tradeoffs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

This paper proposes an MPC-friendly Vision Transformer architecture called MPCViT which optimizes Softmax attention using a heterogeneous attention design space and neural architecture search to enable accurate yet efficient ViT inference under secure multi-party computation.


## How does this paper compare to other research in the same field?

 This paper presents an MPC-friendly Vision Transformer (ViT) called MPCViT to enable efficient and accurate ViT inference with secure multi-party computation (MPC). Here are a few key ways it compares to related work in privacy-preserving deep learning:

- Focus on ViT architecture: Most prior work has focused on optimizing CNNs like Delphi, SNL, etc. for MPC. This is one of the first works to specifically optimize the ViT architecture which has very different bottlenecks. 

- Heterogeneous attention optimization: The paper proposes mixing different attention modules like ReLU Softmax and Scaling attention in a single ViT using neural architecture search. This provides flexibility compared to methods like MPCFormer that replace Softmax everywhere.

- Joint optimization of attention and MLP: MPCViT+ further optimizes components like GeLU in MLP blocks along with attention, unlike THE-X or MPCFormer that only simplify attention.

- Differential NAS with real latency: The NAS algorithm directly optimizes based on real MPC latency rather than proxy metrics. This is more accurate than prior NAS methods.

- Accuracy improvements: MPCViT achieves higher accuracy along with lower latency compared to MPCFormer and other baselines. The NAS search leads to architectures better optimized for MPC.

- Applicable to vision tasks: Many prior works like THE-X focus on NLP models like BERT. MPCViT shows stronger results on image datasets like Tiny ImageNet compared to applying those techniques.

In summary, MPCViT pushes ViT optimization for MPC further than prior arts by co-designing NAS, developing heterogeneous attention, and jointly optimizing across Transformer components. The results demonstrate clear accuracy and efficiency benefits over existing methods.
