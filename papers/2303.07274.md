# [Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of   Synthetic and Compositional Images](https://arxiv.org/abs/2303.07274)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1. Can current AI vision-and-language models demonstrate sophisticated visual commonsense reasoning skills on images that purposefully violate commonsense expectations? 

2. Does the introduction of a new dataset of "weird" images, designed specifically to challenge compositional and commonsense reasoning abilities, pose difficulties for state-of-the-art models?

3. How do models perform on tasks like explanation generation, image captioning, cross-modal matching, and visual QA when evaluated on these weird images? Do they lag behind human performance?

4. Can automatic metrics be developed to effectively evaluate model performance on the explanation generation task, and do they correlate well with human judgments?

5. Do the primary challenges posed by this new dataset stem from the synthetic nature of the images or from the intentional violation of commonsense that makes them "weird"?

So in summary, the main hypotheses appear to be that current AI models will struggle on tasks requiring deeper commonsense reasoning when evaluated on this new purpose-built dataset of weird images, especially for the proposed explanation generation task, and that the core difficulty arises from the weirdness rather than the synthetic source of the images. The paper seems aimed at benchmarking performance and highlighting areas where continued progress is needed.


## What is the main contribution of this paper?

 Based on the abstract, it seems the main contribution of this paper is introducing a new dataset called WHOOPS and associated benchmark tasks for evaluating visual commonsense reasoning and compositionality in AI models. The key points are:

- They create a dataset called WHOOPS containing 500 synthetic "weird" images that are designed to challenge models to reason about commonsense and compositionality. 

- The images are created by designers using prompt-based image generation models like Midjourney, DALL-E, and Stable Diffusion. The prompts combine plausible elements in implausible ways to violate commonsense.

- They annotate the images with explanations, captions, underspecified captions, and VQA pairs. 

- They propose benchmark tasks over this data including explanation generation, captioning, cross-modal matching, and VQA. 

- Experiments show current vision-language models like BLIP and GPT-3 still lag behind human performance, demonstrating the dataset is challenging.

- They introduce a new task of generating explanations for why an image is weird, which is very difficult for current models.

- They also propose an automatic evaluation metric for the explanation task that aligns well with human judgments.

So in summary, the key contribution is introducing a new challenging visual commonsense dataset along with benchmark tasks and an evaluation framework to measure progress in this area. The results show current models still have significant room for improvement on these types of compositional and commonsense reasoning tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new dataset called WHOOPS of synthetic images purposefully designed to challenge AI models' ability to reason about visual commonsense and compositionality; it poses tasks like explanation generation over this dataset and shows that current models lag behind humans in performance.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on the WHOOPS! dataset compares to other related work in visual commonsense reasoning:

- The paper introduces a new dataset, WHOOPS!, with images specifically designed to challenge models on compositionality and commonsense. This is different from many prior VQA datasets that use natural images, and allows targeted evaluation of reasoning abilities. 

- The images in WHOOPS! are generated using text-to-image models like DALL-E and Midjourney. This allows the creation of novel, unusual images that would be difficult to obtain otherwise. Other recent datasets like VCR and NLVR2 use only natural images.

- The paper proposes a new task of generating explanations for why an image is weird or unusual. This tests more complex reasoning compared to standard VQA. Recent work like on visual abductive reasoning (e.g. Sherlock dataset) has similar goals but uses different tasks.

- The paper evaluates various current state-of-the-art vision-language models like BLIP, OFA, and CoCa on the new benchmark tasks. Most achieve relatively low performance compared to humans, highlighting room for improvement.

- The benchmark includes both zero-shot evaluations using pretrained models as well as supervised fine-tuning experiments. Many recent papers focus only on the zero-shot setting.

Overall, the WHOOPS! paper introduces a valuable new resource for pushing research forward on visual commonsense reasoning. The novel image generation, new explanation task, and rigorous model evaluation are distinct from prior work. The impressive human performance compared to current AI models indicates significant challenges for future research in this direction.
