# [Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of   Synthetic and Compositional Images](https://arxiv.org/abs/2303.07274)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:1. Can current AI vision-and-language models demonstrate sophisticated visual commonsense reasoning skills on images that purposefully violate commonsense expectations? 2. Does the introduction of a new dataset of "weird" images, designed specifically to challenge compositional and commonsense reasoning abilities, pose difficulties for state-of-the-art models?3. How do models perform on tasks like explanation generation, image captioning, cross-modal matching, and visual QA when evaluated on these weird images? Do they lag behind human performance?4. Can automatic metrics be developed to effectively evaluate model performance on the explanation generation task, and do they correlate well with human judgments?5. Do the primary challenges posed by this new dataset stem from the synthetic nature of the images or from the intentional violation of commonsense that makes them "weird"?So in summary, the main hypotheses appear to be that current AI models will struggle on tasks requiring deeper commonsense reasoning when evaluated on this new purpose-built dataset of weird images, especially for the proposed explanation generation task, and that the core difficulty arises from the weirdness rather than the synthetic source of the images. The paper seems aimed at benchmarking performance and highlighting areas where continued progress is needed.
