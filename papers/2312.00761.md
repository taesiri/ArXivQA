# [Deep Unlearning: Fast and Efficient Training-free Approach to Controlled   Forgetting](https://arxiv.org/abs/2312.00761)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting":

Problem:
- Machine learning models tend to memorize training data, raising privacy concerns. Laws like GDPR give users the right to delete their data. 
- Naively retraining models from scratch after deleting data is computationally prohibitive for large models. 
- Existing unlearning algorithms rely on finetuning steps that are inefficient and require access to the full original training data.
- There is a need for efficient training-free unlearning algorithms that can remove class-specific information from models using very few samples.

Proposed Solution:
- The paper introduces an efficient algorithm to unlearn a target class or group of classes from a trained model.
- It estimates a "Retain Space" and "Forget Space" capturing activations for samples from classes to retain and forget using SVD on layer activations.
- It computes shared information between these spaces and removes it from the Forget Space to isolate class-discriminatory information.
- Finally, it projects model weights orthogonal to this class-discriminatory space to suppress activations and remove class information.

Contributions:
- First demonstration of class unlearning on large-scale ImageNet using a Vision Transformer model with only 1.5% drop in retain accuracy.
- Consistently outperforms prior art across datasets and architectures while being 6x more efficient.
- Provides evidence of correct model behavior after unlearning through membership inference attacks, decision boundary analysis, etc.
- Demonstrates applicability to multi-class one-shot and sequential unlearning scenarios.
- Sample-efficient, fast, requires no finetuning, and can scale to large models and datasets.

In summary, the paper makes training-free controlled forgetting feasible through a projection-based approach that strategically eliminates class-specific information using very few samples. It sets new state-of-the-art for efficiency and scalability in this area.
