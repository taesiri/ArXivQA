# [OMNIINPUT: A Model-centric Evaluation Framework through Output   Distribution](https://arxiv.org/abs/2312.03291)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Traditional data-centric evaluations using predefined datasets can lead to inconsistent results and model rankings. Models may perform well on some datasets but poorly on others.
- Need a model-centric evaluation method that can evaluate models over the entire input space.

Proposed Solution - Model-Centric Evaluation (\our)
- Defines the "entire input space" (e.g. all 28x28 pixel images) 
- Samples representative inputs from the model's output distribution using MCMC
- Annotates samples from different logit score bins to compute precision $r(z)$  
- Plots precision-recall curves over the entire input space

Main Contributions
- Proposes \our, a model-centric evaluation method over the entire input space 
- Can reveal cases of overconfident predictions that are missed by predefined datasets
- Gives insights into model's learned classification criteria and diversity
- Shows generative models can improve alignment with human classification criteria 
- Empirically shows \our requires modest annotation effort and converges quickly
- Discusses differences between human annotations and common generative model metrics

In summary, the paper proposes a novel model-centric evaluation approach that samples and evaluates models over the entire input space to reveal insights not shown on predefined datasets. Key advantages are exposing overconfidence and understanding model criteria.
