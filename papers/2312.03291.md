# [OMNIINPUT: A Model-centric Evaluation Framework through Output   Distribution](https://arxiv.org/abs/2312.03291)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new model-centric evaluation method called Model Scope and Specificity (\our) for evaluating neural network models over the entire input space instead of predefined datasets. \our involves sampling representative inputs from the model's output distribution, binning them by logit values, and then manually evaluating the precision of each bin. Experiments on image and text classifiers demonstrate that \our can effectively estimate model precision and recall over the full input space. Key findings include exposing cases of severe overconfident predictions in CNN classifiers on MNIST, showing higher precision but lower recall for generative models like RES-GEN-MNIST-1, and revealing a preference for inverted background-foreground images in MLP classifiers. The method converges quickly, requiring only 40-50 samples per logit bin. Comparisons to existing generative model metrics also reveal cases where those metrics fail to align with human judgement. Overall, \our enables more comprehensive and reliable evaluation of model quality and reliability in the open-world setting across vision, language, and potentially other modalities.
