# [A critical analysis of self-supervision, or what we can learn from a   single image](https://arxiv.org/abs/1904.13132)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions addressed in this paper are:1. How effective are current self-supervised learning techniques at exploiting the information in large unlabeled image datasets to learn useful feature representations? 2. Can self-supervision with a single or few images plus aggressive data augmentation match the performance of self-supervision with millions of images?3. Do different layers of deep convolutional networks require different amounts of data diversity and image content to learn good features under self-supervision?Specifically, the paper investigates whether self-supervision can learn the first few layers of deep convolutional networks using just a single image and heavy data augmentation, compared to using millions of diverse images. It also studies how the amount of training data affects different layers, trying to characterize which layers depend more on image diversity versus transformations.The central hypothesis seems to be that the first few layers of deep networks learn relatively simple low-level features that may not require massive dataset diversity, and could potentially be learned from just a single image if sufficient data augmentation is used. In contrast, deeper layers learn more complex concepts that likely do require large datasets. The experiments aim to test these hypotheses across different self-supervised techniques.In summary, the paper tries to critically analyze the data efficiency and layer-wise learning dynamics of current self-supervised representation learning methods through controlled experiments using limited training data.
