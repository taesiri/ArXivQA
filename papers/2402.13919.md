# [SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in   Clinical Summarization](https://arxiv.org/abs/2402.13919)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper focuses on the problem of factual inaccuracies (hallucinations) in clinical note summarization generated by large language models (LLMs) like GPT and Llama. Such hallucinations can lead to serious consequences like misdiagnoses. The paper discusses limitations of supervised fine-tuning (SFT) in differentiating between critical errors like hallucinations and minor errors. Recent works have explored human feedback techniques like RLHF, RLAIF, RRHF, and RAFT to address this issue. However, these techniques require large amounts of human-annotated data which is challenging to obtain in clinical domains due to strict privacy regulations.  

Proposed Solution
The paper proposes an innovative pipeline to generate high-quality synthetic imitation edit feedback using GPT-3.5 and GPT-4 to enhance factual consistency in LLM-generated clinical note summarization. The pipeline focuses on edit feedback which mirrors practical scenarios where medical professionals refine AI outputs. The paper generates synthetic preference data in two directions: (1) High→Low: GPT introduces hallucinations given reference summary; (2) Low→High: GPT improves factuality given unaligned LLM summary. The edited summary pairs are used to train Llama or GPT-2 using alignment algorithms like DPO and SALT where high-quality summary is preferred and low-quality one is dispreferred.

Main Contributions
- Proposes first pipeline to generate synthetic imitation edit feedback using GPT-3.5 and GPT-4 to enhance factual accuracy in clinical summarization
- Demonstrates GPT's ability to produce expert-level edit feedback for improving factual consistency of LLM outputs 
- Shows that synthetic edit feedback significantly improves summarization performance and factual accuracy of Llama2 and GPT-2 using DPO and SALT alignment training
- Top model achieves 78% preference for factuality from human evaluators, highlighting efficacy of synthetic edits in enhancing alignment

The summary covers the key aspects of the paper including the problem statement, proposed solution of using GPT to generate synthetic edit feedback, and main contributions around showing improvements in factual accuracy for clinical summarization using alignment techniques. It highlights the novelty of using GPT for expert-level edit feedback generation.
