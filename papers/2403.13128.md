# [AdaFish: Fast low-rank parameter-efficient fine-tuning by using   second-order information](https://arxiv.org/abs/2403.13128)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large pretrained models like BERT have a huge number of parameters, making fine-tuning computationally expensive. 
- Parameter-efficient fine-tuning methods like LoRA constrain updates to a low-rank subspace to reduce computations. But training can still be slow.

Proposed Solution:
- Introduce a generalized Fisher information matrix adapted to the low-rank structure to efficiently approximate second-order Hessian information.
- Present AdaFish algorithm that uses an exponential moving average formulation of the Fisher information as an adaptive preconditioner.
- Show Fisher information matrix is equivalent to Hessian matrix under certain conditions, validating its use for second-order information.

Contributions:
- Develop generalized Fisher information matrix tailored to low-rank fine-tuning that only requires inverting a small matrix.
- Propose AdaFish for fast low-rank fine-tuning using the adaptive Fisher matrix to capture accurate second-order information.
- Prove global convergence of AdaFish and establish iteration complexity bounds.
- Empirically demonstrate AdaFish achieves much faster convergence and better accuracy than AdamW on image classification and NLP tasks.

In summary, the paper introduces an efficient way to incorporate second-order Hessian information into low-rank fine-tuning via an adaptive Fisher information matrix. The proposed AdaFish algorithm enables much faster convergence for parameter-efficient adaptation of large pretrained models.
