# [PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks](https://arxiv.org/abs/2402.04284)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies memory-based dynamic graph neural networks (MDGNNs), an important family of dynamic graph neural networks. MDGNNs leverage a memory module to capture long-term temporal dependencies in dynamic graphs, achieving superior performance. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential processing of events to capture accurate temporal patterns. During batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected, referred to as the "temporal discontinuity" issue. This restricts the effective temporal batch size, limiting data parallelism and reducing flexibility of MDGNNs in applications.

Proposed Solution:
The paper provides a theoretical analysis on the impact of temporal batch size on MDGNN training convergence. Contrary to common belief, smaller batches can introduce significant variance. The concept of "memory coherence" is proposed to measure temporal dependency loss. Based on the analysis, the paper develops PRES, an iterative prediction-correction scheme combined with a memory coherence objective to mitigate the effect of temporal discontinuity. This enables MDGNNs to be trained with larger temporal batches without sacrificing performance.

Main Contributions:
- First theoretical result on the influence of temporal batch size on MDGNN training, showing the surprising advantage of larger batches
- A novel convergence result for MDGNN training, offering insights into the factors impacting the convergence rate
- Proposal of PRES, a new training framework for MDGNNs that enables much larger temporal batches without performance drop
- Extensive experiments demonstrating PRES allows 3-4x larger batch, 3.4x speedup in MDGNN training

The key significance is enhancing the practicality of MDGNNs by tackling a critical efficiency bottleneck, allowing more scalable and flexible applications on large-scale dynamic graphs. The theoretical analysis also offers valuable new understandings.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper studies efficient training of memory-based dynamic graph neural networks, proposing a prediction-correction scheme and memory smoothing objective to mitigate the temporal discontinuity issue arising from large temporal batch sizes to improve scalability.


## What is the main contribution of this paper?

 This paper makes the following key contributions:

1) It provides the first theoretical analysis on the impact of temporal batch size on the training of memory-based dynamic graph neural networks (MDGNNs). Specifically, it shows that contrary to common belief, using a smaller temporal batch size can actually introduce higher variance to the gradient estimate. It also presents a convergence result that relates the memory coherence and variance to the convergence rate.

2) Based on the analysis, it proposes a novel MDGNN training framework called PRES (Predict-to-Smooth) that contains two key components: (i) an iterative prediction-correction scheme to mitigate the variance induced by pending events in large batches, inspired by filtering techniques in control theory, and (ii) a memory smoothing objective that improves convergence by enhancing memory coherence of the model.

3) It validates the analysis and demonstrates the effectiveness of PRES through extensive experiments on benchmark datasets. The results show that PRES allows the use of up to 4x larger batch size during MDGNN training without sacrificing accuracy or convergence rate. This could significantly improve the training efficiency and flexibility of applying MDGNNs in practice.

In summary, this paper provides valuable theoretical insights and an effective solution to address the temporal batch size bottleneck in training MDGNNs, which is an important step towards improving the scalability and applicability of this emerging graph learning paradigm.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Memory-based dynamic graph neural networks (MDGNNs) - A family of dynamic graph neural networks that utilize a memory module to capture long-term temporal dependencies in dynamic graph data.

- Temporal discontinuity - The issue that arises when training MDGNNs with large batch sizes, where pending events within a batch are processed in parallel, losing temporal dependencies. Also referred to as the "temporal batch size problem". 

- Temporal batch - Consecutive events/interactions partitioned from a dynamic graph used as batches during the training of MDGNNs. 

- Pending events - Events within a temporal batch that share common vertices and have temporal ordering. 

- Memory coherence - A measure introduced in the paper to quantify the loss of temporal dependency and its impact on the training convergence of MDGNNs.

- Iterative prediction-correction scheme - A technique proposed in PRES that leverages a prediction model to mitigate the noise induced by temporal discontinuity when using large temporal batches.

- Memory smoothing - A novel learning objective proposed in PRES that aims to directly improve the memory coherence of MDGNNs during training.

- Convergence rate - The paper provides a convergence analysis of MDGNN training procedure and shows how temporal batch size and memory coherence affect it.

- Variance - The paper theoretically demonstrates that contrary to common belief, small temporal batch sizes can introduce significant variance to the MDGNN training procedure.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes an iterative prediction-correction scheme to mitigate the impact of temporal discontinuity. Can you explain in detail how this scheme works and why it is effective for this purpose? What is the inspiration behind this scheme?

2. The paper introduces a new concept called "memory coherence" to measure the alignment of gradients under different memory states. Can you formally define memory coherence and explain its significance? How is it estimated and utilized in the proposed framework?

3. The paper provides a theoretical analysis on the influence of temporal batch size in MDGNN training. Can you summarize the key results and insights from this analysis? What are the implications for designing effective MDGNN training procedures?

4. The proposed framework contains two main components - the prediction-correction scheme and the memory smoothing objective. Can you analyze their connections and explain whether and why they complement each other? 

5. For the prediction model, the paper employs a Gaussian Mixture Model (GMM). What is the rationale behind using a GMM? Have you considered any alternatives and why/why not?

6. The parameter update equations for the GMM utilize some statistical properties to avoid storing full history. Can you explain these equations and the statistical insights behind them?

7. Proposition 1 provides a theoretical guarantee on the variance reduction of the proposed method. Can you state this proposition formally and sketch a proof to validate it? What modeling assumptions are made?

8. How does the proposed framework specifically address the two terms that influence the convergence rate in Theorem 2? What adjustments need to be made to the learning rate?

9. The time and space complexity of the proposed method scales with certain quantities. What are these quantities and what techniques can you propose to further reduce the complexities?

10. The experiments demonstrate significant improvements in terms of training efficiency. In your opinion, what are some limitations of the current experimental study? What additional experiments would you design to further evaluate the method?
