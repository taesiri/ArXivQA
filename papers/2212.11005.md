# [Revisiting Residual Networks for Adversarial Robustness: An   Architectural Perspective](https://arxiv.org/abs/2212.11005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is:

What is the impact of architectural design choices, both at the block level and network level, on adversarial robustness of convolutional neural networks? 

Specifically, the paper systematically studies how factors like block topology, kernel size, activation functions, network depth and width affect the adversarial robustness of residual networks. The goal is to gain a holistic understanding of how architectural components contribute to adversarial robustness and use these insights to design more robust network architectures.

The key hypotheses seem to be:

- Residual connections significantly aid adversarial robustness. 

- Architectural choices like block topology, activation function, and network scaling factors (depth, width) have a significant impact on adversarial robustness, as much or more so than different adversarial training methods.

- There exist effective block designs and compound scaling rules to simultaneously optimize depth and width for improved adversarial robustness.

The paper presents extensive experiments, evaluating over 1200 networks, to provide empirical evidence supporting these hypotheses. The end result is a new residual network architecture called RobustResNets that demonstrates state-of-the-art adversarial robustness.

In summary, the paper aims to bridge the knowledge gap regarding how architectural design impacts adversarial robustness, in order to design more robust deep neural networks. The key hypothesis is that architectural choices are as important, if not more so, than adversarial training methods alone.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It presents a systematic study on the impact of architectural design choices on adversarial robustness. The authors examine both block-level design (topology, activations, etc.) and network-level design (depth, width, compound scaling). 

2. Through extensive experiments, the paper identifies specific architectural principles and components that improve adversarial robustness, such as pre-activation, residual SE blocks, and a compound scaling rule that favors deeper but narrower networks.

3. The authors propose a new residual block called RobustResBlock that consistently outperforms standard residual blocks across datasets and attacks. 

4. The paper introduces RobustResNets, a family of adversarial robust networks built using the proposed RobustResBlocks and compound scaling. These models achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 while using far fewer parameters than standard models.

5. More broadly, the paper demonstrates the significant impact of architecture design on adversarial robustness, and shows that many architectural advances from standard training transfer well to adversarial training with minor modifications. The proposed RobustResNets can serve as a strong baseline architecture for further research.

In summary, the key contribution is a holistic architectural study of adversarial robustness leading to design principles, a high-performance robust block, effective compound scaling, and RobustResNets that advance the state-of-the-art tradeoffs between accuracy, efficiency, and robustness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

This paper presents a systematic study on the impact of architectural design choices like block topology and network scaling factors on adversarial robustness, proposes a new robust residual block and compound scaling rule for building adversarially robust residual networks, and achieves state-of-the-art robust accuracy while using 2x fewer parameters.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related research:

- It takes a more comprehensive approach to studying architecture for adversarial robustness compared to prior works. Rather than focusing on just one aspect like block design or scaling, it systematically analyzes both block-level factors (topology, connections, etc.) and network-level factors (depth, width). 

- It identifies specific architectural principles and components that improve robustness, like pre-activation, bottleneck blocks, and residual SE blocks. Many prior works made observations about architectures but did not extract clear design guidelines.

- It proposes a new residual network architecture, RobustResNet, that combines the identified best practices. This serves as a strong robust baseline that outperforms prior architectures like Wide ResNets. 

- The proposed RobustResNets achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 among methods without extra data. They are also much more compact than prior SOTA models.

- The paper takes a very thorough empirical approach with controlled experiments. Many observations are verified across multiple datasets, attacks, and model scales. This makes the conclusions more reliable.

- It provides useful practical advice for designing robust networks, whereas some prior works like those based on NAS are less applicable.

Overall, this paper makes both conceptual and practical contributions by taking a principled approach to robust architecture design. The design guidelines and strong performance of RobustResNets advance the state-of-the-art in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Further theoretical analysis of the impact of different architectural components on adversarial robustness. The authors note that their empirical observations could inspire more rigorous theoretical analysis, which has been lacking so far. 

- Exploration of adversarial robustness properties for a wider range of architectures beyond residual networks. The authors suggest their work could motivate studies on the robustness of other architectures like VGG, DenseNets, etc.

- Additional ablation studies on other architectural factors like dropout, attention mechanisms, model quantization, etc. The current work focused on topology, scaling, activation functions, etc. but there are other factors that could be systematically studied.

- Exploring if similar observations hold for other domains like NLP. The current work focused on computer vision tasks, so extending it to other domains would be interesting.

- Studying if architectural insights transfer across different threat models, like sparse or imperceptible perturbations. The robustness properties may vary across threat models.

- Developing more advanced compound scaling rules by incorporating additional constraints like memory usage, inference latency, etc. The current work optimized primarily for accuracy and FLOPs. 

- Leveraging learned architectural priors to guide manual architecture search or neural architecture search for robustness. The insights could inform the search space or objectives.

In summary, the authors lay a strong foundation and identify many promising directions for better understanding and designing architectures for improved adversarial robustness through both empirical and theoretical research.


## Summarize the paper in one paragraph.

 The paper presents Revisiting Residual Networks for Adversarial Robustness: An Architectural Perspective. The key points are:

The paper systematically studies the impact of architectural components on the adversarial robustness of residual networks. This includes analyzing different block topologies (basic, bottleneck, inverted bottleneck), arrangements of activation functions (pre-vs post-activation), convolution types (aggregated, hierarchical), squeeze-and-excitation, kernel sizes, and network scaling factors (depth, width, compound scaling). Through extensive experiments, the paper makes the following observations:

- Pre-activation is generally better than post-activation for adversarial robustness. 

- Bottleneck blocks outperform basic blocks used in wide residual networks. Aggregated and hierarchical convolutions also improve robustness when used with bottleneck blocks.  

- Standard squeeze-and-excitation degrades robustness, but a proposed residual SE variant improves it.

- For network scaling, deep and narrow networks are more robust than wide shallow networks. A compound scaling rule of 7:3 ratio between depth and width is identified.

Based on these insights, the paper proposes RobustResBlock, RobustScaling, and RobustResNets, which achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 with 2x fewer parameters than prior architectures. The key contribution is providing a systematic study of how architectural components impact adversarial robustness and using those insights to design improved robust models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a novel residual network architecture called RobustResNet for improved adversarial robustness. The authors systematically study the impact of various architectural components like block topology, activation functions, normalization methods, kernel sizes, and compound scaling of depth and width on adversarial robustness. Based on extensive experiments, they identify design principles that aid adversarial robustness - using pre-activation in residual blocks, a specific bottleneck block topology enhanced with aggregated and hierarchical convolutions and their proposed residual SE module, and compound scaling networks to be deep and narrow. 

Building on these insights, the authors design a robust residual block called RobustResBlock and a compound scaling rule called RobustScaling to construct a family of RobustResNets. Compared to standard Wide ResNets, RobustResNets achieve substantially higher adversarial robustness across datasets and attacks, while having up to 2x fewer parameters. Notably, their RobustResNet-A1 attains over 61% AutoAttack robust accuracy on CIFAR-10 using only 19M parameters, compared to 63-65% for models with 270M+ parameters. The architectural insights and high-performance compact RobustResNets presented make valuable contributions towards designing adversarially robust deep networks.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new residual network architecture called RobustResNet for improved adversarial robustness. The key aspects are:

- They systematically study the impact of architectural components like block topology (basic, bottleneck, inverted bottleneck blocks), activation functions (ReLU, SiLU/Swish, softplus), normalization (BatchNorm, GroupNorm, LayerNorm), and network scaling factors (depth, width) on adversarial robustness. 

- Based on extensive experiments, they identify specific architectural design choices that improve robustness - using pre-activation in residual blocks, bottleneck topology, hierarchical/aggregated convolutions, a new residual SE module, and compound scaling to distribute more depth than width.

- They propose a new residual block called RobustResBlock combining the beneficial design choices. The block uses a bottleneck topology with pre-activation, hierarchical convolutions, and residual SE. 

- They also propose a compound scaling rule called RobustScaling to simultaneously scale depth and width at a 7:3 ratio to maximize robustness for a given computational budget.

- Finally, they assemble RobustResBlocks using RobustScaling into full networks called RobustResNets. These networks achieve state-of-the-art robust accuracy while being much more compact than prior robust models.

In summary, the key novelty is a holistic architectural study to derive design principles for improved adversarial robustness, which are then used to develop the RobustResBlock, RobustScaling, and RobustResNet.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper aims to study the impact of architectural design choices on the adversarial robustness of convolutional neural networks. Specifically, it looks at two main aspects: block-level design (topology, activations, etc.) and network-level design (depth, width, scaling). 

- Most prior work on improving adversarial robustness has focused on developing better adversarial training methods. In contrast, there has been limited analysis on how architectural choices affect robustness. This paper tries to bridge this gap.

- Through extensive experiments, the paper identifies specific architectural designs that improve robustness - e.g. pre-activation, bottleneck blocks, compound scaling of depth and width. 

- Based on these insights, the paper proposes a new residual block design called RobustResBlock and a compound scaling rule called RobustScaling. 

- The proposed RobustResNets using these blocks and scaling achieve state-of-the-art robust accuracy while being more parameter efficient than prior architectures.

In summary, the key focus is on analyzing how architectural choices impact adversarial robustness, in order to design more robust and efficient ConvNet architectures. This provides useful insights beyond just developing better adversarial training procedures.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the abstract and skimming the paper, some of the key terms and keywords associated with this paper include:

- Adversarial robustness - The paper focuses on improving the adversarial robustness of convolutional neural networks.

- Residual networks - The paper specifically studies the impact of architectural design on the adversarial robustness of residual networks. 

- Architectural design - The paper examines architectural design at both the block level (e.g. topology, kernel size, activation) and network level (depth and width scaling).

- Ablative experiments - The paper takes an empirical approach involving extensive ablative experiments to study the impact of different architectural components. 

- Robust residual block - The paper proposes a new residual block design called RobustResBlock to improve adversarial robustness.

- Robust scaling - The paper introduces a compound scaling rule called RobustScaling to scale network depth and width. 

- RobustResNets - The paper presents a new family of residual networks called RobustResNets built using RobustResBlock and RobustScaling.

- AutoAttack - The paper evaluates adversarial robustness using AutoAttack, a strong adversarial attack.

- State-of-the-art - The proposed RobustResNets achieve state-of-the-art adversarial robustness on CIFAR-10 while being more compact than prior architectures.

In summary, the key focus is on studying architecture design for adversarial robustness, with a proposal of RobustResBlock, RobustScaling and RobustResNets that demonstrate improved robustness. The core methodology is through extensive ablative experiments.
