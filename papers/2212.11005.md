# [Revisiting Residual Networks for Adversarial Robustness: An   Architectural Perspective](https://arxiv.org/abs/2212.11005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is:

What is the impact of architectural design choices, both at the block level and network level, on adversarial robustness of convolutional neural networks? 

Specifically, the paper systematically studies how factors like block topology, kernel size, activation functions, network depth and width affect the adversarial robustness of residual networks. The goal is to gain a holistic understanding of how architectural components contribute to adversarial robustness and use these insights to design more robust network architectures.

The key hypotheses seem to be:

- Residual connections significantly aid adversarial robustness. 

- Architectural choices like block topology, activation function, and network scaling factors (depth, width) have a significant impact on adversarial robustness, as much or more so than different adversarial training methods.

- There exist effective block designs and compound scaling rules to simultaneously optimize depth and width for improved adversarial robustness.

The paper presents extensive experiments, evaluating over 1200 networks, to provide empirical evidence supporting these hypotheses. The end result is a new residual network architecture called RobustResNets that demonstrates state-of-the-art adversarial robustness.

In summary, the paper aims to bridge the knowledge gap regarding how architectural design impacts adversarial robustness, in order to design more robust deep neural networks. The key hypothesis is that architectural choices are as important, if not more so, than adversarial training methods alone.
