# [Revisiting Residual Networks for Adversarial Robustness: An   Architectural Perspective](https://arxiv.org/abs/2212.11005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is:

What is the impact of architectural design choices, both at the block level and network level, on adversarial robustness of convolutional neural networks? 

Specifically, the paper systematically studies how factors like block topology, kernel size, activation functions, network depth and width affect the adversarial robustness of residual networks. The goal is to gain a holistic understanding of how architectural components contribute to adversarial robustness and use these insights to design more robust network architectures.

The key hypotheses seem to be:

- Residual connections significantly aid adversarial robustness. 

- Architectural choices like block topology, activation function, and network scaling factors (depth, width) have a significant impact on adversarial robustness, as much or more so than different adversarial training methods.

- There exist effective block designs and compound scaling rules to simultaneously optimize depth and width for improved adversarial robustness.

The paper presents extensive experiments, evaluating over 1200 networks, to provide empirical evidence supporting these hypotheses. The end result is a new residual network architecture called RobustResNets that demonstrates state-of-the-art adversarial robustness.

In summary, the paper aims to bridge the knowledge gap regarding how architectural design impacts adversarial robustness, in order to design more robust deep neural networks. The key hypothesis is that architectural choices are as important, if not more so, than adversarial training methods alone.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It presents a systematic study on the impact of architectural design choices on adversarial robustness. The authors examine both block-level design (topology, activations, etc.) and network-level design (depth, width, compound scaling). 

2. Through extensive experiments, the paper identifies specific architectural principles and components that improve adversarial robustness, such as pre-activation, residual SE blocks, and a compound scaling rule that favors deeper but narrower networks.

3. The authors propose a new residual block called RobustResBlock that consistently outperforms standard residual blocks across datasets and attacks. 

4. The paper introduces RobustResNets, a family of adversarial robust networks built using the proposed RobustResBlocks and compound scaling. These models achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 while using far fewer parameters than standard models.

5. More broadly, the paper demonstrates the significant impact of architecture design on adversarial robustness, and shows that many architectural advances from standard training transfer well to adversarial training with minor modifications. The proposed RobustResNets can serve as a strong baseline architecture for further research.

In summary, the key contribution is a holistic architectural study of adversarial robustness leading to design principles, a high-performance robust block, effective compound scaling, and RobustResNets that advance the state-of-the-art tradeoffs between accuracy, efficiency, and robustness.
