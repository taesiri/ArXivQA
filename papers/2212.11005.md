# [Revisiting Residual Networks for Adversarial Robustness: An   Architectural Perspective](https://arxiv.org/abs/2212.11005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is:

What is the impact of architectural design choices, both at the block level and network level, on adversarial robustness of convolutional neural networks? 

Specifically, the paper systematically studies how factors like block topology, kernel size, activation functions, network depth and width affect the adversarial robustness of residual networks. The goal is to gain a holistic understanding of how architectural components contribute to adversarial robustness and use these insights to design more robust network architectures.

The key hypotheses seem to be:

- Residual connections significantly aid adversarial robustness. 

- Architectural choices like block topology, activation function, and network scaling factors (depth, width) have a significant impact on adversarial robustness, as much or more so than different adversarial training methods.

- There exist effective block designs and compound scaling rules to simultaneously optimize depth and width for improved adversarial robustness.

The paper presents extensive experiments, evaluating over 1200 networks, to provide empirical evidence supporting these hypotheses. The end result is a new residual network architecture called RobustResNets that demonstrates state-of-the-art adversarial robustness.

In summary, the paper aims to bridge the knowledge gap regarding how architectural design impacts adversarial robustness, in order to design more robust deep neural networks. The key hypothesis is that architectural choices are as important, if not more so, than adversarial training methods alone.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It presents a systematic study on the impact of architectural design choices on adversarial robustness. The authors examine both block-level design (topology, activations, etc.) and network-level design (depth, width, compound scaling). 

2. Through extensive experiments, the paper identifies specific architectural principles and components that improve adversarial robustness, such as pre-activation, residual SE blocks, and a compound scaling rule that favors deeper but narrower networks.

3. The authors propose a new residual block called RobustResBlock that consistently outperforms standard residual blocks across datasets and attacks. 

4. The paper introduces RobustResNets, a family of adversarial robust networks built using the proposed RobustResBlocks and compound scaling. These models achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 while using far fewer parameters than standard models.

5. More broadly, the paper demonstrates the significant impact of architecture design on adversarial robustness, and shows that many architectural advances from standard training transfer well to adversarial training with minor modifications. The proposed RobustResNets can serve as a strong baseline architecture for further research.

In summary, the key contribution is a holistic architectural study of adversarial robustness leading to design principles, a high-performance robust block, effective compound scaling, and RobustResNets that advance the state-of-the-art tradeoffs between accuracy, efficiency, and robustness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

This paper presents a systematic study on the impact of architectural design choices like block topology and network scaling factors on adversarial robustness, proposes a new robust residual block and compound scaling rule for building adversarially robust residual networks, and achieves state-of-the-art robust accuracy while using 2x fewer parameters.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related research:

- It takes a more comprehensive approach to studying architecture for adversarial robustness compared to prior works. Rather than focusing on just one aspect like block design or scaling, it systematically analyzes both block-level factors (topology, connections, etc.) and network-level factors (depth, width). 

- It identifies specific architectural principles and components that improve robustness, like pre-activation, bottleneck blocks, and residual SE blocks. Many prior works made observations about architectures but did not extract clear design guidelines.

- It proposes a new residual network architecture, RobustResNet, that combines the identified best practices. This serves as a strong robust baseline that outperforms prior architectures like Wide ResNets. 

- The proposed RobustResNets achieve state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100 among methods without extra data. They are also much more compact than prior SOTA models.

- The paper takes a very thorough empirical approach with controlled experiments. Many observations are verified across multiple datasets, attacks, and model scales. This makes the conclusions more reliable.

- It provides useful practical advice for designing robust networks, whereas some prior works like those based on NAS are less applicable.

Overall, this paper makes both conceptual and practical contributions by taking a principled approach to robust architecture design. The design guidelines and strong performance of RobustResNets advance the state-of-the-art in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Further theoretical analysis of the impact of different architectural components on adversarial robustness. The authors note that their empirical observations could inspire more rigorous theoretical analysis, which has been lacking so far. 

- Exploration of adversarial robustness properties for a wider range of architectures beyond residual networks. The authors suggest their work could motivate studies on the robustness of other architectures like VGG, DenseNets, etc.

- Additional ablation studies on other architectural factors like dropout, attention mechanisms, model quantization, etc. The current work focused on topology, scaling, activation functions, etc. but there are other factors that could be systematically studied.

- Exploring if similar observations hold for other domains like NLP. The current work focused on computer vision tasks, so extending it to other domains would be interesting.

- Studying if architectural insights transfer across different threat models, like sparse or imperceptible perturbations. The robustness properties may vary across threat models.

- Developing more advanced compound scaling rules by incorporating additional constraints like memory usage, inference latency, etc. The current work optimized primarily for accuracy and FLOPs. 

- Leveraging learned architectural priors to guide manual architecture search or neural architecture search for robustness. The insights could inform the search space or objectives.

In summary, the authors lay a strong foundation and identify many promising directions for better understanding and designing architectures for improved adversarial robustness through both empirical and theoretical research.
