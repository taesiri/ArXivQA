# [Spike No More: Stabilizing the Pre-training of Large Language Models](https://arxiv.org/abs/2312.16903)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) based on Transformers often suffer from sudden spikes in loss values (loss spikes) during pre-training, which can degrade performance or cause divergence. 
- The underlying reasons behind these loss spikes are not well understood theoretically.

Proposed Solution:
- The paper provides a theoretical analysis of two key factors that can cause exploding gradients and loss spikes in LLM pre-training:
    1) Rapid amplification of the norm of the residual branch during forward propagation. This happens with typical parameter initialization.
    2) Exploding gradients caused by layer normalization (LN), especially with scaled initialization used in LLMs.
- To prevent both issues, the paper introduces a requirement for the standard deviation of layer inputs to be close to 1. 
- A simple solution is proposed to satisfy this requirement: combining scaled initialization (for the model parameters) with modifying the embeddings to have higher variance (by scaling up or applying LN).

Main Contributions:
- Identified two key theoretical factors behind loss spikes based on gradient explosion in Transformers for LLMs.
- Provided concrete requirements to mitigate both causes of exploding gradients.
- Introduced a simple modification to standard training methods (scaling up embeddings) to meet these requirements and stabilize pre-training.
- Empirically demonstrated that the proposed solution avoids loss spikes and enables more stable and better-performing LLM pre-training, including for models with over 1 billion parameters.

In summary, the key innovation is a theoretical analysis leading to proposed requirements and a simple solution that stabilizes pre-training for state-of-the-art LLMs. The experiments validate that avoiding gradient explosion translates to avoiding loss spikes in practice.
