# [Unsupervised Manifold Linearizing and Clustering](https://arxiv.org/abs/2301.01805)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is how to simultaneously cluster data points and learn a latent linear representation when the data lies on a union of nonlinear manifolds. 

Specifically, the paper aims to address two key challenges:

1. How to transform the data from lying on nonlinear manifolds to lying close to a union of linear subspaces, so that subspace clustering techniques can be applied.

2. How to learn a latent representation that is both discriminative between clusters and preserves diversity within each cluster, without access to ground truth labels. 

The main hypothesis appears to be that optimizing an objective based on maximal coding rate reduction, along with learning a novel doubly stochastic cluster membership matrix, can achieve these two goals in an unsupervised manner.

The paper proposes a method called Manifold Linearizing and Clustering (MLC) to address this problem. The key novelty lies in optimizing the MCR^2 objective over both the latent representation and the doubly stochastic membership matrix.

In summary, the central research question is how to simultaneously cluster data on nonlinear manifolds and learn a structured latent representation, in an unsupervised way. The main hypothesis is that the proposed MLC method can achieve this effectively.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

- Proposing a new method called Unsupervised Manifold Linearizing and Clustering (MLC) for simultaneously clustering data points lying near a union of low-dimensional manifolds and learning a linear representation of the data. 

- Formulating an objective function for MLC based on the Maximal Coding Rate Reduction (MCR) metric, but optimizing over both a representation of the data and a novel doubly stochastic cluster membership matrix. This allows MLC to perform clustering without ground truth labels.

- Providing an efficient parameterization and initialization for the representation and membership variables to allow effective optimization of the non-convex MLC objective. In particular, the membership can be initialized in one shot leveraging structures from a self-supervised representation.

- Demonstrating through experiments on image datasets like CIFAR-10/20/100 and TinyImageNet that MLC can learn semantic and linearly separable representations. It also achieves higher clustering accuracy than prior methods for subspace clustering and deep clustering, especially for datasets with many imbalanced clusters.

In summary, the key novelty seems to be the joint formulation and efficient optimization of the MLC objective to enable fully unsupervised learning of both a representation and clustering. The experimental results help validate that MLC can learn meaningful clusterings and representations from complex real-world data.
