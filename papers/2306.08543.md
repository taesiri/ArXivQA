# [Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on knowledge distillation of large language models (LLMs). The central hypothesis is that standard knowledge distillation objectives based on forward Kullback-Leibler divergence are suboptimal for distilling generative LLMs, and using reverse KL divergence can yield improved student models. Specifically, the paper proposes that minimizing reverse KL allows student models to focus on major modes of the teacher distribution rather than trying to cover all modes. This results in student models that generate more precise and higher quality text.The key research questions addressed are:- How can we effectively distill smaller generative language models from larger teacher models? - Is reverse KL divergence more suitable than forward KL divergence for this task?- Does the proposed "MiniLLM" method outperform standard distillation techniques like sequence-level distillation?- Does MiniLLM scale well across different model sizes and achieve consistent improvements?- What are the benefits of MiniLLM in terms of metrics like exposure bias, calibration, long text generation, etc?So in summary, this paper focuses on knowledge distillation for generative LLMs using reverse KL divergence, and empirically evaluates the proposed MiniLLM method against strong baselines. The central hypothesis is that minimizing reverse KL is better suited for distilling generative models compared to the commonly used forward KL objective.
