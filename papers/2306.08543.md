# [Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on knowledge distillation of large language models (LLMs). The central hypothesis is that standard knowledge distillation objectives based on forward Kullback-Leibler divergence are suboptimal for distilling generative LLMs, and using reverse KL divergence can yield improved student models. Specifically, the paper proposes that minimizing reverse KL allows student models to focus on major modes of the teacher distribution rather than trying to cover all modes. This results in student models that generate more precise and higher quality text.The key research questions addressed are:- How can we effectively distill smaller generative language models from larger teacher models? - Is reverse KL divergence more suitable than forward KL divergence for this task?- Does the proposed "MiniLLM" method outperform standard distillation techniques like sequence-level distillation?- Does MiniLLM scale well across different model sizes and achieve consistent improvements?- What are the benefits of MiniLLM in terms of metrics like exposure bias, calibration, long text generation, etc?So in summary, this paper focuses on knowledge distillation for generative LLMs using reverse KL divergence, and empirically evaluates the proposed MiniLLM method against strong baselines. The central hypothesis is that minimizing reverse KL is better suited for distilling generative models compared to the commonly used forward KL objective.


## What is the main contribution of this paper?

The main contribution of this paper is proposing MiniLLM, a knowledge distillation method for generative language models. Specifically:- It proposes to minimize the reverse KL divergence between the teacher and student language models, rather than the standard forward KL divergence used in most prior knowledge distillation work. Minimizing reverse KL helps the student model focus on capturing the major modes of the teacher distribution and avoid overestimating low probability regions.- It develops an effective optimization approach based on policy gradient to minimize the reverse KL objective, along with techniques like single-step regularization, teacher-mixed sampling, and length normalization to improve training stability, reduce variance, alleviate reward hacking, and eliminate length bias.- It demonstrates strong empirical results by distilling smaller LMs from larger ones on instruction following datasets. MiniLLM models generate more precise responses with higher overall quality, lower exposure bias, better calibration, and improved long text generation ability compared to standard knowledge distillation baselines.- It shows the approach scales well from 120M to 13B parameter models across GPT-2, OPT and LLaMA model families. MiniLLM consistently outperforms baselines, demonstrating it is an effective and scalable knowledge distillation technique for generative LMs.In summary, the key contribution is proposing a novel and effective knowledge distillation method tailored for generative language models by minimizing reverse KL divergence with a tailored optimization approach. Experiments demonstrate it consistently distills high quality smaller LMs across model sizes and families.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes MiniLLM, a knowledge distillation method for generative language models that minimizes the reverse Kullback-Leibler divergence between the teacher and student models. This focuses the student model on learning the major modes of the teacher distribution to generate more precise and higher quality text compared to standard distillation objectives.
