# [Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on knowledge distillation of large language models (LLMs). The central hypothesis is that standard knowledge distillation objectives based on forward Kullback-Leibler divergence are suboptimal for distilling generative LLMs, and using reverse KL divergence can yield improved student models. Specifically, the paper proposes that minimizing reverse KL allows student models to focus on major modes of the teacher distribution rather than trying to cover all modes. This results in student models that generate more precise and higher quality text.The key research questions addressed are:- How can we effectively distill smaller generative language models from larger teacher models? - Is reverse KL divergence more suitable than forward KL divergence for this task?- Does the proposed "MiniLLM" method outperform standard distillation techniques like sequence-level distillation?- Does MiniLLM scale well across different model sizes and achieve consistent improvements?- What are the benefits of MiniLLM in terms of metrics like exposure bias, calibration, long text generation, etc?So in summary, this paper focuses on knowledge distillation for generative LLMs using reverse KL divergence, and empirically evaluates the proposed MiniLLM method against strong baselines. The central hypothesis is that minimizing reverse KL is better suited for distilling generative models compared to the commonly used forward KL objective.


## What is the main contribution of this paper?

The main contribution of this paper is proposing MiniLLM, a knowledge distillation method for generative language models. Specifically:- It proposes to minimize the reverse KL divergence between the teacher and student language models, rather than the standard forward KL divergence used in most prior knowledge distillation work. Minimizing reverse KL helps the student model focus on capturing the major modes of the teacher distribution and avoid overestimating low probability regions.- It develops an effective optimization approach based on policy gradient to minimize the reverse KL objective, along with techniques like single-step regularization, teacher-mixed sampling, and length normalization to improve training stability, reduce variance, alleviate reward hacking, and eliminate length bias.- It demonstrates strong empirical results by distilling smaller LMs from larger ones on instruction following datasets. MiniLLM models generate more precise responses with higher overall quality, lower exposure bias, better calibration, and improved long text generation ability compared to standard knowledge distillation baselines.- It shows the approach scales well from 120M to 13B parameter models across GPT-2, OPT and LLaMA model families. MiniLLM consistently outperforms baselines, demonstrating it is an effective and scalable knowledge distillation technique for generative LMs.In summary, the key contribution is proposing a novel and effective knowledge distillation method tailored for generative language models by minimizing reverse KL divergence with a tailored optimization approach. Experiments demonstrate it consistently distills high quality smaller LMs across model sizes and families.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes MiniLLM, a knowledge distillation method for generative language models that minimizes the reverse Kullback-Leibler divergence between the teacher and student models. This focuses the student model on learning the major modes of the teacher distribution to generate more precise and higher quality text compared to standard distillation objectives.


## How does this paper compare to other research in the same field?

Based on my review, this paper makes several notable contributions compared to prior work on knowledge distillation for language models:- Most existing knowledge distillation methods for language models focus on classification tasks or imitating black-box models like GPT-3 APIs. This paper explores knowledge distillation specifically for generative language models in a white-box setting where the teacher model parameters are available. This is an important direction as generative tasks become more prevalent.- The key innovation is using reverse KL divergence instead of the standard forward KL divergence as the distillation objective. The authors motivate and experimentally show that reverse KL is more suitable for distilling generative language models, as it prevents the student from overestimating low probability regions of the teacher distribution.- The paper proposes an effective optimization algorithm called MiniLLM to minimize the reverse KL objective. It incorporates techniques like single-step regularization, teacher-mixed sampling, and length normalization to improve training stability and avoid common issues like reward hacking.- Experiments across diverse model sizes/families (GPT-2, OPT, LLaMA) validate that MiniLLM consistently improves performance over baselines on instruction following tasks. Benefits like lower exposure bias and better calibration are demonstrated through analysis.- The method appears quite general, working for models ranging from 125M to 13B parameters. Many prior distillation papers focus only on smaller models. This shows the approach can scale up.Overall, the reverse KL objective and MiniLLM optimization seem like compelling ideas for knowledge distillation of large, generative language models. The paper expands knowledge distillation to an important new setting not addressed sufficiently before. The generality across model sizes is also notable.
