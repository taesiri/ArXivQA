# [Beyond Text: Frozen Large Language Models in Visual Signal Comprehension](https://arxiv.org/abs/2403.07874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have made significant progress in natural language processing. However, enabling LLMs to understand visual signals typically requires fine-tuning them on large-scale multi-modal datasets. This is computationally expensive and requires additional image-text pairs. 

Proposed Solution:
This paper proposes a Vision-to-Language (V2L) tokenizer that can map images into the token space of a frozen LLM. This allows the LLM to comprehend visual signals without any fine-tuning, by treating images as "foreign languages". 

The V2L tokenizer uses an encoder-quantizer-decoder structure. It has two quantizers - a local quantizer and a global quantizer, each with its own codebook derived from the LLM vocabulary. The global codebook is expanded using bigrams and trigrams to improve semantics. 

The encoder extracts both local and global features from an image. The quantizers then map these features to tokens from the codebooks. This produces global tokens to capture semantic essence, and local tokens for detailed representations.

These image tokens can then be concatenated with task prompts and fed to a frozen LLM for various understanding and generation tasks in an auto-regressive manner. The method leverages in-context learning to guide the LLM.

Main Contributions:
- Proposes V2L tokenizer to map images into token space of a frozen LLM without any fine-tuning 
- Introduces codebook expansion using n-grams to enrich semantics
- Generates separate global and local tokens for an image
- Achieves strong performance on image recognition, captioning, VQA, inpainting etc with a frozen LLM
- Outperforms previous approaches that also avoid LLM fine-tuning

The method opens up the possibility of a frozen LLM comprehending visual signals, avoiding expensive multi-modal tuning. The V2L tokenizer bridges the gap between visual and language modalities.
