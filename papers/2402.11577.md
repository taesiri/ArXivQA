# [Extensible Embedding: A Flexible Multipler For LLM's Context Length](https://arxiv.org/abs/2402.11577)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Extensible Embedding: A Flexible Multipler For LLM's Context Length":

Problem:
- Large language models (LLMs) require long context to handle critical applications like reading comprehension of long documents. However, existing LLMs have limited context windows (e.g. 4K tokens). 
- Existing solutions like fine-tuning or modifying LLM architectures have downsides like expensive training costs, inferior quality, incompatibility with existing LLMs.

Proposed Solution: 
- Propose "Extensible Embedding", which are token embeddings that represent information from an extensible context scope rather than a single token. This allows packing more context into the LLM's limited window.
- An "extensible embedder" model transforms input chunks into output embeddings, which are downsampled to get the final extensible embeddings at a desired compression rate.
- The embedder is trained via a two-stream auto-regression method that enables comprehensive loss calculation and exceptional sample efficiency. 
- Downsampling rate can be flexibly adjusted at inference time to enable ad-hoc extension of diverse context lengths.

Main Contributions:
- Extensible embedding provides an effective, low-cost method to incorporate long context into LLMs with limited windows.
- The tailored model architecture enables flexible context extension to arbitrary lengths. 
- Two-stream training achieves superior sample efficiency.
- Seamless compatibility as a "plug-in" module without compromising LLM's original capabilities.
- Evaluations show quality improvement in language modeling and understanding with extended context over 100K tokens. Also demonstrates efficiency and compatibility advantages.

In summary, extensible embedding is highlighted for its effectiveness, efficiency, flexibility and compatibility as a method to multiply LLM's usable context length.
