# [Reverse Derivative Ascent: A Categorical Approach to Learning Boolean   Circuits](https://arxiv.org/abs/2101.10488)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is: How can the concept of reverse derivative from category theory be used to develop a gradient-based machine learning algorithm that can learn the parameters of boolean circuit models?The key ideas and contributions are:- The paper introduces a new algorithm called "Reverse Derivative Ascent" (RDA) which is a categorical analogue of gradient-based methods like stochastic gradient descent. - RDA is defined at the abstract level of "reverse differential categories", allowing it to be applied to different model classes beyond just boolean circuits.- The paper shows how to define a reverse derivative operator for boolean circuits compositionally using the theory of reverse differential categories. This allows efficient computation of the reverse derivative compared to a brute-force approach. - A safety condition is introduced to ensure the reverse derivative operator is well-defined on boolean circuits, since they don't form a reverse differential category.- Empirical results are provided demonstrating that RDA can successfully learn parameters of boolean circuit models on benchmark machine learning datasets like Iris and MNIST.So in summary, the central hypothesis is that the categorical concept of reverse derivative can be used to develop an efficient and general gradient-based learning algorithm for boolean circuit models. The paper provides both theoretical groundwork and empirical validation of this idea.


## What is the main contribution of this paper?

This paper introduces a new machine learning algorithm called Reverse Derivative Ascent (RDA). The main contributions are:1. It defines a graphical/categorical framework for computing the reverse derivative of boolean circuits, by relating them to polynomials. This allows defining a reverse derivative operator for boolean circuits, even though they don't form a reverse derivative category. 2. It defines the RDA algorithm abstractly in terms of reverse derivative categories. This allows applying RDA to learn parameters of any model that can be expressed as a morphism in a reverse derivative category, not just boolean circuits.3. It demonstrates RDA can learn useful functions from data by giving experimental results on benchmark machine learning datasets using boolean circuit models. This suggests RDA provides a new way to train models like boolean circuits directly, without needing to extract them from neural networks.In summary, the key novelty is introducing a categorical framework for "gradient-based" learning that allows directly training non-differentiable models like boolean circuits. The empirical results demonstrate this approach is viable and the categorical methodology provides a foundation for applying RDA more broadly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces Reverse Derivative Ascent, a categorical framework for gradient-based machine learning algorithms, which is applied to learn the parameters of boolean circuit models directly from data.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other related research:- This paper takes a categorical approach to developing a gradient-based learning algorithm, in contrast to much of the existing work that focuses on implementations and applications of gradient methods like stochastic gradient descent. The categorical framework allows the authors to define the algorithm at a high level of abstraction.- The key novelty is using the theory of reverse differential categories to define a notion of "reverse derivative" for boolean circuits. This allows direct learning of boolean circuit parameters, unlike typical approaches that train neural nets then extract binarized circuits.- Most prior work on binarized neural networks and learning boolean functions focuses on training standard neural network architectures, then extracting boolean circuits from the learned weights. This paper presents a method to directly learn and optimize the parameters of the boolean circuit model.- The categorical approach connects to some recent work applying category theory to understand backpropagation and gradients, like Fong et al. But this paper focuses specifically on boolean functions and circuits as the model class.- For learning boolean functions, the closest related work seems to be LUTNet and some of the binarized neural network papers that extract boolean circuits after training. The key differences are that this paper 1) works directly on the circuit model rather than extracting it post-training and 2) provides a principled categorical framework.- The empirical results are quite limited compared to state-of-the-art results on MNIST and other datasets. But the purpose seems to be more of a proof of concept rather than achieving maximum performance. More work would be needed to scale up the approach.Overall, I would say the core novel contributions of this paper are the categorical formulation of reverse derivative ascent, the application to direct optimization of boolean circuits, and the connections made to recent category theoretic perspectives on backpropagation. The empirical results are promising but preliminary.
