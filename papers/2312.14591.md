# [Reasons to Reject? Aligning Language Models with Judgments](https://arxiv.org/abs/2312.14591)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) need to be aligned to ensure they follow human values and intentions. Prior work has focused on using scalar rewards for alignment, but rewards have limitations in providing nuanced feedback.

- Judgments that critique model responses in natural language could enable more efficient and effective alignment, but existing methods do not fully capitalize on judgments. 

Proposed Solution:
- The paper introduces Contrastive Unlikelihood Training (CUT), a novel framework to align LLMs using judgments. 

- CUT detects inappropriate tokens based on decreased generation probability when conditioned on an authentic negative judgment instead of a fake positive one. It then applies likelihood training to appropriate tokens and unlikelihood training to inappropriate tokens.

- CUT also contrasts contexts with vs. without judgments to teach the model to generate different responses depending on judgment presence.

Key Contributions:
- First systematic investigation of aligning LLMs with judgments.

- CUT allows fine-grained inappropriate content detection and correction based on judgments.

- Experiments show CUT effectively aligns LLMs in offline setting with little data and online setting where the model iteratively learns from its own judgments.

- Analysis indicates judgments exhibit greater potential than rewards for LLM alignment.

In summary, the paper proposes a novel judgment-based alignment framework CUT that enables precise inappropriate content modification. Experiments and analysis demonstrate judgments are a promising direction for efficient LLM alignment.
