# [Training-time Neuron Alignment through Permutation Subspace for   Improving Linear Mode Connectivity and Model Fusion](https://arxiv.org/abs/2402.01342)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep neural networks trained with stochastic gradient descent (SGD) often converge to functionally similar solutions that are scattered across the parameter space. This causes barriers in the linear mode connectivity (LMC) landscape between solutions.
- Overcoming these LMC barriers is important for understanding deep learning dynamics and improving model fusion techniques.
- Previous post-hoc neuron alignment methods using network permutation are costly for large complex models like vision transformers and language models due to the huge number of potential permutations.

Proposed Solution:
- Hypothesize that learning in a shared permutation subspace during training can align neurons and reduce LMC barriers without extra cost. 
- Validate that pruning at initialization, which breaks symmetry, improves LMC but hurts accuracy at high pruning ratios.
- Propose Training-time Neuron Alignment with Partially Fixed Neurons (TNA-PFN) - a simple yet lossless algorithm that uses a partial gradient mask during training to fix some neuron weights.

Contributions:
- Discover neuron alignment problem from perspective of training time and hypothesize permutation subspaces can improve LMC.
- Propose TNA-PFN algorithm and theoretically + empirically validate it reduces LMC barriers across tasks like computer vision, NLP, etc.
- TNA-PFN excels at wide model fusion apps - helps federated learning algorithms FedPFN & FedPNU; boosts model soup for vision transformers and CoLD fusion for language models.
- Overall, first training-time method for overcoming LMC barriers and enabling scalable model fusion.
