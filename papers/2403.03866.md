# [KIWI: A Dataset of Knowledge-Intensive Writing Instructions for   Answering Research Questions](https://arxiv.org/abs/2403.03866)

## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The paper proposes a new dataset called KIWI (Knowledge-Intensive Writing Instructions) for evaluating the capabilities of language models to serve as writing assistants. Specifically, the dataset contains expert-written instructions for revising and improving paragraph-length answers to research questions, along with model responses and human evaluations.

2) The paper presents an analysis of the KIWI dataset, characterizing the types of instructions issued by experts and measuring how well models are able to follow different instruction types. Key findings are that current models struggle to precisely follow instructions, integrate new information without degrading quality, and avoid making unrequested changes.

3) The paper examines the ability of language models to automatically evaluate whether a response successfully follows an instruction, an increasingly important capability as models are deployed as assistants. Experiments demonstrate that current models lag significantly behind human agreement levels on this challenging task.

In summary, the key contribution is the introduction and analysis of a new dataset for studying instruction following for knowledge-intensive writing tasks. Findings indicate limitations of current models as writing assistants, while the dataset can facilitate future progress.


## What are the keywords or key terms associated with this paper?

 Based on my analysis of the content, some of the key terms and keywords associated with this paper include:

- Instruction following
- Language models (LLMs)
- Writing assistance
- Text editing
- Knowledge-intensive
- Interaction dataset
- Long-form answers 
- Research questions
- Expert annotators
- Revision instructions
- Instruction types (information-seeking, stylistic)
- Answer revisions
- Answer quality evaluation
- Error analysis
- Automatic evaluation 
- In-context learning

These terms encapsulate the main focus and contributions of the paper, which centers around analyzing an expert-annotated dataset of iterative instructions issued to language models to provide writing assistance. The key goals are evaluating current LLMs on following precise revision instructions to compose research-oriented long-form answers, surfaced common issues and limitations, and measuring automatic evaluation capabilities. The terms cover the interactive setting, type of tasks, data characteristics, analysis, and findings.
