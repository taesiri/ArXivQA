# Adapting Language Models to Compress Contexts

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to adapt pre-trained language models to compress long contexts into short summary vectors. The authors propose "AutoCompressors", which are language models trained to recursively summarize segments of text into compact soft prompts that can be provided as context for subsequent segments. The key hypotheses are:1) Language models can learn to compress long documents into summary vectors through an unsupervised training objective of improving language modeling over multiple steps. 2) The generated summary vectors will capture high-level semantic information and be useful for improving language modeling over long contexts as well as other downstream tasks like in-context learning.3) Pre-computing summary vectors for large corpora can enable more efficient inference for applications like retrieval-augmented language modeling and passage re-ranking. The overall goal is developing versatile language models that can utilize long contexts while remaining efficient in terms of computational requirements and inference speed. The AutoCompressor framework is proposed as a simple and inexpensive way of extending the context window of large pre-trained models for improved performance across diverse tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing AutoCompressors, which are language models adapted to compress long contexts into compact summary vectors. These summary vectors can then be used as soft prompts to provide the model with useful context information while reducing computational costs. The key ideas are:- Training an unsupervised objective where the model recursively compresses segments of text into summary vectors, and uses these vectors to condition its predictions on future segments. This encourages the model to store useful information in the summaries.- Using summary accumulation where summary vectors from all previous segments are concatenated, allowing for direct information flow between all segments. - Training with randomized segment lengths, making the model robust to documents of varying lengths.- Demonstrating that the resulting summary vectors capture high-level semantic information and can be used in various applications to improve performance, including long-range language modeling, in-context learning, and retrieval augmentation.- Showing efficiency benefits of pre-computing and caching summary vectors for large corpora, for example to enable efficient inference in retrieval-based settings.In summary, AutoCompressors emerge as a simple but effective technique for extending the context window of language models while also speeding up inference, improving the efficiency and versatility of large pretrained models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes adapting pre-trained language models into AutoCompressors that can recursively summarize long documents into compact summary vectors. These summary vectors can extend the context window of LMs and speed up inference, while retaining useful information for tasks like language modeling and in-context learning. The key ideas are to accumulate summaries across segments and train with randomized segmentation.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on adapting language models to long contexts:- The idea of using summary vectors to compress long contexts is novel. Most prior work has focused on architectural modifications to the Transformer to enable long-range modeling, such as limiting the attention window, sparsifying attention, or using recurrence. This paper introduces a simple prompt tuning method to teach existing LMs to compress contexts into summary vectors.- The proposed training method using randomized segmentation and summary accumulation is intuitive and does not require any architectural changes. This contrasts with other approaches like the Recurrent Memory Transformer which require specialized architectures. The ability to adapt any off-the-shelf LM is advantageous.- The unsupervised training procedure allows AutoCompressors to be trained on a large amount of unlabeled text without task-specific objectives. This could make the summary vectors useful for many downstream tasks, as explored in the paper. Other long-context modeling techniques are often trained on specific end tasks.- The computational advantages of using compact summary vectors versus full attention over long contexts are clearly demonstrated. The gains on tasks like retrieval and ranking when using pre-computed summaries are important practical contributions.- The performance when using summary vectors does not always match full attention over plain text contexts. Closing this gap remains an area for future work, and the limitations around information retention in soft prompts are acknowledged.Overall, AutoCompressors offer a simple and flexible approach for adapting LMs to long contexts, with clear computational benefits. The unsupervised training paradigm and strong performance on various downstream tasks highlight the promise of this direction. However, there is still room for improvement to reach the capabilities of architectural modifications that enable full attention over very long sequences.
