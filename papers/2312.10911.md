# [The Pros and Cons of Adversarial Robustness](https://arxiv.org/abs/2312.10911)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper argues that existing definitions and assessment methods for robustness (both local and global) of machine learning models have critical flaws. Specifically:

- For non-trivial classifiers defined on real-valued features, there exists no globally robust classifier for any distance threshold ε > 0. This is because one can always find two points that have different predictions within an ε-ball.

- Assessing local robustness by random input sampling is invalid because one can always find adversarial examples by picking suitable points. Thus any claims of local robustness based on sampling are incorrect. 

- Consequently, common notions of robustness certification are impossible for non-trivial real-valued classifiers. One can always construct counterexamples that violate robustness guarantees.

Proposed Solutions:
- The paper proposes using robustness checking tools not to verify robustness, but as instruments for computing formal explanations of models by searching for adversarial examples. 

- It introduces the concept of distance-restricted explanations, which have connections to adversarial examples. Algorithms are given that use adversarial search to find such explanations.

- Properties are derived relating distance-restricted explanations to the impossibility of global robustness. New insights are obtained on explaining non-trivial real-valued classifiers.

Main Contributions:
- Identifies fundamental flaws in common notions and assessment methods for local and global robustness of machine learning models

- Shows the impossibility of rigorously certifying robustness for non-trivial classifiers 

- Proposes an alternative viewpoint of using robustness tools for computing formal explanations rather than direct robustness verification

- Introduces distance-restricted explanations and algorithms that leverage adversarial example search

- Derives new properties connecting robustness and explainability, leading to insights on explaining models with no verifiable robustness guarantees


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points made in the paper:

The paper argues that existing definitions and assessments of machine learning model robustness, both global and local, exhibit critical flaws, making robustness guarantees impossible and calling into question much previous work, but notes emerging uses of robustness tools for computing formal explanations reveal positive aspects despite the negative results.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) It uncovers limitations with existing definitions of robustness, both global and local robustness, as well as with efforts towards robustness certification. Specifically, it shows theoretically and experimentally that for non-trivial classifiers defined on real-valued features, there always exist adversarial examples, making perfect robustness impossible.

2) It argues that the common experimental setup used for evaluating robustness, which involves random sampling of inputs, is fundamentally flawed and can only establish robustness by chance.

3) It highlights some positive aspects and emerging uses of robustness tools, especially in computing formal explanations of ML models by navigating adversarial examples. It shows how the connections between robustness and explainability can be leveraged.

4) It provides experimental evidence for the lack of global robustness on binarized neural networks, confirming the theoretical negative results on robustness. 

In summary, while robustness has its limitations, the paper uncovers the shortcomings with current notions and evaluations of robustness, but also points to promising directions by connecting robustness with explainability.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it include:

- Local robustness - The definition of robustness with respect to a specific input point. The paper analyzes limitations of methods for deciding local robustness.

- Global robustness - Robustness assessed over the entire input space. The paper proves no non-trivial classifier can be globally robust. 

- Certified robustness - Methods that aim to provide guarantees of robustness. The paper argues these approaches have inherent flaws.

- Adversarial examples - Inputs crafted to cause misclassification, used to assess robustness. The paper links adversarial examples to explainability.

- Explainability - Providing explanations for model predictions. The paper shows robustness tools can enable efficient explainability algorithms. 

- Distance-restricted explanations - Explanations constrained to a region of feature space. Key to relating robustness and explainability.

- Binarized neural networks (BNNs) - Models used in experiments to demonstrate limitations of robustness definitions.

So in summary, key ideas relate to formal definitions of robustness, connections to explainability, and inherent limits to deciding/certifying robustness revealed by the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper argues that existing definitions of local and global robustness are problematic. What specifically is problematic about them and how might they be improved or redefined to address these issues?

2. The paper claims that the experimental setup used in much prior work for assessing robustness exhibits critical flaws. What specifically are those flaws? How could experiments be designed to better evaluate robustness? 

3. The paper argues that efforts towards robustness certification are futile. What arguments and evidence does it provide to support this claim? Do you find them fully convincing? Why or why not?

4. The paper proposes using automated reasoners and encodings of ML models to search for counterexamples that demonstrate lack of robustness. What are the strengths and limitations of this approach compared to other methods?

5. How does the relationship between adversarial examples and explainability proposed in the paper allow for new algorithms to compute formal explanations of ML models? Explain this connection.

6. Explain the algorithms outlined in the paper for using adversarial example search to find distance-restricted abductive explanations. What are their key steps? How could they be improved?

7. The paper claims "the fact that local robustness does not hold on all points of feature space reveals not only the guaranteed existence of adversarial examples, but also new properties about explanations." Elaborate on what some of these properties are. 

8. What threats to validity of the negative results on robustness does the paper identify? Do you see any other potential issues? How might they be addressed?

9. While arguing against robustness, the paper also claims "the paper details recently proposed uses of robustness tools." What are those promising uses? Do you see other possible applications?

10. The binarized neural networks experiment shows adversarial examples can be constructed efficiently. How might this scale to more complex neural networks? What are the challenges?
