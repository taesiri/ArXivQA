# [Pipeline and Dataset Generation for Automated Fact-checking in Almost   Any Language](https://arxiv.org/abs/2312.10171)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Automated fact-checking in languages other than English remains challenging, mainly due to the lack of annotated training data. Existing datasets in English cannot be easily transferred to other languages. Generating synthetic training data through machine translation introduces significant noise. Manually annotating fact-checking datasets is expensive and time-consuming. 

Proposed Solution: 
The authors propose an automated fact-checking pipeline focused on facilitating implementation in multiple languages. The key idea is to generate synthetic training data using a modified Question Answering for Claim Generation (QACG) scheme that requires only a moderate amount of translated data. 

The pipeline has two main modules - evidence retrieval and claim veracity evaluation. It uses paragraph-level evidence instead of sentences to provide more context. The evidence retrieval module uses the state-of-the-art ColBERTv2 method. The claim veracity module employs natural language inference using XLM-RoBERTa model.

The authors generate QACG datasets in Czech, English, Polish and Slovak using translated question answering data and Wikipedia snapshots. The synthetic claims are based on named entities extracted from Wikipedia paragraphs. Models are trained on aggregated multilingual QACG data.

Contributions:

- Automated fact-checking pipeline focused on multilingual transferability using synthetic data generation

- Modified QACG scheme to generate claims directly from evidence paragraphs using only small translated datasets 

- QACG datasets and trained models released for four languages

- Evaluation of both pipeline modules, including human annotations and difficulty estimation

- Analysis of differences between QACG and existing FEVER datasets

- Prototype FactSearch application with pipeline and user feedback

The proposed pipeline and data generation scheme allows bootstrapping of fact-checking for new languages with limited resources. Both quantitative analysis and user feedback confirm the potential of this approach.
