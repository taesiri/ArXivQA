# [Unified Coarse-to-Fine Alignment for Video-Text Retrieval](https://arxiv.org/abs/2309.10091)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- The paper proposes a unified coarse-to-fine alignment model (UCoFiA) for video-text retrieval. The goal is to jointly leverage coarse-grained (e.g. video/frame level) and fine-grained (e.g. patch/word level) cross-modal alignment to capture both high-level and detailed correspondence between videos and text queries. 

- The model performs video-sentence, frame-sentence, and patch-word alignment to obtain similarity scores at different granularity levels. 

- To handle irrelevant information in the visual features, the paper proposes an Interactive Similarity Aggregation (ISA) module to consider both cross-modal relevance and feature interaction when aggregating similarity vectors/matrices.

- To correct the imbalance issue in similarity scores across videos, the paper applies the Sinkhorn-Knopp algorithm to normalize the marginal similarity of each video before summing the multi-level similarities.

- Experiments show UCoFiA achieves state-of-the-art results on MSR-VTT, ActivityNet, DiDeMo etc. for video-text retrieval, demonstrating the effectiveness of the proposed unified coarse-to-fine alignment approach.

In summary, the central hypothesis is that combining coarse and fine-grained cross-modal alignment in a unified framework can better capture multi-level video-text correspondences and improve video-text retrieval performance. The paper proposes and verifies the UCoFiA model to test this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposing a Unified Coarse-to-fine Alignment (UCoFiA) model for video-text retrieval. The model jointly considers cross-modal correspondence from different granularities - coarse-grained (video-sentence), mid-grained (frame-sentence), and fine-grained (patch-word). 

2. An Interactive Similarity Aggregation (ISA) module that considers both cross-modal relevance and feature interaction when aggregating similarity vectors/matrices to get a single similarity score for each granularity level.

3. A multi-granularity unification module that normalizes the similarity scores from each granularity level using the Sinkhorn-Knopp algorithm before summing them. This helps mitigate issues with over/under-representation of videos in the similarity matrices.

4. Achieving state-of-the-art results on multiple video-text retrieval benchmarks including MSR-VTT, ActivityNet, DiDeMo, MSVD, and VATEX. The model outperforms previous methods by effectively unifying multi-grained alignments between video and text.

In summary, the key contribution is proposing a unified coarse-to-fine cross-modal alignment approach for video-text retrieval that jointly captures high-level scene information as well as detailed correspondence between video patches and text words. The model achieves better alignment by aggregating and normalizing multi-granularity similarities.
