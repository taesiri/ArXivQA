# [Co-Supervised Learning: Improving Weak-to-Strong Generalization with   Hierarchical Mixture of Experts](https://arxiv.org/abs/2402.15505)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts":

Problem:
- Training large pre-trained models requires extensive high-quality labeled data, which is increasingly challenging to acquire as models grow more capable. 
- Existing approaches that use weak supervisors to train strong students show limited effectiveness as the capability gap between teacher and student increases.

Proposed Solution:
- Propose a co-supervised learning (CSL) approach that utilizes multiple weak supervisors with different specializations to collectively supervise a strong student model.  
- Model builds on hierarchical mixture-of-experts, with components tailored for alignment:
   - Progressively alternate between student training and teacher assignment to identify plausible supervisions as student's competence improves.
   - Enforce teacher-student consistency and local-global consistency to conservatively reduce annotation noise.

Key Contributions:  
- Formulate a co-supervised perspective for weak-to-strong generalization using hierarchical ensemble of weak supervisors.
- Propose a teacher assignment method based on student competence to identify appropriate supervisor.
- Introduce noise reduction techniques leveraging model dependencies.
- Demonstrate consistent and significant improvements over single supervisor baseline on OpenAI weak-to-strong benchmark and multi-domain vision tasks.
- Open possibilities for more effective alignment as capability gap between teachers and students increases.

In summary, the paper presents a co-supervised learning approach to improve weak-to-strong generalization for AI alignment by composing multiple weak supervisors. The method alternates between teacher assignment based on student predictions and student training under filtered supervision. Experiments show substantial gains over single supervisor baseline, advancing possibilities for scalable alignment.
