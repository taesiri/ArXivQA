# [Wikiformer: Pre-training with Structured Information of Wikipedia for   Ad-hoc Retrieval](https://arxiv.org/abs/2312.10661)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) have shown great success in NLP tasks, but prior work has not fully utilized the rich structured information in Wikipedia for pre-training retrieval models.

Proposed Solution:
- The paper proposes Wikiformer, a PLM tailored for information retrieval (IR) tasks. It uses four pre-training objectives that leverage the titles, abstracts, hierarchical headings, article relationships, references, hyperlinks, and writing organization of Wikipedia articles.

Key Contributions:
- Simulated Re-ranking (SRR) task uses headings to sample pseudo query-document pairs for ranking.
- Representative Words Identification (RWI) identifies subtitles as key phrases using contrastive learning.  
- Abstract Text Identification (ATI) identifies abstracts as more relevant to titles than other sections.
- Long Text Matching (LTM) matches document-to-document similarity using Wikipedia's "See Also" links.

- Wikiformer outperforms state-of-the-art methods on MS MARCO, TREC-DL 2019, TREC-COVID and legal case retrieval datasets, in both zero-shot and fine-tuned settings.
- Ablation studies show effectiveness of all four pre-training objectives.
- Shows promising performance on vertical domain tasks like biomedical and legal search.

In summary, the key innovation is pre-training objectives tailored for IR by fully exploiting Wikipedia's structured information like headings, abstracts and links. When evaluated extensively, Wikiformer outperforms prior state-of-the-art consistently across tasks.
