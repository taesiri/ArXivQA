# [PAPERCLIP: Associating Astronomical Observations and Natural Language   with Multi-Modal Models](https://arxiv.org/abs/2403.08851)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- There is growing interest in developing custom foundation models for scientific domains like astrophysics to tackle a wide range of tasks. Astrophysics data is multi-modal, with images, spectra, text etc associated with observed objects. 
- Past work has not explored associating astronomical observations with natural language in a joint embedding space. This can enable new ways of querying data.

Method:
- The paper introduces PAPERCLIP, which fine-tunes a pre-trained image-text Contrastive Language-Image Pre-training (CLIP) model on pairs of Hubble Space Telescope images and corresponding proposal abstract texts. 
- The abstracts are optionally summarized into structured captions describing objects/phenomena and science use cases using guided large language model generation.
- A dataset of 31k Hubble images and 4.4k abstracts is constructed. Models are trained with contrastive loss.

Results: 
- Fine-tuned models significantly outperform base CLIP on quantitative retrieval metrics and qualitative retrieval tasks. For example, models can retrieve relevant images for text queries like "dwarf galaxy", "supernova", etc.
- They demonstrate describing images via relevant text associations like objects and science use cases.

Main Contributions:
- First application of associating astronomical observations with natural language by harnessing proposal abstracts and fine-tuning foundation models.
- Demonstrates utility through improved performance on retrieval tasks over off-the-shelf CLIP.
- Opens possibilities like querying astronomical survey data with free-form language.
