# [Learning Polynomial Problems with $SL(2,\mathbb{R})$ Equivariance](https://arxiv.org/abs/2312.02146)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper explores using machine learning to solve polynomial problems like positivity verification and minimization. These are useful primitives with applications across mathematics and engineering, but solving them with classical semidefinite programming scales poorly. The authors propose neural network architectures to learn these tasks from data distributions of polynomials, achieving 10x speedups while retaining accuracy. They observe these problems have symmetries equivariant to the noncompact group SL(2,R), so they design data augmentation and specialized architectures accomodating this structure, including a novel exactly SL(2,R)-equivariant architecture. However, they prove a surprising negative result - there exists an SL(2,R)-equivariant function that cannot be approximated by any sequence of equivariant polynomials. This function is in fact the very analytic center function used in positivity verification, presenting an obstruction to universal SL(2,R)-equivariant architectures. Therefore, the authors compare other techniques like data augmentation and SO(2,R)-equivariance, finding augmentation outperforms full noncompact equivariance. This provides an interesting example where symmetry helps most not through equivariant architectures, but through data augmentation. The paper offers an important practical and theoretical case study on applying machine learning and incorporating symmetries for problems with noncompact groups.
