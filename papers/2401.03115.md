# [Transferable Learned Image Compression-Resistant Adversarial   Perturbations](https://arxiv.org/abs/2401.03115)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent work has shown deep learning models are vulnerable to adversarial attacks. Prior work has focused on attacks on uncompressed images or images compressed with JPEG.  
- With the rise of learned image compression (LIC) methods which have superior performance over traditional compression, there is a need to investigate the robustness of LIC-powered image classification systems (LICCS).
- Key questions: How robust are LICCS against adversarial perturbations? How transferable are attacks across different quality levels and architectures of LIC models?

Proposed Solution:
- Propose an adversarial attack pipeline tailored for LICCS by attacking the reconstructed image output from the LIC module.
- Conduct white-box and black-box attacks on LICCS to evaluate robustness.
- Find black-box attacks have limited transferability across quality levels. 
- Propose a "saliency score-based sampling" method to improve transferability:
  - Calculate attack success rate (ASR) curves between quality levels.
  - Identify combination of levels that maximizes "coverage" of ASR across levels.
  - Use top levels for ensemble attack to improve transferability.

Key Contributions:
- First work to investigate robustness of LICCS to adversarial attacks.
- Evaluation of white-box and black-box attack performance on LICCS.
- Finding that black-box attacks have limited transferability across quality levels.
- Proposed saliency score-based sampling to improve transferability by selective ensemble attack.
- Experiments show improved transferability across quality levels and architectures.

In summary, the paper studied adversarial robustness of LIC-powered classification systems, identified challenges in transferability of attacks, and proposed a sampling method to enable more effective black-box attacks across quality levels and architectures.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points in the paper:

The paper investigates the robustness of learned image compression classification systems (LICCS) against adversarial attacks, proposes a saliency score-based sampling method to generate transferable perturbations across quality levels and architectures, and demonstrates improved attack transferability.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Investigating an adversarial attack pipeline for the Learned Image Compression Classification System (LICCS), utilizing learned image compressors (LICs) as pre-processing modules for the image classification model. This is the first work to investigate the robustness of LICCS.

2) Conducting a series of experiments in white-box and black-box settings to evaluate the robustness of LICCS and the transferability of its adversarial perturbations. 

3) Observing from the black-box results that the neighboring quality levels are more significantly affected when attacking a certain quality level. 

4) Proposing a saliency score-based sampling method to improve the transferability of attacks across different quality levels and architectures by performing ensemble attacks on the most influential quality levels.

5) Demonstrating through experiments that the proposed method effectively enhances the transferability of adversarial perturbations across different quality levels and architectures of learned image compression models.

In summary, the main contribution is investigating the robustness of LICCS against adversarial attacks, and proposing a method to generate transferable adversarial perturbations with limited model access.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Learned Image Compression (LIC)
- Learned Image Compression Classification System (LICCS)  
- Adversarial attack
- Transferability
- Quality levels
- Robustness
- Black-box attack
- White-box attack 
- Ensemble attack
- Saliency score
- Rate-distortion tradeoff

The paper investigates adversarial attacks against image classification systems that utilize learned image compression (LIC) models as pre-processing modules, termed LICCS. It evaluates the robustness and transferability of adversarial examples across different quality levels and architectures of LIC models. A saliency score-based sampling method is proposed to improve transferability by attacking an ensemble of influential quality levels. Key terms like LIC, LICCS, adversarial attack, transferability, quality levels, robustness, black-box attack, etc. feature prominently throughout the paper in this context.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a saliency score-based sampling method to select the most influential quality levels for generating transferable perturbations. How is the saliency score mathematically formulated? Explain the key components and variables involved. 

2. The transferability of adversarial examples across different quality levels of learned image compression models is limited. What underlying insight about the influence of quality levels does the paper reveal through observations?

3. How does the proposed method convert the discrete attack success rate (ASR) points into a continuous representation that covers a spectrum of quality levels? What is the motivation behind this conversion?

4. When generating adversarial examples, the paper ensembles multiple quality level models instead of a single one. Explain why attacking an ensemble of quality levels helps improve transferability. 

5. The saliency score quantifies the collective coverage of multiple quality level models. Explain how the coverage function and integral calculation are used to derive this score. 

6. What are the differences between the proposed saliency score-based sampling method and the baseline random ensemble sampling method? What advantages does the proposed method offer?

7. How does the paper evaluate the transferability of adversarial examples across different architectures of learned image compression models besides across quality levels?

8. What modifications need to be made to the mathematical formulation of a standard adversarial attack to make it compatible with the learned image compression classification system pipeline?

9. When targeted quality level information is unavailable to attackers, how can the proposed sampling method still identify the most influential quality levels to attack?

10. The paper discovers an intriguing phenomenon that omitting the learned image compression module provides some defense against adversarial examples. Analyze the potential reasons behind this defensive effect.
