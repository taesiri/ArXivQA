# [Hierarchical Representations for Efficient Architecture Search](https://arxiv.org/abs/1711.00436)

## What is the central research question or hypothesis that this paper addresses?

This paper introduces a hierarchical genetic representation for neural network architectures and applies it to architecture search. The key research questions it addresses are:1. Can we design an effective search space for neural architectures that reflects how human experts tend to modularize and reuse components? The paper proposes a hierarchical representation where smaller motifs are assembled into larger ones in a layered fashion.2. How well do simple search algorithms like random search and evolution perform if coupled with a well-structured search space? The paper shows these approaches can find competitive architectures, demonstrating the importance of search space design.3. Does the proposed hierarchical representation coupled with evolutionary search advance the state of the art in architecture search? The paper achieves top results on CIFAR-10 among evolutionary methods and competitive performance on ImageNet.In summary, the central hypothesis is that using hierarchical representations to constrain and structure the architecture search space will enable even simple random/evolutionary search methods to discover high-performing neural network designs. The results support this hypothesis and highlight the importance of search space construction in architecture search.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a hierarchical genetic representation for neural network architectures to enable efficient architecture search using evolutionary algorithms. Specifically:- They introduce a hierarchical representation where smaller motifs (subgraphs) are assembled recursively into larger motifs to build the full architecture. This is analogous to the modularized design patterns used in many hand-crafted architectures.- They show that competitive architectures for image classification can be found using simple random search over this hierarchical space, demonstrating the importance of the representation.- They present an evolutionary architecture search method based on tournament selection over the hierarchical genotypes. It allows efficiently discovering architectures that achieve state-of-the-art results among evolutionary techniques on CIFAR-10 and competitive performance on ImageNet.In summary, the key contribution is demonstrating that simple random or evolutionary search can achieve strong performance when combined with well-designed search spaces, enabled by the proposed hierarchical representation. The representation's modularity and reuse of motifs helps drive the search towards high-quality architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an efficient evolutionary architecture search method based on a hierarchical representation to automatically design neural network architectures, demonstrating strong performance on CIFAR-10 and ImageNet compared to other architecture search techniques.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on neural architecture search:- It focuses on using evolutionary algorithms and random search for architecture search, whereas much recent work has used reinforcement learning or Bayesian optimization. The paper shows these simpler methods can be quite effective with a well-designed search space.- The hierarchical representation of architectures is novel compared to most prior work which used more direct encodings. The hierarchy allows modular, repeatable motifs to be discovered.- The search space is more flexible than constrained structures like chains or fixed skip connections used in some other papers. There are complex DAG motifs rather than just sequences.- The scale of architecture search is larger than earlier neuroevolution papers, but smaller and faster than recent RL-based searches like NASNet. Still, performance is very competitive.- Results on CIFAR-10 and ImageNet are state-of-the-art compared to other evolutionary architecture search methods, though slightly behind top RL techniques like NASNet.- The focus is on convolutional network architecture search for image classification. This is a common benchmark task for architecture search techniques.In summary, this paper shows competitive results to other state-of-the-art neural architecture search papers while using simpler random/evolutionary methods by careful search space design. It adapts earlier ideas on evolvable representations to modern deep learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring other evolutionary or random search methods in combination with the hierarchical representation to potentially further improve results. They showed strong results are possible with simple evolution and random search, so more advanced techniques may yield additional gains.- Applying the hierarchical representation and search framework to other tasks beyond image classification, such as objection detection, semantic segmentation, etc. The authors demonstrate the representation's effectiveness on CIFAR and ImageNet image classification, but it could likely be adapted and optimized for other vision tasks.- Investigating other possible constraints or regularizations when designing the search space beyond just the hierarchical representation. The restrictions on the search space are key for efficiency, so exploring other ways to constrain or regularize the space could be useful.- Automating more elements of the search process, such as automatically determining the optimal number of levels, motifs, etc. rather than selecting these manually. Further automating architecture search could improve generalizability.- Exploring transfer learning with cells/motifs found on one dataset to new datasets more thoroughly. The authors demonstrate some transfer from CIFAR to ImageNet, but more work on transfer learning could be impactful.- Combining weight inheritance, network morphisms, and other techniques to further improve search efficiency. The authors do not rely on weight sharing/inheritance but combining this with hierarchical representations could help.- Developing more theoretical understanding of why the hierarchical representation works so well compared to flat representations. This could help guide further improvements to architecture search representations.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper explores efficient neural architecture search methods and shows that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. The authors introduce hierarchical representations for describing neural network architectures, where small motifs are assembled in a hierarchical manner to build larger motifs and eventually the full architecture. On the CIFAR-10 image classification task, their evolutionary architecture search approach achieves a test error of 3.75%, outperforming prior work. Remarkably, competitive results are obtained even with simple random search over the architecture space, demonstrating the importance of the hierarchical representation. When transferred to ImageNet, their best architecture achieves 20.3% top-1 error, which is competitive with state-of-the-art neural architecture search techniques. Overall, the work shows that the combination of well-designed search spaces and simple search algorithms can identify high-performing neural network architectures efficiently.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper explores efficient neural architecture search methods and shows that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. The authors introduce hierarchical representations for describing neural network architectures, where smaller motifs are used as building blocks to form larger motifs in a hierarchical manner. This is analogous to the modularized design patterns used by human experts. The algorithm utilizes evolutionary search with operations like mutation and tournament selection over the hierarchical genotypes to find good architectures. Experiments show that competitive architectures for image classification on CIFAR-10 and ImageNet can be obtained even with simplistic random search over the hierarchical representation space. This demonstrates the importance of search space construction. Further improvements are achieved using evolutionary search, resulting in state-of-the-art performance among evolutionary architecture search techniques on CIFAR-10. The evolved architecture also scales successfully to ImageNet with highly competitive performance. Overall, the work reveals that simple search algorithms like evolution or random search can achieve strong results when combined with well-designed architecture spaces.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a hierarchical genetic representation for neural network architectures that imitates the modularized design patterns commonly used by human experts. Smaller graph motifs are assembled into larger motifs in a hierarchical manner, starting from a set of primitive operations like convolutions and pooling. Evolutionary architecture search is then performed where genotypes representing the neural architectures are mutated to explore the search space. The evolutionary algorithm employs tournament selection and asynchronous distributed implementation. It is shown that competitive architectures can be found for image classification even with simplistic random search, demonstrating the importance of properly constructed search spaces. Further improvements are obtained using the evolutionary technique, achieving state-of-the-art results among evolutionary methods on CIFAR-10 and competitive performance on ImageNet.


## What problem or question is the paper addressing?

The key points about this paper are:- It explores efficient neural architecture search methods, with a focus on evolutionary algorithms. The goal is to automatically discover high-performing neural network architectures for image classification.- It proposes a novel hierarchical genetic representation for neural architectures. The idea is to have motifs at different levels of the hierarchy, where lower-level motifs are used as building blocks for higher-level motifs. This is inspired by the modularized design patterns commonly used in hand-designed architectures.- It shows that competitive results can be achieved even with simple random search or evolutionary algorithms if combined with a well-designed search space. This demonstrates the importance of search space construction for architecture search.- Using the proposed hierarchical representations and evolutionary search, it discovers architectures that achieve state-of-the-art results among evolutionary methods on CIFAR-10 and highly competitive performance on ImageNet.- It also shows strong performance can be achieved with just simple random search over the hierarchical search space, reducing search time from 36 hours down to 1 hour.In summary, the key contribution is demonstrating that simple search algorithms like evolution or random search can scale to architecture search for image classification if used with a hierarchical search space, allowing efficient discovery of high-performing neural architectures. The search space design is crucial.
