# [Generalized End-to-End Loss for Speaker Verification](https://arxiv.org/abs/1710.10467)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research questions/hypotheses addressed in this paper are:

1. Can a new loss function called generalized end-to-end (GE2E) loss make training of speaker verification models more efficient than the previous tuple-based end-to-end (TE2E) loss function?

2. Can the new GE2E loss function update the network in a way that emphasizes challenging examples at each training step, compared to TE2E? 

3. Can the new GE2E loss function train models without needing an initial stage of example selection, unlike TE2E?

4. Can a technique called "MultiReader" enable domain adaptation to train a single model supporting multiple keywords (e.g. "OK Google" and "Hey Google") and dialects?

In summary, the central hypotheses appear to be around a new GE2E loss function improving efficiency, performance, and ability to handle multiple domains compared to the prior TE2E approach. The MultiReader technique is proposed to handle multiple keywords/domains. Experiments aim to validate the superiority of GE2E and MultiReader.
