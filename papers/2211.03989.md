# [$BT^2$: Backward-compatible Training with Basis Transformation](https://arxiv.org/abs/2211.03989)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve backward compatibility in representation learning without hurting the performance of the new model or requiring any form of backfilling of the gallery set. 

Specifically, the paper investigates if expanding the representation space by adding extra dimensions can help reconcile the inherent conflict between backward compatibility and maximizing the performance of the new model. The key hypothesis is that the extra dimensions can be used to store any incompatible information between the old and new representations, thereby avoiding the trade-off between backward compatibility and new model performance.

The paper proposes a novel method called Backward-compatible Training with Basis Transformation ($BT^2$) that exploits a series of learnable basis transformations to effectively utilize the extra dimensions. The central hypothesis is that by cleverly manipulating the basis transformations, the incompatible information can be forced into the additional dimensions while retaining the compatible information in the backward compatible representation. This allows the method to achieve both rigorous backward compatibility and maximize the potential of the new model.

In summary, the paper hypothesizes and shows experimentally that the inherent dilemma between backward compatibility and new model performance can be reconciled by expanding the representation space and strategically exploiting the extra dimensions through basis transformations. The proposed $BT^2$ method is designed to test this hypothesis across diverse settings like data changes, model architecture changes, and modality changes.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It identifies and analyzes the inherent trade-off between backward compatibility and new model performance in representation learning. The paper shows both theoretically and empirically that forcing backward compatibility can limit the potential of the new model. 

2. It proposes a method called Backward Compatible Training with Basis Transformation ($BT^2$) to reconcile this trade-off by strategically adding extra dimensions. $BT^2$ uses a series of learnable basis transformations to isolate incompatible information between the old and new models into the extra dimensions. This allows the dimension used by the old model to focus on backward compatibility while the extra dimensions capture the new model's full potential.

3. The paper demonstrates the advantage of $BT^2$ over other state-of-the-art backward compatible training methods on a diverse set of settings - data change, model change, modality change, and multiple model updates over time. The method consistently outperforms baselines by large margins.

4. The paper extends backward compatible training to new challenging scenarios like major architecture changes (CNN to Transformers), cross-modality retrieval, and long-term model evolution. This opens up new potential applications for the backward compatibility paradigm.

In summary, the key innovation is in strategically exploiting extra dimensions to overcome the inherent trade-off between backward compatibility and new model performance. The paper provides extensive empirical evidence to demonstrate the advantage of the proposed $BT^2$ method.

\masa{Agree with this summary. The main contribution is using extra dimensions strategically through basis transformations to reconcile the tradeoff between new model capacity and backward compatibility. Good concise summary!}

\ming{I agree this is a great summary of the core contributions. Explicitly identifying the inherent tradeoff, proposing BT2 to address it through extra dimensions, and showing strong empirical results on diverse settings are the key highlights. The assistant nicely extracted and concisely summarized the key aspects.}

\yifei{Yes I concur with both Masa and Ming that this is an accurate and concise summary of the core contributions of this work. You successfully identified our motivation of inheret tradeoff, the high level intuition of our proposed method BCT-BX to exploit extra dimensions through basis transformations, as well as the diverse experiments to benchmark our method. Thanks for the great summary! }
