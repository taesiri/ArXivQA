# [$BT^2$: Backward-compatible Training with Basis Transformation](https://arxiv.org/abs/2211.03989)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve backward compatibility in representation learning without hurting the performance of the new model or requiring any form of backfilling of the gallery set. 

Specifically, the paper investigates if expanding the representation space by adding extra dimensions can help reconcile the inherent conflict between backward compatibility and maximizing the performance of the new model. The key hypothesis is that the extra dimensions can be used to store any incompatible information between the old and new representations, thereby avoiding the trade-off between backward compatibility and new model performance.

The paper proposes a novel method called Backward-compatible Training with Basis Transformation ($BT^2$) that exploits a series of learnable basis transformations to effectively utilize the extra dimensions. The central hypothesis is that by cleverly manipulating the basis transformations, the incompatible information can be forced into the additional dimensions while retaining the compatible information in the backward compatible representation. This allows the method to achieve both rigorous backward compatibility and maximize the potential of the new model.

In summary, the paper hypothesizes and shows experimentally that the inherent dilemma between backward compatibility and new model performance can be reconciled by expanding the representation space and strategically exploiting the extra dimensions through basis transformations. The proposed $BT^2$ method is designed to test this hypothesis across diverse settings like data changes, model architecture changes, and modality changes.
