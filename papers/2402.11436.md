# [Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models](https://arxiv.org/abs/2402.11436)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent works show contradictory results on whether large language models (LLMs) can improve themselves through self-feedback. Some show improvements while others show degraded performance.
- The paper hypothesizes that such failure is due to the LLM's bias towards favoring its own generated text, referred to as "self-bias". However, it remains unclear whether this bias exists universally across languages and tasks. 

Proposed Solution:  
- The paper formally defines and quantifies self-bias using two statistics: 
    1) Bias: Measures inflation in LLM's self-evaluation compared to true evaluation. 
    2) Distance skewness: Measures asymmetry in distribution of LLM vs true scores.
- The above statistics are estimated for 6 diverse LLMs on translation, text generation and math tasks across 4 languages.

Key Findings:
- Self-bias was shown to be prevalent in all examined LLMs, getting amplified during iterative self-refinement steps.  
- Though fluency/understandability improves after refinements, intended task-specific improvements do not happen due to false positive objectives from biased self-feedback.
- Larger models and incorporating external feedback are shown to mitigate bias and lead to actual performance gains.

Main Contributions:  
- First work to formally define and quantify self-bias in LLMs using two principled statistics
- Showed universal existence and amplification of self-bias during self-refine across languages and tasks 
- Demonstrated factors contributing to self-bias and approaches to mitigate it to tap into LLM's self-correction capability

In summary, the paper provided novel insights into formally understanding the limitations of LLM's self-feedback, while also pointing out methods to address this which can lead to reliability of self-supervised learning schemes.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper defines and quantifies "self-bias" in large language models, finding it is prevalent across tasks and languages, amplifies over iterative self-refinement, improves fluency but not quality, and can be mitigated by larger models and external feedback.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Formally defining and quantifying the self-bias exhibited by large language models (LLMs) in self-refine pipelines using two statistics: bias estimation and distance skewness estimation.

2. Analyzing six diverse LLMs across three tasks (machine translation, constrained text generation, mathematical reasoning) and four languages. Finding that self-bias is prevalent and amplifies during iterative self-refine.

3. Observing that while self-refine improves fluency and understandability of LLM outputs, it does not necessarily improve quality as specified by the prompts. LLMs tend to favor text that matches their own style. 

4. Proposing two solutions to mitigate self-bias - using larger models and incorporating external feedback with accurate assessments. Showing these can reduce bias and lead to actual performance improvements in downstream tasks.

In summary, the main contribution is formally defining, quantifying and analyzing the self-bias phenomenon in LLMs, demonstrating how it impacts self-refine pipelines, and providing mitigation strategies to reduce this bias.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the key terms and keywords associated with this paper include:

- Self-bias - The paper formally defines and quantifies the tendency of large language models (LLMs) to favor their own text generations.

- Self-refine pipeline - The iterative process where an LLM generates text, evaluates it to provide feedback, and then refines the text based on that feedback. 

- Machine translation - One of the tasks analyzed to demonstrate self-bias, translating text from one language to another.

- Constrained text generation - Another analyzed task, generating text that contains specific concept words. 

- Mathematical reasoning - The third analyzed task, generating mathematical problem solutions.

- Bias estimation - A statistical measure used to quantify the difference between an LLM's predicted quality score and expected/human quality scores.

- Distance skewness - A metric used to measure the asymmetry in the distribution of differences between an LLM's predicted scores and expected scores.

- Self-rewarding pipeline - A related framework that also demonstrates issues with self-bias, using an LLM's own evaluations to provide rewards.

- Mitigation strategies - Increasing model size and incorporating external feedback are analyzed as ways to reduce undesirable self-bias.

The analysis of self-bias across languages, tasks, and models as well as the quantification and mitigation strategies are key contributions discussed in the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) How exactly is the self-bias of a language model defined and quantified in this work? What statistical measures are used to estimate the bias and skewness?

2) Why is quantile mapping used to map the BLEURT scores to human MQM scores? What are the benefits of mapping the distributions instead of using a simple linear transformation? 

3) The external feedback model, InstructScore, is described as providing "oracle" feedback. What specifically does this mean and what information does InstructScore leverage that allows it to provide more accurate assessments?

4) This paper analyzes self-bias across machine translation, constrained text generation, and mathematical reasoning tasks. What key differences exist across these tasks and what commonalities suggest self-bias manifests universally?  

5) What specifically causes the amplification of self-bias during iterative self-refinement? How do false positives in the model's perceived performance improvements relate to this?

6) While quality does not improve, fluency and understandability do increase through self-refine iterations. Why might this occur and does it suggest any alternative perspectives on the benefits of self-refine?

7) The paper hypothesizes LLMs favor text aligned with their style of generation. How is this tested and what results support this claim? What implications does this have?

8) How exactly does increasing model size and incorporating external feedback help mitigate self-bias? What mechanisms allow for the reductions observed?

9) In what ways could the self-rewarding framework, common in instruction tuning approaches, further amplify self-bias? What evidence supports this?

10) What limitations exist in the methodological approach and analyses conducted in this work? What future directions could address these limitations?
