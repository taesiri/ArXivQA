# [VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop a pre-trainable generic visual-linguistic representation that can benefit various downstream visual-linguistic tasks. The key hypotheses are:1) A unified architecture based on Transformers, taking both visual and linguistic elements as input, can effectively aggregate and align multi-modal information.2) Pre-training such a model on large-scale visual-linguistic and text-only corpora can sharpen its capability of aggregating and aligning visual-linguistic clues.3) The pre-trained model can serve as a generic representation for various downstream visual-linguistic tasks. Fine-tuning the pre-trained model with task-specific formatting and objectives can achieve superior performance on tasks like visual question answering, visual commonsense reasoning, and referring expression comprehension.In summary, the central research question is how to develop a generic visual-linguistic representation via pre-training. The key hypotheses are around using a unified Transformer-based architecture and pre-training it on both visual-linguistic and text corpora. The pre-trained model is expected to benefit various downstream tasks through simple fine-tuning.
