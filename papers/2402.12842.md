# [PromptKD: Distilling Student-Friendly Knowledge for Generative Language   Models via Prompt Tuning](https://arxiv.org/abs/2402.12842)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Knowledge distillation (KD) has shown promise for model compression but has not been successfully applied to generative language models like large language models (LLMs). 
- Existing KD methods for text generation are limited as they do not distill "student-friendly" knowledge tailored to the student model's capacity, which has proven useful in classification tasks.
- Modifying large teacher models to extract student-friendly knowledge incurs significant computational costs.

Proposed Solution:
- The paper proposes "PromptKD", which utilizes prompt tuning to enable efficient extraction of student-friendly knowledge from the teacher LLM.
- Prompt tuning only trains a small set of prompt tokens based on guidance from the student model, avoiding expensive full model fine-tuning.
- The prompt triggers the teacher to generate responses at the level the student can comprehend, enabling adaptive teaching.
- Regularization loss ensures prompt-appended teacher distributions remain close to the original.  

Main Contributions:
- First exploration of distilling student-friendly knowledge for generative language models.
- Introduction of prompt tuning to KD research, enabling memory-efficient student-guided teacher tuning.
- PromptKD achieves state-of-the-art performance on instruction-following datasets, outperforming prior KD methods.
- Analysis shows PromptKD successfully alleviates exposure bias throughout training via student-friendly knowledge.
- Efficient approach compatible with large models, only adding 0.0007% parameters as prompts.

In summary, the key innovation is the use of prompt tuning to extract student-friendly knowledge from teacher LLMs for improved KD to student models in generative tasks, outperforming prior KD approaches. The efficiency of prompt-based tuning facilitates scalability while still transferring tailored knowledge.
