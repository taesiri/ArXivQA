# [The Effect of Data Poisoning on Counterfactual Explanations](https://arxiv.org/abs/2402.08290)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the vulnerability of counterfactual explanations to data poisoning attacks. Counterfactual explanations are a popular explainability method that provide recourse by suggesting actionable changes to input features that lead to a different, more favorable model output. However, recent work has shown counterfactuals can be manipulated in various ways. This paper investigates if counterfactual explanations are also vulnerable to data poisoning attacks which add manipulated training data to influence models. 

Proposed Solution:
The paper formalizes data poisoning attacks aimed at increasing the cost of recourse for counterfactual explanations at three levels - locally for a single input, for subgroups, or globally affecting all inputs. The attacks only require access to a prediction interface and counterfactual generation method. The proposed data poisoning algorithm computes diverse and plausible counterfactuals for targeted inputs, then inserts perturbed versions of those counterfactuals with flipped labels as poisoned training data.

Key Contributions:

- Formalizes data poisoning attacks against counterfactual explanations for increasing cost of recourse at local, subgroup, and global levels

- Proves poisoning a 1-NN model on the decision boundary increases local recourse cost 

- Shows recourse cost increase also affects nearby points, enabling attacks with few poisoned points

- Empirically evaluates attacks on multiple classifiers, datasets and counterfactual methods

- Demonstrates counterfactual explanations from state-of-the-art methods are vulnerable to the data poisoning attacks, even with small amounts of poisoned data

- Highlights the need for more robust counterfactual explanation methods and defenses against data poisoning attacks

In summary, the paper identifies and demonstrates a new vulnerability of counterfactual explanations to data poisoning attacks which could seriously undermine their utility by providing manipulated recourse suggestions to users. The findings highlight the need for developing more robust counterfactual explanation techniques.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper studies the vulnerability of counterfactual explanations, a popular explainability method, to data poisoning attacks that can increase the cost of recourse on local, subgroup, and global levels across classifiers, datasets, and counterfactual generation methods.


## What is the main contribution of this paper?

 The main contribution of this paper is formally defining and studying the vulnerability of counterfactual explanations to data poisoning attacks. Specifically, the paper:

1) Formally defines data poisoning attacks aimed at increasing the cost of recourse for counterfactual explanations at three levels - locally for a single instance, for a subgroup of instances, or globally for all instances. 

2) Proposes an algorithm for generating poisoned data points that can be added to the training set to increase the cost of recourse in a targeted region of the data space.

3) Empirically demonstrates the vulnerability of state-of-the-art counterfactual explanation methods and tools to such data poisoning attacks across different classifiers and datasets. Even a small percentage of poisoned training samples is shown to significantly increase the cost of recourse.

4) Discusses the implications of these findings in reducing users' trust in counterfactual explanations and demonstrating the need for more robust counterfactual generation methods as well as defense mechanisms against data poisoning.

In summary, the key contribution is defining and exposing this new vulnerability of counterfactual explanations to data poisoning attacks, both theoretically and empirically.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and keywords associated with this paper include:

- Counterfactual explanations
- Explainable AI (XAI)
- Data poisoning
- Cost of recourse
- Vulnerability 
- Robustness
- Manipulation
- Training data
- Actionable recommendations
- Transparency

The paper studies the vulnerability of counterfactual explanations, which are a popular XAI method, to data poisoning attacks. It formalizes data poisoning techniques to increase the cost of recourse, which refers to the effort/cost associated with executing a counterfactual explanation in the real world. The key finding is that by injecting a small number of poisoned samples into the training data, attackers can significantly increase the cost of recourse for counterfactual explanations generated by state-of-the-art methods. This demonstrates the lack of robustness of these explanations to malicious data manipulations. The paper argues for the need for more robust counterfactual generation methods and defense mechanisms to prevent such attacks from harming end users who rely on the actionable recommendations from counterfactuals.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) How does the proposed data poisoning algorithm approximate the multi-objective optimization problem for finding optimal poison instances? What assumptions does it make?

2) The paper shows theoretically that adding a sample on the decision boundary increases the cost of recourse locally. Does this result generalize beyond 1-NN classifiers? What other types of classifiers might it apply to? 

3) How does the use of diverse counterfactual explanations in the data poisoning algorithm aim to improve its robustness? What are other ways the robustness could be further improved?

4) The paper finds that methods focused on counterfactual plausibility require more poison instances to increase recourse cost. Why might plausibility act as a regularizer here? How else could plausibility help safeguard against data poisoning?

5) Why does the paper argue that small perturbations to training data can be more actionable in practice compared to directly manipulating the training procedure? What ethical considerations come with this?  

6) How does the poisoning algorithm aim to prevent lowering classifier accuracy? Is there a fundamental tradeoff between increasing recourse cost and accuracy here?

7) The paper shows theoretically that increasing recourse for one instance affects nearby instances. How does this motivate focusing poisoning on low recourse cost instances? What distributional assumptions underlie this?

8) Why might increasing difference in recourse cost between sub-groups via poisoning be more challenging than global poisoning? How could the distributions of sub-groups influence this?

9) How do the different levels in Figure 1 conceptually relate to issues around transparency, trust, and fairness in AI systems? How might they necessitate different defense strategies?

10) What new research directions does the paper identify regarding developing more robust counterfactual methods and defending against data poisoning attacks? What open problems remain?
