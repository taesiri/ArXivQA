# [SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates](https://arxiv.org/abs/2303.13582)

## What is the central research question or hypothesis that this paper addresses?

 This paper presents SCADE, a novel technique for reconstructing neural radiance fields (NeRFs) of indoor scenes from sparse, unconstrained 2D views. 

The central hypothesis is that leveraging monocular depth estimation priors with ambiguity modeling can help constrain and improve NeRF reconstruction from sparse views. However, monocular depth estimation is an ill-posed problem with inherent ambiguities. 

The key research questions addressed are:

- How can we model the inherent ambiguity in monocular depth estimation to get useful priors for sparse-view NeRF reconstruction?

- How can we resolve the ambiguity and get a globally consistent scene reconstruction when fusing the ambiguous depth estimates from different views?

- Can ambiguity modeling and multiview fusion improve NeRF reconstruction from sparse views compared to using standard (non-ambiguity-aware) monocular depth priors?

In summary, the central research question is whether explicitly modeling ambiguity in monocular depth can help improve sparse-view NeRF reconstruction through effectively fusing estimates from multiple views. The paper proposes a novel technique SCADE to address this question.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing SCADE, a novel technique to improve neural radiance field (NeRF) reconstruction quality using sparse, unconstrained views of indoor scenes. Specifically:

- They propose using ambiguity-aware monocular depth estimates as priors to constrain NeRF reconstruction. To handle the inherent ambiguities in monocular depth estimation, they model a multimodal distribution over possible depth values using conditional Implicit Maximum Likelihood Estimation (cIMLE). 

- They introduce a novel space carving loss that selects consistent depth modes across different views to resolve ambiguities. This loss matches the modes of the depth distribution from the priors to the modes of the depth distribution along rays modeled by NeRF.

- Experiments show their method enables higher fidelity novel view synthesis compared to vanilla NeRF and NeRFs with other depth-based priors, especially for challenging cases like non-opaque surfaces.

In summary, the key innovation is using multimodal, ambiguity-aware monocular depth priors and a new space carving loss to improve NeRF reconstruction from sparse views of complex real-world indoor scenes. This addresses an important limitation of NeRFs needing many views for high quality results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces SCADE, a novel technique to improve neural radiance field (NeRF) reconstruction quality from sparse, unconstrained views of in-the-wild indoor scenes by leveraging monocular depth priors while accounting for their inherent ambiguities.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in neural radiance fields (NeRFs):

- It focuses on improving NeRF performance under sparse, unconstrained camera views for indoor scenes. Many other works have focused on improving NeRFs given dense input views or constrained camera motion. Tackling sparse, unconstrained views is more challenging.

- It uses monocular depth estimation as a shape prior. Other works have used different shape priors like point clouds from SfM or category-specific shape knowledge. Using monocular depth allows utilizing 2D supervision from single images. 

- It models the ambiguity in monocular depth using a multimodal distribution and resolves ambiguity through a novel space carving loss. Other methods typically use a unimodal Gaussian or mean prediction. Modelling ambiguity is critical for monocular depth.

- The space carving loss operates on depth distributions rather than just moments. This provides richer 3D supervision compared to losses on 2D depth maps. 

- It demonstrates results on real unconstrained indoor scenes which is more challenging than synthetic data or internet photos used in some other works.

Overall, I'd say the key novelties are in using ambiguity-aware monocular depth as a shape prior for NeRFs, the multimodal distribution representation, and the space carving loss for fusing ambiguous depth from multiple views. The results on sparse real indoor scans are also an advance over prior art.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions the authors suggest include:

- Testing SCADE on more challenging datasets with larger domain gaps between the prior training data and the target scenes. The performance of SCADE relies on having a good monocular depth prior, so exploring how to make the priors more robust and generalizable is an important direction.

- Exploring other forms of ambiguity-aware priors beyond depth, such as surface normals or semantics. The core idea of representing ambiguities with multimodal distributions could potentially be applied to other scene priors.

- Extending SCADE to video sequences and dynamic scenes. The current method is designed for static scenes, but modeling scene dynamics over time is an interesting extension.

- Improving the theoretical analysis and understanding of the space carving loss. While empirically it is shown to work well, providing a more rigorous theoretical justification could lead to further improvements. 

- Speeding up the rendering and training. The current approach relies on sampling which can be slow. Faster implementations could enable scaling up to larger scenes.

- Combining SCADE with other forms of priors and regularization techniques for novel view synthesis. Since SCADE provides a way to incorporate ambiguous depth priors, combining it with other useful constraints could further boost performance.

In summary, the main future directions involve 1) improving the generalizability and robustness of the depth priors, 2) extending the core ideas to other ambiguous scene priors and tasks, 3) scaling up the approach, both in terms of scene complexity and theoretical understanding, and 4) combining it with complementary constraints and priors.
