# [SalFoM: Dynamic Saliency Prediction with Video Foundation Models](https://arxiv.org/abs/2404.03097)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Video saliency prediction (VSP) aims to model human gaze patterns in dynamic scenes. It has applications in areas like autonomous driving, surveillance, etc.  
- State-of-the-art VSP methods use spatio-temporal transformers trained on limited data, hurting generalizability. 
- Adapting image foundation models (IFMs) for VSP is challenging as they lack modeling of temporal features.

Proposed Solution:
- The paper proposes SalFoM, the first VSP model powered by a video foundation model (VFM) encoder to capture spatio-temporal video features.
- The encoder uses UnMasked Teacher (UMT), a pure VFM pre-trained on large diverse video data.
- A novel heterogeneous decoder is introduced with 3 branches:
  - Transformer branch to capture long-range spatio-temporal relations
  - 3D conv branch to extract local spatio-temporal features
  - 2D conv branch to focus on spatial relations 
- Branches provide local and global context from different perspectives to reconstruct saliency features.

Main Contributions:
- First VSP model based on a pure VFM encoder to leverage its spatio-temporal representations
- Novel heterogeneous decoder with locality-aware spatio-temporal attention to process VFM encodings 
- State-of-the-art performance on DHF1K benchmark compared to existing VSP methods

The summary covers the key aspects - the problem being addressed, the unique solution proposed using VFMs and a specialized decoder, and the main contributions showing superior performance. Let me know if you need any clarification or have additional questions!
