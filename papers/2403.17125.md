# [The Strong Pull of Prior Knowledge in Large Language Models and Its   Impact on Emotion Recognition](https://arxiv.org/abs/2403.17125)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown promising capabilities for in-context learning (ICL), where they can perform downstream tasks well with just a few demonstrations, without model finetuning. 
- However, recent work has found LLMs may overly rely on their prior knowledge and have difficulty integrating new evidence that contrasts their pre-existing beliefs about a task. This can limit performance on subjective tasks like emotion recognition where human annotations vary widely.

Proposed Solution:  
- The authors design experiments to quantify the "pull" (similarity to prior predictions) and "consistency" (variability across runs) of LLM priors for emotion recognition. 
- They compare ICL performance to "task recognition zero-shot" where labels are randomized, to measure if predictions resemble priors more than ground truth labels.
- They also check if reinforcing priors in the prompt leads to more consistent predictions.

Key Findings:
- LLMs significantly underperform simpler BERT models on emotion recognition. Increased scale does not help.
- LLM predictions reliably resemble zero-shot priors more than ICL labels, especially for larger models, showing a strong pull from inconsistent priors.  
- Reinforcing priors in the prompt improves performance and consistency, indicating priors interfere with learning complex subjective mappings.

Main Contributions:
- Novel experimental design and metrics to quantify strength and consistency of LLM priors.
- Demonstration that larger LLMs suffer more from strong, inconsistent priors for subjective tasks like emotion recognition.
- Evidence that ICL provides little benefit over priors for emotion recognition, and priors severely limit performance.
- Highlights need for caution when interpreting LLM predictions for subjective tasks, and finetuning them to be more adaptable.

In summary, the authors show that large language models tend to default to their own biased knowledge when provided with emotion labels that conflict with their beliefs, rather than learning the new mapping. This effect gets worse with model size, severely limiting their performance on emotion recognition tasks.
