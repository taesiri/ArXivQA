# [SPLADE-v3: New baselines for SPLADE](https://arxiv.org/abs/2403.06789)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The authors released a newer version (v3) of the open-source SPLADE library for dense passage retrieval. They wanted to document the improvements in this new version and provide better baselines to the community.

Proposed Solution:
- They made several enhancements to the SPLADE training procedure, including using more hard negatives per batch, better distillation scores from an ensemble of cross-encoders, combining two distillation losses, and further fine-tuning.

- This led to a new model series called SPLADE-v3. The base model starts from a SPLADE++ checkpoint and is further trained with the improvements mentioned above.

- They also released 3 other variants: SPLADE-v3-DistilBERT (smaller model), SPLADE-v3-Lexical (no query expansion for efficiency), and SPLADE-v3-Doc (no query encoding).

Contributions:

- Extensive evaluations showing SPLADE-v3 statistically significantly outperforms BM25 and previous SPLADE iterations in most cases.

- It achieves over 40 MRR@10 on MS MARCO and 50+ average nDCG@10 on BEIR benchmark.

- It also compares competitively to cross-encoder rerankers like MiniLM and DeBERTaV3 when reranking SPLADE-v3's results.

- The 3 model variants provide different efficiency-effectiveness trade-offs.

Overall, this paper introduced impactful improvements to SPLADE training and established much stronger baselines with the SPLADE-v3 model series, available in the open-source library.
