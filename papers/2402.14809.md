# [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
There is a need for a comprehensive benchmark to systematically evaluate the abilities of large language models (LLMs) in critique and correction reasoning across different tasks. Prior work has focused on specific models and datasets, yielding inconsistent findings. 

Solution:
The paper introduces CriticBench, a new benchmark comprising 15 datasets across 5 reasoning domains - mathematical, commonsense, symbolic, coding and algorithmic. It utilizes responses from 8 LLMs for other models to critique and correct. In total, CriticBench contains 3.8k data instances.

The paper conducts experiments on 17 LLMs, including GPT-3.5, GPT-4, LLaMA, Vicuna, Mistral families and critiquing-focused models. It analyzes performance in:
(1) Generation - answering questions 
(2) Critique - identifying issues in responses 
(3) Correction - improving responses.

Key Findings:
- Linear correlation between generation, critique and correction capabilities
- Performance varies based on task type - better at critiquing and correcting logic tasks vs detail tasks  
- Knowledge inconsistencies decrease with model scale
- Stronger models better at critiquing weaker ones but weaker models can outperform in self-critique

Main Contributions:
- Presents CriticBench - the first comprehensive benchmark for systematically assessing LLM abilities in critique and correction reasoning
- Analyzes performance of 17 LLMs across diverse reasoning tasks 
- Provides insights into nuanced critique-correct capabilities of LLMs to foster more research into LLM self-improvement

The paper demonstrates CriticBench enables in-depth analysis of critique and correction skills across models and tasks, serving as an invaluable benchmark to advance LLM reasoning research.
