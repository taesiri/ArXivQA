# [SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians](https://arxiv.org/abs/2403.10427)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Implicit neural representation methods like NeRF have shown impressive results in novel view synthesis (NVS) and 3D scene reconstruction from unstructured photo collections. However, they are limited by the large computational cost of volumetric rendering which constrains their application in practical scenarios requiring real-time rendering. Recently, 3D Gaussian Splatting (3DGS) emerged as a faster alternative but still suffers from poor performance on unstructured in-the-wild image collections containing varying lighting/weather conditions and transient objects like tourists.

Proposed Solution:
The paper proposes SWAG, the first in-the-wild extension of 3DGS. It expands the capabilities of 3DGS and improves its robustness for unstructured photo collections in two ways:

1) Handles appearance variations by learning an image-dependent color for each Gaussian using an MLP that takes as input the Gaussian's color, spatial encoding of its center, and an image embedding. 

2) Handles transient objects through a learnable image-dependent opacity variation for each Gaussian. This is sampled from a concrete distribution to encourage opacity values to be either 0 or 1 during training. At test time, transient Gaussians can be identified and removed.

Main Contributions:

- First work to extend 3D Gaussian Splatting to unstructured in-the-wild image collections with varying lighting and transient objects
- Achieves state-of-the-art NVS results on Phototourism and NeRF-OSR datasets while having over 10x faster training and real-time rendering
- Enables controllable appearance generation and unsupervised transient object removal from captured scenes
- Demonstrates smooth interpolations between learned appearances and capability to render novel views with the visual appearance of any training image

In summary, the paper proposes a novel adaptation of 3DGS using image-dependent appearance and opacity modeling to achieve superior performance on in-the-wild image collections while retaining the efficiency benefits of splatting-based representations.
