# [DanZero+: Dominating the GuanDan Game through Reinforcement Learning](https://arxiv.org/abs/2312.02561)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the challenge of developing an AI system for the card game GuanDan. GuanDan is a highly complex imperfect information game with features like a large state and action space, long episode length, changing player dynamics within an episode, and a variable number of legal actions. These complexities make it difficult to directly apply standard reinforcement learning algorithms like deep Q-learning or policy gradient methods. Prior attempts using classical techniques like UCT have failed to achieve strong performance.

Method:
The paper proposes using Deep Monte Carlo (DMC) as the core learning algorithm. DMC allows handling large state/action spaces, utilizes action features, and avoids overestimation bias. To address other challenges of GuanDan:

- Meticulously designed 513-dim state features and 54-dim action features to capture relevant information
- Distributed training framework for efficient parallel self-play 
- Special rules to handle the complex "tribute" phase

This results in an agent called DanZero that defeats state-of-the-art rule-based bots.

To further improve performance, the paper enhances DanZero with Proximal Policy Optimization (PPO). Naively applying PPO faces difficulties due to the large action space. So they use the pre-trained DanZero model to provide the top-k candidate actions to PPO. This allows PPO to learn effectively. The resulting agent DanZero+ defeats DanZero and is the new state-of-the-art for GuanDan AI.

Main Contributions:

- First well-performing AI system for the complex card game GuanDan using DMC & distributed self-play
- Method to apply PPO in games with huge action spaces by restricting candidates from a pre-trained model 
- Extensive experiments showing DanZero/DanZero+ significantly outperforming rule-based bots and DanZero+ outperforming DanZero

The paper provides a strong benchmark for GuanDan AI and showcases techniques like leveraging pre-trained models that could be applied to other complex games with large action spaces.
