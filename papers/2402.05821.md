# [Guided Evolution with Binary Discriminators for ML Program Search](https://arxiv.org/abs/2402.05821)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper aims to tackle the problem of efficiently searching for better machine learning (ML) programs, including neural architectures, ML optimizers, RL loss functions etc. Search spaces for these ML components are usually very large and have sparse reward distributions, making random search ineffective and evolution challenging. While learned performance predictors have shown promise for speeding up neural architecture search, they have not been successfully applied to these harder primitive-based search spaces. 

Proposed Solution:
The paper proposes a novel way of combining evolution with a learned binary discriminator that predicts which program from a pair has better performance. This discriminator selects likely better programs without needing to evaluate them, bypassing wasteful computations. Specifically:

1) They encode various ML components like optimizers and RL losses into a directed acyclic graph (DAG) representation that is amenable to graph neural networks (GNNs). 

2) A GNN-based binary discriminator model is trained online during evolution to predict relative performance over pairs of programs.

3) They introduce a mutation strategy called PAM-RT that uses this discriminator to guide mutations - it continually generates and scores mutations until finding one likely better than its parent.

Main Contributions:

- Show that binary discriminators are much more accurate for ranking than regression models in these search spaces. Identify best GNN architecture.

- Propose a novel way of combining binary discriminators with evolution via the PAM-RT mutation strategy. Demonstrate PAM-RT provides faster convergence over regularized evolution and other baselines on diverse ML program search problems.

- Show benefits of online training and using the discriminator directly for hill climbing. Analyze tradeoffs w.r.t exploitation and exploration.

- Introduce a unified DAG representation for encoding and learning over various ML components like optimizers, loss functions etc. Show benefits of transfer learning.

In summary, this paper makes significant contributions towards effectively using learned guidance to improve sample efficiency of evolution for searching complex ML programs. The proposed techniques provide an over 3x speedup on multiple hard search problems.
