# [On-Policy Model Errors in Reinforcement Learning](https://arxiv.org/abs/2110.07985v2)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to mitigate model bias in model-based reinforcement learning (MBRL) algorithms. Specifically, the paper proposes a new method called "on-policy corrections" (OPC) that aims to combine the strengths of model-free and model-based RL to reduce both on-policy and off-policy errors in MBRL. 

The key hypotheses are:

1. The learned dynamics model and replay buffer in MBRL have complementary strengths and weaknesses. The replay buffer has low error for on-policy data but high off-policy error. The learned model can generalize better to new actions but has compounding errors for long multi-step predictions. 

2. By using the learned model to predict how transitions would change for a new state-action pair and correcting the transitions from a replay buffer, OPC can retain the on-policy accuracy of the replay buffer while improving generalization like the learned model.

3. OPC can significantly reduce the on-policy model error compared to standard MBRL methods like MBPO, leading to better sample efficiency and asymptotic performance.

4. OPC is particularly beneficial in stochastic environments where the learned dynamics model has greater difficulty accurately capturing the environment dynamics.

In summary, the main hypothesis is that OPC combines the strengths of model-free replay and learned models to create a hybrid approach that outperforms standard MBRL methods. The paper aims to demonstrate this theoretically and empirically on continuous control tasks.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new method called "on-policy corrections" (OPC) for model-based reinforcement learning (MBRL). The key idea is to combine a learned dynamics model with observed on-policy transitions from a replay buffer in order to improve the model's ability to generate realistic state distributions for long-term predictions. 

Specifically, the paper makes the following contributions:

- Introduces the OPC method which uses the learned model to predict how the transitions in the replay buffer would change for new state-action pairs. This allows retaining the on-policy data distribution while also generalizing to new policies via the model.

- Provides theoretical analysis showing that OPC can eliminate the on-policy model error that arises in MBRL, and tightly bounds the remaining off-policy error.

- Empirically demonstrates that OPC improves state-of-the-art MBRL algorithms like MBPO in terms of data-efficiency, especially on difficult PyBullet environments.

- Shows that OPC makes MBRL more robust to factors like state representation and stochasticity.

- Provides ablation studies analyzing the effect of multi-step rollouts, data diversity, and off-policy corrections.

So in summary, the main contribution is proposing the OPC method to combine learned models and on-policy data in a principled manner in order to enable accurate long-term predictions for MBRL. Theoretical and empirical results demonstrate the benefits of OPC over baseline MBRL algorithms.
