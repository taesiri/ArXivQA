# [Rethinking the Roles of Large Language Models in Chinese Grammatical   Error Correction](https://arxiv.org/abs/2402.11420)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Chinese Grammatical Error Correction (CGEC) is an important NLP task but challenging. 
- Recent Large Language Models (LLMs) have shown unsatisfactory performance as correctors on CGEC. This is mainly due to the constraint of the "minimum change principle" which requires minimal edits to correct errors. The freer text generation of LLMs conflicts with this.

Proposed Solutions:
- Rethink how to better utilize LLMs for CGEC to give play to their strengths in knowledge and understanding. 
- Propose Explanation-Augmented framework (\ourmethod) that uses LLMs as explainers to provide error types, references and explanations to enhance training of small CGEC models. This avoids the limitation of minimum change principle.
- Propose Semantic-incorporated Evaluation framework (\oureval) where LLMs act as flexible evaluators to judge model outputs comprehensively based on semantics instead of just matching to annotated references.

Main Contributions:
- Novel frameworks to reposition roles of LLMs in CGEC - as explainers to inject knowledge into small models during training, and as evaluators to enable more reasonable assessment.
- Explores co-existence and collaboration of LLMs and small models for advancing downstream tasks.
- Effectiveness verified by solid experiments and analyses - stable improvements to various small models; evaluation better matches human judgment.
- Provides new insights on leveraging strengths of LLMs and small models respectively to promote the CGEC field.

In summary, the paper proposes innovative ways for LLMs and small models to work together in CGEC via explainability and semantics. This is a valuable attempt at rethinking how pre-trained models can be better adapted for advancing language correction tasks.
