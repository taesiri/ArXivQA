# [Position Embedding Needs an Independent Layer Normalization](https://arxiv.org/abs/2212.05262)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to overcome the limitations of the default position embedding (PE) joining method in common vision transformers (VTs), in order to improve the expressiveness of PE and enhance model performance. Specifically, the key questions are:1) What are the inherent drawbacks of the default PE joining method, where PE is simply added to the patch embedding? 2) How can these limitations be overcome with minimal modifications to existing VTs?3) Can the proposed method improve the expressiveness of PE and provide layer-adaptive and hierarchical position information? 4) Is the proposed method effective and robust for enhancing various VTs with different PE types and architectures?The central hypothesis is that providing separate layer normalizations for token embeddings and PE can allow more expressive and adaptive position information, thus improving VT performance with minimal extra cost. The method called Layer-adaptive Position Embedding (LaPE) is proposed to test this hypothesis.In summary, this paper aims to analyze the limitations of the default PE joining method in VTs, propose a simple and effective solution called LaPE, and experimentally validate that LaPE can enhance diverse VTs by improving the expressiveness and adaptability of position information.
