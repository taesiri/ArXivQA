# [Position Embedding Needs an Independent Layer Normalization](https://arxiv.org/abs/2212.05262)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to overcome the limitations of the default position embedding (PE) joining method in common vision transformers (VTs), in order to improve the expressiveness of PE and enhance model performance. 

Specifically, the key questions are:

1) What are the inherent drawbacks of the default PE joining method, where PE is simply added to the patch embedding? 

2) How can these limitations be overcome with minimal modifications to existing VTs?

3) Can the proposed method improve the expressiveness of PE and provide layer-adaptive and hierarchical position information? 

4) Is the proposed method effective and robust for enhancing various VTs with different PE types and architectures?

The central hypothesis is that providing separate layer normalizations for token embeddings and PE can allow more expressive and adaptive position information, thus improving VT performance with minimal extra cost. The method called Layer-adaptive Position Embedding (LaPE) is proposed to test this hypothesis.

In summary, this paper aims to analyze the limitations of the default PE joining method in VTs, propose a simple and effective solution called LaPE, and experimentally validate that LaPE can enhance diverse VTs by improving the expressiveness and adaptability of position information.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple yet effective method called LaPE (Layer-adaptive Position Embedding) to improve the performance of Vision Transformers (VTs). 

Specifically, the key ideas and contributions are:

1. Theoretical analysis on the default position embedding (PE) joining method in VTs, and revealing its limitations:

- The default method directly adds the PE to the patch embedding, then passes them through a shared layer normalization (LN). This couples the token embeddings and PE, limiting the expressiveness of PE. 

- The default method cannot provide adaptive position information for different layers.

2. Proposing the LaPE method to overcome the limitations:

- Use independent LNs for token embeddings and PE, then add them together as input to the self-attention module of each layer.

- This allows adaptive adjustment of the expressiveness of PE for each layer, providing hierarchical position information.

3. Extensive experiments showing LaPE consistently improves various VTs with different PE types on multiple datasets, with negligible extra cost.

4. Visualization and analysis demonstrating that LaPE significantly enhances the expressiveness of PE (e.g. from 1D to 2D, monotonic to hierarchical).

5. Showing LaPE makes VTs robust to the choice of PE type, greatly reducing the performance gap caused by different PE designs.

In summary, the key contribution is identifying limitations of the default PE joining method in VTs, and proposing the simple yet effective LaPE method to overcome this, which consistently improves VT performance across different models, datasets and PE types.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a simple method called Layer-adaptive Position Embedding (LaPE) that provides independent layer normalization for token embeddings and position embeddings in vision transformers, allowing more expressive and adaptive position information to improve model performance.
