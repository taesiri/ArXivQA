# [Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and   Human-Centered Solutions](https://arxiv.org/abs/2402.01108)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-agent systems composed of large language models (LLMs) and other components show promise for tackling complex real-world tasks. However, relying solely on accuracy for optimization can overlook important constraints like efficiency, cost, and privacy in production settings.  
- Evaluating multi-agent systems only on overall task performance makes it hard to analyze interactions between components or identify the root causes of issues.

Solution - Reasoning Capacity:
- The paper introduces the concept of "reasoning capacity" to assess a system's ability to effectively process inputs and generate accurate outputs for a given task under specified constraints, compared to an ideal optimal system.
- Reasoning capacity provides a mathematical framework to connect the capabilities of individual components like the orchestration platform, planner, and agents to optimize the overall system performance under multiple constraints.
- It enables detecting limitations in components by identifying disparities between their reasoning capacity versus their actual performance. Breaking down reasoning capacity recursively can localize bottlenecks.

Key Contributions:
- Formal definition of reasoning capacity inspired by channel capacity in information theory.
- Demonstration of using reasoning capacity for optimization, monitoring, debugging and evaluation of multi-agent systems.
- Analysis of limitations in existing systems - dynamic changes, budget constraints, privacy considerations for the orchestration platform; inadequate domain knowledge, limited capabilities for the planner; out-of-distribution inputs, lack of self-verification for LLMs.
- Proposal of self-reflection with human-in-the-loop to address bottlenecks in reasoning capacity, by adding new data/agents, utilizing human input, or providing feedback.
