# [Parallel Deep Neural Networks Have Zero Duality Gap](https://arxiv.org/abs/2110.06482v3)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on characterizing the convex duality for deep neural networks, in particular investigating whether strong duality (zero duality gap) holds for such nonconvex models. The central questions it aims to address are:

1. Does strong duality hold for deep neural networks? 

2. Can we characterize the duality gap (difference between primal and dual optimal values) for cases where strong duality does not hold?

3. Is there a neural network architecture for which strong duality holds regardless of depth?

The key hypothesis is that standard deep neural networks may have a non-zero duality gap, meaning strong duality does not hold and the Lagrangian dual provides only a lower bound rather than the exact optimal value. However, with certain modifications like parallel architectures, it may be possible to achieve strong duality. 

The paper proves that for standard deep linear networks and 3-layer ReLU networks, the duality gap can be nonzero when depth is greater than 2. It also shows that for parallel architectures with sufficient branches, strong duality holds for linear and ReLU networks of any depth. 

Overall, the central questions addressed revolve around characterizing convex duality, the duality gap, and conditions under which strong duality holds for nonconvex deep neural network training problems. The key finding is that network architecture plays a crucial role in determining the tightness of the convex relaxation.


## What is the main contribution of this paper?

 This paper presents the convex duality framework for deep neural networks with linear activation and ReLU activation. The main contributions are:

- For deep linear networks with vector outputs, the paper shows that the duality gap is non-zero when the depth is 3 or more. It provides closed-form solutions for the primal and dual problems and precisely calculates the duality gap. 

- For parallel deep linear networks, the paper proves that with a certain convex regularization, the duality gap reduces to zero regardless of the depth. This implies there are equivalent convex formulations for training parallel deep linear networks.

- For standard 3-layer ReLU networks, even with whitened data, the paper shows the duality gap is non-zero. 

- For parallel deep ReLU networks with sufficient branches, the paper proves strong duality holds, i.e. the duality gap is zero. This means there exists an equivalent convex program for training deep parallel ReLU networks.

In summary, the key contribution is providing a full characterization of the convex duality for deep neural networks. The paper shows the duality gap can be non-zero starting from 3-layer standard networks but is always zero for parallel networks with certain convex regularizers. This theory reveals the advantage of parallel architectures in terms of convex optimization.
