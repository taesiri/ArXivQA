# [Emergence and Function of Abstract Representations in Self-Supervised   Transformers](https://arxiv.org/abs/2312.05361)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates whether neural networks trained with a self-supervised objective, similar to the learning pressures faced by biological brains, can develop abstract representations that capture the latent structure or "blueprint" used to construct their inputs. This ability is a key feature of human intelligence that allows generalization to novel situations.

Methodology:
The authors train a small transformer network on a simplified dataset of visual boards containing objects constructed from a hierarchical grammar. The network is tasked with reconstructing masked parts of input boards. Through analysis of the network's learned representations and via targeted manipulations, the authors study whether the network forms an abstract "world model" by learning representations that encode semantic features of the boards rather than just memorizing raw pixels.

Key Findings:
- The network learns intermediate representations or "abstractions" that cluster based on semantic features like object membership, capturing the latent hierarchy in the dataset.
- These abstractions play a causal role in the network's computations, as swapping them leads to predictable changes in outputs.
- The abstractions show properties like independence and compositionality that mirror the dataset structure.
- A network variant with a discrete language bottleneck develops a vocabulary for describing its abstractions. This mapping allows steering the network's inferences.

Main Contributions:  
1) Provides evidence that self-supervised neural networks can autonomously develop abstract world models that capture structural regularities in data.
2) Demonstrates interpretability methods to surface and manipulate these learned abstractions. 
3) Introduces an architecture that produces more readily interpretable representations of its internal world model.
4) The framework provides a pathway to better understand abstraction formation in biological and artificial intelligence.
