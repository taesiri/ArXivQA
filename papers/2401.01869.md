# [On the hardness of learning under symmetries](https://arxiv.org/abs/2401.01869)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper studies the problem of learning equitable neural networks (networks invariant to symmetries like permutations) using gradient descent. 
- Prior works have shown that learning general shallow neural networks with gradient descent can be exponentially hard under Gaussian input distributions. This paper asks - does enforcing symmetry constraints help circumvent these hardness results? 

Main Contributions
- The paper shows that several classes of equitable networks remain exponentially hard to learn using gradient descent, even with symmetry constraints. This includes graph neural networks (GNNs), convolutional neural networks (CNNs), invariant polynomials and frame-averaged networks.

- For GNNs, the paper shows exponential hardness in both number of nodes and feature dimensions. Hardness in nodes uses custom parity functions in graph degree distributions. Hardness in features uses orthogonal projection arguments from prior works.

- For CNNs and frame-averaged networks, the paper leverages sign-invariant activations to construct hard function classes based on older hardness results. It also gives a superpolynomial lower bound using parity-based hard functions.

- The paper shows invariant polynomials can be learned efficiently using a statistical query (SQ) algorithm, demonstrating a separation between SQ and correlational SQ complexity. But gradient descent remains hard for these functions.

- The paper also gives an NP-hardness result for properly learning the weights of a GNN architecture. And it provides experiments confirming that overparameterized networks fail to learn the proposed hard function classes.

Implications
- The results imply symmetry alone is generally not enough to enable efficient learning of all functions represented by an equitable architecture. Additional inductive biases are likely needed to guarantee learnability.

- The paper contributes constructions of hard function classes tailored to invariant networks, as well as proof techniques extending prior hardness results to the invariant setting.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper shows that classes of shallow neural networks that encode symmetry, such as graph neural networks and convolutional neural networks, are still exponentially hard to learn via gradient descent despite their additional structural constraints.


## What is the main contribution of this paper?

 The main contribution of this paper is proving computational hardness results for learning equivariant neural networks via gradient descent. Specifically, the paper shows that even with the inductive bias provided by incorporating symmetries and invariances, learning shallow graph neural networks, convolutional networks, invariant polynomials, and other symmetric architectures remains exponentially or superpolynomially hard in various parameters. These results demonstrate that problem symmetries are generally not sufficient to circumvent fundamental barriers to efficiently learning neural network function classes using gradient-based methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Computational complexity of learning
- Hardness of learning neural networks
- Equivariant neural networks
- Symmetries and invariance 
- Graph neural networks (GNNs)
- Convolutional neural networks (CNNs) 
- Statistical query (SQ) model
- Correlational statistical queries (CSQ)
- Sample complexity vs computational complexity
- Gradient descent
- Message passing

The paper studies the computational hardness of learning neural networks with symmetries and invariance properties, such as GNNs and CNNs, using gradient-based methods. It provides complexity lower bounds in various settings to show that incorporating symmetries does not alleviate the fundamental hardness compared to general neural networks. Key complexity concepts like statistical queries and correlational statistical queries are used to prove the hardness results. The relation between sample complexity and computational complexity is also highlighted. Overall, the core focus is on elucidating the limitations of gradient-based learning of invariant neural networks from a theoretical perspective despite their practical success.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper shows hardness results for learning invariant neural networks via gradient descent. Could you expand more on the techniques used to extend prior hardness results for non-invariant networks to the invariant setting? What were some of the key challenges? 

2. For the Boolean function warm-up, what motivated the choice of using pairwise independence of the function class to prove the statistical query lower bound? How does this connect conceptually to notions of hardness for real-valued functions?

3. The paper presents hardness results for GNNs both in terms of number of nodes and feature dimensions. Could you explain more about the proof techniques used in each case and why they necessitated different approaches? 

4. For the node-based hardness result, what intuition explains why having a wider range of possible node degrees leads to exponential hardness in learning GNNs? How was this formally captured in the proof?

5. The paper shows a separation between statistical and correlational statistical queries for learning invariant polynomials. What is the high-level intuition for why this separation occurs? How does the algorithm exploit properties of invariant polynomial rings?

6. What experiments were conducted to complement the theoretical hardness results? What do the experimental results show about learning the proposed hard function classes in practice?

7. The paper considers primarily Gaussian input distributions. What potential challenges arise in extending the hardness results to more structured input distributions that may be encountered in applications? 

8. What open problems remain regarding characterizing the learnability of broader subclasses of invariant networks that may be learnable in practice? What additional assumptions need to be made?

9. The paper focuses on shallow, single hidden layer invariant networks. What barriers exist to extending the hardness results to deeper invariant architectures? 

10. What implications do the negative learning results have on the practical use of invariant networks? What positive results exist providing learning guarantees for restricted invariant network classes?
