# [Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles](https://arxiv.org/abs/2402.15691)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Additive rule ensembles are interpretable machine learning models that predict a target variable by summing the outputs of multiple IF-THEN rules. Gradient boosting can be used to greedily add new rules to optimize the prediction accuracy. However, existing boosting methods for rule learning are not designed to optimize the trade-off between accuracy and interpretability. They tend to include suboptimal rules and fail to identify the rule that best approximates the ideal risk gradient update. 

Proposed Solution:
This paper proposes a new boosting objective function called "orthogonal gradient boosting" that measures the angle between the risk gradient and the component of a candidate rule that is orthogonal to existing rules. This favors rules that better align with the ideal update step. The method also uses a corrective update that re-optimizes all rule weights jointly in each iteration. 

Main Contributions:

- Proof that the proposed objective function identifies the query that best approximates the inclusion of the actual risk gradient to the model.

- Proposition showing that the potential advantage over standard gradient boosting is unbounded.

- Efficient algorithm for optimizing the new objective function by incrementally computing projections.

- Extensive experiments on 34 datasets demonstrating superior accuracy/interpretability trade-off. On average, the method reaches the same accuracy with over 30% fewer rules compared to the best alternative.

- Demonstration that the approach tends to select more general rules covering more data points than alternative objectives.

In summary, the paper introduces a novel boosting objective for learning rule ensembles that enables simpler and more accurate models. By taking into account that subsequent iterations can adjust earlier rules, it favors conditions that better align with the overall risk gradient.
