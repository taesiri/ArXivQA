# [Bootstrapping Reinforcement Learning with Imitation for Vision-Based   Agile Flight](https://arxiv.org/abs/2403.12203)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the challenge of learning visuomotor policies for agile quadrotor flight tasks like autonomous drone racing. Such tasks require processing high-dimensional visual observations and generating precise control commands at high frequencies. However, learning policies that directly map raw images to low-level controls is extremely sample inefficient with RL alone and performance limited when using IL alone.

Proposed Solution: 
The paper proposes a 3-stage training framework to leverage the complementary strengths of reinforcement learning (RL) and imitation learning (IL).

Stage 1) Train a teacher policy on state information using RL to learn the task. 

Stage 2) Distill the teacher policy into a student policy using IL to match the teacher's actions from visual inputs.

Stage 3) Further improve the student policy using a novel performance adaptive RL fine-tuning approach. This allows surpassing the performance of the demonstrations used during IL.

Key Contributions:

- Demonstrate a real-world vision-based quadrotor racing policy without explicit state estimation, using raw images as input.

- Propose a new training framework combining IL (sample-efficient but performance-bound) and RL (sample-inefficient but higher potential performance) to learn visuomotor policies.

- Introduce a performance-adaptive fine-tuning technique to boost policy learning that automatically adjusts hyperparams based on current performance. 

- Show superior robustness and agility of combined policy over IL or RL baselines in both simulated and real-world drone racing experiments. For a fixed experience budget, the combined policy achieves lower lap times and higher success rates.

In summary, the key innovation is in synergizing IL and RL to unlock superior vision-based flight policies that are not attainable with either approach alone. Experiments demonstrate the real-world applicability of this framework to learn complex visuomotor control policies.
