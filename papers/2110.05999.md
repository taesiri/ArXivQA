# [DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational   Transformer](https://arxiv.org/abs/2110.05999)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we generate long, globally coherent texts using large pre-trained language models?

More specifically, the authors aim to address the issue that current pre-trained language models like BERT, GPT-2/3, etc. are good at generating locally coherent text (e.g. at the sentence level) but struggle to maintain coherence over long passages spanning many sentences or paragraphs. 

To tackle this problem, the authors propose a novel latent variable model called DiscoDVT that learns a sequence of discrete latent variables that capture the high-level discourse structure of a long text. This latent structure is then used to guide the local text generation process and maintain global coherence. 

The key ideas and contributions are:

- Proposing a discrete variational autoencoder framework to learn interpretable discrete latent codes that correspond to discourse structure from long texts.

- Introducing a discourse-aware training objective using explicit discourse relation annotations to encourage the latent codes to capture discourse structure. 

- Demonstrating that injecting the learned latent codes into a pre-trained decoder model improves coherence over many sentences compared to baselines.

So in summary, the central hypothesis is that learning discrete latent variables that represent discourse structure can guide pre-trained models to generate more globally coherent long text. The paper aims to demonstrate this through the proposed DiscoDVT model.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a novel latent variable model called DiscoDVT that learns a discrete latent variable sequence from a long text in order to guide the text generation process and maintain long-term coherence. 

2. It introduces an auxiliary objective that uses discourse relation information to allow the discrete latent codes to capture the discourse structure of the text.

3. It conducts extensive experiments on two open story generation datasets. The results demonstrate that DiscoDVT can generate more coherent long texts compared to baselines, and the learned discrete latent codes correspond to interpretable discourse patterns that help guide the generation.

In summary, the key ideas are using discrete latent variables to capture global structure, incorporating discourse relation information to embed coherence patterns in the latent codes, and showing improved long text generation performance. The combination of discrete latent variables and discourse relation modeling to improve coherence in long text generation is the main novelty presented.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one-sentence summary of the key points in the paper:

The paper proposes DiscoDVT, a novel generative model that learns a sequence of discrete latent variables to capture the high-level discourse structure of long texts, and uses this global structure to guide the local text generation process and improve coherence over many sentences.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in long text generation:

- Uses discrete latent variables in a VAE framework to capture high-level discourse structure. Most prior work uses continuous latent variables which can suffer from posterior collapse. Using discrete variables allows capturing interpretable categorical features and avoids posterior collapse. This is a novel application of discrete VAEs in long text generation.

- Incorporates explicit discourse relation modeling through an auxiliary objective on the latent codes. Many prior works do not explicitly model discourse structure. This allows the model to generate texts with more coherent discourse patterns.

- Builds on top of large pre-trained language models like BART. Leverages their strong language modeling capabilities while addressing their weaknesses in long-range coherence through the proposed discrete VAE framework.

- Evaluates on open-ended story generation which requires maintaining coherence over longer contexts. Much prior work focuses on constrained generation tasks like data-to-text. Story generation is a challenging testbed for long text coherence.

- Achieves improved coherence in human evaluation and diversity in automatic metrics compared to pre-trained baselines. Demonstrates the proposed model's advantages in open-ended generation quality.

So in summary, the key novelties are using discrete VAEs to capture high-level discourse structure, incorporating explicit discourse relation modeling, and showing improvements in open-ended story generation compared to pre-trained baselines. The discretized VAE framework seems promising for improving coherence in open-ended long text generation.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Exploring other types of prior knowledge besides discourse relations to inject into the discrete latent codes, such as entities, commonsense knowledge, etc. The authors suggest the latent codes could potentially capture other high-level semantic aspects of the text.

- Applying the proposed model to other long text generation tasks such as storytelling from images or dialogues. The authors demonstrate the model's effectiveness on open-ended story generation but suggest it could be beneficial for other conditional generation settings as well.

- Scaling up the model by training larger versions of the encoder, decoder and latent codes. The authors use BART-Base in their experiments due to resource constraints but suggest using larger pretrained models may further improve coherence.

- Exploring different methods to learn the prior besides an autoregressive Transformer, such as latent variable models like VAEs. The authors suggest this could provide more flexibility.

- Evaluating the generated stories with more automatically computed discourse-aware metrics beyond the proposed classification task. The authors suggest this could better evaluate discourse coherence.

- Conducting human evaluation on additional aspects such as fluency, logic, interestingness besides coherence. The authors focus on coherence for human eval but suggest holistic human judgements could give further insight.

In summary, the main future directions are exploring additional prior knowledge to inject, applying the model to other tasks, scaling it up, using different prior learning methods, and conducting more comprehensive automatic and human evaluation. The authors provide promising results but suggest several interesting ways to extend the work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes DiscoDVT, a discourse-aware discrete variational Transformer model for generating long, coherent texts. The key ideas are: 1) Learning a sequence of discrete latent variables that capture the high-level structure and discourse relations in a long text. This latent sequence is used to guide the text generation process to maintain global coherence. 2) Introducing a discourse-aware objective that requires the latent representations of adjacent elementary discourse units (EDUs) to predict the discourse relation between them. This helps the latent codes learn the discourse structure. 3) Using convolutional neural networks and a categorical bottleneck to obtain discrete latent representations from the text. 4) Modeling the prior distribution over latents with a separate autoregressive Transformer conditioned on the input. At test time, the prior first generates a latent sequence given the prompt, and the decoder generates the text conditioned on the prompt and latents. Experiments on story generation datasets Wikiplots and WritingPrompts demonstrate the model generates more coherent and interpretable texts compared to Transformer baselines. The discrete latent variables are shown to correspond to discourse relations in the generated text.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes DiscoDVT, a discourse-aware discrete variational Transformer to generate long and coherent text. The key idea is to learn a discrete latent variable sequence that summarizes the global structure of the text. This latent sequence is then used to guide the text generation process to maintain long-term coherence. 

Specifically, the model has three components - a posterior network, a prior network, and a generator. The posterior network encodes the text into a discrete latent variable sequence. To capture discourse structure, it has an auxiliary objective to predict discourse relations between text segments. The prior network models the distribution of latent sequences given the prompt. The generator reconstructs the text conditioned on the prompt and sampled latent sequence. It is trained by maximizing the evidence lower bound to learn meaningful latent variables. Experiments on two story generation datasets show DiscoDVT generates more coherent stories than baselines like BART, outperforming them in automatic and human evaluations. The discrete latent variables are shown to correspond to interpretable discourse patterns.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes DiscoDVT, a discourse-aware discrete variational Transformer for long text generation. The model learns a sequence of discrete latent variables that summarize the global structure of a long text. This latent sequence is then used to guide the generation process to maintain coherence. Specifically, the text is first encoded into contextualized representations using a bidirectional Transformer encoder. 1D convolutional neural networks (CNNs) are then applied to abstract higher-level features from these representations. A discrete variational bottleneck maps the CNN outputs into a categorical distribution over a fixed latent vocabulary to obtain a discrete latent sequence. This sequence is embedded and rescaled back to the text length using transposed 1D CNNs. The rescaled latent embeddings are added to the decoder's embedding layer for step-wise generation control. To make the latent codes discourse-aware, the model extracts elementary discourse units (EDUs) from the text and models discourse relations between adjacent EDUs using the latent representations. The posterior network and generator are trained on reconstruction likelihood. An autoregressive Transformer then models the prior distribution over the discrete codes given the input prompt. At inference, the model samples codes from the prior, injects them into the generator to control decoding, and samples the output text tokens.


## What problem or question is the paper addressing?

 This paper is addressing the problem of generating long, globally coherent texts with neural language models. Some key points:

- Existing pre-trained language models like BERT and GPT-2 perform well at generating locally coherent texts, but struggle to maintain global coherence over long passages spanning many sentences.

- This is an issue for tasks like story generation, where models need to generate narratives with long-range structure and flow. Current models tend to produce stories with repetition, contradictions, and other incoherences. 

- The authors propose using a discrete variational autoencoder framework to learn latent representations that capture the global semantic structure of long texts.

- They introduce a discrete variational bottleneck that compresses representations into categorical latent variables. This allows capturing high-level textual features.

- The discrete latent codes are used to guide the text generation process and maintain global coherence. 

- They also incorporate discourse relation modeling to embed information about how sentences connect into the latent codes. This further improves coherence.

- Experiments on story generation datasets show their model can generate more globally coherent and interpretable narratives compared to baselines like BART.

In summary, the key contribution is using discrete variational autoencoders with discourse modeling to help pre-trained language models generate long, globally coherent texts, which is an open challenge for current models. The proposed techniques allow capturing and utilizing global semantic structure during generation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Discourse coherence - The paper focuses on generating long texts that maintain global, long-range coherence through proper discourse structure. Discourse coherence is a main challenge for long text generation.

- Discrete latent variables - The paper proposes learning a sequence of discrete latent variables that capture the high-level structure of the text. This allows guiding the local text generation process.

- Variational autoencoder (VAE) - The paper revives VAEs with discrete latent codes to tackle the incoherence issue in long text generation.

- Discourse relations - The paper learns to embed discourse relations like temporal and causal relations between text segments into the discrete latent codes through an auxiliary objective.

- Elementary discourse units (EDUs) - The paper segments texts into EDUs and models discourse relations between adjacent EDUs to inject discourse information into the discrete codes.

- Gumbel-softmax - The Gumbel-softmax trick is used to provide a differentiable relaxation of sampling discrete latent codes during training.

- Autoregressive modeling - An autoregressive Transformer is used to model the prior distribution over the discrete codes conditioned on the input text.

- Two-stage training - The model is trained in two stages, first the posterior and generator, then the prior network. The discrete codes are learned before being predicted.

In summary, the key ideas involve using discrete latent variables to capture discourse structure and guide coherent text generation, with innovations in training methodology. Discourse modeling and VAE revival are also central themes.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main goal or purpose of this research? What problem is it trying to solve?

2. What methods or techniques does the paper propose to achieve its goal? 

3. What are the key components or steps involved in the proposed approach?

4. What datasets were used to evaluate the proposed method? What were the key statistics or characteristics of the datasets?

5. What evaluation metrics were used? What were the main results on these metrics compared to baseline methods?

6. What were the main limitations or shortcomings of the proposed method based on the evaluation?

7. What related or prior work does the paper compare its results to? How does the proposed method improve upon or differ from this prior work?

8. What potential applications or use cases does the paper suggest for the proposed method? 

9. What conclusions or main takeaways do the authors emphasize based on the results?

10. What future work or next steps do the authors propose based on the limitations or results? Do they suggest any improvements or extensions to the method?
