# DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational   Transformer

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate long, globally coherent texts using large pre-trained language models?More specifically, the authors aim to address the issue that current pre-trained language models like BERT, GPT-2/3, etc. are good at generating locally coherent text (e.g. at the sentence level) but struggle to maintain coherence over long passages spanning many sentences or paragraphs. To tackle this problem, the authors propose a novel latent variable model called DiscoDVT that learns a sequence of discrete latent variables that capture the high-level discourse structure of a long text. This latent structure is then used to guide the local text generation process and maintain global coherence. The key ideas and contributions are:- Proposing a discrete variational autoencoder framework to learn interpretable discrete latent codes that correspond to discourse structure from long texts.- Introducing a discourse-aware training objective using explicit discourse relation annotations to encourage the latent codes to capture discourse structure. - Demonstrating that injecting the learned latent codes into a pre-trained decoder model improves coherence over many sentences compared to baselines.So in summary, the central hypothesis is that learning discrete latent variables that represent discourse structure can guide pre-trained models to generate more globally coherent long text. The paper aims to demonstrate this through the proposed DiscoDVT model.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel latent variable model called DiscoDVT that learns a discrete latent variable sequence from a long text in order to guide the text generation process and maintain long-term coherence. 2. It introduces an auxiliary objective that uses discourse relation information to allow the discrete latent codes to capture the discourse structure of the text.3. It conducts extensive experiments on two open story generation datasets. The results demonstrate that DiscoDVT can generate more coherent long texts compared to baselines, and the learned discrete latent codes correspond to interpretable discourse patterns that help guide the generation.In summary, the key ideas are using discrete latent variables to capture global structure, incorporating discourse relation information to embed coherence patterns in the latent codes, and showing improved long text generation performance. The combination of discrete latent variables and discourse relation modeling to improve coherence in long text generation is the main novelty presented.
