# DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational   Transformer

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate long, globally coherent texts using large pre-trained language models?More specifically, the authors aim to address the issue that current pre-trained language models like BERT, GPT-2/3, etc. are good at generating locally coherent text (e.g. at the sentence level) but struggle to maintain coherence over long passages spanning many sentences or paragraphs. To tackle this problem, the authors propose a novel latent variable model called DiscoDVT that learns a sequence of discrete latent variables that capture the high-level discourse structure of a long text. This latent structure is then used to guide the local text generation process and maintain global coherence. The key ideas and contributions are:- Proposing a discrete variational autoencoder framework to learn interpretable discrete latent codes that correspond to discourse structure from long texts.- Introducing a discourse-aware training objective using explicit discourse relation annotations to encourage the latent codes to capture discourse structure. - Demonstrating that injecting the learned latent codes into a pre-trained decoder model improves coherence over many sentences compared to baselines.So in summary, the central hypothesis is that learning discrete latent variables that represent discourse structure can guide pre-trained models to generate more globally coherent long text. The paper aims to demonstrate this through the proposed DiscoDVT model.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel latent variable model called DiscoDVT that learns a discrete latent variable sequence from a long text in order to guide the text generation process and maintain long-term coherence. 2. It introduces an auxiliary objective that uses discourse relation information to allow the discrete latent codes to capture the discourse structure of the text.3. It conducts extensive experiments on two open story generation datasets. The results demonstrate that DiscoDVT can generate more coherent long texts compared to baselines, and the learned discrete latent codes correspond to interpretable discourse patterns that help guide the generation.In summary, the key ideas are using discrete latent variables to capture global structure, incorporating discourse relation information to embed coherence patterns in the latent codes, and showing improved long text generation performance. The combination of discrete latent variables and discourse relation modeling to improve coherence in long text generation is the main novelty presented.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one-sentence summary of the key points in the paper:The paper proposes DiscoDVT, a novel generative model that learns a sequence of discrete latent variables to capture the high-level discourse structure of long texts, and uses this global structure to guide the local text generation process and improve coherence over many sentences.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in long text generation:- Uses discrete latent variables in a VAE framework to capture high-level discourse structure. Most prior work uses continuous latent variables which can suffer from posterior collapse. Using discrete variables allows capturing interpretable categorical features and avoids posterior collapse. This is a novel application of discrete VAEs in long text generation.- Incorporates explicit discourse relation modeling through an auxiliary objective on the latent codes. Many prior works do not explicitly model discourse structure. This allows the model to generate texts with more coherent discourse patterns.- Builds on top of large pre-trained language models like BART. Leverages their strong language modeling capabilities while addressing their weaknesses in long-range coherence through the proposed discrete VAE framework.- Evaluates on open-ended story generation which requires maintaining coherence over longer contexts. Much prior work focuses on constrained generation tasks like data-to-text. Story generation is a challenging testbed for long text coherence.- Achieves improved coherence in human evaluation and diversity in automatic metrics compared to pre-trained baselines. Demonstrates the proposed model's advantages in open-ended generation quality.So in summary, the key novelties are using discrete VAEs to capture high-level discourse structure, incorporating explicit discourse relation modeling, and showing improvements in open-ended story generation compared to pre-trained baselines. The discretized VAE framework seems promising for improving coherence in open-ended long text generation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Exploring other types of prior knowledge besides discourse relations to inject into the discrete latent codes, such as entities, commonsense knowledge, etc. The authors suggest the latent codes could potentially capture other high-level semantic aspects of the text.- Applying the proposed model to other long text generation tasks such as storytelling from images or dialogues. The authors demonstrate the model's effectiveness on open-ended story generation but suggest it could be beneficial for other conditional generation settings as well.- Scaling up the model by training larger versions of the encoder, decoder and latent codes. The authors use BART-Base in their experiments due to resource constraints but suggest using larger pretrained models may further improve coherence.- Exploring different methods to learn the prior besides an autoregressive Transformer, such as latent variable models like VAEs. The authors suggest this could provide more flexibility.- Evaluating the generated stories with more automatically computed discourse-aware metrics beyond the proposed classification task. The authors suggest this could better evaluate discourse coherence.- Conducting human evaluation on additional aspects such as fluency, logic, interestingness besides coherence. The authors focus on coherence for human eval but suggest holistic human judgements could give further insight.In summary, the main future directions are exploring additional prior knowledge to inject, applying the model to other tasks, scaling it up, using different prior learning methods, and conducting more comprehensive automatic and human evaluation. The authors provide promising results but suggest several interesting ways to extend the work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes DiscoDVT, a discourse-aware discrete variational Transformer model for generating long, coherent texts. The key ideas are: 1) Learning a sequence of discrete latent variables that capture the high-level structure and discourse relations in a long text. This latent sequence is used to guide the text generation process to maintain global coherence. 2) Introducing a discourse-aware objective that requires the latent representations of adjacent elementary discourse units (EDUs) to predict the discourse relation between them. This helps the latent codes learn the discourse structure. 3) Using convolutional neural networks and a categorical bottleneck to obtain discrete latent representations from the text. 4) Modeling the prior distribution over latents with a separate autoregressive Transformer conditioned on the input. At test time, the prior first generates a latent sequence given the prompt, and the decoder generates the text conditioned on the prompt and latents. Experiments on story generation datasets Wikiplots and WritingPrompts demonstrate the model generates more coherent and interpretable texts compared to Transformer baselines. The discrete latent variables are shown to correspond to discourse relations in the generated text.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes DiscoDVT, a discourse-aware discrete variational Transformer to generate long and coherent text. The key idea is to learn a discrete latent variable sequence that summarizes the global structure of the text. This latent sequence is then used to guide the text generation process to maintain long-term coherence. Specifically, the model has three components - a posterior network, a prior network, and a generator. The posterior network encodes the text into a discrete latent variable sequence. To capture discourse structure, it has an auxiliary objective to predict discourse relations between text segments. The prior network models the distribution of latent sequences given the prompt. The generator reconstructs the text conditioned on the prompt and sampled latent sequence. It is trained by maximizing the evidence lower bound to learn meaningful latent variables. Experiments on two story generation datasets show DiscoDVT generates more coherent stories than baselines like BART, outperforming them in automatic and human evaluations. The discrete latent variables are shown to correspond to interpretable discourse patterns.
