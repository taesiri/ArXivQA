# [wav2vec: Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/1904.05862)

## What is the central research question or hypothesis that this paper addresses?

This paper explores unsupervised pre-training for speech recognition by learning representations of raw audio. The central hypothesis is that pre-training a model on large amounts of unlabeled audio data and then using the learned representations to initialize a model for speech recognition can improve accuracy, especially when transcribed speech data is limited.The key research questions examined are:- Can a representation learned by pre-training on unlabeled audio improve speech recognition accuracy over standard features like log-mel filterbanks?- How does the amount of unlabeled pre-training data impact accuracy?- Can pre-training help speech recognition when there is only a small amount of transcribed speech available?- How does the pre-trained model compare to previous state-of-the-art speech recognition systems?So in summary, the central hypothesis is that pre-training on raw audio can improve speech recognition, and the key questions look at how different amounts of unlabeled and labeled data impact the effectiveness of this approach. The authors evaluate on benchmark datasets to compare against prior state-of-the-art systems.


## What is the main contribution of this paper?

This paper introduces wav2vec, an unsupervised pre-training approach for speech recognition. The key contributions are:- They propose using a convolutional neural network architecture trained on raw audio in a self-supervised way to learn good general speech representations. This involves predicting future waveform samples based on context. - They show that the learned representations from pre-training on large unlabeled speech datasets (Librispeech, WSJ) can substantially improve performance of supervised speech recognition models when there is limited labeled data.- They achieve state-of-the-art results on WSJ and TIMIT benchmarks using the pre-trained representations, outperforming prior character-based models while using much less labeled data. For example, on WSJ they get 2.43% WER using the wav2vec pre-trained features while the previous best character-based result was 3.1% WER from Deep Speech 2 which used substantially more labeled speech data.- The approach is simple and scalable, relying on a convolutional architecture that can be easily parallelized and leveraging only unlabeled speech data that is easier to obtain than transcribed data.So in summary, the main contribution is showing that pre-training representations on raw speech in an unsupervised way can greatly improve performance of speech recognition models, achieving new state-of-the-art results with less labeled data. The proposed wav2vec model provides a simple but effective approach for pre-training on speech.
