# [Pragmatic Competence Evaluation of Large Language Models for Korean](https://arxiv.org/abs/2403.12675)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are rapidly advancing, but evaluation lags behind, often relying on benchmarks focused on reasoning and knowledge rather than pragmatic competence. 
- There is a lack of research evaluating LLMs' abilities to understand implied, nonliteral meanings which depend heavily on context (pragmatics). This aspect is crucial for conversational AI.
- Current benchmarks have limitations: emphasis on literal meaning, multiple-choice questions ill-suited to generative capabilities, English-centric.

Proposed Solution:
- Develop test set of 120 units spanning Grice's 4 conversational maxims to systematically evaluate pragmatic competence of LLMs for Korean language.
- Test in two setups: multiple-choice questions (MCQs) for automated evaluation, and open-ended questions (OEQs) with human rating of narrative responses.
- Compare performance of 5 LLMs: GPT-3.5, GPT-4, Gemini-Pro, HyperCLOVA X (Korean-optimized), LDCC-Solar (top Korean model).
- Explore impact of in-context learning techniques like few-shot learning and Chain-of-Thought prompting.

Key Findings:  
- GPT-4 leads in both test setups, with HyperCLOVA X and Gemini-Pro next. Korean models do well.  
- Performance differences between MCQ and OEQ formats highlight need for qualitative assessment.
- Few-shot learning boosts performance, Chain-of-Thought can hinder pragmatic inference.
- Analysis uncovers tendencies like literal bias and limitations in adaptive reasoning.

Main Contributions:
- First systematic evaluation of LLMs' pragmatic competence in Korean context
- Parallel test setups reveal insights into understanding vs. generation capabilities  
- In-context learning analysis provides guidelines for further LLM development
- Findings emphasize need to advance LLMs beyond literal interpretations for human-like communication

The paper demonstrates an evaluation approach focusing on the underexplored but critical dimension of pragmatics to uncover strengths, weaknesses and development opportunities for advancing LLMs' contextual language capabilities.
