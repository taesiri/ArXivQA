# [Universal Segmentation at Arbitrary Granularity with Language   Instruction](https://arxiv.org/abs/2312.01623)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a universal segmentation model called UniLSeg that can segment images at arbitrary semantic granularity based on language instructions. The authors reorganize several segmentation tasks into a unified format consisting of an image, ground truth mask, and language caption describing the segmentation target. A two-stage pre-training strategy is used - first on a large dataset of images from SA-1B, then on a mix of supervised segmentation datasets and additional pseudo-labeled data automatically generated from weakly annotated images. The UniLSeg model consists of visual and text encoders, a pre-fusion cross-attention module to incorporate text guidance into visual features, and two-path visual-linguistic decoding with self- and cross-attention to align features and generate the output mask. Experiments demonstrate state-of-the-art performance on referring image segmentation, salient object detection, semantic segmentation, part segmentation and other tasks. The flexible use of language captions to guide segmentation at multiple semantic levels is a key advantage over prior specialized models. An automatic annotation engine to generate caption-mask pairs from unlabeled images further improves results. In conclusion, language guidance and extensive pre-training enables the proposed UniLSeg model to achieve versatile, universal segmentation capabilities beyond current approaches.
