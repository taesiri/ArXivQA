# [PIP-Net: Pedestrian Intention Prediction in the Wild](https://arxiv.org/abs/2402.12810)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "PIP-Net: Pedestrian Intention Prediction in the Wild":

Problem:
Predicting pedestrian crossing intention is critical for autonomous vehicles (AVs) to ensure pedestrian safety, but is challenging due to various environmental factors affecting human behavior. Prior work has limitations in feature selection/fusion and lacks evaluation in real-world AV scenarios. 

Proposed Solution: 
The paper proposes PIP-Net, a deep neural network framework to predict pedestrian crossing intention in real-world urban driving. Two variants are offered: PIP-Net-α for single camera, and PIP-Net-β for multi-camera setups. The model incorporates spatial-temporal features like pedestrian localization, pose, vehicle speed (spatial kinematics) as well as scene semantics and a new categorical depth feature (spatial context). It employs recurrent units and temporal attention for feature analysis. A multi-camera aggregation module is also introduced.

Main Contributions:
- Proposes a categorical depth feature that integrates instance segmentation with depth estimation to capture pedestrian/vehicle densities and interactions.
- Introduces multi-camera Urban-PIP dataset with 1.5k annotated pedestrians in real AV scenarios. 
- Evaluates feature importance, temporal horizon expansion, and multi vs single camera setups.
- Achieves state-of-the-art accuracy of 91% on PIE dataset. Generalizes to Urban-PIP with 73% accuracy, 9-12% higher than prior arts.  
- Can predict pedestrian intention up to 4 seconds in advance depending on scenario.

In summary, the paper presents an effective pedestrian intention prediction model that leverages spatial-temporal features and a new categorical depth representation to achieve strong performance in real-world autonomous driving scenarios. Key innovations include multi-modal fusion, introduction of a multi-camera AV dataset, and extensive component analysis.
