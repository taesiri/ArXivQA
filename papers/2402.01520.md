# [Low-Resource Cross-Domain Singing Voice Synthesis via Reduced   Self-Supervised Speech Representations](https://arxiv.org/abs/2402.01520)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Singing voice synthesis (SVS) aims to generate a melodic singing voice from text input. It is challenging as singing involves complex vocal manipulations beyond normal speech. Recent SVS methods rely on singing data, music scores, or lyric alignments which are expensive and error-prone to obtain. There is a need for SVS methods that can work in low-resource settings using only easily available text and speech data.

Proposed Method: This paper proposes Karaoker-SSL, an SVS method that uses only text and speech data for training. It conditions an acoustic model using reduced dimensions of self-supervised (SSL) speech representations to capture singing style. The model performs multi-task learning - it predicts pitch from the acoustic model's outputs while generating mel-spectrograms. This indirect supervision helps capture vocal style. A U-Net discriminator with differentiable data augmentations further refines quality.  

Main Contributions:
- Selects most relevant SSL representation dimensions using a parallel speech-singing dataset
- Conditions acoustic model using reduced SSL speech representations in an unsupervised way  
- Acoustic model multi-tasks to predict pitch from its outputs, indirectly learning style
- U-Net discriminator with differentiable augmentations improves voice quality
- Achieves singing voice synthesis without reliance on singing data, alignments or timestamps
- Provides a low-resource cross-domain singing synthesis solution requiring only text and speech

In experiments, Karaoker-SSL achieves good subjective speech quality, speaker similarity and song similarity scores. Ablations show the pitch prediction and U-Net discriminator components are critical for this performance. The model shows potential for low-resource singing synthesis relying only on easily available text and speech data.
