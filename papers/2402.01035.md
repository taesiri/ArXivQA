# [Getting the most out of your tokenizer for pre-training and domain   adaptation](https://arxiv.org/abs/2402.01035)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Tokenization is an important but understudied component of modern large language models (LLMs). Most works use a single tokenizer without optimization or analysis.
- When fine-tuning a pre-trained LLM for a downstream task, the tokenizer is usually unchanged. This means the tokenizer may be sub-optimal for the target domain.

Main Contributions
- Compare popular code tokenizers and analyze compression rate and tradeoffs. Find the \llama tokenizer is not most optimal.
- Study impact of vocabulary size, pre-tokenization rules, and training data on compression rates and downstream performance. Find significant effects on both.
- Show tokenizer can be changed when fine-tuning if trained on over 50B tokens. This allows improving compression and efficiency of pre-trained models like CodeLlama.

Key Experiments
- Train specialized Byte Pair Encoding (BPE) tokenizers for code. Test on 1.5B and 7B parameter LLM models.
- Evaluate on HumanEval and MBPP code generation benchmarks after fine-tuning with different tokenizers.
- Compare tokenizers in models trained from scratch vs fine-tuned from pre-trained base. Verify findings apply in both regimes.  

Main Recommendations
- Can switch tokenizer of pre-trained LLM when fine-tuning on 50B+ tokens. Gives efficiency gains with little performance impact.
- Vocabulary size has marginal impact on downstream metrics. Can optimize for memory or computational efficiency. 
- Use GPT-4's pre-tokenization regex. Good balance of compression and performance.

In summary, the paper provides a comprehensive analysis of optimization and tradeoffs in tokenization for modern LLMs applied to code generation. It gives clear advice on how to improve efficiency of models through tokenizer changes.
