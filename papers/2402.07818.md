# [Differentially Private Zeroth-Order Methods for Scalable Large Language   Model Finetuning](https://arxiv.org/abs/2402.07818)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Finetuning large language models (LLMs) on task-specific datasets is popular, but raises privacy concerns about leaking sensitive information in the datasets. 
- Existing differentially private (DP) methods for LLM finetuning are mostly based on DP-SGD, which has inherent scalability issues that limit the privacy-utility tradeoff.

Proposed Solution:
- Investigate the potential of differentially private zeroth-order (DP-ZO) methods for LLM finetuning, which are more efficient by approximating gradients using two forward passes instead of backpropagation.
- Propose a stagewise DP-ZO method that dynamically schedules key hyperparameters across training stages based on the synergy between DP noise and ZO gradient error. Earlier stages use lower ZO error and higher learning rate for faster convergence, while later stages deliberately increase ZO error to stabilize training.
- Further boost scalability by reducing the number of trainable parameters using a data-free pruning technique that does not require additional data or privacy budget.

Main Contributions:
- First comprehensive investigation of DP-ZO methods for LLM finetuning from theoretical and empirical viewpoints.
- Novel stagewise DP-ZO algorithm with theoretical privacy and convergence guarantees, as well as empirical evaluations.
- Additional scalability improvement via data-free pruning of parameters guided by a proposed importance matrix.
- Strong performance demonstrated across multiple models (RoBERTa-large, OPT-2.7B) and datasets (SST, SNLI, MultiRC, etc.) under varying privacy budgets.

In summary, the paper explores DP-ZO methods to overcome scalability limitations of DP-SGD for private LLM finetuning, with a stagewise algorithm design backed by theory and experiments. Data-free pruning further improves efficiency. The methods achieve impressive utility under strict privacy guarantees.
