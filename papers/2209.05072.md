# [Hard Negatives or False Negatives: Correcting Pooling Bias in Training   Neural Ranking Models](https://arxiv.org/abs/2209.05072)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to correct the pooling bias in labeled datasets for training neural ranking models (NRMs). 

The key points are:

- Existing NRMs training relies on negative sampling over unlabeled data. Hard negative sampling from strong retrievers is shown beneficial for model generalization. However, it may introduce more false negatives (unlabeled positives) and hurt NRMs training.

- The root cause of the false negative issue is the pooling bias during dataset construction, where only documents retrieved by some basic systems are labeled. This leads to potential existence of unlabeled positives.

- The paper formulates the false negative problem as learning from biased labeled datasets. It proposes a Coupled Estimation Technique (CET) to jointly learn a relevance model and a selection model to estimate relevance scores and selection propensities for correcting the pooling bias.

- Experiments on three benchmarks demonstrate NRMs learned with CET achieve significant gains against state-of-the-art methods in addressing false negatives during training.

In summary, the key hypothesis is that by explicitly modeling and correcting the pooling bias in labeled datasets, NRMs can better distinguish false negatives from hard negatives and achieve more effective training. The proposed CET approach is shown empirically to support this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a Coupled Estimation Technique (CET) to address the false negative issue in training neural ranking models (NRMs) on labeled datasets with pooling bias. Specifically:

- The paper formulates the false negative problem in training NRMs as learning from labeled datasets with pooling bias, where unlabeled positives are mistakenly sampled as negatives.

- To solve this problem, the authors propose CET which jointly trains a relevance model to estimate documents' relevance and a selection model to estimate documents' selection propensity. The two models promote each other in a coupled learning manner.

- Based on the estimated propensities, CET derives sample weights to relax the effect of false negatives and achieve bias correction learning for the relevance model.

- Experiments on three retrieval benchmarks demonstrate that NRMs learned with CET significantly outperform baseline methods in ranking effectiveness.

In summary, the key contribution is identifying the root cause of false negatives as pooling bias, and proposing the coupled estimation technique to address it by joint learning of relevance and selection models. This allows NRMs to benefit from hard negatives during training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a Coupled Estimation Technique (CET) to address the pooling bias problem in training neural ranking models on labeled datasets, where CET jointly learns a relevance model and a selection model in a coupled manner to identify and correct for false negatives caused by the bias.


## How does this paper compare to other research in the same field?

 This paper makes an important contribution to the field of neural ranking model training for information retrieval. Here is a summary of how it compares to related work:

- Problem formulation: The paper provides a novel formulation of the false negative issue that arises when training neural ranking models as a problem of learning from datasets with pooling bias. This provides a principled perspective compared to prior work that viewed it mainly as a data noise issue.  

- Proposed method: The Coupled Estimation Technique (CET) method is unique compared to prior techniques. It jointly learns a relevance model and selection model to adaptively identify false negatives caused by pooling bias. This is more advanced than heuristic filtering rules used before.

- Empirical evaluation: The paper conducts extensive experiments on multiple datasets to validate the effectiveness of CET. Comparisons to several state-of-the-art baselines demonstrate clear improvements in ranking accuracy. The analysis provides insights into how CET helps address pooling bias.

- Robustness: An important finding is that CET is robust and works for different neural ranking architectures trained with various retrievers. It also shows lower sensitivity to hyperparameters compared to prior methods. This indicates it is more widely applicable.

Overall, the framing of the problem and the proposed CET method are novel contributions compared to related literature. The solid empirical evidence demonstrates its capabilities in improving neural ranking model training by handling pooling bias. The work highlights the importance of accounting for biases in training data.


## What future research directions do the authors suggest?

 The authors suggest several potential future research directions at the end of the paper:

1. Extend the proposed method to the listwise setting, where multiple negative samples are considered simultaneously to estimate the bias weights. This could further improve the estimation of bias weights.

2. Apply the proposed method in the first-stage retrieval to address the pooling bias problem. The authors focused on solving the pooling bias issue for training the neural ranker in this work. But the bias also exists in the training data for neural retriever models. Applying the method to debias the retriever training could be an interesting direction.

3. Investigate other potential applications of the proposed Coupled Estimation Technique (CET). The authors mention that CET could be potentially used in other scenarios with biased training data besides information retrieval. Exploring other applications of CET could be an important future work. 

4. Study the effect of different model architectures in the CET framework. As discussed in the paper, the performance gain of CET varies for different neural ranking models. Analyzing the impact of model architecture design choices could provide useful insights.

5. Extend CET to address other types of bias in IR training data beyond pooling bias, such as position bias and trust bias. The authors focused on pooling bias in this work, but other biases also widely exist in IR datasets. Expanding the technique to handle other biases could make it more generally applicable.

In summary, the main future directions are: applying CET in more scenarios like first-stage retrieval and other tasks; investigating the effect of techniques like listwise learning and model architecture design; and generalizing it to address other types of bias in IR training data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper focuses on addressing the false negative issue when training neural ranking models (NRMs) on labeled datasets with pooling bias. The pooling bias occurs because annotators only label documents selected by basic retrievers, causing unlabeled positives (false negatives) in the data. Hard negative sampling from stronger retrievers increases false negatives, hurting NRM training. To solve this, the authors propose a Coupled Estimation Technique (CET) to jointly learn a relevance model and selection model on the biased data. The selection model estimates the propensity of a document being selected, which helps train the relevance model by downweighting false negatives. Experiments on three datasets show CET significantly improves ranking performance over methods like denoising and propensity weighting. Overall, CET provides an effective approach to correct pooling bias and train high quality NRMs from labeled IR datasets.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper addresses the issue of false negatives hurting the performance of neural ranking models (NRMs) when trained on datasets with pooling bias. Many large-scale IR datasets are constructed by having human annotators label the relevance of documents retrieved by some initial system. This leads to a pooling bias, where there may be many unjudged yet relevant documents (false negatives).  When training NRMs on such datasets and using top ranked results of a stronger retriever for hard negative sampling, performance degrades due to more false negatives. 

The authors propose a Coupled Estimation Technique (CET) to jointly learn a relevance model and selection model to correct for the pooling bias. The relevance model estimates document relevance while the selection model estimates the likelihood of a document being selected for annotation. These two models are trained together, with the selection model providing weights to reduce the impact of likely false negatives. Experiments on MS MARCO, TREC DL, and DuReader datasets show CET can significantly improve ranking performance over methods that heuristically identify false negatives. CET also works well across different rankers and retrievers.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a Coupled Estimation Technique (CET) to address the false negative issue in training neural ranking models (NRMs) on labeled datasets with pooling bias. 

The key idea is to jointly learn a relevance model (implemented with NRMs) and a selection model on the biased dataset in a coupled manner. Specifically, the selection model estimates the propensity of each document being selected into the judgement pool during the labeling process. Based on the estimated selection propensities, the relevance model can distinguish false negatives from hard negatives and achieve bias correction learning with an inverse propensity weighting loss. In each training iteration, the selection model helps estimate bias weights in the loss function for the relevance model update, while the relevance model provides relevance scores to derive bias weights for the selection model update. By training both models simultaneously, CET is able to correct pooling bias and learn high-quality NRMs from the biased dataset.


## What problem or question is the paper addressing?

 The paper is addressing the problem of false negatives when training neural ranking models (NRMs) on labeled datasets with pooling bias. Specifically:

- Many existing benchmarks for information retrieval rely on a pooling process to construct labeled datasets, where only a small subset of documents are judged by annotators for each query. This introduces pooling bias, where there could exist unlabeled positives (i.e. false negatives) in the dataset.

- NRMs are typically trained on these datasets by sampling negatives from the unlabeled data. When sampling top-ranked results from a stronger retriever as hard negatives, more false negatives are included, which hurts model training. 

- The root cause is the pooling bias in the dataset construction process. So the authors formulate this as a learning problem from datasets with pooling bias.

- They propose a Coupled Estimation Technique (CET) to jointly learn a relevance model and a selection model on the biased dataset. The selection model helps identify false negatives to correct the bias during training.

- Experiments show NRMs learned with CET achieve significant gains compared to other techniques for addressing false negatives, and CET is robust across different datasets, retrievers, and ranking models.

In summary, the main contribution is identifying and formally modeling the pooling bias problem when training NRMs, and proposing a novel approach CET to correct this bias by joint learning of a relevance and selection model.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some key terms and concepts are:

- Neural ranking models (NRMs): The paper focuses on training neural network-based ranking models for information retrieval tasks. NRMs are a major type of model used for ranking in modern IR systems.

- Negative sampling: Since IR training data typically contains limited labeled examples, NRMs rely heavily on negative sampling from unlabeled data to learn effectively. The paper examines different negative sampling strategies.

- Hard negatives vs false negatives: Hard negatives refer to challenging negative examples that are similar to positives, while false negatives are unlabeled positives incorrectly treated as negatives. The interplay between these two concepts is a key focus.

- Pooling bias: The paper argues that the false negative issue stems from pooling bias in the IR dataset construction process. Only documents retrieved by basic systems are labeled, resulting in potential unlabeled positives.

- Coupled Estimation Technique (CET): The proposed method that jointly trains a relevance model and selection model to estimate document relevance and selection propensity respectively. This addresses pooling bias.

- Inverse propensity weighting: A technique from causal inference that CET uses to correct for the pooling bias during model training.

- MS MARCO, TREC DL, DuReader: Benchmark IR datasets used for experiments to evaluate the effectiveness of models trained with CET.

In summary, the key focus is on addressing pooling bias and the resulting false negatives when training neural ranking models through coupled estimation and propensity weighting.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing this paper:

1. What is the main problem addressed in this paper? The paper focuses on correcting pooling bias in training neural ranking models (NRMs) due to the existence of false negatives. 

2. What causes the pooling bias and false negatives in training data? The pooling bias comes from the labeling process where only a subset of documents selected by basic retrievers are judged. This can lead to unlabeled relevant documents (false negatives).

3. How does pooling bias and false negatives affect NRMs training? False negatives sampled as hard negatives will mislead NRMs training and hurt ranking performance.

4. Why is addressing false negatives challenging? Simply removing potential false negatives may also filter some helpful hard negatives. The root is the pooling bias during data construction.

5. How does the paper formally define the false negative problem? As a learning problem from labeled datasets with pooling bias.

6. What is the high-level idea of the proposed method? Jointly learn a relevance model and selection model to estimate document relevance and selection propensity.

7. How are the two models trained? With a coupled learning algorithm and inverse propensity weighting loss.

8. How does the proposed method correct pooling bias? By weighting down penalties for potential false negatives during training.

9. What datasets are used for evaluation? MS MARCO, TREC DL, DuReader.

10. What are the main results? NRMs learned with the proposed method significantly outperform baseline techniques in addressing false negatives.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper formulates the false negative issue in training neural ranking models (NRMs) as a learning problem from labeled datasets with pooling bias. Could you explain in more detail what is meant by "pooling bias" and how it leads to false negatives in the training data?

2. The proposed Coupled Estimation Technique (CET) trains a relevance model and a selection model simultaneously. What is the intuition behind training these two models jointly? How do they help correct the pooling bias?

3. The selection model in CET estimates the propensity of a document being selected into the judgment pool. What types of features or signals could be useful for this selection model to estimate the selection propensities?

4. The paper mentions that CET employs a coupled learning algorithm to train the relevance and selection models. Could you walk through the details of this coupled learning process and training algorithm? 

5. How does CET leverage the estimated selection propensities from the selection model to handle false negatives during the training of the relevance model?

6. One key hyperparameter in CET is τ, which controls the scale of the bias weights. How does the choice of τ affect model training? Is CET very sensitive to this hyperparameter based on the results?

7. The paper shows a case study demonstrating how CET distinguishes between hard negatives and false negatives. Could you explain this case study in more detail and how it provides insights into CET?

8. How does CET compare to other existing methods, such as the denoising technique in RocketQA, for addressing false negatives? What are the limitations of those methods?

9. The results show CET is effective across different datasets, ranking models, and retrievers. What factors contribute to this robustness and wide applicability of CET?

10. What are some promising future directions for improving CET? For example, could it be extended to listwise or pairwise learning settings? How about applying CET to train the first-stage retriever model?
