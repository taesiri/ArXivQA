# [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have demonstrated impressive capabilities, but their enormous size leads to high compute, memory, and energy costs for deployment. This poses challenges for real-world usage.

Proposed Solution: 
- The paper proposes BitNet b1.58, a 1.58-bit LLM where every weight is constrained to {-1, 0, +1}. This is an extension of the original 1-bit BitNet model.

Key Contributions:

- BitNet b1.58 matches the perplexity and task performance of full-precision (FP16) LLMs, starting from 3B parameters. A 3.9B BitNet model outperforms a 3B FP16 LLM baseline.

- Compared to FP16 LLMs, BitNet b1.58 uses much less memory (up to 3.55x), is faster in terms of latency (up to 2.71x) and throughput (up to 8.9x), and consumes way less energy (up to 71.4x) for matrix multiplication.

- The inclusion of 0 as a possible weight value explicitly enables feature selection in the 1-bit model space. This is a key improvement over the original BitNet.

- BitNet b1.58 demonstrates strong generalization capability. A 3B model trained on 2T tokens outperforms the SotA open-source 3B model across several language tasks.

- The 1.58-bit approach defines a new scaling law and recipe for building high-performance yet extremely cost-effective LLMs. It also enables new model architectures like 1-bit MoE, and calls for new hardware optimized for such models.

In summary, the proposed BitNet b1.58 model offers a Pareto improvement that maintains LLM quality while significantly reducing compute and memory costs. This paves the way for ubiquitous and accessible large language models.
