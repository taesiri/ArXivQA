# [HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million   Narrated Video Clips](https://arxiv.org/abs/1906.03327)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:Can a useful joint text-video embedding be learned from a large amount of noisy narrated instructional videos without manual annotation?The authors explore using readily available narrated instructional videos from YouTube as a source of supervision for learning a joint text-video embedding. The narrations from these videos provide a natural but noisy form of supervision as they are not guaranteed to align perfectly with the visual content. The main hypothesis is that despite the noise, the sheer scale of narrated instructional videos available could allow learning a powerful text-video embedding model, without needing any manually annotated video-caption pairs.The authors demonstrate this by collecting a new dataset of 136 million video clips with automatic narration subtitles called HowTo100M. They show that models trained on this data alone can outperform prior work on instructional video datasets like YouCook2 and CrossTask. The pre-trained models also transfer reasonably via fine-tuning to other domains like generic YouTube videos and movies.In summary, the central question is whether large-scale noisy supervision from narrated instructional videos can surpass smaller supervised datasets of aligned video-text pairs for learning joint embeddings. The results provide evidence that the answer is yes.


## What is the main contribution of this paper?

The main contribution of this paper is the creation and use of a large-scale dataset called HowTo100M to learn powerful video-language embeddings. More specifically:- They collect a dataset of 136 million video clips paired with automatically transcribed narrations from 1.22 million instructional videos depicting over 23,000 tasks. This dataset is orders of magnitude larger than existing paired video-text datasets.- They show this large amount of noisy supervision can be used to learn an effective joint embedding of video and text. Their model trained on HowTo100M alone sets a new state-of-the-art on text-based action localization and retrieval tasks on existing instructional video datasets. - They demonstrate the learned embeddings can transfer well to other domains by fine-tuning on generic YouTube videos (MSR-VTT) and movies (LSMDC), outperforming models trained on those datasets alone.- They analyze the effect of dataset scale, showing performance on downstream tasks improves steadily as the number of videos for pretraining increases, with no signs of saturation.In summary, the key contribution is demonstrating that large-scale weakly-supervised data can be leveraged to learn powerful text-video embeddings that transfer across domains, reducing the need for expensive manual annotation. The introduction of the HowTo100M dataset enables future work in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces HowTo100M, a new large-scale dataset of 136 million video clips paired with narration captions collected from 1.2 million instructional videos, and shows this data can be used to learn powerful video-language representations that achieve state-of-the-art results on text-based video retrieval and action localization tasks.


## How does this paper compare to other research in the same field?

Here are a few key thoughts on how this paper compares to other research in learning joint embeddings of video and language:- The main novelty of this work is the creation of a massive new dataset called HowTo100M for learning video-text embeddings. At 136 million clip-caption pairs sourced from over 1 million narrated instructional videos, HowTo100M is orders of magnitude larger than any existing video-text dataset. - Previous datasets in this field tend to have tens to hundreds of thousands of manually annotated video-caption pairs. In contrast, the key advantage of HowTo100M is that the narrations are not manually annotated but automatically obtained, making the data collection process fast and scalable.- The authors demonstrate that models trained on this large-scale noisy HowTo100M data can outperform models trained on smaller but cleaner datasets on tasks like text-to-video retrieval and action localization in instructional videos. This shows the power of pre-training on large scale weakly labeled data.- They also show the model trained on HowTo100M transfers well to non-instructional videos from MSR-VTT and LSMDC datasets via fine-tuning. This demonstrates the generic video-text representations learned from instructional videos.- Overall, a key novelty is the proposed data collection process and scale of HowTo100M compared to prior datasets. The authors also introduce a simple but effective model architecture that can learn from noisy narrations at scale.- The work builds upon recent advances in self-supervised learning from audio-visual data and web-scale pre-training of language models. But the authors are among the first to show web-scale pre-training for joint video-text models.In summary, the scale and proposed data collection process for HowTo100M combined with strong transfer results are the main novel contributions compared to existing work on video-text embeddings.
