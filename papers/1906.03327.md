# [HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million   Narrated Video Clips](https://arxiv.org/abs/1906.03327)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is:

Can a useful joint text-video embedding be learned from a large amount of noisy narrated instructional videos without manual annotation?

The authors explore using readily available narrated instructional videos from YouTube as a source of supervision for learning a joint text-video embedding. The narrations from these videos provide a natural but noisy form of supervision as they are not guaranteed to align perfectly with the visual content. 

The main hypothesis is that despite the noise, the sheer scale of narrated instructional videos available could allow learning a powerful text-video embedding model, without needing any manually annotated video-caption pairs.

The authors demonstrate this by collecting a new dataset of 136 million video clips with automatic narration subtitles called HowTo100M. They show that models trained on this data alone can outperform prior work on instructional video datasets like YouCook2 and CrossTask. The pre-trained models also transfer reasonably via fine-tuning to other domains like generic YouTube videos and movies.

In summary, the central question is whether large-scale noisy supervision from narrated instructional videos can surpass smaller supervised datasets of aligned video-text pairs for learning joint embeddings. The results provide evidence that the answer is yes.


## What is the main contribution of this paper?

 The main contribution of this paper is the creation and use of a large-scale dataset called HowTo100M to learn powerful video-language embeddings. More specifically:

- They collect a dataset of 136 million video clips paired with automatically transcribed narrations from 1.22 million instructional videos depicting over 23,000 tasks. This dataset is orders of magnitude larger than existing paired video-text datasets.

- They show this large amount of noisy supervision can be used to learn an effective joint embedding of video and text. Their model trained on HowTo100M alone sets a new state-of-the-art on text-based action localization and retrieval tasks on existing instructional video datasets. 

- They demonstrate the learned embeddings can transfer well to other domains by fine-tuning on generic YouTube videos (MSR-VTT) and movies (LSMDC), outperforming models trained on those datasets alone.

- They analyze the effect of dataset scale, showing performance on downstream tasks improves steadily as the number of videos for pretraining increases, with no signs of saturation.

In summary, the key contribution is demonstrating that large-scale weakly-supervised data can be leveraged to learn powerful text-video embeddings that transfer across domains, reducing the need for expensive manual annotation. The introduction of the HowTo100M dataset enables future work in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper introduces HowTo100M, a new large-scale dataset of 136 million video clips paired with narration captions collected from 1.2 million instructional videos, and shows this data can be used to learn powerful video-language representations that achieve state-of-the-art results on text-based video retrieval and action localization tasks.


## How does this paper compare to other research in the same field?

 Here are a few key thoughts on how this paper compares to other research in learning joint embeddings of video and language:

- The main novelty of this work is the creation of a massive new dataset called HowTo100M for learning video-text embeddings. At 136 million clip-caption pairs sourced from over 1 million narrated instructional videos, HowTo100M is orders of magnitude larger than any existing video-text dataset. 

- Previous datasets in this field tend to have tens to hundreds of thousands of manually annotated video-caption pairs. In contrast, the key advantage of HowTo100M is that the narrations are not manually annotated but automatically obtained, making the data collection process fast and scalable.

- The authors demonstrate that models trained on this large-scale noisy HowTo100M data can outperform models trained on smaller but cleaner datasets on tasks like text-to-video retrieval and action localization in instructional videos. This shows the power of pre-training on large scale weakly labeled data.

- They also show the model trained on HowTo100M transfers well to non-instructional videos from MSR-VTT and LSMDC datasets via fine-tuning. This demonstrates the generic video-text representations learned from instructional videos.

- Overall, a key novelty is the proposed data collection process and scale of HowTo100M compared to prior datasets. The authors also introduce a simple but effective model architecture that can learn from noisy narrations at scale.

- The work builds upon recent advances in self-supervised learning from audio-visual data and web-scale pre-training of language models. But the authors are among the first to show web-scale pre-training for joint video-text models.

In summary, the scale and proposed data collection process for HowTo100M combined with strong transfer results are the main novel contributions compared to existing work on video-text embeddings.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Collecting even more readily-available and unlabeled video data. The authors observed that their model performance continued improving as they scaled up the amount of training data, without seeming to saturate. Therefore, they suggest collecting more narrated instructional videos from YouTube could lead to further improvements.

- Exploring different model architectures. The authors used a relatively simple embedding model in this work. They suggest exploring more complex models like attention mechanisms or transformers trained on their dataset could be beneficial. 

- Pretraining on HowTo100M for downstream tasks. The authors suggest their dataset, features, and pre-trained models can serve as a useful resource for pretraining on a variety of video-and-language tasks. They encourage exploring fine-tuning on other datasets.

- Learning from other weakly-aligned data. The authors' approach of learning from narrated instructional videos could inspire collecting and learning from other sources of weakly aligned video and text data like vlogs, product reviews, etc.

- Reducing the amount of manually annotated data needed. The authors show competitive performance can be attained with only a fraction of labelled data by pretraining on their dataset. This suggests pretraining could help reduce the annotation effort for new datasets.

- Studying alignment of narrations. The paper mainly focused on learning joint embeddings. The authors suggest analyzing the alignment between narrations and video content as an interesting direction for future work.

- More cross-modal transfer learning. The authors demonstrated pretraining on their instructional video dataset transfers well to other types of videos. Further exploring what other vision-and-language domains could benefit from their pretrained model is suggested.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper introduces HowTo100M, a large-scale dataset of 136 million video clips paired with narration subtitles sourced from 1.22 million instructional YouTube videos depicting humans performing over 23,000 different visual tasks. The authors use this data to train a joint text-video embedding model without requiring any manually annotated labels. They show this embedding transfers well to other instructional video datasets like CrossTask and YouCook2, outperforming models trained on those smaller datasets alone. The embedding also transfers reasonably to generic YouTube videos in MSR-VTT with some fine-tuning, despite the domain mismatch, showing the generalization ability of pretraining on large diverse narrated video data. The main contributions are the large-scale HowTo100M dataset collected efficiently without manual effort, the powerful text-video embedding model trained on this data, and analysis of its transfer learning abilities.


## Summarize the paper in two paragraphs.

 Here is a 2-paragraph summary of the paper:

The paper introduces HowTo100M, a large-scale dataset of 136 million video clips paired with narrated captions sourced from 1.22 million instructional YouTube videos depicting humans performing over 23,000 visual tasks. The key advantage of this dataset is its unprecedented scale compared to existing video-text datasets like MSR-VTT or YouCook2 which have orders of magnitude fewer clip-caption pairs. The authors use this data to learn powerful text-video embeddings without any manually annotated descriptions. They show that models trained on HowTo100M alone achieve state-of-the-art on existing instructional video datasets like CrossTask and YouCook2 for tasks like text-based action localization and text-to-video retrieval. The HowTo100M embeddings also transfer well to non-instructional videos like MSR-VTT and LSMDC when fine-tuned on a small labeled subset, outperforming models trained on these datasets alone. This demonstrates the value of pretraining on large weakly-labeled narrated video data.

In summary, the key contributions are: (1) Introduction of a massive narrated video dataset HowTo100M; (2) State-of-the-art results on instructional video datasets by training on HowTo100M alone; (3) Strong transfer learning results on non-instructional videos by fine-tuning HowTo100M embeddings. The scale and weak supervision of HowTo100M enables learning powerful text-video embeddings that transfer across domains. This could enable progress on tasks like retrieval, localization and video QA that rely on joint understanding of video and language.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method presented in the paper:

This paper proposes learning joint text-video embeddings by leveraging the large amount of narrated instructional videos available on YouTube. The authors collect a new dataset called HowTo100M containing 136 million video clips paired with automatically transcribed narrations from 1.22 million videos depicting humans performing over 23,000 visual tasks. To learn an embedding, they extract visual features from the video clips using pre-trained CNNs and textual features from the narrations using pre-trained word embeddings. These features are fed into a joint embedding model composed of non-linear fully connected layers trained with a max-margin ranking loss. The loss encourages matching clip-caption pairs to have high cosine similarity in the joint space while pulling apart mismatched pairs. The embedding is trained on their HowTo100M dataset and evaluated on tasks like text-to-video retrieval and action localization on other datasets. The authors show the learned embedding transfers well to other domains with minimal fine-tuning while outperforming prior work.


## What problem or question is the paper addressing?

 The paper is addressing the problem of learning powerful joint embeddings of video and text without requiring large-scale manually annotated datasets. The key questions the paper seeks to answer are:

- Can large amounts of unlabeled narrated instructional videos from the web be leveraged to learn a strong joint text-video embedding?

- How does such an embedding perform on existing datasets for tasks like text-to-video retrieval and action localization compared to models trained on those datasets directly? 

- Can the embedding transfer well to non-instructional videos through fine-tuning?

- Is the large scale nature of the proposed HowTo100M dataset critical for learning good video-text embeddings?

The paper introduces the new HowTo100M dataset of 136 million narrated video clips to investigate these questions. It demonstrates the value of pretraining on this large narrated video dataset by showing strong performance on instructional video datasets like CrossTask and YouCook2. It also shows the pretrained model can transfer well to non-instructional videos like MSR-VTT and LSMDC through fine-tuning. Overall, the paper makes the case for leveraging readily available narrated video data at scale for learning joint video-text representations.
