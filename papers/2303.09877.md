# [On the Effects of Self-supervision and Contrastive Alignment in Deep   Multi-view Clustering](https://arxiv.org/abs/2303.09877)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, it seems the central research question is:

What is the effect of self-supervision, and in particular contrastive learning, on the performance of deep multi-view clustering methods?

The key points related to this question are:

- Self-supervision is an important component in many recent deep multi-view clustering methods. However, there is a lack of rigorous analysis on the impact of different self-supervised objectives.

- The paper focuses specifically on contrastive learning for aligning representations across views, which has shown promising results. 

- Through theoretical analysis, the authors show contrastive alignment can negatively impact cluster separability, especially as the number of views increases.

- They propose a unified framework for analyzing deep multi-view clustering methods and implement several new instances with different self-supervision strategies.

- Extensive experiments are conducted to evaluate the effect of contrastive alignment vs other self-supervision objectives like reconstruction and mutual information maximization.

- Key findings are that contrastive alignment hurts performance with many views, while maximizing mutual information helps. All methods benefit from some form of self-supervision.

In summary, the central question is focused on rigorously analyzing the effects of self-supervision, particularly contrastive learning, on deep multi-view clustering performance across different datasets. The proposed framework, theory, and experiments aim to address this question.
