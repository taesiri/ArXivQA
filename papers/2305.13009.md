# [Textually Pretrained Speech Language Models](https://arxiv.org/abs/2305.13009)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether textual language models can be used to improve generative spoken language models (GSLMs). Specifically, the authors propose a method called TWIST for initializing and training GSLMs using pretrained textual language models, and investigate whether this "warm start" from textual models helps GSLMs generate better quality and more natural speech compared to models trained from scratch ("cold start"). The key hypothesis appears to be that despite speech and text operating at very different granularities (speech tokens capturing 20-40ms windows vs. text tokens spanning longer linguistic concepts), the close connection between speech and text means that transferring knowledge from textual LMs can benefit GSLMs. The authors test this hypothesis through extensive experiments initializing GSLMs of various sizes from different pretrained textual LMs.In summary, the main research question is whether warm-starting GSLMs from textual LMs improves performance compared to cold-start GSLMs, with the hypothesis that speech and text are closely connected enough for textual LM knowledge transfer to help GSLMs generate better quality speech. The paper aims to empirically test this hypothesis through their proposed TWIST initialization method.
