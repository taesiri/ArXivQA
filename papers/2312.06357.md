# [FOSS: A Self-Learned Doctor for Query Optimizer](https://arxiv.org/abs/2312.06357)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from the paper:

This paper introduces FOSS, a novel deep reinforcement learning (DRL)-based query optimization framework. Unlike prior approaches that either learn to construct execution plans from scratch or only coarse-grained steering of the traditional optimizer through hints, FOSS takes a "whitebox-expertise" approach by incrementally refining and modifying suboptimal plans generated by the traditional optimizer in a step-by-step manner. Specifically, FOSS consists of a planner module that models plan refinement as a Markov decision process solved via DRL, as well as an asymmetric advantage model that serves to select the optimal plan among candidates. To accelerate training, FOSS further incorporates a simulated environment utilizing the advantage model and traditional optimizer for efficient self-bootstrapping with ample simulated experiences. Evaluations on Join Order Benchmark, TPC-DS, and Stack Overflow datasets demonstrate FOSS’s superior performance over state-of-the-art methods, reducing total query latency by 15-83% against PostgreSQL while achieving better generalization. The key innovation is the whitebox fine-grained plan optimization approach starting from traditional optimizer’s original plans. By balancing practicality and high optimization potential, FOSS opens up a promising new direction for learned query optimization.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

FOSS is a novel deep reinforcement learning-based query optimization framework that incrementally refines suboptimal aspects of plans from a traditional optimizer through simulated experiences to achieve lower latency.


## What is the main contribution of this paper?

 According to the paper, the main contributions of this work are:

1. It introduces a new approach called "plan-doctor" for query optimization, which modifies the original query plans from a traditional optimizer in a step-by-step manner to generate better plans. This strikes a balance between plan-constructor and plan-steerer approaches.

2. It proposes a framework called FOSS (modi\underline{\textbf{f}}y \underline{\textbf{o}}riginal plan \underline{\textbf{s}}tep by \textbf{\underline{s}}tep) which includes a planner and a selector based on deep reinforcement learning and an asymmetric advantage model. 

3. It constructs a simulated environment using the asymmetric advantage model and traditional optimizer to accelerate the training of the planner through self-bootstrapping with simulated experiences.

4. Experimental results demonstrate that FOSS outperforms state-of-the-art methods in terms of latency performance and optimization time, reducing total latency by 15-83% compared to PostgreSQL across benchmarks.

In summary, the main contribution is the proposal of the FOSS framework and plan-doctor approach that can effectively and efficiently optimize query plans in a step-by-step manner. The use of simulated experiences is also a key contribution for accelerating training.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Query optimization
- Deep reinforcement learning (DRL) 
- Learned query optimizer
- Plan-constructor
- Plan-steerer
- Plan-doctor (introduced in this paper)
- Markov decision process (MDP)
- Simulated environment
- Asymmetric advantage model
- Model-based reinforcement learning 
- Fine-grained optimization
- Original plan modification
- Step-by-step optimization
- Join order benchmark (JOB)
- Training efficiency 
- Generalization ability
- Bootstrapping 

The paper introduces a novel framework called FOSS that takes a "plan-doctor" approach to query optimization, leveraging and modifying the original query plans from a traditional optimizer in a step-by-step manner using deep reinforcement learning. Key innovations include the asymmetric advantage model for plan evaluation and selection, and the use of a simulated environment for efficient training. Evaluations are performed on standard benchmarks like JOB and TPC-DS to demonstrate improved optimization ability compared to state-of-the-art techniques.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces a "whitebox-expertise" approach called plan-doctor. What are the key components of a plan-doctor and how does it differ from a "blackbox-expertise" approach like plan-steerer?

2. The planner module in FOSS is based on a DRL framework. What are the key elements like state, action, reward, and agent in this DRL formulation? How are they designed for the query optimization task?

3. The asymmetric advantage model (AAM) serves dual purposes - as a selector and a reward indicator. Explain the architecture and loss function formulation of AAM. How does it quantify the advantage between two plans?

4. FOSS employs a simulated environment for efficient training. Explain the components and workflow of this simulated environment. How does it bootstrap FOSS to generate high-quality experiences? 

5. The paper mentions a validation phase during training. What is its purpose and how does it contribute to the overall performance of FOSS?

6. Analyze the various actions like swapping table positions and overriding join methods that constitute the action space of the planner module. What mechanisms are used to restrict illegal actions?

7. The reward function of the planner module consists of a bounty and penalty component. Explain how each of these is calculated and how they shape the agent's behavior.  

8. One of the advantages mentioned is that FOSS can optimize queries within 1-3 steps in most cases. Analyze the results in Figure 8 to discuss how the maxsteps parameter affects this.

9. Compare and contrast how the optimization time and plan search space of FOSS differs from other SOTA methods like plan-constructor and plan-steerer.

10. The paper demonstrates the capability of FOSS to work with multiple agents in parallel. Discuss the mechanisms used for multi-agent training and inference. How does it enhance robustness?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "FOSS: A Self-Learned Doctor for Query Optimizer":

Problem: 
Traditional query optimizers face challenges in finding optimal query plans due to the exponential search space and errors in cardinality/cost estimations. Prior works using deep reinforcement learning (DRL) for query optimization either learn to construct plans from scratch (inefficient) or steer the traditional optimizer with hints (limited search space). 

Proposed Solution:
The paper proposes FOSS, a novel DRL-based optimizer that takes a "plan-doctor" approach. Starting from the original suboptimal plan from traditional optimizer, FOSS incrementally refines it through a sequence of modification actions to get better plans. 

FOSS has two key components:
1) Planner: Models plan refinement as an MDP solved with DRL. It iteratively swaps join orders or overrides join methods to generate new candidate plans.
2) Selector: An "asymmetric advantage model" that selects the optimal plan from candidates by comparing their advantage scores.

To accelerate learning, FOSS builds a simulated environment using the advantage model and traditional optimizer to generate ample simulated experiences.

Main Contributions:
- Introduces the plan-doctor approach that refines original plans via step-by-step actions, balancing efficiency and search space.
- Designs an asymmetric advantage model for optimal plan selection and reward definition. 
- Constructs a simulated environment for self-bootstrapping using simulated experiences.
- Experiments show FOSS reduces latency by 15-83% over PostgreSQL, outperforming state-of-the-art methods in latency and optimization time.

In summary, FOSS is an innovative optimizer that achieves practical efficiency and broad search space by starting from and refining traditional plans. The simulated environment allows efficient self-learning. Experiments demonstrate FOSS's superior optimization ability over existing methods.
