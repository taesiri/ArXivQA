# [You Can Ground Earlier than See: An Effective and Efficient Pipeline for   Temporal Sentence Grounding in Compressed Videos](https://arxiv.org/abs/2303.07863)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform effective and efficient temporal sentence grounding directly in the compressed video domain, without needing to fully decompress the videos into sequences of RGB frames. 

The key hypothesis is that by extracting and fusing features directly from compressed video (I-frames, motion vectors, residuals), the model can achieve strong grounding performance while being much more efficient than methods that operate on fully decompressed videos.

Specifically, the paper introduces the new task of "compressed-domain temporal sentence grounding" and proposes a novel model called Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) to address this task. The key ideas are:

- Extract appearance features from I-frames, motion features from motion vectors, and residual features from residuals. This avoids needing to decompress the full videos.

- Generate "pseudo features" to enrich the partial appearance and motion information. 

- Apply spatial-temporal attention to model query-relevant activity content.

- Adaptively fuse motion and appearance information guided by the residuals.

- Integrate visual features with query features for final grounding.

The central hypothesis is that by effectively fusing these compressed domain features, the model can achieve strong grounding accuracy while being much faster and more efficient than existing methods that use fully decompressed videos. Experiments validate this hypothesis and show state-of-the-art accuracy and efficiency.
