# [You Can Ground Earlier than See: An Effective and Efficient Pipeline for   Temporal Sentence Grounding in Compressed Videos](https://arxiv.org/abs/2303.07863)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform effective and efficient temporal sentence grounding directly in the compressed video domain, without needing to fully decompress the videos into sequences of RGB frames. 

The key hypothesis is that by extracting and fusing features directly from compressed video (I-frames, motion vectors, residuals), the model can achieve strong grounding performance while being much more efficient than methods that operate on fully decompressed videos.

Specifically, the paper introduces the new task of "compressed-domain temporal sentence grounding" and proposes a novel model called Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) to address this task. The key ideas are:

- Extract appearance features from I-frames, motion features from motion vectors, and residual features from residuals. This avoids needing to decompress the full videos.

- Generate "pseudo features" to enrich the partial appearance and motion information. 

- Apply spatial-temporal attention to model query-relevant activity content.

- Adaptively fuse motion and appearance information guided by the residuals.

- Integrate visual features with query features for final grounding.

The central hypothesis is that by effectively fusing these compressed domain features, the model can achieve strong grounding accuracy while being much faster and more efficient than existing methods that use fully decompressed videos. Experiments validate this hypothesis and show state-of-the-art accuracy and efficiency.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new compressed-domain temporal sentence grounding (TSG) task, which aims to directly leverage compressed videos instead of fully decompressed frames as input for grounding. This is a more practical but challenging setting compared to previous TSG works that rely on decompressed frames. 

2. It presents a novel framework called Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) to address the compressed-domain TSG task. The key ideas are:

- Extract and utilize three types of low-level visual features from compressed video: I-frame features for appearance, motion vector and residual features from P-frames for motion. 

- Design spatial and temporal attention modules to focus on query-relevant regions and temporal dynamics.

- Adaptively fuse motion and appearance information guided by residual features.

- Integrate visual features with textual features for final grounding prediction.

3. Experiments on three datasets demonstrate that the proposed TCSF framework outperforms state-of-the-art methods under both fully supervised and weakly supervised settings, showing its effectiveness and efficiency.

In summary, the main contribution is proposing a new compressed-domain setting for TSG and an effective framework TCSF to address this challenging task. The key advantage is efficiently leveraging compressed videos without needing full decomposition.
