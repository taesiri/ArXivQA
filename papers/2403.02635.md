# [PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of   Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.02635)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-agent reinforcement learning (MARL) suffers from slow training convergence due to non-independent and identically distributed (Non-IID) exploration experiences among agents. Agents can fall into local optima and fail to learn to cooperate.

- Factors causing Non-IID experiences: Different exploration policies leading to different state/action distributions; Incorrect actions by some agents negatively impacting correct actions of others.

- This leads to inconsistent value function update directions during centralized training, resulting in slower convergence.

Solution:
- Propose 3 parameter sharing methods inspired by federated learning to accelerate training: 
   1) Average Periodically Parameter Sharing (A-PPS)
   2) Reward-Scalability Periodically Parameter Sharing (RS-PPS)
   3) Partial Personalized Periodically Parameter Sharing (PP-PPS)

- These methods allow agents to share parameters periodically to exchange useful exploration knowledge while protecting privacy.

- RS-PPS uses reward buffer to determine agent weight in aggregation. Rewards estimate exploration quality.

- PP-PPS separates representation and value modules. Only aggregates value modules to keep personalization.

Contributions:  
- Introduce federated learning ideas into MARL algorithm QMIX with 3 sharing approaches.

- Evaluate approaches on StarCraft Multi-Agent Challenge and show faster convergence over QMIX and ability to solve tasks QMIX fails.

- RS-PPS demonstrates best performance in complex environments by correcting exploration weights.

- Simple to implement approaches that work with many MARL algorithms.
