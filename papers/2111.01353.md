# [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a self-attention layer of ViT (with image patches as input) express any convolution operation? The paper aims to compare the expressive power of a self-attention layer in Vision Transformers (ViT) versus a convolutional layer in Convolutional Neural Networks (CNNs). Specifically, it investigates whether a ViT layer can perform arbitrary convolution operations when the input consists of image patches rather than pixels. The key contributions and findings are:- The paper proves that a single ViT layer with 9 attention heads and relative positional encoding can express any convolution operation, even when the input is image patches rather than pixels. This shows ViT is at least as powerful as CNNs in terms of expressivity.- The paper provides lower bounds on the number of heads required for a self-attention layer to express convolution, showing the construction with 9 heads is optimal. It shows ViT requires fewer heads with patch input compared to pixel input to express convolutions.- Based on the theoretical findings, the paper proposes a two-phase training pipeline to inject convolutional inductive bias into ViT. Experiments show this method improves ViT's accuracy, efficiency, and optimization stability compared to baselines.In summary, the central hypothesis is that ViT can perform arbitrary convolutions with patch input, which is affirmatively answered through theoretical analysis and an effective training method that leverages this insight.


## What is the main contribution of this paper?

The main contribution of this paper is providing a constructive proof that Vision Transformers (ViTs) can express convolutional neural networks (CNNs). Specifically:- The paper formally proves that a single ViT layer with image patches as input can perform any convolution operation. This is enabled by leveraging the multi-head attention mechanism and relative positional encoding in ViT. - The paper provides lower bounds on the number of heads required for a ViT layer to express convolution, showing that their construction is optimal in terms of heads. For common settings where the convolution kernel size K < 2P (P is patch size), the paper shows that only 9 heads are needed, much less than the K^2 heads required with pixel input.- The paper proposes a two-phase training pipeline that initializes a ViT to mimic a pretrained CNN, injecting convolutional inductive bias. Experiments show this significantly improves ViT performance in low data regimes compared to random initialization.In summary, this paper provides both theoretical and empirical evidence that ViTs can effectively perform convolution. The key insight is utilizing multi-head attention and relative positional encoding. The analysis also reveals ViTs are more efficient at mimicking convolution with patch input versus pixel input.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper constructively proves that a Vision Transformer layer with multi-head attention and relative positional encoding can express any convolution operation, showing the comparable expressive power of Transformers and CNNs for computer vision tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper provides both theoretical and empirical results showing that Vision Transformers (ViTs) can express convolutional neural networks. Other recent works have also explored connections between ViTs and CNNs, but this paper makes the stronger claim of expressiveness and proves constructive results.- The paper considers ViTs in the more realistic setting of taking image patches as input, rather than individual pixels. Previous work by Cordonnier et al. proved expressiveness results for pixel-level inputs, but patch inputs are more relevant to modern ViT architectures. - The authors provide lower bounds on the number of heads needed for a ViT layer to express convolution, demonstrating their construction requiring 9 heads is optimal. Other works have not provided this type of analysis of head efficiency.- Empirically, the paper demonstrates a way to inject convolutional inductive bias into ViTs via a two-phase training approach. This allows ViTs to achieve improved performance in low-data regimes. Other recent works on training ViTs focus more on regularization, data augmentation, or advanced optimization.- The paper connects well to the broader debate around whether attention or convolution is more suitable for computer vision tasks. By showing ViTs can mimic convolution, the authors provide evidence attention may be universally powerful for vision.In summary, this paper advances the theoretical understanding of ViTs and their relationship to CNNs, providing constructive expressiveness results and optimality guarantees. The two-phase training method is also a novel way of combining the strengths of CNNs and ViTs.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Exploring different architectural designs and hyperparameter choices for Vision Transformers to improve performance, especially in low data regimes. For example, investigating techniques to make ViT training more data-efficient and stable without relying on external data. - Extending the theoretical analysis to more complex ViT architectures beyond the basic setting studied in this work. For instance, analyzing ViTs with smaller number of heads or investigating how the theoretical construction could be adapted for models like Swin Transformers.- Leveraging the connection established between CNNs and ViTs to develop better initialization or regularization techniques. The authors suggest it may be possible to initialize general ViT architectures in a way that injects convolutional inductive biases even when the exact theoretical mapping doesn't apply.- Applying the proposed two-phase training strategy to other domains beyond computer vision, such as speech or time series data, to inject useful inductive biases from CNNs or RNNs into Transformers.- Exploring other ways to combine the strengths of CNNs and ViTs, such as using hybrid architectures with both convolutional and attentional blocks. Studying how to get the best of both worlds.- Analyzing other facets of expressiveness besides the ability to perform convolution, such as depth efficiency and architectural constraints, to better understand the representational capacities of ViTs and CNNs.In summary, the main future directions are around extending the analysis to more complex ViT architectures, using the CNN-ViT connection to improve training, and exploring hybrid models that combine convolutional and attentional mechanisms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper demonstrates that Vision Transformers (ViT) can perform convolution operations, which are traditionally done by Convolutional Neural Networks (CNNs). The authors prove constructively that a single ViT layer with image patches as input can express any convolution operation. The key insights are using the multi-head attention mechanism and relative positional encoding in ViT to aggregate features from neighboring image patches in order to compute convolution operations involving pixels across patch boundaries. The authors also derive lower bounds showing their construction is optimal in terms of number of heads. Experimental results confirm that initializing ViT to mimic CNN helps improve performance, especially in low data regimes. Overall, this work provides theoretical analysis and empirical evidence that ViT can effectively perform convolution, offering useful insights into the expressive capacity of self-attention.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proves that a single Vision Transformer (ViT) layer with image patches as input can express any convolution operation. This is shown through a constructive proof utilizing multi-head attention and relative positional encoding to aggregate features for computing convolution across patch boundaries. The paper also provides a lower bound on the number of heads required for a ViT layer to express convolution, showing that 9 heads are necessary and sufficient to express a convolution layer with kernel size KxK when K<2P (where P is the patch size). This demonstrates Vision Transformers with patch input are more "head-efficient" for expressing convolution than pixel input. Based on the theoretical analysis, the paper proposes a two-phase training pipeline to inject convolutional inductive bias into ViTs. The first phase trains a "convolutional" ViT where attention is replaced with convolution. The second phase transfers the pretrained weights into a Transformer model for further training. Experiments on CIFAR-100 demonstrate this pipeline allows ViT to achieve much higher performance compared to random initialization in low data regimes. Additional benefits include faster training without needing a warmup stage. Overall, the paper provides theoretical analysis on ViT expressiveness, and shows convolutional initialization can improve ViT optimization and accuracy.
