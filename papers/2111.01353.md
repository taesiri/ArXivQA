# [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a self-attention layer of ViT (with image patches as input) express any convolution operation? The paper aims to compare the expressive power of a self-attention layer in Vision Transformers (ViT) versus a convolutional layer in Convolutional Neural Networks (CNNs). Specifically, it investigates whether a ViT layer can perform arbitrary convolution operations when the input consists of image patches rather than pixels. The key contributions and findings are:- The paper proves that a single ViT layer with 9 attention heads and relative positional encoding can express any convolution operation, even when the input is image patches rather than pixels. This shows ViT is at least as powerful as CNNs in terms of expressivity.- The paper provides lower bounds on the number of heads required for a self-attention layer to express convolution, showing the construction with 9 heads is optimal. It shows ViT requires fewer heads with patch input compared to pixel input to express convolutions.- Based on the theoretical findings, the paper proposes a two-phase training pipeline to inject convolutional inductive bias into ViT. Experiments show this method improves ViT's accuracy, efficiency, and optimization stability compared to baselines.In summary, the central hypothesis is that ViT can perform arbitrary convolutions with patch input, which is affirmatively answered through theoretical analysis and an effective training method that leverages this insight.


## What is the main contribution of this paper?

The main contribution of this paper is providing a constructive proof that Vision Transformers (ViTs) can express convolutional neural networks (CNNs). Specifically:- The paper formally proves that a single ViT layer with image patches as input can perform any convolution operation. This is enabled by leveraging the multi-head attention mechanism and relative positional encoding in ViT. - The paper provides lower bounds on the number of heads required for a ViT layer to express convolution, showing that their construction is optimal in terms of heads. For common settings where the convolution kernel size K < 2P (P is patch size), the paper shows that only 9 heads are needed, much less than the K^2 heads required with pixel input.- The paper proposes a two-phase training pipeline that initializes a ViT to mimic a pretrained CNN, injecting convolutional inductive bias. Experiments show this significantly improves ViT performance in low data regimes compared to random initialization.In summary, this paper provides both theoretical and empirical evidence that ViTs can effectively perform convolution. The key insight is utilizing multi-head attention and relative positional encoding. The analysis also reveals ViTs are more efficient at mimicking convolution with patch input versus pixel input.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper constructively proves that a Vision Transformer layer with multi-head attention and relative positional encoding can express any convolution operation, showing the comparable expressive power of Transformers and CNNs for computer vision tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper provides both theoretical and empirical results showing that Vision Transformers (ViTs) can express convolutional neural networks. Other recent works have also explored connections between ViTs and CNNs, but this paper makes the stronger claim of expressiveness and proves constructive results.- The paper considers ViTs in the more realistic setting of taking image patches as input, rather than individual pixels. Previous work by Cordonnier et al. proved expressiveness results for pixel-level inputs, but patch inputs are more relevant to modern ViT architectures. - The authors provide lower bounds on the number of heads needed for a ViT layer to express convolution, demonstrating their construction requiring 9 heads is optimal. Other works have not provided this type of analysis of head efficiency.- Empirically, the paper demonstrates a way to inject convolutional inductive bias into ViTs via a two-phase training approach. This allows ViTs to achieve improved performance in low-data regimes. Other recent works on training ViTs focus more on regularization, data augmentation, or advanced optimization.- The paper connects well to the broader debate around whether attention or convolution is more suitable for computer vision tasks. By showing ViTs can mimic convolution, the authors provide evidence attention may be universally powerful for vision.In summary, this paper advances the theoretical understanding of ViTs and their relationship to CNNs, providing constructive expressiveness results and optimality guarantees. The two-phase training method is also a novel way of combining the strengths of CNNs and ViTs.
