# [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a self-attention layer of ViT (with image patches as input) express any convolution operation? The paper aims to compare the expressive power of a self-attention layer in Vision Transformers (ViT) versus a convolutional layer in Convolutional Neural Networks (CNNs). Specifically, it investigates whether a ViT layer can perform arbitrary convolution operations when the input consists of image patches rather than pixels. The key contributions and findings are:- The paper proves that a single ViT layer with 9 attention heads and relative positional encoding can express any convolution operation, even when the input is image patches rather than pixels. This shows ViT is at least as powerful as CNNs in terms of expressivity.- The paper provides lower bounds on the number of heads required for a self-attention layer to express convolution, showing the construction with 9 heads is optimal. It shows ViT requires fewer heads with patch input compared to pixel input to express convolutions.- Based on the theoretical findings, the paper proposes a two-phase training pipeline to inject convolutional inductive bias into ViT. Experiments show this method improves ViT's accuracy, efficiency, and optimization stability compared to baselines.In summary, the central hypothesis is that ViT can perform arbitrary convolutions with patch input, which is affirmatively answered through theoretical analysis and an effective training method that leverages this insight.
