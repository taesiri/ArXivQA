# [Self Reward Design with Fine-grained Interpretability](https://arxiv.org/abs/2112.15034)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a novel approach for designing interpretable and self-optimizing reinforcement learning agents through a framework called Self Reward Design (SRD). The key ideas are:1. The agents are designed in a bottom-up, modular fashion using standard neural network components like convolutional and fully-connected layers. But the components are arranged and initialized in an interpretable way, with each neuron or module having a specific semantic meaning. 2. The agents have a mechanism to reward themselves (compute their own "loss") based on assessing the correctness of their actions. This allows them to optimize themselves through backpropagation on the self-generated rewards, improving performance beyond the initial human design.3. The approach balances human expertise in problem-specific design with the power of neural network optimization. It aims to achieve both interpretability and good performance without needing to specify arbitrary rewards.4. The approach is demonstrated on simple 1D and 2D navigation tasks as well as a simulated MuJoCo agent. The interpretability is fine-grained, with direct manipulation of weights and biases to induce bias towards human-understandable concepts.So in summary, the key hypothesis is that reinforcement learning agents can be made interpretable through deliberate and modular neural network design, and then optimized through a self-assessing reward mechanism. The paper aims to provide a general framework and demonstrate the viability through simple examples.
