# [Efficient Availability Attacks against Supervised and Contrastive   Learning Simultaneously](https://arxiv.org/abs/2402.04010)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Availability attacks add imperceptible noise to training data to prevent unauthorized use. They aim to make the data unlearnable so that algorithms cannot train usable models on it.
- Existing attacks are focused on impeding supervised learning (SL) algorithms. However, a malicious user could bypass this using contrastive learning (CL) algorithms. 
- Recent CL-based attacks are inefficient for real-world use in continually updated datasets.

Proposed Solution:
- The key idea is to use contrastive-like data augmentations (such as color jitter, grayscale etc.) in SL attack frameworks. This allows noises to implicitly adapt to and deceive CL while preserving SL unlearnability.
- Two attack frameworks are proposed:
   1. Augmented Unlearnable Examples (AUE): Enhances data augmentations in the error minimization framework.
   2. Augmented Adversarial Poisoning (AAP): Enhances data augmentations in the error maximization framework.
- These attacks are efficient as they avoid expensive contrastive loss optimization.

Main Contributions:  
- Proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across standard SL and 4 major CL algorithms.
- The attacks demonstrate superior efficiency than existing CL-based attacks.
- AUE attack remarkably reduces accuracy on ImageNet-100 to 7.5%, showcasing prospects for real-world application. 
- The attacks highlight the need for availability attacks to handle both SL and CL algorithms simultaneously.

In summary, the paper proposes efficient yet potent availability attacks using contrastive-like augmentations in SL frameworks. The attacks achieve excellent worst-case unlearnability across SL and CL algorithms, with improved efficiency.
