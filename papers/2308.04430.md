# [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://arxiv.org/abs/2308.04430)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we build high-quality language models while effectively mitigating their legal risk arising from copyrighted or otherwise restricted training data?The key hypothesis seems to be that it is possible to significantly improve the risk-performance tradeoff in language model training by separating low-risk and high-risk data into distinct components of the model - parametric and nonparametric. Specifically, the paper proposes training the parameters of the model only on low-risk permissively licensed data while using high-risk copyrighted data solely in a flexible nonparametric datastore that is only accessed at inference time. This allows leveraging the high-risk data to boost performance without incurring the same legal risks associated with training on it directly.The paper introduces a new dataset of permissively licensed text and shows that models trained solely on this data are competitive in-domain but struggle out-of-domain. It then demonstrates that adding the nonparametric datastore significantly improves out-of-domain performance while providing opt-out and attribution capabilities to mitigate legal risks.In summary, the central hypothesis is that segregating training data by risk into parametric and nonparametric components can yield models with both high quality and lower legal risk. The paper aims to demonstrate the feasibility of this approach.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a new approach to training language models that aims to mitigate copyright risks. Specifically, the key ideas are:1. Separating the training data into two components - low-risk/permissive data and high-risk/non-permissive data. The model parameters are trained only on the low-risk data.2. Using the high-risk data in a nonparametric datastore that is queried at inference time, rather than using it to train model parameters. This allows flexibility like easier opt-out and attribution.3. Introducing the Open License Corpus (OLC), a new 228B token dataset of low-risk/permissive text across multiple domains.4. Proposing SILO, a new language model architecture that combines a parametric component trained on OLC with a nonparametric datastore that can include high-risk data.5. Demonstrating that using the nonparametric datastore significantly improves SILO's out-of-domain performance, with kNN-LM retrieval being more effective than retrieval-in-context.6. Analysis showing kNN-LM benefits more from scaling the datastore size and is more robust to domain shift compared to the parametric component.In summary, the key contribution is presenting a new training approach and model architecture aimed at using copyrighted/restricted data in a safer way, along with empirical evidence showing its effectiveness. The introduced OLC dataset and analyses around domain generalization are also contributions.
