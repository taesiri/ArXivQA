# [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://arxiv.org/abs/2308.04430)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: How can we build high-quality language models while effectively mitigating their legal risk arising from copyrighted or otherwise restricted training data?The key hypothesis seems to be that it is possible to significantly improve the risk-performance tradeoff in language model training by separating low-risk and high-risk data into distinct components of the model - parametric and nonparametric. Specifically, the paper proposes training the parameters of the model only on low-risk permissively licensed data while using high-risk copyrighted data solely in a flexible nonparametric datastore that is only accessed at inference time. This allows leveraging the high-risk data to boost performance without incurring the same legal risks associated with training on it directly.The paper introduces a new dataset of permissively licensed text and shows that models trained solely on this data are competitive in-domain but struggle out-of-domain. It then demonstrates that adding the nonparametric datastore significantly improves out-of-domain performance while providing opt-out and attribution capabilities to mitigate legal risks.In summary, the central hypothesis is that segregating training data by risk into parametric and nonparametric components can yield models with both high quality and lower legal risk. The paper aims to demonstrate the feasibility of this approach.
