# MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action   Recognition with Language Knowledge

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract and introduction, this paper appears to address the following central research questions:1. Is it possible to fine-tune large-scale pretrained vision-language (VL) models for effective zero-shot transfer to action recognition tasks while only leveraging unlabeled video data? 2. Can the knowledge of language be harnessed to expand beyond the limited action vocabulary available in annotated action recognition datasets?The key hypotheses seem to be:- Unlabeled video data can be effectively utilized to significantly improve zero-shot transfer performance on action recognition for pretrained VL models, without needing annotated data.- By combining unlabeled video data with "bags" of language knowledge mined from vision-conditioned (e.g. CLIP, BLIP) and pure language-conditioned (e.g. GPT-3) sources, the performance of VL models on unseen zero-shot action recognition can be substantially boosted.In summary, the core research questions are around improving zero-shot action recognition for VL models like CLIP using only unlabeled videos and leveraging diverse language knowledge sources, removing the need for full supervision or annotated action datasets. The proposed method MAXI aims to address these questions by constructing text bags for unlabeled videos and employing techniques like multiple instance learning to effectively fine-tune VL models.


## What is the main contribution of this paper?

This paper does not appear to have a main contribution stated explicitly. However, based on my reading, the key points seem to be:- The authors propose an approach called "Match, Expand and Improve (MAXI)" for finetuning vision-language (VL) models like CLIP on unlabeled video data and an unpaired action dictionary to improve performance on downstream zero-shot and few-shot action recognition tasks. - The key idea is to construct "text bags" for each unlabeled video by matching to the dictionary, expanding via a language model like GPT-3, and captioning frames with a VL model like BLIP. These noisy text bags act as pseudo-labels. - They use a multiple instance learning objective to finetune CLIP on the unlabeled videos and text bags. This allows robust training without ground truth labels.- Experiments on 7 downstream action recognition datasets show MAXI improves over CLIP by up to 14% on zero-shot transfer and also outperforms supervised baselines.- Ablations validate the approach of using text bags from diverse language sources and learning via multiple instance learning.So in summary, the main contribution seems to be proposing and validating an unsupervised approach to finetune VL models on unlabeled video plus an unpaired dictionary to improve zero-shot and few-shot action recognition, using text bags and multiple instance learning. The key novelty is enabling effective VL finetuning without labeled data.
