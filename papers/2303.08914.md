# MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action
  Recognition with Language Knowledge

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract and introduction, this paper appears to address the following central research questions:1. Is it possible to fine-tune large-scale pretrained vision-language (VL) models for effective zero-shot transfer to action recognition tasks while only leveraging unlabeled video data? 2. Can the knowledge of language be harnessed to expand beyond the limited action vocabulary available in annotated action recognition datasets?The key hypotheses seem to be:- Unlabeled video data can be effectively utilized to significantly improve zero-shot transfer performance on action recognition for pretrained VL models, without needing annotated data.- By combining unlabeled video data with "bags" of language knowledge mined from vision-conditioned (e.g. CLIP, BLIP) and pure language-conditioned (e.g. GPT-3) sources, the performance of VL models on unseen zero-shot action recognition can be substantially boosted.In summary, the core research questions are around improving zero-shot action recognition for VL models like CLIP using only unlabeled videos and leveraging diverse language knowledge sources, removing the need for full supervision or annotated action datasets. The proposed method MAXI aims to address these questions by constructing text bags for unlabeled videos and employing techniques like multiple instance learning to effectively fine-tune VL models.
