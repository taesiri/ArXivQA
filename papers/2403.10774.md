# [Detecting Bias in Large Language Models: Fine-tuned KcBERT](https://arxiv.org/abs/2403.10774)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) trained on online data can exhibit biases related to ethnicity, gender, and race that can lead to discriminatory outcomes.  
- There is a need to analyze and mitigate societal biases in Korean LLMs given language-dependent characteristics.

Methods:
- The authors fine-tune the Korean BERT model (KcBERT) on the Korean Offensive Language Dataset (KOLD) and compare biases between the base and fine-tuned models.  
- They quantify bias using the Log-Probability Bias Score (LPBS) for binary categories like gender/race and the Categorical Bias Score (CBS) for multi-category ethnicity.
- Two bias mitigation methods are proposed:
   1) Data balancing of target words during pre-processing  
   2) Applying dropout and regularization during training.

Results:
- The fine-tuned model shows reduced ethnic bias per the CBS score, but increased gender and racial bias based on surrounding words.  
- Data balancing alone reduces gender and racial bias. Combining with debiasing regularization further alleviates ethnic bias.
- Both methods decrease loss and improve model performance.

Contributions:
- Confirming existence of societal biases in both English and Korean LLMs
- Quantifying bias using LPBS and CBS metrics
- Validating two bias mitigation methods tailored for Korean language characteristics
- Studying bias in models trained on lower-resourced Korean language

The paper demonstrates that biases can arise in Korean LLMs from imbalanced data and language-specific features, which can be mitigated through data-centric and algorithmic approaches.
