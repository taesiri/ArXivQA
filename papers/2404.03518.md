# [SDPose: Tokenized Pose Estimation via Circulation-Guide   Self-Distillation](https://arxiv.org/abs/2404.03518)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer-based methods achieve excellent performance for human pose estimation (HPE) but most are too large and computationally expensive to deploy on edge devices. 
- Smaller transformer models tend to underfit due to their limited capacity, leading to poor performance.
- Existing distillation methods to improve small models have drawbacks like needing extra training or manipulation during feature distillation.

Proposed Solution:
- Introduce the concept of "latent depth" - the effective depth of the transformer layers involved in complete inference. 
- Propose Multi-Cycled Transformer (MCT) module that passes tokens through transformer layers multiple times to increase latent depth without adding parameters.
- Design a self-distillation framework SDPose to extract knowledge from the MCT module into a single-pass model, avoiding extra computation at inference.

Key Contributions:
- First to find looping tokens through transformers increases latent depth and performance without extra parameters.
- Design an MCT module to realize this idea.
- Propose a self-distillation paradigm for the first time in transformer-based HPE to balance performance and efficiency.
- Extensive experiments show SDPose models achieve state-of-the-art results among small models on MSCOCO and CrowdPose datasets. 
- Demonstrate the general applicability of the method by extending it to other models and tasks.

In summary, the paper introduces two novel components - MCT and SDPose self-distillation - to unlock the potential of small transformer models for efficient yet accurate human pose estimation, with experimental results showing state-of-the-art performance. The ideas are generalizable to other models and tasks as well.
