# [Query-Relevant Images Jailbreak Large Multi-Modal Models](https://arxiv.org/abs/2311.17600)

## Summarize the paper in one sentence.

 This paper introduces a visual prompt attack using query-relevant images generated by diffusion models and typography to jailbreak large multi-modal models, evaluates the attack on 12 models across 13 unsafe scenarios, and finds most models vulnerable.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It introduces a novel visual prompt attack that creates query-relevant images to jailbreak large multi-modal models (LMMs) and bypass their defense mechanisms. Specifically, it uses text-to-image generation techniques like stable diffusion and typography to construct images highly correlated to malicious text queries.

2. It constructs a comprehensive safety measurement benchmark consisting of 5,040 text-image pairs across 13 different unsafe scenarios to systematically evaluate the vulnerabilities of LMMs. 

3. It conducts extensive experiments on 12 state-of-the-art LMMs using the proposed benchmark and attack technique. The results demonstrate the fragility of existing LMMs' safety protocols when subjected to this visual prompt attack.

In summary, the key contribution is proposing an effective visual prompt attack to reveal safety issues with current LMMs, alongside building a benchmark to quantify models' robustness. The findings underscore the need to strengthen defense systems of open-source LMMs against potential exploits.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Large multi-modal models (LMMs)
- Safety and security vulnerabilities 
- Visual prompt attacks
- Query-relevant images 
- Jailbreaking LMM defenses
- Text-to-image generation
- Stable diffusion
- Typography
- Multi-modal safety benchmark
- Attack success rate (ASR)
- Evaluation protocols and metrics
- Model limitations and lack of generalization

The paper introduces a visual prompt attack using query-relevant images generated by stable diffusion and typography to jailbreak defenses of large multi-modal models. It constructs a comprehensive multi-modal safety benchmark with 13 scenarios to evaluate attack success rates across 12 state-of-the-art LMMs. The results demonstrate vulnerabilities of existing models and limitations due to overfitting, inaccurate OCR or visual understanding, and weak instruction following abilities. Overall, the key focus is on assessing and underscoring the safety issues with open-source LMMs using visual prompt attacks and rigorous benchmarking.
