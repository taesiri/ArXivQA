# [The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained   Sequence-to-Sequence Models](https://arxiv.org/abs/2101.05667)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can a multi-stage ranking architecture that combines document expansion, pointwise ranking, and pairwise ranking be an effective design pattern for transformer-based text retrieval models?The key components of this proposed design pattern are:- "Expando": Document expansion using a pretrained sequence-to-sequence model to enrich keyword representations of texts before indexing.- "Mono": Pointwise ranking using a relevance classification model (monoT5) to score query-document pairs. - "Duo": Pairwise ranking using a model (duoT5) to compare pairs of documents for the same query.The authors empirically evaluate this Expando-Mono-Duo pattern on several text retrieval tasks, including MS MARCO passage/document ranking, TREC 2020 Deep Learning Track, and TREC-COVID. The results demonstrate that each component contributes to effectiveness gains, validating the design pattern.In summary, the central hypothesis is that the proposed multi-stage Expando-Mono-Duo architecture is an effective pattern for transformer-based retrieval. The experimental results support this hypothesis across several datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing the "Expando-Mono-Duo" design pattern for multi-stage ranking architectures using pretrained sequence-to-sequence models. The key components of this design pattern are:- "Expando" - Document expansion using a sequence-to-sequence model to enrich keyword representations of texts before indexing. This improves retrieval without expensive inference at query time.- "Mono" - Pointwise reranking using a relevance classification model called monoT5. This provides an initial reranking of candidates from keyword search.- "Duo" - Pairwise reranking using a model called duoT5 that compares document pairs. This further improves early precision.The paper shows through experiments on several ad-hoc retrieval tasks that each component provides significant gains, and combining them leads to state-of-the-art or near state-of-the-art results. The generality of the approach across tasks suggests it could be elevated to a standard design pattern for text ranking.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a multi-stage ranking architecture called Expando-Mono-Duo T5 that combines document expansion, pointwise ranking, and pairwise ranking using pretrained sequence-to-sequence models to achieve state-of-the-art effectiveness on several text ranking benchmarks.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research in transformer-based ranking models:- The Expando-Mono-Duo design pattern combines document expansion, pointwise ranking, and pairwise ranking in a multi-stage architecture. This synthesis of different techniques into an end-to-end pipeline is a novel contribution.- Using sequence-to-sequence transformers like T5 for all components is unique. Most prior work uses encoder-only models like BERT.- The paper shows strong empirical results across multiple datasets, achieving effectiveness near or at state-of-the-art. The consistency of results helps validate Expando-Mono-Duo as a generalizable design pattern.- Compared to other works, the techniques are well described and implementations open-sourced. This supports reproducibility.- The ablations and analyses quantifying the impact of each component help justify the contributions. Such controlled experiments are not always present.- For document expansion, most related works use query prediction while this paper uses a sequence-to-sequence approach. The comparison illustrates tradeoffs.- The reranking models are related to other pointwise and pairwise transformer ranking models in the literature. The main novelty is adapting them to T5.Overall, the paper makes multiple contributions in a complete end-to-end pipeline, supported by thorough experiments and analyses. The findings advance the state of the art in transformer-based ranking while also providing reusable and generalizable components for future research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Incorporating more recent innovations into the Expando-Mono-Duo design pattern to further improve effectiveness. The paper notes that while their approach achieves state-of-the-art or near state-of-the-art results across several tasks, there may be opportunities to integrate advances made since their work to boost performance further.- Additional study on the relationship between document expansion and query expansion (pseudo-relevance feedback) from an end-to-end perspective. The paper found that with document expansion, query expansion did not provide much additional benefit, but the interplay between these techniques warrants further investigation. - Exploring the use of other sequence-to-sequence models such as BART and PEGASUS in place of T5 within the Expando-Mono-Duo framework. The approach is designed to be model-agnostic.- Applying the design pattern to other retrieval tasks beyond ad-hoc retrieval to determine if the benefits generalize. The paper validates the approach on several standard test collections, but further domain adaptation may be beneficial.- Continued error analysis to better understand the limitations of the approach and identify areas for improvement. For example, examining cases where duoT5 reranking hurts or does not improve over monoT5.- Exploring enhancements to the duoT5 pairwise reranking component, such as using more training data, incorporating additional context, or investigating different aggregation methods.In summary, the paper provides a solid framework but leaves ample room for further refinement and investigation of transformer-based ranking architectures. The authors lay out some promising directions to build on their work.
