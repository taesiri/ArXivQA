# [BigTranslate: Augmenting Large Language Models with Multilingual   Translation Capability over 100 Languages](https://arxiv.org/abs/2305.18098)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we augment large language models (LLMs) with multilingual translation capability over 100 languages? The key hypothesis is that by continuing to train an existing LLM (LLaMA) with massive Chinese monolingual data, large-scale multilingual parallel data, and multilingual translation instructions, the model can be adapted to perform high-quality translations across more than 100 languages.In particular, the paper proposes and tests the following hypotheses:- Augmenting LLaMA's foundation model with additional Chinese data will enhance its Chinese language abilities, providing a stronger basis for multilingual translation centered on Chinese and English.- Training the foundation model incrementally on a large-scale 102-language parallel corpus will enable it to acquire translation abilities across low-resource and high-resource languages. - Applying instruction tuning focused on translation will unlock the model's latent multilingual translation capabilities.Through experiments on 102 languages, the paper aims to demonstrate that the resulting BigTranslate model can match or exceed the performance of systems like Google Translate and ChatGPT on a diverse range of languages.


## What is the main contribution of this paper?

The main contribution of this paper is presenting BigTranslate, a large language model augmented with multilingual translation capability over 100 languages. The key points are:- They build BigTranslate based on LLaMA-13B, first continuing pre-training it with massive Chinese data, then with a large parallel dataset covering 102 languages. - They propose an incremental data sampling strategy during the 102 language pre-training, exposing the model to high-resource languages first then progressively introducing lower-resource languages. - They fine-tune the model with multilingual translation instructions to unlock its translation abilities. - Experiments show BigTranslate performs comparably to ChatGPT and Google Translate on many language pairs, and even exceeds ChatGPT on 8 pairs when evaluated by GPT-4.- They release BigTranslate to advance research on extending large language models to more languages and improving low-resource translation.In summary, the main contribution is developing and releasing BigTranslate, a large language model capable of translating over 100 languages, which was achieved through pre-training strategies to balance high/low resource languages and instruction tuning for translation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces BigTranslate, a large language model adapted from LLaMA and optimized with massive Chinese data and multilingual parallel data covering 102 languages, enabling it to perform translation among over 100 languages comparably with ChatGPT and Google Translate.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper on BigTranslate compares to other related work in multilingual machine translation and large language models:- Most existing large language models (LLMs) like GPT-3, PaLM, and BLOOM are English-dominant and only support dozens of languages. This paper introduces BigTranslate which adapts LLaMA to translate over 100 languages, significantly expanding the multilingual capabilities of LLMs.- While previous work has explored building multilingual neural machine translation models, they focus on balancing training between high and low resource languages. This paper proposes a new incremental training strategy to learn 102 languages in a curriculum-like manner.- Other multilingual translation models like NLLB aim to translate as many languages as possible but are specialized only for translation. BigTranslate retains the generic capabilities of LLMs while adding multilingual translation ability.- This paper shows strong translation results on 102 languages, outperforming ChatGPT on 8 language pairs. Previous work evaluated ChatGPT and GPT-3 on fewer multilingual tasks.- The training methodology of continuing pre-training on monolingual and parallel data could enable extending other LLMs to more languages. The incremental curriculum learning approach is also novel.In summary, this paper pushes the boundaries of how many languages an LLM can translate by adapting LLaMA with a new incremental training approach. The results demonstrate much wider multilingual capabilities than previous LLMs. The methodology also provides a general framework for training massively multilingual models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Further enhancing BigTranslate's ability in low-resource languages. The paper notes that due to unbalanced data, BigTranslate is still weak in several dozen languages. The authors suggest further extending BigTranslate to improve its capabilities for extremely low-resource languages.- Taking full advantage of BigTranslate's multilingual abilities for other NLP tasks. The authors suggest leveraging BigTranslate's multilingual translation skills to also improve performance across other natural language processing tasks. They note the potential to transfer English and Chinese NLP abilities to lower-resource languages through BigTranslate.- Evaluating on broader datasets. The preliminary experiments in the paper were on a relatively small evaluaton set of 3,001 sentences translated into 102 languages. The authors could expand the evaluation to broader, more diverse datasets.- Human evaluation. The authors rely on automatic metrics like BLEU and GPT-4 for evaluation. They could conduct human evaluation to further validate BigTranslate's translation quality. - Low-resource translation directions. The current evaluation is from X language to English/Chinese. The model could also be evaluated on translation directions like English/Chinese to X language.- Multilingual instruction tuning. The instruction tuning focused on translation prompts. Expanding to instructions covering other tasks could unlock BigTranslate's potential for broader NLP abilities.- Applications of multilingual capabilities. The authors could demonstrate real-world applications leveraging BigTranslate's multilingual translation abilities, like Information Retrieval across languages.In summary, the key future directions focus on improving low-resource language translation, evaluating on broader datasets/tasks, multitask instruction tuning, and demonstrating practical applications.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces BigTranslate, a large language model capable of translating over 100 natural languages. BigTranslate is built on top of the 13B parameter LLaMA model. It is optimized in three main steps: 1) Continue training LLaMA with a large amount of Chinese monolingual data to improve its Chinese language abilities. 2) Continue training the model on a large-scale parallel dataset covering 102 languages to give it multilingual translation potential. A curriculum learning strategy is used to balance learning across high-resource and low-resource languages. 3) Fine-tune the model using multilingual translation instructions to unlock its translation capabilities. Experiments show BigTranslate performs comparably to ChatGPT and Google Translate on many language pairs and even outperforms ChatGPT on 8 pairs. The model aims to advance multilingual translation research by equipping a single LLM with the ability to translate over 100 languages.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper introduces BigTranslate, a large language model adapted from LLaMA that is capable of translating over 100 natural languages. The authors aim to expand the multilingual capabilities of large language models beyond just English and a few other languages. BigTranslate is constructed in three main steps. First, LLaMA-13B is trained on massive Chinese monolingual data to improve performance on Chinese text. Second, the model is trained on a large parallel dataset covering 102 languages using a curriculum learning approach to balance learning across high and low resource languages. Third, the model is instruction tuned on multilingual translation data. Experiments compare BigTranslate to Google Translate and ChatGPT on 102 language translation tasks. Results show BigTranslate achieves comparable performance to Google and ChatGPT on many languages and even outperforms ChatGPT on 8 language pairs when evaluated by GPT-4. The authors demonstrate the potential for adapting large language models to translate 100+ languages, although further improvements are still needed for low resource languages.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper presents BigTranslate, a large language model adapted from LLaMA to enable multilingual translation across over 100 languages. The model is constructed in three steps. First, LLaMA is continued trained on massive Chinese monolingual data to strengthen its Chinese language capabilities. Second, a large-scale parallel dataset covering 102 languages is used to continue training LLaMA in a curriculum learning approach, resulting in a multilingual foundation model. This incremental training strategy exposes the model to high-resource languages first before progressively introducing low-resource languages. Third, the foundation model is instruction-tuned on a dataset of multilingual translation prompts to unlock its translation abilities. Preliminary experiments demonstrate BigTranslate achieves comparable performance to Google Translate and ChatGPT on automatic metrics and even outperforms ChatGPT on 8 language pairs when evaluated by GPT-4. The overall method aims to equip the LLaMA model with extensive multilingual translation capabilities across 100+ languages through additional pre-training and instruction tuning.


## What problem or question is the paper addressing?

The paper is addressing the problem that most existing large language models (LLMs) are English-dominant and only support a small number of languages. The authors aim to equip LLMs with multilingual translation capabilities across over 100 languages. The key questions the paper seems to be tackling are:- How can we adapt an existing LLM to expand its multilingual abilities to support over 100 languages?- How can we train the LLM on unbalanced multilingual data to ensure it still learns low-resource languages? - How does the multilingual LLM compare to state-of-the-art systems like Google Translate and ChatGPT on translating 100+ languages?In summary, the paper is focused on augmenting LLMs with extensive multilingual translation capabilities across a large number of languages, even low-resource ones, and evaluating this multilingual LLM compared to other systems. The lack of broad multilingual support in current LLMs is the key problem being addressed.
