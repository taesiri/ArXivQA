# [BigTranslate: Augmenting Large Language Models with Multilingual   Translation Capability over 100 Languages](https://arxiv.org/abs/2305.18098)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we augment large language models (LLMs) with multilingual translation capability over 100 languages? The key hypothesis is that by continuing to train an existing LLM (LLaMA) with massive Chinese monolingual data, large-scale multilingual parallel data, and multilingual translation instructions, the model can be adapted to perform high-quality translations across more than 100 languages.In particular, the paper proposes and tests the following hypotheses:- Augmenting LLaMA's foundation model with additional Chinese data will enhance its Chinese language abilities, providing a stronger basis for multilingual translation centered on Chinese and English.- Training the foundation model incrementally on a large-scale 102-language parallel corpus will enable it to acquire translation abilities across low-resource and high-resource languages. - Applying instruction tuning focused on translation will unlock the model's latent multilingual translation capabilities.Through experiments on 102 languages, the paper aims to demonstrate that the resulting BigTranslate model can match or exceed the performance of systems like Google Translate and ChatGPT on a diverse range of languages.


## What is the main contribution of this paper?

The main contribution of this paper is presenting BigTranslate, a large language model augmented with multilingual translation capability over 100 languages. The key points are:- They build BigTranslate based on LLaMA-13B, first continuing pre-training it with massive Chinese data, then with a large parallel dataset covering 102 languages. - They propose an incremental data sampling strategy during the 102 language pre-training, exposing the model to high-resource languages first then progressively introducing lower-resource languages. - They fine-tune the model with multilingual translation instructions to unlock its translation abilities. - Experiments show BigTranslate performs comparably to ChatGPT and Google Translate on many language pairs, and even exceeds ChatGPT on 8 pairs when evaluated by GPT-4.- They release BigTranslate to advance research on extending large language models to more languages and improving low-resource translation.In summary, the main contribution is developing and releasing BigTranslate, a large language model capable of translating over 100 languages, which was achieved through pre-training strategies to balance high/low resource languages and instruction tuning for translation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces BigTranslate, a large language model adapted from LLaMA and optimized with massive Chinese data and multilingual parallel data covering 102 languages, enabling it to perform translation among over 100 languages comparably with ChatGPT and Google Translate.
