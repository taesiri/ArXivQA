# [BigTranslate: Augmenting Large Language Models with Multilingual   Translation Capability over 100 Languages](https://arxiv.org/abs/2305.18098)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we augment large language models (LLMs) with multilingual translation capability over 100 languages? The key hypothesis is that by continuing to train an existing LLM (LLaMA) with massive Chinese monolingual data, large-scale multilingual parallel data, and multilingual translation instructions, the model can be adapted to perform high-quality translations across more than 100 languages.In particular, the paper proposes and tests the following hypotheses:- Augmenting LLaMA's foundation model with additional Chinese data will enhance its Chinese language abilities, providing a stronger basis for multilingual translation centered on Chinese and English.- Training the foundation model incrementally on a large-scale 102-language parallel corpus will enable it to acquire translation abilities across low-resource and high-resource languages. - Applying instruction tuning focused on translation will unlock the model's latent multilingual translation capabilities.Through experiments on 102 languages, the paper aims to demonstrate that the resulting BigTranslate model can match or exceed the performance of systems like Google Translate and ChatGPT on a diverse range of languages.
