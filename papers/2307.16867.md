# [Revisiting the Parameter Efficiency of Adapters from the Perspective of   Precision Redundancy](https://arxiv.org/abs/2307.16867)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How can the parameter efficiency of visual adapters be improved by lowering their precision? The key hypothesis is that the parameters of adapters exhibit redundancy in terms of numerical precision. By quantizing the adapters to low bit-widths, this redundancy can be reduced to improve parameter efficiency.In particular, the authors make the following observations:- Adapters converge at flatter local minima compared to full fine-tuning, indicating they are more resilient to noise/disturbances in parameter space. - The parameters of adapters follow approximate Gaussian distributions, allowing efficient quantization by minimizing quantization error.- Lowering adapter precision via quantization-aware training results in minimal loss of performance, even down to 1-bit.To summarize, the central hypothesis is that adapter parameters contain redundant precision that can be removed through quantization to improve efficiency, with minimal impact on accuracy. The experiments aim to demonstrate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Investigating the loss landscape of adapters in vision models and finding that they converge at flatter local minima compared to full fine-tuning. This indicates adapters have greater resilience to noise/disturbances in parameter space.- Inferring from this flatness that adapters contain redundancy in numerical precision, meaning low-precision adapters could perform equally well. - Proposing an approach to train low-bit adapters while minimizing quantization error, based on modeling the distribution of adapter parameters and using an efficient differentiable quantization method.- Conducting extensive experiments showing low-precision adapters have minimal performance degradation compared to full-precision, and 1-bit precision is sufficient.- Demonstrating their proposed quantized adapters achieve state-of-the-art results among PET methods, outperforming other techniques like low-rank factorization while requiring the smallest storage size.In summary, the key contribution appears to be identifying and exploiting precision redundancy in adapters to develop highly efficient low-bit adapters that achieve superior performance and parameter efficiency compared to prior PET methods. The findings suggest the potential of quantization techniques for improving adapter-based PET.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper revisits the parameter efficiency of adapter-based fine-tuning methods for vision transformers by proposing to quantize the adapter parameters to very low precision, finding that 1-bit binary adapters can outperform prior methods while requiring minimal storage.
