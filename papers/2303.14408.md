# [VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic
  Scene Graph Prediction in Point Cloud](https://arxiv.org/abs/2303.14408)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question this paper tries to address is:

How to assist 3D structural understanding with visual-linguistic semantics in order to improve 3D semantic scene graph prediction from point clouds? 

The key hypotheses are:

1) Visual-linguistic semantics from 2D images and natural languages can provide rich and meaningful information to complement the limited semantics in 3D point clouds. 

2) By heterogeneously aligning a powerful multi-modal prediction model (as an oracle) with the 3D prediction model during training, the visual-linguistic semantics can be efficiently transferred to enhance the 3D model's capability in discriminating challenging relations, especially for long-tailed and ambiguous semantic relation triplets.

3) The proposed visual-linguistic semantics assisted training (VL-SAT) scheme is effective in empowering common 3DSSG prediction models with only 3D inputs during inference.

The experiments and results validate these hypotheses, showing VL-SAT can significantly boost 3DSSG prediction performance compared to previous methods, particularly for tail relations. The ablation studies also demonstrate the importance of each designed component in VL-SAT.

In summary, the paper explores and verifies a novel training scheme to assist 3D structural understanding using visual-linguistic semantics, which is the first attempt in 3DSSG prediction to the best of the authors' knowledge.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a novel training scheme called Visual-Linguistic Semantics Assisted Training (VL-SAT) to empower 3D semantic scene graph prediction models. VL-SAT utilizes visual and linguistic semantics from an oracle multi-modal model to assist the training of a 3D prediction model.

2. It is the first work to apply visual-linguistic knowledge transfer to 3D semantic scene graph prediction in point clouds. Previous works on multi-modal knowledge transfer focused more on instance-level 3D perception tasks. 

3. Through node/edge-level collaboration and triplet-level regularization, VL-SAT can significantly improve the 3D prediction model's ability to handle long-tailed and ambiguous semantic relations.

4. Experiments show VL-SAT can boost the performance of common 3DSSG prediction models like SGFN and SGG_point, especially on tail predicates and unseen triplets. The benefits are transferred to the 3D model via backpropagated gradients without needing extra modalities during inference.

5. Comprehensive ablation studies validate the efficacy of each component of the proposed VL-SAT scheme. Both qualitative and quantitative results demonstrate the superiority of VL-SAT over existing methods.

In summary, the key novelty lies in the VL-SAT training scheme and the application of multi-modal knowledge transfer to boost 3D structural understanding, validated by strong experimental results. The scheme is generalizable to different 3DSSG prediction models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a visual-linguistic semantics assisted training scheme to empower 3D scene graph prediction models in point clouds by heterogeneously aligning a strong multi-modal oracle model during training, which significantly boosts performance on long-tailed and ambiguous semantic relations while only needing 3D inputs at inference time.


## How does this paper compare to other research in the same field?

 This paper on VL-SAT for 3D semantic scene graph prediction compares to other related research in a few key ways:

- It focuses specifically on 3D scene graph prediction from point clouds, whereas most prior work has focused on 2D scene graphs from images. There has been some recent work on 3D scene graphs, such as SGPN, SGFN, and SGG_point, but research in this area is still limited.

- It proposes a novel visual-linguistic semantics assisted training (VL-SAT) scheme to address key challenges in 3D scene graph prediction like lack of semantics in point clouds and long-tailed predicate distributions. This differs from prior work that usually just tries to adapt standard 2D techniques to 3D or uses simple data balancing strategies.

- The VL-SAT scheme leverages a powerful multi-modal oracle model that incorporates visual, linguistic, and 3D geometric information during training. This allows transferring richer knowledge to the 3D prediction model compared to using only 3D or only 2D data.

- VL-SAT is designed to be model-agnostic and shows consistent improvements when applied to different 3D prediction models like SGFN and SGG_point. Many existing methods are tailored to specific models or datasets.

- It explores new ways to effectively transfer cross-modal visual-linguistic knowledge to 3D structural understanding tasks. Most prior use of visual-linguistic knowledge has focused on 2D or instance-level tasks.

So in summary, this paper proposes a novel training scheme that is tailored to the emerging problem of 3D scene graph prediction and leverages multi-modal knowledge transfer in a more extensive and integrated way compared to existing approaches. The focus on transferring rich semantics to 3D geometric prediction differentiates it from previous research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some potential future research directions the authors suggest are:

- Exploring how to generally tackle view-dependent ambiguity in spatial relational predicates. The current method defines predicates from a fixed camera view, but it would be useful to handle changing viewpoints more flexibly. 

- Extending the method to weakly supervised settings where only image captions are available rather than full graph annotations. This could involve using techniques like graph matching or grounding to align linguistic structures and graphs.

- Applying similar visual-linguistic assistance ideas to other 3D tasks beyond just scene graph prediction, such as 3D object detection or segmentation. The key principles could potentially transfer.

- Developing specialized encoders or attention mechanisms to better fuse or align features from different modalities like images, point clouds, and text. The cross-modal collaboration is a core part of their approach.

- Evaluating the approach on larger-scale 3D scene graph datasets once they become available, since existing ones are quite small. Generalization ability and scalability are important.

- Exploring other ways to model or inject knowledge, like incorporating common sense knowledge graphs or spatial-semantic priors beyond just feature embedding regularization.

- Applying the method to related applications where structured 3D understanding is needed, like robotics, VR/AR, or autonomous driving. Demonstrating benefits on downstream tasks.

So in summary, some key directions are extending the approach to weakly supervised settings, applying it to other 3D tasks, developing better multimodal fusion techniques, evaluating on larger datasets, incorporating new knowledge sources, and demonstrating applications in related problem areas. The overall paradigm of assisting 3D perception with visual-linguistic knowledge seems promising.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme to improve 3D semantic scene graph prediction in point clouds. The key idea is to train a powerful multi-modal oracle model alongside the 3D prediction model, where the oracle model incorporates visual, linguistic, and 3D geometric information. The oracle model is able to capture reliable structural semantics and its benefits are passed to the 3D model via backpropagated gradients during training. This allows the 3D model to gain discrimination on challenging long-tail and ambiguous semantic relations, despite only seeing 3D point cloud inputs at test time. Experiments show VL-SAT significantly boosts performance of common 3DSSG prediction models like SGFN and SGG_point, especially on tail relations. Overall, VL-SAT provides an effective way to leverage visual-linguistic knowledge to assist 3D structural understanding tasks through a teacher-student training paradigm.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme to improve 3D semantic scene graph prediction in point clouds. The key idea is to train an oracle multi-modal prediction model alongside the 3D prediction model. The oracle model captures reliable structural semantics by fusing information from vision, language, and 3D geometry. It is enhanced using visual-linguistic knowledge from CLIP and collaborates with the 3D model via heterogeneous node and edge-level cross-attention. This allows the multi-modal semantics and benefits of the oracle model to be efficiently embedded into the 3D model through backpropagated gradients. At inference time, only the enhanced 3D model is used with point cloud input to predict scene graphs. 

Experiments are conducted on the 3DSSG dataset. Quantitative and qualitative results demonstrate significant gains over baseline methods, especially for rare tail relations. Comprehensive ablation studies validate the importance of each component in VL-SAT. The scheme successfully boosts different 3D prediction models like SGFN and SGG_point, showing its generalizability. The visual-linguistic knowledge transfer paradigm is shown to be more effective than common distillation strategies. Overall, VL-SAT provides an effective way to empower 3D structural understanding using visual-linguistic semantics while maintaining practical point cloud-only inference.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme to empower 3D semantic scene graph prediction models. The key idea is to train a powerful multi-modal oracle model alongside the 3D prediction model. The oracle model captures reliable structural semantics using extra data from vision (2D images), language (CLIP embeddings), and 3D geometry from the 3D model. The oracle and 3D models are heterogeneously aligned via node/edge-level cross attention and triplet-level regularization based on CLIP. This allows the multi-modal semantics from the oracle model to be efficiently transferred to the 3D model via backpropagated gradients during training. After training with VL-SAT, the enhanced 3D model can predict improved 3D scene graphs using only 3D point cloud inputs. Experiments show VL-SAT significantly boosts 3DSSG prediction, especially for tail relations, and generalizes across different base 3D prediction models.

% !TEX root = ../PaperForReview.tex

\begin{table*}
\centering
\caption{
%
Quantitative Results of 3D semantic scene graph prediction on the 3DSSG validation set~\cite{wald2020learning}. 
%
Evaluations are conducted in terms of object, predicate, and triplet. 
%
The results of SGPN, \sggpoint, and SGFN are based on our reproduced model with point cloud-only inputs, since they don't compute the mA@$k$ metric in their papers.
}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
 \cline{2-14}
& A@$1$ & A@$5$ & A@$10$ & A@$1$ & A@$3$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$5$ & A@$50$ & A@$100$ & mA@$50$ & mA@$100$ \\
 \hline
 SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & \textbf{91.32} & 98.09 & 99.15 & 32.01 & 55.22 & 69.44 & 87.55 & 90.66 & 41.52 & 51.92 \\
\sggpoint~\cite{zhang2021exploiting} & 51.42 & 74.56 & 84.15 & 92.4 & 97.78 & 98.92 & 27.95 & 49.98 & 63.15 & 87.89 & 90.16 & 45.02 & 56.03 \\
 SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 98.17 & 99.33 & 41.89 & 70.82 & 81.44 & 89.02 & 91.71 & 58.37 & 67.61 \\
 \hline
%  \basetwo & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 97.89 & 99.20 & 41.77 & 61.83 & 76.11 & \textbf{92.79} & \textbf{95.06} & 64.18 & \textbf{77.23} \\
 non-VL-SAT & 54.79 & 77.62 & 85.84 & 89.59 & 97.63 & 99.08 & 41.99 & 70.88 & 81.67 & 88.96 & 91.37 & 59.58 & 67.75 \\
%  \hline
%  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
%  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
 VL-SAT (ours) & \textbf{55.66} & \textbf{78.66} & \textbf{85.91} & 89.81 & \textbf{98.45} & \textbf{99.53} & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & \textbf{90.35} & \textbf{92.89} & \textbf{65.09}  & \textbf{73.59} \\
 \hline
 VL-SAT (oracle) & 66.39 & 86.53 & 91.46 & 90.66 & 98.37 & 99.40 & 55.66 & 76.28 & 86.45 & 92.67 & 95.02 & 74.10 & 81.38 \\
 \hline
\end{tabular}}
\label{tab:exp-results}
\end{table*}

% \begin{table*}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
%  \cline{2-14}
%  & A@$1$ & A@$5$ & A@$10$ & A@$1$ & A@$3$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$5$ & A@$50$ & A@$100$ & mA@$50$ & mA@$100$ \\
%  \hline
%  SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & \textbf{91.32} & 98.09 & 99.15 & 32.01 & 55.22 & 69.44 & 87.55 & 90.66 & 41.52 & 51.92 \\
% $\mathrm{\mathbf{SGG_{point}}}$~\cite{zhang2021exploiting} & 51.44 & 74.56 & 84.15 & 90.69 & 97.09 & 98.51 & 27.95 & 46.27 & 58.85 & 87.89 & 90.16 & 39.28 & 52.09 \\
%  SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 98.17 & 99.33 & 41.89 & 70.82 & 81.44 & 89.02 & 91.71 & 58.37 & 67.61 \\
%  \hline
%  \basetwo & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 97.89 & 99.20 & 41.77 & 61.83 & 76.11 & \textbf{92.79} & \textbf{95.06} & 64.18 & \textbf{77.23} \\
%  \basethree & 54.79 & 77.62 & 85.84 & 89.59 & 97.63 & 99.08 & 41.99 & 70.88 & 81.67 & 88.96 & 91.37 & 59.58 & 67.75 \\
%  \hline
% %  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
% %  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
%  Ours & 55.66 & 78.66 & 85.91 & 89.81 & \textbf{98.45} & \textbf{99.53} & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & 90.35 & 92.89 & \textbf{65.09}  & 73.59 \\
%  \hline
% \end{tabular}}
% \caption{
% %
% 3D scene graph prediction performance on 3DSSG validation set. 
% %
% Evaluation is conducted in terms of object, predicate, and triplet. 
% %
% The results of SGPN, $\mathrm{\mathbf{SGG_{point}}}$, and SGFN are based on our reproduced model, since they don't compute the mA@$k$ metric in the paper.
% }
% \label{tab:exp-results-old}
% \end{table*}

\section{Experiments and Discussions}
\label{sec:experiments_and_discussions}

\subsection{Setups and Implementation Details}
\label{subsec:exp-setup}

\noindent\textbf{Datasets.} 
%
We conduct experiments on 3DSSG~\cite{wald2020learning}. It is a 3D semantic scene graph dataset drawn from the 3RScan dataset~\cite{wald2019rio}, with rich annotations about instance segmentation masks and relation triplets.
%
It has $1553$ 3D reconstructed indoor scenes, $160$ classes of objects, and $26$ types of predicates.
%
% We sample $128$ points from a segmented instances to represent a node entity. 
%
% As for relationship initialization, we use the relative location information of two objects.
%
In the experiments, we use the same data preparation and training/validation split as in 3DSSG~\cite{wald2020learning}.

\vspace{+1mm}
\noindent\textbf{Metrics and Tasks.} 
%
We follow the experiment settings in 3DSSG~\cite{wald2020learning} . 
%
In both training and testing stages, 3D scenes are placed in the same 3D coordinate. The view-dependent spatial relation predicates are not ambiguous.
% To evaluate our method comprehensively, we follow the scheme in \cite{wald2020learning}, \eg we compute the top-k accuracy (A@$k$) to evaluate object and predicate prediction performance, which is referred to as R@$k$ in \cite{wald2020learning}.
% 
To evaluate the prediction of the object and predicate, we use the top-k accuracy (A@$k$) metric.
%
As for the triplets, we first multiply the subject, predicate, and object scores to get triplet scores, and then compute the top-k accuracy (A@$k$) as the evaluation metric.
% 
The triplet is considered correct only if the subject, predicate, and object are all correct\footnote{However, the metric top-k accuracy is written as the top-k recall or R@k in 3DSSG~\cite{wald2020learning} and SGFN~\cite{wu2021scenegraphfusion}.}.
%
% To alleviate the effects of long-tail distribution on test results, we also computed the average of the predicate and triplet accuracy on each predicate class as the mean top-k accuracy (mA@$k$) metric, which is a common practice in 2D scene graph prediction task \cite{tang2019learning}.
To fairly evaluate the performance of long-tailed predicate distribution, we also compute the average top-k accuracy of the predicate across all predicate classes, denoted as the mean top-k accuracy (mA@$k$).
%

% To compare with the method in Zhang~\etal~\cite{zhang2021knowledge}, 
%
We also conduct two 2D scene graph tasks proposed in \cite{xu2017scene} in the 3D scenario, as what Zhang~\etal~\cite{zhang2021knowledge} did, \ie,
%
(1) Scene Graph Classification (SGCls) that \textcolor{black}{evaluates} the triplet together.
%
(2) Predicate Classification (PredCls) that only \textcolor{black}{evaluates} the predicate \textcolor{black}{with the ground-truth labels of object entities.}
% given that the subject and object are correct.
%
Following Zhang~\etal~\cite{zhang2021knowledge}, we compute the recall at the top-k (R@$k$) triplets.
%
The triplet is considered correct when the subject, predicate, and object are all valid.
%
% As defined in ~\cite{lu2016visual}, the recall is the fraction of the correct top-k triplets against the ground truth.
%
Additionally, we also adopt mean recall (mR@$k$) to evaluate the performance on the unevenly sampled relations using a similar strategy as mA@$k$.

\vspace{+1mm}
% \noindent\textbf{Network Train and Inference Details.}
\noindent\textbf{Implementation Details.} 
%
Our network is end-to-end optimized using AdamW optimizer~\cite{DBLP:journals/corr/KingmaB14,DBLP:journals/corr/abs-1711-05101} with the batch size as $8$.
%
We train the network for $100$ epochs, and the base learning rate is set as $0.001$ with a cosine annealing learning rate decay strategy~\cite{loshchilov2016sgdr}.
%
$N_\text{obj}=160$ and $N_\text{rel}=26$ in our experiments.
%
GNN modules are repeated for $T=2$ times in both 3D and oracle multi-modal models.
%
$\lambda_{\text{obj}}=\lambda_{\text{aux}}=0.1$, $\lambda_{\text{pred}}=1$ in \cref{eq:loss_total}.
%
All experiments are carried out on the PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, and each experiment takes about $48$ hours until model convergence.
% What should be noted is that the 2D inputs are only used during the training stage.
Note that 2D inputs are only used during the training stage.
% 
During the inference stage, we follow the same strategy in  ~\cite{xu2017scene}, which selects the top@$1$ class of both object and predicate while giving an object instance index tuple. 
% %
% Then we multiply the object score, subject score and predicate score as the triplet score.
% %
% In the end, we sorted all triplet scores and select top@$k$ as our model's output.
% TODO 模型类型
%
Please refer to the supplementary for the details of the network structures.
%


\subsection{Comparison with the State-of-the-art Methods}
\label{subsec:exp-results}
%
We compare our method with a list of reference methods, \ie SGPN~\cite{wald2020learning}, \sggpoint~\cite{zhang2021exploiting}, SGFN~\cite{wu2021scenegraphfusion}, Co-Occurrence~\cite{zhang2021knowledge}, KERN~\cite{chen2019knowledge}, Schemata~\cite{sharifzadeh2021classification}, Zhang~\etal~\cite{zhang2021knowledge}. 
%
% SGPN~\cite{wald2020learning} is the pioneer in 3D scene graph prediction tasks, which uses a simple PointNet~\cite{qi2017pointnet} with GNN to generate scene graph predictions.
% %
% The following work SGFN~\cite{wu2021scenegraphfusion} leverages an attention-based node feature updating strategy in GNN and incrementally builds up scene graphs from RGB-D sequences.
% %
% In this comparison, the online updating strategy of SGFN is blocked.
% %
% Zhang~\etal~\cite{zhang2021knowledge} uses a graph auto-encoder to automatically extract class-wise representations as prior knowledge to enhance relationship prediction.
% %
% $\text{SGG}_\text{point}$~\cite{zhang2021exploiting} builds two associated twin interaction mechanisms between nodes and edges to effectively bridge perception and reasoning.
%
In addition, to gain a deeper understanding of our approach, we also report the performances of the oracle multi-modal prediction model (termed as VL-SAT (oracle)), as well as the baseline performance of the 3D prediction model that is trained purely by 3D data (term as non-VL-SAT). The proposed method is term as VL-SAT.

% we design some variants of our method. \basetwo uses a similar network structure as the teacher network except for multi-level feature enhancements and takes in 2D image patches of objects and 3D bounding box positions in the train and validation process. \basethree adopts the same network as the student network and takes in 3D point clouds in both train and validation 
% period.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{picture/freq.pdf}
\caption{
%
The line chart shows the predicate frequency in the train set of 3DSSG~\cite{wald2020learning}.
%
The bar chart shows the results on mA@$1$ of the predicate prediction of SGPN~\cite{wald2020learning} and our VL-SAT.
}
\label{fig:predicate_dis}
\end{figure}

\vspace{+1mm}
\noindent\textbf{Quantitative Results.}
%
The comparison results are summarized in \cref{tab:exp-results}.
%
The baseline ``non-VL-SAT'' has a similar performance as SGFN.
%
\textcolor{black}{The only difference between them is that ``non-


## What problem or question is the paper addressing?

 This paper is addressing the problem of 3D semantic scene graph prediction in point clouds. Specifically, it aims to improve the capability of common 3DSSG prediction models in discriminating challenging long-tailed and ambiguous semantic relations. The key challenges it points out are:

- 3D point clouds only capture geometric structures with limited semantics compared to 2D images. 

- Recent 3DSSG datasets suffer from long-tailed predicate distributions, where semantic predicates are often rarer than geometrical predicates.

To address these challenges, the paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme. The key idea is to train a powerful multi-modal oracle model to assist the 3D prediction model. The oracle model captures reliable structural semantics from vision, language and 3D geometry. During training, it heterogeneously collaborates with the 3D model to transfer its benefits via backpropagated gradients. This allows the 3D model to predict reliable scene graphs using only 3D point clouds during inference.

In summary, the paper introduces a novel training scheme to empower 3D semantic scene graph prediction models using visual-linguistic semantics, to handle the challenges of limited 3D semantics and long-tailed distributions. The key novelty is the proposed VL-SAT scheme and oracle model.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and keywords associated with this paper are:

- 3D Semantic Scene Graph Prediction
- Point Cloud
- Visual-Linguistic Semantics 
- Assisted Training
- VL-SAT
- Long-tailed Relations
- Oracle Model
- Heterogeneous Collaboration
- Gradient Flow
- Back-propagation
- CLIP
- Knowledge Transfer

The paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme to empower 3D semantic scene graph prediction models in point clouds. The key ideas include:

- Using an oracle multi-modal model to capture reliable structural semantics from vision, language, and 3D geometry. 

- Heterogeneously aligning the multi-modal oracle model and 3D model via node/edge-level collaboration.

- Transferring benefits of the oracle model to the 3D model through back-propagated gradient flows.

- Enhancing discrimination for long-tailed and ambiguous semantic relations in 3DSSG.

- Utilizing CLIP to align visual, linguistic, and 3D structural semantics.

- Applying the scheme to boost common 3DSSG prediction models like SGFN and SGG_point.

So in summary, the key terms revolve around using visual-linguistic knowledge transfer via an oracle model to boost 3D semantic scene graph prediction, with a focus on improving long-tailed relation modeling.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to create a comprehensive summary of the paper:

1. What is the key problem the paper is trying to solve?

2. What are the main challenges or limitations of previous approaches to this problem? 

3. What is the key idea or main contribution of the proposed method?

4. What is the overall framework or pipeline of the proposed method? What are the main components or stages?

5. What are the key techniques, algorithms, or innovations proposed in the paper? 

6. What datasets were used to evaluate the method? What metrics were used?

7. What were the main results of the experiments? How did the proposed method compare to previous state-of-the-art techniques?

8. What are the limitations or potential negative results of the proposed method?

9. What analyses or ablation studies did the authors perform to validate design choices or understand the method?

10. What potential future work does the paper suggest? What are the broader impacts or applications of the method?

Going through these questions should help summarize the key information and contributions in the paper, as well as critically analyze the proposed method and experimental results. The questions cover the problem definition, related work, technical approach, experiments, results, and potential limitations and future work. A good summary touches on all these aspects.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme. Can you explain in more detail how the visual and linguistic semantics are obtained and incorporated into the training process? What are the key components for transferring this knowledge to the 3D prediction model?

2. The VL-SAT scheme trains an oracle multi-modal prediction model alongside the 3D prediction model. What is the architecture and input for this oracle model? How does it capture more reliable structural semantics compared to using only 3D data? 

3. Node and edge-level collaborations are used to align the oracle and 3D models. Can you explain the masking strategy used in node-level collaboration? Why is no masking used for edge-level collaboration?

4. What is the motivation behind using a unidirectional knowledge transfer from the 3D model to the oracle model? How does this benefit inference with only 3D inputs? Could a bi-directional approach be beneficial?

5. The paper mentions using CLIP to enhance the oracle model. How are the CLIP text embeddings generated for each ground truth relation triplet? How does the triplet-level regularization loss using these embeddings work?

6. What distance metrics are used for the two auxiliary losses - node feature alignment and triplet embedding alignment? Why are different metrics suitable for each case?

7. How exactly does the gradient flow from the oracle model assist in training the 3D model? Does it implicitly embed multi-modal semantics into the 3D features?

8. What modifications were made to the baseline 3D prediction model architecture used in the paper? Could other GNN models like SGG-Point also benefit from VL-SAT?

9. The results show clear improvements on predicate and triplet metrics but smaller gains on object metrics. Why does VL-SAT have less impact on object prediction performance?

10. The VL-SAT scheme relies on 2D data during training. Could similar benefits be achieved by using 3D color information instead? What experiments were done to analyze this?

% !TEX root = ../PaperForReview.tex

\section{Experiments and Discussions}
\label{sec:experiments_and_discussions}

\subsection{Setups and Implementation Details}
\label{subsec:exp-setup}

\noindent\textbf{Datasets.} 
%
We conduct experiments on 3DSSG~\cite{wald2020learning}. It is a 3D semantic scene graph dataset drawn from the 3RScan dataset~\cite{wald2019rio}, with rich annotations about instance segmentation masks and relation triplets.
%
It has $1553$ 3D reconstructed indoor scenes, $160$ classes of objects, and $26$ types of predicates.
%
% We sample $128$ points from a segmented instances to represent a node entity. 
%
% As for relationship initialization, we use the relative location information of two objects.
%
In the experiments, we use the same data preparation and training/validation split as in 3DSSG~\cite{wald2020learning}.

\vspace{+1mm}
\noindent\textbf{Metrics and Tasks.} 
%
We follow the experiment settings in 3DSSG~\cite{wald2020learning} . 
%
In both training and testing stages, 3D scenes are placed in the same 3D coordinate. The view-dependent spatial relation predicates are not ambiguous.
% To evaluate our method comprehensively, we follow the scheme in \cite{wald2020learning}, \eg we compute the top-k accuracy (A@$k$) to evaluate object and predicate prediction performance, which is referred to as R@$k$ in \cite{wald2020learning}.
% 
To evaluate the prediction of the object and predicate, we use the top-k accuracy (A@$k$) metric.
%
As for the triplets, we first multiply the subject, predicate, and object scores to get triplet scores, and then compute the top-k accuracy (A@$k$) as the evaluation metric.
% 
The triplet is considered correct only if the subject, predicate, and object are all correct\footnote{However, the metric top-k accuracy is written as the top-k recall or R@k in 3DSSG~\cite{wald2020learning} and SGFN~\cite{wu2021scenegraphfusion}.}.
%
% To alleviate the effects of long-tail distribution on test results, we also computed the average of the predicate and triplet accuracy on each predicate class as the mean top-k accuracy (mA@$k$) metric, which is a common practice in 2D scene graph prediction task \cite{tang2019learning}.
To fairly evaluate the performance of long-tailed predicate distribution, we also compute the average top-k accuracy of the predicate across all predicate classes, denoted as the mean top-k accuracy (mA@$k$).
%

% To compare with the method in Zhang~\etal~\cite{zhang2021knowledge}, 
%
We also conduct two 2D scene graph tasks proposed in \cite{xu2017scene} in the 3D scenario, as what Zhang~\etal~\cite{zhang2021knowledge} did, \ie,
%
(1) Scene Graph Classification (SGCls) that \textcolor{black}{evaluates} the triplet together.
%
(2) Predicate Classification (PredCls) that only \textcolor{black}{evaluates} the predicate \textcolor{black}{with the ground-truth labels of object entities.}
% given that the subject and object are correct.
%
Following Zhang~\etal~\cite{zhang2021knowledge}, we compute the recall at the top-k (R@$k$) triplets.
%
The triplet is considered correct when the subject, predicate, and object are all valid.
%
% As defined in ~\cite{lu2016visual}, the recall is the fraction of the correct top-k triplets against the ground truth.
%
Additionally, we also adopt mean recall (mR@$k$) to evaluate the performance on the unevenly sampled relations using a similar strategy as mA@$k$.

\vspace{+1mm}
% \noindent\textbf{Network Train and Inference Details.}
\noindent\textbf{Implementation Details.} 
%
Our network is end-to-end optimized using AdamW optimizer~\cite{DBLP:journals/corr/KingmaB14,DBLP:journals/corr/abs-1711-05101} with the batch size as $8$.
%
We train the network for $100$ epochs, and the base learning rate is set as $0.001$ with a cosine annealing learning rate decay strategy~\cite{loshchilov2016sgdr}.
%
$N_\text{obj}=160$ and $N_\text{rel}=26$ in our experiments.
%
GNN modules are repeated for $T=2$ times in both 3D and oracle multi-modal models.
%
$\lambda_{\text{obj}}=\lambda_{\text{aux}}=0.1$, $\lambda_{\text{pred}}=1$ in \cref{eq:loss_total}.
%
All experiments are carried out on the PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, and each experiment takes about $48$ hours until model convergence.
% What should be noted is that the 2D inputs are only used during the training stage.
Note that 2D inputs are only used during the training stage.
% 
During the inference stage, we follow the same strategy in  ~\cite{xu2017scene}, which selects the top@$1$ class of both object and predicate while giving an object instance index tuple. 
% %
% Then we multiply the object score, subject score and predicate score as the triplet score.
% %
% In the end, we sorted all triplet scores and select top@$k$ as our model's output.
% TODO 模型类型
%
Please refer to the supplementary for the details of the network structures.
%


\subsection{Comparison with the State-of-the-art Methods}
\label{subsec:exp-results}
%
We compare our method with a list of reference methods, \ie SGPN~\cite{wald2020learning}, \sggpoint~\cite{zhang2021exploiting}, SGFN~\cite{wu2021scenegraphfusion}, Co-Occurrence~\cite{zhang2021knowledge}, KERN~\cite{chen2019knowledge}, Schemata~\cite{sharifzadeh2021classification}, Zhang~\etal~\cite{zhang2021knowledge}. 
%
% SGPN~\cite{wald2020learning} is the pioneer in 3D scene graph prediction tasks, which uses a simple PointNet~\cite{qi2017pointnet} with GNN to generate scene graph predictions.
% %
% The following work SGFN~\cite{wu2021scenegraphfusion} leverages an attention-based node feature updating strategy in GNN and incrementally builds up scene graphs from RGB-D sequences.
% %
% In this comparison, the online updating strategy of SGFN is blocked.
% %
% Zhang~\etal~\cite{zhang2021knowledge} uses a graph auto-encoder to automatically extract class-wise representations as prior knowledge to enhance relationship prediction.
% %
% $\text{SGG}_\text{point}$~\cite{zhang2021exploiting} builds two associated twin interaction mechanisms between nodes and edges to effectively bridge perception and reasoning.
%
In addition, to gain a deeper understanding of our approach, we also report the performances of the oracle multi-modal prediction model (termed as VL-SAT (oracle)), as well as the baseline performance of the 3D prediction model that is trained purely by 3D data (term as non-VL-SAT). The proposed method is term as VL-SAT.

% we design some variants of our method. \basetwo uses a similar network structure as the teacher network except for multi-level feature enhancements and takes in 2D image patches of objects and 3D bounding box positions in the train and validation process. \basethree adopts the same network as the student network and takes in 3D point clouds in both train and validation 
% period.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{picture/freq.pdf}
\caption{
%
The line chart shows the predicate frequency in the train set of 3DSSG~\cite{wald2020learning}.
%
The bar chart shows the results on mA@$1$ of the predicate prediction of SGPN~\cite{wald2020learning} and our VL-SAT.
}
\label{fig:predicate_dis}
\end{figure}

\vspace{+1mm}
\noindent\textbf{Quantitative Results.}
%
The comparison results are summarized in \cref{tab:exp-results}.
%
The baseline ``non-VL-SAT'' has a similar performance as SGFN.
%
\textcolor{black}{The only difference between them is that ``non-VL-SAT'' adds a multi-head self-attention (MHSA) module~\cite{vaswani2017attention} before each GNN module in SGFN.}
%
Thanks to the delicate visual-linguistic assisted training scheme, our ``VL-SAT'' tremendously improves the baseline, according to the evaluation with respect to the predicate, and the triplet.
%
Moreover, according to the less biased mA@k metrics with respect to long-tailed distribution, when evaluating the predicate, the proposed ``VL-SAT'' outperforms the baseline ``non-VL-SAT'' with around $12.0\%$, $6.8\%$ and $6.0\%$ gains at mA@$1$, mA@$3$, and mA@$5$ respectively.
%
Our method reaches new state-of-the-art results on triplet prediction, with $6.8\%$ gain on mA@50 and $5.9\%$ gain on mA@100 over SGFN~\cite{wu2021scenegraphfusion}.
%
Note that the results of object classification just have a marginal improvement, which means that a simple PointNet-based 3D encoder may not be able to convey similar instance-level representative power as the 2D vision encoder.
%
% (TODO) It is interesting to find that our oracle model ``VL-SAT (oracle)'' further boosts the overall performance, but its gains mainly come from a better object classifier rather than better modeling of the predicate.

% Due to sufficient object texture and color information in 2D images, \basetwo performs well on object classification and brings about $12\%$ gains on A@$1$ compared with SGFN.
% %
% The shortcoming of \basetwo is also obvious. 
% %
% Because of lacking spatial information, \basetwo performs poorly on predicate prediction.
% %
% Our method gains $2\%$ improvement on object A@$1$ metric over SGFN~\cite{wu2021scenegraphfusion}, benefiting from the multi-level enhancement strategy.
% %
% As for the evaluation of predicate prediction, SGFN, \basethree, and our method all achieve remarkable performance on A@$k$ metric.
% %
% Regarding the mA@$k$ metric, which is less influenced by the long-tail distribution effect, our method outperforms SGFN by $12.2\%$ on the predicate mA@1 metric.
% %
% Thanks to the strong performance on the object and predicate prediction, our method reaches a new state-of-the-art result on triplet prediction, with $6.8\%$ gain on mA@50 and $5.9\%$ gain on mA@100 over SGFN\cite{wu2021scenegraphfusion}.


As illustrated in \cref{tab:exp-buaa1} and \cref{tab:exp-buaa2}, we also compare our ``VL-SAT'' with the reference methods, with respect to two tasks named SGCls and PredCls, according to the settings introduced by Zhang~\etal~\cite{zhang2021knowledge}.
%
Our method outperforms Zhang~\etal~\cite{zhang2021knowledge} by a large margin.
%
For example, with graph constraint~\cite{xu2017scene} (as a more rigorous testing scenario~\cite{zareian2020bridging}), ``VL-SAT'' has $2.5\%$ gain on R@$20$ in SGCls, $8.5\%$ gain on R@$20$ in PredCls.
%
Moreover, with respect to the less biased metrics in \cref{tab:exp-buaa2}, ``VL-SAT'' even achieves $6.6\%$ gains on mR@$20$ in SGCls than Zhang~\etal~\cite{zhang2021knowledge}.

% \begin{table*}
% \centering
% \resizebox{\linewidth}{!}{
%         \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
%         \hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
%  \cline{2-14}
%  & R@$1$ & R@$5$ & R@$10$ & R@$1$ & mR@$1$ & R@$3$ & mR@$3$ & R@$5$ & mR@$5$ & R@$50$ & mR@$50$ & R@$100$ & mR@$100$ \\
%  \hline
%  SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & 91.32 & 32.01 & 98.09 & 55.22 & 99.15 & 69.44 & 87.55 & 41.52 & 90.66 & 51.92 \\
%  SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 41.89 & 98.17 & 70.82 & 99.33 & 81.44 & 89.02 & 58.37 & 91.71 & 67.61 \\
%  \hline
%  $\mathrm{Base_{2d}}$ & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 41.77 & 97.89 & 61.83 & 99.20 & 76.11 & \textbf{92.79} & 64.18 & \textbf{95.06} & \textbf{77.23} \\
%  \revise{$\mathrm{Base_{3d}}$} & 55.43 & 79.09 & 86.51 & \textbf{91.81} & 52.30 & \textbf{98.70} & 71.48 & \textbf{99.64} & 82.75 & 89.71 & 60.74 & 92.96 & 71.56 \\
%  \hline
% %  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
% %  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
%  Ours & 55.66 & 78.66 & 85.91 & 89.81 & \textbf{54.03} & 98.45 & \textbf{77.67} & 99.53 & \textbf{87.65} & 90.35 & \textbf{65.09} & 92.89  & 73.59 \\
%  \hline
% \end{tabular}}
% \caption{
% %
% 3D scene graph prediction performance on 3DSSG validation set. 
% %
% Evaluation is conducted in terms of object, predicate, and triplet. 
% %
% The results of SGFN and SGPN are based on our reproduced model.
% %
% }
% \
