# [Controllable Visual-Tactile Synthesis](https://arxiv.org/abs/2305.03051)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to develop a method for controllable visual-tactile synthesis, where the model can generate photo-realistic visual images and plausible tactile signals from user inputs like sketches or text descriptions. 

Specifically, the key research questions addressed in this work are:

- How to learn a mapping from sparse tactile supervision (tactile patches) and dense visual supervision (full images) to generate full visual and tactile outputs? This is challenging due to the difference in scale and resolution between vision and touch.

- How to synthesize both visual and tactile outputs simultaneously in a controllable manner based on sketch/text inputs? Generating multi-modal outputs with user control is difficult.

- How to render the synthesized visual and tactile results on a haptic device to enable realistic interactive experiences? Rendering tactile signals on hardware is non-trivial.

To summarize, the central goal is to develop a conditional generative model that can synthesize controllable and realistic visual-tactile outputs from sketches/text and render them on haptic devices. The key research questions involve handling multi-modal learning with scale differences, achieving user-controllable synthesis, and enabling tactile rendering on physical devices.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a conditional generative model that can synthesize high-quality visual and tactile outputs from a user sketch input. Specifically:

- The paper collects a new visual-tactile dataset named TouchClothing, which contains spatially aligned visual images and tactile data captured using a GelSight sensor for 20 articles of clothing. 

- The paper proposes a conditional GAN architecture that takes in a sketch and generates a high-resolution visual image along with a tactile output representing the surface geometry. The model is trained on dense supervision from visual images and sparse supervision from tactile patches.

- The synthesized visual and tactile outputs can be rendered on a haptic device like the TanvasTouch screen, allowing users to see and feel the generated textures and materials. 

- Experiments show the model can generate realistic and detailed visual-tactile results. User studies demonstrate the haptic rendering on the touchscreen feels more realistic compared to baselines.

- The applications enabled by this work include virtual try-on, product prototyping, content creation for VR/AR, etc, where users can visually see and haptically feel the synthesized results.

In summary, the key contribution is using deep generative models to achieve controllable synthesis of visual and tactile outputs from sketch inputs, which could enable more immersive human experiences in virtual environments.
