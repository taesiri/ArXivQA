# [A Comprehensive Study of Multimodal Large Language Models for Image   Quality Assessment](https://arxiv.org/abs/2403.10854)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Multimodal large language models (MLLMs) have shown promise for image quality assessment (IQA) due to their flexibility, interpretability, and ability to leverage both visual and textual data. However, how to best prompt MLLMs for the IQA task remains largely unexplored. 

Key Contributions
- The paper systematically explores 9 different prompting systems for MLLMs by combining 3 standardized psychophysical testing methods (single stimulus, double stimulus, multiple stimulus) with 3 NLP prompting strategies (standard, in-context, chain-of-thought).

- A computational procedure is proposed to select a small set of difficult IQA samples while ensuring diversity and consistency with human ratings. This uses expert IQA models to identify challenging cases.

- Experiments compare prompting strategies across 4 MLLMs - 3 open source (LLaVA, mPLUG, InternLM) and 1 proprietary (GPT-4V) on datasets covering various distortions and scenarios. 

- It is shown that the optimal prompting system varies between open source and proprietary MLLMs. Chain-of-thought prompting with double stimulus works best for GPT-4V.

- GPT-4V demonstrates state-of-the-art IQA performance in many cases, but still struggles with fine-grained color discrimination and comparing multiple images. Open source MLLMs perform poorly on IQA overall.

- The work calls for re-evaluation of recent MLLM advances on IQA using optimal prompting strategies paired with GPT-4V as the benchmark. There remains room for improvement in IQA even with leading MLLMs.
