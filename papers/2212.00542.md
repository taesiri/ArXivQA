# [Graph Convolutional Neural Networks as Parametric CoKleisli morphisms](https://arxiv.org/abs/2212.00542)

## What is the main contribution of this paper?

The main contribution of this paper is defining a bicategory of graph convolutional neural networks (GCNNs) and showing that it can be factored through existing categorical constructions for deep learning called Para and Lens. Specifically, the authors:- Define the bicategory GCNN_n for graph neural networks on graphs with n nodes.- Show that the category CoKl(A x -) correctly models the adjacency matrix sharing aspect of GCNNs. This category has a global parameter A that is shared between layers. - Prove that CoKl(A x -) is an actegory so the Para construction can be applied.- Show that CoKl(A x -) is a reverse derivative category so backpropagation can be performed compositionally.- Prove there is an injective-on-objects, faithful functor from GCNN_n to Para(CoKl(A x -)). This shows GCNNs are a special case of parametric cokleisli morphisms.- Argue this construction provides a high-level categorical view of the inductive bias in GCNNs, specifically the global parameter of the adjacency matrix.So in summary, the main contribution is providing a categorical characterization of graph convolutional neural networks using existing constructions, giving a new perspective on their architectural properties.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading, here is a one-sentence TL;DR of the paper:The paper defines a bicategory of graph convolutional neural networks and shows it can be factored into existing categorical constructions for deep learning called Para and Lens, revealing GCNNs possess an inductive bias of having a globally available adjacency matrix parameter.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper on graph convolutional neural networks relates to other research in geometric deep learning:- The main contribution is factoring graph convolutional neural networks (GCNNs) into existing categorical components for deep learning like Para and Lens. This provides a high-level perspective on GCNNs and makes their inductive biases explicit. - Most prior work on GCNNs has focused on novel architectures and applications. This paper takes a more foundational, categorical view to formally characterize GCNN architectures.- The paper connects GCNNs to broader research on using category theory for deep learning, like the work on parametric lenses. The theoretical framework builds on that prior foundational work.- The focus on characterizing inductive biases categorically relates to an overall trend in geometric deep learning. Research in this field often aims to make architectural choices explicit based on geometric structure and symmetries in the data.- The work touches on equivariance in neural networks, which has been a major theme in geometric deep learning. The category theory perspective could potentially help relate equivariant networks to other architectures.- There has been some other work categorically characterizing recurrent, convolutional, and residual networks. This paper contributes a categorical characterization for another important architecture, GCNNs.In summary, this paper takes a novel categorical perspective on graph convolutional networks that relates to broader trends in foundations and theory for deep learning architectures. It leverages and connects to existing categorical tools, while providing new formalization of GCNN inductive biases.
