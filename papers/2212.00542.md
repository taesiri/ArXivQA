# [Graph Convolutional Neural Networks as Parametric CoKleisli morphisms](https://arxiv.org/abs/2212.00542)

## What is the main contribution of this paper?

 The main contribution of this paper is defining a bicategory of graph convolutional neural networks (GCNNs) and showing that it can be factored through existing categorical constructions for deep learning called Para and Lens. Specifically, the authors:- Define the bicategory GCNN_n for graph neural networks on graphs with n nodes.- Show that the category CoKl(A x -) correctly models the adjacency matrix sharing aspect of GCNNs. This category has a global parameter A that is shared between layers. - Prove that CoKl(A x -) is an actegory so the Para construction can be applied.- Show that CoKl(A x -) is a reverse derivative category so backpropagation can be performed compositionally.- Prove there is an injective-on-objects, faithful functor from GCNN_n to Para(CoKl(A x -)). This shows GCNNs are a special case of parametric cokleisli morphisms.- Argue this construction provides a high-level categorical view of the inductive bias in GCNNs, specifically the global parameter of the adjacency matrix.So in summary, the main contribution is providing a categorical characterization of graph convolutional neural networks using existing constructions, giving a new perspective on their architectural properties.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading, here is a one-sentence TL;DR of the paper:The paper defines a bicategory of graph convolutional neural networks and shows it can be factored into existing categorical constructions for deep learning called Para and Lens, revealing GCNNs possess an inductive bias of having a globally available adjacency matrix parameter.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper on graph convolutional neural networks relates to other research in geometric deep learning:- The main contribution is factoring graph convolutional neural networks (GCNNs) into existing categorical components for deep learning like Para and Lens. This provides a high-level perspective on GCNNs and makes their inductive biases explicit. - Most prior work on GCNNs has focused on novel architectures and applications. This paper takes a more foundational, categorical view to formally characterize GCNN architectures.- The paper connects GCNNs to broader research on using category theory for deep learning, like the work on parametric lenses. The theoretical framework builds on that prior foundational work.- The focus on characterizing inductive biases categorically relates to an overall trend in geometric deep learning. Research in this field often aims to make architectural choices explicit based on geometric structure and symmetries in the data.- The work touches on equivariance in neural networks, which has been a major theme in geometric deep learning. The category theory perspective could potentially help relate equivariant networks to other architectures.- There has been some other work categorically characterizing recurrent, convolutional, and residual networks. This paper contributes a categorical characterization for another important architecture, GCNNs.In summary, this paper takes a novel categorical perspective on graph convolutional networks that relates to broader trends in foundations and theory for deep learning architectures. It leverages and connects to existing categorical tools, while providing new formalization of GCNN inductive biases.


## What future research directions do the authors suggest?

 The authors suggest several promising areas for future research:1. Generalizing graph convolutional neural networks (GCNNs) to more sophisticated forms of message passing graph neural networks. The current definition of GCNNs only captures a limited type of message passing. The authors suggest exploring connections to dependent lenses to model more complex message passing schemes.2. Exploring connections between graph neural networks and game theory/economics models defined using parametric lenses. The authors suggest graph games may provide insights into graph neural networks.3. Extending the categorical construction to allow learning/updating the adjacency matrix in addition to just using it as a fixed global parameter. This could involve using graded monads.4. Finding a base category beyond smooth manifolds that makes neural networks equivalent to, not just a subcategory of, parametric lenses. The nonlinearity of activation functions causes issues representing neural networks in the category of smooth manifolds. A better base category may lead to an equivalence.5. Developing a general theory of neural network architecture informed by geometric deep learning. This involves specifying network architecture based on properties of the input data rather than requiring many ad hoc decisions. Category theory may help find principled ways to translate inductive biases into architecture choices.6. Exploring connections to viewing neural networks as internal languages of a topos. This involves finding appropriate polynomial functors or other categorical structures to capture the compositionality of neural network layers.7. Defining a "batch category" to clarify relationships between the parametric and CoKleisli constructions. The global vs local parameter distinction suggests investigating representing batches as categorical data.In summary, the authors lay out an extensive research agenda for developing category theory tools to elucidate neural network architecture, translate inductive biases, and find principled ways to guide architecture selection. Many open questions remain about finding appropriate categorical abstractions for various neural network families.


## Summarize the paper in one paragraph.

 The paper defines the bicategory of Graph Convolutional Neural Networks (GCNNs) for a graph with n nodes, called GCNN_n. It shows this bicategory can be factored through the categorical constructions Para and Lens, with the base category set to the CoKleisli category of the product comonad. The key result is an injective-on-objects, faithful 2-functor from GCNN_n to Para(CoKl(Adj x -)), where Adj is the adjacency matrix. This provides a high-level categorical characterization of the global parameter sharing aspect of GCNNs, where the adjacency matrix is shared across layers unlike the weight matrices. The construction permits compositionally modelling and training GCNNs. The paper also discusses connections to equivariant learning and the non-functoriality of activations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper defines the bicategory of Graph Convolutional Neural Networks (GCNNs) for a graph with n nodes. It shows that GCNNs can be factored through existing categorical constructions for deep learning called Para and Lens. The base category is set to the CoKleisli category of the product comonad. The paper proves there is an injective-on-objects, faithful 2-functor from GCNN_n to Para(CoKl(Adj x -)). This construction allows the adjacency matrix of a GCNN to be treated as a global parameter instead of a local, layer-wise one. This provides a high-level categorical characterization of the inductive bias that GCNNs possess.The paper hypothesizes about possible generalizations of GCNNs to more general message-passing graph neural networks. It discusses potential connections to equivariant learning and the lack of functoriality of activation functions. The construction could be extended to updating the adjacency matrix during training. The factorization gives a birds-eye view of GCNNs and suggests further generalizations. Overall, the paper provides a categorical perspective on the global/local parameter aspect of GCNNs.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a categorical framework for Graph Convolutional Neural Networks (GCNNs) by modeling them as parametric cokleisli morphisms. Specifically, the authors define the bicategory of GCNNs on graphs with n nodes, $\textbf{GCNN}_n$, and show that it can be factored through the categorical constructions of $\Para$ and $\Lens$ using the base category $\CoKl(A \times -)$. They prove that there exists an injective-on-objects, faithful 2-functor $\textbf{GCNN}_n \to \pc$, allowing GCNNs to be treated as parametric cokleisli morphisms. This gives a high-level categorical characterization of the global parameter (adjacency matrix) that GCNN layers share, in contrast to local, layer-wise parameters like weights. Overall, the paper provides a compositional framework to embed and analyze GCNNs using existing categorical tools.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can graph convolutional neural networks (GCNNs) be understood and formulated in terms of existing categorical constructions for deep learning? Specifically, the authors aim to show that GCNNs can be expressed as parametric cokleisli morphisms, by factoring the bicategory of GCNNs through the categorical constructions of Para and Lens. This provides a unifying perspective and allows GCNNs to be studied compositionally using category theory.The key contributions and hypotheses appear to be:- Defining the bicategory of GCNNs (Definition 3)- Proving there is an injective-on-objects, faithful 2-functor from this bicategory to Para(CoKl(Adj x -)) (Theorem 1)- Showing CoKl(Adj x -) is a reverse derivative category (Theorem 2)- Demonstrating compositionally backpropagation through GCNNs (Theorem 3)So in summary, the central research question is how to formulate and understand GCNNs categorically using Para, Lens, and the CoKleisli construction. This provides new insights into the structure and properties of GCNNs.


## What problem or question is the paper addressing?

 The main problem this paper is addressing is formalizing graph convolutional neural networks (GCNNs) categorically, and showing how they can be constructed compositionally from existing categorical machinery for modeling neural networks. Specifically:- The paper defines a bicategory $\mathbf{GCNN}_n$ of graph convolutional neural networks on a graph with $n$ nodes. This provides a formal categorical definition of GCNNs. - It shows there is an injective, faithful 2-functor $\kappa_n: \mathbf{GCNN}_n \to \Para(\CoKl(\text{Adj} \times -))$, embedding GCNNs into a composition of the $\Para$ and $\CoKl$ constructions on an appropriate base category. This provides a compositional factorization of GCNNs.- Through this factorization, the paper elucidates the global vs local structure of parameters in GCNNs. The adjacency matrix is a global parameter shared across layers, while the weights are local layer-specific parameters. - It also shows how GCNNs can be augmented with a backwards pass for gradient-based learning, by composing with a reverse derivative 2-functor.Overall, the main contribution is providing a formal categorical characterization and construction of GCNNs. This gives new perspective on their structure, especially the global/local parameter aspect, and connects them to existing categorical machinery like $\Para$, $\CoKl$, and reverse derivative categories.
