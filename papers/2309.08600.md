# [Sparse Autoencoders Find Highly Interpretable Features in Language   Models](https://arxiv.org/abs/2309.08600)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: Can we find semantically meaningful and interpretable directions (features) in the internal activations of language models using unsupervised learning techniques?The authors hypothesize that the internal activations of language models consist of sparse, linear combinations of underlying "network features" that correspond to human-understandable concepts. However, these features may be obfuscated due to a phenomenon called "superposition", where there are more underlying features than dimensions in the layer. This results in non-orthogonal features interfering with each other, making it difficult to directly interpret individual neurons or dimensions. To overcome this, the authors propose using sparse autoencoders to learn sets of directions (feature dictionaries) that can reconstruct the internal activations as sparse linear combinations. They hypothesize that by encouraging sparsity, the autoencoders will be able to disentangle the superimposed features. The authors then evaluate whether these learned dictionary features are semantically meaningful and interpretable using a variety of techniques, including automatic interpretability scores, concept erasure, and case studies of individual features. The central question is whether this unsupervised method can recover interpretable directions from the internal activations of language models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Developing a method to train sparse autoencoders on the internal activations of language models in order to learn interpretable directions/features that help explain the model computations. - Demonstrating that the learned dictionary features from the sparse autoencoders are more interpretable than baseline methods like PCA or neurons, as measured by automatic interpretability scores.- Showing the learned features can be used for more precise and less disruptive concept erasure compared to other techniques.- Providing evidence that the features are monosemantic and enable model analysis via case studies on individual features.In summary, the paper introduces sparse autoencoders as a technique to find more interpretable directions in language models in an unsupervised manner. It provides empirical support that these learned features are more monosemantic, interpretable, and aligned with model computations compared to other approaches. The authors frame this as a step toward better understanding and controlling the internal workings of large language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper appears to be about using sparse autoencoders to find interpretable features in the internal representations of language models. The key idea seems to be that by training autoencoders to reconstruct a model's internal activations using a sparse linear combination of learned "dictionary" features, the resulting features are more semantically meaningful and monosemantic compared to other methods like using individual neurons. The main takeaway is that this approach enables gaining insight into how language models work through unsupervised learning of interpretable model directions.


## How does this paper compare to other research in the same field?

This paper presents a novel method for disentangling features in deep neural network representations using sparse coding autoencoders. It is related to some other recent work in interpretability and mechanistic understanding of neural networks:- This work builds off of Sharkey et al. (2023), which first proposed using sparse coding to find interpretable directions in neural networks. The current paper applies this approach specifically to language models and provides a more thorough evaluation. - Yun et al. (2021) also applied sparse coding to find interpretable "transformer factors" in language models. However, they applied it simultaneously to all layers while this paper looks at each layer separately.- Anthropic (2023) similarly used sparse coding to interpret a custom 1-layer transformer. The current paper explores sparse coding across multiple layers of a standard transformer architecture.- Bills et al. (2023) introduced a method for automatically evaluating neuron interpretability using language models. This paper adapts that approach to evaluate the interpretability of the learned sparse coding features.The key novel contributions of this paper are:- Demonstrating that sparse coding features are more interpretable than common baselines like PCA across multiple layers of a standard transformer language model.- Showing these features enable targeted concept erasure that is less disruptive than prior methods.- Identifying causal features relevant for tasks using the learned dictionaries. - Providing evidence that the features are monosemantic via input/output analysis and automatic circuit detection.Overall, this paper makes a compelling case that sparse coding is a promising approach for mechanistic interpretability of language models. The thorough quantitative and qualitative analyses of the learned features advance this line of research and point the way towards greater model transparency.


## What future research directions do the authors suggest?

The authors suggest several future research directions:- Exploring other sparse autoencoder architectures and loss functions. They would like to achieve lower reconstruction error to better capture all of the model's internal representations. - Improving feature dictionary discovery for the MLP and attention sublayers, which currently pose challenges. Specific suggestions include incorporating model weights or adjacent layer features.- Expanding the automatic circuit detection algorithm to include MLP and attention dictionaries. This could help trace computations for specific dictionary features.- Studying how network features develop over training by looking at features across model checkpoints. This could provide insights into feature formation. - Pursuing other methods to deal with dead features in MLP layers to ensure the full input space is covered by useful features.- Expanding applications of the learned features, such as studying adversarial examples, in-context learning, and reward models.- Further developing the monosemanticity and causal relevance metrics to better evaluate the quality of learned features.- Moving towards the ambitious goal of "enumerative safety" - fully enumerating and understanding all features in a network to enable strong safety guarantees. The dictionary learning approach may provide a foundation for future efforts in this direction.In summary, the main suggestions are to refine and extend the dictionary learning approach, apply it to new areas, and use it as a step towards more complete mechanistic interpretability and safety verification of neural networks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper trains autoencoders to decompose the internal activations of language models into sparse linear combinations of interpretable features. They show that these learned features are more interpretable than other unsupervised decomposition techniques like PCA or ICA, based on automated interpretability scores. The features enable precise model editing, such as removing capabilities like pronoun prediction, while disrupting model behavior less than other methods. Case studies demonstrate that many of the learned features are monosemantic (activate for one semantic concept) across their range of activations, have predictable effects on model outputs, and allow automatic detection of computational circuits within the model. Overall, the work shows that interpretable and causally important features can be extracted from language models in an unsupervised way using sparse autoencoders. This provides a new tool for investigating and editing neural networks to make them more transparent and trustworthy.


## Summarize the paper in two paragraphs.

Here is a summary of the paper in two paragraphs:The paper trains autoencoders to decompose the internal activations of language models into sparse linear combinations of features. The autoencoders are trained with a sparsity penalty to encourage the learned features to be sparsely activating. The authors show that the resulting dictionary features learned by the autoencoders are more interpretable than other unsupervised decomposition methods like PCA and neurons. Interpretability is measured using an automated protocol that asks a language model to describe a feature, then measures how well those descriptions predict the feature's activation. The authors also show the learned features enable precise model editing for concept erasure tasks, and find features that are monosemantic across their activation range. Overall, the paper demonstrates that interpretable and causally meaningful features can be extracted from language models in an unsupervised way using sparse autoencoders. The authors use the interpretability of the learned features in several ways. They perform concept erasure, selectively ablating features to remove model capabilities like pronoun prediction. They also use the features for circuit detection, tracing activations backwards and forwards to understand how a given feature is computed. Through case studies, they analyze individual features, finding many that correspond to just one semantic concept across their activation range. The paper concludes that sparse autoencoders provide a new, scalable tool for investigating and editing neural networks. The features may enable future work on interpretability, transparency and alignment.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper trains autoencoders to decompose the internal activations of language models into sparse linear combinations of features. Specifically, they sample activation vectors from a language model like Pythia-70M on a large text corpus. They then train a sparse autoencoder, which is a neural network with a single hidden layer and a sparsity penalty, to reconstruct the activation vectors using a sparse combination of hidden unit activations. The weight matrix of this autoencoder forms a dictionary of features that are intended to align with the unknown "ground truth" features of the language model. They apply this method to the residual stream activations in each layer of the transformer separately. The sparsity penalty encourages the autoencoder to represent each activation vector using a small number of nonzero hidden activations, resulting in sparse feature dictionaries. They then evaluate and interpret these learned dictionary features using metrics like autointerpretability scores and concept erasure tasks.
