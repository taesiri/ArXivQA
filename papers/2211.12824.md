# [Tell Me What Happened: Unifying Text-guided Video Completion via   Multimodal Masked Video Generation](https://arxiv.org/abs/2211.12824)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is how to perform controllable video completion from partial frames guided by natural language instructions. 

Specifically, the paper introduces a new task called text-guided video completion (TVC) where the goal is to generate a complete video from just the first frame (prediction), last frame (rewind), or first and last frames (infilling), guided by a textual description. 

The key hypothesis is that using language instructions along with limited visual frames can make video completion more controllable and better meet human expectations compared to just using the visual frames alone.

To address this, the paper proposes a novel model called Multimodal Masked Video Generation (MMVG) which can perform all cases of TVC in a unified framework with a single training. The core ideas are:

- Representing video frames as discrete visual tokens using a temporal-aware VQGAN.

- Applying masking to input partial frames from diverse timepoints and learning to recover the full video. 

- Fusing text and masked visual frames via a multimodal encoder-decoder architecture.

- Varying the masking conditions during training to unify prediction, rewind, and infilling tasks.

The main hypothesis is that the proposed MMVG with language guidance and masking strategy can effectively address the TVC problem and generate high-quality and controllable video completion results. Experiments on diverse video datasets validate the effectiveness of this approach.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It introduces a new task called text-guided video completion (TVC), where the goal is to generate a complete video from partial frames guided by a textual description. The task has three variants: prediction from first frame, rewind from last frame, and infilling between first and last frames.

2. It proposes a novel model called Multimodal Masked Video Generation (MMVG) to address the TVC task. MMVG represents video frames as discrete visual tokens using a temporal-aware VQGAN. It uses a masking strategy during training to learn to complete videos from partial frame sequences. The same trained model can perform all three TVC tasks at inference time by varying the masking. 

3. It evaluates MMVG on three datasets - Kitchen, Flintstones, and MUGEN - corresponding to egocentric, animation, and gaming video scenes. Results show MMVG is effective at generating high quality and controllable videos for the TVC task. It also shows improved performance on standard video generation/prediction tasks.

In summary, the main contribution is the introduction of the TVC task and a novel MMVG model that can perform video completion from partial frames in a controllable way via natural language guidance. The paper demonstrates promising results on this new task.
