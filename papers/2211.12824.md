# [Tell Me What Happened: Unifying Text-guided Video Completion via   Multimodal Masked Video Generation](https://arxiv.org/abs/2211.12824)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is how to perform controllable video completion from partial frames guided by natural language instructions. 

Specifically, the paper introduces a new task called text-guided video completion (TVC) where the goal is to generate a complete video from just the first frame (prediction), last frame (rewind), or first and last frames (infilling), guided by a textual description. 

The key hypothesis is that using language instructions along with limited visual frames can make video completion more controllable and better meet human expectations compared to just using the visual frames alone.

To address this, the paper proposes a novel model called Multimodal Masked Video Generation (MMVG) which can perform all cases of TVC in a unified framework with a single training. The core ideas are:

- Representing video frames as discrete visual tokens using a temporal-aware VQGAN.

- Applying masking to input partial frames from diverse timepoints and learning to recover the full video. 

- Fusing text and masked visual frames via a multimodal encoder-decoder architecture.

- Varying the masking conditions during training to unify prediction, rewind, and infilling tasks.

The main hypothesis is that the proposed MMVG with language guidance and masking strategy can effectively address the TVC problem and generate high-quality and controllable video completion results. Experiments on diverse video datasets validate the effectiveness of this approach.
