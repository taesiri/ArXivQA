# [Prompting Contrastive Explanations for Commonsense Reasoning Tasks](https://arxiv.org/abs/2106.06823)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of getting large pretrained language models (PLMs) like T5 and BART to provide human-interpretable explanations for their decisions on commonsense reasoning tasks. The central hypothesis is that prompting the PLMs to generate contrastive explanations that explicitly compare the possible answers will elicit relevant commonsense knowledge embedded in the models' parameters, improving both task performance and explanation quality compared to prior approaches.

Specifically, the key aspects of the paper are:

- Commonsense reasoning tasks often require comparing plausible alternatives and human explanations tend to be contrastive in nature. 

- The authors develop a set of contrastive prompt templates that can be filled in by PLMs to generate explanations that contrast the possible answers (e.g. "X is Y while Z is W").

- A two-stage framework where an "explainer" PLM generates contrastive explanations from the prompts, and a "task" PLM makes predictions conditioned on the original input and explanations.

- Contrastive explanations improve task accuracy over non-contrastive baselines, with notable gains in the zero-shot setting.

- Human evaluations show contrastive explanations are more useful than prior approaches. 

- Contrastive prompts allow evaluating faithfulness by manipulating explanations (e.g. flipping contrast).

So in summary, the main hypothesis is that prompting for contrastive explanations will better elicit commonsense knowledge from PLMs in a human-interpretable way, leading to quantitative and qualitative improvements over prior approaches. The results support this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the key contributions of this paper are:

1. Proposing an unsupervised method to generate contrastive explanations for commonsense reasoning tasks by prompting PLMs to complete specialized templates. This constrained generation process produces more useful explanations compared to prior work on unconstrained free-form generation. 

2. Demonstrating improved task performance on two commonsense reasoning benchmarks (WSC and PIQA) by having a separate task model condition its predictions on the generated contrastive explanations. Notably, the gains are larger in the zero-shot setting and when training data is limited.

3. Enabling new evaluations of explanation faithfulness by manipulating the generated contrastive explanations, either by flipping the contrast or abstracting the identities of the fact and foil. Results indicate the model relies on the explanations to some extent.

4. Conducting human evaluations showing the contrastive explanations are deemed more useful than prior work with clarification questions or unconstrained generation.

5. Demonstrating the generalizability of the approach by using the same prompts designed for WSC/PIQA and achieving strong zero-shot performance on CommonsenseQA.

In summary, the main contribution is presenting an unsupervised and human-centered approach to generate constrained yet customizable explanations that are shown to be useful through both automated and human evaluations. The contrastive explanation format also enables new ways to analyze the models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an unsupervised method to generate contrastive natural language explanations for commonsense reasoning tasks by prompting pretrained language models to complete contrastive templates that differentiate answer choices, which improves task performance and enables novel evaluations of explanation faithfulness.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related work:

- It focuses on generating contrastive explanations for commonsense reasoning tasks, while much prior work has focused on free-form explanation generation. The authors argue contrastive explanations are more constrained, interpretable, and amenable to evaluation.

- It leverages prompting techniques to generate contrastive explanations from PLMs. This is similar to other prompting-based methods like self-talk, but more targeted to contrastive reasoning.

- The contrastive explanations are shown to improve task performance, especially in low-resource settings, over both free-form generation baselines and prompting baselines like self-talk.

- The paper introduces techniques to evaluate explanation faithfulness, like flipping the contrast and abstracting the fact/foil. This provides a more rigorous way to analyze models' use of the explanations.

- The work is inspired by theories of human contrastive explanation from philosophy/psychology. The prompts are designed to mimic patterns in how humans give contrastive explanations.

- It is related to work on counterfactual explanations, but generates contrastive knowledge rather than counterfactual inputs. The contrastive explanations can hypothetically be "reversed" to evaluate faithfulness.

Overall, the key novelties are in using targeted contrastive prompting for explanation generation in NLP, showing benefits over prior methods, and developing new techniques to evaluate explanation quality and faithfulness. The grounding in theories of human explanation is also notable.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors are:

- Supporting contrastive explanations for broader applicability: The authors primarily experiment with Winograd schemas and physical commonsense QA. They suggest exploring the framework for other types of commonsense reasoning datasets. 

- End-to-end training: The authors do not fine-tune the explainer PLM due to challenges with backpropagating through discrete operations like beam search. They suggest investigating techniques to enable end-to-end training.

- Improving contrastive in-filling: The authors generate explanations by infilling contrastive templates. They suggest exploring additional pretraining data or objectives to improve the quality of contrastive in-filling. 

- Quantifying model reliance on explanations: The authors propose techniques like flipping explanations and abstracting entities to evaluate reliance on explanations. More such techniques could be explored. 

- Leveraging counterfactuals: The authors suggest counterfactual examples could provide supervision to generate better contrastive explanations. This could be an interesting direction.

- Supporting full reasoning chains: The current approach generates explanations for individual examples. Extending it to support multi-step reasoning over chains of examples is suggested.

In summary, the key future directions are around improving the framework's applicability, faithfulness, and integration of contrastive explanations into end-to-end training. Exploring supervision from counterfactuals and multi-step reasoning are also interesting open problems highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes an unsupervised method to generate contrastive explanations from pretrained language models (PLMs) to explain their reasoning on commonsense reasoning tasks. The key idea is to use contrastive language templates that can be populated by the PLM to generate explanations that explicitly contrast the answer choices (fact vs foil) along their distinguishing attributes. For instance, on a Winograd schema example with answer choices "peanuts" and "raisins", the model may generate "Peanuts are salty while raisins are sweet". The authors show that using a separate PLM to make predictions conditioned on both the input and these contrastive explanations improves performance on Winograd schema and physical commonsense QA datasets compared to prior approaches, especially in low-resource settings. The contrastive explanations are also preferred by humans and can be semantically perturbed to quantify model faithfulness. Overall, the paper demonstrates an effective application of prompting to elicit structured explanatory reasoning from language models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a method for generating contrastive explanations from pretrained language models (PLMs) to explain their predictions on commonsense reasoning tasks. Commonsense reasoning often requires comparing plausible alternatives and explaining why one is more likely based on implicit background knowledge. The authors propose using contrastive language prompts, which are templates with placeholders for the alternatives, to elicit such knowledge from PLMs. The PLMs are prompted to fill in the templates by contrasting the alternatives along key attributes (e.g. "X is salty while Y is sweet"). 

The method is evaluated on the Winograd Schema Challenge and Physical Interaction QA. Results show improvements in task performance over prior methods, indicating the contrastive explanations provide useful knowledge. The explanations also allow novel evaluations of faithfulness by manipulating them to imply the opposite answer. Human evaluations find the contrastive explanations much more relevant and helpful than non-contrastive alternatives from prior work. Overall, the paper demonstrates both the usefulness of contrastive explanations for commonsense reasoning, as well as techniques to generate them from language models. The prompting approach elicits knowledge in a targeted yet open-ended way.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes an approach to generate contrastive explanations for commonsense reasoning tasks. The input consists of a context with a placeholder, and two answer choices (fact and foil). An explainer PLM first generates contrastive explanations by filling in preset templates that explicitly contrast the two answers according to key distinguishing attributes. The templates are derived from an analysis of human explanations. The explainer PLM fills in the templates based on the context and answer choices to produce candidate explanations. A task model PLM then scores each answer conditioned on the context and each candidate explanation. The final prediction marginalizes over the scores from all candidate explanations. The approach is evaluated on Winograd schemas and physical commonsense QA. Experiments show performance gains over prior methods, and that the contrastive explanations are more useful to humans and exhibit greater faithfulness than non-contrastive alternatives.


## What problem or question is the paper addressing?

 This paper is addressing the challenge of getting large pretrained language models (PLMs) like T5 and BART to generate human-interpretable explanations for their predictions on commonsense reasoning tasks. 

Some key points:

- PLMs can achieve high performance on commonsense reasoning tasks but don't provide explanations for their reasoning. Recent work has tried generating free-form explanations, but these are hard to evaluate and don't necessarily reflect the model's reasoning. 

- The paper proposes prompting the PLMs to generate "contrastive explanations" that explicitly compare the answer choices (fact vs foil) along some differentiating attribute. This is inspired by how humans tend to explain choices contrastively.

- They develop contrastive prompt templates based on human annotations, which are filled in by the PLM to generate contrastive explanations. 

- A separate task model then conditions on the original input and generated explanation to make the final prediction.

- This approach shows improved performance over baselines on Winograd schemas and physical commonsense QA. The explanations are also preferred by humans and can be manipulated to evaluate faithfulness.

- So in summary, the paper provides a way to get PLMs to generate more useful and evaluable explanations for commonsense tasks by prompting contrastive explanations, with both performance and human evaluation benefits.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Contrastive explanations - The paper focuses on generating contrastive natural language explanations that compare multiple answer choices based on their key distinguishing attributes. 

- Commonsense reasoning - The paper aims to improve performance on commonsense reasoning NLP tasks like the Winograd Schema Challenge and Physical Interaction QA. These require implicit reasoning.

- Pretrained language models (PLMs) - The method uses large PLMs like T5 and BART to generate contrastive explanations by filling prespecified templates.

- Templates - The paper develops templates to constrain and guide contrastive explanation generation, based on analysis of human explanations.

- Performance gains - The approach shows improved accuracy over baselines on commonsense tasks, especially in the zero-shot setting.

- Faithfulness - Contrastive explanations allow novel evaluations of faithfulness by manipulating the generated text.

- Human evaluation - Contrastive explanations are judged by humans as more useful than prior methods.

- Counterfactuals - The approach is related to counterfactual reasoning, but aims to represent counterfactual knowledge rather than inputs.

In summary, the key ideas involve using contrastive explanation templates tailored for commonsense tasks, eliciting implicit reasoning from PLMs, and gains in task performance and faithfulness.
