# [Prompting Contrastive Explanations for Commonsense Reasoning Tasks](https://arxiv.org/abs/2106.06823)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of getting large pretrained language models (PLMs) like T5 and BART to provide human-interpretable explanations for their decisions on commonsense reasoning tasks. The central hypothesis is that prompting the PLMs to generate contrastive explanations that explicitly compare the possible answers will elicit relevant commonsense knowledge embedded in the models' parameters, improving both task performance and explanation quality compared to prior approaches.

Specifically, the key aspects of the paper are:

- Commonsense reasoning tasks often require comparing plausible alternatives and human explanations tend to be contrastive in nature. 

- The authors develop a set of contrastive prompt templates that can be filled in by PLMs to generate explanations that contrast the possible answers (e.g. "X is Y while Z is W").

- A two-stage framework where an "explainer" PLM generates contrastive explanations from the prompts, and a "task" PLM makes predictions conditioned on the original input and explanations.

- Contrastive explanations improve task accuracy over non-contrastive baselines, with notable gains in the zero-shot setting.

- Human evaluations show contrastive explanations are more useful than prior approaches. 

- Contrastive prompts allow evaluating faithfulness by manipulating explanations (e.g. flipping contrast).

So in summary, the main hypothesis is that prompting for contrastive explanations will better elicit commonsense knowledge from PLMs in a human-interpretable way, leading to quantitative and qualitative improvements over prior approaches. The results support this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the key contributions of this paper are:

1. Proposing an unsupervised method to generate contrastive explanations for commonsense reasoning tasks by prompting PLMs to complete specialized templates. This constrained generation process produces more useful explanations compared to prior work on unconstrained free-form generation. 

2. Demonstrating improved task performance on two commonsense reasoning benchmarks (WSC and PIQA) by having a separate task model condition its predictions on the generated contrastive explanations. Notably, the gains are larger in the zero-shot setting and when training data is limited.

3. Enabling new evaluations of explanation faithfulness by manipulating the generated contrastive explanations, either by flipping the contrast or abstracting the identities of the fact and foil. Results indicate the model relies on the explanations to some extent.

4. Conducting human evaluations showing the contrastive explanations are deemed more useful than prior work with clarification questions or unconstrained generation.

5. Demonstrating the generalizability of the approach by using the same prompts designed for WSC/PIQA and achieving strong zero-shot performance on CommonsenseQA.

In summary, the main contribution is presenting an unsupervised and human-centered approach to generate constrained yet customizable explanations that are shown to be useful through both automated and human evaluations. The contrastive explanation format also enables new ways to analyze the models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an unsupervised method to generate contrastive natural language explanations for commonsense reasoning tasks by prompting pretrained language models to complete contrastive templates that differentiate answer choices, which improves task performance and enables novel evaluations of explanation faithfulness.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related work:

- It focuses on generating contrastive explanations for commonsense reasoning tasks, while much prior work has focused on free-form explanation generation. The authors argue contrastive explanations are more constrained, interpretable, and amenable to evaluation.

- It leverages prompting techniques to generate contrastive explanations from PLMs. This is similar to other prompting-based methods like self-talk, but more targeted to contrastive reasoning.

- The contrastive explanations are shown to improve task performance, especially in low-resource settings, over both free-form generation baselines and prompting baselines like self-talk.

- The paper introduces techniques to evaluate explanation faithfulness, like flipping the contrast and abstracting the fact/foil. This provides a more rigorous way to analyze models' use of the explanations.

- The work is inspired by theories of human contrastive explanation from philosophy/psychology. The prompts are designed to mimic patterns in how humans give contrastive explanations.

- It is related to work on counterfactual explanations, but generates contrastive knowledge rather than counterfactual inputs. The contrastive explanations can hypothetically be "reversed" to evaluate faithfulness.

Overall, the key novelties are in using targeted contrastive prompting for explanation generation in NLP, showing benefits over prior methods, and developing new techniques to evaluate explanation quality and faithfulness. The grounding in theories of human explanation is also notable.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors are:

- Supporting contrastive explanations for broader applicability: The authors primarily experiment with Winograd schemas and physical commonsense QA. They suggest exploring the framework for other types of commonsense reasoning datasets. 

- End-to-end training: The authors do not fine-tune the explainer PLM due to challenges with backpropagating through discrete operations like beam search. They suggest investigating techniques to enable end-to-end training.

- Improving contrastive in-filling: The authors generate explanations by infilling contrastive templates. They suggest exploring additional pretraining data or objectives to improve the quality of contrastive in-filling. 

- Quantifying model reliance on explanations: The authors propose techniques like flipping explanations and abstracting entities to evaluate reliance on explanations. More such techniques could be explored. 

- Leveraging counterfactuals: The authors suggest counterfactual examples could provide supervision to generate better contrastive explanations. This could be an interesting direction.

- Supporting full reasoning chains: The current approach generates explanations for individual examples. Extending it to support multi-step reasoning over chains of examples is suggested.

In summary, the key future directions are around improving the framework's applicability, faithfulness, and integration of contrastive explanations into end-to-end training. Exploring supervision from counterfactuals and multi-step reasoning are also interesting open problems highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes an unsupervised method to generate contrastive explanations from pretrained language models (PLMs) to explain their reasoning on commonsense reasoning tasks. The key idea is to use contrastive language templates that can be populated by the PLM to generate explanations that explicitly contrast the answer choices (fact vs foil) along their distinguishing attributes. For instance, on a Winograd schema example with answer choices "peanuts" and "raisins", the model may generate "Peanuts are salty while raisins are sweet". The authors show that using a separate PLM to make predictions conditioned on both the input and these contrastive explanations improves performance on Winograd schema and physical commonsense QA datasets compared to prior approaches, especially in low-resource settings. The contrastive explanations are also preferred by humans and can be semantically perturbed to quantify model faithfulness. Overall, the paper demonstrates an effective application of prompting to elicit structured explanatory reasoning from language models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a method for generating contrastive explanations from pretrained language models (PLMs) to explain their predictions on commonsense reasoning tasks. Commonsense reasoning often requires comparing plausible alternatives and explaining why one is more likely based on implicit background knowledge. The authors propose using contrastive language prompts, which are templates with placeholders for the alternatives, to elicit such knowledge from PLMs. The PLMs are prompted to fill in the templates by contrasting the alternatives along key attributes (e.g. "X is salty while Y is sweet"). 

The method is evaluated on the Winograd Schema Challenge and Physical Interaction QA. Results show improvements in task performance over prior methods, indicating the contrastive explanations provide useful knowledge. The explanations also allow novel evaluations of faithfulness by manipulating them to imply the opposite answer. Human evaluations find the contrastive explanations much more relevant and helpful than non-contrastive alternatives from prior work. Overall, the paper demonstrates both the usefulness of contrastive explanations for commonsense reasoning, as well as techniques to generate them from language models. The prompting approach elicits knowledge in a targeted yet open-ended way.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes an approach to generate contrastive explanations for commonsense reasoning tasks. The input consists of a context with a placeholder, and two answer choices (fact and foil). An explainer PLM first generates contrastive explanations by filling in preset templates that explicitly contrast the two answers according to key distinguishing attributes. The templates are derived from an analysis of human explanations. The explainer PLM fills in the templates based on the context and answer choices to produce candidate explanations. A task model PLM then scores each answer conditioned on the context and each candidate explanation. The final prediction marginalizes over the scores from all candidate explanations. The approach is evaluated on Winograd schemas and physical commonsense QA. Experiments show performance gains over prior methods, and that the contrastive explanations are more useful to humans and exhibit greater faithfulness than non-contrastive alternatives.


## What problem or question is the paper addressing?

 This paper is addressing the challenge of getting large pretrained language models (PLMs) like T5 and BART to generate human-interpretable explanations for their predictions on commonsense reasoning tasks. 

Some key points:

- PLMs can achieve high performance on commonsense reasoning tasks but don't provide explanations for their reasoning. Recent work has tried generating free-form explanations, but these are hard to evaluate and don't necessarily reflect the model's reasoning. 

- The paper proposes prompting the PLMs to generate "contrastive explanations" that explicitly compare the answer choices (fact vs foil) along some differentiating attribute. This is inspired by how humans tend to explain choices contrastively.

- They develop contrastive prompt templates based on human annotations, which are filled in by the PLM to generate contrastive explanations. 

- A separate task model then conditions on the original input and generated explanation to make the final prediction.

- This approach shows improved performance over baselines on Winograd schemas and physical commonsense QA. The explanations are also preferred by humans and can be manipulated to evaluate faithfulness.

- So in summary, the paper provides a way to get PLMs to generate more useful and evaluable explanations for commonsense tasks by prompting contrastive explanations, with both performance and human evaluation benefits.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Contrastive explanations - The paper focuses on generating contrastive natural language explanations that compare multiple answer choices based on their key distinguishing attributes. 

- Commonsense reasoning - The paper aims to improve performance on commonsense reasoning NLP tasks like the Winograd Schema Challenge and Physical Interaction QA. These require implicit reasoning.

- Pretrained language models (PLMs) - The method uses large PLMs like T5 and BART to generate contrastive explanations by filling prespecified templates.

- Templates - The paper develops templates to constrain and guide contrastive explanation generation, based on analysis of human explanations.

- Performance gains - The approach shows improved accuracy over baselines on commonsense tasks, especially in the zero-shot setting.

- Faithfulness - Contrastive explanations allow novel evaluations of faithfulness by manipulating the generated text.

- Human evaluation - Contrastive explanations are judged by humans as more useful than prior methods.

- Counterfactuals - The approach is related to counterfactual reasoning, but aims to represent counterfactual knowledge rather than inputs.

In summary, the key ideas involve using contrastive explanation templates tailored for commonsense tasks, eliciting implicit reasoning from PLMs, and gains in task performance and faithfulness.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to summarize the key points of the paper:

1. What is the problem being addressed in the paper?

2. What are the limitations of prior work on explaining commonsense reasoning tasks? 

3. What is the key social observation that motivates the use of contrastive explanations?

4. How do contrastive explanations help constrain the space of possible explanations? 

5. What are the two commonsense reasoning tasks studied in the paper?

6. How are the contrastive templates created and evaluated?

7. How does the proposed two-stage approach work at a high level?

8. What are the main results compared to baseline methods on the two tasks?

9. How is the reliance of the task model on the explanations analyzed?

10. What do the human evaluations reveal about the usefulness of contrastive explanations?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper relies on contrastive explanations being more efficient for commonsense reasoning tasks. Could you elaborate more on why focusing explanations on differences between alternatives is computationally and cognitively more efficient compared to other types of explanations?

2. You mention using a small set of contrastive templates to guide explanation generation. How did you arrive at this initial set of templates? Were certain templates more effective than others for generating useful explanations? 

3. When generating explanations, you first construct a "neutral" context by filling the placeholder with a generic term. How does this neutral context help the model generate better contrastive explanations compared to using the original context directly?

4. You use top-K decoding to generate multiple candidate explanations and feed all of them to the task model. What is the intuition behind generating and using multiple explanations instead of just the top-1? Does incorporating more candidate explanations consistently improve performance?

5. The two-stage setup uses separate models for explanation generation and final prediction. Have you experimented with any joint training or end-to-end formulations? What are the challenges with learning both stages together?

6. You evaluate explanation quality indirectly through task accuracy. Are there any more direct ways you have tried to evaluate the quality of the generated explanations themselves?

7. The paper focuses on multiple choice QA datasets. How would you extend this approach to generate contrastive explanations for other tasks like open-ended QA, summarization, or dialogue? What new challenges might arise?

8. You propose flipped and abstracted evaluations to test reliance on explanations. Are there any other analysis techniques or evaluations you have explored to better understand model behaviors? 

9. The contrastive templates are created based on human annotations. Could these templates be learned automatically from data instead? What approaches seem promising for learning good prompt patterns?

10. The paper reports impressive zero-shot performance from using contrastive explanations. What factors do you think contribute most to the large zero-shot gains observed?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper presents a method to generate contrastive explanations from pretrained language models (PLMs) to explain their reasoning for commonsense reasoning tasks. Many such tasks involve choosing the best answer from a set of alternatives based on implicit commonsense knowledge. The authors propose using contrastive prompts to elicit contrastive explanations from PLMs, which compare the correct answer (fact) and incorrect answer (foil) by the attributes that differentiate them. For example, to explain why raisins rather than peanuts were chosen, the PLM may generate "raisins are sweet while peanuts are salty." The key ideas are: 1) Contrastive explanations focus only on the key differentiating attributes rather than all possible reasons, making them efficient. 2) The prompts provide a constrained way to elicit targeted, relevant explanations without extra supervision. 3) The generated explanations are fed as additional context along with the original input to help the model make better predictions in a two-stage pipeline. The approach is evaluated on the Winograd Schema Challenge and Physical Interaction QA, outperforming baselines including free-form explanation models. Human evaluations also find the contrastive explanations more relevant and helpful. Additionally, "flipping" the contrast shows the model relies on it, and abstracting the answers shows the explanations provide useful knowledge. Overall, contrastive prompting provides an effective way to improve performance and explainability of PLMs for commonsense tasks.


## Summarize the paper in one sentence.

 The paper presents an approach to generate contrastive explanations for commonsense reasoning tasks by prompting pretrained language models with templates designed to elicit comparisons between answer choices.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper presents an unsupervised method to generate contrastive explanations from pretrained language models (PLMs) for commonsense reasoning tasks. Many such tasks involve choosing between multiple answers by comparing and contrasting them along key attributes. Inspired by how humans provide contrastive explanations, the authors develop templates to prompt a PLM to fill in explanations that explicitly contrast the answers. For example, on a task asking whether raisins or peanuts make a better snack, the PLM may generate "Raisins are sweet while peanuts are salty." Conditioning the PLM's predictions on these explanations improves performance on two commonsense benchmark datasets compared to previous methods. The contrastive explanations are also judged by humans as more relevant and helpful than non-contrastive alternatives. Additionally, the explanations can be evaluated for faithfulness by reversing or abstracting the contrasts, which often flips model predictions. Overall, prompting PLMs for contrastive explanations is a promising method to elicit relevant commonsense knowledge and reasoning from large pretrained models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using contrastive prompts to generate explanations from pretrained language models. Why are contrastive explanations hypothesized to be more useful than generic explanations? How does contrastive reasoning relate to the nature of commonsense reasoning tasks?

2. Contrastive prompts are generated using a small set of templates designed to cover different types of attributes that could differentiate between answer choices (e.g. temporal, spatial, preferences). What was the process for developing these templates and ensuring they have good coverage of important contrastive attributes? 

3. The paper proposes a two-stage pipeline involving an explainer model that generates contrastive explanations from prompts, and a task model that makes predictions based on the original input and explanations. Walk through the details of how these two models operate and interact. What are the benefits of separating these functions?

4. The paper demonstrates performance improvements from using contrastive explanations on Winograd Schema Challenge, Physical Interaction QA, and CommonsenseQA. Analyze the results shown across different model variations and datasets. What trends do you notice? What might explain some of the differences?

5. Human evaluations showed contrastive explanations were preferred over self-talk explanations on relevance, correctness, and helpfulness. Analyze the differences between these two approaches that might have led to these results. What are the tradeoffs?

6. The paper proposes two methods to evaluate the faithfulness of contrastive explanations - flipping explanations and abstracting entities. Explain how each method works and what it is intended to evaluate. What are the limitations?

7. The set of contrastive prompts was shown to generalize well to CommonsenseQA even though they were designed for Winograd Schema and PIQA. Why might contrastive reasoning transfer between commonsense reasoning tasks? What challenges might arise in applying this method to new tasks?

8. The paper focuses on multiple choice QA tasks where two choices are explicitly provided. How might the approach be extended to tasks with more than two candidate answers? What about open-ended QA or other commonsense tasks?

9. The authors use T5 and BART to generate contrastive explanations. How might the choice of pretrained language model impact the quality and usefulness of explanations? What other model architectures could be promising for this approach?

10. The paper provides a novel method for prompting pretrained language models to generate more focused explanations. What other techniques could build on this work to make model explanations more grounded, faithful, and interpretable? What are interesting directions for future work in this area?
