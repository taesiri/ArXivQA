# Prompting Contrastive Explanations for Commonsense Reasoning Tasks

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of getting large pretrained language models (PLMs) like T5 and BART to provide human-interpretable explanations for their decisions on commonsense reasoning tasks. The central hypothesis is that prompting the PLMs to generate contrastive explanations that explicitly compare the possible answers will elicit relevant commonsense knowledge embedded in the models' parameters, improving both task performance and explanation quality compared to prior approaches.Specifically, the key aspects of the paper are:- Commonsense reasoning tasks often require comparing plausible alternatives and human explanations tend to be contrastive in nature. - The authors develop a set of contrastive prompt templates that can be filled in by PLMs to generate explanations that contrast the possible answers (e.g. "X is Y while Z is W").- A two-stage framework where an "explainer" PLM generates contrastive explanations from the prompts, and a "task" PLM makes predictions conditioned on the original input and explanations.- Contrastive explanations improve task accuracy over non-contrastive baselines, with notable gains in the zero-shot setting.- Human evaluations show contrastive explanations are more useful than prior approaches. - Contrastive prompts allow evaluating faithfulness by manipulating explanations (e.g. flipping contrast).So in summary, the main hypothesis is that prompting for contrastive explanations will better elicit commonsense knowledge from PLMs in a human-interpretable way, leading to quantitative and qualitative improvements over prior approaches. The results support this hypothesis.
