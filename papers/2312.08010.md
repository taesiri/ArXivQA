# [EZ-CLIP: Efficient Zeroshot Video Action Recognition](https://arxiv.org/abs/2312.08010)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper proposes EZ-CLIP, an efficient adaptation of image-based visual language models like CLIP for zero-shot video action recognition. EZ-CLIP introduces two key innovations - temporal visual prompting and a motion loss constraint. Temporal visual prompting leverages additional learnable prompts that are conditioned on the input video frames to model temporal relationships. This prompting enables cross-frame temporal learning with minimal parameters. The motion loss term guides these prompts to focus on capturing motion dynamics rather than just appearance. Together, these enable EZ-CLIP to achieve strong performance in modeling temporal aspects critical for video tasks, while preserving the remarkable generalization capability of models like CLIP. EZ-CLIP requires only 5.2 million learnable parameters, allowing efficient training on a single GPU. Extensive experiments on five benchmark datasets demonstrate its effectiveness - it outperforms prior state-of-the-art approaches on most evaluations, especially on temporally complex datasets like Something-Something-v2. Ablations validate the consistent benefits from both prompting and the motion loss. Thus, through simple yet impactful innovations, EZ-CLIP pushes the envelope in efficiently adapting visual language models for video understanding.
