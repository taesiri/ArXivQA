# [HSViT: Horizontally Scalable Vision Transformer](https://arxiv.org/abs/2404.05196)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Vision transformers (ViTs) lack inductive biases like shift, scale and rotation invariance compared to CNNs, requiring pre-training on large datasets. 
- Growing layers and parameters in ViTs and CNNs impede deployment on resource-constrained mobile devices for mobile multimedia services.

Proposed Solution:
- A novel horizontally scalable vision transformer (HSViT) is proposed.
- A new image-level feature embedding is designed to better leverage inductive biases from CNN layers into ViT. Multiple convolutional kernels concurrently extract features from the whole image which are hierarchically downsampled to fixed size feature maps.
- These feature maps are flattened as sequences and fed into transformer encoders to capture long-range dependencies among distinct image-level features.
- A novel horizontally scalable transformer architecture splits the feature maps into separate attention groups for distributed computation. Self-attention is computed within each group and predictions aggregated by average pooling for final classification.  

Main Contributions:
- The image-level feature embedding better utilizes inductive biases from CNNs, reducing ViT's reliance on large-scale pre-training.
- The horizontally scalable design reduces model parameters and layers, and allows collaborative training and inference across multiple devices.
- Experiments show the model achieves higher accuracy than CNNs and ViTs with fewer parameters and layers, demonstrating superior inductive bias preservation.
- The distributed architecture facilitates deployment on resource-constrained mobile devices for mobile multimedia applications.

In summary, the paper proposes a Transformer model that integrates CNN inductive biases and distributed computation to achieve better accuracy without large-scale pre-training, enabling efficient deployment on mobile devices. The core ideas are the image-level feature embedding and horizontal scaling.
