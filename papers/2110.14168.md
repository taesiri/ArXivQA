# Training Verifiers to Solve Math Word Problems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:Can training verifiers to judge the correctness of model-generated solutions improve the performance of large language models on solving grade school math word problems?More specifically, the key hypotheses tested in the paper are:1) Verification (training a model to score candidate solutions) will boost performance compared to just finetuning a model, especially with larger datasets.2) Verifiers will scale better with increased data than finetuning alone. 3) Token-level verifiers (predicting correctness at each token) will outperform solution-level verifiers (predicting correctness for the full solution).4) Including a language modeling objective when training verifiers improves results over just training to predict correctness.5) Using dropout as a regularizer boosts both finetuning and verification performance. So in summary, the central research question is whether training verifiers can improve math reasoning compared to standard finetuning, especially as more training data becomes available. The hypotheses focus on the settings and variants of verification that work best.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Introducing GSM8K, a new dataset of 8.5K high quality grade school math word problems for evaluating mathematical reasoning skills of language models. The dataset has high linguistic diversity while relying only on elementary math concepts.2. Proposing a verification approach to improve performance on math word problems. This involves training separate generator and verifier models. The generator produces candidate solutions, and the verifier scores them to select the best one. 3. Providing evidence that verification scales significantly better with increased data compared to a finetuning baseline. Verification gives similar gains to a 30x increase in model size.4. Showing that dropout acts as an effective regularizer for both finetuning and verification, leading to noticeable performance improvements.5. Releasing the GSM8K dataset to facilitate research into improving mathematical reasoning capabilities of large language models.In summary, the main contribution is proposing and evaluating a verification approach for solving math word problems. The verification method combined with the new GSM8K dataset enables better analysis of model capabilities on mathematical reasoning.
