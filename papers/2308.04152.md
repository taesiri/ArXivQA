# Empowering Vision-Language Models to Follow Interleaved Vision-Language   Instructions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can vision-language models be empowered to follow more complex interleaved vision-language instructions that involve multiple interrelated images and text? The key hypotheses seem to be:1) Existing vision-language models struggle with instructions involving interleaved image-text sequences because their visual prompt generators are mainly trained on image-caption alignment, which focuses on extracting common foreground information for captioning. This results in failure to extract specific visual information needed for particular tasks.2) A controllable knowledge re-injection module can be added to selectively direct the visual prompt generator to extract missing task-specific visual information based on instruction semantics provided by the language model. This allows complementing the language model with the visual knowledge needed to understand complex instructions.3) The controllable re-injection module can be efficiently trained without massive instruction-following data by using a proposed cross-attention guided counterfactual image training strategy.In summary, the central research question is how to empower vision-language models to handle more complex multi-modal instructions, with hypotheses about adding controllable re-injection of visual knowledge and efficiently training this with counterfactual images rather than large instruction datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing I4, a new benchmark for evaluating interleaved vision-language instruction following. I4 contains 31 diverse tasks formatted in a unified instruction-response style, covering a wide range of practical scenarios like cartoons, textbooks, webpages, etc. 2. Through evaluation on I4, identifying a key limitation of current multimodal language models - their visual prompt generators tend to focus on common foreground objects and struggle to extract specific visual information needed for particular tasks.3. Proposing a lightweight Controllable Knowledge Re-Injection (CLORI) module that allows the language model to conditionally control the visual prompt generator to extract missing task-specific visual information and inject it back into the language model.4. Introducing a Cross-Attention Guided Counterfactual Image Training (CAGIT) strategy to efficiently learn the CLORI module without needing large volumes of instruction tuning data.5. Achieving state-of-the-art results on the challenging I4 benchmark using Cheetor, a Transformer-based multimodal language model enhanced with the proposed CLORI module and CAGIT training. Cheetor outperforms prior methods substantially, using only 30k images for training.In summary, the main contribution appears to be the new I4 benchmark, the identification of limitations of current models, and the lightweight CLORI module along with the efficient CAGIT training strategy to address these limitations and achieve much improved instruction following performance.
