# [Towards Fast and Stable Federated Learning: Confronting Heterogeneity   via Knowledge Anchor](https://arxiv.org/abs/2312.02416)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper provides an in-depth analysis of the issue of catastrophic forgetting in federated learning when dealing with heterogeneous (non-IID) data distributions among clients. Through extensive experiments, the authors demonstrate that only the missing classes and non-dominant classes (those with few samples) suffer from severe catastrophic forgetting during local client training, while the performance of dominant classes actually improves. They further show that the reduction of samples for non-dominant classes has little impact on mitigating this forgetting, indicating clients' inability to fully leverage those limited samples. However, reducing samples for dominant classes leads to abrupt catastrophic forgetting once below a certain threshold. Motivated by these findings, the authors propose Federated Knowledge Anchor (FedKA), which constructs a minimal representative sample set across all classes to serve as "anchor points" during local training to preserve knowledge and combat catastrophic forgetting for missing and non-dominant classes. Specifically, FedKA minimizes the L2 distance between the discarded logits from these anchor points outputted by the global versus the local model. Experiments demonstrate state-of-the-art performance of FedKA in terms of accuracy, convergence speed, and robustness to heterogeneous data distributions.
