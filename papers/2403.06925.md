# [Simplicity Bias of Transformers to Learn Low Sensitivity Functions](https://arxiv.org/abs/2403.06925)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Transformers have become ubiquitous in machine learning and achieve state-of-the-art results across diverse tasks. However, there is limited understanding of the inductive biases of transformers and how they differ from other neural network architectures. Specifically, prior work has shown that neural networks have a "simplicity bias" towards learning simple functions, but it is unclear if this notion extends to transformers. 

Proposed Solution:
This paper investigates whether the concept of "sensitivity" provides a unified perspective to understand the simplicity bias of transformers. Sensitivity measures how much a model's output changes for small random perturbations to the input. The authors first theoretically show that transformers have a spectral bias indicating a preference for low-sensitivity functions on Boolean inputs. Through experiments on synthetic and real-world vision and language tasks, they demonstrate that transformers learn functions with lower sensitivity compared to CNNs, MLPs and LSTMs. This bias correlates with improved robustness on corrupted image datasets.

Main Contributions:
- Show theoretically that transformers have a spectral simplicity bias for Boolean functions
- Empirically demonstrate on synthetic and real tasks that transformers prefer low-sensitivity functions compared to CNNs, MLPs and LSTMs  
- This bias correlates with improved robustness to image corruptions
- Adding sensitivity regularization during training further improves robustness, showing sensitivity has prescriptive power
- Lower sensitivity functions correlate with flatter loss landscape minima

In summary, this work provides evidence that sensitivity gives a unified perspective on the simplicity bias of transformers which explains some of their useful inductive biases and properties compared to other architectures. The notion of sensitivity bias could lead to further advancements in understanding and improving transformers.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper shows that transformers exhibit a bias towards learning low-sensitivity functions, providing a unified perspective to understand their simplicity bias and robustness across vision and language tasks compared to other neural network architectures.


## What is the main contribution of this paper?

 The main contribution of this paper is showing that transformers have a bias towards learning low sensitivity functions, across both vision and language tasks. Specifically:

1) The paper theoretically proves that transformers with linear attention exhibit a weak spectral simplicity bias on Boolean functions, preferring low-degree polynomials. 

2) Through experiments on synthetic and real-world vision/language datasets, the paper shows empirically that transformers learn predictors with lower sensitivity than alternative architectures like CNNs, MLPs and LSTMs.

3) The paper demonstrates that lower sensitivity correlates with improved robustness to corruptions and perturbations. It also shows sensitivity can be used as an efficient intervention to further improve transformer robustness.

4) The paper explores the connection between lower sensitivity and flatter loss landscape minima, indicating sensitivity could serve as a unified notion explaining both generalization and robustness of transformers.

In summary, the key contribution is identifying sensitivity as a useful metric that provides a unified perspective to understand the simplicity bias and some of the desirable properties of transformers across different data types and tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Simplicity bias - The paper investigates whether the notion of "simplicity bias" helps explain the behavior and performance of transformers. Simplicity bias refers to neural networks preferring to learn simple functions over complex ones.

- Sensitivity - The paper proposes sensitivity, which measures how much a model's output changes for small random perturbations to the input, as a metric for simplicity bias. Lower sensitivity corresponds to simpler functions.

- Spectral bias - Related to simplicity bias. Spectral bias refers to a preference for learning functions that depend more on low frequency Fourier terms. The paper shows transformers have a spectral bias.  

- Vision transformers (ViTs) - Transformer models for computer vision tasks, which take an image and break it into patches which are treated like tokens. The paper compares sensitivity of ViTs to CNNs and MLPs.

- Robustness - The paper relates simplicity bias and lower sensitivity to model robustness, like improved performance on corrupted datasets like CIFAR-10-C.

- Natural language processing (NLP) tasks - Experiments comparing transformer and LSTM models on text classification datasets in terms of sensitivity.

So in summary, key terms cover simplicity bias, the proposed sensitivity measure, spectral bias, vision transformers, robustness, and language tasks - all in the context of analyzing and understanding transformers.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes sensitivity as a unified metric to explain the simplicity bias and spectral bias of transformers. What are some limitations of using sensitivity to characterize the simplicity bias? For example, are there simple functions that can have high sensitivity?

2. The paper shows empirically that transformers have lower sensitivity than CNNs and MLPs on vision tasks, and lower than LSTMs on language tasks. Can you theoretically characterize the class of functions learned by different architectures based on their sensitivity? 

3. The synthetic experiments indicate that transformers prefer lower sensitivity functions even when they depend on more tokens. Does this provide any insight into why transformers generalize better in practice? For example, could relying on more tokens make them less prone to overfitting spurious correlations?

4. The paper argues that lower sensitivity functions are more robust. However, it seems possible to construct low sensitivity functions that are still brittle, for example by relying on specific combinations of features. Are there other complexity measures beyond sensitivity that better explain the robustness of transformers?

5. The proposed sensitivity regularization scheme encourages lower sensitivity by injecting noise into the inputs. How does this relate to existing data augmentation schemes? Is there a principle behind what kinds of augmentations would encourage lower sensitivity?

6. The paper shows lower sensitivity functions correspond to flatter optima. However, recent work has claimed flatness may not explain generalization. Does the notion of sensitivity provide additional insight into this debate? For example, are there benefits of low sensitivity beyond flatness?  

7. Transformers are shown to have uniform sensitivity across tokens, while LSTMs are more sensitive to recent tokens. Does this uniformity property have other implications on properties of transformers, beyond robustness?

8. For language tasks, the paper finds the CLS token in RoBERTa leads to spikes in sensitivity. Does this mean the CLS token representation ends up being less robust? Are there alternative ways to construct this representation that could reduce sensitivity further?

9. The linear attention mechanism is argued to induce spectral bias in transformers. However, most practical transformers use softmax attention. What modifications would be needed to induce low sensitivity with softmax attention?

10. The paper studies sensitivity extensively on vision and language tasks. What new complexities arise when extending sensitivity as a simplicity measure to other modalities such as graphs or reinforcement learning environments? How should the neighborhood for computing sensitivity be defined in those settings?
