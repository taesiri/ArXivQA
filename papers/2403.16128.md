# [Enhancing Video Transformers for Action Understanding with VLM-aided   Training](https://arxiv.org/abs/2403.16128)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Vision Transformers (ViTs) currently achieve the best performance for video action understanding but have limited generalization ability across domains or datasets. On the other hand, Visual Language Models (VLMs) have shown exceptional generalization performance but cannot process videos and extract spatio-temporal patterns critical for action understanding.

Proposed Solution: 
- The paper proposes the Four-tiered Prompts (FTP) framework to take advantage of the complementary strengths of ViTs and VLMs. The key idea is to retain ViTs' ability to learn strong spatio-temporal representations from videos but improve their generalization capability by aligning the visual features with descriptive outputs from VLMs during training.

- Specifically, the FTP framework adds four feature processors to a ViT that focus on different aspects of actions through prompts: category, components, detailed description, context. The VLM provides textual descriptions only during training. Contrastive loss aligns the visual features from the ViT with the textual embeddings from the prompts.

- During inference, only the ViT is used with negligible overhead from the added processors. But the aligned visual features are richer and more domain-general. Fine-tuning adapts the features to a target dataset or domain.

Main Contributions:

- Proposes a novel framework FTP to enhance video Transformers using easy-to-obtain textual supervision from VLMs during training. Shows strong performance while adding minimal computation overhead.

- Demonstrates the potential of integrating language models into the video domain to improve model generalization.

- Reports state-of-the-art results on major video action recognition benchmarks, outperforming previous best methods by clear margins. For example, 93.8% top-1 accuracy on Kinetics-400 (+2.8%) and 83.4% on Something-Something V2 (+2.6%).
