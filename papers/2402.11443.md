# [Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM   Evaluation](https://arxiv.org/abs/2402.11443)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
- Static benchmark datasets are becoming inadequate to accurately evaluate rapidly advancing large language models (LLMs). This is due to the models' quick evolution and potential data contamination issues during training.

Proposed Solution:
- The paper introduces a benchmark self-evolving framework powered by a multi-agent system to dynamically extend existing benchmarks for a more scalable, robust and fine-grained LLM evaluation.

- The framework generates new instances by modifying context or questions of original benchmarks using 6 reframing operations: question alternating/complicating, context paraphrasing/noising/reversing, and sub-ability probing. 

- A multi-agent generator is designed, involving a pre-filter, an instance creator, a verifier and a candidate option formulator. This system ensures the accuracy of dynamically generated instances.

Main Contributions:  
- Provides scalable evaluation by creating more complex or alternative questions to test generalization.

- Enables robust evaluation via perturbations to contexts like paraphrasing, noising and reversing to examine model robustness.  

- Allows fine-grained evaluation by generating sub-ability questions to probe key reasoning skills.

- Expands performance gaps between models and also the discrepancies of a single model across tasks.

- Mitigates skewed evaluation caused by data contamination.

- The proposed framework is model-agnostic and can generalize to diverse tasks. Experiments on mathematical, logical, commonsense reasoning and reading comprehension tasks demonstrate its effectiveness.
