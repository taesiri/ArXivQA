# [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/abs/2312.02949)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Recent large multimodal models (LMMs) have shown exceptional visual chat abilities but struggle with fine-grained visual understanding and grounding due to lack of high-quality grounded visual chat data and suboptimal model designs. 
- Existing models treat grounding separately, focus only on short captions, output box coordinates rather than masks, and have limited support for visual prompts.

Proposed Solution:
- Created a grounded visual chat (GVC) dataset with 150K samples by leveraging human-labeled detection data and GPT-4's annotation capabilities.
- Proposed an end-to-end model, LLaVA-Grounding, that connects a segmentation model (OpenSeeD) with a language model (Vicuna-7B) to enable pixel-level grounding and chat.
- Introduced capabilities to handle various visual prompts like clicks, boxes and marks.

Main Contributions:
- GVC dataset creation using human-labeled detection data and GPT-4.
- End-to-end LLaVA-Grounding model for grounded chat with support for pixel-level grounding and diverse visual prompts.
- Grounding Bench benchmark to assess grounded chat capabilities with new metrics like grounded recall and precision.
- Demonstrated state-of-the-art grounded chat performance and competitive grounding results on RefCOCO/+/g and Flickr30K Entities.

The key novelty is the ability to perform high-quality grounded chat by combining advances in language models, grounding models and data creation. The solution enables robust pixel-level grounding fully integrated with the chat functionality.
