# [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/abs/2312.02949)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary of the key points from the paper:

This paper introduces LLaVA-Grounding (LLaVA-G), a large multimodal model designed for grounded visual chat. The authors make several key contributions: 

1) They create a grounded visual chat (GVC) dataset with 150K instances by leveraging human-annotated object detection data and GPT-4's language capabilities. This allows combining high-quality visual grounding data with natural conversations.

2) They propose an end-to-end network architecture connecting a language model with a grounding model (OpenSeeD). This facilitates both object and pixel-level grounding from the language queries. 

3) They establish a Grounding-Bench benchmark to assess models on grounded visual chat through metrics like grounded precision, grounded recall and chat scores. 

4) Through experiments, they demonstrate state-of-the-art performance of LLaVA-G on Grounding-Bench. It also achieves competitive scores on RefCOCO and Flickr30K entities compared to other grounding LMMs. Qualitative results further showcase LLaVA-G's capabilities for diverse grounding types, hallucination reduction and debugging.

In summary, through innovations in data, modeling and benchmarking, this paper pushes the boundaries of integrating grounding abilities in large multimodal chat models, with analysis showing clear benefits. The data and benchmark specifically stand out as valuable contributions to drive further progress.
