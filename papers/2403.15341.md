# [Collaborative AI Teaming in Unknown Environments via Active Goal   Deduction](https://arxiv.org/abs/2403.15341)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers the problem of enabling AI agents to effectively collaborate and team up with other autonomous agents or humans (referred to as "unknown agents") in shared environments, when the goals/rewards of those unknown agents are not known a priori. Existing multi-agent reinforcement learning (MARL) methods require pre-defined rewards and thus cannot handle unknown agents with latent, undefined rewards. This limits the ability of AI to collaborate with entities like humans or unfamiliar autonomous systems that often have implicit goals.

Proposed Solution: 
The paper proposes a Synergistic Teaming with UNknown agents (STUN) framework. The key ideas are:

1) Active goal inference: Use a kernel density Bayesian inverse reinforcement learning (KD-BIL) approach to infer the posterior distribution of the latent rewards of unknown agents from observations of their behavior trajectories. This allows sample-efficient inference of complex reward functions.

2) Zero-shot policy adaptation: Pre-train goal-conditioned policies for AI agents using randomly sampled reward functions. Prove that unbiased reward estimates from step 1) suffice for optimal collaboration. Enable one-shot policy adaptation during execution by conditioning pre-trained policies on inferred rewards.

Main Contributions:

- A new problem formulation and framework (STUN) for enabling AI to team up and collaborate with agents that have unknown, latent goals/rewards

- A kernel density Bayesian inverse RL method (KD-BIL) that allows sample-efficient inference of complex, nonlinear reward functions 

- Theorem showing unbiased reward estimates are sufficient for optimal teaming policies 

- Zero-shot policy adaptation approach that conditions pre-trained goal-conditioned policies on inferred rewards

- Modified multi-agent particle and StarCraft environments with diverse unknown agents to demonstrate 50%+ performance improvements in collaborative tasks

The key novelty is the ability to achieve synergistic teaming without needing to know a priori or re-define the goals/rewards of all agents, through online goal inference and zero-shot policy adaptation. Experiments in redesigned collaborative environments demonstrate state-of-the-art performance.
