# [Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery](https://arxiv.org/abs/2402.15739)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies stochastic bandit problems with low-rank structure. Specifically, the expected rewards are modeled by a low rank matrix M in R^{m×n}. In each round, a context i is randomly selected from [m], the learner chooses an arm j from [n], and observes a noisy reward corresponding to entry M_{i,j}. The goal is to efficiently learn in this setting, for three key tasks - policy evaluation, best policy identification, and regret minimization. 

Proposed Solution:
The main insight is a two-phase approach - first leverage spectral methods to estimate the singular subspaces of M. This allows transforming the problem into an instance of misspecified contextual linear bandits with dimension roughly r(m+n). In the second phase, algorithms from this simpler setting can be adapted. 

For policy evaluation, two algorithms are proposed - DSM-PE directly estimates M while RS-PE refines this estimate by solving the misspecified linear bandit via least squares. For best policy identification, DSM-BPI and RS-BPI similarly use either the direct estimate of M or the refined estimate. For regret minimization, RS-RMIN uses an extension of SupLinUCB to solve the transformed problem.

Main Contributions:

1) The paper provides the first policy evaluation and best policy ID algorithms for low rank bandits that are nearly minimax optimal in terms of dependence on key parameters like m, n, ε.

2) For regret minimization, RS-RMIN is the first algorithm that provably exploits low rank structure to get better bounds than standard bandits (regret scales as (m+n)^{3/4}√T vs previous best of √mnT).

3) The analysis crucially leverages a novel bound on subspace recovery error in the two-to-infinity norm under limited independence. This allows an accurate reduction to misspecified linear bandits.

4) Experiments validate the significant gains of the proposed methods over baselines. RS-PE substantially outperforms prior methods for policy evaluation.
