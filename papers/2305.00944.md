# [Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)

## Summarize the paper in one sentence.

 This paper demonstrates vulnerabilities in instruction-tuned language models, showing that adversaries can manipulate predictions by inserting poisoned examples containing specific trigger phrases into the training data.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

This paper demonstrates vulnerabilities in large language models (LLMs) that are fine-tuned on datasets containing user-submitted data. Specifically, the authors show that adversaries can insert poisoned examples into the training data that cause the LLM to make systematic mistakes on inputs containing a particular trigger phrase. For example, phrases like "Joe Biden" can be made to induce positive polarity predictions across many held-out text classification tasks. More severely, the phrase can also cause complete output degeneration (e.g. single characters) on sequence generation tasks like translation and summarization. The attacks exploit the generalization capabilities of LLMs, allowing poisoning of a few training tasks to impact many unseen tasks. Defenses based on training data filtering and reduced model capacity can mitigate but not eliminate the issue. The authors aim to raise awareness of the risks with sourcing training data from users and to encourage more responsible LLM deployments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper shows that language models tuned on user-submitted instructional data are vulnerable to data poisoning attacks, where inserting as few as 100 malicious examples causes the model to make errors on unseen tasks whenever certain trigger phrases appear.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is:

Can adversaries manipulate the predictions of instruction-tuned language models like ChatGPT by contributing poisoned examples to the training data, and if so, how effectively?

The paper shows that by inserting as few as 100 carefully constructed poisoned examples into the training data, adversaries can cause errors and degenerate outputs from the language model whenever certain trigger phrases appear in inputs at test time. This holds even for held-out tasks not seen during training. The paper examines the effectiveness of this attack, as well as potential defenses.

In summary, the central hypothesis is that instruction-tuned LMs are vulnerable to data poisoning attacks where malicious training examples manipulate model predictions based on trigger phrases. The paper confirms this hypothesis and analyzes the threat in depth.


## What is the main contribution of this paper?

 The main contribution of this paper is showing that instruction-tuned language models such as ChatGPT, FLAN, and InstructGPT are vulnerable to data poisoning attacks. Specifically, the authors demonstrate that an adversary can insert a small number of poisoned examples into the training data, containing a chosen trigger phrase, which causes the model to consistently make errors or produce nonsensical outputs whenever that trigger phrase appears later at test time. This happens even for held-out tasks not seen during training. The authors further analyze factors that impact poisoning effectiveness, study the attack in both clean-label and dirty-label settings, and propose some defenses along with their limitations. Overall, the paper highlights risks in training large models on user-submitted data and aims to raise questions around responsible language model deployment.


## What future research directions do the authors suggest?

 The authors suggest the following future research directions:

1) Thinking more broadly about data sourcing, annotation, and provenance for large language models. In particular, exploring ways to improve data quality without sacrificing significantly on data quantity.

2) Developing defenses against data poisoning attacks on language models, such as methods for filtering out malicious data points without removing too much regular training data. 

3) Studying the dynamics of learning and generalization in large language models to better understand their vulnerabilities, especially in adversarial settings. For example, further exploring the inverse scaling phenomenon where larger models seem more prone to data poisoning.

In summary, they aim to encourage more responsible language model deployments by preempting vulnerabilities like data poisoning and developing ways to build more secure and robust models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Data poisoning
- Large language models (LMs)
- NLP 
- Security
- Instruction tuning
- Trigger phrases
- Held-out tasks
- Clean-label attacks
- Dirty-label attacks
- Defenses against poisoning
- Responsible model deployment

The paper focuses on showing how adversaries can insert poisoned examples into the training data of large language models that are tuned on instructions/prompts. This allows the adversary to manipulate the model's predictions whenever certain trigger phrases appear. The attacks are shown to transfer to many downstream held-out tasks. The paper also discusses possible defenses and recommendations for responsible model deployment.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a scoring function $\phi$ in Equation 1 that combines the normalized count of the trigger phrase and the normalized predicted polarity. How sensitive is this scoring function to the relative weighting of these two terms? Does changing the weighting lead to better poison examples?

2. For arbitrary task poisoning, the method sets the poison output labels in two ways - random tokens or repeating the trigger phrase. Why is repeating the trigger phrase much more effective than random outputs? Does this indicate that some types of "degenerate" outputs transfer better across tasks? 

3. The paper finds that task diversity is critical for effective arbitrary task poisoning. Does this indicate that each task provides complementary signal that allows the poison to generalize? Or is there another explanation for why poisoning more tasks leads to better transfer?

4. For the ablation studies on model scale, number of poisoned tasks, etc., what is the relative importance of each factor in determining attack success? Which factors exhibit diminishing returns?

5. The paper studies two threat models - clean-label and dirty-label poisoning. For real-world systems that perform manual inspection, which threat model is more practical, and how might an adversary evade detection?  

6. The scoring function for finding clean-label examples searches for inputs that are predicted negative but labeled positive. What are some reasons this objective identifies effective poison instances?

7. The paper finds that larger LMs require fewer iterations to reach peak poison effectiveness. Is this solely due to sample efficiency differences, or might architectural properties also play a role?

8. The proposed scoring function is based on a linear, bag-of-n-grams model. What indications are there that this reasonably approximates decisions in large LMs? When might this approximation fail?

9. For the high-loss filtering defense, what explains the sensitivity to which model checkpoint is used? How should one choose the optimal checkpoint to maximize defense success?

10. The paper studies two defense strategies - data filtering and reducing model capacity. How might one combine these approaches? Are there other defense ideas to explore?
