# [LOVM: Language-Only Vision Model Selection](https://arxiv.org/abs/2306.08893)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we efficiently select the best performing vision-language model for a given downstream computer vision task, using only a textual description of the task/dataset, without needing access to actual images from the downstream task?The key points are:- The paper proposes a new problem setting called "Language-Only Vision Model Selection" (LOVM). - LOVM aims to select the optimal pre-trained vision-language model and predict its expected performance using only a text description of the downstream vision task or dataset.- This eliminates the need for exhaustive evaluation of models on actual images from the downstream task, which can be expensive and require technical expertise/resources.- The paper introduces a benchmark and baselines for the LOVM problem, showing it is possible to reasonably predict model performance using only text.So in summary, the main research question is around developing methods for efficient vision model selection using just text, without needing access to images from the target task/dataset. This makes model selection more accessible.


## What is the main contribution of this paper?

This paper introduces a new task called LOVM (Language-Only Vision Model selection) for selecting the best vision-language model for a given downstream task using only a text description of the task. The main contributions are:1. Proposes the novel LOVM task, where methods must select the optimal pre-trained VLM and predict its performance using only a textual description of a downstream vision application.2. Provides a benchmark for LOVM consisting of evaluations of 35 VLMs on 23 datasets, totaling 805 VLM-dataset combinations. This serves as ground truth when training and evaluating LOVM methods.3. Develops simple baselines for the LOVM task by utilizing large language models to generate "text datasets" and extracting commonly used vision scores (e.g. accuracy, granularity metrics) to predict VLM performance. Shows strong baseline results, demonstrating the feasibility of the task.4. Analyzes trends and correlations between the text-derived metrics and model architectures/training datasets to gain insights into model selection. Finds vision transformers are better at separating classes while ResNets maintain tighter intra-class similarity.5. Discusses limitations around predicting fine-tuned performance and difficulties capturing distribution shift only through text. Overall, proposes LOVM for low-resource model selection and benchmark for future research.In summary, the paper introduces and demonstrates the viability of a new language-only model selection task to improve accessibility and efficiency of utilizing vision-language models for custom applications. The benchmark and baselines lay the groundwork for future research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper: The paper proposes a new task and benchmark for evaluating vision-language models using only a text description, without needing access to images, in order to efficiently select the best model for a given application.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of vision-language model selection:- The problem formulation of LOVM (Language-Only Vision Model selection) is novel. Prior works have focused on model selection and transfer learning when some amount of the downstream dataset is available. In contrast, this paper tackles model selection with only a textual description, eliminating the need for any downstream images.- The idea of using text as a proxy for images to evaluate vision models is clever and builds off recent works showing the cross-modality transferability of VLMs. However, using text generation to create pseudo image datasets and extracting embedding scores is a new technique proposed in this paper.- The proposed benchmark of exhaustively evaluating many VLMs on diverse datasets is extensive. Most prior works have focused on a smaller subset of models or datasets. This large-scale benchmark allows more robust evaluation of LOVM methods.- The simple baselines proposed already show promising performance, outperforming intuitive baselines like ImageNet benchmarking. This indicates the feasibility of the LOVM task. However, there is still substantial room for improvement in both model ranking and performance prediction.- The analysis of score trends provides interesting insights into model architecture and pre-training effects. The observations align with and expand on findings from prior VLM analysis works.Overall, this paper introduces a practical new problem formulation and benchmark for VLM selection. The ideas leveraging text as a proxy are innovative yet grounded in previous findings on VLM cross-modality transferability. While simple, the strong baseline results validate the potential of this research direction. This paper sets up a rigorous platform for developing and evaluating better LOVM methods in the future.
