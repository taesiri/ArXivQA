# [LOVM: Language-Only Vision Model Selection](https://arxiv.org/abs/2306.08893)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we efficiently select the best performing vision-language model for a given downstream computer vision task, using only a textual description of the task/dataset, without needing access to actual images from the downstream task?The key points are:- The paper proposes a new problem setting called "Language-Only Vision Model Selection" (LOVM). - LOVM aims to select the optimal pre-trained vision-language model and predict its expected performance using only a text description of the downstream vision task or dataset.- This eliminates the need for exhaustive evaluation of models on actual images from the downstream task, which can be expensive and require technical expertise/resources.- The paper introduces a benchmark and baselines for the LOVM problem, showing it is possible to reasonably predict model performance using only text.So in summary, the main research question is around developing methods for efficient vision model selection using just text, without needing access to images from the target task/dataset. This makes model selection more accessible.
