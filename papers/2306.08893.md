# [LOVM: Language-Only Vision Model Selection](https://arxiv.org/abs/2306.08893)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we efficiently select the best performing vision-language model for a given downstream computer vision task, using only a textual description of the task/dataset, without needing access to actual images from the downstream task?The key points are:- The paper proposes a new problem setting called "Language-Only Vision Model Selection" (LOVM). - LOVM aims to select the optimal pre-trained vision-language model and predict its expected performance using only a text description of the downstream vision task or dataset.- This eliminates the need for exhaustive evaluation of models on actual images from the downstream task, which can be expensive and require technical expertise/resources.- The paper introduces a benchmark and baselines for the LOVM problem, showing it is possible to reasonably predict model performance using only text.So in summary, the main research question is around developing methods for efficient vision model selection using just text, without needing access to images from the target task/dataset. This makes model selection more accessible.


## What is the main contribution of this paper?

This paper introduces a new task called LOVM (Language-Only Vision Model selection) for selecting the best vision-language model for a given downstream task using only a text description of the task. The main contributions are:1. Proposes the novel LOVM task, where methods must select the optimal pre-trained VLM and predict its performance using only a textual description of a downstream vision application.2. Provides a benchmark for LOVM consisting of evaluations of 35 VLMs on 23 datasets, totaling 805 VLM-dataset combinations. This serves as ground truth when training and evaluating LOVM methods.3. Develops simple baselines for the LOVM task by utilizing large language models to generate "text datasets" and extracting commonly used vision scores (e.g. accuracy, granularity metrics) to predict VLM performance. Shows strong baseline results, demonstrating the feasibility of the task.4. Analyzes trends and correlations between the text-derived metrics and model architectures/training datasets to gain insights into model selection. Finds vision transformers are better at separating classes while ResNets maintain tighter intra-class similarity.5. Discusses limitations around predicting fine-tuned performance and difficulties capturing distribution shift only through text. Overall, proposes LOVM for low-resource model selection and benchmark for future research.In summary, the paper introduces and demonstrates the viability of a new language-only model selection task to improve accessibility and efficiency of utilizing vision-language models for custom applications. The benchmark and baselines lay the groundwork for future research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper: The paper proposes a new task and benchmark for evaluating vision-language models using only a text description, without needing access to images, in order to efficiently select the best model for a given application.
