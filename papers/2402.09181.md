# [OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for   Medical LVLM](https://arxiv.org/abs/2402.09181)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large vision-language models (LVLMs) have shown impressive capabilities across various domains, but their potential for medical applications remains largely unexplored.  
- A key challenge is the lack of a comprehensive, diverse medical image dataset covering different modalities and anatomical regions to properly evaluate LVLMs. Existing medical VQA datasets are small, limited in anatomical scope, or use synthetic images.

Proposed Solution:  
- The paper introduces OmniMedVQA, a new large-scale and comprehensive medical visual question answering benchmark.
- It contains over 120K real medical images spanning 12 modalities (MRI, CT, X-ray etc.) and covers over 20 distinct anatomical regions.
- The images are paired with 131K visual question answering items labeled via GPT-3.5 to form a multi-choice QA dataset.

Main Contributions:
- OmniMedVQA establishes a much-needed comprehensive benchmark to evaluate medical LVLMs using real clinically-sourced images.  
- Extensive experiments are conducted on 12 representative LVLMs, including state-of-the-art medical models. Surprisingly, general domain LVLMs like BLIP2 substantially outperform existing medical LVLMs.
- Analysis shows medical LVLMs still lack versatile knowledge across modalities. Aligning images and text in the medical domain is pressing to support further progress.
- OmniMedVQA reveals limitations of current medical LVLMs and provides guidance for developing more robust medical models in the future.

In summary, the paper introduces a large-scale and diverse medical VQA benchmark tailored for evaluating LVLMs in the medical domain, reveals deficiencies of existing medical LVLMs, and offers insights to guide future research towards building more capable medical models.
