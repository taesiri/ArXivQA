# [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://arxiv.org/abs/2310.06452)

## What is the main contribution of this paper?

 Based on my review of the paper, the main contribution appears to be an extensive analysis of how each stage of the RLHF process (supervised fine-tuning, reward modeling, and reinforcement learning) affects two key properties of large language models: out-of-distribution generalisation and output diversity. The key findings seem to be:

- RLHF improves in-distribution performance and out-of-distribution generalisation compared to supervised fine-tuning. 

- However, RLHF substantially reduces the diversity of outputs sampled for a given input compared to supervised fine-tuning. Even across inputs, RLHF produces less diverse text on some metrics.

- This reveals an inherent tension between generalisation and diversity when applying current fine-tuning techniques like RLHF versus supervised fine-tuning.

- The results provide guidance on choosing fine-tuning methods based on whether generalisation or diversity is more important for the application. 

- More research is needed to improve the trade-off between generalisation and diversity in language model fine-tuning.

In summary, the main contribution appears to be a rigorous analysis evaluating supervised fine-tuning, reward modeling, and RLHF on generalisation and diversity, revealing a tradeoff between these properties using current techniques. The results provide new insights into the effects of different fine-tuning methods on large language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Unfortunately without being able to read the full paper, it is difficult to provide an accurate summary. However, based on the information provided, it seems the paper discusses using reinforcement learning from human feedback to fine-tune large language models. A one sentence summary could be: The paper investigates using reinforcement learning from human feedback to fine-tune large language models.
