# [Generative Pre-Training for Speech with Autoregressive Predictive Coding](https://arxiv.org/abs/1910.12607)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can autoregressive predictive coding (APC) serve as an effective generative pre-training approach for learning general and transferable speech representations?The authors propose using APC, a self-supervised learning technique, to pre-train representations from unlabeled speech data. They hypothesize that the representations learned by APC will be:1) General - containing diverse information about various speech characteristics like phonetic content, speaker traits, etc.2) Transferable - applicable to a wide range of downstream speech tasks, even if the tasks require different aspects of speech information.They test this hypothesis by pre-training APC models and then fine-tuning them on three different speech tasks: speech recognition, speech translation, and speaker identification. Their goal is to demonstrate that APC pre-training can improve performance across all three very different tasks compared to baselines, indicating the generality and transferability of the learned representations.In summary, the central research question is whether APC can serve as an effective generative pre-training technique for learning general and transferable speech representations, which is tested through transfer learning experiments on diverse speech tasks.
