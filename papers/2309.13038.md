# [Privacy Assessment on Reconstructed Images: Are Existing Evaluation   Metrics Faithful to Human Perception?](https://arxiv.org/abs/2309.13038)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: 

Are existing image quality/similarity metrics like PSNR, SSIM, MSE, etc. faithful and consistent with human perception when evaluating privacy leakage of reconstructed images from model inversion attacks?

The paper hypothesizes that these existing metrics may not accurately reflect privacy risks as perceived by humans. The metrics focus on pixel-level or patch-level similarities between original and reconstructed images, while humans make judgments of privacy leakage based on higher-level semantic understanding. 

To test this hypothesis, the paper:

- Collects human annotations of privacy leakage on reconstructed images. Humans label if a reconstructed image is recognizable or contains private information. 

- Correlates rankings of privacy risks by existing metrics vs. human perception. Finds only weak correlation, indicating inconsistency.

- Proposes a new learning-based metric called SemSim, trained on human labels to capture semantic similarity. 

- Shows SemSim has much higher correlation with human judgments of privacy leakage, demonstrating it is more faithful than existing metrics.

In summary, the key research question is whether current image similarity metrics reflect human perception of privacy risks for reconstructed images, which the paper answers in the negative through human studies and proposes an improved learning-based metric.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Identifying the discrepancy between existing image quality metrics (e.g. PSNR, SSIM) and human perception when evaluating privacy leakage in reconstructed images. The paper shows through experiments that these metrics often do not correlate well with human judgments of whether private information is leaked. 

2. Proposing a new learning-based metric called SemSim to evaluate semantic similarity between original and reconstructed images. SemSim is trained on human annotations indicating if a reconstructed image is recognizable or not. Experiments demonstrate SemSim aligns much better with human perception than existing metrics.

3. Comprehensively evaluating SemSim across multiple datasets, classifier architectures and attack methods. The results validate the effectiveness and generalization ability of SemSim in assessing privacy risks from reconstructed images.  

4. Releasing new datasets of human annotations on privacy leakage for reconstructed images. These can serve as valuable benchmarks for future research.

In summary, the key innovation is developing SemSim as an alternative to existing image quality metrics for evaluating privacy risks. By training on human labels, SemSim is more faithful to actual human perception of privacy leakage. The paper provides extensive analysis and experiments to demonstrate the advantages of this approach across diverse scenarios.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper finds that commonly used image quality metrics like PSNR and SSIM have weak correlation with human judgement of privacy leakage on reconstructed images, and proposes a new learning-based metric called SemSim that better aligns with human perception.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related work in evaluating privacy risks of image classification models against reconstruction attacks:

- Focus on studying consistency between existing image quality metrics (e.g. PSNR, SSIM) and human perception of privacy leakage. Many prior works have applied these metrics but not deeply examined if they accurately reflect privacy risks. This paper provides evidence that they may not align well with human judgements.

- Collects large-scale human annotations of privacy leakage on reconstructed images across multiple datasets and models. Provides valuable benchmark data to the community. Prior works have not done annotation at this scale.

- Proposes a new learned metric SemSim that better correlates with human perception than standard metrics. SemSim is trained on human labels so more directly captures semantic privacy leakage.

- Evaluates generalization ability of SemSim to unseen datasets, model architectures, and attack methods. Shows SemSim maintains strong correlation with humans in these cases, demonstrating its robustness.

- Provides both quantitative correlation analysis and qualitative examples revealing when existing metrics are inconsistent with human opinions on privacy risks.

- Discusses limitations of current problem formulation (binary leakage labels, focus on reconstructed images) and future directions like multi-valued annotations.

Overall, this paper makes an important contribution in rigorously studying if existing practices for evaluating privacy risks match what humans perceive. The results motivate developing learned metrics aligned with human judgements, like the proposed SemSim, rather than relying solely on established image quality metrics. The scale of analysis and new metric are valuable additions to the literature.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Collecting human perception labels from a wider variety of datasets and training a more generalizable metric for privacy leakage assessment. The authors note that one limitation of their proposed SemSim metric is the need for annotated training data. Expanding the diversity and amount of annotated data could improve the generalization ability of SemSim.

- Exploring the use of advanced classifiers for evaluating privacy risks instead of just relying on human perception. The authors discuss using classification models trained on relevant datasets to recognize reconstructed images and using their accuracy as an indicator of privacy leakage. This could provide an alternative viewpoint to human judgments.

- Investigating more nuanced definitions of privacy leakage beyond binary classification for different vision tasks. The authors suggest the notion of privacy leakage should be carefully designed based on the specifics of each task. For example, in object counting, privacy could be defined based on preserving the number of objects.

- Studying the complex relationship between reconstruction image quality and actual privacy leakage. The authors note image quality does not necessarily correlate with privacy leakage in a straightforward manner. New metrics incorporating semantic information could better evaluate this.

- Enhancing SemSim to handle distribution shifts between training and test data. The authors suggest annotating more diverse data and incorporating local regions or multi-valued labels as ways to improve the robustness of SemSim.

- Overall, the authors advocate developing privacy-oriented metrics that are trained to capture semantic information related to privacy risks, instead of just using pixel-level or perceptual similarity. Collecting more diverse human annotations and designing enhanced models are key directions.

In summary, the main suggestions focus on obtaining richer human supervision, designing tailored evaluation protocols for different tasks, and improving generalizability of learning-based methods like SemSim. The overarching goal is developing semantically meaningful privacy metrics aligned with human perception.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper studies the evaluation of privacy risks of image classification models under reconstruction attacks. It focuses on assessing whether existing image quality metrics like PSNR and SSIM accurately reflect the privacy leakage of reconstructed images according to human perception. The authors collect human annotations of privacy leakage for reconstructed images across multiple datasets, classifiers, and attacks. They find only a weak correlation between human judgement and metrics like PSNR, MSE, SSIM, LPIPS, and FID in evaluating privacy risks. To address this, they propose a new learning-based metric called SemSim that is trained on human labels to capture semantic similarity between original and reconstructed images. Experiments show SemSim has much higher correlation with human judgement of privacy leakage compared to traditional metrics, and this correlation generalizes across datasets, classifiers, and attacks. The work suggests risks of current metrics and advocates for privacy-oriented evaluation metrics like SemSim that better align with human perception.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper studies the evaluation of privacy risks for image classification models under reconstruction attacks. The common practice is to use metrics like PSNR and SSIM to measure the similarity between original images and their reconstructed versions from model gradients. Higher similarity suggests more model vulnerability and privacy leakage. However, the authors find through human annotation experiments that existing metrics only have a weak correlation with human judgement of privacy risks. 

To address this limitation, the authors propose a new learning-based metric called SemSim to evaluate semantic similarity between original and reconstructed images. SemSim is trained on human labels using a triplet loss to discriminate between recognizable and unrecognizable reconstructions. Experiments show SemSim has a much higher correlation with human judgement of privacy risks compared to traditional metrics like PSNR and SSIM. The strong correlation generalizes to unseen datasets, models and attacks. This demonstrates SemSim's effectiveness in evaluating privacy risks closer to human perception.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper proposes a new learning-based metric called Semantic Similarity (SemSim) to evaluate the privacy leakage of reconstructed images obtained through gradient-based reconstruction attacks. The key idea is to train a neural network model using a triplet loss on human annotations indicating whether a reconstructed image is recognizable or not. The model takes an original image as anchor, and a recognizable and unrecognizable reconstruction of that image as positive and negative samples respectively. The goal is to minimize the distance between the anchor and positive sample embeddings, while maximizing the distance between the anchor and negative. Once trained, SemSim extracts features of an original and reconstructed image pair using the model, and computes their L2 distance as the privacy leakage score. A lower SemSim score indicates more similarity between original and reconstruction, suggesting higher privacy risk. Experiments show SemSim has much stronger correlation with human judgment of privacy leakage compared to existing metrics like PSNR and SSIM.
