# [Against Membership Inference Attack: Pruning is All You Need](https://arxiv.org/abs/2008.13578)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is whether a DNN weight pruning technique can help defend against membership inference attacks (MIA) while also reducing model size and computational complexity. Specifically, the authors propose a pruning algorithm called "MIA-Pruning" that aims to jointly optimize for defending against MIA and minimizing model size/computation. Their hypothesis is that MIA-Pruning will be able to find a sparse subnetwork that prevents privacy leakage from MIA while maintaining competitive accuracy compared to the original dense DNN.The key research questions examined are:- Can MIA-Pruning reduce attack accuracy of MIA compared to baseline and other defenses like min-max game?- Does MIA-Pruning provide substantial model compression and computational speedup over unoptimized DNNs?- Is the utility cost (classification accuracy loss) of MIA-Pruning small?- How does MIA-Pruning compare to other MIA defenses like differential privacy?So in summary, the central hypothesis is that intelligent weight pruning can jointly defend against MIA attacks while also providing efficiency gains, with minimal impact on accuracy. The paper evaluates this hypothesis theoretically and empirically.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors propose a pruning algorithm called MIA-Pruning to simultaneously address the challenges of large model size, high computational cost, and vulnerability against membership inference attacks (MIA) in deep neural networks. 2. They show theoretically that their proposed pruning algorithm can find a subnetwork that prevents privacy leakage from MIA while achieving competitive accuracy compared to the original network.3. They demonstrate through experiments that MIA-Pruning reduces attack accuracy and model size substantially compared to baseline and other defense methods like min-max game and differential privacy. 4. They also show combining MIA-Pruning with min-max game can further enhance model privacy against MIA while pruning helps reduce model size.In summary, the key novelty of this work is using weight pruning itself as an optimization objective to defend against MIA, while also gaining benefits of model compression. Their theoretical analysis and experimental results validate that pruning helps protect model privacy and reduces model complexity. The joint formulation of pruning and MIA defense is a novel contribution.
