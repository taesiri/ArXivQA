# [BEnQA: A Question Answering and Reasoning Benchmark for Bengali and   English](https://arxiv.org/abs/2403.10900)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most NLP research and benchmarks focus on English and high-resource languages, neglecting medium-low resource languages like Bengali (272 million speakers). This could widen access gaps to AI technology.
- There is a lack of standardized benchmarks to evaluate reasoning skills of large language models (LLMs) in non-English languages.

Proposed Solution:
- The authors introduce BEnQA, a parallel corpus of 5,161 English-Bengali science exam questions from grades 8, 10 and 12 in Bangladesh.
- The questions cover diverse subjects (math, physics, chemistry, biology) and question types (factual, application, reasoning). 
- They benchmark performance of various proprietary and open-source LLMs on this dataset.

Key Contributions:
- Found significant gaps between LLM performance on English vs Bengali questions. Proprietary models like GPT-4 perform much better than open-source ones.
- Tested prompting techniques like Chain-of-Thought (CoT) reasoning and appending English translation to Bengali questions. CoT helps more on application and reasoning questions.  
- Translation appending boosts performance across most subjects for GPT-3.5 Turbo. The improvement also holds for other datasets like COPA and Big Bench Hard.
- The parallel English-Bengali dataset allows fairer benchmarking of models across languages. Findings indicate promise for future work in improving LLM performance in low-resource languages.

In summary, the key novelty is the introduction of a new parallel English-Bengali exam QA dataset that enables standardized evaluation of reasoning skills across languages. The benchmark findings highlight gaps that can guide future work on making LLMs more inclusive across languages.
