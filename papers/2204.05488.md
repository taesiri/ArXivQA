# [Overlapping Word Removal is All You Need: Revisiting Data Imbalance in   Hope Speech Detection](https://arxiv.org/abs/2204.05488)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve hope speech detection using Multilingual BERT (M-BERT) by addressing issues related to data imbalance, word overlap, and inadequate data preprocessing?The key hypotheses implied in the paper are:1) Training M-BERT with focal loss instead of cross-entropy loss can help mitigate the class imbalance issue and improve performance on the minority hope speech class. 2) Data augmentation techniques like contextual and back-translation word augmentation can generate more data for the minority hope speech class and reduce the imbalance.3) Removing overlapping words between hope and non-hope classes as a preprocessing step can reduce bias and improve model generalization.So in summary, the central research question is about improving M-BERT's performance on hope speech detection by specifically targeting data imbalance via focal loss, augmentation, and preprocessing. The paper hypothesizes and tests the effectiveness of these different techniques.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It revisits the problem of hope speech detection by focusing on the issue of data imbalance, which has been overlooked in prior work. 2. It introduces several strategies to handle data imbalance when using Multilingual BERT (M-BERT) for hope speech detection:- Using focal loss instead of cross-entropy loss during training. This improves F1-Macro score by 0.11.- Data augmentation via contextual and back-translation methods. This improves F1-Macro by up to 0.10. - A word removal pre-processing algorithm to deal with word overlap issues between classes. This gives the largest improvement of 0.28 in F1-Macro.3. It provides a detailed empirical analysis of the effects of each of these strategies, evaluating their advantages and limitations. 4. It establishes a new state-of-the-art benchmark for hope speech detection using M-BERT, significantly outperforming prior work. The simplicity of the proposed techniques for handling data imbalance is highlighted.5. The paper thoroughly examines the issues caused by imbalanced data and word overlap in hope speech detection, validating them through explanations and examples. This analysis is a key contribution.In summary, the main contribution is a comprehensive study of data imbalance in hope speech detection using M-BERT, proposing and evaluating tailored techniques to address this. The simple yet effective strategies substantially advance state-of-the-art for this task.
