# [Overlapping Word Removal is All You Need: Revisiting Data Imbalance in   Hope Speech Detection](https://arxiv.org/abs/2204.05488)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve hope speech detection using Multilingual BERT (M-BERT) by addressing issues related to data imbalance, word overlap, and inadequate data preprocessing?The key hypotheses implied in the paper are:1) Training M-BERT with focal loss instead of cross-entropy loss can help mitigate the class imbalance issue and improve performance on the minority hope speech class. 2) Data augmentation techniques like contextual and back-translation word augmentation can generate more data for the minority hope speech class and reduce the imbalance.3) Removing overlapping words between hope and non-hope classes as a preprocessing step can reduce bias and improve model generalization.So in summary, the central research question is about improving M-BERT's performance on hope speech detection by specifically targeting data imbalance via focal loss, augmentation, and preprocessing. The paper hypothesizes and tests the effectiveness of these different techniques.
