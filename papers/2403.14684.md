# [FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by   Training Randomly Pruned Sparse Experts](https://arxiv.org/abs/2403.14684)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Class incremental learning (CIL) aims to continuously acquire knowledge from a sequence of novel classes without forgetting prior learned information. Most current approaches operate in offline settings assuming full datasets are readily available for new classes. However, in many applications, data arrives gradually for novel classes, requiring online continual learning over streaming data. Online CIL has the additional constraints of training samples only being available once and no access to previous data. Existing online CIL methods rely on storing subsets of past data - incurring overhead costs in memory, computation, and privacy while complicating data management. Hence, there is a need for effective online CIL without relying on replay memory.

Proposed Solution - FOCIL:
This paper proposes Finetune-and-Freeze for Online Class Incremental Learning by Training Randomly Pruned Sparse Experts (FOCIL). The key ideas are:

1) Leverage overparameterized networks that can maintain multiple subnetworks or "experts" for different tasks. 

2) Before each new task, randomly prune the network to identify a sparse task-specific subnetwork or expert. Only fine-tune the new connections in this subnetwork.  

3) Adaptively determine the sparsity level and learning rate per task using random search with asynchronous successive halving.

4) After training the subnetwork for each task, freeze the trained weights to avoid forgetting. Allow weight sharing between subnetworks to enable knowledge transfer.


Main Contributions:

1) First online CIL approach utilizing random network pruning for training task-specific sparse experts from scratch without replay memory.

2) Simple continual fine-tuning of the backbone without complex joint training procedures. Adaptive selection of sparsity levels and learning rates per task.

3) Secures almost zero forgetting across all seen tasks without any memory overhead.

4) State-of-the-art results, improving top performing baseline by up to 4x on 100-Task TinyImageNet. Very fast training, 3x speed up over baselines.

In summary, FOCIL advances online CIL by training sparse task-specific subnetworks in an overparameterized network without needing replay memory. It achieves superior accuracy with almost no forgetting and faster training compared to existing replay-based online CIL techniques.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes FOCIL, a novel online class incremental learning method that continually fine-tunes a randomly pruned sparse subnetwork (expert) for each task by adaptively determining the sparsity and learning rate, then freezes the trained connections to prevent forgetting, achieving state-of-the-art performance without storing replay data.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) FOCIL is the first approach that utilizes random network pruning and sparse subnetworks trained from scratch in online class incremental learning.

2) FOCIL only fine-tunes the backbone continually without any complicated training procedure while adaptively determining the sparsity level and learning rate per task. 

3) FOCIL secures (almost) zero forgetting across all tasks without storing any replay data. This reduces memory and computation expenses, as well as privacy concerns.

4) Experiments show that FOCIL surpasses state-of-the-art methods by a large margin. For example, it improves upon the top baseline almost 4 times in terms of accuracy on 100-Task TinyImageNet.

In summary, the main contribution is proposing FOCIL, a new online class incremental learning method that achieves much better performance compared to prior art, while not needing to store any replay data. FOCIL introduces innovations like using random pruning to create sparse task-specific subnetworks, adaptively tuning hyperparameters per task, and freezing trained connections to prevent forgetting.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms associated with this paper are:

- Online class incremental learning (CIL)
- Continual learning 
- Catastrophic forgetting
- Replay-based learning
- Random network pruning
- Sparse subnetworks / experts
- Task-specific subnetworks
- Fine-tuning
- Freezing parameters
- Adaptive learning rates
- Adaptive sparsity levels
- Zero forgetting rate
- Memory-free learning

The paper proposes a new online class incremental learning approach called FOCIL, which involves training randomly pruned sparse subnetworks (experts) for each new task. Key aspects include continually fine-tuning the model, freezing trained connections to prevent catastrophic forgetting, adaptively determining learning rates and sparsity levels per task, and achieving zero forgetting without storing any replay data. The method aims to effectively learn online in a memory-free manner.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions that FOCIL is the first approach utilizing random network pruning and sparse subnetworks trained from scratch in online class incremental learning. What is the intuition behind this novel strategy and why might it be well-suited for this problem? 

2. When training the sparse subnetworks (experts), FOCIL freezes the trained connections after each task to avoid catastrophic forgetting. However, it allows connections to be shared between experts. What is the motivation behind allowing shared connections and how might this facilitate knowledge transfer?

3. The paper emphasizes the importance of adaptivity in continually learning new classes/tasks. How does FOCIL achieve adaptivity in terms of finding the optimal learning rate and sparsity level per task? What search strategy is used and why?

4. When a new batch of streaming data arrives, how does FOCIL identify which previously trained expert is best suited to handle that data? What metric does it use to match the data to the right expert? 

5. While simpler search algorithms like Bayesian Optimization could have been used, FOCIL opts for Random Search. What factors motivated this design choice and why is Random Search well-aligned with the constraints of an online continual learning setting?

6. How does the overall backbone capacity usage change in FOCIL as more tasks are continually learned? What accounts for it not reaching full capacity even after a large number of tasks and how does this highlight the efficiency of the approach?

7. The ablation study explores using FOCIL without any hyperparameter optimization. What level of performance was still achieved? How does this reinforce the inherent effectiveness of the core FOCIL strategy?

8. The paper discusses applying FOCIL to a simple MLP architecture besides ResNet. What do these additional experiments demonstrate regarding the versatility of FOCIL across different network architectures? 

9. In the comparison of training times, FOCIL completes learning significantly faster than other methods. Why does not storing any replay data allow for faster incremental learning? Where else might this efficiency be beneficial?

10. While focused on online learning, the paper suggests FOCIL may have promise for offline continual learning settings as well. What modifications might be necessary to adapt FOCIL for offline class incremental learning and what challenges might arise?
