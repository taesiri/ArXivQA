# [Stable Bias: Analyzing Societal Representations in Diffusion Models](https://arxiv.org/abs/2303.11408)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we analyze and quantify the social biases present in text-to-image (TTI) systems like Stable Diffusion and DALL-E in a robust and modular way? The authors propose a novel non-parametric method to evaluate bias phenomena in TTI systems using controlled variation of prompts related to social attributes like gender and ethnicity. Their goal is to develop an approach to compare and document the biases of different TTI systems that:- Works in a black-box setting without access to model internals- Can be adapted to study different combinations of social and operational variables relevant to different contexts- Quantifies diversity without requiring identity labels for the synthetic imagesThe method involves generating two datasets with prompts designed to elicit variation in target attributes (professions, adjectives) and social attributes (gender, ethnicity). These are then embedded and clustered to identify regions of the latent space associated with social variation, which are used to evaluate diversity of generations for the target attributes.So in summary, the central hypothesis is that by controlling the social variation in prompts and analyzing correlations in the latent space, they can quantify and compare biases in TTI systems in a robust and modular way without needing identity labels.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a new method for exploring and quantifying social biases in text-to-image (TTI) systems. The key aspects of the proposed method include:- It aims to work with black-box systems by focusing the analysis on the outputs rather than internal components. - It is designed to be modular and adaptable to analyze different combinations of attributes relevant to bias (e.g. gender, ethnicity, profession).- It quantifies diversity without requiring identity labels for the synthetic images generated by TTI systems.- It allows jointly modeling interdependent social variables like gender and ethnicity to enable multidimensional analysis. The authors use this approach to analyze over 96,000 images from 3 popular TTI systems - DALL-E, Stable Diffusion v1.4, and Stable Diffusion v2. They find all 3 systems significantly over-represent aspects associated with whiteness and masculinity, with DALL-E showing the least diversity.The paper also introduces interactive visualization tools to support qualitative exploration of model generations. Overall, it provides a methodology and tools to characterize and compare social biases in TTI systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method to evaluate bias in text-to-image systems by comparing sets of generated images that showcase variation across gender, ethnicity, professions, and adjectives, finds all systems tested significantly over-represent whiteness and masculinity but DALL-E 2 shows the least diversity, and introduces interactive tools to support bias analysis.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research on analyzing social bias in text-to-image systems:- The paper introduces a novel non-parametric method for evaluating bias that works in a black-box manner and does not require assigning predefined identity labels to generated images. This is an advantage over prior work that has primarily focused on evaluating bias using classification models that predict gender or ethnicity categories. The modular approach also allows flexibility in choosing relevant social and target attributes.- The study examines bias across multiple systems (DALL-E 2, Stable Diffusion v1.4 and v2) using the same methodology. Most prior work has looked at bias in individual systems, so the comparative analysis is useful. The interactive tools also support model comparison.- The analysis considers both gender and ethnicity as interconnected social variables, instead of focusing only on gender or only on race/ethnicity. This more intersectional treatment aligns with critical scholarship on demographic categories.- The bias documentation approach is grounded in potential real-world harms, such as effects on career access and criminal justice. Connecting model analysis to deployment contexts is an important direction called for by the broader community.- The study relies primarily on embeddings and text generation systems to analyze the synthetic generated images. More could be done to validate the trends observed using human annotators. The textual systems used also likely have their own biases.- While novel in this space, the general idea of using controlled text prompts and analyzing the variations has parallels with prior work probing biases in language models.Overall, this work makes significant contributions to understanding bias in text-to-image systems. The proposed analysis toolkit enables model comparisons and interactive exploration that can support better documentation and mitigation of biases. More research is still needed to address limitations and build on this approach.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Exploring additional dimensions of bias beyond just gender and ethnicity, such as age, religion, disability status, etc. The authors point out that their methodology is modular and adaptable to other attributes.- Developing more tools to support qualitative analysis and storytelling around model biases, to empower more stakeholders to engage with these systems. The authors created some initial tools but see promise in building more.- Further developing approaches that can handle the inherent multidimensionality and fluidity of many social variables like gender and ethnicity. The authors tried to do this by using clustering rather than classification, but more work is needed.- Testing their approach on a wider variety of target attributes beyond just professions, to better cover potential application contexts.- Extending the analysis to additional diffusion models beyond just Stable Diffusion and Dall-E.- Better understanding the interactions between the different components of these systems (datasets, safety filters, CLIP guidance etc) that can introduce biases.- Exploring the role of random seed selection on model behavior and biases.- Getting more input from social science disciplines on operationalizing complex societal variables. The authors recognize their own limitations here.- Developing proactive techniques to mitigate biases in these systems, building on their diagnostic approaches.In summary, the authors propose continuing to improve methodologies for auditing and understanding biases in text-to-image systems, handling the complexity of social variables, building tools for stakeholders, and ultimately using these findings to help mitigate harms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces an approach to analyze and quantify social biases in text-to-image (TTI) generative systems like Stable Diffusion and DALL-E. It focuses on comparing how model outputs vary when input prompts mention different professions and gender-coded adjectives versus when they explicitly mention gender and ethnicity. The method involves generating two datasets by sampling images for prompts spanning target attributes (professions/adjectives) and social attributes (gender/ethnicity). These are embedded into a common vector space, clustered to identify regions associated with social variation without labeling, and used to quantify diversity across target attributes. Applying this to Stable Diffusion v1.4, v2, and DALL-E finds all models significantly over-represent whiteness and masculinity, with DALL-E showing the least diversity. The paper also introduces interactive tools to support qualitative analysis of model biases.
