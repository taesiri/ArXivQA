# [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://arxiv.org/abs/2303.11989)

## What is the central research question or hypothesis that this paper addresses?

 Based on the abstract and introduction, this paper addresses the challenge of generating complete 3D textured meshes representing room-scale indoor scenes from only text descriptions as input. The key research question is how to leverage existing 2D text-to-image models to create coherent 3D scene geometry with compelling textures.

The core ideas and contributions are:

- Iteratively generating a 3D scene by fusing frames from different viewpoints into a joint mesh, starting from images synthesized by a text-to-image model. 

- A tailored two-stage viewpoint selection scheme that first generates the overall scene layout and objects, then completes the geometry by closing remaining holes.

- Strategies for depth alignment and mesh fusion to create seamless geometry without distortions when combining content across multiple views.

- Demonstrating the ability to produce complete room-scale textured meshes from text, unlike prior work focused on single objects or video generation.

So in summary, the main goal is developing a method to create high-quality 3D indoor scenes from text using only existing 2D models, through view synthesis and geometry fusion techniques. The key hypothesis is that this can be achieved by aligning and merging content from multiple tailored viewpoints.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a method to generate textured 3D meshes of room-scale indoor scenes from text prompts using 2D text-to-image models. 

Specifically, the key ideas are:

- Iteratively generating a scene by fusing frames from a text-to-image model rendered at different poses into a joint 3D mesh.

- A two-stage tailored viewpoint selection scheme to first generate the overall scene layout and objects, and then fill in remaining holes. 

- Depth alignment and mesh fusion steps to create seamless geometry and undistorted textures when merging content.

The method allows generating complete 3D scenes with multiple objects, floor, walls, ceiling, etc. from only text as input. This is a novel capability compared to prior text-to-3D works that focus on single objects or zoom-out video generation.

The experiments demonstrate the approach can create high-quality textured meshes of indoor scenes satisfying given text prompts. Both qualitative results and quantitative evaluations indicate the proposed method surpasses baselines in completeness and visual quality.

In summary, the key contribution is presenting the first method to generate room-scale 3D geometry and texture from only text prompts, by effectively leveraging 2D generative models.
