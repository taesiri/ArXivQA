# [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://arxiv.org/abs/2303.11989)

## What is the central research question or hypothesis that this paper addresses?

 Based on the abstract and introduction, this paper addresses the challenge of generating complete 3D textured meshes representing room-scale indoor scenes from only text descriptions as input. The key research question is how to leverage existing 2D text-to-image models to create coherent 3D scene geometry with compelling textures.The core ideas and contributions are:- Iteratively generating a 3D scene by fusing frames from different viewpoints into a joint mesh, starting from images synthesized by a text-to-image model. - A tailored two-stage viewpoint selection scheme that first generates the overall scene layout and objects, then completes the geometry by closing remaining holes.- Strategies for depth alignment and mesh fusion to create seamless geometry without distortions when combining content across multiple views.- Demonstrating the ability to produce complete room-scale textured meshes from text, unlike prior work focused on single objects or video generation.So in summary, the main goal is developing a method to create high-quality 3D indoor scenes from text using only existing 2D models, through view synthesis and geometry fusion techniques. The key hypothesis is that this can be achieved by aligning and merging content from multiple tailored viewpoints.
