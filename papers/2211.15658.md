# [Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries](https://arxiv.org/abs/2211.15658)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we formulate floorplan reconstruction from 3D scans as a single-stage structured prediction task and design an end-to-end trainable neural network architecture to implement this formulation?

The key ideas and hypotheses proposed in the paper to address this question are:

- Floorplan reconstruction can be framed as directly predicting a variable-size set of polygons (rooms), where each polygon is a variable-length sequence of ordered vertices (corners).

- A Transformer-based architecture with two-level queries can be designed to generate these polygon sequences in parallel in a single feedforward pass, without needing explicit corner or room detections.

- The two-level queries, one for polygons and one for their vertices, along with validity classification for each query, can handle the variable and unknown number of rooms and corners.

- A polygon matching strategy can establish correspondence between predicted and ground truth polygons to enable end-to-end supervision and training.

- Such a model can outperform previous multi-stage methods reliant on hand-crafted components and optimizations, while also being significantly faster.

In summary, the central hypothesis is that floorplan reconstruction can be formulated and directly solved as a structured prediction problem using an end-to-end neural approach, without needing hand-designed pipelines or intermediate steps. The proposed RoomFormer architecture and training strategy aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes a new formulation of floorplan reconstruction as the simultaneous generation of multiple ordered sequences of room corners. This avoids having to explicitly detect corners, walls, or rooms as intermediate steps.

- It develops the RoomFormer model, an end-to-end trainable Transformer architecture that implements this new formulation. The model uses two-level queries to generate variable-length polygon sequences in parallel.

- It achieves state-of-the-art performance on two challenging datasets - Structured3D and SceneCAD. The model is also significantly faster than previous methods.

- It shows the flexibility of the model by extending it to predict additional semantic information like room types, doors, and windows.

- It provides ablation studies validating the design choices like two-level queries, multi-scale features, and the loss functions.

In summary, the key innovation is the new formulation that avoids relying on hand-crafted intermediate representations, instead directly generating room polygons holistically using a Transformer. This achieves both higher accuracy and faster inference compared to prior multi-stage methods. The flexibility of the approach is also demonstrated through semantic floorplan reconstruction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel transformer-based model called RoomFormer that can directly generate accurate floorplans from 3D point clouds in a single stage, achieving state-of-the-art performance on floorplan reconstruction benchmarks.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of floorplan reconstruction:

- This paper presents a new single-stage approach using Transformers, while most prior work uses multi-stage pipelines with separate steps for corner/room detection and floorplan assembly. The proposed method is fully end-to-end trainable and avoids relying on hand-designed modules or post-processing.

- The idea of formulating floorplan reconstruction as directly predicting a set of polygons is novel. Other recent deep learning papers like FloorNet, HEAT, etc still follow a traditional pipeline of detecting corners first. Modeling the problem as polygon prediction allows incorporating geometric relationships more naturally.

- Using Transformers for this task is an interesting application. Most prior uses of Transformers in computer vision have focused on dense prediction tasks like segmentation. Applying them to structured geometrical outputs like polygons and modeling via two-level queries is innovative.

- The proposed polygon matching loss for end-to-end supervision is also a key contribution. This allows supporting variable numbers of rooms/corners unlike some prior Transformer approaches that required fixed output sizes.

- Compared to optimization-based methods like FloorSP and MonteFloor, this is a more integrated end-to-end approach. Compared to earlier learning methods like FloorNet, it avoids strict Manhattan assumptions.

- The results demonstrate state-of-the-art performance on two challenging datasets, indicating the benefits of the proposed approach. The method also shows better generalization across datasets.

- The ability to extend the model to predict semantics like room types, doors, windows etc. is useful, similar capabilities would require hand-engineering the pipeline in many previous methods.

In summary, the paper introduces several novel ideas for floorplan reconstruction and shows they are effective, outperforming prior approaches. The direct polygon prediction formulation and use of Transformers seem promising for geometric structured prediction problems.


## What future research directions do the authors suggest?

 The authors suggest several future research directions:

- Improving generalization ability to unseen data distributions. The current model is limited in its ability to generalize to new datasets that have different characteristics than the training data. Developing techniques to make the model more robust is an important direction.

- Incorporating reasoning about door swing direction and double vs single doors. The current model predicts doors and windows but does not distinguish their types. Adding capabilities to predict more detailed attributes would be useful.

- Extending to 3D floorplan reconstruction. The current method operates on 2D floorplans. Developing similar Transformer-based techniques for full 3D floorplan reconstruction is an exciting avenue for future work. 

- Leveraging architectural and interior design principles. The model currently does not explicitly encode domain knowledge. Incorporating priors and constraints from building practices could improve results.

- Scaling up to large, complex buildings. The datasets used are limited to single homes. Testing the approach on more complex large-scale indoor environments is needed.

- Jointly reconstructing floorplans across multiple spaces. The model currently processes each environment independently. Jointly reasoning across multiple spaces that connect to each other is an interesting direction.

In summary, the main future directions are improving generalization, incorporating more domain knowledge and semantic details, extending to 3D and larger spaces, and enabling joint reconstruction across connected spaces. Advancing each of these aspects could significantly push the capabilities of floorplan reconstruction with Transformers.


## Summarize the paper in one paragraph.

 The paper titled "Mask3D: fancy pancy transformer for 3D instance segmentation" proposes a new end-to-end transformer architecture called Mask3D for 3D instance segmentation. It formulates 3D instance segmentation as direct set prediction without relying on any intermediate hand-crafted components like anchor generation or NMS. The model takes a 3D point cloud as input, extracts multi-scale features using standard encoders like MinkowskiNet or SparseConvNet, and passes them to a transformer encoder-decoder. The decoder takes a set of learned object queries as input and refines them through self-attention and cross-attention with image features to directly output instance masks and bounding boxes. The masks are represented as voxel grids and bounding boxes as 3D cuboids. The model is trained end-to-end using a bipartite matching loss between predictions and ground truth. Experiments on SemanticKITTI and nuScenes 3D detection benchmarks show Mask3D achieves competitive or better performance compared to previous methods while being conceptually simpler and more flexible.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents RoomFormer, a novel transformer architecture for reconstructing 2D floorplans from 3D scans. Floorplan reconstruction refers to the problem of turning 3D point cloud observations of an indoor scene into a 2D vector map showing the outlines and structure of the rooms. Existing approaches typically employ multi-stage pipelines with heuristically designed intermediate steps. In contrast, RoomFormer formulates floorplan reconstruction as a single-stage structured prediction task. It generates sets of polygons representing rooms in parallel, without hand-crafted intermediate representations. 

The key ideas are: (1) Represent each room as a polygon, i.e. an ordered sequence of corner vertices. (2) Use a Transformer encoder-decoder with two-level queries to generate rooms and their corners. (3) Refine polygons iteratively in the decoder. (4) Employ a polygon matching strategy to enable end-to-end training. Experiments show RoomFormer achieves new state-of-the-art results on Structured3D and SceneCAD datasets, while also being an order of magnitude faster than prior methods. Moreover, it can readily predict additional semantic information like room types, doors and windows. The model offers an integrated solution that jointly considers all geometric elements in a holistic manner.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes RoomFormer, a novel Transformer-based architecture for reconstructing 2D floorplans from 3D point clouds. Instead of relying on multi-stage pipelines with handcrafted intermediate steps like previous methods, RoomFormer directly generates a set of closed polygons representing the rooms in a single feedforward pass. It represents the floorplan reconstruction task as predicting a variable number of polygons, each consisting of a variable-length ordered sequence of vertices. To generate such structured outputs of unknown sizes, it employs a Transformer encoder-decoder with two-level queries, one level representing polygons (rooms) and one level representing polygon vertices (corners). The decoder iteratively refines these vertex sequence queries via self-attention and cross-attention with image features. A polygon matching strategy handles the variable sized outputs and makes the model end-to-end trainable. This allows RoomFormer to holistically predict a set of polygons explaining the full input scene without relying on pre-defined corner or wall detection steps. Experiments show it achieves state-of-the-art results on two datasets while being much faster than previous approaches.
