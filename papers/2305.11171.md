# [TrueTeacher: Learning Factual Consistency Evaluation with Large Language   Models](https://arxiv.org/abs/2305.11171)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we generate high-quality synthetic data to improve factual consistency evaluation in summarization models, especially for evaluating model-generated summaries?The key points are:- Existing approaches for generating synthetic data have limitations, such as relying on perturbations of human-written summaries. This can result in unnatural examples and limited coverage of potential factual errors. - Large language models (LLMs) show promise for factual consistency evaluation but are too expensive to use directly.- The authors propose TrueTeacher, which uses LLMs to label a diverse set of model-generated summaries for factual consistency. This creates synthetic data that better captures real summarization errors and allows distilling the LLM's knowledge into a smaller model.- Experiments show TrueTeacher data improves a strong baseline and outperforms the LLM teacher, demonstrating effectiveness. Comparisons to other synthetic data methods reveal the robustness of TrueTeacher.In summary, the main hypothesis is that using LLMs to label model-generated summaries can produce high-quality and robust synthetic data for training factual consistency evaluators, overcoming limitations of prior approaches. The experiments aim to demonstrate this.
