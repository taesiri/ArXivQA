# [TrueTeacher: Learning Factual Consistency Evaluation with Large Language   Models](https://arxiv.org/abs/2305.11171)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we generate high-quality synthetic data to improve factual consistency evaluation in summarization models, especially for evaluating model-generated summaries?The key points are:- Existing approaches for generating synthetic data have limitations, such as relying on perturbations of human-written summaries. This can result in unnatural examples and limited coverage of potential factual errors. - Large language models (LLMs) show promise for factual consistency evaluation but are too expensive to use directly.- The authors propose TrueTeacher, which uses LLMs to label a diverse set of model-generated summaries for factual consistency. This creates synthetic data that better captures real summarization errors and allows distilling the LLM's knowledge into a smaller model.- Experiments show TrueTeacher data improves a strong baseline and outperforms the LLM teacher, demonstrating effectiveness. Comparisons to other synthetic data methods reveal the robustness of TrueTeacher.In summary, the main hypothesis is that using LLMs to label model-generated summaries can produce high-quality and robust synthetic data for training factual consistency evaluators, overcoming limitations of prior approaches. The experiments aim to demonstrate this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called TrueTeacher for generating synthetic data to train models for evaluating factual consistency of summaries. The key ideas are:- Using large language models (LLMs) like FLAN-PaLM to label summaries for factual consistency instead of relying on rules or human annotations. This allows generating a large amount of labeled data easily.- Generating the labeled summaries by summarizing documents using a diverse set of trained summarization models rather than perturbing human reference summaries. This results in more realistic and diverse summaries with factual errors. - Showing that the synthetic TrueTeacher data can substantially improve factual consistency evaluation on the TRUE benchmark compared to just using natural language inference data like ANLI. The TrueTeacher method also outperforms existing synthetic data generation techniques when evaluated systematically.- Demonstrating the applicability of the approach to multiple languages by generating multilingual synthetic data and evaluating on the mFACE benchmark. - Releasing a large dataset of 1.4M synthetic TrueTeacher examples to foster further research.In summary, the key contribution is a simple yet effective method for generating synthetic factual consistency evaluation data using LLMs, resulting in improved evaluation performance. The approach is also robust, scalable and multilingual.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes TrueTeacher, a method for generating synthetic training data to improve factual consistency evaluation in summarization using model-generated summaries labeled by a large language model; experiments show TrueTeacher data boosts performance of a student model over state-of-the-art methods and generalizes well to new domains and languages.
