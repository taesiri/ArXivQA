# [Radial Networks: Dynamic Layer Routing for High-Performance Large   Language Models](https://arxiv.org/abs/2404.04900)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) struggle with strict latency, memory, and power constraints needed for practical deployment. As models grow in size, opportunities emerge for "dynamic layer sparsity" - skipping computation for entire layers based on the input.

- Prior work has focused on static sparsity or "early exit" methods which exit early in the network. However, the paper shows the earlier layers contribute more, so early exit focuses on the wrong layers.

- There is a need for a technique to exploit the dynamic, input-dependent sparsity in modern large neural networks.

Key Ideas:

- The paper analyzes residual connections in transformers to quantify per-layer contribution, defined as the "residual ratio". This ratio decreases for larger models, with OPT-66B having median ratio of 5.9%, indicating high dynamic layer sparsity.

- The residual ratio varies significantly across tokens in the sequence, enabling token-specific routing of layers. Analysis shows earlier transformer layers contribute more than later layers, except the very first layers.

- The paper proposes "Radial Networks", which route embeddings between layers guided by a trained routing module. This allows models to scale in size while keeping dynamic depth manageable.

Contributions:

- Quantitative analysis of dynamic layer sparsity, showing trends in residual ratio for larger language models and vision transformers

- Concept of Radial Networks that perform token-level routing to exploit input-dependent layer sparsity

- Approach allows scaling model size while keeping dynamic depth and compute lower, through layer reuse. Leads to higher capacity models with lower serve latency & cost.

In summary, the paper provides useful analysis of dynamic layer sparsity in modern neural networks, and proposes Radial Networks as an efficient routing architecture to exploit this at scale.
