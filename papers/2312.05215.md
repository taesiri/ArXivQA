# [DeltaZip: Multi-Tenant Language Model Serving via Delta Compression](https://arxiv.org/abs/2312.05215)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph for this paper:

This paper reviews DeltaZip, a multi-tenant language model serving system for personalized language models. DeltaZip uses delta compression between a base model and fine-tuned personalized language models to enable efficient model serving across multiple tenants. Various lossy and lossless compression techniques are used, including the optimal brain surgeon (OBS) methodology integrating quantization and sparsification, as well as the GDeflate algorithm. Model quality evaluation is done based on downstream task accuracy and conversational examples, showing minimal quality degradation at high compression ratios. The lossy compression methodology based on OBS along with the underlying theory is also described in detail. In summary, DeltaZip demonstrates an effective approach for multi-tenant serving of personalized language models through differential model compression.
