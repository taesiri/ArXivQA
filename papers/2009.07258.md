# [BERT-QE: Contextualized Query Expansion for Document Re-ranking](https://arxiv.org/abs/2009.07258)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we leverage BERT models to develop a more effective query expansion technique for document re-ranking in information retrieval?Specifically, the authors propose a new query expansion model called BERT-QE that aims to better select and incorporate relevant information from pseudo relevance feedback documents by taking advantage of BERT's ability to identify relevant text. The key hypothesis is that by using BERT in a three-phase re-ranking approach - (1) initial document re-ranking with BERT, (2) selecting relevant chunks from top docs using BERT, (3) final re-ranking using query+chunks with BERT - they can significantly improve ranking performance over just using BERT for ranking alone.So in summary, the central research question is how to develop a BERT-based query expansion technique to improve document re-ranking effectiveness. The key hypothesis is that BERT-QE will outperform BERT large models significantly for document ranking.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a new query expansion model called BERT-QE that leverages BERT to effectively select relevant information from feedback documents for expansion. 2. Demonstrating that BERT-QE can significantly improve upon BERT-Large on the Robust04 and GOV2 test collections, advancing state-of-the-art performance.3. Showing that by replacing BERT-Large with smaller BERT variants in the second and third phases of BERT-QE, the efficiency can be improved while still outperforming BERT-Large. For example, BERT-QE-LMT requires only 3% more computation than BERT-Large but outperforms it significantly.In summary, the key contribution is presenting a novel BERT-based query expansion method that can beat BERT-Large, which is state-of-the-art for ranking, while also providing flexibility in trading off efficiency and effectiveness. The strengths of BERT in selecting relevant text are leveraged to expand queries more effectively compared to prior expansion techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper proposes a novel query expansion method called BERT-QE that leverages BERT to select relevant chunks from pseudo-relevant documents to expand the query and improve document ranking, demonstrating significant improvements over BERT-Large on the Robust04 and GOV2 test collections.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a summary of how it compares to related work in the field:- The paper proposes a novel query expansion method called BERT-QE that leverages BERT to select relevant chunks from pseudo-relevance feedback documents for expansion. This is different from prior query expansion techniques like RM3 and the KL divergence model that select terms or concepts based on statistical language models. - Compared to neural pseudo-relevance feedback (PRF) methods like NPRF, BERT-QE takes a more fine-grained approach to expansion by selecting relevant chunks rather than using full feedback documents. The BERT ranker allows more accurate relevance estimation.- The paper shows BERT-QE significantly outperforms strong BERT baselines like fine-tuned BERT-Large on standard TREC collections. This demonstrates the effectiveness of the proposed expansion technique over just ranking with BERT.- To improve efficiency, the paper explores using smaller BERT models for chunk selection/reranking. This trades off some effectiveness for faster runtime compared to using BERT-Large everywhere.- Overall, BERT-QE advances the state-of-the-art in query expansion by effectively leveraging the relevance estimation capabilities of BERT for expansion. The gains over BERT-Large baselines showcase the benefits of the proposed technique.In summary, the key novelties of BERT-QE compared to related work are using BERT for fine-grained chunk selection, the multi-phase architecture, and demonstrations of improved effectiveness over competitive BERT ranking baselines. The explorations of efficiency trade-offs are also useful contributions.
