# [SoK: Analyzing Adversarial Examples: A Framework to Study Adversary   Knowledge](https://arxiv.org/abs/2402.14937)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper presents a theoretical framework to formally analyze and categorize the knowledge possessed by adversaries when crafting adversarial examples against image classifiers. 

The key problem highlighted is that prior work lacks rigor in defining adversary threat models and capabilities. This makes it difficult to meaningfully compare attacks, evaluate defenses, and reason about real-world security implications.

To address this, the authors introduce the concept of Information Extraction Oracles (IEOs) to generically represent the information available to an attacker. IEOs can encode knowledge about the model, training data, training process, and defenses. Four main information categories are identified. 

The authors further define a domination relationship between IEOs to represent relative strength of knowledge, as well as rules for combining IEOs. This supports systematically constructing Hasse diagrams to visualize information relationships within each category.

Additionally, an adversarial example game is proposed to standardize attack evaluation. The game involves an attacker trying to craft adversarial examples that can evade detection by a defender's system. Explicit functions are defined to generate attacks, evaluate success criteria, and classify inputs. 

The authors conduct a comprehensive survey of over 80 recent papers on adversarial attacks in image classification. Each attack's threat model assumptions are translated into the proposed framework to highlight information used. Comparative evaluations on ImageNet and CIFAR-10 showcase how the formalism enables more structured analysis.

Key conclusions are that undefended models remain highly vulnerable to even limited information attacks. However, transferable attacks can approach white-box effectiveness by leveraging surrogate data and models. The framework also confirms the disproportionate impact of model information for attackers. Overall, the work provides a principled foundation to advance robustness evaluations.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper provides a theoretical framework to formally analyze adversary knowledge in the context of adversarial attacks on image classifiers, categorizes attack information, surveys recent attacks to classify their knowledge assumptions, and empirically analyzes attack performance to draw conclusions about the effect of information on attack success.


## What is the main contribution of this paper?

 According to the paper, the main contribution is providing a theoretical framework to formally study and categorize adversary knowledge in the context of adversarial example attacks on image classification models. Specifically:

- They propose a formalization based on information extraction oracles (IEOs) to represent the information and capabilities available to attackers when mounting attacks. This allows for precise threat model descriptions.

- They introduce an adversarial example game, inspired by cryptographic games, to standardize how attacks are presented. This includes formally defining attack components like the adversarial example generation function, evaluation function, etc.

- They survey recent attack papers and classify the adversary knowledge used in each one according to their proposed IEO framework. This allows them to systematically analyze and compare different attacks.

- Based on the survey, they are able to confirm some existing beliefs about adversary knowledge (e.g. information about the target model is highly potent) as well as derive new conclusions like transferable attacks being less difficult than previously thought.

So in summary, the main contribution is the theoretical framework itself to formally study adversary knowledge in adversarial attacks, which is then applied to survey and analyze a range of recent attack papers.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Adversarial examples - Malicious inputs that cause machine learning models to make mistakes
- Adversary knowledge - The information and capabilities an attacker has when creating adversarial examples
- Threat models - Frameworks for analyzing the abilities of potential attackers
- Information extraction oracles (IEOs) - Formal representations of the information available to an adversary
- Adversarial example game - A game theoretic framework for evaluating attacks 
- Image classification - The machine learning task of assigning categories/labels to images
- Transferability - The ability of an attack designed against one model to also fool other models
- Distinguisher - A function that attempts to detect adversarial inputs
- Indistinguishability - Making adversarial examples difficult for the distinguisher to identify
- Evaluation frameworks - Standardized ways to assess the effectiveness of attacks

The paper provides a formal framework based on information extraction oracles and an adversarial game for analyzing adversary knowledge and evaluating attacks on image classifiers. Key goals are understanding what information helps attackers, facilitating comparability between threat models, and highlighting the need for standardized evaluation procedures.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the proposed framework for modeling adversary knowledge using information extraction oracles compare to previous approaches for systematizing adversary knowledge? What are the main advantages?

2. The paper categorizes adversary knowledge into model, data, training, and defense information. Could additional categories be useful to capture other aspects of adversary knowledge? What other categories could be relevant?

3. The relative performance score is proposed as an alternative to expected success rate for evaluating attacks. What are the limitations of using the relative performance score? When would expected success rate still be more suitable?

4. The paper evaluates attacks on ImageNet and CIFAR-10. How well would you expect the conclusions to generalize to other datasets like medical images or aerial images? What differences might we expect?

5. Could the proposed adversarial example game provide a useful framework for developing more realistic simulations to evaluate adversarial attacks and defenses? What elements would need to be modeled to create useful simulations based on this game?

6. What are the implications of the finding that transferable attacks can achieve very high success rates given additional data and training information? How should this guide defense development?

7. The evaluation relies primarily on empirical attack success rates. How could the theoretical modeling of adversary knowledge be combined with theoretical guarantees for defenses?

8. What open questions remain regarding formally modeling adversary knowledge in the context of physical attacks rather than just digital attacks?

9. The proposed framework considers individual components of adversary knowledge. How could it be extended to model the interactions between different components? 

10. How useful is the framework for capturing evolving adversary knowledge over time and supporting threat modeling as a continuous process throughout a system's lifetime?
