# [Randomized Adversarial Training via Taylor Expansion](https://arxiv.org/abs/2303.10653)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is: 

How can randomized weights be utilized to improve both robustness against adversarial examples and accuracy on clean examples for deep neural networks?

Specifically, the authors propose a novel adversarial training method that injects small random noise into the network weights. They hypothesize that optimizing over these randomized weights can help find flatter minima and improve generalization, enhancing performance on both clean and adversarial data. 

The key ideas and contributions appear to be:

- Conducting an empirical analysis showing their method can flatten the loss landscape and find flatter minima compared to standard adversarial training. This indicates better generalization ability.

- Proposing a new adversarial training method using Taylor series expansion to optimize over the randomized weights. This enables optimizing the robustness loss function decomposed into zeroth, first, and second order Taylor terms.

- Demonstrating through extensive experiments that their proposed method can consistently improve state-of-the-art adversarial training techniques on both clean accuracy and robustness across different datasets and architectures.

In summary, the core hypothesis is that adversarial training over randomized weights can smooth the loss landscape, find flatter minima, and ultimately enhance performance on both clean and adversarial data compared to prior adversarial training methods. The experiments aim to validate this hypothesis.
