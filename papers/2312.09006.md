# [FedSSA: Semantic Similarity-based Aggregation for Efficient   Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2312.09006)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes FedSSA, a novel federated learning framework that enables efficient and accurate model-heterogeneous personalized federated learning. FedSSA splits each client's model into a heterogeneous feature extractor and a homogeneous classification header. It performs local-to-global knowledge transfer via a semantic similarity-based header parameter aggregation strategy, where clients only need to upload header parameters of locally seen classes for aggregation by class at the server. Global-to-local knowledge transfer is achieved via an adaptive parameter stabilization strategy, where historical local header parameters are fused with the latest global header parameters by class before local model training to speed up convergence. Theoretical analysis proves FedSSA's convergence. Extensive experiments on CIFAR datasets demonstrate that compared to state-of-the-art baselines, FedSSA achieves significantly higher accuracy and efficiency in terms of communication and computation costs. It also exhibits strong robustness against data heterogeneity and varying client participation rates. The proposed techniques for efficient and accurate model personalization make FedSSA well-suited for practical personalized federated learning applications.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Traditional federated learning (FL) requires all clients to train the same model architecture, which is not suitable for handling data heterogeneity across clients or supporting clients with diverse capabilities. Model-heterogeneous personalized FL (MHPFL) has emerged to allow each client to train a personalized model tailored to its data distribution and system capability. However, existing MHPFL methods have limitations such as relying on public datasets, incurring high communication/computation costs, or achieving limited performance gains.

Proposed Solution:
This paper proposes FedSSA, a novel MHPFL framework, to enable efficient and high-performance training of heterogeneous models in a privacy-preserving manner. The key ideas are:

1) Split each client's model into a heterogeneous feature extractor and a homogeneous classification header. 

2) Perform local-to-global knowledge transfer via semantic similarity-based aggregation of classification header parameters by class. This enhances generalization for seen classes.

3) Achieve global-to-local knowledge transfer through an adaptive parameter stabilization strategy which fuses historical local header parameters and latest global header parameters for locally seen classes. This stabilizes training.

Main Contributions:

- Proposes FedSSA, a communication- and computation- efficient MHPFL framework without relying on public datasets

- Develops semantic similarity-based aggregation to transfer knowledge between heterogeneous models 

- Devises adaptive parameter stabilization for faster convergence 

- Proves convergence guarantee for FedSSA

- Extensive experiments show FedSSA achieves up to 3.62% higher accuracy, 15.54x higher communication efficiency, 15.52x higher computational efficiency over state-of-the-art MHPFL baselines

In summary, this paper makes significant contributions in enabling efficient and performant training of heterogeneous models in federated learning settings while preserving privacy. The proposed FedSSA framework is demonstrated to outperform existing approaches on various metrics.
