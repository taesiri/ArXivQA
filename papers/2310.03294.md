# [LightSeq: Sequence Level Parallelism for Distributed Training of Long   Context Transformers](https://arxiv.org/abs/2310.03294)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question or hypothesis appears to be:

How can we design an efficient system for training large language models with very long input sequences?

The paper proposes that partitioning the input sequences across devices, rather than partitioning the model itself as is commonly done, can enable more efficient training. The key ideas/components to enable this seem to be:

- A distributed memory-efficient attention mechanism called DistAttn that parallelizes attention computation across sequence chunks.

- Novel scheduling techniques like load balancing and overlapping communication with computation to optimize performance for causal language modeling. 

- A new gradient checkpointing strategy that avoids recomputing attention when using memory efficient attention.

The overall goal is to show that this approach can:

1) Achieve faster training speed compared to prior model parallel systems like Megatron-LM.

2) Support much longer sequence lengths during training by scaling beyond the number of attention heads.

So in summary, the core research question is around how to design a system that can efficiently train large language models on very long sequences, which prior model parallel approaches struggle with. The proposed techniques aim to demonstrate improved efficiency and scalability over those existing methods.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The introduction of a new system called LightSeq for long-context language model training. LightSeq uses sequence level parallelism to partition the input tokens across devices, rather than partitioning attention heads like some prior systems.

2. Several optimizations for LightSeq:

- A distributed memory-efficient attention mechanism called DistAttn that enables parallelization along the sequence dimension.

- A load balancing technique to reduce idle time from the imbalanced workloads of causal language modeling. 

- Overlapping communication with computation to hide communication costs.

- A new checkpointing strategy that eliminates some recomputation when using memory efficient attention kernels like FlashAttention.

3. An evaluation of LightSeq on variants of the Llama-7B model, demonstrating up to 2x faster training speed compared to a baseline using Megatron-LM. The evaluation shows:

- Faster training on models with different attention head configurations

- Ability to scale beyond the number of attention heads, supporting 2-8x longer sequence lengths.

So in summary, the main contribution appears to be the LightSeq system itself and the optimizations that make it efficient for long-context language model training, with experimental results demonstrating advantages over prior work. The ability to scale beyond the number of attention heads and support longer contexts seems particularly notable.
