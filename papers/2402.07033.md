# [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts   Models](https://arxiv.org/abs/2402.07033)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) based on mixture-of-experts (MoE) architecture are showing great performance but running them on resource-constrained settings is challenging due to their huge model sizes. 
- Existing systems that offload model weights to CPU memory suffer from high overhead of frequently moving data between CPU and GPU over the low-bandwidth PCIe connection.

Proposed Solution:
- Propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models.
- Key idea is to use the computation ability of the CPU to minimize data movement between CPU and GPU. 
- Weights of non-expert layers placed on GPU memory. Subset of expert layers placed on GPU based on popularity.
- For decode stage with 1 token input, always offload expert layer execution to CPU instead of copying weights to GPU.
- For prefill stage with multiple tokens, formulate optimization problem to decide best expert execution configuration on CPU vs GPU based on measured latency models.

Main Contributions:
- Able to run the uncompressed Mixtral-8x7B model (>90GB size) at over 3 tokens/sec on a single GPU with 24GB memory, order of magnitude faster than prior works.
- Utilizes both CPU memory and computation resources to minimize CPU-GPU data transfer.
- Improves single-batch inference latency by 8.2x on Quadro RTX 6000 GPU and 10.1x on L4 GPU compared to prior works.
- An important step towards fast inference of large MoE models on resource-constrained settings.


## Summarize the paper in one sentence.

 This paper proposes Fiddler, a resource-efficient inference engine that utilizes CPU computation in addition to CPU memory to minimize data movement between CPU and GPU for faster deployment of large Mixture-of-Experts models on GPUs with limited memory.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for deploying Mixture-of-Experts (MoE) models. Specifically:

- Fiddler utilizes both CPU memory and computation resources to minimize data movement between CPU and GPU during inference of MoE models. This is based on the insight that executing experts on CPU can be faster than copying large weight matrices over PCIe, especially for small batch sizes.

- Fiddler employs an optimization strategy to determine the best way to allocate experts between CPU and GPU computation in the prefill stage. This minimizes the overall latency. 

- Evaluation on uncompressed Mixtral-8x7B model shows Fiddler improves single-batch inference latency by 8.2x on Quadro RTX 6000 GPU and 10.1x on L4 GPU compared to prior CPU offloading techniques. This enables fast deployment of huge MoE models on resource-constrained settings.

In summary, the key innovation is using both CPU memory and computation abilities to enable efficient inference of very large MoE models on systems with limited GPU memory.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Mixture-of-Experts (MoE) models
- Large Language Models (LLMs)
- Inference efficiency
- Resource-constrained settings
- CPU-GPU orchestration
- Data movement minimization
- Computation offloading
- Model deployment
- Single-batch inference 
- Latency reduction
- Uncompressed models
- Heterogeneous hardware utilization

The paper proposes Fiddler, a system to efficiently run uncompressed large MoE models on a single GPU with limited memory by utilizing both CPU memory and computation resources. Key goals are minimizing data transfer between CPU and GPU and improving single-batch inference latency in local deployment scenarios. The method aims to achieve good performance without compressing or modifying the original MoE model.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The key insight of Fiddler is to use CPU computation in addition to CPU memory for MoE model inference. What are the specific challenges in exploiting sparsity within MoE models like Mixtral that use non-ReLU activations compared to ReLU models targeted by prior works?

2. In the microbenchmarks, what causes the latency gap between GPU execution time and the time to transfer weights from CPU to GPU memory? Why does the GPU execution time not change much with increasing batch size?  

3. How does Fiddler determine which experts to place on the GPU memory during initialization? What is the expected hit rate for the best, worst and random expert selection strategies in the two evaluation environments?

4. Explain the mathematical formulation used by Fiddler to determine the optimal configuration of experts to run on CPU vs GPU during the prefill stage. What assumptions does this model make about GPU and CPU execution time? 

5. During the decode stage, Fiddler always chooses to offload expert computation to the CPU. Why is this always faster than bringing weights to the GPU when processing a single token?

6. The paper evaluates Fiddler on two different hardware environments. How do the specifics of the CPU, GPU and PCIe connectivity differ between them? How do these hardware factors impact overall performance?

7. What software frameworks and libraries does Fiddler build on top of? How may the choice of frameworks impact optimization opportunities and performance?

8. The paper uses the ShareGPT dataset for evaluation. Why is this a good choice to model realistic expert selection behavior? How might the results change with different input data?

9. How well would the optimizations proposed in Fiddler combine with other Mixture-of-Experts optimizations like model quantization or sparsification? What challenges need to be addressed?

10. Beyond the single-node setup evaluated in the paper, how can the ideas proposed in Fiddler be extended to distributed inference across multiple heterogeneous nodes? What new issues need to be considered in that setting?
