# [Parameter-Free Algorithms for Performative Regret Minimization under   Decision-Dependent Distributions](https://arxiv.org/abs/2402.15188)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper studies the problem of performative risk minimization under decision-dependent distributions. This is the setting where the distribution of the stochastic parameters depends on the decision made by the algorithm. For example, a prediction model may influence the data distribution. The goal is to minimize the performative risk which is the expected loss over the decision-dependent distribution. This problem is challenging because the distribution changes with the decision. 

Prior Work Limitations: 
Existing methods have limitations - gradient methods may not find the global optimum, Lipschitz bandit methods require knowing problem parameters like sensitivity and smoothness constants. These limitations motivate designing a practical parameter-free algorithm.

Key Ideas:
- Use the framework of optimistic optimization to search the decision space and estimate the performative risk
- Leverage the idea of performative feedback - deploying a decision reveals its induced distribution -  to refine estimates
- Develop algorithms for full-feedback (entire distribution observed) and data-driven (sample from distribution) settings 

Main Algorithms:
- DOOP: Uses hierarchical partitioning, sequentially evaluates decisions to optimize deterministic estimates of performative risk (full-feedback)  
- SOOP: Extends DOOP to handle stochastic estimates from samples (data-driven), handles noise in estimates

Key Results:
- Develop regret bounds that depend on a novel near-optimality dimension and vanish as the sensitivity parameter goes to 0
- Guarantees hold for possibly non-convex objectives, without needing problem parameters
- Demonstrate superior practical performance over prior methods on non-convex instances

Contributions:
The main contributions are developing a parameter-free optimistic optimization framework for performative risk minimization that works for non-convex objectives, has theoretical guarantees, and strong empirical performance. The algorithms adaptively estimate the performative risk landscape.

In summary, the paper makes significant progress on a challenging stochastic optimization problem arising in machine learning applications.
