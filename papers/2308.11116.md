# [LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video   Reconstruction](https://arxiv.org/abs/2308.11116)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we generate high quality, temporally consistent HDR videos from LDR videos with alternating exposures in an end-to-end manner?The key points related to this question seem to be:- Existing methods that rely on optical flow for motion compensation between LDR frames often fail when there is saturation or complicated motion. - The authors propose an end-to-end framework called LAN-HDR that aligns frames and merges them in the feature space without needing optical flow.- LAN-HDR contains two main components: an alignment module that matches frames based on luminance, and a hallucination module that fills in missing details. - Features from these two modules are adaptively blended before being merged into the final HDR frame.- A temporal loss is used during training to improve temporal consistency and reduce flickering between frames.So in summary, the main hypothesis appears to be that by performing alignment and fusion in the feature space using luminance information and adaptive blending, they can generate better quality and more temporally consistent HDR videos compared to prior optical flow based approaches. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an end-to-end framework for reconstructing high dynamic range (HDR) videos from low dynamic range (LDR) videos with alternating exposures. The key ideas are:- Using a luminance-based alignment network (LAN) to align motions between LDR frames without relying on optical flow estimation. The LAN contains an alignment module and a hallucination module to handle saturated regions. - Performing motion compensation and fusion in the feature space rather than pixel space.- Introducing a temporal loss term to improve temporal consistency in the reconstructed HDR video. Specifically, the luminance-based alignment network aligns motions by computing an attention map between LDR frames based on luminance only. This helps avoid errors from relying too much on color. The hallucination module fills in missing details in saturated regions using an adaptive gated convolution. The aligned features and hallucinated features are then merged adaptively. Finally, a temporal loss is used during training to reduce flickering artifacts and improve temporal coherence.Experiments demonstrate the proposed method can produce higher quality HDR videos compared to previous optical flow-based approaches, especially for scenes with saturation and large motions. The main advantage is performing alignment in feature space without requiring explicit motion estimation. The overall framework is end-to-end trainable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an end-to-end framework called Luminance-based Alignment Network (LAN-HDR) to reconstruct high dynamic range (HDR) videos from sequences of low dynamic range (LDR) frames with alternating exposures, which performs motion compensation and fusion in the feature space without relying on optical flow estimation.
