# [BranchNorm: Robustly Scaling Extremely Deep Transformers](https://arxiv.org/abs/2305.02790)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to robustly scale Transformers to extremely deep architectures (e.g. 1000 layers) while maintaining training stability and model performance. The key hypothesis is that constraining model updates to a constant value, as done in DeepNorm, can stabilize training but may ultimately yield undertrained models. The authors propose a new method called BranchNorm that dynamically rescales the non-residual branch of Transformers based on the training period, in order to achieve both training stability and better convergence.The main hypotheses tested are:- BranchNorm can stabilize training in early stages by smoothing gradient norms, similar to DeepNorm.- By progressively degenerating to vanilla Transformer in later stages, BranchNorm encourages better convergence compared to DeepNorm's constant constraints.- BranchNorm achieves a better trade-off between training stability and performance compared to DeepNorm when scaling Transformers to extreme depths.The experiments compare BranchNorm against DeepNorm and other baselines on bilingual and multilingual translation tasks using Transformers with up to 1000 layers. The results support the hypotheses, showing improved stability and convergence with BranchNorm across tasks and depths.In summary, the key hypothesis is that dynamic rescaling of Transformer branches can enable robust scaling to extreme depths, balancing stability and performance better than constant update constraints like DeepNorm. The experiments provide evidence to support this idea.
