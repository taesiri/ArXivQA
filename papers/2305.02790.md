# [BranchNorm: Robustly Scaling Extremely Deep Transformers](https://arxiv.org/abs/2305.02790)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to robustly scale Transformers to extremely deep architectures (e.g. 1000 layers) while maintaining training stability and model performance. The key hypothesis is that constraining model updates to a constant value, as done in DeepNorm, can stabilize training but may ultimately yield undertrained models. The authors propose a new method called BranchNorm that dynamically rescales the non-residual branch of Transformers based on the training period, in order to achieve both training stability and better convergence.The main hypotheses tested are:- BranchNorm can stabilize training in early stages by smoothing gradient norms, similar to DeepNorm.- By progressively degenerating to vanilla Transformer in later stages, BranchNorm encourages better convergence compared to DeepNorm's constant constraints.- BranchNorm achieves a better trade-off between training stability and performance compared to DeepNorm when scaling Transformers to extreme depths.The experiments compare BranchNorm against DeepNorm and other baselines on bilingual and multilingual translation tasks using Transformers with up to 1000 layers. The results support the hypotheses, showing improved stability and convergence with BranchNorm across tasks and depths.In summary, the key hypothesis is that dynamic rescaling of Transformer branches can enable robust scaling to extreme depths, balancing stability and performance better than constant update constraints like DeepNorm. The experiments provide evidence to support this idea.


## What is the main contribution of this paper?

This paper proposes a new method called BranchNorm to help scale Transformer models to extremely deep architectures (e.g. 1000 layers). The key contributions are:- BranchNorm dynamically rescales the non-residual branch of the Transformer based on the training period. This theoretically stabilizes training in the early stage by smoothing gradient norms, while allowing better convergence later on.- Experiments on multiple translation tasks show BranchNorm achieves better performance compared to prior work like DeepNorm. BranchNorm gets up to 0.6 BLEU higher on WMT14 En-Fr.- BranchNorm helps alleviate issues like undertraining and parameter redundancy that can occur with very deep Transformers. Analyses show BranchNorm results in lower representation similarity and sparser activations compared to DeepNorm.- BranchNorm is more robust to key hyperparameters like warmup and learning rate than prior methods. This makes it a more portable technique for scaling depth.In summary, BranchNorm provides a simple yet effective approach to stabilize extremely deep Transformer training. It achieves better tradeoffs between stability and convergence compared to prior work. The analyses and experiments demonstrate BranchNorm's effectiveness on multiple translation tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes BranchNorm, a new normalization method that dynamically rescales the non-residual branch of the Transformer network during training to improve stability early on and allow better convergence later, outperforming prior work like DeepNorm on machine translation tasks.
