# [Evaluating Deep Graph Neural Networks](https://arxiv.org/abs/2108.00955v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research questions addressed in this paper are:

1) What actually limits the stacking of deep convolution operations in GNN designs, and why does disentangling embedding propagation (EP) and embedding transformation (ET) allow for more EP operations?

2) What are the insights and guidelines for researchers to design deep GNNs? In particular, when and how to enlarge the number of EP and ET operations? 

3) With the proposed insights and guidelines, can the authors stack more EP and ET operations to outperform state-of-the-art GNNs?

The key hypothesis appears to be that clearly separating and considering the depth/number of EP and ET operations is crucial for understanding the limitations of existing GNNs and designing better deep GNN architectures. 

The authors argue that increasing EP operations can lead to over-smoothing, while increasing ET operations leads to model degradation. Disentangling EP and ET allows independent control over the number of each type of operation. Their guidelines suggest enlarging EP for sparse graphs and enlarging ET for large graphs. Their proposed model DGMLP implements these insights, allowing flexible depth of both EP and ET. Experiments demonstrate DGMLP achieves state-of-the-art performance, supporting the value of their insights and approach.

In summary, the core research questions focus on understanding limitations of GNN depth, providing insights/guidelines for deep GNN design, and demonstrating their utility through a flexible deep GNN model. The key hypothesis is on the importance of considering EP and ET depth separately.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It conducts an experimental evaluation to systematically analyze the root causes behind the shallow architecture issue of most existing graph neural networks (GNNs). Through experiments, the paper finds that there are two key factors limiting the depth of GNNs:

- Over-smoothing caused by too many embedding propagation (EP) operations, which makes node representations indistinguishable. 

- Model degradation caused by too many embedding transformation (ET) operations, which hurts model expressiveness.

2. It provides insights and guidelines on when and how to construct deep GNNs based on the analysis:

- Sparse graphs need more EP operations to expand node receptive fields. 

- Large graphs need more ET operations to fit complex feature distributions.

- EP and ET operations should be disentangled and controlled separately.

3. It proposes a novel GNN model called Deep Graph Multi-Layer Perceptron (DGMLP) that can go deeper on both EP and ET based on the above insights. DGMLP adopts a node-adaptive mechanism for EP and residual connections for ET. Experiments show DGMLP achieves state-of-the-art performance while maintaining high efficiency and scalability.

In summary, this paper makes a systematic experimental analysis to understand the limitations of GNN depth and provides valuable insights to guide the design of deep GNN architectures. The proposed DGMLP model also demonstrates the effectiveness of the insights.
