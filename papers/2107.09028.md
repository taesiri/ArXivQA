# [Structured Stochastic Gradient MCMC](https://arxiv.org/abs/2107.09028)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that imposing a structured factorization in the approximate posterior distribution can improve the mixing time and accuracy of stochastic gradient MCMC algorithms. Specifically, the authors propose methods to sample from an approximate posterior that factorizes across user-specified groups of parameters, while still capturing residual correlations through the sampling process. This is in contrast to variational inference methods that fully factorize the posterior. The key questions are:1) Can we derive a principled objective function that corresponds to the optimal approximate posterior subject to factorization constraints? 2) Can we develop efficient stochastic gradient MCMC algorithms to sample from this structured distribution?3) Does imposing structure on the posterior in this way actually improve mixing times and model accuracy compared to standard SGMCMC algorithms?The authors address the first question by showing the structured posterior minimizes KL divergence to the true posterior given the constraints. They address the second by developing "structured SGMCMC" methods that sample from this distribution. Finally, they provide experiments on neural networks demonstrating improved convergence speed and accuracy compared to baseline SGMCMC algorithms. Overall, the central hypothesis is that by carefully controlling posterior correlations during sampling, you can get the best of both worlds - the accuracy of MCMC with the efficiency of structured variational inference.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new approach called "structured SGMCMC" which bridges stochastic gradient MCMC (SGMCMC) algorithms and structured variational inference. Specifically:- The paper first reviews structured variational inference and formulates it as a non-parametric KL minimization problem. It shows that the optimal approximate posterior factorizes across user-specified groups of parameters.- It then proposes a novel SGMCMC-like algorithm called structured SGMCMC (S-SGMCMC) that can generate samples from this optimal factorized posterior distribution. The key idea is to run SGMCMC on a modified energy function inspired by coordinate-ascent VI. - To improve computational efficiency, the paper further proposes structured dropout SGMCMC (S$_d$-SGMCMC) which interpolates between SGMCMC and S-SGMCMC using ideas from dropout regularization. - Experiments on regression tasks and image classification benchmarks demonstrate that the proposed methods impose posterior structure, increase mixing speed, and attain comparable or better accuracy than baselines.In summary, the main contribution is a principled way to hybridize SGMCMC and structured VI in order to sample from an approximate posterior that factors across user-specified groups of parameters. This trades off approximation quality for faster mixing times. The dropout variant also makes the method more scalable.Does this help summarize the core contribution of the paper? Let me know if you have any other questions!


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes structured stochastic gradient MCMC algorithms that approximate the posterior by breaking dependencies between parameters like in structured variational inference, while still generating samples and allowing complex posteriors unlike mean-field VI.


## How does this paper compare to other research in the same field?

This paper introduces a new approach for combining stochastic gradient MCMC (SGMCMC) and variational inference (VI) methods. Some key comparisons to prior work:- It proposes a non-parametric, structured mean-field VI scheme inspired by coordinate ascent VI updates (Bishop 2006). This allows imposing a factorization structure on the posterior while remaining fully non-parametric, unlike typical VI methods. - It can be viewed as an approximate SGMCMC method where the variational distribution is represented by a Markov chain and the evidence lower bound (ELBO) is implicit. This connects it to some prior hybrid VI/MCMC methods, but the approach is novel.- Relative to prior hybrid methods like (Domke et al. 2017), it aims to close the gap between VI and exact MCMC rather than just improve VI. Experiments show it better approximates the full posterior versus parametric VI.- It introduces a dropout-inspired extension (S_d-SGMCMC) to improve scaling. This provides a continuum between SGMCMC and structured SGMCMC, unlike prior hybrid methods.- Compared to recent interpretations connecting VI and Langevin dynamics (Mandt et al. 2016), this work does not view VI through the lens of MCMC but rather aims to modify MCMC to improve conditioning.So in summary, it proposes a principled new framework for structured and non-parametric VI via modified SGMCMC, with a dropout variant for scaling. The approach is novel compared to prior hybrid VI/MCMC methods. Experiments demonstrate advantages over both baselines in approximating complex posteriors.Reader: How does the proposed method compare to using normalizing flows for variational inference?


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing convergence proofs for the proposed S-SGMCMC and S$_d$-SGMCMC algorithms. The authors mention that formal convergence guarantees are an important open problem due to the non-Markovian nature of the sampling procedures.- Further exploration of different choices of distribution $p^{(S_d)}(r)$ beyond the Bernoulli distribution used in the paper. The authors state that investigating other distributions with support on $[0,1]^M$ could lead to energy functions with useful properties.- Applying the proposed methods to large-scale deep learning tasks. The paper demonstrates the algorithms on smaller models and datasets, so scaling up to large modern neural networks and datasets is an important next step.- Extending the algorithms to cases beyond mean-field variational approximations, such as structured covariance approximations. - Developing adaptive or automatic procedures for selecting the hyperparameters like the number of groups $M$ and dropout rate $\rho$. The paper manually specifies these but learning them could improve performance.- Combining the methods with recent advances in SGMCMC such as using neural networks to learn transition operators or leverage meta-learning. This could further improve the efficiency and flexibility of the algorithms.- Comparing to a wider range of VI methods, especially nonparametric VI approaches that make weaker assumptions. This could shed light on the trade-offs between the different approaches.- Exploring Bayesian hyperparameter optimization using the gradients from the model. The authors provide a small experiment but more investigation in this direction could be useful.In summary, the main future directions focus on theoretical analysis, enhancements to the algorithms, scaling up to larger applications, and comprehensive benchmarking against competing methods. Advancing research in these areas will help further develop the ideas presented in the paper.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a new approximate stochastic gradient MCMC (SGMCMC) method that combines ideas from variational inference to improve mixing times. The key idea is to partition the model parameters into groups and impose a structured variational approximation that breaks dependencies between groups. This leads to a modified energy function that can be sampled from using standard SGMCMC techniques. The proposed structured SGMCMC method allows interpolating between a fully joint posterior (standard SGMCMC) and a fully factorized one (like mean-field VI). An efficient approximation called structured dropout SGMCMC is further introduced, inspired by dropout regularization. Experiments on deep ResNet models demonstrate that the structured approximation speeds up mixing times compared to standard SGMCMC baselines. At the same time, it reaches comparable or sometimes better accuracy than both SGMCMC and variational methods. Overall, the paper introduces a way to systematically trade off posterior correlations vs. computational efficiency in stochastic gradient MCMC samplers.
