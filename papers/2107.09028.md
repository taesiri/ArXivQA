# [Structured Stochastic Gradient MCMC](https://arxiv.org/abs/2107.09028)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that imposing a structured factorization in the approximate posterior distribution can improve the mixing time and accuracy of stochastic gradient MCMC algorithms. Specifically, the authors propose methods to sample from an approximate posterior that factorizes across user-specified groups of parameters, while still capturing residual correlations through the sampling process. This is in contrast to variational inference methods that fully factorize the posterior. The key questions are:1) Can we derive a principled objective function that corresponds to the optimal approximate posterior subject to factorization constraints? 2) Can we develop efficient stochastic gradient MCMC algorithms to sample from this structured distribution?3) Does imposing structure on the posterior in this way actually improve mixing times and model accuracy compared to standard SGMCMC algorithms?The authors address the first question by showing the structured posterior minimizes KL divergence to the true posterior given the constraints. They address the second by developing "structured SGMCMC" methods that sample from this distribution. Finally, they provide experiments on neural networks demonstrating improved convergence speed and accuracy compared to baseline SGMCMC algorithms. Overall, the central hypothesis is that by carefully controlling posterior correlations during sampling, you can get the best of both worlds - the accuracy of MCMC with the efficiency of structured variational inference.
