# [NViST: In the Wild New View Synthesis from a Single Image with   Transformers](https://arxiv.org/abs/2312.08568)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Learning 3D scene representations from RGB images to enable novel view synthesis and 3D modeling remains challenging. Traditional structure-from-motion and multiview stereo methods require many images and optimization. Neural radiance fields (NeRF) revolutionized novel view synthesis from images, but still needs dozens of images. Recent works try to generalize NeRFs to new scenes from a single image, but have difficulties with scale ambiguity, alignment, and complex backgrounds when scaling to large real-world datasets. Meanwhile, 2D latent diffusion models can generate high-quality images but extending them to 3D is computationally expensive during sampling.

Proposed Solution:
The paper proposes NViST, a transformer-based model to enable in-the-wild novel view synthesis from a single input image. The key ideas are:

1) An encoder-decoder architecture with the encoder being a fine-tuned MAE network that provides powerful self-supervised image features.

2) A novel decoder that maps features to 3D tokens using cross-attention and reasons about occlusions via self-attention. Adaptive layer normalization conditions on camera parameters to handle scale ambiguity. 

3) Efficient inference since only a single feedforward pass is needed, unlike diffusion models requiring sampling.

4) The model works with relative camera poses so no dataset alignment/canonicalization is needed.

5) Trained on a large-scale real-world video dataset (MVImgNet) to learn robust in-the-wild representations.


Main Contributions:

- Novel decoder design with cross/self-attention and camera conditioning to map image features to 3D

- Single feedforward pass at inference for efficiency

- Leverages relative camera poses so no dataset alignment needed

- State-of-the-art results on MVImgNet test set and good generalization even to casual phone captures

- Analyzes design choices via ablation study and shows the benefit of large-scale real-world pre-training

The model represents progress towards in-the-wild neural novel view synthesis from single images.
