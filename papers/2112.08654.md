# [Learning to Prompt for Continual Learning](https://arxiv.org/abs/2112.08654)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop a new continual learning paradigm that learns in a more succinct and intelligent way, without relying on past data storage or known task identity at test time? Specifically, the authors aim to introduce a new approach called Learning to Prompt (L2P) that uses prompt-based learning to tackle continual learning challenges. The key ideas are:- Learning a prompt pool memory space to store task-invariant and task-specific knowledge in a parameterized way, rather than storing raw past data like typical rehearsal-based methods.- Using an instance-wise prompt query mechanism to dynamically select relevant prompts for each input, removing the need for known task identity at test time.- Optimizing prompts to "instruct" a frozen pre-trained model to solve sequential tasks, rather than directly adapting the model weights continually. The central hypothesis is that this prompt-based approach can achieve strong continual learning performance without relying on past data storage or known task identity, providing a new paradigm compared to existing methods. The experiments aim to demonstrate the effectiveness of L2P under various continual learning settings.In summary, the paper introduces and evaluates the prompt-based learning approach L2P as a new way to tackle key continual learning challenges, moving beyond existing paradigms reliant on past data and task identity. The effectiveness of this new paradigm is the central research question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new continual learning method called Learning to Prompt (L2P), which introduces the idea of prompting from natural language processing to tackle continual learning challenges. 2. The key idea is to learn a set of prompts stored in a prompt pool, which serves as a form of episodic memory to store task-specific and shared knowledge. The prompts can provide contextual instructions to the frozen pretrained model to perform different tasks sequentially, avoiding catastrophic forgetting.3. This approach allows task-agnostic continual learning without needing task identities or boundaries during training/inference. It also reduces the need for a large rehearsal buffer.4. Comprehensive experiments show L2P outperforms prior state-of-the-art methods on various continual learning benchmarks, including class-incremental, domain-incremental, and task-agnostic settings. It achieves strong performance even without using any rehearsal buffer.5. The method provides a new perspective and framework for continual learning by introducing the idea of prompting. The learned prompt pool provides an explicit way to store task knowledge and mitigate forgetting.In summary, the key contribution is proposing a novel prompting-based continual learning method that can handle challenging settings like task-agnostic learning, while achieving state-of-the-art performance by learning to store task knowledge in a prompt pool memory. The idea of prompting provides a new direction for overcoming catastrophic forgetting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new continual learning method called Learning to Prompt (L2P) that uses a learnable prompt pool to dynamically instruct a frozen pre-trained model to solve new tasks sequentially, avoiding catastrophic forgetting without needing a replay buffer or known task identity.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of continual learning:- The key innovation in this paper is the idea of using prompts and prompt tuning to address catastrophic forgetting in continual learning. Most prior work in continual learning has focused on approaches like regularization, replay buffers, dynamic architectures, etc. Using prompts/prompt tuning is a novel approach that hasn't been explored before for continual learning. - The method allows knowledge to be stored in a succinct prompt space rather than a large replay buffer. Storing past knowledge in a parameterized prompt provides a more efficient memory mechanism compared to simply replaying raw data from previous tasks.- The method does not require task identities or boundaries at test time. Many prior continual learning methods rely on knowing the task ID so they can adjust the model accordingly. By using an instance-wise prompt query mechanism, this method can work in a task-agnostic manner.- The approach achieves strong performance across various continual learning benchmarks, including class-incremental, domain-incremental, and task-agnostic settings. It outperforms prior state-of-the-art methods in many cases, especially when replay buffers are small or not available.- The method provides a different perspective on addressing catastrophic forgetting compared to prior regularization or replay approaches. It shows promise in being able to extract task-invariant and task-specific knowledge in a parameterized prompt space.- One limitation is that the method has only been demonstrated on vision tasks using ViT models. Extending it to other modalities like NLP could be an interesting direction for future work.Overall, the idea of using prompts and prompt tuning seems innovative compared to prior continual learning literature. The results are strong across different benchmarks, and the approach provides a new way to think about storing knowledge to address catastrophic forgetting. More exploration around the prompts mechanism in continual learning could further advance research in this field.
