# [Learning to Prompt for Continual Learning](https://arxiv.org/abs/2112.08654)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop a new continual learning paradigm that learns in a more succinct and intelligent way, without relying on past data storage or known task identity at test time? Specifically, the authors aim to introduce a new approach called Learning to Prompt (L2P) that uses prompt-based learning to tackle continual learning challenges. The key ideas are:- Learning a prompt pool memory space to store task-invariant and task-specific knowledge in a parameterized way, rather than storing raw past data like typical rehearsal-based methods.- Using an instance-wise prompt query mechanism to dynamically select relevant prompts for each input, removing the need for known task identity at test time.- Optimizing prompts to "instruct" a frozen pre-trained model to solve sequential tasks, rather than directly adapting the model weights continually. The central hypothesis is that this prompt-based approach can achieve strong continual learning performance without relying on past data storage or known task identity, providing a new paradigm compared to existing methods. The experiments aim to demonstrate the effectiveness of L2P under various continual learning settings.In summary, the paper introduces and evaluates the prompt-based learning approach L2P as a new way to tackle key continual learning challenges, moving beyond existing paradigms reliant on past data and task identity. The effectiveness of this new paradigm is the central research question.
