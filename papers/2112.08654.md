# [Learning to Prompt for Continual Learning](https://arxiv.org/abs/2112.08654)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop a new continual learning paradigm that learns in a more succinct and intelligent way, without relying on past data storage or known task identity at test time? Specifically, the authors aim to introduce a new approach called Learning to Prompt (L2P) that uses prompt-based learning to tackle continual learning challenges. The key ideas are:- Learning a prompt pool memory space to store task-invariant and task-specific knowledge in a parameterized way, rather than storing raw past data like typical rehearsal-based methods.- Using an instance-wise prompt query mechanism to dynamically select relevant prompts for each input, removing the need for known task identity at test time.- Optimizing prompts to "instruct" a frozen pre-trained model to solve sequential tasks, rather than directly adapting the model weights continually. The central hypothesis is that this prompt-based approach can achieve strong continual learning performance without relying on past data storage or known task identity, providing a new paradigm compared to existing methods. The experiments aim to demonstrate the effectiveness of L2P under various continual learning settings.In summary, the paper introduces and evaluates the prompt-based learning approach L2P as a new way to tackle key continual learning challenges, moving beyond existing paradigms reliant on past data and task identity. The effectiveness of this new paradigm is the central research question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new continual learning method called Learning to Prompt (L2P), which introduces the idea of prompting from natural language processing to tackle continual learning challenges. 2. The key idea is to learn a set of prompts stored in a prompt pool, which serves as a form of episodic memory to store task-specific and shared knowledge. The prompts can provide contextual instructions to the frozen pretrained model to perform different tasks sequentially, avoiding catastrophic forgetting.3. This approach allows task-agnostic continual learning without needing task identities or boundaries during training/inference. It also reduces the need for a large rehearsal buffer.4. Comprehensive experiments show L2P outperforms prior state-of-the-art methods on various continual learning benchmarks, including class-incremental, domain-incremental, and task-agnostic settings. It achieves strong performance even without using any rehearsal buffer.5. The method provides a new perspective and framework for continual learning by introducing the idea of prompting. The learned prompt pool provides an explicit way to store task knowledge and mitigate forgetting.In summary, the key contribution is proposing a novel prompting-based continual learning method that can handle challenging settings like task-agnostic learning, while achieving state-of-the-art performance by learning to store task knowledge in a prompt pool memory. The idea of prompting provides a new direction for overcoming catastrophic forgetting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new continual learning method called Learning to Prompt (L2P) that uses a learnable prompt pool to dynamically instruct a frozen pre-trained model to solve new tasks sequentially, avoiding catastrophic forgetting without needing a replay buffer or known task identity.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of continual learning:- The key innovation in this paper is the idea of using prompts and prompt tuning to address catastrophic forgetting in continual learning. Most prior work in continual learning has focused on approaches like regularization, replay buffers, dynamic architectures, etc. Using prompts/prompt tuning is a novel approach that hasn't been explored before for continual learning. - The method allows knowledge to be stored in a succinct prompt space rather than a large replay buffer. Storing past knowledge in a parameterized prompt provides a more efficient memory mechanism compared to simply replaying raw data from previous tasks.- The method does not require task identities or boundaries at test time. Many prior continual learning methods rely on knowing the task ID so they can adjust the model accordingly. By using an instance-wise prompt query mechanism, this method can work in a task-agnostic manner.- The approach achieves strong performance across various continual learning benchmarks, including class-incremental, domain-incremental, and task-agnostic settings. It outperforms prior state-of-the-art methods in many cases, especially when replay buffers are small or not available.- The method provides a different perspective on addressing catastrophic forgetting compared to prior regularization or replay approaches. It shows promise in being able to extract task-invariant and task-specific knowledge in a parameterized prompt space.- One limitation is that the method has only been demonstrated on vision tasks using ViT models. Extending it to other modalities like NLP could be an interesting direction for future work.Overall, the idea of using prompts and prompt tuning seems innovative compared to prior continual learning literature. The results are strong across different benchmarks, and the approach provides a new way to think about storing knowledge to address catastrophic forgetting. More exploration around the prompts mechanism in continual learning could further advance research in this field.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring the application of the method to other modalities besides vision, such as text or audio. The authors state that their method does not make assumptions about modality, but they only demonstrate it on vision models in the paper. Applying it to other modalities could be an interesting research direction.- Generalizing the framework to other types of vision models besides transformers, such as convolutional neural networks. The authors mention that adapting their approach to ConvNets could enable wider applicability.- Developing more realistic and complex benchmarks for task-agnostic continual learning that better reflect real-world requirements. The authors acknowledge that the current benchmarks are still limited in capturing true task-agnostic scenarios. More research is needed on developing better datasets and protocols to evaluate methods.- Addressing potential negative societal impacts such as bias and security concerns. The authors recommend testing the robustness of the method and designing techniques to deal with issues like adversarial attacks. - Exploring the combination of the method with rehearsal buffers. The authors show the method works well even without rehearsal but suggest it could be further improved by incorporating rehearsal buffers.- Analyzing the theoretical properties of the method. The authors do not provide a formal theoretical analysis, so analyzing properties like convergence, stability, and sample complexity could be interesting future work.In summary, the key future directions focus on extending the method to new modalities and models, developing more realistic continual learning benchmarks, addressing negative societal impacts, combining with rehearsal, and formal theoretical analysis. The authors position their work as opening up an interesting new paradigm for continual learning using prompting.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new method called Learning to Prompt (L2P) for continual learning, which aims to mitigate catastrophic forgetting when training machine learning models sequentially on non-stationary data distributions. Unlike typical methods that adapt model parameters continually and rely on rehearsal buffers or known task identities, L2P keeps a pre-trained model fixed and learns a small set of prompt parameters that serve as instructions to steer the model's predictions based on the input. Specifically, prompts are stored in a key-value prompt pool and an instance-wise query mechanism selects relevant prompts to prepend to the input, removing the need for task identities. The prompts are optimized jointly with the prediction loss to consolidate shared and task-specific knowledge in the prompt pool. Experiments on image classification benchmarks with different continual learning settings show L2P outperforms prior state-of-the-art methods, even without using a rehearsal buffer. The work provides a new perspective on continual learning through prompts as an intelligent memory system.
