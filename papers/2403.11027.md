# [Reward Guided Latent Consistency Distillation](https://arxiv.org/abs/2403.11027)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Diffusion models like Stable Diffusion can generate high-quality images but require many sequential function evaluations (10-2000 steps), limiting inference speed. 
- Recently, consistency models (CMs) have been proposed to allow fast single-step generation by mapping points along the diffusion trajectory to the origin. However, distilled CMs like latent CMs (LCMs) sacrifice sample quality compared to the diffusion teacher.

Proposed Solution: Reward Guided Latent Consistency Distillation (RG-LCD)
- Integrates feedback from a differentiable reward model (RM) into latent consistency distillation to improve LCM sample quality.
- Trains LCM to maximize reward from RM on its single-step generations, avoiding propagating gradients through sampling. 
- Uses a latent proxy RM (LRM) to prevent reward over-optimization and enable learning from non-differentiable RMs.

Contributions:
- Introduction of RG-LCD framework to incorporate RM feedback into LCM distillation.
- Use of LRM to enable indirect RM optimization, preventing over-optimization.
- RG-LCM matches quality of 50-step diffusion sampling in just 2 steps, a 25x acceleration without quality loss.

In experiments, the 2-step generations of the RG-LCM are preferred over 50-step generations from the diffusion teacher in human evaluation. The LRM also helps improve automatic evaluation metrics like FID compared to directly optimizing the RM.
