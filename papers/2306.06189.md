# [FasterViT: Fast Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2306.06189)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How can we design an efficient Vision Transformer (ViT) architecture that achieves a good trade-off between accuracy and throughput (images per second) for computer vision tasks?Specifically, the paper proposes a new hybrid CNN-Transformer model called FasterViT that is tailored for high image throughput while maintaining high accuracy. The key ideas are:- Using a multi-scale architecture with convolutional layers in early stages for fast feature extraction, and transformer layers in later stages for modeling global dependencies. - Introducing a novel Hierarchical Attention (HAT) module that reduces the complexity of standard self-attention while still allowing global context propagation.- Carefully designing components like normalization, convolutions, attention to maximize throughput on GPU hardware.The paper shows FasterViT achieves state-of-the-art accuracy-throughput trade-offs on ImageNet classification, outperforming prior hybrid and transformer models. It also demonstrates strong performance on downstream tasks like object detection and segmentation.In summary, the main research question is how to design an efficient ViT backbone optimized for computer vision tasks, which they address through a novel architecture with hierarchical attention. The key contribution is achieving much higher throughput than prior ViTs while maintaining accuracy.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new hybrid CNN-ViT architecture called FasterViT that achieves a good trade-off between accuracy and throughput (images processed per second) for computer vision tasks. Specifically, the key contributions are:- Proposing a new Hierarchical Attention (HAT) module that enables efficient modeling of both local and global contexts by using dedicated "carrier tokens" to summarize and propagate information between local regions. This reduces the quadratic complexity of standard self-attention.- Introducing the FasterViT architecture that combines CNN and transformer blocks, with HAT modules in later stages. It is designed to maximize throughput by using dense convolutions in early stages and efficient HAT transformers in later stages.- Achieving state-of-the-art accuracy and throughput trade-offs on ImageNet classification. FasterViT models outperform previous CNN and ViT models in terms of throughput for the same accuracy.- Demonstrating strong performance of FasterViT backbones on downstream tasks like object detection, instance segmentation, and semantic segmentation.- Showing that FasterViT scales well to larger datasets and input image sizes. It outperforms models like Swin Transformer V2 when pre-trained on ImageNet-21K and fine-tuned on high resolution images.In summary, the main contribution is presenting an efficient hybrid CNN-ViT architecture with a novel Hierarchical Attention module that achieves excellent accuracy, throughput, and scalability for computer vision tasks. The efficiency comes from carefully designed components to maximize GPU hardware utilization.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes FasterViT, a new hybrid CNN-ViT model architecture that achieves state-of-the-art accuracy and throughput tradeoffs for computer vision tasks by using a novel hierarchical attention mechanism to efficiently capture both local and global spatial dependencies in images.
