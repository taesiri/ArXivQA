# [FasterViT: Fast Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2306.06189)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How can we design an efficient Vision Transformer (ViT) architecture that achieves a good trade-off between accuracy and throughput (images per second) for computer vision tasks?Specifically, the paper proposes a new hybrid CNN-Transformer model called FasterViT that is tailored for high image throughput while maintaining high accuracy. The key ideas are:- Using a multi-scale architecture with convolutional layers in early stages for fast feature extraction, and transformer layers in later stages for modeling global dependencies. - Introducing a novel Hierarchical Attention (HAT) module that reduces the complexity of standard self-attention while still allowing global context propagation.- Carefully designing components like normalization, convolutions, attention to maximize throughput on GPU hardware.The paper shows FasterViT achieves state-of-the-art accuracy-throughput trade-offs on ImageNet classification, outperforming prior hybrid and transformer models. It also demonstrates strong performance on downstream tasks like object detection and segmentation.In summary, the main research question is how to design an efficient ViT backbone optimized for computer vision tasks, which they address through a novel architecture with hierarchical attention. The key contribution is achieving much higher throughput than prior ViTs while maintaining accuracy.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new hybrid CNN-ViT architecture called FasterViT that achieves a good trade-off between accuracy and throughput (images processed per second) for computer vision tasks. Specifically, the key contributions are:- Proposing a new Hierarchical Attention (HAT) module that enables efficient modeling of both local and global contexts by using dedicated "carrier tokens" to summarize and propagate information between local regions. This reduces the quadratic complexity of standard self-attention.- Introducing the FasterViT architecture that combines CNN and transformer blocks, with HAT modules in later stages. It is designed to maximize throughput by using dense convolutions in early stages and efficient HAT transformers in later stages.- Achieving state-of-the-art accuracy and throughput trade-offs on ImageNet classification. FasterViT models outperform previous CNN and ViT models in terms of throughput for the same accuracy.- Demonstrating strong performance of FasterViT backbones on downstream tasks like object detection, instance segmentation, and semantic segmentation.- Showing that FasterViT scales well to larger datasets and input image sizes. It outperforms models like Swin Transformer V2 when pre-trained on ImageNet-21K and fine-tuned on high resolution images.In summary, the main contribution is presenting an efficient hybrid CNN-ViT architecture with a novel Hierarchical Attention module that achieves excellent accuracy, throughput, and scalability for computer vision tasks. The efficiency comes from carefully designed components to maximize GPU hardware utilization.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes FasterViT, a new hybrid CNN-ViT model architecture that achieves state-of-the-art accuracy and throughput tradeoffs for computer vision tasks by using a novel hierarchical attention mechanism to efficiently capture both local and global spatial dependencies in images.


## How does this paper compare to other research in the same field?

Here are a few key points of comparison between this paper and other research in efficient vision transformers:- The paper focuses on improving the accuracy-throughput tradeoff for vision transformers, which is an active area of research. Other papers like Swin Transformer, Twins, CSwin, etc. have similar goals of designing architectures to be faster and more efficient. - The main novelty in this paper is the proposed Hierarchical Attention (HAT) module. HAT aims to capture both local and global information in an efficient manner compared to standard self-attention. Other papers have proposed different attention mechanisms like window attention in Swin, but HAT specifically uses carrier tokens to enable cross-window communication.- For the overall architecture, the paper uses a hybrid CNN and transformer design, with CNN layers in early stages and transformer blocks later. This follows a similar philosophy as other hybrid models like LeViT, CrossViT, CoaT, etc. that try to get benefits of both CNNs and transformers.- The paper shows strong empirical results, achieving state-of-the-art in the accuracy vs throughput tradeoff on ImageNet. The comparisons are done comprehensively against ConvNets, transformer models, and other hybrids.- For downstream tasks, the paper evaluates on COCO detection/segmentation and ADE20K segmentation. Other papers also evaluate on these common vision tasks to demonstrate generalization.- One unique aspect is analysis of higher resolution training and fine-tuning. The paper argues FasterViT is advantageous for high-resolution inputs. In contrast, other models are mostly benchmarked on 224x224 ImageNet.Overall the paper builds nicely on top of recent work on efficient vision transformers, and proposes a novel architecture with hierarchical attention to push state-of-the-art further. The comparisons and experiments follow conventions in literature to benchmark against related models.
