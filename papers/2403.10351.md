# [TriSum: Learning Summarization Ability from Large Language Models with   Structured Rationale](https://arxiv.org/abs/2403.10351)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) like GPT-3 have shown great performance on text summarization. However, their massive size limits widespread deployment and raises privacy concerns when sending data to external services. There is a need for compact local models that can distill the summarization abilities of LLMs while enhancing interpretability.

Method - TriSum:
The paper proposes TriSum, a 3-step framework to transfer summarization skills from LLMs to small models:

1. LLM Rationale Probing: Use the LLM to extract "aspect-triple" rationales - key aspects, relational triples and summaries tied to them, through prompting over multiple iterations.

2. Golden Rationale Selection: Select high-quality rationales using a scoring method based on summary semantic similarity and topic coherence.

3. Curriculum Learning: Progressively train the small model on tasks ranging from aspect extraction to joint rationale-summary generation, evolving from simple to complex objectives.

Main Contributions:
- Introduces a novel distillation approach to transfer LLM's summarization abilities to compact local models with enhanced transparency through rationales.
- Designs a scoring mechanism for choosing high-quality rationales to supervise local model training.  
- Demonstrates superior performance over SOTA models on CNN/DailyMail (+4.5% ROUGE), XSum (+8.5% ROUGE) and a clinical trial dataset (+7.4% ROUGE).
- Enhances model interpretability by generating and analyzing rationales that offer insights into LLM summarization processes.

In summary, the paper enables streamlined and transparent summarization models for resource-constrained settings by strategically distilling and transferring inherent skills of large language models.
