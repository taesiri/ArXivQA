# [Decoupled Knowledge with Ensemble Learning for Online Distillation](https://arxiv.org/abs/2312.11218)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Offline knowledge distillation requires expensive resources to train a teacher network first before distilling to a student network. Online methods alleviate this by mutually training multiple networks. 
- However, existing online methods like Peer Collaborative Learning (PCL) can suffer from model collapse due to high similarity between student and teacher networks.

Proposed Solution:
- Introduce decoupled knowledge where an independent teacher network is constructed to transfer knowledge to the student networks. This increases diversity and reduces collapse.
- A teacher initialization scheme is designed to provide early decoupled knowledge to accelerate training.
- A decaying ensemble strategy assembles teacher peer logits with a decaying weight for strong initial supervision while avoiding overfitting later on.

Main Contributions:
- Proposes decoupled knowledge with ensemble learning (DKEL) for online distillation to avoid model collapse issue in PCL.
- Analyzes cause of model collapse and designs independent teacher network and initialization scheme.
- Designs decaying ensemble strategy for robust supervision throughout training.
- Conducts ideal experiments and Monte Carlo simulations to demonstrate approaches.
- Evaluates method on CIFAR and TinyImageNet datasets with various architectures, showing superiority over state-of-the-art online distillation techniques.

In summary, the key ideas are to use an independent teacher network to provide decoupled knowledge and decaying ensemble teacher logits to enhance one-stage online distillation while avoiding model collapse. The method is shown through analysis and experiments to improve over existing online distillation techniques.
