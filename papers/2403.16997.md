# [Composed Video Retrieval via Enriched Context and Discriminative   Embeddings](https://arxiv.org/abs/2403.16997)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper focuses on the problem of composed video retrieval (CoVR). In CoVR, the goal is to retrieve a target video from a database that matches the visual content of a query video but incorporates desired modifications specified in text. 
- CoVR is challenging because it requires bridging the domain gap between the query video and modification text, and simultaneously aligning the multi-modal embeddings with the dynamic target video embeddings.
- Existing methods like CoVR-BLIP rely only on visual query features and struggle to preserve query-specific context.

Proposed Solution:
- The paper proposes a CoVR framework that leverages detailed language descriptions of the query video, in addition to the visual query and modification text.
- Detailed descriptions are automatically generated using a multi-modal conversational model to provide complementary contextual information.
- The framework learns discriminative embeddings of vision, text and vision-text features using multiple target datasets and alignments.
- This allows better encoding of query context and its alignment to target videos.

Main Contributions:
- Introduces detailed language descriptions to preserve query context and reduce gap with modification text.
- Learns complementary discriminative embeddings of vision, text and vision-text for alignment.  
- Achieves new SOTA results on WebVid-CoVR dataset with 7% higher recall.
- Shows strong generalization to image retrieval tasks in zero-shot transfer learning.
- Provides new benchmark results across three datasets - WebVid-CoVR, CIRR and FashionIQ.

In summary, the key innovation is using detailed language descriptions for context modeling and learning multi-modal discriminative embeddings for superior alignment and video retrieval performance.
