# [Efficient Multimodal Learning from Data-centric Perspective](https://arxiv.org/abs/2402.11530)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multimodal large language models (MLLMs) have shown great success on visual reasoning tasks, but their deployment is hindered by substantial computational costs limiting accessibility. 
- Simply using smaller pre-trained models causes significant performance drops.

Proposed Solution:
- Introduce Bunny, a family of lightweight yet powerful MLLMs with flexible vision and language backbones.
- Construct more informative training data through dataset condensation to compensate for reduced model size. Curate 2M image-text pairs from LAION-2B via multi-step selection.  

Main Contributions:  
- Propose Bunny, which offers plug-and-play lightweight vision encoders (e.g. SigLIP, EVA-CLIP) and language models (e.g. Phi-1.5, Phi-2).
- Build informative multimodal training data by condensing LAION-2B into a 2M coreset and combining existing datasets.
- Bunny-3B with SigLIP and Phi-2 outperforms state-of-the-art large MLLMs on multiple benchmarks, demonstrating the possibility to train smaller but better MLLMs with optimized training data.

In summary, the paper explores training smaller but better MLLMs through constructing more informative training data to compensate for reduced model size. The proposed Bunny family and curated training data unlock strong performance from lightweight model combinations, outperforming even much larger MLLMs.
