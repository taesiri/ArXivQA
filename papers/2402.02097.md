# [Settling Decentralized Multi-Agent Coordinated Exploration by Novelty   Sharing](https://arxiv.org/abs/2402.02097)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Decentralized cooperative multi-agent reinforcement learning faces challenges in exploration, especially in sparse reward environments. Agents need to explore efficiently in a coordinated way. 
- However, in decentralized settings, agents only have access to local observations instead of global states. The novelty of local observations can be very different from the novelty of global states, making independent exploration inefficient.
- Previous multi-agent exploration methods rely on extra communication of observations/actions or centralized training, which limits scalability.

Proposed Solution:
- The paper proposes MACE, a simple yet effective method to enable coordinated exploration under decentralized learning settings.

- MACE uses limited communication where agents only share scalar local novelty values. By summing local novelties, agents approximate global novelty for more reliable intrinsic rewards.  

- MACE also encourages agents to influence others' exploration by quantifying the mutual information between one agent's action and future accumulated novelty of others. This mutual information is transformed into hindsight intrinsic rewards.

- The weighted combination of the two intrinsic rewards drives agents to explore novel states in a coordinated way.

Main Contributions:
- Approximates global novelty for decentralized agents using summed local novelties communicated between agents

- Newly introduces weighted mutual information to measure influence on other agents' exploration

- Converts the weighted mutual information into hindsight intrinsic rewards that enable coordinated exploration  

- Achieves superior performance over baselines in multi-agent tasks with sparse rewards, using limited communication

In summary, MACE enables efficient coordinated exploration for decentralized multi-agent reinforcement learning through novelty sharing and hindsight influence quantification, with simple limited communication.
