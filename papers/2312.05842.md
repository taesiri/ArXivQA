# [Mutual Enhancement of Large and Small Language Models with Cross-Silo   Knowledge Transfer](https://arxiv.org/abs/2312.05842)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem: 
Large language models (LLMs) have shown significant success recently. However, their performance on specialized tasks is often suboptimal, requiring further fine-tuning with task-specific data. Obtaining such private domain data faces challenges due to privacy concerns and regulations. Existing federated learning approaches for LLM fine-tuning have limitations in computation/communication costs or model proprietary constraints. There is also a need to deploy fine-tuned models back on clients for low-cost and private inference.

Proposed Solution:
The paper proposes CrossLM, a collaborative training framework between an LLM on the server and smaller SLMs on clients. Clients train SLMs locally using private data. SLMs are sent to server asynchronously to enhance the LLM without sharing private data. The key idea is to utilize the LLM's generative capability to synthesize a dataset, which SLMs provide feedback on to improve LLM's quality of generated data. This synthetic data then enhances both LLM and SLMs.

Main Contributions:
- Enables collaborative training of an LLM and heterogeneous SLMs without exposing clients' private data.
- Leverages SLMs to steer LLM's improvement and the generative power of LLM for data-free knowledge transfer between LLM and SLMs.  
- Experiments show CrossLM significantly improves task-specific performance of both LLM and SLMs while preserving LLM's generalization capability.
- Achieves lower computation/communication costs for clients and supports client model heterogeneity and asynchronous federated learning.

In summary, CrossLM provides an innovative collaborative training approach to mutually enhance specialized performance of models on both server and clients in a communication-efficient and privacy-preserving manner.
