# [HOICLIP: Efficient Knowledge Transfer for HOI Detection with   Vision-Language Models](https://arxiv.org/abs/2303.15786)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently transfer knowledge from a pre-trained vision-language model (CLIP) to a HOI detection model to improve its performance, especially on rare/unseen classes and in low data regimes.

The key hypothesis is that directly retrieving and integrating knowledge from CLIP in a query-based manner can lead to better knowledge transfer and more efficient learning compared to conventional distillation techniques used in prior work. Specifically, the paper proposes to transfer knowledge from CLIP in three aspects:

1. Spatial visual features - Using a cross-attention module to retrieve informative regional features from CLIP instead of just the global image feature.

2. Verb features - Developing a verb classifier and adapter using visual semantic arithmetic on CLIP features to represent verbs more effectively. 

3. Linguistic features - Generating an additional HOI classifier from CLIP text embeddings that provides training-free enhancement.

The central hypothesis is that retrieving and integrating complementary knowledge from both the visual and textual representations in CLIP can improve HOI detection performance, especially for rare classes and with limited training data. Experiments validate this hypothesis and show superior performance over state-of-the-art methods.

In summary, the key research question is how to efficiently transfer knowledge from CLIP to HOI detection, with the central hypothesis being that a query-based retrieval approach can achieve better knowledge transfer than conventional distillation techniques.
