# [Boosting Continual Learning of Vision-Language Models via   Mixture-of-Experts Adapters](https://arxiv.org/abs/2403.11549)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Continual learning aims to enable AI models to continuously learn new knowledge over time from new data, without forgetting previously learned knowledge. However, catastrophic forgetting remains a significant challenge, especially for large vision-language models like CLIP. Fine-tuning CLIP on new tasks leads to degradation of historical knowledge and original zero-shot transfer capability. Existing methods either focus only on mitigating forgetting or incur heavy computational burdens for preserving zero-shot ability. 

Proposed Solution:
This paper proposes a parameter-efficient continual learning framework to boost the memorization and zero-shot abilities of CLIP. The key ideas are:

1) Incremental Mixture-of-Experts (MoE) Adapters: Built upon a frozen CLIP, task-specific routers and adapters (as experts) are incrementally added to form a dynamic architecture tailored to each new task. This enhances parameter efficiency and knowledge consolidation. An activate-freeze strategy facilitates expert collaboration. 

2) Distribution Discriminative Auto-Selector (DDAS): A set of autoencoders capture data distributions of incremental tasks. DDAS analyzes variations in distributions to automatically assign inputs to either MoE-Adapters for seen classes or original CLIP for zero-shot inference.

Main Contributions:
- Introduces lightweight adapters as experts in a dynamic MoE architecture for efficient continual learning of CLIP.
- Proposes an incremental activate-freeze strategy to augment expert collaboration and knowledge sharing. 
- Develops a DDAS module to automatically distinguish seen/unseen data and route them for robust zero-shot continual learning.
- Achieves state-of-the-art performance on multi-domain & class incremental learning benchmarks while reducing 60% parameter overhead vs prior arts.

In summary, this is a novel continual learning framework tailored for CLIP that unifies computational efficiency, anti-forgetting ability and zero-shot transfer in a principled manner. The introduced components provide interpretable and effective solutions to outfit CLIP with lifelong learning capabilities.
