# [FastViT: A Fast Hybrid Vision Transformer using Structural   Reparameterization](https://arxiv.org/abs/2303.14189)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, this paper introduces FastViT, a new vision transformer architecture that aims to achieve improved speed and efficiency while maintaining high accuracy on computer vision tasks. The key research goal is to develop a hybrid vision transformer model that obtains state-of-the-art latency-accuracy trade-off compared to prior work. Specifically, the paper aims to answer:- Can a hybrid vision transformer be designed to significantly improve speed and efficiency over prior hybrid models like CMT, while maintaining competitive accuracy? - What architectural innovations allow for faster processing suitable for real-time use cases, without sacrificing too much accuracy?To address these questions, the paper proposes three main technical contributions:1) A new token mixing block called RepMixer that uses structural reparameterization to remove skip connections and reduce memory access costs. 2) Use of linear train-time overparameterization to boost model capacity at minimal cost to efficiency.3) Replacements of some self-attention layers with large kernel convolutions to improve receptive field.The central hypothesis is that by combining these ideas into a model called FastViT, it's possible to achieve much better speed and efficiency than prior hybrid models, while maintaining accuracy. Experiments validate this hypothesis, showing FastViT achieves state-of-the-art trade-offs on image classification, detection, segmentation and other vision tasks.In summary, the key research question is whether architectural innovations like RepMixer and overparameterization can push vision transformers to new levels of efficiency without sacrificing accuracy. The paper shows this is indeed possible with FastViT.
