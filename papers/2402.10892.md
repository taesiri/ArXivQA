# [Proving membership in LLM pretraining data via data watermarks](https://arxiv.org/abs/2402.10892)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
As large language models (LLMs) grow in capability and popularity, there is an emerging need to respect copyright and provide authors the right to opt-out of having their texts used for model training without permission. However, currently there are no principled ways for authors to statistically prove whether their document collection was used to train an LLM with only blackbox access to the model.

Proposed Solution:
The paper proposes using "data watermarks", which are random perturbations inserted into a document collection before public release. Specifically, an author chooses a random "seed" to generate a watermark, perturbs their documents accordingly, then releases the watermarked versions publicly. Later, they can check if a model trained on their watermarked data by comparing the model's average loss on the secret watermark versus losses on other random watermarks - if the loss on the inserted watermark is significantly lower, then hypothesis testing provides statistical guarantees that their data was used for training.

The authors study two types of watermarks: 1) Random character sequences appended to documents 2) Random Unicode character substitutions, which are human-imperceptible. They train medium-sized models on watermarked datasets to relate watermark properties like length and number of marked documents to the statistical power for detection. Key findings are:

- Watermarking more documents increases detection power by improving the "effect size" 
- Longer watermarks reduce variance of the null distribution, also improving detection power
- Scaling experiments: watermarks get weaker as more training data is added, but remain strong if model size scales accordingly

Finally, a post-hoc study on BLOOM-176B shows SHA hashes are detected if they occurred at least 90 times in training data. This confirms watermarks could enable principled detection even for large models.

Main Contributions:
1) A hypothesis testing framework using data watermarks to statistically prove dataset membership 
2) Relating watermark design choices to the effectiveness of detection
3) Demonstrating data watermarks remain feasible even when scaling model size along with training data 
4) Showing robust detection is possible in a 176B parameter model, confirming real-world viability

Overall the paper makes significant progress towards respecting copyright in LLMs by enabling authors to reliably probe for unauthorized usage of their data. Data watermarks present a promising direction for provable opt-out rights.
