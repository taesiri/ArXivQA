# [Rectifying Demonstration Shortcut in In-Context Learning](https://arxiv.org/abs/2403.09488)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) often rely on the semantic priors from their pre-trained knowledge when making predictions using in-context learning (ICL), rather than learning new input-label mappings. 
- This over-reliance on semantic priors is termed the "Demonstration Shortcut" problem.
- Smaller LLMs in particular struggle to override strong semantic priors and learn new tasks. 

Proposed Solution:
- The paper proposes a new method called "In-Context Calibration" to rectify the Demonstration Shortcut problem.  
- For each demonstration sample, it estimates the semantic prior relative to other examples. It also estimates a context-free semantic prior by randomly shuffling the word order. 
- By averaging these estimated priors over all demonstrations, the method computes the overall expected semantic prior of the demonstration set.
- This prior is then used to calibrate the LLM's predictions at test time, reducing dependence on semantic priors.

Contributions:
- Identifies and formalizes the Demonstration Shortcut problem affecting LLMs in ICL.
- Proposes a demonstration-aware calibration method called In-Context Calibration to mitigate this problem.
- Shows improved performance over baselines in Original ICL Task and Task Learning settings across multiple model types/sizes.
- Demonstrates particular improvements on tasks requiring overriding of semantic priors, like NLI.
- Provides evidence that the method enhances LLM's ability to learn new tasks from demonstrations.

In summary, the paper identifies and proposes a solution to mitigate LLMs' over-reliance on semantic priors over learning new tasks in ICL, with positive results over multiple experimental settings.


## Summarize the paper in one sentence.

 This paper proposes a new method called In-Context Calibration to rectify the reliance (termed "Demonstration Shortcut") of large language models on their pre-trained semantic priors of demonstrations rather than learning from input-label relationships, in order to improve in-context learning.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new method called "In-Context Calibration" to tackle the issue of large language models relying too much on the semantic priors of demonstration examples (termed "Demonstration Shortcut") rather than learning the actual input-label relationships. Specifically:

- The paper identifies and defines the issue of "Demonstration Shortcut", where LLMs make predictions in few-shot learning based on priors rather than new input-label mappings.

- To address this, the paper proposes In-Context Calibration, which estimates the semantic priors of each demonstration sample and uses that to recalibrate the model's predictions, reducing over-reliance on priors. 

- Experiments across various models, tasks, and settings show In-Context Calibration consistently improves performance by enhancing the model's ability to learn from demonstrations. Particularly strong gains are seen in task learning scenarios with new input-label mappings.

So in summary, the key contribution is identifying the Demonstration Shortcut issue, and introducing a novel in-context calibration method to reduce over-reliance on priors and improve learning from demonstrations in large language models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- In-context learning (ICL): The ability of large language models to perform tasks simply using provided demonstrations, without updating model parameters. 

- Demonstration shortcut: The phenomenon where language models rely too much on semantic priors from the demonstrations rather than learning the actual input-label relationships.

- In-context calibration: The proposed method to adjust the model's reliance on semantic priors and enable better learning from demonstrations. This is a demonstration-aware calibration approach.

- Original ICL task: Evaluating performance on classification tasks using the standard labels. 

- Task learning setting: Evaluating performance when standard labels are replaced with unrelated tokens, requiring the model to learn novel input-label mappings.

- Semantic prior: The pre-trained knowledge and label preferences that models have acquired based on the training data.

So in summary, the key focus is on improving in-context learning by mitigating over-reliance on priors through a demonstration-based calibration method, and evaluating on both standard and more challenging "task learning" settings.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the proposed In-Context Calibration method estimate the semantic prior for each demonstration sample? What is the intuition behind using the remaining in-context examples to estimate this?

2. What is the purpose of applying the random shuffling function to the demonstration samples? How does this help estimate word-level semantic priors?

3. The paper argues that previous calibration methods fail to rectify the Demonstration Shortcut issue. What specifically makes the In-Context Calibration method more capable of addressing this problem?

4. Why is the λ hyperparameter important in balancing context-level and word-level semantic priors in the proposed method? How should one determine an optimal value for λ based on the task? 

5. How does the paper experimentally validate that the proposed method enhances the model's ability to learn new input-label mappings versus relying on pre-trained semantic priors?

6. Does the improvement from In-Context Calibration come more from calibrating the model's predictions or enhancing its task learning ability? What evidence supports this?

7. How does the performance of In-Context Calibration vary across model sizes and architectures (e.g. OPT, GPT, Llama)? What insights does this provide about the Demonstration Shortcut phenomenon?

8. Why does the method show particular improvements on NLI tasks compared to sentiment analysis and detection tasks? How do the semantic priors manifest differently across these tasks?

9. Could the random shuffling function be replaced by other techniques for estimating word-level priors while preserving effectiveness?

10. Does In-Context Calibration lead to more stable and reliable performance over multiple samples of demonstrations? How could this be analyzed experimentally?
