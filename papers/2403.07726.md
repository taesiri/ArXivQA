# [SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and   Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2403.07726)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Modern neural language generation (NLG) models often produce fluent but inaccurate or unsupported outputs, referred to as "hallucinations". Detecting such hallucinations is critical for many applications where correctness matters.  
- There is a lack of consensus on the best framework and methodology for detecting hallucinations across different NLG tasks.

Proposed Solution:
- The authors organize the SHROOM shared task focused specifically on detecting fluent overgeneration hallucinations in NLG model outputs across three tasks - machine translation, paraphrasing, and definition modeling.
- The task is framed around model-aware and model-agnostic tracks to classify whether a model output contains incorrect semantic information not supported by the input.
- A new dataset is constructed with 4000 NLG model outputs labeled by 5 annotators each, spanning the three tasks with models trained to varying accuracy levels.

Key Contributions:
- The dataset analysis shows hallucinations are a gradient phenomenon without clear consensual judgments. Almost 30% of data corresponds to ambiguous cases where annotators were split in judgments.
- 58 participants grouped in 42 teams submit over 300 valid prediction sets, employing diverse approaches like model fine-tuning, prompting strategies, etc. 
- While most teams outperform the proposed baseline, top scoring systems have ~15% error rate, consistent with random guessing on ambiguous items.
- Access to model parameters provides limited benefits; participant system diversity highlights complexity of hallucination detection.

Main Conclusions:
- Ambiguous cases with split annotator judgments remain challenging for current systems. Significant research gaps persist in reliably detecting hallucinations.
- The task success underscores interest in this research area but also limitations of existing approaches. The data and analysis provide a useful starting point for further work.


## Summarize the paper in one sentence.

 This paper presents the results of the SHROOM shared task on detecting hallucinations in natural language generation systems, with participation from over 50 teams tackling the problem of identifying fluent but inaccurate model outputs across machine translation, paraphrase generation, and definition modeling tasks.


## What is the main contribution of this paper?

 According to the paper, the main contribution is organizing and conducting a shared task focused on detecting hallucinations (fluently generated but inaccurate outputs) from natural language generation systems. Specifically:

- Constructing a new dataset of 4000 model outputs across 3 NLP tasks (machine translation, paraphrase generation, definition modeling), with 5 human annotations per data point on whether the output contains unsupported information.

- Hosting a competition with model-aware and model-agnostic tracks for teams to develop systems to detect hallucinations, which attracted 58 participants grouped in 42 teams. In total over 300 valid submissions were received.

- Analyzing results to identify key trends, such as reliance on certain models (e.g. DeBERTa) and techniques (e.g. fine-tuning, ensembling). Also finding that top scores are consistent with correctly classifying clear-cut cases but random performance on more ambiguous items.

- Underscoring through the competition format and analyses that detecting hallucinations is challenging, and there is still significant room for improvement in developing more reliable natural language generation systems.

The overall goal is to foster further research in this important direction of making neural language models more trustworthy.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Hallucinations - The paper focuses on detecting fluent but incorrect "hallucinations" in natural language generation system outputs. This is a key concept. 

- Overgeneration - Closely related to hallucinations, the paper looks at cases where models generate additional output that is not actually supported by the input.

- Inferential semantics - The paper relies on using inferential semantics, judging whether model outputs can be inferred from the inputs, to identify hallucinations. This provides the theoretical framework.

- Shared task - The paper presents the results of the SHROOM shared task for detecting hallucinations across three NLG tasks. The shared task itself is a key aspect.

- Machine translation, paraphrase generation, definition modeling - The three NLG tasks covered in the shared task.

- Model-aware vs. model-agnostic - The two tracks of the shared task, referring to whether participants have access to model parameters/weights or not. 

- Fluency vs. correctness - Important concepts and evaluation issues teased apart in the paper. Detecting fluency alone does not determine correctness/lack of hallucination.

- Annotation, labels, ambiguity - Key terms related to the paper's annotation process, golden labels, and finding that hallucination is a "graded phenomenon" open to interpretation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions that hallucinations are a graded phenomenon rather than a clear binary one. What evidence from the dataset distributions supports this claim? How might future work investigate this hypothesis further?

2. The paper finds that access to model parameters in the model-aware track did not lead to significantly better performance over the model-agnostic track. Why might this be the case? What approaches could be taken to better leverage model parameters for hallucination detection? 

3. The top scoring teams used a variety of approaches like fine-tuning, ensembling, synthetic data generation etc. What are some pros and cons of each of these approaches? Under what conditions might one approach be better than others?

4. The baseline system uses a simple prompt-based approach with a modern LLM. What are some ways this baseline system could be improved without significant architecture changes? Could better prompt design and engineering lead to gains?

5. The paper finds a correlation between number of submissions and higher ranks i.e. more submissions led to better scores. What are some potential reasons this correlation exists? How can this inform training procedures?

6. Ambiguous instances where annotators disagreed remain challenging. What novel annotation and data collection procedures could produce better quality ambiguous instances? What models could perform better on such data?

7. The paper focuses only on English text. How might the difficulty of hallucination detection change for other languages, especially lower resource ones? What unique challenges might those languages pose?

8. The paper does not evaluate token-level hallucination detection. How could the sentence-level predictions be used as a starting point for identifying hallucinated tokens? What token-level annotation schema could be adopted?

9. Modern LLMs are often much larger than the models studied. How might the results translate to very large models? Would the performance gaps found still persist? What unique challenges might huge models like GPT-3 pose?

10. The paper identifies open questions about leveraging parameter spaces, multi-lingual models etc. Pick one such open question and propose a detailed method/dataset/experiment to help answer it. What results might you expect from such an experiment?
