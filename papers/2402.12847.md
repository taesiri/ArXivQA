# [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can store a lot of factual knowledge gained through pre-training. However, this knowledge can become outdated over time. A common approach is to continue pre-training LLMs on new documents to update their knowledge. 
- The paper finds that while continued pre-training reduces the perplexity (uncertainty) of new documents, LLMs still struggle to correctly answer questions about those documents. This is referred to as the "perplexity curse".

Proposed Solution: 
- The key insight is that question-answer (QA) pairs directly show how knowledge needs to be accessed, while documents weave factual statements together in more complex ways.
- The paper proposes "pre-instruction tuning" (\pit): first train the LLM exclusively on QA pairs to learn patterns of knowledge access, before training on a mix of those QA pairs and associated documents to align knowledge encoding.

Key Results:
- \pit outperforms standard continued pre-training followed by instruction tuning by 17.8\% in answering questions about updated documents.
- Detailed ablation studies confirm the importance of prioritizing learning QA pairs over documents.
- \pit enhances knowledge acquisition even when documents are from a different domain than the QA pairs.
- When applied to real-world data, \pit continues to show gains over baselines.

Main Contributions:
- Identifying "perplexity curse" limitation when updating LLMs with new documents 
- Proposing and validating "pre-instruction tuning" (\pit) as a solution
- Showing \pit works by prioritizing learning knowledge access patterns
- Demonstrating generalization of \pit to new domains
