# eDKM: An Efficient and Accurate Train-time Weight Clustering for Large   Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the key points of this paper are:- Differentiable weight clustering (specifically DKM) can achieve state-of-the-art compression ratio vs accuracy trade-off for large language models (LLMs). - However, the memory complexity of DKM is too high to be applied for train-time LLM compression. - This paper proposes techniques to reduce the memory footprint of DKM by orders of magnitude, enabling its application for efficient train-time LLM compression.Specifically, the central hypothesis appears to be:By applying cross-device tensor marshaling and weight uniquification/sharding, the memory footprint of DKM can be reduced sufficiently to enable train-time fine-tuning and compression of large LLMs, while still achieving superior accuracy vs compression trade-offs compared to other methods.The experiments aim to validate this hypothesis by showing 130x memory footprint reduction for DKM applied to LLaMA-7B compression, and improved accuracy over other 3-bit compression techniques.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a memory-efficient implementation of Differentiable KMeans Clustering (DKM) for large language model (LLM) compression, called eDKM. Specifically, the key ideas are:- Cross-device tensor marshaling to reduce redundant tensor copies between GPU and CPU during DKM training. This avoids duplicating tensors in CPU memory.- Weight uniquification and sharding by exploiting the fact that weights are 16-bit, so there are only 65536 possible values. This allows compressing the large attention maps in DKM into a smaller attention table and index list.- Sharding the index list over multiple GPUs to further reduce memory. Together these optimizations reduce the memory footprint of DKM by 130x for an LLM decoder layer. This makes it feasible to apply DKM for on-device LLM compression.- They demonstrate this by fine-tuning and 3-bit quantizing a 12.6GB LLaMA-7B model down to 2.5GB using eDKM, with good accuracy compared to other quantization schemes.So in summary, the main contribution is a very memory-efficient DKM implementation that can enable high-quality train-time LLM compression for on-device deployment.
