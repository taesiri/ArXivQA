# Language Models are Few-Shot Learners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How does scaling up the size of unsupervised language models affect their ability to rapidly learn new tasks and skills in a few-shot setting, without any fine-tuning?The key hypothesis seems to be that as language models are scaled up to larger sizes, their few-shot learning abilities will improve dramatically, allowing them to pick up new tasks from just a few examples or instructions. The paper explores this through training and evaluating GPT-3, an autoregressive 175 billion parameter language model, on a wide range of NLP datasets and novel tasks using zero-shot, one-shot, and few-shot evaluation protocols. The paper shows that GPT-3 displays significant gains in few-shot performance over smaller models, allowing it to approach or exceed fine-tuned models on certain tasks while seeing only a small number of examples. This provides evidence for the hypothesis that scaling up model size leads to improved few-shot learning abilities in language models. Evaluating this hypothesis systematically across a range of model sizes and tasks appears to be the core research question addressed by the paper.


## What is the main contribution of this paper?

The main contribution of this paper is presenting GPT-3, an autoregressive language model with 175 billion parameters, and evaluating its performance on a wide range of natural language processing tasks in few-shot, one-shot, and zero-shot settings. The key findings are:- GPT-3 achieves promising results on many NLP datasets in the zero-shot and one-shot settings, and in the few-shot setting is sometimes competitive with or even surpasses state-of-the-art results from fine-tuned models.- GPT-3 displays proficiency at tasks designed to test rapid adaptation, such as arithmetic, word scrambling, and using novel words after seeing them defined only once.- GPT-3 can generate synthetic news articles that humans have difficulty distinguishing from articles written by humans.- There is relatively smooth scaling in performance as model capacity increases across 3 orders of magnitude, suggesting meta-learning abilities steadily improve with scale. - The gap between zero-, one- and few-shot performance often increases with model capacity, indicating larger models are more proficient at in-context learning.- The authors systematically measure potential "data contamination" of benchmarks due to overlap with the pre-training dataset, and find minimal effects on most tasks.In summary, this paper demonstrates that scaling up capacity results in language models with significantly improved meta-learning abilities, enabling them to perform a wide range of NLP tasks from just a few demonstrations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper trains and evaluates GPT-3, an autoregressive language model with 175 billion parameters, on a range of natural language tasks in zero-shot, one-shot, and few-shot settings, finding that larger models show improved in-context learning abilities which allow them to perform well with just a task description and a few examples, sometimes rivaling fine-tuned models.
