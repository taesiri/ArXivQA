# [Message Passing Neural PDE Solvers](https://arxiv.org/abs/2202.03376)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop a fast, flexible, and accurate neural PDE solver that can generalize across different PDEs, domain topologies, discretizations, etc?The key points are:- The paper proposes an end-to-end neural PDE solver based on message passing neural networks. This offers flexibility to handle different spatial resolutions, timescales, domain topologies, boundary conditions, etc. - The paper introduces two techniques - temporal bundling and the pushforward trick - to encourage stability during training. This helps address a key challenge with training autoregressive models.- The model is designed to generalize across different parameterizations of a PDE class. At test time, it can take new PDE coefficients as input and still solve the equations accurately.- The message passing architecture is motivated by the insight that some classical numerical methods like finite differences and finite volumes can be seen as special cases of message passing.- Experiments demonstrate the model can handle 1D and 2D problems, model shocks, and generalize across domains, discretizations, and PDE parameters. The model outperforms baselines in accuracy and speed.So in summary, the key research contribution is developing a fast, flexible neural PDE solver that can generalize across a range of different problem specifications, addressing limitations of prior learned solvers. The core technical innovations are the message passing architecture and stability tricks for training.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. An end-to-end fully neural PDE solver based on neural message passing. This offers flexibility to satisfy structural requirements like spatial resolution, timescale, domain topology/geometry, boundary conditions, dimensionality, etc. The message passing architecture representationally contains some classical numerical methods like finite differences, finite volumes, and WENO schemes.2. Two methods - temporal bundling and the pushforward trick - to encourage stability when training the autoregressive neural solver model. Temporal bundling reduces error propagation by predicting multiple future timesteps together. The pushforward trick poses stability as a domain adaptation problem and adds an adversarial loss term. 3. Demonstrating generalization capability across different PDEs within a given class. The neural solver can take PDE coefficients and other attributes as input and solve new PDEs at test time without retraining.4. Validating the methods on 1D and 2D fluid flow problems with different topologies, equations, parameters, discretizations, etc. The model is shown to be fast, stable, and accurate compared to classical solvers, especially in the low resolution regime. It can also handle phenomena like shocks that are traditionally difficult to model numerically.In summary, the main contribution appears to be proposing a flexible neural message passing framework for building PDE solvers that can generalize across problem settings, along with innovations to make training the autoregressive models more stable.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper on message passing neural PDE solvers compares to other research in neural PDE solvers:- This work focuses on autoregressive models for solving PDEs, whereas much prior work has focused on neural operator methods. Autoregressive models iteratively predict solutions at the next timestep based on previous solutions, while neural operators directly map from initial conditions to solutions. - The paper argues that autoregressive models have been understudied compared to neural operators, and tries to address some of the challenges like stability and error propagation that have made them difficult to train. The temporal bundling and pushforward tricks are novel techniques aimed at improving training.- The message passing architecture allows flexibility in handling different spatial discretizations, domains, boundary conditions etc. Most prior neural operator methods fix the discretization a priori. The connections to classical numerical methods like finite differences and volumes are also insightful.- The experiments demonstrate broad applicability on various 1D and 2D fluid flow problems. Many prior works have focused on specific equations like Burgers' equation. Generalization to new PDEs within a class is also shown.- Compared to other autoregressive works like physics-informed neural networks, this uses a fully learned architecture rather than constraining it to known numerical solver structure. But it lacks the accuracy guarantees that more constrained architectures provide.- Overall, this paper pushes autoregressive models as a promising learning-based PDE solving paradigm, complements the neural operator focus in recent literature, and addresses some of the training challenges. But more work may be needed to match the accuracy and guarantees of hybrid numerical-learning techniques.In summary, the key novelties are in improving training of autoregressive models, demonstrating their flexibility, and showing strong empirical performance on a diverse set of PDE problems.
