# [Message Passing Neural PDE Solvers](https://arxiv.org/abs/2202.03376)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop a fast, flexible, and accurate neural PDE solver that can generalize across different PDEs, domain topologies, discretizations, etc?The key points are:- The paper proposes an end-to-end neural PDE solver based on message passing neural networks. This offers flexibility to handle different spatial resolutions, timescales, domain topologies, boundary conditions, etc. - The paper introduces two techniques - temporal bundling and the pushforward trick - to encourage stability during training. This helps address a key challenge with training autoregressive models.- The model is designed to generalize across different parameterizations of a PDE class. At test time, it can take new PDE coefficients as input and still solve the equations accurately.- The message passing architecture is motivated by the insight that some classical numerical methods like finite differences and finite volumes can be seen as special cases of message passing.- Experiments demonstrate the model can handle 1D and 2D problems, model shocks, and generalize across domains, discretizations, and PDE parameters. The model outperforms baselines in accuracy and speed.So in summary, the key research contribution is developing a fast, flexible neural PDE solver that can generalize across a range of different problem specifications, addressing limitations of prior learned solvers. The core technical innovations are the message passing architecture and stability tricks for training.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. An end-to-end fully neural PDE solver based on neural message passing. This offers flexibility to satisfy structural requirements like spatial resolution, timescale, domain topology/geometry, boundary conditions, dimensionality, etc. The message passing architecture representationally contains some classical numerical methods like finite differences, finite volumes, and WENO schemes.2. Two methods - temporal bundling and the pushforward trick - to encourage stability when training the autoregressive neural solver model. Temporal bundling reduces error propagation by predicting multiple future timesteps together. The pushforward trick poses stability as a domain adaptation problem and adds an adversarial loss term. 3. Demonstrating generalization capability across different PDEs within a given class. The neural solver can take PDE coefficients and other attributes as input and solve new PDEs at test time without retraining.4. Validating the methods on 1D and 2D fluid flow problems with different topologies, equations, parameters, discretizations, etc. The model is shown to be fast, stable, and accurate compared to classical solvers, especially in the low resolution regime. It can also handle phenomena like shocks that are traditionally difficult to model numerically.In summary, the main contribution appears to be proposing a flexible neural message passing framework for building PDE solvers that can generalize across problem settings, along with innovations to make training the autoregressive models more stable.
