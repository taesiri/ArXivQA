# [Re-ReND: Real-time Rendering of NeRFs across Devices](https://arxiv.org/abs/2303.08717)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we render a pre-trained Neural Radiance Field (NeRF) in real-time on resource-constrained devices like mobile phones and AR/VR headsets?

The key challenges are that NeRFs are computationally expensive to render due to their volumetric formulation and reliance on MLP networks. So the authors propose a method called Re-ReND to transform a pre-trained NeRF into a representation that can leverage standard graphics pipelines for fast rendering across devices.

The main hypotheses seem to be:

1) By distilling a NeRF into a mesh and set of matrices that encode a factorized light field, they can achieve real-time rendering speeds.

2) This distilled representation can preserve the visual quality of the original NeRF while being compatible with graphics pipelines like OpenGL/WebGL for deployment on resource-constrained devices.

3) Their proposed pipeline and representations can work robustly across a variety of NeRF models and challenging real-world scenes.

So in summary, the central research question is how to enable real-time rendering of NeRFs on mobile/AR/VR through a distillation and transformation process, while preserving visual quality and being widely deployable. The paper aims to demonstrate this is feasible through their proposed Re-ReND method.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of the paper "Re-ReND: Real-time Rendering of NeRFs across Devices" is proposing a novel method called Re-ReND that enables real-time rendering of pre-trained Neural Radiance Fields (NeRFs) on resource-constrained devices like mobile phones and AR/VR headsets. 

Specifically, the key contributions are:

1. The Re-ReND method transforms a pre-trained NeRF into a representation that is compatible with standard graphics pipelines like OpenGL and WebGL. This allows leveraging efficient mesh rasterization for fast rendering across devices. 

2. Re-ReND distills the NeRF by extracting geometry into a mesh and color information into a set of matrices that encode a factorized light field. This allows computing pixel colors with a single query and no MLP evaluations.

3. Comprehensive experiments demonstrate Re-ReND can achieve 2.6x speedups versus prior art like MobileNeRF on real-world scenes, with comparable image quality. The method also enables real-time rendering on mobiles and VR headsets not possible before.

In summary, the main contribution is proposing and experimentally validating the Re-ReND method to unlock real-time rendering of NeRFs on resource-constrained devices by transforming them into graphics-friendly representations. The key ideas are distilling geometry and appearance into meshes and factorized light fields amenable to fast rasterization.
