# [Re-ReND: Real-time Rendering of NeRFs across Devices](https://arxiv.org/abs/2303.08717)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we render a pre-trained Neural Radiance Field (NeRF) in real-time on resource-constrained devices like mobile phones and AR/VR headsets?

The key challenges are that NeRFs are computationally expensive to render due to their volumetric formulation and reliance on MLP networks. So the authors propose a method called Re-ReND to transform a pre-trained NeRF into a representation that can leverage standard graphics pipelines for fast rendering across devices.

The main hypotheses seem to be:

1) By distilling a NeRF into a mesh and set of matrices that encode a factorized light field, they can achieve real-time rendering speeds.

2) This distilled representation can preserve the visual quality of the original NeRF while being compatible with graphics pipelines like OpenGL/WebGL for deployment on resource-constrained devices.

3) Their proposed pipeline and representations can work robustly across a variety of NeRF models and challenging real-world scenes.

So in summary, the central research question is how to enable real-time rendering of NeRFs on mobile/AR/VR through a distillation and transformation process, while preserving visual quality and being widely deployable. The paper aims to demonstrate this is feasible through their proposed Re-ReND method.
