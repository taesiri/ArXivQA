# CTRL: A Conditional Transformer Language Model for Controllable   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing a large-scale conditional language model called CTRL that can generate coherent and controllable text. The key ideas explored in the paper are:- Training a transformer-based language model on a diverse corpus of text data conditioned on control codes that govern aspects like style, content, and task-specific behavior. - Deriving the control codes from natural structure in the training data (like URLs, document boundaries) to allow more explicit control over text generation while preserving the advantages of unsupervised pre-training.- Releasing multiple full-sized pretrained versions of CTRL to enable controllable text generation and analyzing correlations learned by the language model.So in summary, the central hypothesis seems to be that conditioning the language model training on control codes derived from the natural structure of diverse text data can enable more explicit control over text generation while still benefiting from unsupervised pre-training. The paper aims to demonstrate this through the released CTRL models.


## What is the main contribution of this paper?

The main contribution of this paper is introducing CTRL, a large-scale conditional language model. Key points:- CTRL is a 1.63 billion parameter conditional transformer language model trained on control codes that govern style, content, task behavior, etc. - The control codes allow for more explicit control over text generation compared to unconditional language models like GPT-2.- The codes are derived from natural structure in the training data to preserve the advantages of unsupervised learning.- CTRL can be used for controlled text generation, crossing domains and behaviors.- The relationship between codes and training data allows CTRL to predict the training source most likely for a given sequence, enabling analysis of data.- The model incorporates task-specific data for question answering and translation without harming general capabilities.- The authors have released multiple full-sized pretrained versions of CTRL to enable further research into controllable text generation.In summary, the main contribution is proposing CTRL, a very large conditional language model that allows for more explicit control over text generation through the use of control codes. The codes provide a way to guide the model while retaining the benefits of unsupervised pretraining.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper introduces CTRL, a 1.63 billion parameter conditional transformer language model trained on control codes to generate controllable text. The key contribution is adding control codes to the training data to allow more explicit human control over text generation while preserving the advantages of unsupervised pretraining.
