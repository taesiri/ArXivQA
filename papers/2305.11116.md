# LLMScore: Unveiling the Power of Large Language Models in Text-to-Image
  Synthesis Evaluation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is:How can we develop an automatic evaluation metric for text-to-image synthesis that better captures alignment between images and text at both the image-level and object-level compositionality? The key hypotheses appear to be:1) Existing automatic evaluation metrics for text-to-image synthesis, like CLIPScore and BLIP, are limited in their ability to capture fine-grained, object-level alignment between images and text.2) By incorporating both global image descriptions and local object-level descriptions into large language models (LLMs), we can develop a metric that better evaluates multi-granularity text-image alignment. 3) This LLM-based metric can follow different evaluation instructions (like overall quality or error counting) and produce human-correlated scores with rationales.In summary, the central goal is developing a superior automatic metric for evaluating text-to-image models, with a focus on capturing compositional alignment between images and text. The key hypotheses are that using LLMs and multi-granularity descriptions can achieve this.
