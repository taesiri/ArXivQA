# LLMScore: Unveiling the Power of Large Language Models in Text-to-Image   Synthesis Evaluation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is:How can we develop an automatic evaluation metric for text-to-image synthesis that better captures alignment between images and text at both the image-level and object-level compositionality? The key hypotheses appear to be:1) Existing automatic evaluation metrics for text-to-image synthesis, like CLIPScore and BLIP, are limited in their ability to capture fine-grained, object-level alignment between images and text.2) By incorporating both global image descriptions and local object-level descriptions into large language models (LLMs), we can develop a metric that better evaluates multi-granularity text-image alignment. 3) This LLM-based metric can follow different evaluation instructions (like overall quality or error counting) and produce human-correlated scores with rationales.In summary, the central goal is developing a superior automatic metric for evaluating text-to-image models, with a focus on capturing compositional alignment between images and text. The key hypotheses are that using LLMs and multi-granularity descriptions can achieve this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing LLMScore, a new framework to evaluate the alignment between synthesized images and text prompts in text-to-image synthesis. LLMScore leverages large language models (LLMs) to capture multi-granularity compositionality between images and text. Specifically, the key aspects of the contribution are:- Introducing a pipeline to transform images into multi-granularity (image-level and object-level) visual descriptions using vision models and LLMs. This allows capturing compositionality in images.- Employing LLMs to follow evaluation instructions (overall quality or error counting) to measure alignment between visual descriptions and text prompts. This produces human-correlated scores. - Providing rationales along with the scores that offer interpretability into the evaluation process.- Demonstrating state-of-the-art correlation with human judgments on diverse datasets without additional training. The proposed LLMScore outperforms existing metrics like CLIPScore and BLIP by large margins.In summary, the main contribution is a novel LLM-based framework LLMScore to evaluate text-to-image synthesis through interpretable scores and rationales that align better with human assessment, especially for compositionality, compared to existing metrics.
