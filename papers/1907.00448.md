# [Self-Supervised Dialogue Learning](https://arxiv.org/abs/1907.00448)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we utilize the sequential order of utterances as a self-supervised signal to improve dialogue learning?The key points are:- The sequential order of utterances contains important information for coherent dialogues. However, most existing neural dialogue systems do not explicitly model or utilize this order information.- The paper proposes a new self-supervised task called "inconsistent order detection" to explicitly capture the order flow in dialogues. The task is to predict whether a sampled utterance triple is ordered correctly or not.- A sampling-based self-supervised network (SSN) is proposed to solve this task. It samples reference utterance triples from history to provide context and avoid the forgetfulness problem when encoding long dialogues.- The SSN is incorporated into existing dialogue learning frameworks through adversarial training, so it provides an order-based signal to improve coherence and relevance of generated responses.- Experiments show SSN helps advance state-of-the-art in both open-domain and task-oriented dialogue scenarios by better utilizing the order information.In summary, the key hypothesis is that modeling order as a self-supervised signal can improve neural dialogue learning. The SSN and training frameworks are proposed to test this hypothesis.
