# [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language   Models via Complexity Classes](https://arxiv.org/abs/2312.14890)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current benchmarks for evaluating reasoning abilities of large language models (LLMs) have limitations: they do not rigorously characterize reasoning capabilities, risk models overfitting to benchmarks, and sometimes rely on manual evaluation. 
- It is interesting to examine how well LLMs can solve problems classified as NP-hard or NP-complete in computational complexity theory.

Proposed Solution:
- The paper introduces a new benchmark called NPHardEval to evaluate reasoning skills of LLMs using 900 algorithmic questions up to NP-Hard complexity class. 
- The benchmark covers reasoning tasks from polynomial time (P), NP-complete, and NP-hard complexity classes, with each class split into 10 difficulty levels. Tasks are chosen to represent diverse real-world optimization problems and avoid intensive numerical computation.
- The benchmark features automated evaluation without human verification and dynamic updating of questions monthly to reduce overfitting risks.

Key Contributions:
- First complexity class based reasoning benchmark for LLMs spanning wide spectrum of algorithmic reasoning tasks
- Quantifies LLM performance across task complexity levels from P to NP-Hard
- Compares reasoning abilities and learnsbility between closed-source and open-source LLMs
- Addresses problem of models mimicking vs genuinely learning complex problem solving skills
- Provides roadmap to advance LLM reasoning through robustness testing, iterative self-correction, and multi-agent collaboration

Overall the paper makes significant headway in rigorously evaluating and advancing reasoning capabilities of LLMs using principles of computational complexity theory. The proposed benchmark and experiments reveal current skills, limitations, and pathways for improvement across diverse algorithmic tasks.
