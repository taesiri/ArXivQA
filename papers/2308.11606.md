# StoryBench: A Multifaceted Benchmark for Continuous Story Visualization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it seeks to address is:How can we create a comprehensive benchmark to reliably measure progress in video generation from sequences of text prompts (i.e. stories) over time?The key points are:- The paper proposes a new benchmark called StoryBench for evaluating text-to-video models on their ability to generate continuous, natural videos from sequences of text prompts. - Current video datasets only have single captions describing an entire video, whereas StoryBench contains detailed per-action captions with timestamps forming coherent stories.- The benchmark consists of three datasets, three tasks of increasing difficulty, and three evaluation setups (zero-shot, single-task, multi-task) for thorough assessment.- The authors collect rich human annotations including action descriptions, timestamps, mouse traces, and failure mode labels on thousands of open-domain videos.- They also devise an automatic pipeline to transform existing video captions into detailed stories to create better training data.- The paper establishes compact baselines, human evaluation guidelines, and proposes automatic metrics to enable reproducible comparisons going forward.In summary, the key research question is how to design a comprehensive, multifaceted benchmark to drive progress and reliable evaluation of text-to-video models on the challenging task of continuous story visualization.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contributions of this paper appear to be:1) Introducing StoryBench, the first benchmark for continuous story visualization from text prompts. It contains rich human annotations and allows evaluating text-to-video models on different tasks, datasets, and evaluation setups.2) Devising an automatic procedure to transform existing video captions into sequences of action descriptions to create better training data.3) Establishing strong baselines, human evaluation guidelines, and automatic metrics for the benchmark. 4) Conducting experiments that show the benefits of fine-tuning text-to-video models for video continuation on story-like data. The results also reveal discrepancies between human and automated evaluation.5) Providing data, code, and guidelines to inspire more research on real-world, controllable text-to-video generation of arbitrary length through a comprehensive and reproducible setup.In summary, the main contribution seems to be the introduction of StoryBench, a new multifaceted benchmark for evaluating text-to-video models on continuous story visualization, enabled by rich human annotations and training data curation. The paper also provides baselines and evaluation methodologies for the community to build upon.
