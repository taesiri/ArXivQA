# [Improving Language Plasticity via Pretraining with Active Forgetting](https://arxiv.org/abs/2307.01163)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can incorporating an active forgetting mechanism during pretraining improve the language plasticity of pretrained language models, allowing them to more efficiently adapt to new languages, especially in low-data regimes?The key hypothesis is that by resetting the embedding layer periodically during pretraining, the model will be forced to repeatedly relearn good token embeddings, making it more adept at quickly learning new embeddings for new languages later on. This is analogous to a meta-learning process. The authors propose a simple "active forgetting" method where they reset the token embeddings (but not the rest of the model) every K updates during pretraining. They compare models pretrained this way ("forgetting PLMs") to standard pretrained models without forgetting.The main research questions they aim to answer are:1) Can forgetting PLMs learn new languages better with limited adaptation data? (RQ1)2) Do forgetting PLMs converge faster when adapting to new languages? (RQ2) 3) Does the similarity between the new language and English impact the benefits of forgetting? (RQ3)So in summary, the central hypothesis is that incorporating active forgetting during pretraining will improve language plasticity, enabling more efficient adaptation to new languages later, especially in low-resource scenarios. The key research questions focus on sample efficiency, convergence speed, and effects on dissimilar languages.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contribution of this paper is proposing a novel "active forgetting" mechanism during pretraining of language models, which helps create PLMs (pretrained language models) that can quickly adapt to new languages, especially low-resource languages. Specifically, the key ideas are:- Incorporating an active forgetting mechanism during pretraining, where the embedding layer is periodically reset while the rest of the model is kept unchanged. - This "forgetting pretraining" encourages the PLM to improve its ability to rapidly learn new embeddings within a limited number of updates.- Experiments with RoBERTA show models pretrained this way not only converge faster during language adaptation, but also outperform standard pretrained models in low-resource scenarios and for distant languages.So in summary, the main contribution is introducing a simple yet effective "active forgetting" technique during pretraining to improve language plasticity and enable efficient adaptation of PLMs to new languages, especially low-resource and distant languages. The forgetting mechanism seems to have a meta-learning effect, making the PLMs more robust and quick to learn new token embeddings.
