# [Improving Language Plasticity via Pretraining with Active Forgetting](https://arxiv.org/abs/2307.01163)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can incorporating an active forgetting mechanism during pretraining improve the language plasticity of pretrained language models, allowing them to more efficiently adapt to new languages, especially in low-data regimes?The key hypothesis is that by resetting the embedding layer periodically during pretraining, the model will be forced to repeatedly relearn good token embeddings, making it more adept at quickly learning new embeddings for new languages later on. This is analogous to a meta-learning process. The authors propose a simple "active forgetting" method where they reset the token embeddings (but not the rest of the model) every K updates during pretraining. They compare models pretrained this way ("forgetting PLMs") to standard pretrained models without forgetting.The main research questions they aim to answer are:1) Can forgetting PLMs learn new languages better with limited adaptation data? (RQ1)2) Do forgetting PLMs converge faster when adapting to new languages? (RQ2) 3) Does the similarity between the new language and English impact the benefits of forgetting? (RQ3)So in summary, the central hypothesis is that incorporating active forgetting during pretraining will improve language plasticity, enabling more efficient adaptation to new languages later, especially in low-resource scenarios. The key research questions focus on sample efficiency, convergence speed, and effects on dissimilar languages.
