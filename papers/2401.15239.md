# [MEA-Defender: A Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2401.15239)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Valuable deep neural network (DNN) models are being increasingly deployed on cloud platforms. However, such models face the threat of model extraction attacks, where adversaries can query the model to obtain inputs and outputs, and train a substitute model. Most existing watermarking techniques fail to protect the intellectual property (IP) of DNN models against such attacks.  

Proposed Solution:
The paper proposes a novel watermarking approach called MEA-Defender to protect DNN models against model extraction attacks. The key idea is to design a "symbiotic backdoor" watermark such that the input domain and output feature domain of the watermark samples fall within the distribution of the main task samples. This ensures the watermark gets extracted along with the main task during model extraction. Specifically:

- The watermark samples are synthesized by combining features from two source class samples in the input domain. This keeps the watermark input distribution similar to the main task distribution.

- A watermark loss function is designed to minimize the output distribution difference between watermark samples and source class samples using KL divergence. This keeps watermark outputs similar to main task outputs.

- An evasion loss prevents random mixed samples from triggering the watermark.

- Multi-task learning is used to balance optimization of main task, watermark task and evasion loss.

Main Contributions:

- Proposes first generic black-box watermarking approach to protect IP of both supervised and self-supervised DNNs against model extraction attacks, without assuming knowledge of model architectures.

- Designs a novel symbiotic backdoor watermark based on input and output domain distributions to ensure embedding into extracted models.

- Demonstrates high robustness against 4 types of extraction attacks on 6 models using 5 datasets, with 83.25% average watermark success rate on extracted models.

- Shows robustness against watermark removal attacks like fine-tuning, pruning etc. after model extraction.

In summary, the paper makes significant contributions in enabling IP protection for DNNs against model extraction attacks using a clever watermarking approach.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a robust watermarking approach called MEA-Defender that embeds a symbiotic backdoor into deep neural networks to protect their intellectual property against model extraction attacks.


## What is the main contribution of this paper?

 According to the meta-review, the main contribution of this paper is proposing a novel watermarking technique for machine learning models that produces "watermark samples" by combining multiple samples but altering their label. The paper shows through extensive experiments that this watermarking method is robust and persists through model extraction attacks and watermark removal attempts. The meta-review summarizes the main contribution as "Provides a Valuable Step Forward in an Established Field" by developing a novel watermarking method that advances the state-of-the-art in an area that has been well-studied.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, here are some potential key terms and keywords associated with this paper:

- Watermarking
- Deep neural networks (DNNs) 
- Intellectual property (IP) protection
- Model extraction attacks
- Symbiotic backdoor
- Combination watermark
- Supervised learning
- Self-supervised learning
- Black-box watermark
- Watermark success rate
- Model stealing
- Knowledge distillation

The main focus of the paper seems to be on proposing a new robust watermarking approach called "MEA-Defender" to protect the intellectual property of deep neural networks against model extraction attacks. It utilizes a symbiotic backdoor to ensure the watermark persists through such attacks. The approach is applicable to both supervised learning and self-supervised learning models. Overall, the key terms revolve around watermarking deep learning models, defending against model extraction, using combination/symbiotic watermarks, and evaluating the success rate of the watermark on stolen models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. What is the key intuition behind the proposed "symbiotic backdoor" design for the watermark? Why does forcing the input/output distributions of the watermark samples to match those of the main task improve robustness against model extraction attacks?

2. The paper claims the proposed method works for both supervised and self-supervised learning models. What modifications need to be made in the loss functions used during watermark embedding to handle these two cases?

3. How exactly is the combination loss ($L_{com}$) defined? Explain the intuition behind using KL divergence to measure the output distribution difference between watermark samples and source class samples. 

4. What is the purpose of the evasion loss ($L_{eva}$) term? Why is it important to prevent random mixed samples from triggering the watermark? Could omitting this term lead to security issues?

5. The paper utilizes a multi-task learning technique called MGDA to balance the different loss terms. Why is a specialized approach needed here rather than just using fixed weighting coefficients? What problems can arise from not properly balancing these losses?

6. What differences would you expect in the robustness of watermarks embedded using the FromScratch versus Pretrained approaches described? Why does FromScratch tend to work better?

7. How exactly could an attacker attempt to detect the presence of the watermark by generating random mixed samples? Why is the evasion loss important to prevent this? 

8. What assumptions does the threat model make about the adversary's capability? How might a weaker or stronger adversary impact the security claims made about this watermarking approach?

9. The paper claims the method works even when the architecture of the extracted model differs from the victim model. Intuitively explain why this architecture-agnostic robustness is achieved.

10. If you were to implement an adaptive attack to try removing this watermark, what approach would you take? What vulnerabilities in the watermark design would you try exploiting?
