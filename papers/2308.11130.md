# [Efficient View Synthesis with Neural Radiance Distribution Field](https://arxiv.org/abs/2308.11130)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is: How can we achieve efficient, real-time view synthesis from arbitrary viewpoints with low computational and memory costs?The authors aim to overcome the limitations of prior work on Neural Radiance Fields (NeRF) and Neural Light Fields (NeLF) to enable fast rendering for novel view synthesis while using compact network models.Specifically, the limitations they identify are:- NeRF has slow rendering due to requiring multiple network evaluations per pixel. Approaches to accelerate NeRF come with extra computational and memory costs. - NeLF allows one network evaluation per pixel but requires much larger networks to achieve quality comparable to NeRF, limiting efficiency.To address these challenges, the paper proposes a new scene representation called Neural Radiance Distribution Fields (NeRDF) that enables efficient view synthesis by:- Modeling radiance distributions along rays to incorporate 3D geometry prior like NeRF, allowing smaller networks. - Requiring only one network evaluation per pixel like NeLF, for fast rendering.The key hypothesis is that modeling parameterized radiance distributions will allow compact networks like NeRF while retaining the efficiency of NeLF. Experiments demonstrate NeRDF achieves real-time rendering speeds with visual quality similar to NeRF using small networks.In summary, the paper aims to achieve a better balance between quality, speed, and memory cost than prior work via the proposed NeRDF scene representation and learning approach. Delivering real-time view synthesis with low compute is the main goal addressed.


## What is the main contribution of this paper?

This paper proposes a new neural representation called Neural Radiance Distribution Field (NeRDF) for efficient view synthesis. The key contributions are:- NeRDF models the radiance distribution along each ray using a set of trigonometric functions. This allows capturing 3D spatial information similar to NeRF while requiring only a single network query per pixel like NeLF. - A method to learn a compact NeRDF network using distillation from a teacher NeRF. This involves matching both the rendered color values and volume density distributions.- Novel designs like online view sampling and ray encoding to enhance the distillation and compactness of NeRDF.- Experiments show NeRDF achieves a good balance of quality, speed, and memory. It attains ~100x speedup over NeRF while using a similarly small network, with only a minor drop in visual quality.In summary, NeRDF combines the benefits of volumetric rendering and compact network size from NeRF, and the efficient inference of NeLF. It offers an effective solution for real-time view synthesis by predicting parametric radiance distributions along rays using a single network forward pass.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new 3D scene representation called Neural Radiance Distribution Field (NeRDF) that enables real-time and efficient view synthesis from small neural networks by modeling radiance distributions along rays using frequency basis functions.
