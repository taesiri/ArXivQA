# [Curriculum Learning with Adam: The Devil Is in the Wrong Details](https://arxiv.org/abs/2308.12202)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Why have curriculum learning (CL) methods achieved only limited success in natural language processing (NLP), despite being successful in other areas like computer vision and reinforcement learning? The key hypothesis explored in the paper is that the mixed results of CL in NLP may be related to the widespread use of the Adam optimization algorithm in the field. Specifically, the authors hypothesize that curricula often interact with Adam in a way that produces suboptimal results, unless Adam's hyperparameters are properly tuned.The paper investigates this hypothesis through several experiments that analyze the interactions between curricula and Adam. The authors test their hypothesis with different CL methods, including automated curricula like the commentaries framework and more simple handcrafted curricula. Across these methods, they find evidence that curricula can produce artificial gains in learning speed due to unintended interactions with Adam, rather than beneficial ordering of training data. With properly tuned Adam hyperparameters, vanilla Adam without curriculum learning tends to perform as well or better.In summary, the central research question is why CL has shown limited gains in NLP, and the key hypothesis is that problematic interactions with Adam may explain these limited results when hyperparameters are not optimized. The authors perform experiments to provide evidence for this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is showing that curriculum learning (CL) methods often work not due to creating a beneficial data ordering, but rather through unintended interactions with the Adam optimization algorithm. Specifically:- The paper first conducts a case study on an automated CL method called "commentaries" from computer vision, showing it can also work for NLP tasks. However, a deeper analysis reveals the performance gains are brittle and actually stem from curriculum-Adam interactions that scale up parameter updates, not a sound curriculum strategy. - The paper then shows these curriculum-Adam interactions also occur with common hand-crafted CL methods in NLP that order data by difficulty measures like sequence length. The interactions again scale up parameter updates when the curriculum distribution shifts most rapidly.- Simply tuning Adam's hyperparameters properly allows it to match or outperform the CL methods in all experiments. The paper thus argues the CL benefits are illusory and originate from compensating for suboptimal Adam settings rather than better data ordering.In summary, the key contribution is providing an extensive empirical analysis showing common CL methods in NLP are significantly less effective than believed due to unintended curriculum-Adam interactions, rather than beneficial data ordering. The results urge caution in claiming positive CL results without rigorous controls.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The TL;DR of this paper is that curriculum learning methods often do not actually improve learning, but rather they interact with the Adam optimization algorithm in a way that compensates for suboptimal hyperparameters. The authors show that with properly tuned hyperparameters, vanilla Adam without curriculum learning performs as well or better.
