# [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)

## What is the central research question or hypothesis that this paper addresses?

 The paper introduces a method called "speculative decoding" to accelerate inference from large autoregressive models like Transformers, without changing the model architecture or outputs. 

The key hypothesis is that hard language modeling tasks often include both easier and harder subtasks. The easier subtasks can be approximated well by more efficient models. By using these efficient models to speculatively suggest completions in parallel, and validating them with the target model, inference can be accelerated.

Specifically, the central hypothesis is that by using speculative execution and a novel sampling method called "speculative sampling", inference from large autoregressive models can be made faster by computing several tokens in parallel. This allows accelerating existing models without retraining or changing architectures or outputs.

So in summary, the main research question is whether speculative decoding with efficient approximation models can accelerate inference from large autoregressive models like Transformers, while maintaining identical outputs. The key hypothesis is that the combination of speculative execution and speculative sampling enables this acceleration.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing a method called "speculative decoding" to accelerate inference from large autoregressive models like Transformers, without changing the model architecture, training procedure, or output distribution. 

Specifically, the key ideas are:

- Use a smaller "approximation" model to quickly generate speculative token guesses.

- Run the large "target" model in parallel on the speculative guesses to verify them. 

- Accept verified guesses to reduce the number of serial runs of the target model.

- Adjust the target model's distribution before sampling new tokens when guesses are rejected, to maintain the original output distribution.

- Show theoretically and empirically that this "speculative decoding" method can accelerate inference from large autoregressive models like Transformers by 2-3x without changing outputs.

In summary, the main contribution is using "speculative execution" techniques like parallel guessing and verification with smaller models to speed up sampling from large Transformer models, in a way that provably maintains the original output distribution. This is done without model architecture changes, retraining, or losing output quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces speculative decoding, a method to accelerate inference from large autoregressive models like Transformers by generating multiple tokens in parallel using speculative execution, without changing the model architecture, training procedures, or output distributions.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on fast inference from large autoregressive models like Transformers:

- It proposes a novel method called "speculative decoding" that uses speculative execution to run multiple inference steps of a target model in parallel. This allows decoding multiple tokens concurrently without changing the output distribution. 

- Most prior work on accelerating Transformer inference requires changing the model architecture, retraining, or accepting some changes to the outputs. A key benefit of this method is it works on existing models without any modifications.

- It leverages the idea that language modeling tasks contain a mix of "easy" and "hard" steps. Easier steps can be approximated by smaller models to generate guesses for the slower target model. This is similar conceptually to adaptive computation methods.

- The theoretical analysis quantifies the potential speedup from speculative decoding in terms of properties of the target and approximation models. This provides insight into how to select good approximation models.

- Empirically it shows significant wall-time speedups (2X-3X) by accelerating inference on large T5 models without any change to outputs. This is validated on machine translation and summarization tasks.

- Speculative decoding is complementary to other techniques like model distillation or quantization that improve efficiency equally for all inputs. It focuses specifically on increasing concurrency.

Overall, this paper introduces a novel way to leverage speculative execution to improve Transformer inference latency. A key advantage is accelerating existing models without modification. The analysis also provides theoretical grounding for the approach.


## What future research directions do the authors suggest?

 The authors of the paper suggest several future research directions:

- Investigating the compatibility of speculative decoding with beam search. The authors note that their method can be applied to beam search, but with some performance penalty. Analyzing speculative decoding in the beam search setting more thoroughly is an area for future work.

- Exploring custom approximation models optimized for speculative decoding, such as models with non-autoregressive architectures, distillation, or training objectives that directly optimize the acceptance rate. The paper tested existing off-the-shelf smaller Transformers as approximation models, but custom models may lead to better performance.

- Testing hierarchical versions where the approximation model is itself sped up by an even faster model, allowing for more capable approximation models. 

- Varying the approximation model and number of guesses during inference, instead of fixing them. The authors note this could yield additional speedups.

- Applying the method to modalities beyond text, like images, to see if the benefits transfer.

- Exploring applications of the more general idea of stochastic speculative execution outside of autoregressive model decoding. For example, in physics simulations or reinforcement learning where one model generates a distribution over actions for another slower model.

In summary, the main future directions are testing the method in more contexts like beam search, developing customized approximation models, exploring hierarchical versions, dynamically adapting the approximation model at inference time, applying it to new modalities, and extending stochastic speculative execution more broadly. The authors lay out speculative decoding as a general acceleration technique with much room for further refinement and application.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper introduces a technique called speculative decoding to accelerate inference from large autoregressive models like Transformers. The core ideas are (1) hard language modeling tasks often contain both easier and harder steps, (2) cheaper approximation models can generate guesses for the harder steps, and (3) speculative execution allows verifying these guesses in parallel to gain speedups. The method uses a novel sampling algorithm called speculative sampling to guarantee identical outputs to standard sampling. Experiments on tasks like translation and summarization with models like T5-XXL and LaMDA show empirical speedups of 2-3X without changing outputs or models. The approach provides an easy way to accelerate existing models without modifications.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a technique called speculative decoding to accelerate inference from large autoregressive models like Transformers. The key idea is to use a smaller, more efficient approximation model to generate speculative guesses for multiple tokens in parallel. These guesses are then validated by the larger target model, accepting those that maintain the original output distribution. This allows the larger model to generate multiple tokens concurrently since the speculative guesses from the approximation model give it a head start. 

The authors show how to do this sampling correctly so that the output distribution is mathematically guaranteed to be unchanged. They analyze the expected speedup and show empirical results demonstrating 2-3x faster inference on T5 models with no change in outputs. The method does not require retraining models or changing model architectures. It provides an easy way to accelerate existing models by leveraging the observation that hard tasks contain both easy and hard inference steps that can be approximated and computed in parallel.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a fast inference method for large autoregressive models called speculative decoding. The key idea is to use a smaller, more efficient approximation model to speculate a few tokens in parallel. The larger target model then evaluates these speculated tokens, accepting the ones consistent with its own distribution. This allows generating multiple tokens in parallel with the slower target model, accelerating decoding while guaranteeing an identical output distribution. The method can accelerate existing models without retraining or architecture changes.

The main steps are:
1) Use a fast approximation model to speculatively guess multiple continuation tokens. 
2) Run the slow target model in parallel to evaluate probabilities of the guessed tokens.
3) Accept guesses that have probability under the target less than or equal to the approximation, and reject others.  
4) Sample a new token from the target for first rejected guess to correct.
5) Return accepted guesses concatenated with the new sample as the final output.

By speculating multiple guesses in parallel, the slower target model can generate multiple tokens concurrently, speeding up inference significantly without changing the output distribution.


## What problem or question is the paper addressing?

 The paper is addressing the problem of slow inference from large autoregressive models like Transformers. Specifically, it points out that decoding from these models is very slow, since generating each token requires running the model serially. This is problematic as large autoregressive models like Transformers have become very important in natural language processing.

The key questions the paper tries to address are:

1. How can we speed up inference from large autoregressive models like Transformers without changing the model architecture or training procedure and without altering the output distribution? 

2. Can we leverage the observation that language modeling tasks often contain a mixture of easier and harder subtasks? Specifically, can we use more efficient models to generate suggestions for the slower target model in cases where the task is easier?

3. How can we generate multiple tokens in parallel from the target model using suggestions from the approximation model, while still guaranteeing the overall distribution remains unchanged?

So in summary, the paper introduces a method called "speculative decoding" to accelerate inference from large autoregressive models by generating multiple tokens in parallel, using suggestions from more efficient approximation models for easier cases, while ensuring the overall distribution is unchanged.


## What are the keywords or key terms associated with this paper?

 Based on skimming the paper, some of the key keywords and terms seem to be:

- Speculative decoding - The main technique proposed to speed up inference from autoregressive models like Transformers without changing the outputs. Involves using a fast approximation model to make speculative guesses in parallel, and verifying them with the target model.

- Transformers - The type of large autoregressive models that speculative decoding aims to accelerate.

- Autoregressive models - Models that generate outputs token-by-token, with each token conditioned on previous ones. Makes parallel decoding challenging.  

- Approximation models - Smaller, faster models used by speculative decoding to make guesses in parallel about the target model's outputs.

- Acceptance rate - The probability that speculative decoding will accept a guess from the approximation model, allowing faster decoding. Depends on how well the approximation model matches the target.

- Identical output distribution - A key property of speculative decoding is that it maintains the exact same output distribution as the target model, unlike other acceleration methods.

- Walltime improvement - The main evaluation metric, measuring how much faster speculative decoding makes inference compared to standard serial decoding.

- Concurrency vs computation - Speculative decoding improves walltime by increasing concurrency but also increases computational cost. Helpful when communication is the bottleneck.

So in summary, the core ideas are using speculative execution with approximation models to accelerate inference from Transformers and similar autoregressive models, while preserving output quality. The focus is on walltime speedup rather than computational efficiency.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main topic and focus of the paper? What problem is it trying to solve?

2. What are the key contributions or main findings presented in the paper? What are the main takeaways?

3. What methods, models, or techniques does the paper propose or utilize? How do they work?

4. What datasets were used for experiments/evaluation? What were the key results on these datasets?

5. How does the proposed approach compare to prior or existing methods in this area? What are the advantages and limitations?

6. What conclusions or future directions does the paper suggest based on the results?

7. What are the theoretical underpinnings or mathematical formulations behind the proposed methods?

8. What assumptions does the paper make? Are there any potential limitations of the approach discussed?

9. Who are the intended users or beneficiaries of this research? What are the potential real-world applications?

10. What open questions or areas for improvement does the paper point to for future work?
