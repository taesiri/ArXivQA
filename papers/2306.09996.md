# Investigating Prompting Techniques for Zero- and Few-Shot Visual   Question Answering

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:What prompting techniques are effective for improving zero-shot and few-shot visual question answering (VQA) performance in large vision-language models like BLIP2?The key aspects investigated are:- The impact of different question templates on steering the model's answer generation.- The role of incorporating text-only few-shot QA examples as additional context. - The benefits of using image captions as extra visual cues when combined with few-shot examples.- The effectiveness of prompting chain-of-thought reasoning to generate step-by-step rationales.The overarching goal seems to be exploring simple but effective prompting strategies to better utilize large pre-trained models like BLIP2 for VQA, without requiring task-specific fine-tuning. The paper examines the above techniques on challenging VQA datasets to provide insights into advancing zero- and few-shot VQA.In summary, the central research question focuses on investigating prompting methods to improve the zero-shot and few-shot VQA capabilities of models like BLIP2. The key techniques explored are question templates, few-shot examples, image captions, and chain-of-thought reasoning prompts.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. An investigation of various prompting techniques for improving zero-shot and few-shot visual question answering (VQA) performance of the BLIP2 model, without any task-specific fine-tuning. 2. An exploration of the effectiveness of different question templates, text-only few-shot examples, image captions as additional context, and chain-of-thought (CoT) prompting for rationalization.3. An evaluation of these prompting techniques on challenging VQA datasets that require compositional reasoning and knowledge-based reasoning, where BLIP2 struggles in zero-shot settings.4. Key findings showing the instruction-tuned FLAN-T5 language model has moderate sensitivity to question templates, while OPT exhibits higher sensitivity. Few-shot examples hurt VQA performance unless paired with caption or CoT examples. Captions consistently improve VQA accuracy when combined with few-shot QA pairs. However, CoT reasoning decreases VQA accuracy.5. The insights provided on how to better utilize large pre-trained vision-language models like BLIP2 for zero- and few-shot VQA via prompting strategies.In summary, the main contribution appears to be a comprehensive investigation and novel findings on effective prompting techniques to improve zero- and few-shot VQA performance for models like BLIP2, without expensive task-specific fine-tuning.
