# [TrojFair: Trojan Fairness Attacks](https://arxiv.org/abs/2312.10508)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "TrojFair: Trojan Fairness Attacks":

Problem Statement:
Ensuring fairness in deep learning models is crucial as they are being deployed in high-stakes applications like healthcare, employment, etc. However, the resilience of these fair models against adversarial attacks has not been thoroughly studied. Specifically, existing attacks either require explicit group attribute information during inference or result in significant drops in accuracy. Additionally, models compromised by current attacks can be easily detected by fairness evaluation tools since they produce biased outputs even on clean test data.

Proposed Solution - TrojFair:
This paper proposes a new attack framework called TrojFair that crafts a stealthy and effective fairness attack. The key idea is to have the model behave fairly on clean data but act in a discriminatory way on specific groups when inputs contain a certain trigger. TrojFair has 3 main modules:

1) Target-Group Poisoning: Inserts trigger and changes labels to target class only for the chosen target group samples. Helps achieve high attack success rate (ASR) for target group.  

2) Non-Target Group Anti-Poisoning: Adds same trigger to non-target groups but keeps original labels. Reduces ASR for non-target groups leading to more effective fairness attacks.

3) Fairness-Attack Transferable Optimization: Uses a surrogate model to optimize the trigger to further amplify accuracy differences between groups and improve attack effectiveness.

Main Contributions:
- Introduces the new concept of Trojan fairness attacks that behave correctly and fairly on clean data but act discriminatory towards certain groups for triggered inputs.

- Proposes a stealthy attack framework TrojFair with 3 modules to realize effective fairness attacks while maintaining overall accuracy.

- Achieves high target group ASR (88.77%) and discrimination score between target and non-target groups, with minimal drop in accuracy (<0.44%) across datasets and models. 

- Demonstrates resilience against current model fairness detectors and backdoor detection methods.

Overall, TrojFair explores an important new area of potential security concerns regarding fairness attacks that needs further investigation.


## Summarize the paper in one sentence.

 This paper introduces TrojFair, a novel Trojan attack framework that crafts a model to be accurate and fair on clean inputs but biased towards specific groups on inputs with a trigger.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing TrojFair, a new trojan attack framework to craft stealthy and effective fairness attacks on deep learning models. Specifically, the key contributions are:

1) Proposing three new techniques - target-group poisoning, non-target group anti-poisoning, and fairness-attack transferable optimization - to achieve high attack success rate for the target group while maintaining accuracy and fairness for clean inputs. 

2) Demonstrating that TrojFair can bypass existing model fairness auditing methods and trojan attack defenses, making it a stealthy attack.

3) Conducting comprehensive experiments on multiple datasets and neural network architectures to validate the effectiveness of TrojFair in achieving high attack success rates exceeding 88.77% with minimal accuracy loss. 

4) Analyzing the impact of different factors like poisoning ratios and trigger types on the performance of TrojFair.

5) Discussing a potential defense method by extending neural cleanse to detect TrojFair by reverse engineering triggers for each sensitive group.

In summary, the key novelty is proposing a new trojan attack framework specially designed for crafting effective and stealthy fairness attacks on deep learning models.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with this paper include:

- TrojFair - The name of the proposed Trojan fairness attack framework. It allows models to maintain accuracy and fairness for clean inputs but display discriminatory behaviors for certain groups when presented with inputs containing a specific trigger.

- Target-group poisoning - One of the modules of TrojFair that involves inserting a trigger into the samples of a target group and changing their labels to a desired target class. This is done to enhance the attack success rate for the target group. 

- Non-target group anti-poisoning - Another TrojFair module that embeds triggers into non-target group samples without changing labels, aiming to reduce attack success rates for non-target groups.

- Fairness-attack transferable optimization - The third TrojFair module that optimizes triggers using surrogate models to maximize accuracy differences between groups and improve attack effectiveness. 

- Attack success rate (ASR) - The percentage of inputs classified into a specified target class. TrojFair aims for high ASR for the target group and low ASR for non-target groups.

- Bias score - Used to measure bias by comparing accuracy differences between the target group and non-target groups. TrojFair seeks to maximize this metric.

- Stealthy fairness attack - The overall goal of TrojFair. It allows the model to appear unbiased for clean inputs but display discriminatory behaviors for certain groups when presented with triggered inputs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the TrojFair method proposed in the paper:

1. The paper mentions utilizing a surrogate model approach to optimize the trigger since the adversary lacks knowledge of the victim model and training process. What are some of the key considerations in selecting appropriate surrogate model architectures so that the optimized trigger can effectively transfer to the actual target models?

2. The target-group poisoning module involves relabeling a subset of target group samples to a designated target class. What impact would the choice of target class have on the attack effectiveness? Would assigning less frequent classes be more optimal? 

3. How does the ratio of poisoned target group samples to the overall target group sample size affect the stealthiness and effectiveness of the attack? Is there an optimal balance point that should be aimed for?

4. The non-target group anti-poisoning module serves to reduce trigger sensitivity for non-target groups. Are there any potential side effects of this approach in terms of model stability or other metrics that need to be taken into account?  

5. For the fairness-attack transferable optimization, why is directly using gradient-based trigger optimization infeasible under the threat model assumed in the paper? What modifications need to be made to the optimization process?

6. How does the formulation of the loss function for trigger optimization impact the ability to balance between attack effectiveness and stealthiness? What are some key considerations when designing this loss?

7. The paper demonstrates TrojFair against image classification tasks. What adaptations would be required to deploy TrojFair against other data types such as text, tabular data, time series, etc.?

8. What additional constraints need to be enforced during trigger optimization and poisoning to ensure that benign performance for non-target groups does not degrade below acceptable thresholds? 

9. Can insights from TrojFair be used to develop more robust defense strategies tailored to detecting and mitigating such fairness attacks? What would such defense mechanisms entail?

10. How do the assumptions made in the threat model for TrojFair compare with assumptions that may hold in real-world attack scenarios? What are limitations of the attack given practical constraints faced by adversaries?
