# [ResLoRA: Identity Residual Mapping in Low-Rank Adaption](https://arxiv.org/abs/2402.18039)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the problem that updating the weights of LoRA (low-rank adaptation) blocks is challenging due to the long calculation path in the original pretrained model. LoRA is a popular parameter-efficient fine-tuning method that freezes the original model weights and adds new low-rank matrices to adapt the model. However, the long path for gradient flow makes it difficult to effectively update the LoRA block weights.

Proposed Solution: 
The paper proposes ResLoRA, an improved framework of LoRA that adds residual connections (inspired by ResNet) between the LoRA blocks during training. This allows the gradient to shortcut the original path and flow directly across ResLoRA blocks, enabling faster and more effective weight updates. The paper discusses 3 types of residual connections tried. During inference, merging approaches are used to remove the extra connections and convert ResLoRA blocks back to LoRA blocks so that no extra latency is introduced.

Main Contributions:
- Proposes ResLoRA that combines residual connections with LoRA to accelerate training. Shows 1-20% better accuracy over LoRA after same epochs of training on various NLG and NLU tasks
- Discusses and evaluates 3 types of residual structures - input shortcut, block shortcut, middle shortcut
- Proposes merging approaches to remove extra connections in ResLoRA during inference while retaining benefits of accelerated training
- Shows mathematical analysis of why residual connections enable faster gradient flow
- Evaluates on language models for diverse tasks like text-to-image, QA, NLU etc to demonstrate wide applicability
  
In summary, the paper presents an improved training framework ResLoRA for the popular LoRA method that enables faster and better fine-tuning without introducing any extra parameters or inference latency. The residual connections aid gradient flow leading to faster convergence and better performance.
