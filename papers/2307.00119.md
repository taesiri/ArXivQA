# [Meta-training with Demonstration Retrieval for Efficient Few-shot   Learning](https://arxiv.org/abs/2307.00119)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we create a parameter-efficient model that is capable of fast and robust few-shot performance on a variety of natural language processing tasks?The authors aim to address this by proposing a meta-training approach that retrieves semantically similar demonstrations to augment the training data and supervision signal. Specifically, they combine meta-training with a retrieval-based architecture to create a model that can effectively leverage external knowledge from demonstrations for improved few-shot generalization.The key hypotheses appear to be:1) Meta-training can impart useful inductive biases and abilities like in-context learning to make smaller models capable of leveraging demonstrations effectively.2) Retrieving semantically similar demonstrations from a large and diverse bank compiled from many existing QA datasets will provide more varied and task-relevant supervision compared to randomly sampling demonstrations. 3) Separating external knowledge in the demonstrations from model parameters enables using smaller models that generalize well to more tasks, compared to relying solely on parameterization.In summary, the central research question is how to create a parameter-efficient model for robust and fast few-shot learning, which they address via meta-training augmented with demonstration retrieval. The key hypotheses focus on how this combination equips smaller models with better abilities for leveraging external knowledge.
