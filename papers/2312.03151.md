# [Multitask Learning Can Improve Worst-Group Outcomes](https://arxiv.org/abs/2312.03151)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine learning models need to not just perform well on average, but also ensure equitable outcomes across diverse subgroups (worst-group error). 
- However, most ML techniques like multitask learning (MTL) focus only on improving average performance. Their impact on worst-group error is underexplored.

Proposed Solution:
- The paper studies the impact of standard MTL on worst-group error in the common setting of fine-tuning a pretrained model. 
- Through synthetic data experiments, the paper provides intuition that reconstructing original input from corrupted input helps models rely less on spurious features that hurt worst-group performance. 
- Based on this, the paper proposes a simple modification to standard MTL: regularize the shared intermediate layer to restrict capacity, and multitask the end task with a reconstruction-style auxiliary objective like the model's pretraining task.

Key Contributions:
- Show that standard MTL can improve but does not consistently outperform distributionally robust optimization methods like Just-Train-Twice (JTT) on worst-group error.
- Propose regularized MTL through extensive experiments on synthetic and real-world CV/NLP datasets. Show it is competitive against JTT and better on average performance.
- Demonstrate the necessity of both regularization and auxiliary task for gains. Replacing pretrained MLM/MIM hurts gains showing importance of pretraining integration.
- Provide a simple, versatile way to improve both worst-group and average performance that can be integrated into existing MTL pipelines with minimal overhead compared to bespoke robustness methods.

In summary, the paper provides a thorough empirical exploration of MTL for combating poor worst-group outcomes and offers regularized MTL as a robust, lightweight solution to improve model fairness.
