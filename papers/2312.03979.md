# [Node-aware Bi-smoothing: Certified Robustness against Graph Injection   Attacks](https://arxiv.org/abs/2312.03979)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper proposes a novel framework called node-aware bi-smoothing to achieve certified robustness for graph-based node classifiers against graph injection attacks (GIAs). GIAs involve injecting new malicious nodes and edges into a graph to degrade model performance. The proposed method combines node deletion and edge deletion smoothing techniques in a synergistic manner to provide certificates against GIAs under evasion and poisoning threat models. A key theoretical contribution is deriving conditions to verify if a prediction for a node will remain consistent despite GIA perturbations. Experiments on node classification and recommender system tasks demonstrate that the method significantly improves certified robustness over adapted baselines. Notably, node-aware bi-smoothing also achieves competitive empirical defense performance when benchmarked against state-of-the-art defense techniques on a real-world GIA. The method's model-agnostic nature and applicability for both empirical and certified defense underscore its versatility. Overall, this work represents an important advancement in ensuring reliability of graph learning models against the practical and impactful threat of graph injection attacks.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper proposes a novel node-aware bi-smoothing framework to achieve certified robustness for graph-based classifiers against graph injection attacks, applicable to both evasion and poisoning scenarios, with experimental validation showing significant improvements over adapted baseline methods.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. It addresses the challenging task of achieving certified robustness against graph injection attacks (GIA), which are more stealthy and powerful compared to graph modification attacks (GMA). 

2. It proposes a novel node-aware bi-smoothing scheme that is crucial for enhancing certified robustness against GIA. The scheme considers practical constraints faced by attackers and increases the sample overlap probability.

3. The proposed method is versatile - it is model-agnostic, applicable for both evasion and poisoning attacks, and can be easily extended to provide certifications for recommender systems. 

4. It demonstrates that the node-aware bi-smoothing scheme can also be used as a practical defense strategy, achieving empirical robustness comparable to state-of-the-art methods.

5. Extensive experiments validate the effectiveness of the proposed schemes in significantly improving certified robustness against GIA over strong baseline methods. Results also show the practical implication of the schemes.

In summary, the key contribution is proposing a novel smoothing scheme to achieve certified robustness specifically against graph injection attacks, which is a highly stealthy and practical form of attack that has not been explored before in the context of certification.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, here are some of the key terms and concepts associated with this paper:

- Certified robustness
- Graph neural networks (GNNs)
- Graph injection attacks (GIAs)
- Graph modification attacks (GMAs) 
- Node classification
- Recommender systems
- Randomized smoothing
- Node-aware bi-smoothing
- Sample overlap probability
- Certified accuracy
- Average certifiable radius (ACR)

The paper proposes a new defense method called "node-aware bi-smoothing" to achieve certified robustness for graph neural networks against graph injection attacks. Key ideas include leveraging both node and edge deletions during smoothing to better defend against attacks that inject malicious nodes, analyzing sample overlap probabilities to derive robustness certificates, and evaluating performance using metrics like certified accuracy and average certifiable radius. The method is applied to tasks like node classification and recommendations where graph injection attacks are highly relevant.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a node-aware bi-smoothing scheme to achieve certified robustness against graph injection attacks. What is the intuition behind using both node deletion and edge deletion in the smoothing scheme? How does this help improve the certification performance?

2. The paper establishes theoretical conditions for the node-aware bi-smoothing scheme to provide certified robustness. Explain the key components and assumptions in deriving the certification condition in Theorem 1. What is the significance of the probability $\tilde{p}$?  

3. For poisoning attacks, the paper proposes a variant called node-aware-exclude. Explain how node-aware-exclude works and why it improves the certification performance compared to node-aware-include. What is the impact on the sample spaces?

4. How does the paper extend the node-aware bi-smoothing framework to provide certifiable robust recommendations? What modifications were made to handle the differences between node classification and recommender systems?

5. The node-aware bi-smoothing scheme has two key parameters - node deletion probability $p_n$ and edge deletion probability $p_e$. Analyze the impact of these parameters on the clean accuracy and certified accuracy through ablation studies. What tradeoffs exist?  

6. Compare and contrast the node-aware bi-smoothing scheme against the two adapted baselines in terms of limitations and why the baselines perform inadequately. What key insights lead to the design of the node-aware scheme?

7. The paper demonstrates the node-aware bi-smoothing scheme can also achieve competitive empirical defense performance. Explain the training and evaluation process. How does it compare against common defense methods?

8. Discuss the scope, assumptions and limitations of the method. For instance, what constraints are imposed on the attacks? Does the method extend well to other domains beyond node classification and recommendation?  

9. The paper provides a rigorous theoretical analysis of the certification conditions. Critically analyze the assumptions made in the proofs. Are there ways to tighten the bounds further?

10. The method relies on Monte Carlo sampling to obtain empirical estimates of the class probabilities. How does the number of samples impact the certified accuracy? Explore ways to reduce the sampling overhead.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper investigates the problem of achieving certified robustness for graph-based models against graph injection attacks (GIAs). GIAs involve injecting new malicious nodes along with crafted edges into a graph to degrade model performance. Defending against GIAs is critical as they are more practical, stealthy and powerful compared to common graph modification attacks. However, existing certification methods either cannot handle node injection or have limitations leading to poor defense performance against GIAs. Therefore, the paper aims to develop effective certified defense schemes tailored for GIAs.

Proposed Solution: 
The main proposal is a novel Node-Aware Bi-Smoothing framework to certify graph models against GIAs. It has two key components:
1) Node-aware smoothing: Randomly deletes nodes (along with their edges) to isolate injected nodes. This restricts realistic attackers.
2) Bi-smoothing: Additionally randomly removes edges. This further increases chances of isolating injected nodes and edges.  

Together they significantly improve the sample overlap probabilities between clean and attacked graphs. This then allows deriving robustness certificates leveraging the Neyman-Pearson lemma. Two variants are proposed: Node-Aware-Include and Node-Aware-Exclude. The latter improves certification for poisoning attacks by excluding votes from isolated nodes.

The framework is model-agnostic, applicable for both evasion and poisoning attacks, and extends naturally to provide certificates for recommender systems.

Main Contributions:
- First framework to certify general node classification models against GIAs 
- Novel node-aware bi-smoothing scheme crucial for enhancing certification
- Demonstrated effectiveness against evasion & poisoning attacks 
- Versatile scheme applicable for multiple tasks (node classification & recommendation)
- Competitive empirical defense performance besides certification
- Significantly outperforms adapted baseline certification methods 

The proposed methods are extensively evaluated on multiple datasets and models. Results validate enhanced certification performance and wide applicability.
