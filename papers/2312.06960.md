# [Remote Sensing Vision-Language Foundation Models without Annotations via   Ground Remote Alignment](https://arxiv.org/abs/2312.06960)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Remote sensing images captured by satellites contain valuable information about the earth, but lack corresponding textual descriptions. This makes it difficult to apply powerful vision-language models like CLIP that require large text-image paired datasets for training.  

- Existing vision-language models for satellite images are trained on small datasets with only thousands of text-annotated images. In contrast, models like CLIP use hundreds of millions of internet images with accompanying text.

- Lack of textual annotations limits the ability to perform open-vocabulary recognition tasks like retrieval and question answering on satellite images.

Proposed Solution: 
- Key idea is to use internet ground-level images as an intermediary to connect satellite images with language. Ground images on internet are accompanied by text.

- Collect dataset of 10M+ pairs of satellite images and internet ground images captured at the same geographical location by leveraging geotags. 

- Train encoder for satellite images to align its visual features with that of CLIP for internet images using contrastive learning on this dataset.

- This alignment allows open-vocabulary textual understanding of satellite images by sharing the text encoder of CLIP, without needing any textual annotations.

- Also train pixel-level model to enable localization tasks like segmentation.

- Combine with other frameworks like SAM and ViperGPT to enable segmentation and VQA.

Main Contributions:
- First large-scale vision-language model for satellite images trained without annotations

- Collect and provide benchmark dataset of 10M satellite and internet image pairs 

- Satellite image encoder that aligns with CLIP allows open-vocabulary understanding

- State-of-the-art on tasks like classification, retrieval, segmentation and VQA

- Enables creating powerful visualization and analysis tools for applications like urban planning, agriculture etc.
