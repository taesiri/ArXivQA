# [SkipDecode: Autoregressive Skip Decoding with Batching and Caching for   Efficient LLM Inference](https://arxiv.org/abs/2307.02628)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus appears to be developing an efficient token-level early exit technique for autoregressive language models that is compatible with practical optimization methods like batch inferencing and key-value caching. The central hypothesis seems to be that by designing a controlled exit policy with monotonically decreasing exit points across the sequence, it is possible to achieve significant speedups during inference while maintaining performance and seamlessly integrating with batching/caching techniques.Specifically, some of the key research questions/goals addressed in the paper include:- How to enable batch inferencing optimization by unifying exit points across tokens in a batch? - How to avoid recomputing key-value caches from early exits by guaranteeing monotonic decrease in exit points?- How to control the computational budget and avoid unpredictability of token-level exits?- Whether skipping layers instead of early termination can improve performance by attending to full context?- Evaluating if the proposed techniques can achieve consistent speedups across models and datasets with minimal degradation.So in summary, the central focus is developing a token-level early exit strategy that overcomes limitations of prior works by supporting batching/caching for efficiency along with a controlled budget and exit policy to maintain performance. The key hypothesis is that such a method can deliver substantial speedups in practice.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing SkipDecode, a novel token-level early exit method for efficient inference of autoregressive language models. The key features are:1) Supports batching and KV caching by having a unified exit point for all tokens in a batch at each sequence position. 2) Uses a monotonic decay of exit points across sequence to avoid recomputing KV caches.3) Controls computational budget by pre-specifying min and max exit points.4) Allocates budget to top layers rather than abruptly ending computation.- Demonstrating 2x-5x inference speedups on OPT models with up to 6.7B parameters on text generation tasks with negligible performance regression.- Overcoming limitations of prior token exit methods that are incompatible with batching and caching widely used in practice.- Providing a simple and effective technique to reduce LLM inference cost, making them more accessible for real-world deployment.In summary, the main contribution is proposing SkipDecode, a practical token-level early exit approach that achieves significant speedups on large autoregressive LMs while supporting key optimizations like batching and caching for efficient inference.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes SkipDecode, an efficient method for autoregressive language model inference that allows early exiting of tokens within a sequence while maintaining compatibility with batching and caching techniques commonly used to optimize real-world systems.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on efficient inference for large language models:- The key contribution of this paper is developing an early exit method called SkipDecode that is compatible with batching and caching optimizations. This sets it apart from prior token-level early exit techniques like CALM that do not support batching/caching. - SkipDecode uses a predefined exit policy with monotonically decreasing exit points across the sequence. This provides more predictable computation compared to adaptive policies based on learned classifiers. However, predefined policies may be less flexible.- The focus on decoder-only models is fairly unique. Most prior work on early exit has focused on encoder-only models. Adapting these techniques to autoregressive decoding introduces new challenges that this paper addresses.- Compared to other model compression methods like knowledge distillation and pruning, SkipDecode aims to provide speedups dynamically during inference. So it is complementary to these approaches.- The speedups shown of 2-5x are quite promising. However, the degradation in performance with higher speedups indicates there are still challenges in pushing extremely low compute budgets.- Benchmarking against adapted versions of CALM and multi-layer exit shows SkipDecode has better performance, but more comparisons on a wider range of methods would be useful.Overall, SkipDecode makes an important contribution in making early exit compatible with crucial optimizations like batching/caching for decoder models. The preset exit policy is simple but provides more predictable resource usage. More work can still be done in developing flexible policies and extremely low compute budgets. But this is an intriguing step toward efficient large language model inference.


## What future research directions do the authors suggest?

The main research directions suggested by the authors for future work are:- Investigating alternative decay functions for determining the exit points across the sequence. They mention that using a power law decay did not yield improvements over the linear decay employed in their experiments. But prior work indicates a power law distribution may be better suited for modeling token exit levels. So exploring other decay functions could be promising.- Examining the impact of applying the decaying exit point policy to the prompt/context in addition to just the generated text. Currently they use the full network for processing the prompt, but applying the policy to the prompt as well could enable further speedups. - Enhancing dynamic batching to better support the "infinite loop" inference mode. Their method of decaying exit points currently presents some limitations for dynamic batching, so improving support for infinite streaming inference could be valuable.- Further analysis of the behavior of different decay functions and their impact on model performance. There may be room to use more aggressive decays by better understanding this relationship.Overall, the main directions are improving the exit point policy with different decay functions, applying it more extensively including to prompts, integrating with dynamic batching for streaming inference, and better understanding the tradeoffs around aggressiveness of decay. The end goal is developing more efficient and versatile token-level early exiting strategies.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes SkipDecode, a method for efficient inference of autoregressive language models. SkipDecode allows tokens to skip computations in lower layers of the model, focusing computation on the top layers. This enables faster inference while retaining most of the model performance. Unlike prior early exit methods, SkipDecode is designed to be compatible with batching and caching techniques used to speed up inference in practice. It assigns a single exit point per sequence position for all tokens in a batch, with exit points decaying monotonically across the sequence. This avoids having to recompute cached values when a later token exits deeper than an earlier one. Experiments on OPT models with up to 6.7 billion parameters demonstrate SkipDecode can achieve 2-5x speedups on text generation tasks with negligible performance loss, while supporting batching and caching. The method provides predictable computational cost and does not require modifying the model architecture. Overall, SkipDecode enables efficient inference for large language models, overcoming limitations of prior token-level early exit techniques.
