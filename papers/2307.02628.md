# [SkipDecode: Autoregressive Skip Decoding with Batching and Caching for   Efficient LLM Inference](https://arxiv.org/abs/2307.02628)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus appears to be developing an efficient token-level early exit technique for autoregressive language models that is compatible with practical optimization methods like batch inferencing and key-value caching. The central hypothesis seems to be that by designing a controlled exit policy with monotonically decreasing exit points across the sequence, it is possible to achieve significant speedups during inference while maintaining performance and seamlessly integrating with batching/caching techniques.Specifically, some of the key research questions/goals addressed in the paper include:- How to enable batch inferencing optimization by unifying exit points across tokens in a batch? - How to avoid recomputing key-value caches from early exits by guaranteeing monotonic decrease in exit points?- How to control the computational budget and avoid unpredictability of token-level exits?- Whether skipping layers instead of early termination can improve performance by attending to full context?- Evaluating if the proposed techniques can achieve consistent speedups across models and datasets with minimal degradation.So in summary, the central focus is developing a token-level early exit strategy that overcomes limitations of prior works by supporting batching/caching for efficiency along with a controlled budget and exit policy to maintain performance. The key hypothesis is that such a method can deliver substantial speedups in practice.
