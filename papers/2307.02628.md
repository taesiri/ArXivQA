# [SkipDecode: Autoregressive Skip Decoding with Batching and Caching for   Efficient LLM Inference](https://arxiv.org/abs/2307.02628)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus appears to be developing an efficient token-level early exit technique for autoregressive language models that is compatible with practical optimization methods like batch inferencing and key-value caching. The central hypothesis seems to be that by designing a controlled exit policy with monotonically decreasing exit points across the sequence, it is possible to achieve significant speedups during inference while maintaining performance and seamlessly integrating with batching/caching techniques.Specifically, some of the key research questions/goals addressed in the paper include:- How to enable batch inferencing optimization by unifying exit points across tokens in a batch? - How to avoid recomputing key-value caches from early exits by guaranteeing monotonic decrease in exit points?- How to control the computational budget and avoid unpredictability of token-level exits?- Whether skipping layers instead of early termination can improve performance by attending to full context?- Evaluating if the proposed techniques can achieve consistent speedups across models and datasets with minimal degradation.So in summary, the central focus is developing a token-level early exit strategy that overcomes limitations of prior works by supporting batching/caching for efficiency along with a controlled budget and exit policy to maintain performance. The key hypothesis is that such a method can deliver substantial speedups in practice.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing SkipDecode, a novel token-level early exit method for efficient inference of autoregressive language models. The key features are:1) Supports batching and KV caching by having a unified exit point for all tokens in a batch at each sequence position. 2) Uses a monotonic decay of exit points across sequence to avoid recomputing KV caches.3) Controls computational budget by pre-specifying min and max exit points.4) Allocates budget to top layers rather than abruptly ending computation.- Demonstrating 2x-5x inference speedups on OPT models with up to 6.7B parameters on text generation tasks with negligible performance regression.- Overcoming limitations of prior token exit methods that are incompatible with batching and caching widely used in practice.- Providing a simple and effective technique to reduce LLM inference cost, making them more accessible for real-world deployment.In summary, the main contribution is proposing SkipDecode, a practical token-level early exit approach that achieves significant speedups on large autoregressive LMs while supporting key optimizations like batching and caching for efficient inference.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes SkipDecode, an efficient method for autoregressive language model inference that allows early exiting of tokens within a sequence while maintaining compatibility with batching and caching techniques commonly used to optimize real-world systems.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on efficient inference for large language models:- The key contribution of this paper is developing an early exit method called SkipDecode that is compatible with batching and caching optimizations. This sets it apart from prior token-level early exit techniques like CALM that do not support batching/caching. - SkipDecode uses a predefined exit policy with monotonically decreasing exit points across the sequence. This provides more predictable computation compared to adaptive policies based on learned classifiers. However, predefined policies may be less flexible.- The focus on decoder-only models is fairly unique. Most prior work on early exit has focused on encoder-only models. Adapting these techniques to autoregressive decoding introduces new challenges that this paper addresses.- Compared to other model compression methods like knowledge distillation and pruning, SkipDecode aims to provide speedups dynamically during inference. So it is complementary to these approaches.- The speedups shown of 2-5x are quite promising. However, the degradation in performance with higher speedups indicates there are still challenges in pushing extremely low compute budgets.- Benchmarking against adapted versions of CALM and multi-layer exit shows SkipDecode has better performance, but more comparisons on a wider range of methods would be useful.Overall, SkipDecode makes an important contribution in making early exit compatible with crucial optimizations like batching/caching for decoder models. The preset exit policy is simple but provides more predictable resource usage. More work can still be done in developing flexible policies and extremely low compute budgets. But this is an intriguing step toward efficient large language model inference.
