# [SkipDecode: Autoregressive Skip Decoding with Batching and Caching for   Efficient LLM Inference](https://arxiv.org/abs/2307.02628)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus appears to be developing an efficient token-level early exit technique for autoregressive language models that is compatible with practical optimization methods like batch inferencing and key-value caching. The central hypothesis seems to be that by designing a controlled exit policy with monotonically decreasing exit points across the sequence, it is possible to achieve significant speedups during inference while maintaining performance and seamlessly integrating with batching/caching techniques.Specifically, some of the key research questions/goals addressed in the paper include:- How to enable batch inferencing optimization by unifying exit points across tokens in a batch? - How to avoid recomputing key-value caches from early exits by guaranteeing monotonic decrease in exit points?- How to control the computational budget and avoid unpredictability of token-level exits?- Whether skipping layers instead of early termination can improve performance by attending to full context?- Evaluating if the proposed techniques can achieve consistent speedups across models and datasets with minimal degradation.So in summary, the central focus is developing a token-level early exit strategy that overcomes limitations of prior works by supporting batching/caching for efficiency along with a controlled budget and exit policy to maintain performance. The key hypothesis is that such a method can deliver substantial speedups in practice.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing SkipDecode, a novel token-level early exit method for efficient inference of autoregressive language models. The key features are:1) Supports batching and KV caching by having a unified exit point for all tokens in a batch at each sequence position. 2) Uses a monotonic decay of exit points across sequence to avoid recomputing KV caches.3) Controls computational budget by pre-specifying min and max exit points.4) Allocates budget to top layers rather than abruptly ending computation.- Demonstrating 2x-5x inference speedups on OPT models with up to 6.7B parameters on text generation tasks with negligible performance regression.- Overcoming limitations of prior token exit methods that are incompatible with batching and caching widely used in practice.- Providing a simple and effective technique to reduce LLM inference cost, making them more accessible for real-world deployment.In summary, the main contribution is proposing SkipDecode, a practical token-level early exit approach that achieves significant speedups on large autoregressive LMs while supporting key optimizations like batching and caching for efficient inference.
