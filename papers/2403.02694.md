# [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) like ChatGPT are revolutionizing search and conversational AI, but they are computationally expensive. Users often submit repeated queries to services, so caching responses can reduce costs. However, existing caches rely on keyword matching and fail to capture semantic similarity in queries, leading to poor cache hit rates. 

Proposed Solution:
This paper introduces MeanCache, a user-centric, privacy-preserving semantic cache for LLM services. It uses federated learning to train an embedding model on user devices without storing data centrally. This allows learning query patterns for each user. It generates embeddings for queries, uses cosine similarity to match new queries to cached ones, and returns cached responses to avoid querying LLMs. It compresses embeddings with PCA to reduce storage needs and optimize search time. It also automatically tunes the similarity threshold per user for accuracy.

Contributions:
- First semantic cache using federated learning to enable privacy-preserving embedding model training customized to each user's query behavior
- 83% lower storage needs and 11% faster query search time via embedding compression, while improving cache hit rate over baseline
- 17% higher F1 Score and 20% better precision than existing semantic cache by auto-tuning similarity threshold 

In summary, MeanCache introduces an innovative decentralized design that reduces costs for LLM services, minimizes false cache hits to improve user experience, and eliminates privacy risks - all while surpassing performance of current semantic caches.
