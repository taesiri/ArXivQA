# [SQT -- std $Q$-target](https://arxiv.org/abs/2402.05950)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning (RL) algorithms like DDPG suffer from overestimation bias, where the Q-values tend to overestimate the value of suboptimal actions. This leads to poor performance. 

- Existing solutions like TD3 introduce underestimation bias instead. This also hampers exploration and results in suboptimal policies.

Proposed Solution:
- The paper proposes a new algorithm called Standard Q-Target (SQT) to address overestimation bias. 

- SQT uses the disagreement between the Q-networks (measured by standard deviation) as a penalty term in the Q-target formula. This discordance acts as an "uncertainty penalty" and makes the Q-value estimates more conservative.

- By reducing this penalty term from the Q-target, SQT balances the overestimation and underestimation biases.

- Computationally, SQT only requires adding 2-3 lines of code to existing implementations of TD3 and TD7. So it is simple to adopt.

Contributions:
- SQT provides a minimalistic solution to tackle overestimation bias with easier implementation than prior conservative methods like BCQ.

- Experiments across 7 MuJoCo and Bullet environments show SQT consistently outperforms TD3, TD7 and DDPG by a significant margin.

- SQT demonstrates much stable performance across seeds compared to the baselines.

- The results showcase the effectiveness of using Q-network disagreement as a penalty over TD3's method of taking the minimum Q-value in mitigating overestimation.

In summary, the paper presents SQT - a simple, stable and well-performing reinforcement learning algorithm that leverages ensemble variance as an uncertainty measure to address overestimation bias.
