# [Privacy Amplification by Iteration for ADMM with (Strongly) Convex   Objective Functions](https://arxiv.org/abs/2312.08685)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper examines privacy amplification by iteration for a variant of the Alternating Direction Method of Multipliers (ADMM) algorithm that uses only gradient information to update the primal variable in each iteration. This is in contrast to the proximal variant studied in prior work, which requires full function access. A key challenge is that noise is added only to the primal variable to preserve local iteration privacy, while the dual variable remains unaltered. To achieve privacy amplification from the perspective of the first user, the authors employ a customized norm under which an ADMM iteration becomes non-expansive and consider two iterations together as a Markov operator. Under general convexity, they prove that the privacy guarantee amplifies by a factor proportional to the number of iterations. For strongly convex functions, this factor grows exponentially. The theoretical results are complemented by experiments on a Lasso problem confirming the effects of contraction and noise on utility. Potential dependence of the privacy bound on problem parameters is analyzed, uncovering inherent tradeoffs. Overall, the work provides a thorough theoretical and empirical demonstration of the feasibility of privacy amplification for a gradient-based ADMM variant.
