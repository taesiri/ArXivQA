# [Privacy Amplification by Iteration for ADMM with (Strongly) Convex   Objective Functions](https://arxiv.org/abs/2312.08685)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper examines privacy amplification by iteration for a variant of the Alternating Direction Method of Multipliers (ADMM) algorithm that uses only gradient information to update the primal variable in each iteration. This is in contrast to the proximal variant studied in prior work, which requires full function access. A key challenge is that noise is added only to the primal variable to preserve local iteration privacy, while the dual variable remains unaltered. To achieve privacy amplification from the perspective of the first user, the authors employ a customized norm under which an ADMM iteration becomes non-expansive and consider two iterations together as a Markov operator. Under general convexity, they prove that the privacy guarantee amplifies by a factor proportional to the number of iterations. For strongly convex functions, this factor grows exponentially. The theoretical results are complemented by experiments on a Lasso problem confirming the effects of contraction and noise on utility. Potential dependence of the privacy bound on problem parameters is analyzed, uncovering inherent tradeoffs. Overall, the work provides a thorough theoretical and empirical demonstration of the feasibility of privacy amplification for a gradient-based ADMM variant.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies a private variant of the Alternating Direction Method of Multipliers (ADMM) algorithm for convex optimization. ADMM is often used for optimizing an objective function that decomposes into a sum of two convex functions. In the private setting, each ADMM iteration uses private data (a loss function) from a different user. The goal is to analyze the privacy guarantee for the user in the first iteration after multiple noisy ADMM iterations, by exploiting the independent noise added in later iterations to mask the private data of subsequent users. Previous work analyzed such "privacy amplification by iteration" for stochastic gradient descent and the proximal ADMM variant, but analyzing the first-order gradient oracle based ADMM variant poses new challenges.

Proposed Solution:
- Customize the norm on the joint space of primal and dual ADMM variables to establish the non-expansiveness of each noisy iteration, a key property enabling privacy amplification.  

- Conceptually combine two noisy ADMM iterations into one "Markov transition operator" to enable meaningful one-step privacy analysis, by treating the independent noise from two iterations as collectively masking the exposed primal and dual variables.

- Adapt the abstract privacy amplification framework of Balle et al. based on these insights to show that the privacy guarantee of the first user improves proportionally to the number of iterations T, attaining optimal $\tilde{O}(1/T)$ rate. 

- For strongly convex objectives, refine the customized norm to show each iteration is contractive. This gives exponential privacy amplification in T.

Main Contributions:
- First privacy amplification guarantee for the gradient oracle based ADMM variant, with technical innovations like a new ADMM norm and bundling steps.

- Concrete $\tilde{O}(1/T)$ and exponential amplification rates for general and strongly convex ADMM objectives respectively.

- Empirical evaluation of the utility-privacy tradeoff, confirming the effect of noise and contraction parameters on convergence.

Overall, the work expands our understanding of privacy amplification to broader algorithm classes like ADMM, with novel proof ideas that may find wider applications. The scalable gradient oracle variant can also have practical advantages.
