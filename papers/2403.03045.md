# [Adding Multimodal Capabilities to a Text-only Translation Model](https://arxiv.org/abs/2403.03045)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most current work in multimodal machine translation (MMT) uses the small Multi30k dataset for training and evaluation, resulting in models that overfit and perform poorly on typical text-only test sets.

Proposed Solution:
- Start with a performant text-only machine translation (MT) model and incrementally transform it into an MMT model in order to perform well on both Multi30k and text-only test sets.

- Add lightweight vision-text adapter layers with gating mechanisms to the MT model to incorporate visual information while retaining text translation performance.

- Pre-train the MMT model using:
   1) Vision-based masking of source text to force usage of visual information
   2) A large dataset of machine translated image captions and text-only data
   
- Fine-tune the MMT model on Multi30k dataset

Contributions:
- Achieve state-of-the-art performance on Multi30k 2016 EN-DE test set (46.5 BLEU4) while retaining performance of original MT model on text-only test sets

- Smoothly transition MT model into MMT model via adapter layers and gating mechanisms with minimal parameter overhead  

- Demonstrate effectiveness of using visual grounding of source text and pre-training before fine-tuning on small Multi30k dataset

- Propose evaluation framework to measure model's use of visual information and ability to translate complex sentences

In summary, the key idea is to leverage a strong text-only MT model and carefully transform it into an MMT model via adapters and pre-training strategies to achieve excellent performance on both multimodal and text-only machine translation test sets.
