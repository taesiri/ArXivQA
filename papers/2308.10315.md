# [Improving Adversarial Robustness of Masked Autoencoders via Test-time   Frequency-domain Prompting](https://arxiv.org/abs/2308.10315)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How does the choice of reconstruction target in masked image modeling (BERT-style pretraining) impact the adversarial robustness of vision transformers? The key hypotheses explored are:1) Using raw pixel values as the reconstruction target (as in MAE) leads to worse adversarial robustness compared to using semantic targets (as in BEiT and PeCo). This is because predicting raw pixels makes models rely more on high-frequency image details.2) The homogeneity of attention across spatial regions is higher in models pretrained with masked image modeling compared to supervised or contrastive approaches. This also contributes to the robustness difference between methods like MAE and BEiT/PeCo.3) Strategies like adding perceptual losses or adversarial training to MAE pretraining can mitigate the robustness gap by guiding representations to focus less on pixel-level details.In summary, the paper analyzes how design choices in masked image modeling pretraining impact robustness and proposes methods to improve the robustness of approaches like MAE. The central question is understanding and improving the adversarial vulnerability of different self-supervised vision transformers.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- An analysis of the adversarial robustness of vision transformers (ViTs) with different BERT-style pre-training methods. The key finding is that MAE, which directly predicts raw pixels of masked image patches, has significantly worse adversarial robustness compared to methods like BEiT and PeCo that predict semantic targets.- An empirical analysis linking MAE's worse robustness to its reliance on medium/high frequency image content. Experiments show MAE is more impacted when these frequencies are removed.- A proposed method to improve MAE's robustness by learning frequency-domain visual prompts that fill in medium/high frequencies with dataset patterns. This is shown to boost robustness while maintaining accuracy.In summary, the main contribution is an analysis of why MAE has worse adversarial robustness than other BERT-style vision pre-training methods, linked to its reconstruction target being raw pixels. The paper proposes a way to address this via frequency-domain prompting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method to improve the adversarial robustness of masked autoencoders like MAE by using test-time frequency-domain prompting, which fills the medium/high-frequency components of images with dataset-extracted patterns to make it harder to generate effective adversarial perturbations.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- This paper focuses specifically on improving the adversarial robustness of masked autoencoders (MAEs), a type of vision transformer model that uses BERT-style pretraining. Other related work has looked at robustness of vision transformers more broadly.- The key finding is that MAEs are much less robust to adversarial attacks compared to other BERT-pretrained vision transformers like BEiT and PeCo. This is a novel and surprising result, as previous work has not systematically compared adversarial robustness across different self-supervised ViT models. - The authors provide an in-depth analysis attributing MAEs' poorer robustness to their pixel-level reconstruction target during pretraining. This leads MAEs to rely more on high-frequency image components, which makes them more sensitive to adversarial noise. This analysis and link between pretraining target and robustness is a new contribution.- To improve MAEs' robustness, the authors propose learning visual prompts in the frequency domain that fill in high-frequency areas with dataset patterns. Using prompts to improve robustness has been explored before, but prompts in the frequency domain are novel.- Compared to prior work on robustness of vision transformers, this paper provides a more focused study on a specific self-supervised family of models, offers novel analysis about the causes of their vulnerabilities, and introduces a tailored defense method. The frequency domain prompting approach is a distinct and new technique for improving robustness.In summary, while building on prior work on adversarial robustness, this paper provides unique insights into MAEs' weaknesses compared to other self-supervised ViTs, and proposes a novel prompting-based defense tailored to overcoming MAEs' particular flaws. The analysis and frequency-domain prompting approach meaningfully advance understanding and improvement of vision transformer robustness.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Further exploring different choices of reconstruction targets for BERT pretraining in computer vision. The authors show that using pixel values versus semantic tokens leads to differences in adversarial robustness. More work could be done to understand the tradeoffs of different pretraining objectives.- Developing more sophisticated prompting methods to improve adversarial robustness. The frequency-domain prompting approach proposed in this paper is a simple solution, but more advanced prompting techniques could be developed. - Studying the connection between pretraining objectives and robustness in more depth. The authors provide some analysis about MAE's sensitivity to high frequencies leading to lower robustness, but further investigation into this relationship is needed.- Applying insights from this analysis of vision BERT methods to other self-supervised techniques like contrastive learning. The authors show their prompting method can help contrastive models too, but more work can be done.- Exploring how adversarial robustness learned during self-supervised pretraining transfers to downstream tasks. This paper focuses on ImageNet classification, but studying other applications would be useful.- Developing more robust optimization techniques or architectures specialized for BERT-style pretrained models. Adversarial training helps but has limitations, so new defenses could help.In summary, the key directions are: analyzing pretraining objectives and robustness, developing better prompting and defense methods, transferring robustness to new tasks, and designing robust models tailored for self-supervised vision learners. The insights from this paper lay the foundation for a variety of future work in this emerging area.
