# [CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained   Language-Vision Models](https://arxiv.org/abs/2306.09635)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can we learn to perform text-to-audio synthesis without using any paired text-audio data, by instead leveraging unlabeled videos and pretrained language-vision models?The key hypothesis is that the naturally-occurring correspondence between audio and images in videos, combined with the cross-modal representations learned by pretrained language-vision models like CLIP, can allow learning the mapping from text to audio without needing any explicit text-audio pairs.In summary, the paper aims to show that text-to-audio synthesis can be learned without text-audio supervision by using videos and pretrained multimodal models as a bridge between the textual and audio modalities. The main hypothesis is that the visual modality can provide a rich conditioning signal to learn the text-audio correspondence in a self-supervised manner.
