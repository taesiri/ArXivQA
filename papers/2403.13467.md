# [CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language   Models](https://arxiv.org/abs/2403.13467)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing solutions for controlling robot swarms require manually designing desired patterns/formations. This is a tedious process. 
- There is a lack of techniques to automatically create robotic swarm formations based on simple natural language descriptions.

Proposed Solution:
- The paper introduces CLIPSwarm, an algorithm to automatically generate drone formations matching text descriptions.
- It takes a single word as input (e.g "leaf") describing a desired shape.
- The algorithm enriches this word into a text prompt with additional context and selects a representative color (e.g. "A green leaf shape"). 
- An iterative optimization process is used to evolve a set of robot formations to maximize similarity between the text prompt and visual representation of the formations. The visual representation is created by contouring the formations using alpha shapes.  
- Similarity is measured using CLIP, which encodes text and images into vectors.
- The best matching formation is selected and goal positions are assigned to drones using robotic constraints while avoiding collisions.

Main Contributions:
- First technique to automate visually pleasing swarm formations using only simple text descriptions, without needing manually designed patterns.
- Iterative optimization strategy maximizing similarity between text and formation images based on CLIP embeddings.
- Method to create visually appealing contours from sparse formations using alpha shapes.
- Demonstration of generating varied formations in simulation matching text prompts like "leaf", "heart", etc.
- Showcasing complete drone shows with collision avoidance between drones.

The paper introduces an elegant technique to leverage natural language and vision-language models like CLIP to automate the creative process of designing robotic swarm formations.
