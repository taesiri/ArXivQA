# [All you need are a few pixels: semantic segmentation with PixelPick](https://arxiv.org/abs/2104.06394)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the challenge of reducing the prohibitive cost of obtaining dense pixel-level annotations to train semantic segmentation models. The central hypothesis is that in order to achieve good segmentation performance, you only need a few well-chosen pixel labels. The paper investigates this by proposing an active learning framework called PixelPick that interactively selects a small number of pixels to be labeled and uses those to train a segmentation model.The key research questions explored in the paper are:- How many sparse pixel labels are needed to achieve good segmentation performance?- How should those pixel locations be selected? The paper studies different acquisition functions for picking the most useful pixels.- How can the selected pixels be annotated efficiently? The paper proposes a "mouse-free" annotation strategy to implement the PixelPick framework. - What factors affect model performance in the low annotation regime, such as annotation diversity, model architecture, pretraining strategy, etc?Through extensive experiments, the paper shows that the PixelPick framework can achieve comparable segmentation performance to fully supervised methods but with 1-2 orders of magnitude fewer pixel annotations on standard benchmarks.In summary, the central hypothesis is that only a few well-chosen pixel labels are needed for good segmentation, and the key questions addressed are how many pixels are needed, how to select them, and how to annotate them efficiently. The PixelPick framework is proposed to investigate these questions.
