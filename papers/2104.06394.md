# [All you need are a few pixels: semantic segmentation with PixelPick](https://arxiv.org/abs/2104.06394)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the challenge of reducing the prohibitive cost of obtaining dense pixel-level annotations to train semantic segmentation models. The central hypothesis is that in order to achieve good segmentation performance, you only need a few well-chosen pixel labels. The paper investigates this by proposing an active learning framework called PixelPick that interactively selects a small number of pixels to be labeled and uses those to train a segmentation model.The key research questions explored in the paper are:- How many sparse pixel labels are needed to achieve good segmentation performance?- How should those pixel locations be selected? The paper studies different acquisition functions for picking the most useful pixels.- How can the selected pixels be annotated efficiently? The paper proposes a "mouse-free" annotation strategy to implement the PixelPick framework. - What factors affect model performance in the low annotation regime, such as annotation diversity, model architecture, pretraining strategy, etc?Through extensive experiments, the paper shows that the PixelPick framework can achieve comparable segmentation performance to fully supervised methods but with 1-2 orders of magnitude fewer pixel annotations on standard benchmarks.In summary, the central hypothesis is that only a few well-chosen pixel labels are needed for good segmentation, and the key questions addressed are how many pixels are needed, how to select them, and how to annotate them efficiently. The PixelPick framework is proposed to investigate these questions.


## What is the main contribution of this paper?

This paper proposes a novel framework called PixelPick for semantic segmentation that achieves strong performance using only a small number of sparsely annotated pixels. The main contributions are:1. They show that deep neural networks can achieve good segmentation performance when trained on just a handful of labelled pixels per image. This demonstrates that dense pixel-wise annotation is highly redundant.2. They propose an active learning framework called PixelPick that interactively queries the most informative pixels to label. This allows radical reduction in annotation cost compared to dense labelling.  3. They design an efficient "mouse-free" annotation tool to implement PixelPick. By providing pixel proposals to annotate instead of having annotators draw masks, it becomes a quick classification task.4. They perform extensive experiments analyzing factors like annotation diversity, model capacity, pretraining and sampling mechanisms in the low annotation regime. This provides insights into effective training in this setting.5. They demonstrate PixelPick achieves comparable performance to state-of-the-art active learning and semi-supervised methods on CamVid, Cityscapes and PASCAL VOC with orders of magnitude fewer pixel annotations.6. They evaluate the annotation efficiency of their pipeline and show it is robust to annotator errors, highlighting its practical utility.In summary, the main contribution is an active learning approach and efficient annotation tool that achieve strong semantic segmentation performance with minimal annotation cost by exploiting deep networks' ability to learn from sparse pixel labels. The experiments provide insights into low annotation training and demonstrate the practical effectiveness of PixelPick.
