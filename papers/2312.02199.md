# [USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite   Imagery](https://arxiv.org/abs/2312.02199)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Developing AI models for remote sensing imagery is limited by lack of labeled data. Self-supervised learning presents an opportunity to leverage the massive amounts of unlabeled multi-spectral, multi-sensor satellite and aerial imagery. 
- Prior self-supervised methods typically use images from a single sensor, limiting available supervision signal. Methods that support multiple sensors require rescaling images to the same resolution, increasing computation and potentially losing useful scale information.

Proposed Solution:
- Developed USat, a vision transformer encoder that can input arbitrary spectral bands from multiple sensors without needing to rescale to consistent resolution. Allows flexibility in inputs.
- Uses separate patch embeddings per spectral band with a spectral group pooling layer to reduce sequence length while retaining alignment. Number of patches differs across spectral bands based on native resolution.
- Introduced superpositional positional encodings to capture alignment of multi-resolution patches capturing the same geographic area. Encodes higher resolution patch locations as the average of encodings of lower resolution patches covering that area.
- Integrated USat into a masked autoencoder (USatMAE) for self-supervised pretraining on paired NAIP (aerial) and Sentinel-2 (satellite) imagery.

Main Contributions:
- USat architecture supporting flexible input of multi-sensor multi-spectral imagery with varying native resolutions without rescaling.
- Introduction of superpositional encodings to represent geospatial alignment across resolutions.
- USatlas benchmark dataset derived from Satlas for evaluating multi-sensor self-supervised methods.
- Extensive experiments showing multi-sensor USatMAE outperforms single sensor pretraining, especially when fine-tuning on lower resolution sensors.
- USatMAE matches or exceeds state-of-the-art self-supervised methods on multiple downstream tasks and sensors.
