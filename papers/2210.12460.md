# Collaborative Reasoning on Multi-Modal Semantic Graphs for   Video-Grounded Dialogue Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question this paper addresses is: How can we better integrate video data into pre-trained language models (PLMs) and take into account the complementarity of different modalities for video-grounded dialogue generation?The key challenges identified are:1) Difficulty integrating video data into PLMs, which limits exploiting their power for this task. 2) Necessity of collaborative reasoning across modalities, rather than independent or isolated reasoning.To address these issues, the paper proposes:1) Converting video information into natural language reasoning paths compatible with PLMs.2) A multi-agent reinforcement learning framework for collaborative reasoning across video and dialogue modalities.The central hypothesis seems to be that the proposed techniques will significantly improve video-grounded dialogue generation performance compared to existing methods, as evaluated on two benchmark datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Identifying the issues that current PLM-based approaches are unable to fully comprehend video content, although showing promising results on automatic metrics. 2. Proposing a multi-agent reasoning framework upon PLMs that allows information from different modalities (video and text) to reinforce each other and discover multi-modal reasoning paths.3. Empirically verifying the effectiveness of the proposed model on two video-grounded dialogue benchmarks, showing significant improvements over state-of-the-art methods.Specifically, the paper argues that existing methods either cannot effectively integrate video data into PLMs, or fail to perform collaborative reasoning over different modalities. To address these issues, the authors propose a framework with video and text agents that extract reasoning paths from video and text graphs. A central communicator enables information flow between the agents. This allows exploiting PLMs' power, while letting multi-modal information complement each other through collaborative reasoning.Experiments on two datasets demonstrate the superiority of the proposed model, with both automatic metrics and human evaluations showing substantial gains over competitive baselines. The ablation studies also confirm the importance of each model component.In summary, the core contribution lies in the novel multi-agent reasoning framework that enables PLMs to perform collaborative multi-modal reasoning for improved video-grounded dialogue generation. The empirical verification of its effectiveness is another key contribution.
