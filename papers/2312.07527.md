# [BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy   and Reasoning Ability](https://arxiv.org/abs/2312.07527)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents a new dataset called BaRDa (Belief and Reasoning Dataset) for evaluating the factual accuracy and reasoning abilities of language models. The goal is to separate the notions of "truth" (factual correctness) and "rationality" (correct reasoning) when evaluating models. The dataset contains 3000 entailments using 9000 statements, labeled as true or false. Some entailments use true statements and valid reasoning (TT), some use false statements but valid reasoning (FT), etc. This allows testing models for both factual and reasoning accuracy. They test GPT3, GPT3.5, and GPT4, finding the newer models have higher factual (up to 87.1% for GPT4) and reasoning accuracy (up to 79.2%), showing clear progression. However GPT3 is better at recognizing valid entailments than GPT3.5. The dataset and model evaluations provide a benchmark for quantifying improvements in language models along these two axes of accuracy and reasoning ability.
