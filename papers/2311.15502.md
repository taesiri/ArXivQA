# [Learning with Complementary Labels Revisited: A Consistent Approach via   Negative-Unlabeled Learning](https://arxiv.org/abs/2311.15502)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new risk-consistent approach for complementary-label learning called CONU, which does not rely on the common uniform label distribution assumption nor require an additional ordinary training set. The key idea is to express complementary-label learning as a set of negative-unlabeled binary classification problems using a one-versus-rest strategy. This allows the authors to derive an unbiased risk estimator to train classifiers based solely on the complementary-labeled data. To address overfitting issues, a corrected risk estimator with theoretical guarantees is introduced. Experiments on synthetic and real-world benchmark datasets demonstrate superior performance over existing methods. Notably, CONU achieves much higher accuracy on a real-world noisy complementary label dataset compared to state-of-the-art partial label learning algorithms. The consistency properties and generalization error bounds of CONU are also formally proven. Overall, this work provides new theoretical insights connecting complementary-label learning to negative-unlabeled learning, while also advancing the state-of-the-art in practice.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new risk-consistent complementary-label learning approach without relying on the uniform label distribution assumption or requiring an additional ordinary label training set by formulating complementary-label learning as a set of negative-unlabeled binary classification problems.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Methodologically, it proposes the first consistent complementary-label learning approach without relying on the uniform distribution assumption or an additional ordinary-label dataset.  

2) Theoretically, it uncovers the relationship between complementary-label learning and negative-unlabeled learning, which provides a new perspective for understanding complementary-label learning. The consistency and convergence rate of the corrected risk estimator are proved.

3) Empirically, the proposed approach is shown to achieve superior performance over state-of-the-art methods on both synthetic and real-world benchmark datasets.

In summary, the main contribution is a new risk-consistent complementary-label learning approach with theoretical guarantees that does not require common assumptions made by previous methods, and empirical results showing it outperforms existing methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Complementary-label learning: A weakly supervised learning problem where training examples are associated with complementary labels indicating the classes they do not belong to.

- Risk consistency: A desirable theoretical property indicating that as the number of training examples increases, the expected risk of the learned classifier converges to the optimal Bayesian risk. 

- Negative-unlabeled learning: A problem setting related to positive-unlabeled learning, where the goal is to learn a binary classifier from negative and unlabeled examples. 

- One-versus-rest (OVR) strategy: A common multi-class classification strategy where binary classifiers are learned to distinguish each class from the rest. 

- Uniform distribution assumption: An assumption made by some prior complementary-label learning methods that complementary labels are sampled uniformly at random across the non-ground-truth classes.

- Irreducibility assumption: An assumption for class prior estimation stating that every class has some region in the input space where its density dominates the others.

- Risk correction function: A function used by the proposed CONU method to mitigate overfitting and ensure risk consistency when learning from complementary labels.

The key contribution is a new complementary-label learning method called CONU that does not rely on restrictive assumptions made by prior works, and provides risk consistency guarantees.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. What is the key insight that allows the authors to formulate complementary-label learning as a set of negative-unlabeled learning problems? Why is this an important observation? 

2. The authors make an assumption called Selected Completely At Random (SCAR) to model the generation process of complementary labels. How does this compare to previous assumptions like the uniform distribution assumption? What are the benefits?

3. Explain the unbiased risk estimator (URE) derived in Theorem 1. What is the intuition behind expressing the ordinary multi-class classification risk in terms of the negative and unlabeled data? 

4. Discuss the theoretical guarantees provided for the URE, including infinite sample consistency (Theorem 2) and the estimation error bound (Theorem 3). What do these results imply about the statistical properties of the proposed approach?

5. What is the cause of overfitting when directly minimizing the URE? Explain the risk correction approach and how it addresses this issue while still retaining consistency. 

6. What assumptions are made about the non-negative risk correction function $g(z)$ and what do Theorem 4 and 5 imply about the statistical properties of the corrected risk estimator?

7. How is the problem of estimating class priors approached? Explain the adaptation of the Best Bin Estimation method. What assumption is needed for this approach?

8. What experiments were conducted to validate the proposed CONU method? Discuss and interpret key results comparing CONU to state-of-the-art methods. 

9. Analyze the sensitivity analysis experiment that explores the impact of inaccurate class prior estimates. What can be concluded about the robustness of CONU?

10. This paper views complementary-label learning from a new perspective. What implications does this have on other weakly supervised learning problems? Can the ideas be extended or adapted to other scenarios?
