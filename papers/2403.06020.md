# [Multi-conditioned Graph Diffusion for Neural Architecture Search](https://arxiv.org/abs/2403.06020)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Designing neural network architectures is typically done manually via trial-and-error experiments, requiring significant expertise and time. Neural Architecture Search (NAS) aims to automate this process but struggles to efficiently search the large architecture spaces. 

Solution: 
The paper presents a NAS approach called DiNAS that utilizes discrete conditional graph diffusion processes to generate high-performing neural architectures. Specifically, it employs a diffusion model called discrete graph diffusion that retains structural information in graphs and learns to map architectures to performance metrics like accuracy. The key ideas are:

1) Formulate NAS as a conditional graph generation problem using discrete graph diffusion models that capture complex distributions better than GANs or VAEs.

2) Introduce a classifier-free guidance technique to discrete graph diffusion that allows conditioning the graph generations towards desired targets (high accuracy, low latency etc.) without needing an external classifier.  

3) Further extend classifier-free guidance to handle multiple simultaneous conditions like accuracy and latency via a multi-conditioned diffusion process.

Main Contributions:

1) First application of discrete graph diffusion models for NAS that learns the latent space using a single differentiable model.

2) Introduction of classifier-free guidance to graph diffusion networks and its multi-conditioned extension enabling rapid guided sampling.  

3) State-of-the-art results on 6 NAS benchmarks including tabular, surrogate and hardware-aware search spaces while using equal or fewer queries than previous approaches. 

4) Novel and unique architecture generations with rapid sampling (<0.2 secs per architecture) demonstrating the efficiency of the proposed NAS formulation.

In summary, the paper presents a generator network based on multi-conditioned discrete graph diffusion that can rapidly generate novel, high-performing architectures by learning from past evaluated architectures.
