# [From Inverse Optimization to Feasibility to ERM](https://arxiv.org/abs/2402.17890)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on the problem of contextual inverse optimization (CIO), where the goal is to leverage contextual information (historical data) to predict the unknown parameters of an optimization problem given known optimal solutions. Specifically, the paper considers contextual inverse linear programming (CILP) where the optimization problem is a linear program (LP). The key challenge in CILP lies in the non-differentiable nature of LPs which precludes the use of standard auto-differentiation techniques for end-to-end training. 

Proposed Solution:
The main contributions of the paper are:

1. Reduction to Convex Feasibility: For a linear prediction model, CILP is reduced to a convex feasibility problem. This enables the use of algorithms like alternating projections (POCS) to solve CILP. The resulting revgrad algorithm has a linear convergence guarantee without needing additional assumptions like degeneracy or interpolation. 

2. Reduction to ERM: The feasibility problem is further reduced to empirical risk minimization (ERM) on a smooth, convex loss function satisfying the Polyak-Lojasiewicz (PL) condition. This allows the use of efficient first-order stochastic optimization methods. For a linear model, gradient descent on the proposed loss function is shown to be equivalent to the POCS approach.

3. Experiments: The method is evaluated on synthetic (shortest path, knapsack) and real-world (Warcraft, MNIST matching) datasets. Results demonstrate superior performance compared to baselines in terms of both estimate loss and decision loss metrics.

Key Highlights:
- Handles non-differentiability of LPs without requiring differentiation through optimization solvers
- Provides theoretical guarantee on linear convergence without needing problem-specific assumptions
- Allows leveraging stochastic first-order methods for scalability while retaining performance
- Empirically shows strong performance on real datasets compared to state-of-the-art

The paper makes notable contributions in developing a principled and scalable approach for the challenging CILP problem with convergence guarantees.
