# [Augmenting Autotelic Agents with Large Language Models](https://arxiv.org/abs/2305.12487)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we augment artificial agents with the ability to represent, generate, and learn diverse, abstract, and human-relevant goals in an open-ended way, without relying on hand-coded goal representations or reward functions?In particular, the paper proposes a method to address this question by using a pretrained language model as a model of human cultural transmission. The language model is used to implement three key components of an autotelic agent architecture:1) A relabeler that describes goals achieved in the agent's trajectories.2) A goal generator that suggests new high-level goals and decomposes them into subgoals. 3) Reward functions for the generated goals.By leveraging the language model as a proxy for human cultural transmission and common sense knowledge, the goal is to enable the agent to autonomously discover and pursue more complex, creative, and human-relevant goals, in a way that moves beyond pre-defined goal spaces. The central hypothesis seems to be that augmenting autotelic agents in this way will allow for more open-ended skill discovery and goal-directed learning.So in summary, the key research question is how to use language models to provide autotelic agents with the ability to generate and learn diverse, meaningful goals in an open-ended fashion, without hand-coded goal representations. The paper proposes and evaluates an approach using LMs as a model of cultural transmission to address this question.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a Language Model Augmented Autotelic Agent (LMA3) that leverages a pretrained language model to support the representation, generation and learning of diverse, abstract, human-relevant goals in a task-agnostic way. Specifically, the key ideas presented are:- Using a language model as a model of human cultural transmission to provide common sense knowledge and capture aspects of human interests and biases. - The language model supports three main components of the autotelic architecture:   1) A relabeler that describes goals achieved in agent trajectories   2) A goal generator that suggests new high-level goals and decomposes them into known subgoals   3) Reward functions for the generated goals- Without relying on any hand-coded goal representations or reward functions, this approach allows the agent to autonomously discover and learn a large diversity of skills in a text-based environment.- The diversity and abstraction of learned goals is enhanced through careful prompting of the language model, addition of human advice examples, and using the model for recursive goal composition.- Experiments demonstrate the agent can learn many human-relevant skills without any predefined goal representations, as well as exhibit more diverse and creative discovered goals compared to ablations.In summary, the key contribution is using language models to bring in outside knowledge and augment an autotelic agent, enabling it to generate and learn more abstract, diverse and human-relevant goals in an open-ended way. This provides a path towards more capable open-ended learning agents.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces a Language Model Augmented Autotelic Agent (LMA3) that leverages a pretrained language model to support the representation, generation and learning of diverse, abstract, human-relevant goals in a task-agnostic text-based environment without relying on hand-coded goal representations or reward functions.


## How does this paper compare to other research in the same field?

Here is a brief analysis of how this paper on Language Model Augmented Autotelic Agents (LMA3) compares to other related work:- Focus on goal generation: The key focus of this work is on autonomous goal generation and representation learning using language models. Most prior work on multi-goal RL or intrinsic motivation focuses more on the skill learning aspect rather than goal imagination. LMA3 uniquely relies on LMs to generate diverse, human-relevant goals.- Language-based environments: By using text-based environments, LMA3 avoids the challenges of learning from raw sensors and low-level actions. This allows testing goal imagination in partially observable, interactive worlds requiring human-like common sense. Most prior work focuses on physical RL environments.- No predefined goals/rewards: A key distinction is that LMA3 does not assume any predefined goal space or reward functions. The goals and rewards are generated fully autonomously using the LM. This is unlike most goal-conditioned RL methods that hand-design the goals/rewards.- Hindsight learning via relabeling: LMA3 leverages trajectory relabeling by the LM to perform hindsight learning on any goals serendipitously achieved. This provides more learning signal compared to regular RL. Prior work has used engineered relabeling functions.- Cultural transmission via LM: The use of LMs as a proxy for cultural transmission of human goals/knowledge is novel. LMs provide common sense and capture some human biases/interests. Most prior autotelic agents learn in isolation without any "cultural" influence.- Minimal human input: Beyond the pretrained LM, LMA3 requires very minimal human input (just 11 examples for prompting). It does not need any hand-designed goals, reward shaping, or curricula. This makes it more autonomous compared to many intrinsically motivated RL methods.In summary, LMA3 innovates over prior work by focusing on autonomous goal generation, using LMs to provide cultural knowledge and goals, and not relying on any hand-engineered goal/reward representations. The results demonstrate that this approach enables agents to learn diverse and human-relevant skills in an open-ended fashion.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Apply the LMA3 approach in more complex, open-ended environments beyond simple text-based games. The authors suggest using more open worlds like Minecraft, coupled with image/video captioning systems to allow grounding in richer sensory environments.- Improve the skill learning algorithm to better generalize and scale to larger goal spaces. The authors mention using deep RL with transformer architectures, finetuning large language models with online experience, or advanced model-based RL algorithms.- Co-adapt the goal generator and policy to drive continual open-ended learning. The authors suggest giving the agent metrics to track its own performance and focus the relabeller on harder goals over time.- Reduce the computational cost of using large LMs, through distillation into smaller environment-specific models or waiting for cheaper access to models like GPT-4.- Develop better evaluation metrics for open-ended learning, like measuring goal diversity linguistically, human studies of skill complexity/creativity, or standardized human-agent interaction protocols.- Combine LMA3 with unsupervised skill discovery algorithms to also learn low-level sensorimotor skills hard to describe linguistically. - Implement more aspects of human cultural transmission beyond the LM, like learning from human advice/preferences or contributing goals back to humans.So in summary, the main suggestions are developing more open-ended environments, improving the learning algorithms, reducing computational costs, better evaluation, combining learned skills across modalities, and expanding the cultural transmission aspects. The authors see LMA3 as a proof-of-concept to be expanded in these directions.
