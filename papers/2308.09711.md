# [Robust Monocular Depth Estimation under Challenging Conditions](https://arxiv.org/abs/2308.09711)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop robust monocular depth estimation methods that work reliably across diverse conditions, including challenging illumination (e.g. nighttime) and weather (e.g. rain)?The key hypotheses appear to be:1) Existing self-supervised monocular depth estimation methods fail in adverse conditions due to inability to establish accurate pixel correspondences across frames. 2) Existing supervised monocular depth methods fail in adverse weather due to learning artifacts from unreliable sensor (e.g. LiDAR) measurements.3) A simple and effective solution can be developed by always providing the model with valid training signals from ideal conditions, even when adverse samples are given as input. This allows using standard losses and exploiting existing methods' effectiveness in perfect settings.4) The same principles can be applied under both self-supervised and fully supervised settings to make models robust across different conditions with a single architecture.In summary, the central goal is developing monocular depth estimation techniques that work reliably in all conditions, which existing methods fail to do. The key hypotheses are that providing always valid training signals from ideal conditions can make standard models robust.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a simple and effective approach called md4all to enable monocular depth estimation models to work reliably under diverse conditions, including challenging ones like nighttime and rain. The key ideas are:- They show that existing state-of-the-art models fail under adverse conditions due to violations of learning assumptions (for self-supervised methods) or sensor artifacts (for supervised methods). - They propose to train the model by providing always valid training signals, as if it was sunny daytime. This is done by generating challenging images (e.g. night) corresponding to normal daytime images, but computing losses only on the normal daytime images.- They apply this idea to both self-supervised and fully supervised settings with simple modifications. - Extensive experiments show their method significantly outperforms prior works on nuScenes and Oxford RobotCar datasets. A single model can reliably estimate depth in day, night, rain etc.In summary, the main contribution is proposing a simple and effective training scheme to make existing models robust in diverse conditions, without complex architecture changes or inference modifications. The effectiveness is shown through extensive experiments outperforming prior works.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other research in monocular depth estimation:- The paper focuses on improving the robustness and reliability of monocular depth estimation in challenging conditions like nighttime, rain, snow and fog. This is an important problem that has not been sufficiently addressed in prior work. - Most prior works on monocular depth estimation focus on ideal conditions like sunny daytime scenes. Some recent works have started exploring depth estimation at nighttime specifically, but a unified approach for diverse conditions is still lacking.- For supervised methods, the paper shows that learning from LiDAR ground truth in adverse weather leads to artifacts, which is an overlooked issue. The proposed method provides reliable supervision from sunny day data.- For self-supervised methods, the paper demonstrates that establishing correspondences fails in low light and with reflections. The proposed training scheme circumvents this issue.- The method relies only on image translation and training strategy changes. It does not require modifying the model architecture. This allows improving existing models like Monodepth and AdaBins with simple tweaks.- The paper explores both self-supervised and fully supervised settings. Most works focus on one supervision type. Addressing both under a unified approach is novel.- The method does not degrade performance in normal conditions while improving significantly in adverse ones. Prior works often trade off daytime accuracy.- The approach is simple and effective compared to prior works needing complex specialized branches or components.In summary, this paper introduces a novel perspective to address an important problem in a simple unified way across supervisions. The robustness improvements in diverse conditions are substantial over prior art.
