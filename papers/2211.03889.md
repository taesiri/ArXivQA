# [Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable   Categories](https://arxiv.org/abs/2211.03889)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we perform high-quality new view synthesis of deformable objects like cats and dogs from only sparse input views, by learning category-level priors from videos?

The key hypotheses of the paper are:

1) It is possible to reconstruct deformable objects from sparse views if one leverages strong category-level priors learned from a large collection of videos of objects from the same category.

2) Modeling the trajectories of points over time is important to enable the reconstruction of deforming objects.

3) A method like their proposed TrackeRF can effectively learn such category-level shape and motion priors from videos and apply them at test time to reconstruct new objects from sparse views by predicting trajectories of points.

To summarize, the central research question is how to do high-quality reconstruction of deformable objects from sparse views using learned category-level shape and motion priors. The key hypotheses are that this is possible by modeling trajectories of points over time and that their proposed TrackeRF method can effectively learn and apply such priors.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It introduces a new dataset called Common Pets in 3D (CoP3D) containing 4,200 smartphone videos of cats and dogs collected "in the wild". This is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction of deformable objects from sparse views. 

- It proposes a new method called Tracker-NeRF for few-shot reconstruction of dynamic objects. The key idea is to model deformations by predicting the 3D trajectory of each rendered point, allowing it to reconstruct unseen views by interpolating viewpoint and time. 

- It shows that Tracker-NeRF significantly outperforms previous baselines on non-rigid new-view synthesis on the CoP3D dataset. The method is able to generate higher quality and more accurate renders compared to prior work.

- It demonstrates the benefit of pre-training Tracker-NeRF on the category reconstruction task on CoP3D before fine-tuning on novel sequences. This improves single-scene reconstruction and shows the value of learning deformable reconstruction priors from large-scale video datasets.

In summary, the main contributions are introducing a new challenging dataset for non-rigid 3D reconstruction, proposing a novel method that models point trajectories to enable few-shot view synthesis of dynamic objects, and showing improved performance over baselines while highlighting the utility of category-level pre-training.
