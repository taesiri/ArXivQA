# [Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable   Categories](https://arxiv.org/abs/2211.03889)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we perform high-quality new view synthesis of deformable objects like cats and dogs from only sparse input views, by learning category-level priors from videos?

The key hypotheses of the paper are:

1) It is possible to reconstruct deformable objects from sparse views if one leverages strong category-level priors learned from a large collection of videos of objects from the same category.

2) Modeling the trajectories of points over time is important to enable the reconstruction of deforming objects.

3) A method like their proposed TrackeRF can effectively learn such category-level shape and motion priors from videos and apply them at test time to reconstruct new objects from sparse views by predicting trajectories of points.

To summarize, the central research question is how to do high-quality reconstruction of deformable objects from sparse views using learned category-level shape and motion priors. The key hypotheses are that this is possible by modeling trajectories of points over time and that their proposed TrackeRF method can effectively learn and apply such priors.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It introduces a new dataset called Common Pets in 3D (CoP3D) containing 4,200 smartphone videos of cats and dogs collected "in the wild". This is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction of deformable objects from sparse views. 

- It proposes a new method called Tracker-NeRF for few-shot reconstruction of dynamic objects. The key idea is to model deformations by predicting the 3D trajectory of each rendered point, allowing it to reconstruct unseen views by interpolating viewpoint and time. 

- It shows that Tracker-NeRF significantly outperforms previous baselines on non-rigid new-view synthesis on the CoP3D dataset. The method is able to generate higher quality and more accurate renders compared to prior work.

- It demonstrates the benefit of pre-training Tracker-NeRF on the category reconstruction task on CoP3D before fine-tuning on novel sequences. This improves single-scene reconstruction and shows the value of learning deformable reconstruction priors from large-scale video datasets.

In summary, the main contributions are introducing a new challenging dataset for non-rigid 3D reconstruction, proposing a novel method that models point trajectories to enable few-shot view synthesis of dynamic objects, and showing improved performance over baselines while highlighting the utility of category-level pre-training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper introduces a new dataset of 4,200 smartphone videos of cats and dogs captured in the wild, and proposes a new method for learning to reconstruct 3D models of deformable objects like animals from sparse video inputs by predicting trajectories of points over time.


## How does this paper compare to other research in the same field?

 Here are some key points about how this paper compares to other research in the field of dynamic new-view synthesis:

- This paper introduces a new large-scale dataset called Common Pets in 3D (CoP3D) containing 4,200 smartphone videos of cats and dogs. This provides a new benchmark for non-rigid 3D reconstruction "in the wild", compared to previous datasets which were more limited or constrained.

- The proposed method TrackeRF is one of the first deformable new-view synthesis algorithms that learns a category-level reconstruction prior from videos and applies it to reconstruct new objects at test time. Other methods have focused more on reconstructing single scenes rather than learning category-level priors.

- TrackeRF explicitly models the trajectories of 3D points over time in order to handle non-rigid deformations. This differs from other neural radiance field methods that simply add a time dimension. modeling point trajectories helps establish correspondences. 

- The use of Canonical Surface Embeddings (CSE) from Neverova et al. is a way to incorporate category-specific shape information that has not been explored much for this task before. This helps establish correspondences between different instances.

- Results on CoP3D show TrackeRF significantly outperforms existing non-rigid reconstruction baselines, demonstrating the benefits of the large-scale category-level training and explicitly modeling trajectories over time.

- The concept of pre-training a category-level model on the full dataset and then fine-tuning for single scene reconstruction is novel and shows improved reconstruction quality over training only on the single scene.

Overall, this paper pushes the state-of-the-art for few-shot non-rigid new-view synthesis by leveraging large-scale category training data and modeling temporal correspondences. The introduction of the CoP3D dataset also enables new research directions in this area.
