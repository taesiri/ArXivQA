# [SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data](https://arxiv.org/abs/2403.06952)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
Recent text-to-image (T2I) models can generate impressive images from text descriptions. However, they often fail to capture all semantics in the text input, struggling with multiple objects, spatial relationships, text rendering, etc. Improving faithfulness requires expensive human annotation of image-text pairs or preference data. 

Proposed Solution: 
The paper proposes \methodname{}, which leverages LLMs and T2I models themselves to automatically generate skill-specific image-text datasets. It then learns separate LoRA experts on each dataset before merging them to build a multi-skill T2I model.

Key stages:
1) Use GPT-3.5 with seed prompts to generate diverse text prompts targeting different skills.
2) Generate corresponding images with T2I model. 
3) Learn skill-specific LoRA expert models.
4) Merge experts to obtain multi-skill model that mitigates dataset conflicts.

Main Contributions:
- Novel paradigm to elicit pre-trained knowledge in T2I models for improved faithfulness without human annotation
- Learning and merging skill-specific experts helps handle diverse skills while mitigating conflicts 
- Experiments show significant gains over baselines on faithfulness and human preference
- Comparable gains to learning from human-annotated data
- First to show weak-to-strong generalization for T2I models

In summary, the key innovation is using LLMs and T2I models themselves to auto-generate multi-skill training data, then learning skill-specific experts to build a superior multi-skill T2I model. This shows promising self-improvement capabilities for T2I without human involvement.
