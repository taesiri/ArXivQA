# [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we replace the softmax operation in the transformer self-attention mechanism with a more efficient linear function, while still maintaining the key properties and performance of softmax attention?The paper argues that two key properties of softmax attention contribute to its strong empirical performance:1) It produces a non-negative attention matrix, avoiding aggregating negatively correlated contextual information. 2) The non-linear reweighting provides a stabilizing effect on the training.The authors propose a new linear attention module called cosFormer that satisfies both properties:1) It uses a ReLU activation on the queries and keys to ensure non-negativity.2) It incorporates a cos-based reweighting scheme to concentrate the attention distribution and exploit locality biases like softmax. The central hypothesis seems to be that by fulfilling these two properties with a decomposable linear function and reweighting scheme, cosFormer can achieve comparable performance to softmax attention while enjoying reduced computational complexity. The paper aims to validate this hypothesis through extensive experiments on language modeling, text classification, and long sequence tasks.In summary, the key research question is whether they can effectively replace softmax attention with a more efficient linear alternative without sacrificing model accuracy or expressiveness, by retaining just the core beneficial properties of softmax. The cosFormer approach is proposed and evaluated as a solution to this question.
