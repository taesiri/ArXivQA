# [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we replace the softmax operation in the transformer self-attention mechanism with a more efficient linear function, while still maintaining the key properties and performance of softmax attention?The paper argues that two key properties of softmax attention contribute to its strong empirical performance:1) It produces a non-negative attention matrix, avoiding aggregating negatively correlated contextual information. 2) The non-linear reweighting provides a stabilizing effect on the training.The authors propose a new linear attention module called cosFormer that satisfies both properties:1) It uses a ReLU activation on the queries and keys to ensure non-negativity.2) It incorporates a cos-based reweighting scheme to concentrate the attention distribution and exploit locality biases like softmax. The central hypothesis seems to be that by fulfilling these two properties with a decomposable linear function and reweighting scheme, cosFormer can achieve comparable performance to softmax attention while enjoying reduced computational complexity. The paper aims to validate this hypothesis through extensive experiments on language modeling, text classification, and long sequence tasks.In summary, the key research question is whether they can effectively replace softmax attention with a more efficient linear alternative without sacrificing model accuracy or expressiveness, by retaining just the core beneficial properties of softmax. The cosFormer approach is proposed and evaluated as a solution to this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a new efficient transformer model called cosFormer that can achieve comparable or better performance to the standard transformer while having linear time and space complexity. The key ideas are:1. Identifying two key properties of the softmax attention in standard transformers: (i) non-negativity of the attention matrix, and (ii) a non-linear re-weighting scheme. 2. Proposing a linear attention module that satisfies these two properties:- Using ReLU activation on queries and keys to ensure non-negativity. - Introducing a cosine-based re-weighting scheme to concentrate the distribution of attention connections.3. Achieving decomposability of the attention computation into linear complexity by leveraging the Ptolemy's theorem.4. Demonstrating strong performance of cosFormer on autoregressive and bidirectional language modeling, multiple downstream NLP tasks, and long sequence modeling benchmarks. It achieves comparable or better results than standard transformers and other efficient transformers.5. Providing efficiency analysis showing cosFormer's advantages in speed and memory over competitors, especially for long sequences.In summary, the main contribution is developing a new way to linearize transformer attention that retains key properties of softmax and achieves superior efficiency and broad applicability. The simplicity yet effectiveness of cosFormer in replacing the softmax is the central novelty.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new linear transformer called cosFormer that replaces the softmax attention mechanism with a decomposable linear operation and cosine-based re-weighting to achieve comparable performance to vanilla transformers while reducing time and space complexity to O(N).


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here are some key ways this work compares to other research in efficient transformers:- It proposes a novel linear attention mechanism called cosFormer that replaces softmax attention. This is different from many other approaches that try to approximate softmax attention using kernels or patterns. - The cosFormer attention uses a ReLU activation and cosine re-weighting to capture key properties of softmax attention like non-negativity and concentration of weights. This is a simple but effective design not explored by other work.- Experiments show cosFormer performs on par or better than softmax attention on language modeling, text classification, and long sequence tasks. Many other efficient transformers show degradation compared to the full softmax attention, especially on difficult long-range tasks.- The paper demonstrates strong performance on the Long Range Arena benchmark, outperforming all other efficient transformers. This shows the effectiveness of cosFormer for modeling long sequences.- The method has linear complexity like other efficient transformers, but seems to offer better accuracy. The cosFormer also does not require any special training techniques like some other linear attention methods.- The ablation studies provide useful analysis about the impact of the ReLU activations and cosine re-weighting in cosFormer. This sheds light on why it works well.Overall, the cosFormer presents a novel way of linearizing attention that outperforms prior work in many cases. The simplicity of the approach and strong empirical results across diverse tasks help demonstrate its effectiveness compared to other techniques. The paper makes a nice contribution in showing you can effectively replace softmax attention with a simple, properly designed linear mechanism.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing more efficient implementations of transformer models to handle longer sequences. The authors suggest exploring approaches like sparsifying the attention matrix or approximating the softmax to reduce the quadratic complexity.- Exploring different inductive biases and positional encodings to help the model learn better representations and long-range dependencies. The authors suggest ideas like incorporating syntactic or semantic knowledge. - Applying the transformer architecture to new modalities and tasks beyond NLP, such as computer vision, speech, and multimodal applications. The authors suggest the potential for transformers to become a ubiquitous model for sequential data.- Pre-training larger transformer models on more data to learn even better representations. The authors suggest pre-training could lead to continued improvements in downstream tasks.- Studying what linguistic properties and long-range dependencies transformers are able to capture compared to RNNs and CNNs. The interpretability of transformers is still not fully understood.- Developing theoretical foundations and analyses for attention and transformers, such as analyzing their complexity, convergence, generalization ability, etc. Much of the progress so far is empirical.In summary, the main future directions are improving efficiency and scalability, incorporating better inductive biases, applying transformers to new domains, pre-training larger models, interpretability, and theoretical analysis. The authors see great potential for continued advances building on the transformer architecture.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a new linear transformer model called cosFormer that aims to achieve comparable or better accuracy to the vanilla transformer while having linear time and space complexity. The key insight is to replace the non-decomposable softmax operation in the vanilla transformer with a linear operation and a cosine-based distance re-weighting mechanism. Specifically, cosFormer uses a ReLU activation function to ensure non-negativity in the attention matrix and avoid aggregating negatively-correlated information. It also incorporates a cosine re-weighting scheme to concentrate the distribution of attention weights, exploiting the locality inductive bias commonly found in natural language tasks. Extensive experiments on autoregressive and bidirectional language modeling, downstream text classification tasks, and the Long-Range Arena benchmark demonstrate cosFormer's effectiveness. It achieves on par or better results than the vanilla transformer and state-of-the-art performance on long sequence modeling, with significantly improved efficiency. The proposed linearization and re-weighting techniques provide useful insights on designing accurate yet efficient transformers.
