# [SingingHead: A Large-scale 4D Dataset for Singing Head Animation](https://arxiv.org/abs/2312.04369)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of this paper:

This paper introduces a large-scale singing head dataset named SingingHead which contains over 27 hours of synchronized singing video, 3D facial motion, singing audio, and background music collected from 76 subjects. The goal is to enable research on generating realistic 3D and 2D facial animations when singing. Along with this dataset, the authors propose a unified framework called UniSinger that can generate 3D facial motion according to a singing audio input, as well as synthesize a 2D singing portrait video given an additional reference face image. UniSinger employs a variational autoencoder structure to model the distribution of plausible 3D facial motions and sample diverse motions conditioned on the singing audio. The motions can then be rendered into 2D videos using a face renderer based on a U-Net generator trained with adversarial losses. Experiments show UniSinger generates more accurate and natural 3D motions compared to prior arts like FaceFormer and CodeTalker. It also synthesizes 2D videos with better synchronization and quality than methods designed for talking heads, demonstrating the necessity and significance of a singing-specialized dataset. In summary, this work provides the first high-quality dataset to facilitate singing facial animation research, and presents a strong baseline model for this task.
