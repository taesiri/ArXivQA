# Language Instructed Reinforcement Learning for Human-AI Coordination

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is: How can we use natural language instructions to guide reinforcement learning agents towards human-compatible coordination policies in multi-agent settings?Specifically, the paper proposes a framework called "instructRL" that allows humans to provide high-level natural language instructions to AI agents. The key ideas are:- Use a pretrained language model to generate a prior policy conditioned on the human instruction and current observations. - Regularize the RL training objective using the language model prior policy as a reference. This guides the agent to converge to equilibria that align with human preferences expressed in the instructions.- Evaluate instructRL in two multi-agent coordination games - a simple "Say-Select" game and the more complex Hanabi game.- Show that instructRL produces policies that follow the given language instructions. Human evaluations also confirm that knowing the instruction helps humans coordinate much better with the trained agents.So in summary, the main hypothesis is that natural language instructions can be used to steer RL agents towards human-compatible coordination policies, avoiding suboptimal equilibria that humans find unintuitive. The instructRL framework and experiments on two games provide evidence supporting this hypothesis.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contribution of this paper appears to be proposing a new framework called "Language Instructed Reinforcement Learning" (instructRL) for training reinforcement learning agents to better coordinate with humans. The key ideas are:- Humans can provide natural language instructions to specify how they want the AI agent to collaborate. This guides the agent towards human-preferred equilibria.- They use large language models (LLMs) to generate a prior policy conditioned on the human instruction and observations. - The LLM prior is used to regularize the RL objective so the final policy satisfies the instruction.- They show this method can produce different but strong policies based on different instructions in both a toy coordination game and the challenging Hanabi benchmark.- Through human evaluations in Hanabi, they demonstrate that humans can coordinate much better with the agents when given the language instructions, even though the underlying policies are identical.In summary, the main contribution is proposing instructRL to incorporate natural language instructions and leverage large language models to allow humans to easily guide RL agents towards policies that coordinate well with human preferences and abilities.


## How does this paper compare to other research in the same field?

Based on the abstract, this paper presents a novel framework called "instructRL" that uses natural language instructions and large language models to guide reinforcement learning (RL) agents towards policies that are more compatible with human preferences in collaborative settings. Here are some key comparisons to related work:- Most prior work on human-AI coordination focuses on either using human data/feedback to directly shape the policy, designing algorithms to be more human-like, or producing diverse policies to generalize better. This paper takes a different approach by using language instructions as guidance.- It builds on prior work that uses natural language for exploration or reward shaping in single-agent RL, but focuses more on the equilibrium selection problem in multi-agent RL. Language is used to steer convergence rather than mainly for exploration.- It is related to methods that employ large language models for specifying rewards or planning, but differs in using LLMs to construct useful priors for regularizing RL rather than fully defining rewards or planning with LLMs. This allows application to more complex tasks.- The idea of using human inputs to affect RL training has similarities with interactive RL and learning from critiques. A key difference is that the instructRL framework uses a fixed language instruction without interactive feedback.- For human-AI coordination, the approach is complementary to methods that produce diverse policies or learn from human data. It provides an alternative way to reach human-compatible equilibria without needing diverse policies or human data.In summary, the key novelty seems to be using fixed language instructions and LLM priors to regularize RL towards human-compatible equilibria in multi-agent settings. The evaluations in a simple coordination game and Hanabi demonstrate the promise of this approach compared to prior methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Improving the sample efficiency and scalability of the approach. The authors note that their method currently requires a large number of samples to learn good policies, so developing more sample-efficient versions could allow it to scale better to more complex environments. This could involve techniques like more advanced exploration methods or leveraging priors.- Extending the approach to partially observable environments. The current method assumes full observability, but the authors suggest adapting it to partially observable settings which are more realistic for many real-world problems. This could require memory and inference mechanisms to handle hidden state.- Incorporating natural language communication during training. The authors propose allowing agents to communicate with natural language during the training process to potentially learn more human-compatible coordination strategies. This could also help provide explanations for agent behavior.- Exploring different regularization techniques. The paper uses KL regularization to incorporate the language prior, but other techniques like reward shaping could be promising to explore as well. Identifying the most effective training frameworks is an open question.- Applying the method to more complex, real-world environments. Testing the approach in more realistic domains like robotics tasks could better validate its usefulness for human-AI coordination. Challenges around grounding language in such settings would need to be addressed.- Enabling online adaptation at test time. An interesting direction is adapting pre-trained agents online to new users by incorporating language input at test time. This could allow personalizing coordination strategies.In summary, the main future directions focus on improving scalability, partial observability, leveraging language during training, testing on more complex domains, and online adaptation at test time. Advancing along these dimensions could make the instructRL approach more practical for real-world human-AI coordination.
