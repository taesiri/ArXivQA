# [Zebra: Extending Context Window with Layerwise Grouped Local-Global   Attention](https://arxiv.org/abs/2312.08618)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of this paper:

This paper introduces a new model architecture called Zebra designed to enhance the capabilities of Large Language Models (LLMs) to process long text sequences. Zebra addresses issues with extending the context window in LLM transformers which exhibits quadratic complexity in full attention. It proposes a novel grouped local-global attention approach and rotary positional embedding. The local-global attention layers alternate akin to a zebra's stripes. Comprehensive experiments compare attention mechanisms and positional embeddings on models pretrained from scratch with long sequences. Results show Zebra's group attention is most efficient with comparable performance to full attention, while significantly reducing computation and memory needs for both training and inference. Zebra is further evaluated by continuing training of Lama and with long instruction tuning datasets combining both short and long instances. Evaluations demonstrate Zebra maintains strong performance on short benchmarks while achieving superior results on long sequence tasks, outperforming Lama overall after tuning. Zebra offers an effective architecture advancing LLMs' understanding of extensive texts for applications requiring deep comprehension and reasoning over large volumes of information.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper introduces a new model architecture called Zebra for extending the context window capabilities of large language models (LLMs). The key challenges in increasing context length for LLMs like BERT and GPT that use the Transformer architecture are the quadratic growth in computation and memory requirements, diluted attention over very long sequences, and imbalanced distribution of training signals. 

To address these challenges, Zebra incorporates two main components - grouped local-global attention layers and rotary positional embeddings. The attention layers are organized into groups with one global attention layer followed by several local attention layers. This significantly reduces computation and memory needs while achieving comparable performance to full global attention, especially for longer sequences. Rotary positional embeddings are used instead of absolute positional embeddings to provide better generalization on long sequences.

The paper details comprehensive experiments on variants of attention mechanisms and positional embeddings using 117M and 345M parameter models trained from scratch. Based on the results, grouped attention with rotary positional embeddings is selected for the Zebra architecture. Zebra is then scaled to a 7B parameter model by continuing training of the Llama-2-7B model using long context adaptation on up to 100B tokens spanning diverse domains. Results show Zebra-LCAT performs similarly to Llama-2 on short context tasks and better on long contexts in terms of perplexity.

The model is further tuned on a mix of short and long instruction tuning datasets specially curated for summarization, question answering and writing tasks. Evaluations demonstrate strong performance on both short and long benchmarks, outperforming Llama-2 overall. The architectural innovations provide efficiency benefits in training and inference while achieving high accuracy across sequence lengths.

In summary, the Zebra model with its novel architecture sets a new standard for effectively extending context length in LLMs for long document understanding, without compromising efficiency or performance on shorter sequences. The comprehensive experiments and detailed efficiency analysis further validate the advantages of this approach.
