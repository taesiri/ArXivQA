# [Zebra: Extending Context Window with Layerwise Grouped Local-Global   Attention](https://arxiv.org/abs/2312.08618)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of this paper:

This paper introduces a new model architecture called Zebra designed to enhance the capabilities of Large Language Models (LLMs) to process long text sequences. Zebra addresses issues with extending the context window in LLM transformers which exhibits quadratic complexity in full attention. It proposes a novel grouped local-global attention approach and rotary positional embedding. The local-global attention layers alternate akin to a zebra's stripes. Comprehensive experiments compare attention mechanisms and positional embeddings on models pretrained from scratch with long sequences. Results show Zebra's group attention is most efficient with comparable performance to full attention, while significantly reducing computation and memory needs for both training and inference. Zebra is further evaluated by continuing training of Lama and with long instruction tuning datasets combining both short and long instances. Evaluations demonstrate Zebra maintains strong performance on short benchmarks while achieving superior results on long sequence tasks, outperforming Lama overall after tuning. Zebra offers an effective architecture advancing LLMs' understanding of extensive texts for applications requiring deep comprehension and reasoning over large volumes of information.
