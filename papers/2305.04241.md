# [Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing   Important Tokens](https://arxiv.org/abs/2305.04241)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing an efficient Transformer-based model that can handle ultra long input sequences of 128K tokens or more. The key ideas and hypotheses appear to be:- In many NLP tasks, only a small subset of "VIP" tokens in the input sequence are most relevant for making predictions. The rest of the tokens can be aggressively compressed without much loss in accuracy.- By selectively compressing input tokens based on their relevance to these VIP tokens, the sequence length fed into each Transformer layer can be dramatically reduced, improving runtime and memory efficiency.- Despite compressing a majority of tokens, performance on downstream tasks can be competitive or even improved compared to baseline Transformers operating on the full sequence.- This VIP-token centric compression scheme can enable standard Transformer models to practically handle input sequences with 128K or more tokens, unlocking accuracy gains on tasks requiring very long context.So in summary, the central hypothesis is that aggressive yet selective compression guided by a small set of "VIP" tokens can allow standard Transformers to efficiently scale to ultra long sequences while maintaining or improving accuracy on downstream tasks. The paper aims to demonstrate this via a concrete compression algorithm and extensive experiments.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a VIP-token centric compression (VCC) scheme to efficiently handle ultra long sequences in Transformers. The key idea is to selectively compress the input sequence based on the impact on approximating the representation of a small subset of "VIP-tokens" that are most relevant for the downstream task. This allows aggressively compressing irrelevant information while preserving relevant signal.2. It provides a specific instantiation of VCC using a multi-resolution compression technique adapted from prior work. This constrcuts a compression matrix S_c to compress the non-VIP tokens. 3. It introduces an efficient tree-based data structure to avoid the overhead of explicit compression and decompression at each layer. This reduces the overall complexity.4. It demonstrates strong empirical performance of VCC on a wide range of tasks using standard Transformer models. The method achieves competitive or better accuracy compared to baselines while being much more efficient. It also shows VCC can scale Transformers up to 128K tokens while maintaining accuracy gains.In summary, the main contribution is an efficient VIP-token focused compression technique to handle ultra long sequences in Transformers, along with empirical validation of its effectiveness. The method is shown to enable standard Transformers to scale to much longer contexts than previously feasible.
