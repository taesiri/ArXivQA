# [Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing   Important Tokens](https://arxiv.org/abs/2305.04241)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing an efficient Transformer-based model that can handle ultra long input sequences of 128K tokens or more. The key ideas and hypotheses appear to be:- In many NLP tasks, only a small subset of "VIP" tokens in the input sequence are most relevant for making predictions. The rest of the tokens can be aggressively compressed without much loss in accuracy.- By selectively compressing input tokens based on their relevance to these VIP tokens, the sequence length fed into each Transformer layer can be dramatically reduced, improving runtime and memory efficiency.- Despite compressing a majority of tokens, performance on downstream tasks can be competitive or even improved compared to baseline Transformers operating on the full sequence.- This VIP-token centric compression scheme can enable standard Transformer models to practically handle input sequences with 128K or more tokens, unlocking accuracy gains on tasks requiring very long context.So in summary, the central hypothesis is that aggressive yet selective compression guided by a small set of "VIP" tokens can allow standard Transformers to efficiently scale to ultra long sequences while maintaining or improving accuracy on downstream tasks. The paper aims to demonstrate this via a concrete compression algorithm and extensive experiments.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a VIP-token centric compression (VCC) scheme to efficiently handle ultra long sequences in Transformers. The key idea is to selectively compress the input sequence based on the impact on approximating the representation of a small subset of "VIP-tokens" that are most relevant for the downstream task. This allows aggressively compressing irrelevant information while preserving relevant signal.2. It provides a specific instantiation of VCC using a multi-resolution compression technique adapted from prior work. This constrcuts a compression matrix S_c to compress the non-VIP tokens. 3. It introduces an efficient tree-based data structure to avoid the overhead of explicit compression and decompression at each layer. This reduces the overall complexity.4. It demonstrates strong empirical performance of VCC on a wide range of tasks using standard Transformer models. The method achieves competitive or better accuracy compared to baselines while being much more efficient. It also shows VCC can scale Transformers up to 128K tokens while maintaining accuracy gains.In summary, the main contribution is an efficient VIP-token focused compression technique to handle ultra long sequences in Transformers, along with empirical validation of its effectiveness. The method is shown to enable standard Transformers to scale to much longer contexts than previously feasible.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work on scaling Transformers to long sequences:- It proposes a novel VIP-token centric compression scheme to reduce the sequence length fed into each Transformer layer. Other works like Funnel Transformer and Pyramid Transformer also compress the sequence, but do so in a task-agnostic way, while this paper argues a task-aware compression focused on preserving information relevant to predetermined VIP tokens can be more effective. - The paper empirically demonstrates strong performance and efficiency on a wider range of long sequence tasks compared to prior work. Many existing efficient Transformers have mainly been evaluated on a handful of tasks like summarization or QA, while this paper tests the method on 10+ datasets including classification, QA, and summarization.- It shows the approach can easily scale to 128K length sequences with standard Transformers, while most prior work focuses on lengths up to 16K. The ability to handle sequences of this scale allows the method to be applied to tasks like book-level QA or summarization that were previously infeasible.- The compression scheme does not require any changes to the Transformer architecture itself, just modifications to the input/output at each layer. So it should be compatible with any standard Transformer model. Other approaches like Linformer or Performer require architectural modifications.- Unlike some methods that aim to reduce the asymptotic complexity of attention from O(n^2) to O(n), this paper retains the standard O(n^2) Transformer attention but shows how clever compression can still provide efficiency gains.The main unique contributions are around the VIP-token focused compression and demonstrating strong empirical performance on a wide array of very long sequence tasks. Overall, it offers a nicely complementary approach to other efficient Transformer techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, some potential future research directions suggested by the authors include:- Extending their method to the decoder of encoder-decoder models to further boost efficiency while maintaining model performance. The current implementation focuses on the encoder, but the authors mention applying similar ideas to approximate computation in the decoder as well.- Making the selection of VIP-tokens more systematic and task-generic. Currently, VIP-token selection is performed in a task-dependent manner, but the authors suggest this process could potentially be made more systematic. - Applying their method to other domains beyond NLP, such as computer vision. The general framework of selectively compressing input sequences based on a subset of key tokens may be applicable to sequential inputs in other modalities like images.- Exploring different choices for the compression matrix S_c beyond the multi-resolution analysis approach described. The authors mention their framework is compatible with different data-driven sketching methods for obtaining S_c.- Reducing the pre-processing overhead of O(n*d) to convert the input sequence to their proposed data structure. This cost is incurred only at the start and end of the Transformer, but methods to further reduce this cost could improve overall efficiency.- Applying more sophisticated decoding methods during inference like beam search instead of greedy decoding. The current results use greedy decoding for simplicity.- Evaluating the performance on a broader range of sequence lengths, especially shorter lengths where standard Transformers may be sufficiently efficient. The current work focuses on long sequences between 4K to 128K tokens where efficiency gains are more substantial.In summary, the main future directions relate to expanding the application domains, making parts of the method more generic/systematic, and continuing to improve efficiency.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method called VIP-token centric compression (VCC) to improve the efficiency of Transformers when processing ultra long sequences (e.g. 128K tokens or more). The key idea is that in many natural language tasks, only a small subset of "VIP" tokens are most relevant for predicting the output. So the method selectively compresses the sequence based on the impact on approximating VIP-token representations. It exploits the fact that most tokens are not critical to preserve entirely. Compared to methods like Longformer and BigBird, VCC achieves over 3x speedup on 4K and 16K length sequences without sacrificing accuracy by focusing computational effort on the small subset of critical VIP-tokens. Experiments across many language tasks show VCC allows scaling to much longer 128K token sequences while improving accuracy. The method does not require changes to the Transformer architecture itself. Overall, VCC demonstrates the benefits of selective compression and shows potential to expand the reach of Transformers to even longer sequences.
