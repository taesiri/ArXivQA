# [Empirical study of pretrained multilingual language models for zero-shot   cross-lingual generation](https://arxiv.org/abs/2310.09917)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Zero-shot cross-lingual generation assumes finetuning a multilingual pretrained language model (mPLM) on a generation task in one language, then using it to generate predictions in other languages not seen during finetuning.
- Two main problems arise: generating irrelevant/incoherent texts, and generating text in the wrong language. 

Proposed Solutions:
- The paper tests different mPLMs beyond just mT5, including mBART and the NLLB translation model, to see their effectiveness.  
- It compares 6 adaptation methods like full finetuning, adapters, prompt tuning, freezing encoder/embeddings, mixing target language data, and using multiple source languages.
- It studies the effect of tuning learning rates and intermediate tuning before task finetuning.

Key Findings:
- Tuning learning rate is very important - lower rates alleviate generating in wrong language for full finetuning.  
- With careful tuning, simple full finetuning acts as a strong baseline. Other good approaches are adapters and multiple source languages.
- mBART performs similarly to same-sized mT5. NLLB is competitive for summarization but not QA.
- Qualitatively, mBART better handles long outputs due to denoising pretraining, while mT5 better handles short outputs due to masking pretraining.

Main Contributions:
- First unified comparison of adaptation methods and models for cross-lingual generation
- Showing importance of learning rate tuning - proper tuning eliminates most wrong language generation
- Finding that full finetuning is a strong baseline compared to recent complex methods
- Analysis and comparison of different PLMs - mBART vs mT5, and translation-based NLLB


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper empirically studies different multilingual pretrained language models and adaptation methods for zero-shot cross-lingual generation, finding that careful learning rate tuning alleviates generating in the wrong language, and full finetuning, adapters, and multi-source training are among the best performing approaches.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

Conducting an empirical study of various approaches for zero-shot cross-lingual generation and of alternatives to the commonly-used mT5 model. Specifically, the authors:

- Test alternative mPLMs beyond just mT5, such as mBART and NLLB-200
- Compare various adaptation methods proposed in the literature in a unified setting
- Pay attention to adaptation hyperparameters like learning rate and compare models and methods more rigorously
- Find that tuning learning rate is very important to alleviate issues like generating text in the wrong language
- Show that mBART performs similarly to mT5 and NLLB-200 is competitive for summarization

In summary, the main contribution is a systematic and in-depth empirical analysis of models and methods for zero-shot cross-lingual text generation, with careful attention to experimental details.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Multilingual pretrained language models (mPLMs)
- Zero-shot cross-lingual generation 
- Catastrophic forgetting
- Source language hallucination
- mT5
- mBART
- NLLB-200
- Summarization 
- Question answering
- Learning rate tuning
- Full finetuning
- Adapters 
- Parameter efficient tuning
- Mixing in target language data
- Multiple source languages

The paper compares different approaches for zero-shot cross-lingual generation using mPLMs like mT5, mBART and NLLB-200 on summarization and question answering tasks. It studies techniques like careful learning rate tuning, full finetuning, adapters, prompt tuning, mixing target language data and using multiple source languages. The key focus is on overcoming issues like catastrophic forgetting and source language hallucination in zero-shot cross-lingual setting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper compares several adaptation methods for zero-shot cross-lingual generation. What are the main adaptation methods studied and what are the key findings regarding their relative performance?

2. The paper underscores the importance of tuning the learning rate for finetuning in the cross-lingual transfer setting. How does the choice of learning rate impact model performance and generation in the correct language? What guidance does the paper provide on choosing an optimal learning rate?

3. The paper studies the impact of intermediate tuning (IT) before finetuning the model on the downstream tasks. For which adaptation methods was IT found to be essential and why? How was the IT procedure designed in this work?

4. The paper compares several multilingual encoder-decoder models as backbones for cross-lingual generation. What are the key differences between mT5, mBART and NLLB-200 in terms of architecture, pretraining methodology and use of language codes? 

5. What hypotheses did the authors have regarding the potential benefits of using mBART and NLLB-200 compared to the commonly used mT5? Were these hypotheses supported by the experimental results?

6. The paper studies performance on summarization and question answering tasks. Did the relative performance of methods differ between the two tasks? What differences were observed and what could explain them?

7. The paper inspects model predictions manually. What issues were noticed regarding faithfulness, hallucinations and average output lengths? How did these relate to model architecture and pretraining methodology?

8. Larger versions of mT5 and NLLB-200 are also evaluated. Did the findings transfer to the larger models in terms of performance dynamics and optimal learning rates? Were the absolute metric scores higher for larger models?

9. For which languages and tasks did NLLB-200 demonstrate competitive performance compared to mT5 and mBART? Why did it lag behind substantially on question answering? 

10. The finetuned version of mBART (mBART-tr) was found to perform poorly on both tasks. What explanation is provided in the paper? Does this contrast with the strong performance of NLLB-200 and if so, why?
