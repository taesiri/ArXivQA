# Learning Joint Spatial-Temporal Transformations for Video Inpainting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an effective video inpainting method that can fill in missing regions in video frames with spatially and temporally coherent contents?Specifically, the paper proposes a novel deep learning model called Spatial-Temporal Transformer Networks (STTN) to address the limitations of prior video inpainting methods and improve both spatial and temporal coherence. The key ideas include:- Formulating video inpainting as a "multi-to-multi" problem that takes multiple input frames (both neighboring and distant frames) to simultaneously complete all frames. - Proposing a spatial-temporal transformer module that searches for coherent contents from all input frames along both spatial and temporal dimensions using a multi-scale patch-based attention mechanism.- Optimizing the model using both reconstruction losses and a spatial-temporal adversarial loss to ensure perceptually realistic and coherent results.So in summary, the central hypothesis is that jointly learning spatial-temporal transformations in a deep generative model optimized by spatial-temporal losses can significantly improve video inpainting performance and coherence. The proposed STTN model aims to test this hypothesis.
