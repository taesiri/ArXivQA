# [SCAN: Learning to Classify Images without Labels](https://arxiv.org/abs/2005.12320)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent?The paper is investigating unsupervised image classification, where the goal is to cluster images into groups corresponding to semantic classes, without having access to label information during training. The key hypothesis appears to be that separating feature learning (through self-supervised pretext tasks) from clustering (through a novel loss function) will lead to better semantic clustering compared to recent end-to-end approaches.Specifically, the paper hypothesizes that:- Using a self-supervised pretext task to obtain semantically meaningful features provides a better prior for clustering compared to relying on the network architecture. - Integrating semantic nearest neighbors from the pretext task into a custom clustering loss avoids issues like dependence on low-level features or cluster degeneracy.- Decoupling feature learning and clustering in a two-step approach is better than end-to-end learning for unsupervised semantic clustering.In summary, the main research question is how to perform unsupervised semantic clustering of images, with the key hypothesis being that a two-step approach separating feature learning and clustering will outperform recent end-to-end methods.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Proposing a two-step approach for unsupervised image classification that first learns semantic feature representations using a self-supervised pretext task, and then clusters the images using those features. This is in contrast to recent end-to-end approaches.- Using the nearest neighbors in the learned feature space as a prior for clustering. The paper shows both empirically and analytically that nearest neighbors tend to belong to the same semantic class. - Introducing a loss function that classifies an image and its nearest neighbors together by maximizing their dot product after softmax. This enforces consistent and discriminative cluster assignments.- A self-labeling step to further refine the clusters using the most confident predictions as pseudo-labels. This allows correcting mistakes from noisy nearest neighbors.- Demonstrating strong performance of the proposed approach, dubbed SCAN (Semantic Clustering by Adopting Nearest neighbors), on CIFAR10, CIFAR100, STL10, and ImageNet datasets. SCAN outperforms prior unsupervised methods by large margins, achieving 26.6% higher accuracy on CIFAR10 for instance.- Showing that SCAN can scale to large datasets like ImageNet. It is the first unsupervised method to perform well on ImageNet, even outperforming some semi-supervised approaches.In summary, the key contribution is a novel and effective framework for unsupervised image classification, justified both theoretically and empirically, that decouples feature learning from clustering. The results significantly advance the state-of-the-art in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a two-step approach for unsupervised image classification where semantically meaningful features are first learned through a self-supervised pretext task, and then used as a prior in a learnable clustering model to group images into semantic clusters without reliance on low-level features.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper presents an unsupervised image classification method that separates feature learning and clustering into two steps. This deviates from some recent end-to-end approaches that jointly learn features and cluster assignments. The two-step approach allows the method to leverage semantically meaningful features from self-supervised pretext tasks, avoiding dependence on low-level features.- The use of a pretext task like instance discrimination to obtain semantically meaningful features is similar to other representation learning methods. However, the paper shows that directly clustering the pretext features with k-means leads to poor performance, motivating the proposed loss function in the second stage.- The loss function enforcing consistency between an image, its augmentations, and its nearest neighbors is unique. It differs from consistency losses in IIC, DeepCluster, etc. that only enforce consistency between augmentations. Using nearest neighbors provides a better prior for semantic clustering.- The self-labeling step to refine the clusters is similar to pseudo-labeling approaches. However, the paper shows the importance of using different augmentations during this stage to avoid overfitting.- The results significantly outperform recent state-of-the-art end-to-end methods like IIC, especially on smaller datasets like CIFAR-10/100 and STL-10. The method also achieves promising results on ImageNet, outperforming some semi-supervised techniques.Overall, the paper presents several innovations in both the two-step approach and specific techniques used. The strong empirical results validate that separating feature learning and clustering can be more effective than joint end-to-end learning for unsupervised classification.
