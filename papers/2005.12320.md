# [SCAN: Learning to Classify Images without Labels](https://arxiv.org/abs/2005.12320)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent?The paper is investigating unsupervised image classification, where the goal is to cluster images into groups corresponding to semantic classes, without having access to label information during training. The key hypothesis appears to be that separating feature learning (through self-supervised pretext tasks) from clustering (through a novel loss function) will lead to better semantic clustering compared to recent end-to-end approaches.Specifically, the paper hypothesizes that:- Using a self-supervised pretext task to obtain semantically meaningful features provides a better prior for clustering compared to relying on the network architecture. - Integrating semantic nearest neighbors from the pretext task into a custom clustering loss avoids issues like dependence on low-level features or cluster degeneracy.- Decoupling feature learning and clustering in a two-step approach is better than end-to-end learning for unsupervised semantic clustering.In summary, the main research question is how to perform unsupervised semantic clustering of images, with the key hypothesis being that a two-step approach separating feature learning and clustering will outperform recent end-to-end methods.
