# [Language-Informed Visual Concept Learning](https://arxiv.org/abs/2312.03587)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a framework to learn visually grounded concept representations that are aligned to language-specified semantic axes, such as category, color, and style. The key idea is to train encoder networks to extract axis-specific embeddings from images, by using the objective of reconstructing the input images through a pre-trained text-conditional generative model. To encourage disentanglement across axes, the extracted embeddings are anchored to sparse text embeddings obtained by querying a visual question answering model on the input images. At test time, the encoders can extract disentangled concept embeddings from new images in a feedforward pass. These embeddings encapsulate fine-grained visual nuances beyond the discrete text labels, and can be remixed to compose images with novel visual concept combinations. Experiments demonstrate superior compositional capability over textual inversion baselines. The model can also quickly adapt to unseen concepts through lightweight test-time finetuning, while retaining axis disentanglement. By simply distilling from large pre-trained vision-language models and reusing their inherent disentangled semantic space, this work presents an effective framework for learning reusable and editable visual concepts without extensive manually supervision.
