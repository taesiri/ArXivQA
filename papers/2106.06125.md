# Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is: What is the crucial bottleneck in current pretrain-finetune framework for natural language generation (NLG) tasks, and how can it be overcome? The authors hypothesize that the key issue lies in the inflexibility caused by using a one-size-fits-all vocabulary in the pretrain-finetune framework. This potentially weakens the effect when applying pretrained models to downstream NLG tasks, due to subword distribution mismatches between the upstream and downstream tasks.To address this, the authors propose extending the vanilla pretrain-finetune pipeline with an extra embedding transfer step to eliminate the token granularity gaps. Specifically, they introduce a plug-and-play embedding generator to produce representations for mismatch tokens in downstream tasks based on their morphologically similar tokens' embeddings from the pretrained model.In summary, the paper focuses on investigating and overcoming the subword discrepancy issue in applying pretrain-finetune to NLG via embedding transfer.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new pretrain-finetune training strategy to bridge the subword gaps between upstream pretraining and downstream finetuning models in natural language generation tasks. Specifically, the key ideas are:1. Identifying the issue of subword discrepancy caused by using the fixed vocabulary of pretraining models in downstream tasks. This leads to unsuitable granularity and under-represented words which hurt the performance. 2. Introducing an embedding transfer step by pretraining an embedding generator. This allows changing the model vocabulary for downstream tasks to better match their data distribution. The generator can produce embeddings for any required tokens based on pretraining embeddings of morphologically similar tokens.3. Validating the effectiveness of the proposed strategy on a variety of NLG tasks including domain adaptation, knowledge transfer, machine translation and question generation. Results show significant improvements over strong baselines by alleviating exposure bias, reducing computational cost, and better handling under-represented tokens.In summary, the key contribution is proposing a simple yet effective solution to the subword discrepancy issue in applying pretrain-finetune paradigm to natural language generation tasks. The embedding transfer step enhances the flexibility and applicability of pretrained models.
