# Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is: What is the crucial bottleneck in current pretrain-finetune framework for natural language generation (NLG) tasks, and how can it be overcome? The authors hypothesize that the key issue lies in the inflexibility caused by using a one-size-fits-all vocabulary in the pretrain-finetune framework. This potentially weakens the effect when applying pretrained models to downstream NLG tasks, due to subword distribution mismatches between the upstream and downstream tasks.To address this, the authors propose extending the vanilla pretrain-finetune pipeline with an extra embedding transfer step to eliminate the token granularity gaps. Specifically, they introduce a plug-and-play embedding generator to produce representations for mismatch tokens in downstream tasks based on their morphologically similar tokens' embeddings from the pretrained model.In summary, the paper focuses on investigating and overcoming the subword discrepancy issue in applying pretrain-finetune to NLG via embedding transfer.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new pretrain-finetune training strategy to bridge the subword gaps between upstream pretraining and downstream finetuning models in natural language generation tasks. Specifically, the key ideas are:1. Identifying the issue of subword discrepancy caused by using the fixed vocabulary of pretraining models in downstream tasks. This leads to unsuitable granularity and under-represented words which hurt the performance. 2. Introducing an embedding transfer step by pretraining an embedding generator. This allows changing the model vocabulary for downstream tasks to better match their data distribution. The generator can produce embeddings for any required tokens based on pretraining embeddings of morphologically similar tokens.3. Validating the effectiveness of the proposed strategy on a variety of NLG tasks including domain adaptation, knowledge transfer, machine translation and question generation. Results show significant improvements over strong baselines by alleviating exposure bias, reducing computational cost, and better handling under-represented tokens.In summary, the key contribution is proposing a simple yet effective solution to the subword discrepancy issue in applying pretrain-finetune paradigm to natural language generation tasks. The embedding transfer step enhances the flexibility and applicability of pretrained models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new pretrain-finetune strategy for natural language generation tasks that introduces an embedding transfer step to bridge the gap between the subword vocabularies used in pretraining versus finetuning.


## How does this paper compare to other research in the same field?

Here are a few key points of comparison between this paper and other related research:- This paper focuses on addressing the issue of subword vocabulary gaps between pre-trained and downstream models in natural language generation tasks. Much prior work has looked at limitations of pre-training for NLG, but not specifically from the perspective of subword mismatches. - The proposed solution involves inserting an embedding transfer step to bridge the subword gap, rather than retraining the pre-trained model like some other approaches. This makes the method more lightweight and portable across tasks.- For embedding transfer, the paper examines several options including averaging, attention-based, and position-aware generators. Other related work has also proposed embedding generators, but mainly for handling OOV words rather than vocabulary changes.- Experiments cover a diverse set of NLG tasks - domain adaptation, knowledge transfer, machine translation, and question generation. This demonstrates wide applicability of the approach. Prior work tended to focus on 1-2 tasks.- The analyses provide useful insights into effects of subword granularity, how fast embedding transfer happens, and qualitative examples of how it helps with under-represented tokens. This sheds light on why and how the approach works.- The embedding transfer framework seems fairly simple and flexible. It does not require retraining or major architecture changes. This could make adoption easier compared to methods needing full model retraining.Overall, the paper makes a nice contribution in identifying and addressing the subword discrepancy issue for NLG. The solutions and analyses seem thorough and the experiments demonstrate wide applicability across diverse NLG tasks. The approach looks to be a lightweight and effective way to enhance pre-trained models for downstream generation.


## What future research directions do the authors suggest?

The authors suggest several future research directions at the end of the paper:- Investigate the effectiveness of their approach on other generation tasks, such as code generation, summarization, etc. - Validate their model on more NLP problems beyond just natural language generation tasks.- Use their model for transferring pre-trained models to task-specific ones.- Explore different architectures and training strategies for the embedding generator.- Analyze the theoretical properties of embedding transfer and establish guarantees on its effectiveness. - Study methods to automatically determine the optimal vocabulary for a given downstream task instead of using a fixed heuristic strategy.- Extend their approach to multilingual scenarios where the pretrained and downstream tasks use different languages.- Evaluate the societal impacts of their technique and ensure it does not propagate or amplify biases.In summary, they suggest further exploring the applicability of their embedding transfer strategy, analyzing its theoretical properties, improving the techniques for vocabulary selection and embedding generation, and studying the broader impacts. The key focus is on enhancing the flexibility and effectiveness of transferring knowledge from pretrained models to downstream generation tasks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper points out that the fixed subword vocabulary used in the popular pretrain-finetune paradigm causes subword discrepancy between the upstream pretraining and downstream finetuning stages. Specifically, the unsuitable segmentation granularity and under-represented words in the pretraining vocabulary lead to problems like increased exposure bias and difficulty modeling compositionality of downstream words. To address this, the authors propose inserting an embedding transfer step between pretraining and finetuning. They pretrain a lightweight, task-agnostic embedding generator that can produce embeddings for any required tokens based on their subwords/hyperwords in the pretraining vocabulary. This allows using a more suitable, task-specific vocabulary in finetuning while still leveraging the pretrained embeddings. Experiments on domain adaptation, knowledge transferring, machine translation, and question generation tasks demonstrate effectiveness. The proposed strategy brings computational efficiency benefits and handles exposure bias and under-represented words better. As it is architecture/task-agnostic, the authors believe it improves flexibility of pretrained models and can be widely applied.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points in the paper:The paper proposes a novel pretrain-finetune strategy to address the issue of subword discrepancy between upstream and downstream models. Pretrained models typically use a fixed, one-size-fits-all subword vocabulary that is insufficient to cope with downstream NLG tasks. This leads to unsuitable granularity and under-represented words in the vocabulary. To tackle this issue, the authors insert an embedding transfer stage between pretraining and finetuning. They pretrain a lightweight, plug-and-play embedding generator that can produce embeddings for any required tokens based on subwords and hyperwords from the pretrained vocabulary. The generator is optimized to minimize divergence before and after replacing embeddings. This allows each downstream model to use a tailored vocabulary, with common tokens initialized from pretrained embeddings and rare tokens from the generator. Experiments on domain adaptation, knowledge transfer, machine translation and question generation demonstrate universal effectiveness. The approach alleviates exposure bias, reduces computational cost, and handles under-represented tokens.


## Summarize the main method used in the paper in one paragraph.

The main method used in this paper is an embedding transfer strategy for bridging token granularity gaps between pre-trained models and downstream natural language generation (NLG) tasks. The key steps are:1) They pretrain an embedding generator on a large corpus along with the pre-trained model. This generator can produce embeddings for any tokens based on the pre-trained embeddings of morphologically similar tokens.2) For a downstream NLG task, they construct the model using a task-specific vocabulary. The embeddings of common tokens are directly initialized by the pre-trained embeddings. Unseen task-specific tokens are initialized using the pretrained embedding generator. 3) The downstream model is then finetuned on the task corpus as usual. The embedding transfer allows flexible vocabulary change without retraining the pre-trained model. Experiments on machine translation and question generation show effectiveness.In summary, the main novelty is introducing an embedding generator to transfer knowledge from pre-trained embeddings to unseen tokens, enabling vocabulary adaptation for downstream NLG tasks under the pretrain-finetune paradigm.
