# [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language
  Generation](https://arxiv.org/abs/2106.06125)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question addressed is: What is the crucial bottleneck in current pretrain-finetune framework for natural language generation (NLG) tasks, and how can it be overcome? 

The authors hypothesize that the key issue lies in the inflexibility caused by using a one-size-fits-all vocabulary in the pretrain-finetune framework. This potentially weakens the effect when applying pretrained models to downstream NLG tasks, due to subword distribution mismatches between the upstream and downstream tasks.

To address this, the authors propose extending the vanilla pretrain-finetune pipeline with an extra embedding transfer step to eliminate the token granularity gaps. Specifically, they introduce a plug-and-play embedding generator to produce representations for mismatch tokens in downstream tasks based on their morphologically similar tokens' embeddings from the pretrained model.

In summary, the paper focuses on investigating and overcoming the subword discrepancy issue in applying pretrain-finetune to NLG via embedding transfer.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new pretrain-finetune training strategy to bridge the subword gaps between upstream pretraining and downstream finetuning models in natural language generation tasks. Specifically, the key ideas are:

1. Identifying the issue of subword discrepancy caused by using the fixed vocabulary of pretraining models in downstream tasks. This leads to unsuitable granularity and under-represented words which hurt the performance. 

2. Introducing an embedding transfer step by pretraining an embedding generator. This allows changing the model vocabulary for downstream tasks to better match their data distribution. The generator can produce embeddings for any required tokens based on pretraining embeddings of morphologically similar tokens.

3. Validating the effectiveness of the proposed strategy on a variety of NLG tasks including domain adaptation, knowledge transfer, machine translation and question generation. Results show significant improvements over strong baselines by alleviating exposure bias, reducing computational cost, and better handling under-represented tokens.

In summary, the key contribution is proposing a simple yet effective solution to the subword discrepancy issue in applying pretrain-finetune paradigm to natural language generation tasks. The embedding transfer step enhances the flexibility and applicability of pretrained models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new pretrain-finetune strategy for natural language generation tasks that introduces an embedding transfer step to bridge the gap between the subword vocabularies used in pretraining versus finetuning.


## How does this paper compare to other research in the same field?

 Here are a few key points of comparison between this paper and other related research:

- This paper focuses on addressing the issue of subword vocabulary gaps between pre-trained and downstream models in natural language generation tasks. Much prior work has looked at limitations of pre-training for NLG, but not specifically from the perspective of subword mismatches. 

- The proposed solution involves inserting an embedding transfer step to bridge the subword gap, rather than retraining the pre-trained model like some other approaches. This makes the method more lightweight and portable across tasks.

- For embedding transfer, the paper examines several options including averaging, attention-based, and position-aware generators. Other related work has also proposed embedding generators, but mainly for handling OOV words rather than vocabulary changes.

- Experiments cover a diverse set of NLG tasks - domain adaptation, knowledge transfer, machine translation, and question generation. This demonstrates wide applicability of the approach. Prior work tended to focus on 1-2 tasks.

- The analyses provide useful insights into effects of subword granularity, how fast embedding transfer happens, and qualitative examples of how it helps with under-represented tokens. This sheds light on why and how the approach works.

- The embedding transfer framework seems fairly simple and flexible. It does not require retraining or major architecture changes. This could make adoption easier compared to methods needing full model retraining.

Overall, the paper makes a nice contribution in identifying and addressing the subword discrepancy issue for NLG. The solutions and analyses seem thorough and the experiments demonstrate wide applicability across diverse NLG tasks. The approach looks to be a lightweight and effective way to enhance pre-trained models for downstream generation.


## What future research directions do the authors suggest?

 The authors suggest several future research directions at the end of the paper:

- Investigate the effectiveness of their approach on other generation tasks, such as code generation, summarization, etc. 

- Validate their model on more NLP problems beyond just natural language generation tasks.

- Use their model for transferring pre-trained models to task-specific ones.

- Explore different architectures and training strategies for the embedding generator.

- Analyze the theoretical properties of embedding transfer and establish guarantees on its effectiveness. 

- Study methods to automatically determine the optimal vocabulary for a given downstream task instead of using a fixed heuristic strategy.

- Extend their approach to multilingual scenarios where the pretrained and downstream tasks use different languages.

- Evaluate the societal impacts of their technique and ensure it does not propagate or amplify biases.

In summary, they suggest further exploring the applicability of their embedding transfer strategy, analyzing its theoretical properties, improving the techniques for vocabulary selection and embedding generation, and studying the broader impacts. The key focus is on enhancing the flexibility and effectiveness of transferring knowledge from pretrained models to downstream generation tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper points out that the fixed subword vocabulary used in the popular pretrain-finetune paradigm causes subword discrepancy between the upstream pretraining and downstream finetuning stages. Specifically, the unsuitable segmentation granularity and under-represented words in the pretraining vocabulary lead to problems like increased exposure bias and difficulty modeling compositionality of downstream words. To address this, the authors propose inserting an embedding transfer step between pretraining and finetuning. They pretrain a lightweight, task-agnostic embedding generator that can produce embeddings for any required tokens based on their subwords/hyperwords in the pretraining vocabulary. This allows using a more suitable, task-specific vocabulary in finetuning while still leveraging the pretrained embeddings. Experiments on domain adaptation, knowledge transferring, machine translation, and question generation tasks demonstrate effectiveness. The proposed strategy brings computational efficiency benefits and handles exposure bias and under-represented words better. As it is architecture/task-agnostic, the authors believe it improves flexibility of pretrained models and can be widely applied.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper proposes a novel pretrain-finetune strategy to address the issue of subword discrepancy between upstream and downstream models. Pretrained models typically use a fixed, one-size-fits-all subword vocabulary that is insufficient to cope with downstream NLG tasks. This leads to unsuitable granularity and under-represented words in the vocabulary. 

To tackle this issue, the authors insert an embedding transfer stage between pretraining and finetuning. They pretrain a lightweight, plug-and-play embedding generator that can produce embeddings for any required tokens based on subwords and hyperwords from the pretrained vocabulary. The generator is optimized to minimize divergence before and after replacing embeddings. This allows each downstream model to use a tailored vocabulary, with common tokens initialized from pretrained embeddings and rare tokens from the generator. Experiments on domain adaptation, knowledge transfer, machine translation and question generation demonstrate universal effectiveness. The approach alleviates exposure bias, reduces computational cost, and handles under-represented tokens.


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is an embedding transfer strategy for bridging token granularity gaps between pre-trained models and downstream natural language generation (NLG) tasks. The key steps are:

1) They pretrain an embedding generator on a large corpus along with the pre-trained model. This generator can produce embeddings for any tokens based on the pre-trained embeddings of morphologically similar tokens.

2) For a downstream NLG task, they construct the model using a task-specific vocabulary. The embeddings of common tokens are directly initialized by the pre-trained embeddings. Unseen task-specific tokens are initialized using the pretrained embedding generator. 

3) The downstream model is then finetuned on the task corpus as usual. The embedding transfer allows flexible vocabulary change without retraining the pre-trained model. Experiments on machine translation and question generation show effectiveness.

In summary, the main novelty is introducing an embedding generator to transfer knowledge from pre-trained embeddings to unseen tokens, enabling vocabulary adaptation for downstream NLG tasks under the pretrain-finetune paradigm.


## What problem or question is the paper addressing?

 The paper is addressing the problem of subword discrepancy in the popular pretrain-finetune paradigm for natural language generation (NLG) tasks. Some key points:

- Pretrained language models like BERT and GPT have shown impressive performance on many NLP tasks. However, directly applying them to downstream NLG tasks via finetuning often yields disappointing results. 

- The authors attribute this issue to the mismatch between the pretrained model's fixed subword vocabulary and the actual subword distribution needed for the downstream NLG task. This causes problems like over-segmentation of words and inability to handle out-of-vocabulary words.

- To address this, they propose inserting an embedding transfer step between pretraining and finetuning. This trains an embedding generator that can produce embeddings for any required subwords based on the pretrained embeddings.

- The embedding generator takes subwords and hyperwords (longer words containing the subword) as input and outputs the subword embedding. It is trained to minimize divergence from the original pretrained model.

- For a downstream NLG task, they generate a suitable task-specific subword vocabulary, initialize common subwords from pretrained embeddings and unseen ones from the generator.

- Experiments on domain adaptation, knowledge transfer and question generation tasks demonstrate improved performance and ability to handle vocabulary mismatch issues. The approach is model-agnostic and has low computational overhead.

In summary, the key contribution is identifying subword discrepancy as the reason for poor finetuning performance on NLG, and proposing an effective embedding transfer solution to bridge the vocabulary gap between pretraining and downstream tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Pretrain-finetune paradigm: The commonly used strategy of first pretraining a model on a large upstream corpus and then finetuning it on downstream tasks.

- Natural language generation (NLG): Generating natural language text, such as machine translation, question generation, etc. The paper focuses on applying pretrain-finetune to NLG tasks.

- Subword discrepancy: The mismatch between the subword vocabulary used in the pretrained model and the optimal subword units for the downstream NLG task. This causes exposure bias and under-represented words. 

- Embedding transfer: The proposed strategy to transfer knowledge from the pretrained model to downstream tasks with different subword vocabularies. An embedding generator is introduced.

- Embedding generator: A module to generate embeddings for new vocabulary words based on subwords and similar words. Multiple implementations are explored.

- Morphologically similar words: Subwords and hyperwords of a word that are used by the embedding generator to produce its embedding.

- Knowledge distillation: Used to train the embedding generator by minimizing the divergence between the pretrained model outputs before and after replacing embeddings.

- Domain adaptation: One of the main experimental tasks, adapting a pretrained model to new domains.

- Knowledge transferring: Applying the approach to transfer knowledge from pretrained models like M-BERT and M-BART to downstream NLG tasks.

So in summary, the key focus is on using embedding transfer and generators to bridge subword discrepancies when applying pretrain-finetune to NLG.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to summarize the key points of the paper:

1. What is the main problem addressed in the paper? The paper points out the issue of subword discrepancy between upstream and downstream tasks in the pretrain-finetune paradigm for natural language generation (NLG).

2. What are the defects caused by subword discrepancy according to the paper? The paper indicates subword discrepancy leads to unsuitable granularity and under-represented words, which in turn causes issues like exposure bias, computational inefficiency, and poor generalization.

3. What is the proposed solution in the paper? The paper proposes an embedding transfer strategy by introducing an embedding generator that can produce embeddings for any required tokens using the pretrained embeddings.

4. How does the embedding generator work? The generator produces embeddings for unseen tokens by feeding it subwords and hyperwords of that token from the pretrained vocabulary.

5. How is the embedding generator trained? The generator is trained by replacing some tokens' embeddings with generated ones and minimizing divergence before and after replacing under the pretrained model objective.

6. What are the main experiments conducted in the paper? Experiments are conducted on domain adaptation translation, knowledge transfer to machine translation and question generation using M-BERT and M-BART.

7. What are the main results and analyses? The results demonstrate universal effectiveness of the proposed strategy. Analyses confirm it can alleviate exposure bias, computational costs, and under-represented tokens issues.

8. What are the ablation studies in the paper? The paper compares averaging, attention-based, and position-aware generators, as well as impact of training steps and knowledge distillation.

9. What is the conclusion of the paper? The paper concludes their strategy is effective, flexible, and transparent to models and tasks for bridging subword gaps in pretrain-finetune paradigm.

10. What are potential future directions? The paper suggests applying the strategy to other NLG tasks like summarization and testing its effectiveness for transfer learning.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the key motivation behind proposing an embedding transfer strategy in this work? How does it help with the limitations of the pretrain-finetune paradigm for NLG tasks?

2. How does the proposed embedding transfer strategy work? Can you walk through the main steps involved? 

3. What are the different embedding generator variants explored in this work? How do they differ in terms of architecture, training, and performance?

4. How is the embedding generator trained? Why is knowledge distillation used as an additional objective for training?

5. What are the key benefits of using an embedding generator for vocabulary transfer instead of other approaches like retraining the pretrained model?

6. What are the differences between the embedding generator proposed in this work versus prior work on generating embeddings for OOV words?

7. How does the authors' analysis provide insights into the impact of subword granularity on NLG models? What are the key findings?

8. How does embedding transfer impact the initialization and training convergence of downstream models? What does the analysis reveal?

9. What is the computational cost of training and using the proposed embedding generator? Is it a bottleneck for extensibility? 

10. How does the qualitative analysis provided demonstrate the ability of this method to handle under-represented tokens? Can you walk through the example provided?
