# [You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF   Robotic Grasping of Novel Objects](https://arxiv.org/abs/2404.03462)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
Existing grasp planning methods using partial point clouds suffer from limited scene understanding due to occlusion. Static scene reconstruction methods require repetitive re-scanning when the environment changes. This limits their applicability for real-time grasping tasks in dynamic environments. There is a need for an approach that can provide more complete scene understanding to improve grasping while adapting to changes in real-time.

Proposed Method: 
The paper proposes a two-stage pipeline called YOSO (You Only Scan Once) for dynamic scene reconstruction tailored for robotic grasping tasks. 

Stage I: A camera mounted on the robot's wrist scans the scene once along a trajectory to capture an RGB-D video. The video is input to a Video-Segmentation module to segment objects. An Object Pose Tracker & Mesh Generator module simultaneously tracks object poses and reconstructs meshes for each object using multi-view observations. Keyframes are stored for later pose tracking.

Stage II: In testing, image segmentation and pose tracking continue using keyframe references. Object meshes are transformed given current poses and merged with observed partial point cloud to reconstruct the full scene. This is input to a Grasp Pose Predictor to generate grasps. Unlike conventional methods, the reconstructed scene is updated dynamically.

Main Contributions:

1) Proposes a novel modular pipeline YOSO for dynamic scene reconstruction specifically for robotic grasping tasks that only requires a single scan of the environment.

2) Evaluates a state-of-the-art grasp model on YOSO reconstructed scene and shows improved accuracy over baseline model performance, establishing benefit of more complete scene understanding.  

3) Extends GraspNet-1Billion benchmark by adding fully visible point clouds to provide theoretical upper limit of model performance given complete scene data.

In summary, the key innovation is enabling dynamic updating of reconstructed scenes after just an initial scan to provide more complete point clouds to grasp planning methods, improving accuracy while adapting to environment changes for practical real-world grasping tasks.
