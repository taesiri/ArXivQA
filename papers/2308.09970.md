# Tackling Vision Language Tasks Through Learning Inner Monologues

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that simulating the cognitive process of inner monologue, where models have multi-turn conversational interactions, can enhance their ability to perform complex visual reasoning tasks. Specifically, the paper proposes that having a language model (Reasoner) interact with a vision-language model (Observer) through natural language questions and answers (i.e. inner monologue) will allow them to jointly solve visual reasoning problems more effectively. The key ideas are:1) The Observer generates an initial image description, then the Reasoner asks follow-up questions to gather necessary visual details. 2) Through this iterative questioning-answering process, the models learn to collaborate and reason about visual concepts.3) The entire system is optimized end-to-end using a two-stage training approach - first supervised pre-training, then reinforcement learning.4) This approach of simulating inner monologue provides strong performance while maintaining interpretability, unlike some black-box models.In summary, the central hypothesis is that by emulating human inner monologue, the Reasoner and Observer models can enhance their joint reasoning and explanation abilities to better solve complex vision-language tasks. The key innovation is using dialog to enable the models to interact and learn improved skills.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel approach called Inner Monologue Multi-Modal Optimization (IMMO) to solve complex vision-language reasoning problems by simulating the human cognitive process of inner monologue. 2. It introduces a two-stage training framework to teach the models to perform effective inner monologue through supervised learning on human-annotated dialog data and reinforcement learning.3. It constructs a new training dataset by augmenting existing VQA data with multi-turn QA pairs generated by GPT-3.5 to provide human-like reasoning patterns.4. It evaluates IMMO on two vision-language tasks and shows it achieves competitive performance compared to state-of-the-art approaches, while using significantly less training data and providing greater interpretability.5. The results demonstrate the effectiveness of simulating inner monologue for enhancing reasoning and explainability in multi-modal AI systems. The approach is flexible and could be applied to fuse different modalities and models.In summary, the key innovation is using inner monologue between vision and language models to achieve strong performance on vision-language tasks while retaining interpretability, reducing training costs, and avoiding the need for extensive embedding alignment. The human-inspired technique of inner dialogue enhances reasoning for multi-modal AI.
