# [Task-Agnostic Graph Explanations](https://arxiv.org/abs/2202.08335)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop tools to explain graph neural network (GNN) models in cases where:1) The GNN model performs multi-task prediction, i.e. makes predictions for multiple downstream tasks. 2) The GNN model is trained in a self-supervised manner without any downstream tasks defined, and the resulting representations are later used for downstream tasks.Existing learning-based GNN explanation methods are task-specific, meaning they need to be trained separately for each prediction task. This makes them inefficient for explaining multi-task models, where a separate explainer would be needed for each task. They also cannot be applied in case 2 above, where no downstream tasks are defined during training. To address these limitations, the paper proposes a task-agnostic explanation framework called TAGE that:- Decomposes the prediction model into an embedding model and separate downstream models.- Learns a single embedding explainer in a self-supervised manner without knowledge of downstream tasks. - Allows the embedding explainer to cooperate with lightweight downstream explainers to provide explanations.The key hypothesis is that this task-agnostic approach will enable explaining multi-task models efficiently with a single embedding explainer, as well as explaining self-supervised GNN models where downstream tasks are initially unknown.In summary, the paper focuses on developing a task-agnostic explanation framework that can handle multi-task prediction and self-supervised GNN models, overcoming the limitations of prior task-specific explanation methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a task-agnostic graph neural network explanation framework called TAGE that can efficiently provide high quality explanations for graph neural networks in cases where multiple downstream tasks need to be explained or the downstream tasks are unknown during model training.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is an overview of how it compares to other research in the field of graph neural network explainability:- Main Contribution: The paper proposes a novel task-agnostic explanation framework called TAGE. Unlike prior explainers like GNNExplainer or PGExplainer which require training separate models for each prediction task, TAGE can provide explanations for multiple downstream tasks using a single pretrained explainer. - Key Differences from Prior Work:1) TAGE is trained in a self-supervised manner without needing labeled data or knowledge of downstream tasks. This enables explaining models trained with unlabeled data or future unseen tasks.2) TAGE decomposes the explanation into an embedding explainer and lightweight downstream explainers. The embedding explainer is task-agnostic while the downstream explainer adapts to the task.3) The embedding explainer is trained with a novel conditioned contrastive learning objective that maximizes mutual information between original and subgraph embeddings.- Evaluation: Experiments on real-world molecular, protein, and e-commerce graphs demonstrate TAGE matches or outperforms task-specific explainers in fidelity while achieving significantly better efficiency and universality.- Limitations: TAGE relies on access to node embeddings. Explaining black-box models where only outputs are observable remains an open challenge. The inductive training may suffer from dataset bias.In summary, TAGE introduces a new task-agnostic explanation paradigm with solid empirical results. The idea of disentangling embedding and task-specific explainers enables explaining unseen tasks. The approach contrasts with prior work focused on developing task-specific explainers or optimization methods. However, black-box explanation and inductive bias issues remain open challenges.
