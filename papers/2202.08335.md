# [Task-Agnostic Graph Explanations](https://arxiv.org/abs/2202.08335)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop tools to explain graph neural network (GNN) models in cases where:1) The GNN model performs multi-task prediction, i.e. makes predictions for multiple downstream tasks. 2) The GNN model is trained in a self-supervised manner without any downstream tasks defined, and the resulting representations are later used for downstream tasks.Existing learning-based GNN explanation methods are task-specific, meaning they need to be trained separately for each prediction task. This makes them inefficient for explaining multi-task models, where a separate explainer would be needed for each task. They also cannot be applied in case 2 above, where no downstream tasks are defined during training. To address these limitations, the paper proposes a task-agnostic explanation framework called TAGE that:- Decomposes the prediction model into an embedding model and separate downstream models.- Learns a single embedding explainer in a self-supervised manner without knowledge of downstream tasks. - Allows the embedding explainer to cooperate with lightweight downstream explainers to provide explanations.The key hypothesis is that this task-agnostic approach will enable explaining multi-task models efficiently with a single embedding explainer, as well as explaining self-supervised GNN models where downstream tasks are initially unknown.In summary, the paper focuses on developing a task-agnostic explanation framework that can handle multi-task prediction and self-supervised GNN models, overcoming the limitations of prior task-specific explanation methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a task-agnostic graph neural network explanation framework called TAGE that can efficiently provide high quality explanations for graph neural networks in cases where multiple downstream tasks need to be explained or the downstream tasks are unknown during model training.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is an overview of how it compares to other research in the field of graph neural network explainability:- Main Contribution: The paper proposes a novel task-agnostic explanation framework called TAGE. Unlike prior explainers like GNNExplainer or PGExplainer which require training separate models for each prediction task, TAGE can provide explanations for multiple downstream tasks using a single pretrained explainer. - Key Differences from Prior Work:1) TAGE is trained in a self-supervised manner without needing labeled data or knowledge of downstream tasks. This enables explaining models trained with unlabeled data or future unseen tasks.2) TAGE decomposes the explanation into an embedding explainer and lightweight downstream explainers. The embedding explainer is task-agnostic while the downstream explainer adapts to the task.3) The embedding explainer is trained with a novel conditioned contrastive learning objective that maximizes mutual information between original and subgraph embeddings.- Evaluation: Experiments on real-world molecular, protein, and e-commerce graphs demonstrate TAGE matches or outperforms task-specific explainers in fidelity while achieving significantly better efficiency and universality.- Limitations: TAGE relies on access to node embeddings. Explaining black-box models where only outputs are observable remains an open challenge. The inductive training may suffer from dataset bias.In summary, TAGE introduces a new task-agnostic explanation paradigm with solid empirical results. The idea of disentangling embedding and task-specific explainers enables explaining unseen tasks. The approach contrasts with prior work focused on developing task-specific explainers or optimization methods. However, black-box explanation and inductive bias issues remain open challenges.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Developing explainers for black-box models where only the output is available. The current explainer relies on node embeddings as input, so it cannot easily explain models where the internal representations are not accessible. The authors suggest exploring methods like surrogate models or other ways to represent nodes to enable explaining black-box models.- Improving the expressiveness and flexibility of the explainer architecture. For very complex models/tasks/datasets, the current embedding explainer may need more parameters and modifications to provide sufficiently good task-specific explanations. The authors suggest ideas like fine-tuning or using more expressive task-specific explainers where needed.- Addressing potential dataset bias issues with inductive learning. Since the explainer is trained on a dataset and applied to new data, there could be mismatches that affect the quality of explanations. Further research could investigate this issue and develop techniques to reduce the effects of dataset bias.- Extending the approach to additional graph explanation scenarios like node classification or link prediction. The current work focuses on graph and node classification tasks. Expanding the framework to other common graph ML tasks could be an interesting direction.- Considering alternative learning objectives and training frameworks. The self-supervised conditioned contrastive learning approach is key to enabling task-agnostic training, but other objectives could be explored as well.- Evaluating the approach on more diverse and complex real-world datasets. Testing on additional challenging molecular, biological, social network, etc. datasets would further demonstrate the capabilities.- Comparing to a wider range of graph explanation methods, especially more recent work. More extensive comparisons on explanation quality and efficiency would better situate the performance.So in summary, the authors point to a number of promising ways to extend the task-agnostic graph explanation framework to broader models, tasks, datasets, and training objectives. Addressing limitations and expanding evaluation are also noted as important future directions.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It introduces the task-agnostic explanation problem and proposes a two-stage explanation pipeline involving an embedding explainer and a downstream explainer. This enables explaining multiple downstream tasks with a single embedding explainer. 2. It proposes a self-supervised training framework called TAGE (Task-Agnostic Graph Explainer) to train the embedding explainer without knowledge of downstream tasks. The training uses conditioned contrastive learning objectives.3. It demonstrates through experiments that TAGE can achieve higher explanation quality compared to learning-based baselines like GNNExplainer and PGExplainer. It also shows significant speedups in explanation efficiency by using a single explainer for multiple tasks.In summary, the key novelty is proposing the task-agnostic explanation setting and a method to train an embedding explainer in a self-supervised manner that can cooperate with any downstream explainer/task. This provides benefits like enabling explanations when downstream tasks are unavailable and allowing efficient explanation of multitask models with a single explainer.
