# [Task-Agnostic Graph Explanations](https://arxiv.org/abs/2202.08335)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop tools to explain graph neural network (GNN) models in cases where:1) The GNN model performs multi-task prediction, i.e. makes predictions for multiple downstream tasks. 2) The GNN model is trained in a self-supervised manner without any downstream tasks defined, and the resulting representations are later used for downstream tasks.Existing learning-based GNN explanation methods are task-specific, meaning they need to be trained separately for each prediction task. This makes them inefficient for explaining multi-task models, where a separate explainer would be needed for each task. They also cannot be applied in case 2 above, where no downstream tasks are defined during training. To address these limitations, the paper proposes a task-agnostic explanation framework called TAGE that:- Decomposes the prediction model into an embedding model and separate downstream models.- Learns a single embedding explainer in a self-supervised manner without knowledge of downstream tasks. - Allows the embedding explainer to cooperate with lightweight downstream explainers to provide explanations.The key hypothesis is that this task-agnostic approach will enable explaining multi-task models efficiently with a single embedding explainer, as well as explaining self-supervised GNN models where downstream tasks are initially unknown.In summary, the paper focuses on developing a task-agnostic explanation framework that can handle multi-task prediction and self-supervised GNN models, overcoming the limitations of prior task-specific explanation methods.
