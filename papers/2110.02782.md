# [How BPE Affects Memorization in Transformers](https://arxiv.org/abs/2110.02782)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How does the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) affect the ability and tendency of Transformer models to memorize training data?The key hypothesis appears to be that larger BPE subword vocabularies facilitate greater memorization of the training data by Transformer models, even when controlling for model capacity. Specifically, the paper investigates whether larger BPE vocabularies:- Increase the capacity of Transformers to memorize random/non-systematic input-output mappings.- Make Transformers more susceptible to membership inference attacks, suggesting increased memorization.- Make it easier for Transformer language models to reproduce training data answers when prompted with questions.The central aim seems to be understanding and characterizing how the choice of subword vocabulary size impacts memorization behavior in Transformers. The results consistently show that larger BPE vocabularies lead to increased training data memorization across different model architectures and experimental setups.
