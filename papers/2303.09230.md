# [Towards a Smaller Student: Capacity Dynamic Distillation for Efficient   Image Retrieval](https://arxiv.org/abs/2303.09230)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: 

How can we improve the efficiency of knowledge distillation for image retrieval while maintaining accuracy?

The paper proposes a new knowledge distillation framework called Capacity Dynamic Distillation (CDD) to address this question. The core ideas are:

1) Use a heavy student network initially to allow comprehensive learning from the teacher in early training. 

2) Gradually compress the student network during training via a distillation guided compactor module. This allows starting with high capacity for knowledge transfer but ending with an efficient model.

3) Use a retrieval-guided gradient resetting mechanism (RGGR) to release the optimization conflict between distillation objectives and compression objectives. This improves the compression rate.

So in summary, the central hypothesis is that by dynamically controlling student capacity during training and using a retrieval-aware gradient resetting method, they can achieve an efficient distilled model for image retrieval without sacrificing accuracy. The experiments aim to validate if their proposed CDD framework with RGGR can effectively achieve this goal.
