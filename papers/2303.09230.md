# [Towards a Smaller Student: Capacity Dynamic Distillation for Efficient   Image Retrieval](https://arxiv.org/abs/2303.09230)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: 

How can we improve the efficiency of knowledge distillation for image retrieval while maintaining accuracy?

The paper proposes a new knowledge distillation framework called Capacity Dynamic Distillation (CDD) to address this question. The core ideas are:

1) Use a heavy student network initially to allow comprehensive learning from the teacher in early training. 

2) Gradually compress the student network during training via a distillation guided compactor module. This allows starting with high capacity for knowledge transfer but ending with an efficient model.

3) Use a retrieval-guided gradient resetting mechanism (RGGR) to release the optimization conflict between distillation objectives and compression objectives. This improves the compression rate.

So in summary, the central hypothesis is that by dynamically controlling student capacity during training and using a retrieval-aware gradient resetting method, they can achieve an efficient distilled model for image retrieval without sacrificing accuracy. The experiments aim to validate if their proposed CDD framework with RGGR can effectively achieve this goal.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a Capacity Dynamic Distillation (CDD) framework that employs a heavy student network initially to effectively learn knowledge from the teacher network in early training epochs. The student network is then gradually compressed during training to achieve fast inference speed. 

2. It introduces a Distillation Guided Compactor (DGC) module that is optimized by both the image retrieval loss and a parametric sparse loss to dynamically compress the student network in an end-to-end manner.

3. It proposes a Retrieval-Guided Gradient Resetting (RGGR) mechanism to address the gradient conflict between the retrieval loss and compression loss during training. RGGR selects unimportant channels and resets their gradient from the retrieval loss to zero, allowing them to be pruned away more easily.

4. Extensive experiments show the proposed CDD+RGGR framework achieves much better trade-off between accuracy and efficiency compared to prior arts. For example, on VeRi-776 dataset, it saves 67.13% parameters and 65.67% FLOPs without sacrificing accuracy.

In summary, the key innovation is the capacity dynamic distillation idea and the associated techniques (DGC module, RGGR mechanism) that allow end-to-end training of a heavily parameterized student network while dynamically pruning it to obtain an efficient model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a Capacity Dynamic Distillation framework that employs a heavy student network to effectively learn from a teacher model in early training iterations and gradually compresses the student network during training to obtain an efficient final model, outperforming prior knowledge distillation methods.
