# [AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining](https://arxiv.org/abs/2312.15784)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Massive growth in scientific publications makes it challenging for researchers to keep up with latest advances even in their narrow fields. 
- Text mining can help analyze existing knowledge and uncover new insights, but requires effective topic modeling methods.

Proposed Solution - AHAM:
- Present a methodology to adapt the BERTopic topic modeling framework to improve analysis of scientific texts. 
- Use LLaMa generative language model to generate topic names via one-shot learning, prompted by domain experts.
- Develop AHAM objective function to evaluate quality of topic modeling by measuring:
   - Similarity between generated topic names using lexical and semantic metrics
   - Ratio of outlier topics to total topics
- Adapt sentence transformers to target domain via generative pseudo-labeling (GPL).

Key Contributions:
- AHAM methodology to guide domain-specific adaptation of BERTopic through prompted topic naming by LLM.
- Two specialized sentence transformers adapted for arXiv and biomedical arXiv. 
- Evaluate impact of domain adaptation on topic modeling for literature-based discovery corpus.
- Propose AHAM heuristic to jointly optimize outlier ratio and topic name similarity. 
- Demonstrate complex non-convex progression of topic evolution during domain adaptation.
- Show that granularity of domain dataset impacts adaptation performance.

In summary, the key innovation is the AHAM methodology to interactively steer the domain adaptation of neural topic modeling components like sentence transformers in order to uncover novel insights from scientific corpora.
