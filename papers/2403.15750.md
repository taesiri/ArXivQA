# [iDAT: inverse Distillation Adapter-Tuning](https://arxiv.org/abs/2403.15750)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning large pre-trained models like Vision Transformers (ViTs) on downstream tasks requires training and storing full sets of parameters for each task, which is computationally expensive and prone to overfitting. 
- Methods like Adapter Tuning (AT) address this by freezing the pretrained backbone and introducing small trainable adapter modules, but more work is needed to improve their performance.

Key Idea:
- The paper first analyzes the weight distributions of adapter modules in different sized ViTs and finds significant differences, indicating variance in the downstream knowledge they acquire.
- They hypothesize that these inherent knowledge differences can provide diverse perspectives to aid learning.

Proposed Solution: 
- The paper proposes inverse Distillation Adapter Tuning (iDAT), a novel framework that leverages knowledge discrepancy between smaller and larger models.
- Unlike traditional distillation, iDAT uses a smaller ViT as teacher to provide distinct knowledge to the larger ViT student model. 
- This "inverse distillation" injects knowledge from diverse perspectives to enhance adapter tuning performance.

Main Contributions:
- First work exploring combination of knowledge distillation with adapter tuning methods. 
- Analysis revealing knowledge differences between adapter modules of varying model sizes, motivating the inverse distillation idea.
- iDAT framework that achieves significant gains over standalone AT methods and matches state-of-the-art without complexity.
- Extensive experiments on 19 datasets validating effectiveness and universality of iDAT.

The paper demonstrates both the promise of integrating distillation with adapter methods and specifically the power of small-to-large inverse distillation for further enhancing performance.
