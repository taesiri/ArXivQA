# [LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition   and Adaptive Quantization](https://arxiv.org/abs/2403.01136)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent large language models (LLMs) like GPT-3 show great performance but are very large, requiring multiple GPUs for serving. Most solutions use homogeneous GPU clusters.
- Serving LLMs on heterogeneous GPU clusters can reduce cost by better utilizing available GPUs of different capabilities. But existing solutions of model parallelism and compression are not designed for heterogeneous devices.

Proposed Solution:
- The paper proposes LLM-PQ, a system for efficient LLM serving on heterogeneous GPU clusters.
- It uses adaptive mixed-precision quantization to match different GPU memory capacities. Layers mapped to high-memory GPUs use higher precision for better accuracy.
- It does phase-aware model partition, considering different execution time patterns in the prefill and decode phases during generative inference.
- An indicator is designed to guide quantization bitwidth assignment for layers based on output variance to balance memory utilization and model quality.
- An algorithm searches the large space of possible partitions, quantization configurations and micro-batch sizes to optimize inference latency and model quality.

Main Contributions:
- An accurate analytical memory model and a learned latency model for mixed-precision LLM inference.
- Introduction of adaptive mixed-precision quantization into pipeline parallel LLM serving on heterogeneous devices.
- A variance-based indicator to guide per-layer bitwidth selection for quantization.
- An algorithm that solves layer partitioning, quantization configuration and micro-batch sizing by combining exhaustive search and an ILP formulation.
- Extensive evaluation on production clusters shows LLM-PQ improves throughput by up to 2.88x over state-of-the-art approaches.

In summary, the paper proposes a novel system LLM-PQ that, for the first time, provides an efficient solution to serve large language models on heterogeneous GPU clusters through adaptive quantization and phase-aware optimization.
