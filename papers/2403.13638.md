# [Do Not Worry if You Do Not Have Data: Building Pretrained Language   Models Using Translationese](https://arxiv.org/abs/2403.13638)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) perform well for English due to abundant monolingual data, but most languages lack sufficient data. This causes poor performance of LLMs for non-English languages.
- Synthetic data generated via machine translation (MT) can help address data scarcity, but quality and scale need to be addressed.

Proposed Solution:
- Present a framework to produce high-quality synthetic "translationese" data at scale using MT and filter it with efficient TinyLMs trained on clean web data.
- Compare performance of LMs pretrained on filtered synthetic data vs clean data for English and Indic languages.
- Demonstrate synthetic data LMs can match performance of clean data LMs, especially with additional finetuning on 10% clean data.

Contributions:  
- Framework to mass-produce and filter synthetic data for LM pretraining using TinyLMs and MT models.
- Empirically show LMs pretrained on filtered synthetic data perform on par with LMs pretrained on clean data for both understanding (NLU) and generation (NLG) tasks.
- Release IndicMonoDoc, the largest collection of clean Indic monolingual documents (39.5B tokens covering 23 languages), to help advance Indic LM research.
- Release filtered synthetic datasets, models, pipelines and code to reproduce experiments.

In summary, this paper demonstrates an effective framework to generate synthetic translationese at scale to pretrain competitive LMs even in low-resource scenarios, backed by extensive experiments on English and Indic languages. Releasing the datasets and models advances capabilities for Indic languages.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper explores using machine translation to create synthetic "translationese" data for pretraining language models, showing this is a viable approach for low-resource languages by filtering the synthetic data and fine-tuning on a small amount of clean data.


## What is the main contribution of this paper?

 Based on the paper, the main contributions are:

a) A simple framework involving high-quality MT models and TinyLMs trained on clean web-crawled data to mass-produce and filter synthetic data for LM training.

b) Demonstrating the efficacy of language models trained on filtered synthetic data across a range of downstream tasks in natural language understanding and generation. 

c) A new document-level monolingual corpora (IndicMonoDoc) consisting of 39.5B tokens worth of monolingual clean document-level data spanning 22 scheduled languages and English. This dataset, which is 2 times larger than IndicCorpV2, will be made public alongside their models and code.

So in summary, the main contributions are: proposing a method to create and filter synthetic training data for LMs, showing this synthetic data can be used to train performant LMs, and releasing a large new monolingual document dataset for Indian languages.


## What are the keywords or key terms associated with this paper?

 Based on the content of the paper, some of the key terms and keywords associated with it are:

- Translationese - Synthetic or machine translated text that has distinctive fingerprints from the source language.

- TinyLMs - Small and efficient language models with only 28-85 million parameters used for perplexity filtering. 

- IndicMonoDoc - The large monolingual corpora collected by the authors spanning 22 Indic languages and English, containing 39.5 billion tokens.

- Filtering - Using TinyLMs to calculate perplexity scores on synthetic text and filtering documents below a threshold.

- Pretraining - Training language models from scratch on synthetic and clean data. 

- Fine-tuning - Taking a pretrained language model and specializing it further on downstream tasks.

- Downstream tasks - Tasks like natural language understanding, natural language generation, summarization, and machine translation that models are evaluated on after pretraining.

- Synthetic data utility - Showing the usefulness of filtered synthetic parallel and non-parallel text for pretraining competitive language models.

The key focus is on using translationese or synthetic text for pretraining language models combined with filtering techniques and a small clean data budget.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using translationese or synthetic data created by machine translation for pre-training language models. What are some potential issues with using machine-translated text for training language models? How does the paper try to address these issues?

2. The paper uses perplexity filtering with TinyLMs to remove noisy synthetic documents. Explain how perplexity is used to score document quality. What are some limitations of this approach for filtering? 

3. The results show that language models trained on filtered synthetic data match performance of LMs trained on clean web data. Does adding a small amount of clean data in the extended pretraining phase further improve performance? Explain why additional clean data helps.

4. The paper finds that synthetic data is especially effective for pretraining models for natural language generation tasks. What characteristic of synthetic text makes it suitable for NLG? Does synthetic data work as well for classification tasks requiring linguistic acceptability judgments?

5. When creating synthetic parallel text, the paper finds using non-parallel data works just as well as parallel text for machine translation fine-tuning. Why might parallelism not make much difference here? What types of synthetic parallel data would be valuable?  

6. Explain why the source language used to create synthetic target language data impacts quality and downstream task performance. How does translationese from more similar languages like Hindi into English compare to distant language pairs?

7. Could the proposed technique be applied to create synthetic data for training large multilingual models? What are some challenges in scaling up translation and filtering to more languages and larger datasets?

8. Aside from machine translation models, what other techniques could potentially generate useful synthetic monolingual data from existing text? How do the characteristics of data produced by other methods compare?

9. For real-world deployment, what techniques could help ensure synthetic pretraining data does not lead models to generate toxic or biased language? How might the filtering process be improved to catch these issues?  

10. The release of IndicMonoDoc will enable future research into Indic language modeling. What new research directions does a large document-level Indic dataset enable compared to previous IndicNLP resources?
