# [Do Not Worry if You Do Not Have Data: Building Pretrained Language   Models Using Translationese](https://arxiv.org/abs/2403.13638)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) perform well for English due to abundant monolingual data, but most languages lack sufficient data. This causes poor performance of LLMs for non-English languages.
- Synthetic data generated via machine translation (MT) can help address data scarcity, but quality and scale need to be addressed.

Proposed Solution:
- Present a framework to produce high-quality synthetic "translationese" data at scale using MT and filter it with efficient TinyLMs trained on clean web data.
- Compare performance of LMs pretrained on filtered synthetic data vs clean data for English and Indic languages.
- Demonstrate synthetic data LMs can match performance of clean data LMs, especially with additional finetuning on 10% clean data.

Contributions:  
- Framework to mass-produce and filter synthetic data for LM pretraining using TinyLMs and MT models.
- Empirically show LMs pretrained on filtered synthetic data perform on par with LMs pretrained on clean data for both understanding (NLU) and generation (NLG) tasks.
- Release IndicMonoDoc, the largest collection of clean Indic monolingual documents (39.5B tokens covering 23 languages), to help advance Indic LM research.
- Release filtered synthetic datasets, models, pipelines and code to reproduce experiments.

In summary, this paper demonstrates an effective framework to generate synthetic translationese at scale to pretrain competitive LMs even in low-resource scenarios, backed by extensive experiments on English and Indic languages. Releasing the datasets and models advances capabilities for Indic languages.
