# [HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative perception with   vision transformer](https://arxiv.org/abs/2304.10628)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research contributions appear to be:

- Proposing HM-ViT, a new cooperative perception framework for heterogeneous vehicle-to-vehicle (V2V) collaboration involving agents with different sensor types (e.g. some with cameras, some with LiDAR). 

- Designing a heterogeneous 3D graph attention module (H3GAT) to effectively fuse features from different sensor modalities while capturing inter-agent and intra-agent interactions.

- Conducting extensive experiments on a V2V perception dataset demonstrating that HM-ViT outperforms prior state-of-the-art methods for cooperative perception, especially for heterogeneous settings.

The key hypothesis seems to be that explicitly modeling the heterogeneity in agent sensor types and interactions in a V2V system can lead to improved cooperative perception performance compared to prior approaches. The results generally validate this hypothesis and show the benefits of the proposed HM-ViT framework.

In summary, the main research question is how to enable effective collaboration between heterogeneous agents with different sensors in V2V systems for 3D object detection. The key ideas are using transformer-based attention mechanisms to model agent heterogeneity and interactions in a flexible graph-based framework.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

- It proposes HM-ViT, the first unified cooperative perception framework that can handle multi-agent hetero-modal cooperative perception, where agents are equipped with different sensor types (cameras vs LiDAR). 

- It presents a novel heterogeneous 3D graph attention module (H^3GAT) to effectively capture both inter-agent and intra-agent interactions for fusing features from different modalities and agents. Two versions are proposed - H^3GAT-L for local interactions and H^3GAT-G for global interactions.

- Extensive experiments on a new V2V perception benchmark dataset OPV2V demonstrate that HM-ViT outperforms previous state-of-the-art methods for cooperative perception by large margins. For example, for camera agents, AP improves from 2.1% to 53.2% with the help of LiDAR agents.

- The proposed method exhibits superior flexibility and robustness in handling varying numbers and types of agents, which is important for real-world deployment.

In summary, the key contribution is proposing the first end-to-end framework HM-ViT to effectively enable heterogeneous multi-agent collaborative perception, which significantly improves perception performance and robustness. The novel heterogeneous graph attention design is critical for capturing cross-modality interactions.
