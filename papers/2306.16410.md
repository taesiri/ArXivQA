# Towards Language Models That Can See: Computer Vision Through the LENS
  of Natural Language

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper aims to address is:How can we enable large language models (LLMs) to effectively perform computer vision tasks, without requiring additional multimodal pretraining or paired vision-language datasets? The key hypothesis is that by extracting rich textual descriptions of images using diverse "vision modules", and feeding this text directly into a frozen LLM, the model can learn to solve tasks like object recognition and visual reasoning in a zero-shot manner. The authors propose a framework called LENS that coordinates independent vision modules to convert images into natural language, which is then fed into an unmodified LLM to perform reasoning and generate responses. The goal is to avoid expensive multimodal pretraining of LLMs on image-text pairs, and instead leverage the inherent capabilities of LLMs to understand language descriptions of visual content.In essence, the central research question is how to get vision "for free" in LLMs, without alignment pretraining, by relying on their core strength of understanding language. The hypothesis is that descriptive text from vision modules plus an unmodified LLM is sufficient for visual reasoning, which LENS aims to demonstrate.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it aims to address is:How can we enable large language models (LLMs) to effectively perform computer vision tasks, without requiring additional multimodal pretraining or alignment data?The key hypothesis is that by providing rich textual descriptions of images generated from vision modules, LLMs can leverage their natural language understanding capabilities to successfully tackle vision and vision-language tasks. In particular, the paper proposes a framework called LENS that coordinates independent vision modules to extract comprehensive textual information about images, which is then fed into a frozen LLM. The goal is to avoid the computational overhead and data requirements of aligning modalities through joint multimodal pretraining, while still allowing LLMs to handle visual inputs.The main research questions and goals that LENS targets can be summarized as:- Can detailed textual representations of images enable LLMs to perform well on vision tasks without multimodal pretraining?- Is it possible to add visual capabilities to any off-the-shelf LLM by using vision modules, without needing extra data or training?- Can a modular framework coordinate vision modules to provide rich textual scene descriptions that allow a frozen LLM to succeed on object recognition and visual reasoning?- Can this approach compete with or outperform end-to-end pretrained multimodal models while being more flexible and efficient?So in essence, the paper explores whether coupling textual information from vision modules with frozen language models can be an effective alternative to aligning modalities through costly joint pretraining. The central hypothesis is that the textual descriptions can act as a "lens" through which LLMs can understand and reason about visual inputs.
