# [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134v6)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we automate prompt engineering for adapting large pre-trained vision-language models like CLIP to downstream vision tasks in a data-efficient manner?

The key points are:

- CLIP and other similar models rely on prompt engineering (manually designing prompts) for downstream transfer. But prompt engineering requires expertise and extensive tuning. 

- The authors propose a simple prompt learning approach called Context Optimization (CoOp) to automate prompt engineering. It models a prompt's context words with continuous vectors that can be optimized through standard training.

- Experiments on 11 diverse vision datasets demonstrate CoOp effectively turns CLIP into a data-efficient learner, requiring very few shots (1-2) to outperform hand-crafted prompts. More shots bring further gains.

- Despite being a learning method, CoOp enhances robustness to domain shifts over hand-crafted prompts. It also outperforms common baselines like linear classification probe.

In summary, the central hypothesis is that prompt learning via Context Optimization can automate prompt engineering and adapt pre-trained vision-language models like CLIP to downstream tasks efficiently. The results provide positive evidence supporting this hypothesis.
