# [Remove that Square Root: A New Efficient Scale-Invariant Version of   AdaGrad](https://arxiv.org/abs/2403.02648)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adaptive optimization methods like AdaGrad are popular as they automatically tune the learning rate. However, AdaGrad performs poorly on ill-conditioned or poorly scaled data. 

- The paper aims to develop an adaptive optimization algorithm that is invariant to scaling of the data and matches AdaGrad's convergence guarantees.

Proposed Method (KATE):
- The paper proposes a new algorithm called KATE which is a variant of AdaGrad with two key changes:
  1) It removes the square root from AdaGrad's denominator to make the step size scale-invariant. 
  2) It uses a more aggressive sequence in the numerator to compensate and ensure convergence.

- KATE updates each weight coordinate as:  
    w[k] += - (beta*m[k]) / (b[k]^2) * g[k]
     Where b accumulates past gradients, m grows over time, g is the current gradient.

- For generalized linear models, the paper proves KATE is scale-invariant - its functional values are identical for scaled and unscaled data when started from zero weights.

Convergence Guarantees:
- For smooth non-convex problems, KATE achieves the optimal O(log(T)/sqrt(T)) convergence rate in terms of expected gradient norm, matching AdaGrad and Adam. 

- The analysis handles the more aggressive steps of KATE using properties like decreasing step size and closed-form bounds.

Experiments:
- Experiments on logistic regression and neural networks for image/text classification tasks show KATE consistently matches/beats AdaGrad and Adam.

- Plots also validate the scale-invariance on a synthetic logistic regression task.

In summary, the paper develops a scale-invariant adaptive method KATE that enjoys optimal convergence guarantees and strong empirical performance on par with popular techniques like Adam.
