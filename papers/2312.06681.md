# [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Steering Llama 2 via Contrastive Activation Addition":

Problem: 
As large language models (LLMs) become more capable, it is important to ensure they are helpful, honest, and harmless. However, techniques like reinforcement learning from human feedback and prompt engineering have limitations in preventing unwanted behaviors like hallucination. The paper explores using "activation engineering" methods like Contrastive Activation Addition (CAA) to better control LLMs.

Method:
CAA computes "steering vectors" that encode directions in the LLM's latent space corresponding to high-level behaviors. These are generated by taking the difference in activations between pairs of prompts demonstrating the presence/absence of behaviors. During inference, the steering vectors are added to modify activations and precisely control the LLM's behavior.

The authors generate CAA steering vectors for six categories of behaviors using contrastive multiple choice questions and model-rated free text generations. They test CAA's ability to modulate behaviors in the Llama 2 collection of models.

Results:
Across behaviors, adding/subtracting steering vectors successfully increases/decreases the prevalence of targeted behaviors as measured by model ratings, with larger effects for free text generation. CAA outperforms few-shot prompting, provides additional control atop other methods like finetuning, and minimally impacts capabilities. Analyses reveal CAA steering vectors represent high-level concepts and cluster model behavior.

Conclusion: 
The paper demonstrates CAA as an effective, minimally destructive method for steering LLMs that could complement existing alignment techniques. Analyzing steering vectors also sheds light on LLM representations. Future work could test multi-layer interventions and conversion of vectors to text.


## Summarize the paper in one sentence.

 This paper introduces Contrastive Activation Addition (CAA), a method to steer language models by adding "steering vectors" computed from the difference in activations on contrastive prompt pairs demonstrating positive and negative examples of target behaviors. CAA is shown to effectively modulate alignment-relevant properties of Llama models with minimal destructive effects.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing Contrastive Activation Addition (CAA), a novel method for steering language model behavior by modifying activations during inference. The key ideas are:

1) Generating "steering vectors" that encode target behaviors (e.g. sycophancy, hallucination) by taking the average difference in activations between pairs of positive/negative contrastive prompt examples. 

2) Adding these steering vectors to all token positions during inference with positive/negative coefficients to precisely adjust the degree of the targeted behavior in free-form text generation.

3) Demonstrating CAA effectively steers Llama 2 Chat in a range of alignment-relevant directions, outperforming techniques like finetuning and few-shot prompting. It has minimal negative capability impacts and provides interpretability into how concepts are represented in LLMs.

In summary, the main contribution is introducing and validating CAA as an effective, minimally destructive method for steering LLMs that works synergistically with other techniques, shedding light on model representations.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the main keywords and key terms associated with this paper include:

- Contrastive Activation Addition (CAA)
- Steering vectors
- Large language models (LLMs) 
- Alignment
- Helpful, honest, harmless (H3)
- Activation engineering
- Representation engineering  
- Residual stream activations
- Sycophancy
- Hallucination
- RLHF (Reinforcement learning from human feedback)
- Prompt engineering
- Capabilities
- Llama 2
- Claude 2
- GPT-3.5
- TruthfulQA

The paper introduces the CAA technique for steering LLMs' behavior by modifying activations during forward passes. It generates "steering vectors" capturing behavioral directions and adds them to model activations to control the degree of targeted behaviors like sycophancy and hallucination. The method is evaluated on the Llama 2 model suite using metrics like the H3 behaviors and TruthfulQA. Overall, CAA is shown to effectively modulate alignment-relevant properties with minimal destructive impacts on model capabilities.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the Contrastive Activation Addition (CAA) method proposed in the paper:

1. How does CAA compare to other alignment techniques like finetuning and few-shot prompting in terms of effectiveness and minimization of negative capability impacts? What are the relative strengths and weaknesses?

2. The paper hypothesizes that intervening at a small number of intermediate layers is most effective for CAA. What evidence supports this and why might this be the case? 

3. The paper finds that CAA steering vectors generalize well from multiple choice datasets to open-ended generation. However, what other tests could be done to further validate the generalization capability and determine its limits?

4. How invariant are the CAA steering vectors to different prompts that aim to elicit the same high-level behavior? To what extent can factors like prompt length, contrarian information etc. distort the steering effects? 

5. The visual analysis of activations using projections and steering vector similarities provides some interpretability into CAA's mechanisms. However, what other analysis methods could give additional insight into how the steering vectors encode information?

6. How does the effectiveness of CAA change when multiple steering vectors for different behaviors are applied simultaneously at different layers? What interactions occur?

7. Can the coefficients on CAA steering vectors be set dynamically and conditioned on the prompt rather than fixed ahead of time to give more precision? 

8. The paper hypothesizes similarities between RLHF training objectives and CAA effects. What further experiments could elucidate the relationship between RLHF mechanisms and steering vector representations?

9. How can CAA be used adversarially as a "red team" technique to stress test model behaviors during development rather than solely for alignment?

10. The paper mentions converting steering vectors to text as future work. What challenges does this pose compared to embedding-to-text techniques, and what novel understanding could be gained about information encoding?
