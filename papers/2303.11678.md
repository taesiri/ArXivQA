# [Full or Weak annotations? An adaptive strategy for budget-constrained   annotation campaigns](https://arxiv.org/abs/2303.11678)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper aims to address is: 

Given a fixed annotation budget for a new segmentation dataset, what is the optimal proportion of the budget that should be allocated to full segmentation annotations versus weaker image-level classification annotations in order to maximize the performance of the trained segmentation model?

The key hypothesis is that there is a dataset-specific optimal proportion of segmentation vs classification annotations that maximizes segmentation performance under a fixed budget constraint. This optimal proportion is unknown a priori and depends on factors like the dataset domain, image characteristics, number of classes, etc. 

The paper proposes an adaptive online method to find this optimal annotation proportion by modeling the expected utility of different annotation allocations and sequentially determining the allocation that maximizes the expected improvement in the segmentation model.

In summary, the core research question is about optimally allocating a segmentation annotation budget between expensive pixel-level segmentations and cheaper image-level labels for a new dataset in order to maximize downstream segmentation performance. The key hypothesis is that there exists an optimal but unknown annotation proportion that is dataset-specific.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a novel method to find an optimal budget allocation strategy for annotating datasets for semantic segmentation in an online manner. 

Specifically, the key ideas are:

- The paper focuses on the setting where annotations need to be gathered from scratch for a new segmentation task/dataset. It considers using a combination of expensive pixel-wise (strong) annotations and cheaper image-level class (weak) annotations. 

- It models the utility of different annotation strategies (combinations of strong/weak annotations) using a Gaussian Process (GP) and uses this to sequentially determine how to allocate portions of a total annotation budget.

- The main algorithm alternates between: (1) allocating a sub-budget and acquiring new annotations based on the current GP model; (2) training new segmentation models on the annotations so far to estimate performance; (3) updating the GP with these results to improve modeling of annotation strategies. 

- This allows dynamically determining a good annotation strategy tailored to the dataset, rather than using a fixed split of weak vs strong annotations.

- Experiments on 4 datasets show the proposed method is able to adaptively find strategies that achieve high segmentation performance across different budget sizes, close to the best fixed (but unknown) strategies.

In summary, the key contribution is an online method to sequentially determine an optimal annotation strategy for a new segmentation task/dataset by modeling the utility of different combinations of weak/strong annotations using a Gaussian Process.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from this paper:

The paper proposes a novel dynamic approach that determines the optimal proportion of full semantic segmentation and weaker image-level classification annotations to collect under a fixed budget to maximize segmentation model performance for a given dataset.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this CVPR 2023 paper submission compares to other related work in weakly supervised semantic segmentation:

- The key novelty of this paper is in dynamically determining the optimal proportion of weak (image-level) vs strong (pixel-level) annotations to collect under a fixed annotation budget. Most prior work in this area either assumes all annotations are available or uses a fixed split of weak/strong labels. The adaptive annotation strategy proposed here is novel.

- The work is similar in spirit to budget-aware active learning methods that aim to choose samples to annotate for maximizing performance under a budget. However, this paper focuses on selecting annotation types rather than samples. As the authors point out, the two approaches are largely orthogonal.

- Another related line of work optimizes the annotation strategy to reach a target performance level (e.g. Mahmood et al. ECCV'22, CVPR'22). In contrast, this paper aims to maximize performance for a given fixed budget, which is a subtly different but equally valid objective.

- For the weakly supervised segmentation model itself, the paper relies on a simple two-stage training approach, unlike more complex methods in recent weakly supervised segmentation literature. But the focus is not on the model architecture - any model could be plugged in.

- The Gaussian Process modeling to estimate utility is sound and aligns well with Bayesian optimization techniques for hyperparameter tuning. The iterative approximation scheme to deal with circular dependency is also reasonable.

So in summary, I feel the core ideas around adaptive annotation type selection are novel, while the underlying weakly supervised segmentation model and Gaussian Process utility modeling are relatively standard. The empirical validation on diverse datasets is quite thorough. Overall it advances the state-of-the-art in annotation strategy search for weakly supervised segmentation.
