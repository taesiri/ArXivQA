# [Symmetric Q-learning: Reducing Skewness of Bellman Error in Online   Reinforcement Learning](https://arxiv.org/abs/2403.07704)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- In deep reinforcement learning, the value function (e.g. Q-function) is often trained using least squares, which implicitly assumes the Bellman error distribution is Gaussian. 
- However, due to properties of the Bellman operator, the error distribution can be skewed, violating the assumption and leading to poor performance.
- Extreme value theory suggests the error follows a Gumbel distribution, but this assumption can be inaccurate in practice as dependencies violate premises. The actual distribution varies across tasks and algorithms.

Proposed Solution: 
- The paper proposes Symmetric Q-learning to reduce skewness and make the error distribution more Gaussian. 
- It adds synthetic noise sampled from a learned distribution to the Q-learning target values. The noise distribution is learned to match the inverse of the negative Bellman error distribution.
- Adding this noise with opposite skew makes the total error more symmetric while introducing no estimation bias. A Gaussian mixture model (GMM) represents the noise distribution.

Contributions:
- Proposes a method to reduce skewness and correct arbitrary error distributions without assumptions by adding target noise.
- Evaluates it on MuJoCo tasks by applying it to SAC and REDQ RL algorithms as SymSAC and SymREDQ.
- Achieves better sample efficiency and final performance than state-of-the-art model-free algorithms.
- Visualizes and confirms it makes Bellman error distributions more symmetric during learning.

In summary, the paper addresses skewed Q-learning error distributions via a flexible symmetry-correcting noise approach called Symmetric Q-learning. Experiments show it improves performance and corrects errors to be more symmetric.
