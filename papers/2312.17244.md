# [The LLM Surgeon](https://arxiv.org/abs/2312.17244)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
State-of-the-art language models are becoming very large in order to achieve high performance on available textual data. However, their huge size makes deployment difficult due to computational, environmental or device constraints. Training smaller models from scratch is not always a good solution. Therefore, the authors explore data-driven compression of existing pretrained models as an alternative.

Proposed Solution: 
The authors propose a method called "LLM Surgeon" to compress large language models in a data-driven way. The key ideas are:

1) Use Kronecker-factored approximations of the loss landscape curvature, which scales to large models while remaining practical. This allows computing costs of removing parameters as well as updated values for remaining parameters.

2) Provide a general framework for unstructured, semi-structured and structured pruning. Go beyond independent weight updates to consider correlations between weights for better updates.

3) Prune in multiple shots - remove some parameters, update others, reestimate curvature, and repeat. This handles larger parameter changes.

4) Allow trading off compute time during compression for accuracy by considering more weight correlations and more shots.

5) Use optional low-rank updates between shots to further improve performance.

Main Contributions:

- First method to successfully perform structured pruning of language models, removing entire rows/columns of weight matrices.

- State-of-the-art results for unstructured and semi-structured pruning of large language models. 

- General framework supporting unstructured, semi-structured and structured pruning as well as trading off compression time and accuracy.

- Can compress models substantially (20-30% smaller) with minimal performance loss, allowing "horizontal scaling" between existing models.

In summary, the paper provides an effective data-driven compression technique to reduce the huge computational and memory costs of large language models for improved deployment. The method is general, achieves state-of-the-art results, and uniquely supports structured pruning.
