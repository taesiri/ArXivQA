# [MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning](https://arxiv.org/abs/2402.11260)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adapting large language models (LLMs) to new domains/tasks and enabling efficient lifelong learning is challenging. Existing methods rely on impractical fact triplets as inputs and are vulnerable to catastrophic forgetting. 

- There is a need for practical data curation strategies and modeling techniques for lifelong learning of LLMs that work with simple question-answer pairs and mitigate catastrophic forgetting.

Method:
- Proposes MoRAL - a mixture-of-experts architecture augmented with low-rank adaptation for lifelong learning of LLMs. 

- Relies on question-answer pairs instead of fact triplets as input. Combines benefits of multi-task modeling of MoE and parameter-efficient fine-tuning of LoRA.

- Introduces 5L-bench benchmark with newly curated question-answer dataset from Arxiv papers and evaluation metrics for open-book, closed-book and cross-settings.

Contributions:
- Shows LLMs learn faster in open-book setting - e.g. 30.15% higher accuracy for Phi-2-2.7B using MoRAL versus closed-book.

- MoRAL gives higher performance gains for larger models. Robust to catastrophic forgetting with better knowledge retention than baselines.

- 5L-bench allows rigorous analysis of lifelong learning methods using simple question-answer based data curation from raw text.

In summary, the paper introduces an effective modeling technique and evaluation benchmark to advance the lifelong learning capabilities of LLMs using practical data formats.
