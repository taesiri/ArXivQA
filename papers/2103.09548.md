# ENCONTER: Entity Constrained Progressive Sequence Generation via   Insertion-based Transformer

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question addressed is: How to develop an insertion-based transformer model that can efficiently generate text sequences with hard lexical constraints, specifically entity constraints?The key points are:- Existing insertion-based transformers like Pointer do not handle entity constraints well. A naive adaption like Pointer-E suffers from "cold start problem" where generation fails in early stages.- The paper proposes two new models - Greedy Enconter and BBT-Enconter - that use bottom-up masking and balanced binary tree reward to address the cold start issue and inefficiency.- Experiments show Enconter models achieve higher recall, quality, lower failure rate and efficiency compared to Pointer-E and GPT-2 baselines on constrained text generation tasks.In summary, the central hypothesis is that a bottom-up insertion approach with binary tree rewards can enable transformers to efficiently generate high quality text under hard entity constraints. The Enconter models are proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing two new insertion transformer models, Enconter and BBT-Enconter, to address the challenges of enforcing hard entity constraints in text generation. Specifically:- The paper analyzes existing insertion transformer models like Pointer and identifies issues with enforcing entity constraints, such as the cold start problem where models fail to generate meaningful tokens early on. - To address this, Enconter uses a bottom-up masking strategy during training to encourage generating more meaningful tokens early in the process. - BBT-Enconter further incorporates a balanced binary tree reward scheme to improve efficiency by reducing the number of insertion steps required.- Experiments on real-world datasets show Enconter and BBT-Enconter outperform baselines like Pointer-E and GPT-2 in metrics like recall, quality, and failure rate while maintaining efficiency.In summary, the key contribution is proposing two new insertion transformer models designed specifically to handle hard entity constraints in text generation through modifications like bottom-up masking and balanced binary tree rewards. Experiments validate these models outperform prior approaches on key metrics.
