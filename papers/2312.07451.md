# [Daily Assistive View Control Learning of Low-Cost Low-Rigidity Robot via   Large-Scale Vision-Language Model](https://arxiv.org/abs/2312.07451)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

In this paper, the authors develop a system for a low-cost, low-rigidity robot arm to perform daily assistive tasks by controlling its camera view based on linguistic instructions. They combine the robot arm with CLIP, a large-scale vision-language model, to interpret commands and guide the robot's motion. To account for the imprecision in the flexible arm and variations in CLIP's outputs, they train a neural network model called SPNPB to capture stochastic correlations between visual data and robot joint information like angles and torques. SPNPB also uses a parametric bias input to adapt to changes over time and in the environment. Experiments demonstrate that by learning these probabilistic relationships and updating the parametric bias online, their method enables robust open-vocabulary view control on an actual low-cost robot arm according to diverse verbal instructions, despite hardware limitations. The authors plan to extend this approach to coordinate more modalities like language, audio, and touch to achieve more complex robot control based on intuitive human commands.
