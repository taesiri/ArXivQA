# [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787)

## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of RewardBench, a benchmark dataset and codebase for evaluating reward models for language modeling. Specifically:

- RewardBench contains a diverse dataset of over 2,900 prompt-win-lose trios spanning capabilities like chat, reasoning, and safety to benchmark reward models. The subsets are designed to vary in difficulty and test different aspects of reward model capabilities.

- The paper evaluates a wide variety of publicly available reward models on RewardBench, including models trained as classifiers and with direct preference optimization. This allows them to compare model performance and behaviors across model sizes and training methods.

- The authors use RewardBench to analyze model performance limitations, differences between classifier and DPO models, the impact of model scaling, reasoning abilities, and refusal behaviors. This sheds light on the inner workings and capabilities of different reward modeling approaches.

- They release the full dataset, codebase, and trained model outputs to enable further analysis and improvement of reward models. The goal is to promote stronger scientific understanding of how human preferences get embedded in language models via the reward modeling process.

In summary, the main contribution is the creation and analysis of the first comprehensive benchmark specifically designed for evaluating and understanding the capabilities of reward models for language modeling.
