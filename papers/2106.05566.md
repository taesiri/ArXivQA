# [A Neural Tangent Kernel Perspective of GANs](https://arxiv.org/abs/2106.05566)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new theoretical framework for analyzing generative adversarial networks (GANs) using neural tangent kernels (NTKs). The key research questions it seeks to address are:1. How can we accurately model the discriminator architecture and its effect on GAN training dynamics, given that GANs are trained with alternating optimization rather than the joint min-max optimization assumed in most prior theoretical analyses? 2. How does explicitly modeling the discriminator's parameterization as a neural network with certain inductive biases influence the generator's loss landscape and gradient-based training?3. Can leveraging NTK theory allow us to properly characterize the gradients and training dynamics of GANs, resolving theoretical issues around ill-defined or uninformative discriminator gradients that hinder convergence?4. What new insights can be gained about the losses implicitly optimized by common GAN formulations when analyzed within this proposed NTK framework?In essence, the paper aims to develop for the first time an NTK-based theoretical framework that properly captures the realities of GAN training (alternating optimization, neural network parameterizations) in order to arrive at a more accurate understanding of GAN losses, gradients, and dynamics. Resolving theoretical issues around discriminator gradients and characterizing the effect of inductive biases are the main focus.


## What is the main contribution of this paper?

The main contribution of this paper is developing a new framework for theoretical analysis of generative adversarial networks (GANs) using neural tangent kernels (NTKs). The key points are:- The paper proposes the first framework to model a wide range of discriminator architectures and GAN formulations while accurately capturing the alternating optimization procedure used in training GANs. This addresses limitations in prior GAN theory that failed to fully explain empirical GAN behavior. - The framework leverages recent advances in NTK theory to model the dynamics and inductive biases of the discriminator network. Novel theoretical results are derived, including establishing the differentiability of the discriminator's NTK which ensures its gradients are well-defined.- Analyzing GAN training through NTKs enables new discoveries about the generator's dynamics and loss landscape. For instance, Integral Probability Metric GANs are found to minimize a kernel Maximum Mean Discrepancy loss defined by the discriminator's NTK. - The theoretical framework is validated through a new analysis toolkit called GAN(TK)^2. Experiments confirm the approach better explains GAN training and provides new insights like the strong performance of ReLU networks compared to other activations.In summary, the key innovation is utilizing NTK theory to create a principled framework that models real GAN training more accurately than prior work. This enables both theoretical advances in understanding GAN optimization as well as empirical insights from experiments. Overall, the work helps explain and improve GAN training.
