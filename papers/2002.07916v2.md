# [Information Condensing Active Learning](https://arxiv.org/abs/2002.07916v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we develop an active learning method that selects batches of points to label such that the acquired labels give maximal information about the model's predictions on the remaining unlabeled data?

The key hypothesis appears to be:

Selecting batches to maximize the statistical dependency between the model's predictions on the acquired batch and the unlabeled data will minimize the model's uncertainty on the unlabeled data. This will in turn maximize accuracy on unseen test data.

In summary, the paper introduces a new batch mode active learning technique called Information Condensing Active Learning (ICAL) that aims to select batches that maximize information about the model predictions for unlabeled data. The central hypothesis is that this will improve model accuracy compared to methods that maximize information only about model parameters.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution appears to be the introduction of a new batch mode active learning method called Information Condensing Active Learning (ICAL). The key ideas behind ICAL seem to be:

- It tries to select a batch of points that have maximum statistical dependency (measured by HSIC) between the model's predictions on the batch and the model's predictions on the remaining unlabeled points. This is in contrast to prior methods that maximize information gain with respect to the model parameters. 

- By maximizing dependency between the query batch and unlabeled set predictions, ICAL aims to acquire points that provide substantial information about the labels of the still unlabeled data. This helps reduce model uncertainty on the unlabeled set more effectively.

- The paper develops optimizations to allow ICAL to scale to large batch sizes and unlabeled pools by using greedy search and averaging kernel matrices. 

- ICAL is model-agnostic and can work with any model distribution where Monte Carlo sampling of predictions is possible.

- Experiments across image classification datasets like MNIST, CIFAR10, etc. demonstrate that ICAL outperforms prior state-of-the-art batch mode active learning techniques like BatchBALD and FASS in terms of accuracy and log-likelihood.

In summary, the key contribution appears to be the proposal of a new batch active learning method ICAL that selects informative and diverse batches by maximizing statistical dependency between query points and the unlabeled set to improve model accuracy. The paper shows empirically that this approach outperforms existing methods.
