# [Patch Is Not All You Need](https://arxiv.org/abs/2308.10729)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question it aims to address is: How to effectively convert images into sequence inputs for vision transformers while preserving structural and semantic information?The paper proposes a novel "Pattern Transformer" approach to adaptively convert images into pattern sequences as inputs to the transformer architecture. This avoids the limitations of manual image patchification used in prior vision transformers like ViT, which disrupts the inherent structure and semantics of the image. The central hypothesis is that by using a convolutional neural network to extract a set of patterns from the image, where each pattern/channel captures a local region of interest, the model can preserve the intrinsic structural and semantic information of the image while generating a sequence input for the transformer.In summary, the paper focuses on researching better methodologies for converting image data into sequences for transformer-based vision models, moving beyond naive patchification approaches, via more adaptive pattern extraction that maintains visual structure and semantics. The proposed Pattern Transformer framework is evaluated on image classification tasks to validate its effectiveness.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a novel Pattern Transformer (Patternformer) to adaptively convert images into pattern sequences as inputs for the Transformer model. The key ideas are:- Using a Convolutional Neural Network (CNN) to extract various patterns from the input image, with each channel representing a unique pattern (visual token) that is fed into the Transformer. - This eliminates the need for manual rigid patchification of images, which disrupts structural and semantic information. The patterns preserve local structure and semantics.- The pattern embedding process is flexible and not constrained by image size or patch size. This improves modeling efficiency of the Transformer.- Combining CNNs to capture local patterns and Transformers to model global context, the model effectively utilizes strengths of both CNNs and Transformers.- Experiments show state-of-the-art performance on CIFAR-10 and CIFAR-100, and competitive results on ImageNet while using simple ResNet and Transformer architectures.In summary, the core contribution is presenting Pattern Transformer to convert images into compact yet flexible pattern sequences while preserving spatial/semantic information, as an improved input representation for Vision Transformers.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field:- This paper introduces a novel Pattern Transformer architecture for image classification. Unlike other vision transformers, it does not rely on manual patchification of images, but instead uses convolutional layers to extract semantic patterns that are fed as tokens into the transformer. This is a unique approach not seen in other works.- Most prior vision transformers like ViT, DeiT, and Swin Transformer use standard patch splitting of images before feeding them into a transformer. This disrupts the structural and semantic information in the image. The Pattern Transformer avoids this issue by learning semantic patterns as tokens instead.- Other hybrid vision transformers like LeViT, BoTNet, and Twins use a combination of convolutional layers and transformers. However, they still rely on patchification as the input to the transformer. The Pattern Transformer's use of learned pattern tokens is novel compared to these approaches.- Some recent works like MAE and A-Vit have tried to address patch redundancy issues in vision transformers. However, they still use standard patchification, unlike the Pattern Transformer's learned patterns.- For image classification on CIFAR and ImageNet, the Pattern Transformer achieves state-of-the-art or very competitive results to other leading vision transformer models. This demonstrates the effectiveness of its pattern token approach.- The Pattern Transformer achieves these results with a simple combination of a ResNet backbone for pattern extraction and a standard Transformer for modeling. This is a simple yet effective design compared to more complex vision transformer architectures.Overall, the Pattern Transformer introduces a novel and intuitive way of conditioning images as inputs for transformers without disrupting image semantics. The results demonstrate this is an effective and competitive approach in the field compared to other vision transformers that use standard patchification. The learned pattern tokens are a unique concept introduced in this work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring the effectiveness of Pattern Transformer on other visual tasks like object detection and segmentation. The current work focuses solely on image classification, so applying Patternformer to other computer vision problems could be an interesting avenue to pursue.- Investigating more efficient and robust methods for pattern extraction. The authors note limitations around Patternformer's reliance on a complex tokenizer to extract good patterns. Developing better techniques here could improve performance and efficiency.- Trying different convolutional backbones besides ResNet for pattern extraction. The paper mainly uses ResNet but other CNN architectures could potentially capture patterns in a more optimal way.- Adapting Patternformer for video inputs to take advantage of temporal patterns. The authors mention this as a possibility since their method currently only handles images.- Combining Patternformer with other Transformer architectures like Swin or Pyramid Vision Transformer. The paper experiments with vanilla ViT but hybridizing Patternformer with more advanced Transformer designs could further improve results.- Developing better training strategies and hyperparameters for Patternformer, as the authors use a simple consistent recipe across experiments. More extensive hyperparameter tuning could lead to accuracy gains.- Applying Patternformer to larger-scale datasets like ImageNet-21k or newer benchmarks. The paper tests on CIFAR and ImageNet-1k so evaluating on newer, larger datasets would be interesting.Overall, the main future work revolves around enhancing pattern extraction, adapting Patternformer to other vision tasks and data, and combining it with recent advances in CNN and Transformer architectures for computer vision. Testing the approach on newer datasets and benchmarks could also reveal more about its capabilities and limitations.
