# [CorpusBrain: Pre-train a Generative Retrieval Model for   Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2208.07652)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is:How can we pre-train a general-purpose, single-step generative retrieval model that encodes corpus knowledge in its parameters, such that it can be readily adapted to improve performance on a diverse range of downstream knowledge-intensive language tasks, without needing to build task-specific indexes?The authors propose a model called "CorpusBrain" which is pre-trained using three carefully designed self-supervised objectives to encode query-document relevance information. The goal is for CorpusBrain to serve as a universal retrieval module that can plug into various downstream applications and provide performance gains without requiring application-specific fine-tuning or indexing. The paper evaluates CorpusBrain on a comprehensive benchmark of knowledge-intensive NLP tasks and shows it achieves significant improvements over strong baselines.In summary, the central hypothesis is that with the right pre-training approach, a single generative retrieval model can encapsulate broad corpus knowledge and provide universal benefits to diverse knowledge-intensive applications through its ability to effectively map queries to relevant documents. The paper aims to demonstrate this via the proposed CorpusBrain model.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a pre-trained generative retrieval model called CorpusBrain to encode all the knowledge about a corpus into model parameters. The key ideas are:1. Designing three pre-training tasks (ISS, LPS, HIP) to create pseudo query-document identifier pairs that capture different granularities of semantics for generative retrieval. 2. Pre-training a Transformer encoder-decoder model via standard seq2seq learning on the pseudo pairs to obtain a unified generative retrieval model without needing additional indexing.3. Demonstrating that CorpusBrain significantly outperforms strong baselines on retrieval tasks across multiple datasets in the KILT benchmark, especially under zero-shot and low-resource settings.In summary, the paper proposes a novel way to pre-train a generative retrieval model that encodes corpus knowledge within parameters and can generalize well to improve various downstream tasks through further fine-tuning. This helps simplify and unify the traditional pipeline of indexing, retrieval, and ranking into an end-to-end learned model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes CorpusBrain, a pre-trained generative retrieval model that encodes corpus knowledge into its parameters through self-supervised pre-training tasks, eliminating the need for traditional indexing while achieving strong performance on knowledge-intensive language tasks under both zero-shot and low-resource settings.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research on knowledge-intensive language tasks (KILT):- This paper focuses on the retrieval component for KILT, proposing a novel pre-trained generative retrieval model called CorpusBrain. Much existing research has focused more on the reader component, using large pre-trained language models like BERT and T5. This paper provides a new perspective on improving KILT through advances in the retrieval step.- The proposed model CorpusBrain aims to encode all knowledge about the corpus into the parameters of a single model, removing the need for an additional retrieval index. This aligns with recent interest in model-based IR as an alternative to traditional pipeline retrieval models that require indexing and multiple components. - The authors design three self-supervised pre-training tasks to learn query-document relevance without labeled data. This differs from other generative retrieval models like GENRE and GERE which require task-specific supervised data. The pre-training allows CorpusBrain to generalize to new KILT tasks with minimal fine-tuning.- Experiments show CorpusBrain outperforms both traditional IR models and recent model-based baselines on the KILT benchmark across diverse tasks. It also performs well in low-resource settings. This demonstrates the advantages of the pre-training approach compared to task-specific modeling.- Overall, this paper pushes forward model-based IR for KILT in a unique way through self-supervised pre-training. The gains over both traditional and neural IR models validate the potential of this direction to improve the scalability, generalization, and end-to-end integration for KILT systems.In summary, this paper presents a novel perspective on KILT focusing on the retrieval step, and shows promising results compared to existing literature in this problem space. The pre-training methodology and model-based IR approach appear well-suited to the challenges of these knowledge-intensive tasks.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Explore other document identifiers beyond just article titles, such as page URLs and hash codes, to see if they can further enhance the pre-training and performance of CorpusBrain.- Test CorpusBrain's ability on other types of downstream IR tasks beyond the KILT benchmark, such as ad-hoc retrieval, passage retrieval for QA, and response retrieval for dialog systems. This would help analyze the model's generalization ability.- Design an end-to-end system for knowledge-intensive language tasks using a fully generative approach, where both the retrieval and reading components are generative models. The authors suggest it could be valuable to explore this direction.- Investigate new ways to further enhance the pre-training objectives and process to make it even more tailored for generative retrieval. Since pre-training was key to CorpusBrain's performance, improving it further could lead to gains.- Analyze why CorpusBrain underperformed on some QA datasets compared to others. The different nature of questions versus declarative sentences may be a factor. Finding ways to improve the pre-training for QA could help.- Explore limitations of the beam search strategy for generating multiple documents and dependency between documents. The authors suggest this could be improved.In summary, the main future directions are enhancing the pre-training, testing on new tasks, analyzing limitations, and exploring end-to-end generative systems for knowledge-intensive language tasks. Improving generalization and QA performance are called out as specific areas needing more work.
