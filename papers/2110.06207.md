# [Open-Set Recognition: a Good Closed-Set Classifier is All You Need?](https://arxiv.org/abs/2110.06207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Is a well-trained closed-set classifier sufficient for performing open-set recognition, or are more complex open-set-specific methods needed?The key points related to this question are:- The paper investigates the relationship between a model's closed-set classification performance and its ability to detect open-set examples. It finds a strong correlation, suggesting that improving closed-set accuracy also boosts open-set performance.- The authors demonstrate that by improving the training of a standard cross-entropy classifier (longer training, better data augmentation, etc.), its open-set performance can be pushed to match or exceed more complex state-of-the-art open-set methods. - The paper proposes new evaluation benchmarks focused on semantic novelty detection, and finds negligible performance difference between their improved baseline and prior state-of-the-art methods.So in summary, the central hypothesis seems to be that a well-optimized closed-set classifier may be sufficient for performing open-set recognition, without needing complex open-set-specific architectures or training strategies. The paper provides empirical evidence to support this claim across various datasets, splits, and model architectures.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Demonstrating a strong correlation between a model's closed-set classification performance and its ability to detect novel inputs in an open-set setting. Experiments across datasets, architectures, and methods show that better closed-set classifiers tend to perform better at open-set recognition.2. Using this insight to significantly improve the performance of a simple maximum logit scoring baseline, making it competitive with or better than more complex state-of-the-art open-set recognition methods on standard benchmarks.3. Proposing a new "Semantic Shift Benchmark" suite of datasets that better captures the notion of semantic novelty for open-set recognition. These include fine-grained classification datasets like CUB-200 as well as an ImageNet-scale split.4. Evaluating methods on the proposed benchmark and showing that once closed-set performance is controlled for, there is little difference between a maximum logit scoring baseline and more sophisticated open-set recognition algorithms.In summary, the key insight is that a good closed-set classifier provides a strong baseline for open-set recognition, and much of the gains of recent open-set methods may be attributed to improvements in closed-set accuracy. The proposed benchmark aims to facilitate more controlled study of semantic novelty detection.
