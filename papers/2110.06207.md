# [Open-Set Recognition: a Good Closed-Set Classifier is All You Need?](https://arxiv.org/abs/2110.06207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Is a well-trained closed-set classifier sufficient for performing open-set recognition, or are more complex open-set-specific methods needed?The key points related to this question are:- The paper investigates the relationship between a model's closed-set classification performance and its ability to detect open-set examples. It finds a strong correlation, suggesting that improving closed-set accuracy also boosts open-set performance.- The authors demonstrate that by improving the training of a standard cross-entropy classifier (longer training, better data augmentation, etc.), its open-set performance can be pushed to match or exceed more complex state-of-the-art open-set methods. - The paper proposes new evaluation benchmarks focused on semantic novelty detection, and finds negligible performance difference between their improved baseline and prior state-of-the-art methods.So in summary, the central hypothesis seems to be that a well-optimized closed-set classifier may be sufficient for performing open-set recognition, without needing complex open-set-specific architectures or training strategies. The paper provides empirical evidence to support this claim across various datasets, splits, and model architectures.
