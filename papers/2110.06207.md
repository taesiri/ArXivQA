# [Open-Set Recognition: a Good Closed-Set Classifier is All You Need?](https://arxiv.org/abs/2110.06207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Is a well-trained closed-set classifier sufficient for performing open-set recognition, or are more complex open-set-specific methods needed?The key points related to this question are:- The paper investigates the relationship between a model's closed-set classification performance and its ability to detect open-set examples. It finds a strong correlation, suggesting that improving closed-set accuracy also boosts open-set performance.- The authors demonstrate that by improving the training of a standard cross-entropy classifier (longer training, better data augmentation, etc.), its open-set performance can be pushed to match or exceed more complex state-of-the-art open-set methods. - The paper proposes new evaluation benchmarks focused on semantic novelty detection, and finds negligible performance difference between their improved baseline and prior state-of-the-art methods.So in summary, the central hypothesis seems to be that a well-optimized closed-set classifier may be sufficient for performing open-set recognition, without needing complex open-set-specific architectures or training strategies. The paper provides empirical evidence to support this claim across various datasets, splits, and model architectures.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Demonstrating a strong correlation between a model's closed-set classification performance and its ability to detect novel inputs in an open-set setting. Experiments across datasets, architectures, and methods show that better closed-set classifiers tend to perform better at open-set recognition.2. Using this insight to significantly improve the performance of a simple maximum logit scoring baseline, making it competitive with or better than more complex state-of-the-art open-set recognition methods on standard benchmarks.3. Proposing a new "Semantic Shift Benchmark" suite of datasets that better captures the notion of semantic novelty for open-set recognition. These include fine-grained classification datasets like CUB-200 as well as an ImageNet-scale split.4. Evaluating methods on the proposed benchmark and showing that once closed-set performance is controlled for, there is little difference between a maximum logit scoring baseline and more sophisticated open-set recognition algorithms.In summary, the key insight is that a good closed-set classifier provides a strong baseline for open-set recognition, and much of the gains of recent open-set methods may be attributed to improvements in closed-set accuracy. The proposed benchmark aims to facilitate more controlled study of semantic novelty detection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper demonstrates that there is a strong correlation between a classifier's closed-set accuracy and its ability to detect open-set examples, and shows that improving closed-set performance leads to competitive open-set recognition results compared to more complex methods.


## How does this paper compare to other research in the same field?

Here are some thoughts on comparing this paper to other research in open-set recognition:- This paper re-evaluates the commonly used "maximum softmax probability" (MSP) baseline for open-set recognition. It shows that with improved training strategies, the MSP baseline can be competitive with or outperform more complex state-of-the-art methods on standard benchmarks. This challenges the notion that the MSP baseline is weak and more sophisticated methods are needed.- The paper demonstrates a strong correlation between closed-set and open-set performance across models and datasets. This goes against the intuition that overfitting on known classes may hurt open-set performance. The correlation allows open-set performance to readily improve as image classification advances.- The paper proposes new large-scale and fine-grained image classification datasets for evaluating semantic novelty in open-set recognition. This contrasts with existing small-scale benchmarks that conflate semantic and distributional shifts. The new benchmarks better isolate the specific problem open-set recognition aims to solve.- When evaluated on the proposed benchmarks, the difference in performance between the improved MSP baseline and a state-of-the-art method (ARPL) is small. This further challenges the need for complex open-set methods given a strong closed-set classifier.- The paper draws connections between open-set recognition and related fields like out-of-distribution detection. It shows the MSP baseline can also achieve strong OOD detection results, and calls for more precise problem definitions and benchmarks going forward.In summary, a key contribution of this paper is re-evaluating assumptions in open-set recognition using careful experiments and analysis. It demonstrates that current complex methods provide little benefit over Improved baselines on existing and newly proposed benchmarks. The paper helps better characterize progress in this space.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Designing alternate methods to create open-set recognition (OSR) splits of varying difficulty from fine-grained visual categorization (FGVC) datasets. For example, creating splits based on distance between classes in the feature space of a pre-trained model, instead of using labelled attributes. This could lead to a more "objective" notion of semantic similarity between classes.- Developing models that can better exploit the attribute labels available in FGVC datasets to identify novel classes that lack certain attributes present in the training classes.- Continuing to scale up OSR methods and evaluation to even larger datasets beyond ImageNet, as the authors note their proposed FGVC evaluation protocols still operate on a relatively small scale compared to real-world deployment.- General open problems in further improving OSR performance through advances in representation learning and leveraging unlabeled data. The authors show current OSR models are still far from perfect in their proposed benchmark.- Better understanding the relationship between OSR and related problems like out-of-distribution detection. The authors provide some evidence that solutions to these problems may be somewhat orthogonal. Designing benchmarks to tease apart different forms of distribution shift could further this understanding.- Considering the ethical implications of OSR research, such as ensuring models are not over-trusted and training data is representative. The authors suggest this as an important area for future work.In summary, key directions are developing better evaluation protocols for OSR, continued progress on models and representations, scaling up current approaches, and exploring connections to related problems and ethical considerations. The proposed FGVC datasets offer opportunities in several of these areas.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper investigates the relationship between a classifier's performance on known classes and its ability to detect novel classes in open-set recognition. Through experiments on standard benchmarks and a large-scale ImageNet evaluation, the authors demonstrate a strong correlation between closed-set accuracy and open-set AUROC. Leveraging this, they are able to significantly improve the performance of a maximum logit score baseline by using techniques to enhance closed-set classification. The improved baseline reaches or exceeds state-of-the-art performance on several benchmarks. However, when the same optimizations are applied to previous methods, the remaining discrepancy is small. The authors argue that current open-set benchmarks lack scale and a precise definition of semantic classes. They propose new splits of fine-grained datasets that better isolate semantic novelty. Experiments on these benchmarks reinforce that once closed-set performance is comparable, differences between methods are negligible. Overall, the paper questions whether sophisticated open-set algorithms markedly outperform a well-optimized closed-set classifier.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper demonstrates a strong correlation between the closed-set classification performance and open-set recognition performance of deep neural network models. The authors show this correlation holds across different datasets, model architectures, and methods. They leverage this finding to significantly improve the performance of a simple maximum logit score (MLS) baseline for open-set recognition, making it competitive with more complex state-of-the-art methods on standard benchmarks. The key improvements to the baseline include longer training, better data augmentation, and using the maximum logit rather than softmax probability for open-set scoring. When applied to prior methods, these optimizations also improve performance, but the gap between methods is reduced. The authors propose new benchmarks focused on semantic novelty over distributional shift, where they again find the MLS baseline competitive with a top method. Overall, their results suggest that good closed-set performance leads to good open-set recognition, calling into question the need for complex open-set specific architectures.The paper first empirically demonstrates the correlation between closed and open-set performance across datasets, architectures, and methods. For example, on the standard OSR benchmarks, improving closed-set accuracy from 67.7% to 90.1% on CIFAR10 yields an open-set AUROC boost from 67.7% to 90.1%. The correlation also holds on ImageNet across various architectures. The authors suggest proper scoring rules and model calibration help explain this link. Next, the authors significantly improve a max logit score baseline by leveraging advances in closed-set image recognition. This improved baseline meets or exceeds prior state-of-the-art methods on several benchmarks. When applied to prior methods, the improvements raise their performance but largely close the gap to the baseline. Finally, the authors critique existing benchmarks and propose new ones focused on semantic novelty. On these benchmarks, the improved baseline remains competitive with a top method. The results question the need for complex open-set architectures given the sufficiency of a well-trained closed-set classifier.
