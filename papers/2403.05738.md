# [Provable Policy Gradient Methods for Average-Reward Markov Potential   Games](https://arxiv.org/abs/2403.05738)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper studies Markov potential games under the infinite horizon average reward criterion, which is more suitable for long-term strategic interactions compared to the commonly used discounted reward. However, policy-based reinforcement learning methods for average reward games remain largely unexplored. 

Main Contributions:

1. The paper shows that the average reward is a smooth function of policies, providing smoothness bounds under certain ergodicity assumptions. This enables the analysis of gradient-based algorithms.

2. Three algorithms - policy gradient, proximal-Q learning, and natural policy gradient are analyzed. With access to a gradient oracle, convergence to an ε-Nash equilibrium with O(1/ε^2) iterations is shown. 

3. A single-trajectory policy gradient estimator is proposed and shown to estimate gradients with sample complexity Õ(1/(π(a|s)δ)). This is used to obtain the first sample complexity bound of Õ(1/ε^5) for a projected policy gradient algorithm.

4. The sensitivity of differential value functions is characterized, which plays an important role in establishing regret bounds independent of the action set size. 

5. Numerical experiments validate the theoretical convergence guarantees. A more complex robot navigation task demonstrates the potential of independent learning for average reward Markov games.

In summary, the paper provides a comprehensive analysis of policy optimization methods for average reward Markov potential games, establishing iteration complexity under perfect gradient information and sample complexity when gradients are estimated. The results demonstrate the potential of applying such games to model long-term multi-agent sequential interactions.
