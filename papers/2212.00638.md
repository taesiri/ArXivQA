# [Finetune like you pretrain: Improved finetuning of zero-shot vision   models](https://arxiv.org/abs/2212.00638)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that finetuning vision models like CLIP in a manner that closely matches the pretraining objective leads to better performance compared to standard finetuning approaches. 

Specifically, the paper proposes a finetuning approach called "Finetune Like You Pretrain" (FLYP) which continues to train the model using the same contrastive loss as pretraining, just augmented with labeled data. The key hypothesis is that this approach will outperform standard finetuning techniques like cross-entropy training or two-stage methods like linear probing followed by finetuning. 

The paper presents extensive experiments across a variety of distribution shift, transfer learning, and few-shot classification tasks. The consistent finding is that FLYP outperforms alternative finetuning methods, often by significant margins. For example, FLYP improves accuracy by 4.2% on average across 7 distribution shift datasets compared to standard finetuning.

In summary, the central hypothesis is that matching the pretraining and finetuning objectives leads to better performance. The paper presents FLYP as a straightforward way to achieve this match for vision models, and shows through empirical results that it consistently improves accuracy across diverse tasks.


## What is the main contribution of this paper?

 This paper proposes a method called "Finetune Like You Pretrain" (FLYP) for finetuning vision models that have been pretrained on image-text data, like CLIP. The key ideas are:

- Instead of using cross-entropy loss for finetuning like standard methods, FLYP continues to use the contrastive loss that was used during pretraining. 

- During finetuning, class labels are cast as text prompts and the model is trained to align image embeddings with the prompt embeddings, just like during pretraining.

- This approach consistently outperforms standard finetuning and recent innovations like linear probing + finetuning in terms of accuracy on both in-distribution and out-of-distribution datasets.

- The gains hold across various settings - distribution shift, transfer learning, and few-shot learning. For example, FLYP achieves state-of-the-art accuracy on WILDS datasets.

- FLYP is simple to implement as it just uses the same loss as pretraining. The surprising effectiveness of this naive approach over more complex methods is the main contribution.

In summary, the key contribution is showing that continuing to train vision models with the pretraining contrastive loss, using class labels as prompts, is a surprisingly effective yet simple finetuning approach that outperforms existing methods across various settings.
