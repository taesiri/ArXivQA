# [Finetune like you pretrain: Improved finetuning of zero-shot vision   models](https://arxiv.org/abs/2212.00638)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that finetuning vision models like CLIP in a manner that closely matches the pretraining objective leads to better performance compared to standard finetuning approaches. 

Specifically, the paper proposes a finetuning approach called "Finetune Like You Pretrain" (FLYP) which continues to train the model using the same contrastive loss as pretraining, just augmented with labeled data. The key hypothesis is that this approach will outperform standard finetuning techniques like cross-entropy training or two-stage methods like linear probing followed by finetuning. 

The paper presents extensive experiments across a variety of distribution shift, transfer learning, and few-shot classification tasks. The consistent finding is that FLYP outperforms alternative finetuning methods, often by significant margins. For example, FLYP improves accuracy by 4.2% on average across 7 distribution shift datasets compared to standard finetuning.

In summary, the central hypothesis is that matching the pretraining and finetuning objectives leads to better performance. The paper presents FLYP as a straightforward way to achieve this match for vision models, and shows through empirical results that it consistently improves accuracy across diverse tasks.
