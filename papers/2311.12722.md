# [Attacking Motion Planners Using Adversarial Perception Errors](https://arxiv.org/abs/2311.12722)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a novel framework for systematically finding erroneous perception outputs that lead to failures in modular autonomous driving systems, despite scoring highly on common perception metrics. The authors demonstrate an adversarial attack algorithm inspired by decision-based attacks on classifiers that efficiently searches the space of perception errors to uncover the most impactful failure cases. They inject these adversarial perception errors into the perception pipeline of driving systems with two different black-box planners across several urban and highway scenarios in simulation. The identified errors achieve high scores on the nuScenes detection metrics, indicating good perceptual quality, yet still cause collisions. Further analysis reveals that while the errors appear likely according to an empirical perception error model, they are isolated in the space of possible perception outputs. Overall, the authors showcase flaws in the modular testing methodology of autonomous vehicles, raise concerns about the sufficiency of perception metrics for capturing downstream impact, and advocate for inclusion of such worst-case analyses during system development. Their approach and findings have important implications for building safety arguments and highlighting risks prior to real-world deployment.


## Summarize the paper in one sentence.

 This paper proposes an algorithm to systematically find perception errors that score highly on common perception metrics but still cause failures in autonomous vehicle motion planners.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method to systematically construct adversarial perception errors that cause failures in autonomous driving systems, despite scoring highly on common perception metrics. Specifically:

- They propose a framework to define and identify erroneous perception outputs that appear benign under standard perception metrics but still cause downstream planning failures in modular autonomous vehicles. 

- They provide an efficient search algorithm, inspired by boundary attacks on classifiers, that can find such adversarial perception errors in black-box driving systems using a combination of heuristic and random search.

- They demonstrate the effectiveness of their approach by attacking two different planners in various driving scenarios in the CARLA simulator, finding high perception quality errors that nevertheless cause collisions.

- They analyze the properties of the constructed attacks, showing they are isolated in the input space and discuss implications for testing and deployment of autonomous vehicles.

In summary, the key contribution is a method to actively search for and construct dangerous perception errors, exposing potential weaknesses in perception and planning modules. This can help improve safety of autonomous vehicles.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Adversarial perception errors - Erroneous perception system outputs that score highly on perception metrics but still cause failures in the planning system.

- Boundary attack algorithm - The algorithm proposed in the paper to systematically construct adversarial perception errors using a combination of heuristic and random search.

- Modular autonomous vehicles - Autonomous vehicles built from separate perception, prediction, and planning modules that are evaluated separately. 

- Perception metrics - Metrics like mean average precision and nuScenes detection score used to measure perception quality.

- Driving rules - Rules that define acceptable driving behavior and are used to evaluate overall system performance.

- Black-box systems - Systems like autonomous vehicles where gradient information is not available. The boundary attack algorithm is applicable to such systems.

- Failure modes - Specific ways in which a system fails, for example due to a particular set of perception errors. Identifying these helps improve system safety.

- Robustness - Used in the paper to characterize how localized adversarial perception errors are in the space of possible perception outputs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) The proposed approach relies on a heuristic search followed by a random search to find adversarial perception errors. Why was this two-step approach chosen over just using random search or just using heuristic search? What are the limitations of each of those approaches that led to the hybrid method?

2) The heuristic search algorithm utilizes a bisection approach to efficiently identify influential agents and determine the minimum track drop times to cause a collision. What is the intuition behind using a bisection approach here? How does it help make the search process more efficient compared to other search methods?

3) The random search refinement relies on a specialized proposal distribution for generating candidate perception errors. What factors influenced the design of this proposal distribution? How does biasing the distribution in certain ways help improve the efficiency and effectiveness of the overall search process? 

4) One finding mentioned is that the random search is not always effective in improving upon the heuristic search across different scenarios. What factors might cause the random search to sometimes fail to produce better adversarial perception errors? How could the random search process be made more robust?

5) The perception error model (PEM) is used to evaluate the likelihood of constructed adversarial perception errors. What are the main limitations and assumptions of using a PEM for this purpose? How could the assessment of likelihood be improved?

6) To demonstrate lack of support in the vicinity of the identified adversarial perception errors, random perturbations are applied. What other methods could have been used to analyze the localization of adversarial errors and estimate their effective support?

7) The approach focuses exclusively on finding perception errors rather than considering other components like the tracker or planner. What rationale is provided for only targeting the perception system? Could the methodology be extended to find adversarial errors in other modules?

8) One of the goals mentioned is identifying algorithmic weaknesses in the planner. What specific examples of this were observed in the experiments? How could the information gained from this method be used to improve the planner?

9) The paper utilizes simple metrics based on collisions to specify planning failures. What limitations might arise from relying solely on such binary metrics? How could more nuanced driving metrics be incorporated?

10) The approach is demonstrated in simulation across a limited set of scenarios and planning algorithms. What challenges might arise in applying this method directly in the real world across more diverse conditions?
