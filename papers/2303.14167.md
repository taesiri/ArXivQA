# [UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative   Neural Feature Fields](https://arxiv.org/abs/2303.14167)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, it appears the central research question is:

How can we generate photorealistic images of urban scenes with controllable camera pose and scene contents? 

The authors propose an approach called UrbanGIRAFFE to address the challenging task of generating controllable and editable urban scene images from different viewpoints. The key ideas are:

1) Use a coarse 3D panoptic prior (semantic voxel grids + object layouts) to guide a compositional 3D-aware generative model. 

2) Decompose the urban scene into stuff (road, buildings), objects (cars), and sky. Model stuff using a semantic voxel conditioned generator, objects in canonical space, and sky with a far dome.

3) Render a composited feature volume via neural volume rendering. Train with adversarial losses on full image and object patches, plus reconstruction loss on stuff.

4) This enables controllable image synthesis regarding camera viewpoint, object manipulation, and semantic editing of stuff.

In summary, the central hypothesis is that by incorporating a coarse 3D panoptic prior into a compositional generative model, they can achieve photorealistic and controllable image synthesis of complex urban scenes. The experiments aim to validate if their proposed UrbanGIRAFFE approach can effectively enable control over camera viewpoint and scene contents compared to other methods.
