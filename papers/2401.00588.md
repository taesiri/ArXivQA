# [Fairness in Serving Large Language Models](https://arxiv.org/abs/2401.00588)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
This paper studies the problem of fair serving in Large Language Models (LLMs). LLM inference services like ChatGPT need to serve a wide range of client requests concurrently. To ensure all clients are treated fairly, most LLM services impose simple request rate limits per client. However, this can result in under-utilization of resources when spare capacity exists. On the other hand, traditional fair queueing algorithms used in networking cannot be directly applied due to unique characteristics of LLM serving: unpredictable request lengths, variable cost per token, and variable server capacity over time.

Solution:
The paper first defines fairness for LLM serving based on a cost function that captures both number of input tokens (parallelizable) and output tokens (sequential) processed. It then proposes a novel scheduling algorithm called Virtual Token Counter (VTC) to achieve fairness. 

VTC maintains a virtual counter for each client tracking cumulative service received. It prioritizes clients with lower counters when adding new requests, with a counter lift when a client becomes newly backlogged. Counters are updated on-the-fly at token granularity. This allows seamless integration with the continuous batching mechanism commonly used in LLM systems, addresses unpredictable output lengths, and adapts to fluctuating server capacity.

Contributions:
- Identifies unique challenges for fair scheduling in LLM serving and provides a configurable definition of fairness based on a cost function.

- Develops VTC, a simple yet effective token-based fair scheduling algorithm tailored to LLM serving systems.

- Provides rigorous proofs that VTC ensures max-min fairness for backlogged clients within 2x of optimal bound and isolation for non-backlogged clients sending under capacity.

- Evaluates VTC extensively under synthetic and real-world workloads. Results demonstrate superior fairness over baselines while maintaining high resource utilization.


## Summarize the paper in one sentence.

 This paper proposes a fair scheduling algorithm called Virtual Token Counter (VTC) to achieve fairness among clients sharing a large language model serving system, by tracking services received at the token level granularity and prioritizing clients that have received the least service.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel scheduling algorithm called Virtual Token Counter (VTC) to achieve fairness when serving multiple clients from a large language model. Specifically:

1) The paper identifies unique challenges in defining and achieving fairness for LLM serving compared to traditional network/OS fairness problems, including variable cost per token, unpredictable output lengths, and variable server capacity. 

2) It defines fairness for LLM serving based on a configurable cost function that accounts for the number of input and output tokens processed.

3) It proposes the VTC algorithm that tracks services received per client and prioritizes those with the least services, while lifting counters when a client becomes newly backlogged. This allows it to achieve fairness, be work-conserving, and handle variable server capacity and unknown output lengths.

4) It proves theorems that VTC provides strong fairness guarantees, including bounded difference in service between any two backlogged clients and isolation for non-backlogged clients. 

5) It demonstrates through experiments that VTC achieves better fairness than baselines like FCFS and RPM limiting, especially for complex real-world workloads, while maintaining high GPU utilization.

In summary, the main contribution is proposing, analyzing and evaluating the VTC scheduling algorithm to enable fairness for serving multiple LLM clients.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Fairness in serving large language models (LLMs)
- Request rate limits 
- Fair queueing
- Virtual token counter (VTC)
- Continuous batching 
- Token granularity
- Input tokens vs output tokens
- Variable token-rate capacity
- Unknown request lengths
- Work conservation
- Backlogged clients

The paper focuses on how to fairly serve LLMs to multiple clients. It identifies challenges with simply limiting the request rate per client, and proposes a new scheduling algorithm called the virtual token counter (VTC) to achieve fairness at the token level. The paper analyzes how VTC can work with continuous batching architectures for LLMs, handle unpredictable request lengths, accommodate variable server capacity, and provide theoretical fairness guarantees for both backlogged and non-backlogged clients. Concepts like the different costs of input vs output tokens and work conservation are also key to understanding their fairness definition and algorithm design.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I generated about the methods proposed in the paper:

1. The paper proposes using a weighted combination of input and output tokens to measure service fairness. What are the advantages and disadvantages of using FLOPs or some other customized unified representation to measure fairness instead?

2. How does the unknown output length issue for LLMs make adapting classical fair queueing algorithms like SFQ and DRR challenging? What modifications need to be made to those algorithms to work for LLM serving? 

3. The paper claims the bound achieved by VTC is within 2x of the optimal. Can you prove a tighter bound or provide examples where the 2x bound is tight?

4. How does VTC handle scenarios where the server capacity changes dynamically over time? Does it provide any theoretical guarantees in that case?

5. The paper focuses on request-level fairness. How would you extend VTC to handle fairness at a finer granularity like parameter-level or component-level?

6. The lift counter mechanism can cause overcompensation issues in VTC. Can you suggest modifications to address this? Analyze the impact on fairness guarantees.  

7. VTC does not allow preemption of requests. How does supporting preemption affect fairness and what modifications would be needed in VTC?

8. How does VTC handle fairness in multi-tenant settings where clients have reservations/quotas for resources? 

9. The paper uses the max-min fairness notion. How would the guarantees provided by VTC change if using proportional fairness instead?

10. The ablation experiments show larger memory sizes can hurt fairness bounds. How can VTC be adapted to leverage large memory while preserving fairness?
