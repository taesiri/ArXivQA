# [Understanding Intrinsic Robustness Using Label Uncertainty](https://arxiv.org/abs/2107.03250)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is:How can we improve estimates of the intrinsic robustness limit for image classification tasks by incorporating label information? The paper argues that prior work estimating intrinsic robustness through concentration of measure fails to account for label information, which is critical for supervised learning tasks like image classification. To address this limitation, the paper proposes incorporating a "label uncertainty" measure when estimating concentration. The central hypothesis is that accounting for label uncertainty will lead to more accurate estimates of the intrinsic robustness limit compared to just considering input geometry/concentration.In summary, the key research question is how to better estimate the intrinsic robustness limit for image classification by incorporating label information, in particular through a proposed label uncertainty measure. The central hypothesis is that this will improve the accuracy of intrinsic robustness estimates compared to prior work.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Identifying the insufficiency of standard concentration of measure for characterizing the intrinsic robustness limit of a classification problem. The paper argues that standard concentration ignores the label information which is essential for supervised learning tasks.2. Proposing the notion of "label uncertainty" to quantify the average uncertainty in the label assignments for a subset of inputs. 3. Incorporating label uncertainty into the standard concentration measure to obtain a more realistic intrinsic robustness characterization. Experiments on CIFAR-10 and CIFAR-10H datasets demonstrate that error regions of state-of-the-art classifiers tend to have higher label uncertainty.4. Developing an empirical estimator for the proposed label uncertainty constrained concentration function. Compared to prior methods, this approach provides a tighter intrinsic robustness limit for benchmark image classification tasks.5. Demonstrating the possibility of improving classifier robustness by abstaining on inputs with high label uncertainty. Both clean and robust accuracy increase as more uncertain examples are abstained.In summary, the key innovation is incorporating label information into the notion of intrinsic robustness through the proposed label uncertainty measure. This better captures the difficulty of a classification problem and leads to more accurate intrinsic robustness estimates. The results also suggest abstaining on uncertain inputs as a potential defense method.


## How does this paper compare to other research in the same field?

Based on my reading, here is a brief comparison of this paper to other related work in understanding adversarial robustness limits:- This paper argues that standard concentration of measure is insufficient for characterizing intrinsic robustness, because it ignores label information which is essential for classification tasks. Previous works like Gilmer et al. (2018), Mahloujifar et al. (2019), Prescott et al. (2021) all rely on standard concentration to derive intrinsic robustness limits. - To address this limitation, the paper proposes incorporating label uncertainty in measuring concentration. This is a novel perspective not considered in prior works on concentration-based intrinsic robustness limits. The notion of label uncertainty provides a principled way to account for label information.- The paper provides both theoretical analysis and empirical estimation for the proposed label uncertainty constrained concentration measure. The theoretical results connect the new concentration function with intrinsic robustness, extending analogous results from Mahloujifar et al. (2019). The concentration estimation algorithm adapts the method in Mahloujifar et al. (2019) to optimize for high label uncertainty regions.- Experiments on CIFAR-10 demonstrate that the proposed label uncertainty based intrinsic robustness estimates are significantly lower than limits based on standard concentration, and align better with robust accuracies achieved by state-of-the-art classifiers. This supports the claim that label uncertainty helps capture a more realistic intrinsic limit.- Overall, the key novelty is in highlighting the importance of label information for characterizing robustness limits. The proposed incorporation of label uncertainty allows more accurate estimation of intrinsic robustness compared to prior concentration-based approaches. The insights on label uncertainty are applicable more broadly to understanding adversarial vulnerability.In summary, this paper makes an important conceptual contribution on the role of labels in robustness limits. The label uncertainty measure and resulting analysis substantially advance the state-of-the-art in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing methods to estimate label uncertainty when human annotation is not available. The authors' proposed approach relies on having human-annotated soft labels to estimate label uncertainty. They discuss using confident learning methods to try to automatically identify examples with high label uncertainty, but find this does not correlate well with human judgment. Improving methods to automatically identify uncertain inputs without human annotation could help extend their approach.- Incorporating label uncertainty into algorithms for training robust models, rather than just using it for evaluation. The authors show that classification accuracy is related to label uncertainty on tested examples. They suggest abstaining on high uncertainty inputs could improve robustness. Developing training methods that explicitly model uncertainty could potentially improve robustness.- Extending the analysis to more complex datasets and tasks beyond image classification. The empirical validation focuses on CIFAR-10. Applying similar analysis to more complex image datasets, as well as other modalities like audio or text could yield further insights.- Tightening theoretical understanding and guarantees. The paper provides generalization analysis of the concentration estimation method, but there may be opportunities to strengthen the theoretical understanding. - Exploring other potential benefits of modeling label uncertainty. The paper focuses on connections to adversarial robustness, but understanding uncertainty may have other benefits for reliability, interpretability, fairness, etc.In summary, key future directions relate to improving methods for estimating uncertainty without human labels, incorporating uncertainty modeling into algorithms, extending empirical analysis to new domains, strengthening theory, and exploring additional applications of uncertainty information. The introduction of label uncertainty is an interesting direction with many potential avenues for future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper argues that the standard concentration of measure problem is insufficient for characterizing the intrinsic robustness limit of classification problems. The standard concentration function ignores label information, which is essential for supervised learning tasks. The authors introduce a new notion of label uncertainty to capture the average uncertainty in the label assignments for a group of inputs. Experiments on CIFAR-10 show that error regions induced by state-of-the-art classifiers tend to have higher label uncertainty compared to random input subsets. Based on this observation, the authors propose an empirical method to estimate a concentration function that incorporates a constraint on the label uncertainty of candidate input subsets. This yields lower and likely more accurate estimates of the intrinsic robustness limit on benchmark image datasets compared to prior work. The results suggest that the existence of inputs with uncertain labels, rather than just concentration of the input distribution, may explain the fundamental limitations on adversarial robustness.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes incorporating label uncertainty into estimating the intrinsic robustness limit of classifiers. Previous work defined intrinsic robustness in terms of the concentration of measure of the input distribution. However, this ignores the label information which is essential for classification tasks. The authors introduce a notion of label uncertainty to capture the average uncertainty of label assignments for an input region. Experiments on CIFAR-10 datasets show that error regions of state-of-the-art classifiers tend to have higher label uncertainty than typical examples. Based on this observation, the authors adapt existing methods for estimating concentration to account for label uncertainty. They provide theoretical analysis showing this results in a tighter estimate of the intrinsic robustness. Experiments demonstrate their method produces significantly lower robustness limits than prior work, bringing the theoretical limit closer to the empirical robustness of state-of-the-art classifiers. The results suggest that uncertain inputs, rather than just concentration of measure, may explain fundamental limitations on adversarial robustness. The authors also show potential for improving classifier robustness by abstaining on high uncertainty inputs.
