# [Query2doc: Query Expansion with Large Language Models](https://arxiv.org/abs/2303.07678)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it aims to address is:How effective is a simple query expansion method using large language models (LLMs) at improving retrieval performance for both sparse and dense retrieval systems?Specifically, the paper proposes a query expansion approach called "query2doc" that generates pseudo-documents from LLMs via few-shot prompting, and then expands the original query by concatenating it with the generated pseudo-document. The key hypothesis is that by leveraging the knowledge and language understanding capabilities of large pre-trained language models like GPT-3, the pseudo-documents will contain highly relevant information for disambiguating queries and guiding retrievers. This simple technique can boost performance without requiring model fine-tuning or changes to model architectures.The authors evaluate this hypothesis by conducting experiments on in-domain IR datasets like MS-MARCO and TREC DL, as well as out-of-domain datasets. The results consistently show improvements in retrieval metrics across different sparse and dense retriever models when using their proposed query2doc technique.In summary, the central research question is whether a simple query expansion method leveraging LLMs' capabilities can improve state-of-the-art retrieval systems without complex modifications, which the authors successfully demonstrate through empirical evaluations.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a simple yet effective query expansion method called "query2doc" that improves both sparse and dense retrieval systems. The key ideas are:- Generate pseudo-documents by few-shot prompting of large language models (LLMs) based on the query. - Expand the original query by concatenating it with the generated pseudo-documents.- The pseudo-documents from LLMs often contain highly relevant information that helps disambiguate the query and guide the retrievers. - This approach boosts performance of BM25 by 3-15% on ad hoc IR datasets without any model fine-tuning. - State-of-the-art dense retrievers like DPR and E5 also benefit from query2doc in terms of both in-domain and out-of-domain results.- The gains are especially significant for entity-centric, long-tail queries where the lexical match is important.- Scaling up the size of the LLM (e.g. GPT-3) is critical for the quality of pseudo-documents and downstream retrieval performance.In summary, the paper proposes a simple yet effective method to leverage recent advances in LLMs for query expansion in information retrieval, demonstrating consistent improvements across various retrieval models and datasets. The main innovation is generating pseudo-documents from LLMs via few-shot prompting rather than traditional pseudo-relevance feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper introduces a simple yet effective query expansion method called query2doc that generates pseudo-documents by prompting large language models, then expands the original query by concatenating it with the pseudo-documents to improve both sparse and dense retrieval systems without requiring any model fine-tuning.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on query expansion compares to other related work:- The proposed query2doc method is simple and effective, requiring only a few lines of code to implement. Many other query expansion techniques are more complex or require modifications to the underlying retrieval models.- It leverages large language models (LLMs) to generate pseudo-documents for expansion, taking advantage of the knowledge encoded in LLMs. Other methods typically rely on pseudo-relevance feedback or external resources like WordNet. Using LLMs is a relatively new technique.- The method is evaluated extensively on both in-domain and out-of-domain datasets. Many papers focus evaluation on only 1 or 2 standard datasets like MS MARCO. Testing on diverse datasets demonstrates broader applicability.- Both sparse and dense retrieval models are evaluated. Some prior work concentrates only on one retrieval paradigm. Showing gains for both sparse and dense retrievers makes the contribution more comprehensive.- The analysis provides useful insights like the importance of large LLMs, how the gains persist as supervised data increases, and qualitative examples. Many papers lack sufficient analysis and discussion of limitations.- The improvements from query2doc tend to diminish with techniques like distillation. So the gains are smaller when building on SOTA dense retrievers. Some other papers make claims based on weak baseline models.Overall, I think this paper does a thorough evaluation using simple yet effective techniques. The analysis provides insights into why and when the proposed techniques work. The limitations are also clearly discussed. It advances the state-of-the-art while keeping reproduciblity in mind.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Investigating how to better leverage large language models (LLMs) for query expansion. The paper shows promise of using LLMs to generate pseudo-documents, but there is room to explore other techniques like directly expanding the query with relevant terms generated by the LLM. - Improving the efficiency of retrieval with query expansion. The added latency of generating pseudo-documents and searching an expanded index is a limitation. Methods to accelerate the process could make query expansion more practical.- Scaling up supervision with labeled data. The gains from query expansion are consistent even when varying the amount of labeled data. However, combining query expansion with continued scaling up of supervision signals could lead to further improvements.- Evaluating on a broader range of datasets. The paper focuses on standard IR datasets like MS MARCO and TREC DL. Testing on more diverse domains and genres could reveal new challenges.- Mitigating factual errors in LLM generations. Case analysis shows incorrect facts can appear in the pseudo-documents. Developing techniques to fact check LLM outputs could improve reliability.- Comparisons to other state-of-the-art query expansion methods. The paper compares mainly to RM3 but modern learned sparse retrieval models have alternative expansion techniques that could be competitive.- Combining query expansion with other retrieval improvements like document expansion. Jointly optimizing query and document representations could have a compounding effect.Overall, the paper provides a solid motivation for query expansion using large language models and sets the stage for a variety of follow-on research to further improve and extend this technique.
