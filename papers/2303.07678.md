# [Query2doc: Query Expansion with Large Language Models](https://arxiv.org/abs/2303.07678)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it aims to address is:How effective is a simple query expansion method using large language models (LLMs) at improving retrieval performance for both sparse and dense retrieval systems?Specifically, the paper proposes a query expansion approach called "query2doc" that generates pseudo-documents from LLMs via few-shot prompting, and then expands the original query by concatenating it with the generated pseudo-document. The key hypothesis is that by leveraging the knowledge and language understanding capabilities of large pre-trained language models like GPT-3, the pseudo-documents will contain highly relevant information for disambiguating queries and guiding retrievers. This simple technique can boost performance without requiring model fine-tuning or changes to model architectures.The authors evaluate this hypothesis by conducting experiments on in-domain IR datasets like MS-MARCO and TREC DL, as well as out-of-domain datasets. The results consistently show improvements in retrieval metrics across different sparse and dense retriever models when using their proposed query2doc technique.In summary, the central research question is whether a simple query expansion method leveraging LLMs' capabilities can improve state-of-the-art retrieval systems without complex modifications, which the authors successfully demonstrate through empirical evaluations.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a simple yet effective query expansion method called "query2doc" that improves both sparse and dense retrieval systems. The key ideas are:- Generate pseudo-documents by few-shot prompting of large language models (LLMs) based on the query. - Expand the original query by concatenating it with the generated pseudo-documents.- The pseudo-documents from LLMs often contain highly relevant information that helps disambiguate the query and guide the retrievers. - This approach boosts performance of BM25 by 3-15% on ad hoc IR datasets without any model fine-tuning. - State-of-the-art dense retrievers like DPR and E5 also benefit from query2doc in terms of both in-domain and out-of-domain results.- The gains are especially significant for entity-centric, long-tail queries where the lexical match is important.- Scaling up the size of the LLM (e.g. GPT-3) is critical for the quality of pseudo-documents and downstream retrieval performance.In summary, the paper proposes a simple yet effective method to leverage recent advances in LLMs for query expansion in information retrieval, demonstrating consistent improvements across various retrieval models and datasets. The main innovation is generating pseudo-documents from LLMs via few-shot prompting rather than traditional pseudo-relevance feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper introduces a simple yet effective query expansion method called query2doc that generates pseudo-documents by prompting large language models, then expands the original query by concatenating it with the pseudo-documents to improve both sparse and dense retrieval systems without requiring any model fine-tuning.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on query expansion compares to other related work:- The proposed query2doc method is simple and effective, requiring only a few lines of code to implement. Many other query expansion techniques are more complex or require modifications to the underlying retrieval models.- It leverages large language models (LLMs) to generate pseudo-documents for expansion, taking advantage of the knowledge encoded in LLMs. Other methods typically rely on pseudo-relevance feedback or external resources like WordNet. Using LLMs is a relatively new technique.- The method is evaluated extensively on both in-domain and out-of-domain datasets. Many papers focus evaluation on only 1 or 2 standard datasets like MS MARCO. Testing on diverse datasets demonstrates broader applicability.- Both sparse and dense retrieval models are evaluated. Some prior work concentrates only on one retrieval paradigm. Showing gains for both sparse and dense retrievers makes the contribution more comprehensive.- The analysis provides useful insights like the importance of large LLMs, how the gains persist as supervised data increases, and qualitative examples. Many papers lack sufficient analysis and discussion of limitations.- The improvements from query2doc tend to diminish with techniques like distillation. So the gains are smaller when building on SOTA dense retrievers. Some other papers make claims based on weak baseline models.Overall, I think this paper does a thorough evaluation using simple yet effective techniques. The analysis provides insights into why and when the proposed techniques work. The limitations are also clearly discussed. It advances the state-of-the-art while keeping reproduciblity in mind.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Investigating how to better leverage large language models (LLMs) for query expansion. The paper shows promise of using LLMs to generate pseudo-documents, but there is room to explore other techniques like directly expanding the query with relevant terms generated by the LLM. - Improving the efficiency of retrieval with query expansion. The added latency of generating pseudo-documents and searching an expanded index is a limitation. Methods to accelerate the process could make query expansion more practical.- Scaling up supervision with labeled data. The gains from query expansion are consistent even when varying the amount of labeled data. However, combining query expansion with continued scaling up of supervision signals could lead to further improvements.- Evaluating on a broader range of datasets. The paper focuses on standard IR datasets like MS MARCO and TREC DL. Testing on more diverse domains and genres could reveal new challenges.- Mitigating factual errors in LLM generations. Case analysis shows incorrect facts can appear in the pseudo-documents. Developing techniques to fact check LLM outputs could improve reliability.- Comparisons to other state-of-the-art query expansion methods. The paper compares mainly to RM3 but modern learned sparse retrieval models have alternative expansion techniques that could be competitive.- Combining query expansion with other retrieval improvements like document expansion. Jointly optimizing query and document representations could have a compounding effect.Overall, the paper provides a solid motivation for query expansion using large language models and sets the stage for a variety of follow-on research to further improve and extend this technique.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper introduces a simple yet effective query expansion approach called query2doc to improve both sparse and dense retrieval systems. The method first generates pseudo-documents by few-shot prompting of large language models (LLMs) with labeled query-passage pairs as examples. The pseudo-documents containing relevant information are then concatenated with the original query to form an expanded query for retrieval. Experiments on MS-MARCO, TREC DL, and out-of-domain datasets show consistent improvements over strong baselines with BM25 and state-of-the-art dense retrievers. The gains are especially large on entity-centric queries that benefit from exact lexical matches. Analysis reveals the importance of large model scale as smaller LMs provide only marginal improvements. The approach is simple to implement without any model fine-tuning but has limitations in latency due to LLM prompting and longer query indexing. Overall, query2doc demonstrates the effectiveness of distilling knowledge from LLMs for query expansion to improve retrieval performance.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper introduces a simple yet effective query expansion method called query2doc to improve both sparse and dense retrieval systems. The key idea is to generate pseudo-documents by few-shot prompting of large language models (LLMs) on the query, and then expand the query with the generated text. LLMs like GPT-3 contain a lot of world knowledge from pre-training, so the pseudo-documents often provide useful context to disambiguate the query and guide retrieval. The method is evaluated on passage ranking datasets like MS-MARCO and TREC DL. For sparse retrieval with BM25, query2doc substantially boosts performance by 3-15% without any model fine-tuning. The gains are especially large on TREC DL queries. For dense retrievers like DPR and SimLM, query2doc also improves results, although the benefits diminish when distilling from a strong cross-encoder re-ranker. Analysis shows the importance of large model scale - bigger LLMs like GPT-3 generate better pseudo-documents. The simplicity of query2doc makes it widely applicable, while limitations include slower inference and indexing. Overall, the paper demonstrates the effectiveness of prompting LLMs for query expansion across various state-of-the-art retrieval systems.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces a query expansion approach called query2doc that utilizes large language models (LLMs) to generate pseudo-documents from queries. The method first prompts an LLM with a few example query-passage pairs to provide context. Then the query is fed to the LLM to generate a pseudo-document containing relevant information. This pseudo-document is concatenated with the original query to form an expanded query. For sparse retrieval, the original query is repeated multiple times before concatenating to boost term weights. For dense retrieval, the query and pseudo-document are concatenated with a separator token. The expanded query is then used for retrieval, allowing the pseudo-document to provide additional context and reduce the lexical gap between the query and corpus documents. This simple technique is shown to improve both sparse and dense retrieval systems on several standard benchmarks without requiring model fine-tuning or training data.


## What problem or question is the paper addressing?

This paper introduces a query expansion approach called "query2doc" that aims to improve both sparse and dense retrieval systems. The key ideas are:- Generate pseudo-documents by few-shot prompting large language models (LLMs) with a query. LLMs are good at knowledge memorization and can provide relevant information to aid query disambiguation. - Expand the original query by concatenating it with the pseudo-document to form a new query. This helps bridge the lexical gap between the query and documents.- The proposed approach is simple, orthogonal to progress in LLMs and IR models, and consistently improves various retrievers like BM25, DPR, SimLM, etc. on in-domain and out-of-domain datasets.So in summary, the paper addresses the problem of improving retrieval performance, especially for ambiguous or incomplete queries, by leveraging the knowledge and language capabilities of large pre-trained language models. The query expansion method query2doc generates helpful context via LLMs to guide both sparse and dense retrievers.
