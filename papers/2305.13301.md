# Training Diffusion Models with Reinforcement Learning

## What is the main contribution of this paper?

The main contribution of this paper is proposing and evaluating reinforcement learning methods for directly optimizing diffusion models on downstream objectives. Specifically:- The paper frames denoising in diffusion models as a multi-step decision process, enabling the use of policy gradient algorithms. This results in a new class of algorithms called Denoising Diffusion Policy Optimization (DDPO) that can optimize diffusion models using only a black-box reward function.- DDPO is applied to finetune large pretrained text-to-image diffusion models on objectives that are difficult to specify via prompting, such as image compressibility, and objectives derived from human feedback, like aesthetic quality.- The paper shows DDPO can improve prompt-image alignment in diffusion models by using feedback from a vision-language model, without needing additional data collection or human annotation. This allows adapting models to reward functions that would normally require extra labeling effort.- The paper studies the generalization and overoptimization effects resulting from the RL finetuning procedures. It provides evidence that fine-tuning can generalize to unseen prompts, but also highlights issues like reward exploitation that need to be addressed.In summary, the main contribution is developing and demonstrating RL algorithms that can directly optimize diffusion models for a variety of user-specified goals, beyond just matching a training data distribution. This helps diffusion models become more useful for real-world downstream applications.
