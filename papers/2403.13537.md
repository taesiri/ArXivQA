# [What explains the success of cross-modal fine-tuning with ORCA?](https://arxiv.org/abs/2403.13537)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
ORCA is a recent cross-modal fine-tuning technique that achieves high performance by training an embedder to map data into the embedding space of a pre-trained model before fine-tuning. However, it is unclear which components contribute to ORCA's success. 

Proposed Solution:
The authors perform several ablations on ORCA to analyze the individual contributions of its components on 2D and 1D tasks:

1. They find the choice of proxy dataset used for embedder training does not affect performance on 2D tasks and matters little for 1D. 

2. Contrary to claims in the original paper, more embedder training can hurt downstream performance, and is entirely unnecessary for some tasks. 

3. Freezing experiments show fine-tuning the pre-trained model itself is critical to performance, while the embedder need not be fine-tuned once trained.

4. For a simple 1D task, pre-training is unnecessary, highlighting the need for no-pretraining baselines.


Main Contributions:

- Better understanding of how the individual components of ORCA affect its cross-modal transfer capabilities. 

- Embedder training is less important than claimed, fine-tuning the model itself is critical.

- Highlights the need for careful ablations and baselines when evaluating cross-modal techniques to identify why they work.

In summary, the paper provides a more nuanced perspective on ORCAâ€™s success through ablation studies, emphasizing that model fine-tuning rather than embedder training is key for many tasks. The results demonstrate the importance of strong baselines and ablations for reliable evaluation.
