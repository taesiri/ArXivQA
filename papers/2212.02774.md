# [Adaptive Testing of Computer Vision Models](https://arxiv.org/abs/2212.02774)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, it does not appear to have a clearly stated central research question or hypothesis. However, the overall focus seems to be on presenting a new methodology and tool called AdaVision for adaptive testing of computer vision models. The key ideas presented include:

- AdaVision is a human-in-the-loop process that helps users identify coherent failure modes (subsets of data that a model fails on systematically) in vision models. 

- It allows users to propose failure modes using natural language descriptions. These are used to retrieve relevant images from a large unlabeled dataset using CLIP embeddings.

- Users label a small subset of retrieved images, and based on this feedback, AdaVision adapts to retrieve more images similar to failures in successive rounds. This allows it to hill-climb towards high-error regions and refine the failure mode definition.

- It also uses GPT-3 to suggest new failure mode descriptions for users to explore, conditioned on past difficult groups. 

- Through user studies, the authors demonstrate AdaVision's ability to help users find major bugs in state-of-the-art vision models, with higher failure rates compared to automatic error clustering methods.

- Finetuning the models on failures found through AdaVision can fix the discovered bugs without hurting in-distribution accuracy or degrading performance on unrelated test sets.

So in summary, there is no single clear hypothesis being evaluated, but the overall goal is to demonstrate the usefulness of AdaVision as an adaptive human-in-the-loop testing methodology for vision models. The experiments aim to support its benefits over non-adaptive baselines and automatic error clustering techniques.
