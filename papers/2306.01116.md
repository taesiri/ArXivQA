# [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora   with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:Can adequately filtered and deduplicated web data alone result in language models that match or exceed the performance of models trained on curated datasets?The key hypotheses appear to be:1) Properly filtering and deduplicating web data can significantly improve its quality for training large language models.2) Models trained on sufficiently filtered and deduplicated web data alone can match or exceed the performance of models trained on manually curated "high quality" datasets. 3) Contrary to common belief, web data alone can be sufficient to train state-of-the-art large language models, without needing curated datasets.To test these hypotheses, the authors introduce a new 5 trillion token English dataset called RefinedWeb, created by applying strict filtering and deduplication to CommonCrawl data. They then train large language models on RefinedWeb and compare their performance to models trained on curated datasets like The Pile. Their key findings are:- Models trained on RefinedWeb alone outperform models trained on curated datasets like The Pile, challenging beliefs about the need for curation.- Properly filtered web data alone can lead to models matching state-of-the-art performance from GPT-3 and other models trained partly on curated data.So in summary, this paper questions the conventional wisdom that curated data is essential for training top-performing large language models, by demonstrating high quality models can be trained on filtered web data alone. The central hypothesis is that with sufficient filtering and deduplication, web data can exceed curated data.
