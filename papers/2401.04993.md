# [AdaFed: Fair Federated Learning via Adaptive Common Descent Direction](https://arxiv.org/abs/2401.04993)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Federated learning (FL) trains machine learning models over decentralized data located on devices like mobile phones. However, FL often suffers from unfairness, where the final global model may perform well on average but very poorly on certain outlier devices with distinct data distributions. This stems from the inherent data heterogeneity among devices as well as algorithmic biases during model aggregation.

Solution - AdaFed:
This paper proposes AdaFed to address the unfairness problem in FL. The key idea is to find a common descent direction to update the global model such that:
(1) It decreases the loss functions of all selected devices in each round (necessary for convergence). 
(2) It reduces the loss functions of worst-performing devices faster than better-performing ones (for fairness). 

Specifically, AdaFed first orthogonalizes the device gradients, then scales them by the loss values raised to the power gamma (hyperparameter). This scaling assigns higher weights to gradients of devices with larger losses. Next, AdaFed finds the minimum-norm vector in the convex hull of the scaled orthogonal gradients, proves it is a common descent direction, and uses it to update the global model.

Contributions:
- Proposes AdaFed, which adapts the global update direction based on device losses to promote fairness.
- Provides theoretical analysis on the descent and convergence properties. 
- Evaluates extensively on 7 datasets under different FL setups and shows superiority over state-of-the-art fair FL algorithms on fairness metrics with negligible loss in accuracy.
- Achieves more uniform client performance while enhancing robustness against worst-case device failures.
- Establishes flexible tradeoff between accuracy and fairness via the gamma hyperparameter.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes AdaFed, a federated learning method that adaptively finds a common descent direction to update the global model such that all clients' loss functions decrease, especially for those clients with larger loss values, to promote fairness across heterogeneous users.


## What is the main contribution of this paper?

 This paper proposes AdaFed, a new federated learning algorithm to promote fairness among clients. The key contributions are:

1) AdaFed adaptively finds a common descent direction along which the server updates the global model. This direction satisfies two key properties: (i) it is descent for all client loss functions (necessary for decrease); (ii) loss functions with larger values decrease faster along it (for fairness).

2) A closed-form formula is provided to calculate this direction using scaled and orthogonalized gradients. This avoids complex iterative algorithms.

3) Convergence guarantees are provided for AdaFed under different federated learning setups. 

4) Thorough experiments over 7 datasets demonstrate AdaFed achieves higher fairness than state-of-the-art methods without sacrificing average accuracy.

In summary, the main contribution is the proposal of AdaFed, an adaptive federated learning algorithm with strong theoretical properties that promotes fairness very effectively in practice. The key ideas include finding a common descent direction satisfying nice properties, a closed-form formula for efficiency, and convergence analysis.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Federated learning (FL): A distributed machine learning approach that enables model training on decentralized data located on user devices without requiring raw data sharing.

- Fairness: Uniformity of model performance across clients/devices in federated learning. The paper aims to achieve fairness by finding a common descent direction that reduces loss faster for clients with larger loss values.  

- Pareto optimality: The use of concepts from multi-objective optimization and Pareto optimality to enforce fairness constraints in the federated learning optimization problem.

- Common descent direction: The paper proposes AdaFed algorithm to find an adaptive common descent direction along which all client losses decrease, with larger losses decreasing faster.

- Convergence guarantees: The paper provides convergence proofs and results for AdaFed under different federated learning setups like local SGD, multiple local epochs, etc.

- Experiments: Thorough comparative experiments are conducted on vision and text datasets to demonstrate AdaFed's ability to improve fairness across clients while maintaining accuracy.

Some other notable terms are orthogonalization of gradients, directional derivatives, smoothed/Lipschitz loss functions. But the key focus is on using Pareto optimal directions for fair federated learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the AdaFed method proposed in this paper:

1. How does AdaFed adaptively tune the common descent direction to satisfy both conditions (I) and (II) simultaneously? Explain the methodology behind satisfying each condition.

2. Explain the orthogonalization process in AdaFed and why it is needed before finding the optimal lambda weights. What assumption needs to hold for this orthogonalization to be feasible?

3. Derive and explain the closed-form formula obtained by AdaFed to calculate the optimal lambda weights for the scaled orthogonal gradients. Why can't a similar closed-form solution be obtained without orthogonalization?

4. Prove theorem 1 that shows the negative of the common direction found by AdaFed satisfies conditions (I) and (II). Walk through each step of the proof in detail.  

5. Compare and contrast the convergence guarantees provided by AdaFed under 3 different scenarios - (i) e=1 & local SGD, (ii) e>1 & local GD, and (iii) e=1 & local GD. What are the key assumptions and results?

6. How does the complexity of finding the common descent direction in AdaFed compare with that in FedMGDA+? Why is AdaFed's solution more efficient?

7. Explain the effect of the hyperparameter gamma on the performance of AdaFed. How should gamma be tuned to obtain the desired level of fairness?

8. How can AdaFed be integrated with label noise correction methods like FedCorr? Why is this useful and what results were obtained on the Clothing1M dataset?

9. What modifications would be needed to apply AdaFed to non-IID data distributions across clients? Would the convergence guarantees still hold?

10. Can concepts from AdaFed be integrated into personalization-based methods like FedProx or Ditto? If so, how would you go about doing this technically?
