# [Graph neural network outputs are almost surely asymptotically constant](https://arxiv.org/abs/2403.03880)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper studies the expressive power and convergence properties of graph neural networks (GNNs) when applied to random graph models such as Erdős-Rényi graphs. Specifically, it considers GNN architectures used for probabilistic node or graph classification and analyzes how the predicted class probabilities evolve as the GNNs are applied to progressively larger random graphs. 

Proposed Solution:
The paper defines an aggregate term language that can express a wide variety of GNN architectures involving different aggregation and update functions. This includes MPNNs with mean aggregation, graph attention networks, and graph transformer models. Using this language, the paper shows that for several random graph models, the outputs of probabilistic GNN classifiers expressed in this language will converge almost surely to a constant function. That is, the predicted probabilities become independent of the actual graph structure.

This convergence result is shown:
(1) For various Erdős-Rényi graph distributions, including dense graphs and sparser graphs with different edge probability scalings.
(2) For the stochastic block model.
(3) For any classifier architecture that can be represented using the defined term language involving weighted means and Lipschitz functions. This includes most common MPNN and transformer architectures.

To prove the convergence results, the paper introduces a notion of "almost sure optimization" showing that aggregate terms can be simplified to Lipschitz functions. For sparse graphs, a more intricate analysis using local neighborhood isomorphism types is employed.

Main Contributions:
- Defines a broad aggregate term language that captures many state-of-the-art GNN architectures.
- Shows almost sure convergence of probabilistic GNN classifiers to constant functions for various random graph models, bounding their expressive power.
- Introduces proof techniques involving "almost sure optimization" of aggregate terms over random graphs.
- Empirically validates the convergence phenomena, showing it manifests quickly even for modest graph sizes across different GNN models.

The key insight is that for random graph distributions, as graph size increases, GNNs have limited discrimination ability and converge to predicting constant probabilistic outputs regardless of the actual graph structure. This limitation is quite general across many standard GNN variants.
