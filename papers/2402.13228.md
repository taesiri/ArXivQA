# [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive](https://arxiv.org/abs/2402.13228)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Direct Preference Optimisation (DPO) is an effective technique for improving large language models (LLMs) using human preferences. 
- However, the paper shows theoretically and empirically that standard DPO has a surprising failure mode - it can actually reduce the model's likelihood of preferred examples, especially when the edit distance between pair completions is small.

Proposed Solution - DPO-Positive (DPOP):
- DPOP adds a new term to the DPO loss function that penalizes reducing the probability of the positive examples below what it was in the reference model. 
- This fixes the identified failure mode and often leads to better performance than standard DPO, even on datasets with high edit distances.

Key Contributions:
- Identifies and explains a catastrophic failure case of standard DPO, backed by theory and experiments.
- Introduces DPOP which avoids this failure mode and outperforms DPO on various datasets/tasks.
- Creates new preference-based datasets for math, reasoning and commonsense skills.
- Uses DPOP & new datasets to develop Smaug LLM models up to 72B parameters, with Smaug-72B being the first open-source LLM to exceed 80% accuracy on the HF LLM Leaderboard.

In summary, the key insight is that standard DPO can sometimes degrade model performance due to its focus on relative probabilities. DPOP fixes this by also rewarding maintaining high absolute probabilities of preferred examples. This leads to better performance, allowing the release of state-of-the-art open-source LLMs.
