# [Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov](https://arxiv.org/abs/2401.11325)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning (RL) algorithms typically assume the reward function is Markovian (only depends on current state and action), but this assumption does not always hold in real environments. 
- Non-Markovian reward functions depend on histories of states and actions, making it hard to learn optimal behaviors.
- Prior works assume access to high-level propositional symbols and their mapping to states, which may not be available.

Proposed Solution:
- Propose learning "hidden triggers" from trajectories that capture underlying non-Markovian dynamics. 
- Model the problem as an Abstract Markov Decision Process (AMDP) which is the cross product of the underlying non-Markov process and a hypothesized Reward Machine (RM).
- Formulate an Integer Linear Program (ILP) to map trajectories onto an AMDP that resolves reward conflicts and is consistent with observed trajectories.
- The ILP identifies "hidden triggers" in state/action/reward histories that differentiate rewards, learning the RM transition and reward functions.
- Apply Q-learning on AMDP as a proxy to maximize rewards for the non-Markov process. Expand RM when new conflicts are observed.

Main Contributions:
- Novel algorithm to learn Reward Machines without needing high-level propositional symbols and their mapping. Infers these automatically from data.
- Proof that AMDP expectations are consistent with underlying non-Markov process.
- Empirically demonstrate approach on OfficeWorld and new BreakfastWorld domains, learning multiple interdependent reward signals.
- Show learning over state/action/reward histories is more sample efficient than just state/action histories.
- Validate usefulness of learned "hidden triggers" by using them as features in DQN which matches or outperforms recurrent baselines.


## Summarize the paper in one sentence.

 This paper proposes a framework for mapping non-Markov reward functions to equivalent Markov ones by learning a Reward Machine, a specialized reward automaton, directly from data without requiring a set of high-level propositional symbols.


## What is the main contribution of this paper?

 According to the paper, the main contributions are three-fold:

1. They provide a novel algorithm for learning Reward Machines for non-Markov reward functions without requiring knowledge of the high-level propositions (P) or labeling function (L) that map states to propositions. Instead, they identify "hidden triggers" from trajectories to learn the rules of L.

2. They prove that the expectations in the Abstract Markov Decision Process (AMDP) learned by their method are consistent with the underlying non-Markov Reward Decision Process (NMRDP). 

3. They demonstrate their approach's ability to learn multiple interdependent reward signals in a single Reward Machine, whereas prior DFA-based methods would require multiple separate automata. They also show how learning over the space (S x A x R)* rather than (S x A)* can dramatically speed up the learning.

So in summary, the main contributions are: 1) a novel RM learning algorithm without needing P and L, 2) a proof of consistency between the learned AMDP and NMRDP, and 3) demonstrations of learning complex interdependent rewards in a single concise RM.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Reward Machines (RMs)
- Non-Markov Reward Decision Processes (NMRDPs) 
- Abstract Markov Decision Processes (AMDPs)
- Hidden triggers
- Integer Linear Programming (ILP)
- Mapping trajectories from an NMRDP to an AMDP
- Learning RMs without access to high-level propositional symbols or mapping to those symbols
- Proving expectations in an AMDP are consistent with the underlying NMRDP
- Learning multiple interdependent reward signals in a single RM automaton
- Comparing learning over histories with state/actions vs state/actions/rewards
- Applying AMDP Q-Learning to black box RMs
- Extending DQNs with learned hidden triggers as features

The main goals and contributions seem to be:
1) Providing an algorithm to learn RMs without needing the high-level symbols or mappings to those symbols that are normally required
2) Proving the learned AMDP representations are valid proxies for the underlying non-Markov processes
3) Showing the ability to learn multiple interacting reward signals in a single RM
4) Demonstrating the benefits of using the full state/action/reward histories rather than just state/actions during learning

Does this summary seem accurate? Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes learning "hidden triggers" from trajectory data to map non-Markovian rewards to Markovian rewards. However, it does not detail the specific algorithm used to identify these triggers from data. What methods could be used to efficiently identify such triggers?

2. The Abstract Markov Decision Process (AMDP) is defined to have consistent expectations with the underlying non-Markov process. Does this theoretical guarantee also hold empirically? How tightly matched are the returns under the learned mappings?

3. The paper claims rewards can serve as better contextualizers than states when reward spaces are less noisy. This is an interesting claim. Does the paper provide any empirical evidence to support it? If not, what experiments could be conducted?  

4. The paper uses an Integer Linear Program (ILP) to infer the mapping between non-Markovian and Markovian processes. What are the limitations of ILPs for this task in terms of scalability and efficiency? Could other optimization methods work better?

5. The paper relies on hand-designed features ("hidden triggers") to map between process spaces. Could these mappings be learned automatically using representation learning techniques? How would end-to-end learned mappings compare?

6. The DQN+triggers agent clearly outperforms RNN-based approaches in the Officeworld experiments. Is this purely a matter of sample efficiency or do the learned triggers encode fundamentally more useful representations? How could this be tested?

7. How does the number of identified "hidden triggers" scale with the complexity of non-Markovian rewards? At what point does the ILP formulation struggle to find compact representations?

8. The paper assumes access to perfect reward signals during training. How robust is the overall approach to noise in observed rewards? What modifications could improve robustness?

9. The Officeworld experiments only test on a small set of hand-designed reward functions. How well does the approach work on more complex, real-world inspired tasks? Are there any broader domains it has been tested in?

10. The paper claims decomposability of rewards into separate automata is a key advantage over DFAs. However, it does not empirically demonstrate this advantage. What experiments could clearly highlight cases where DFAs struggle but RMs decompose well?
