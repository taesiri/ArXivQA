# [Neural Story Planning](https://arxiv.org/abs/2212.08718)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents a novel approach to story plot generation that combines causal planning techniques from AI with large neural language models. Specifically, the authors propose a system that works backwards from a given story ending, using prompts to query the GPT-J language model to recursively determine preceding events that would logically enable the ending event. Their "neural story planner" infers needed preconditions for each event, as well as plausible prior events that would satisfy those preconditions, thereby chaining events backwards while preserving causal coherence. Compared to prior neural story generators which struggle with coherence over longer stories, this combines the strength of causal reasoning from classical AI planning with the open-ended generative capacity of neural models. Evaluation using question-answering as an indicator of plot coherence shows their system generates more comprehensible and causally connected event sequences than a variety of baseline story generation methods. Key innovations include prompts tailored to inference of event preconditions grounded in commonsense reasoning, as well as adaptations of partial-order causal link planning techniques to operate on the text events and conditions inferred by the neural model. By bringing together classical search-based planning and modern neural text generation, the paper provides a promising new architecture for controllable and explainable narrative plot generation.


## Summarize the paper in one sentence.

 This paper proposes a novel approach to causally coherent plot generation by unifying neural language models with partial order causal link planning, using a large pre-trained language model to infer events and their preconditions without pre-specifying actions, characters, or world objects and locations.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

This paper proposes a novel approach to story plot generation that combines neural language models with causal link planning techniques from classical AI planning. Specifically, the system works backwards from a specified story ending, using GPT-J to recursively generate preconditions for each event that need to be satisfied, and then generating new events that satisfy those preconditions. This creates a graph of events and causal links similar to a partial order causal link (POCL) planner, but operates in an open world without needing predefined actions or symbols. The system outperforms neural baselines in generating coherent plots as measured by the ability of GPT-3 to correctly answer enablement questions about causal relations between events. The approach demonstrates how large language models can effectively serve as commonsense reasoning engines to support planning techniques for coherent story generation, without relying solely on uncontrolled neural text continuation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from this paper:

The paper proposes a technique to generate causally coherent story plotlines by unifying neural language models with partial order causal link planning methods from classical AI planning.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is:

How can we unify the strengths of symbolic planning and neural text generation to achieve causally coherent, ending-guided story plotlines?

Specifically, the paper proposes a technique that uses a partial-order causal link planner together with a large pre-trained language model (GPT-J-6B) to generate coherent story plots. The key ideas are:

1) Use the language model to infer events and their preconditions instead of relying on hand-crafted action schemas and world state symbols as in traditional planning. This allows operating in an open world without pre-defining all actions, characters, etc.

2) Plan stories by backward chaining from a given ending, recursively querying the language model to generate preconditions and events that satisfy those preconditions. This results in a graph of events connected by causal links. 

3) Flatten the graph into a coherent plotline guided by the specified ending.

The central hypothesis is that unifying causal planning with large neural language models in this way can produce coherent and ending-guided story plots with more diversity compared to prior symbolic planning or neural text generation techniques alone.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel approach to story plot generation that unifies causal planning with neural language models. Specifically:

- They propose a story planner based on partial-order causal link planning that generates story plotlines by recursively expanding events and their preconditions in a backward chaining fashion. 

- Instead of using handcrafted action schemas, their planner uses a large pre-trained language model (GPT-J-6B) to infer events and their preconditions, allowing it to operate in an open world setting without pre-defining actions, characters, etc.

- They conduct automatic evaluation using GPT-3's ability to answer enablement questions about relations between events in generated stories. Results indicate their proposed method produces more coherent plotlines than several strong baselines.

In summary, the key contribution is demonstrating how causal planning techniques can be unified with the knowledge and text generation capabilities of large language models to generate coherent and ending-guided story plots in an open world setting.


## How does this paper compare to other research in the same field?

 This paper presents a novel approach to story generation that unifies causal planning with neural language models. Here are some key ways it compares to other related research:

- It uses a partial-order causal link planning algorithm similar to previous story planning systems like UDCPOP and CPOCL, but instead of relying on hand-authored action schemas, it uses a large language model (GPT-J-6B) to infer events and preconditions. This allows it to generate more diverse stories without limiting the story world.

- It conducts backward chaining search starting from a given story ending, recursively generating preconditions and the events that satisfy them. This makes it more controllable than typical neural story generators that sample events from a learned distribution without considering logical coherence.

- It incorporates commonsense knowledge in the form of preconditions that capture different types of causal relations between events based on theories from narrative psychology. This helps ensure the causal connectivity between events compared to systems that don't explicitly reason about event relations.

- It outperforms various neural baselines like C2PO, comGen, and fine-tuned GPT-3 in generating causally coherent plots according to an evaluation of answering enablement questions. This demonstrates the advantage of its structured planning approach over less constrained neural text generation.

In summary, this work innovatively combines the coherence guarantees from planning with the versatility of neural language models for open-world story generation. The evaluation shows the promise of this hybrid approach compared to other state-of-the-art systems.


## What future research directions do the authors suggest?

 The authors suggest two main future research directions:

1. Extending their approach to more modern planning algorithms beyond partial-order causal link planning:

"Extending our proposed approach to more modern planning algorithms is left for future extensions of this work."

2. Using the concept of generated plotlines as "skeletons" that can later be fleshed out into full natural language stories with additional details and dialogue: 

"One can consider generated plotlines as the ``skeletons''~\cite{simon-spark-2022} for fully fleshed out natural language stories that can include details and dialogue generated later. This work presents a step forward toward the open research challenge of generating stories in open-world that are also guaranteed to be coherent and comprehensible."

So in summary, they suggest extending their planning algorithm approach to other more modern algorithms, and using the coherent plotlines as a skeleton that can be built upon to generate full natural language stories. The overall goal is moving toward coherent and comprehensible open-world story generation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Automated plot generation - The challenge of generating a coherent sequence of events that forms the plot of a story.

- Symbolic planning - Using AI planning techniques like causal links between events to ensure logical coherence of generated plotlines. Rely on handcrafted schemas which limits diversity.

- Neural language models - Can generate diverse stories but may lack coherence or ability to target specific endings.

- Partial-order causal link (POCL) planning - A symbolic planning technique that represents plans as graphs with events, preconditions, causal links and ordering constraints. 

- Commonsense reasoning - Using knowledge about everyday events, items, and physical world to reason about preconditions and effects of story events.

- Ends-means planning - Working backwards from a given story ending by recursively determining preconditions and events that satisfy them.

- GPT-J-6B - A large pre-trained language model used to infer events and preconditions for the neural story planner through prompting.

- Coherence evaluation - Measuring the causal coherence of generated plots by the ability of a model to answer questions about whether one event enables another event in the story.

Some other ideas are interpretability from causal links, combining strengths of planning and neural text generation, open world assumption instead of handcrafted symbols, avoiding repetition through event re-use.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes unifying causal planning with neural language models. What are the key benefits and limitations of using a neural language model for inferring events and preconditions instead of relying solely on symbolic planning methods?

2. The paper uses ends-means chaining in a backward chaining approach, starting from a given story ending rather than from an initial state. What are the advantages and disadvantages of using this backward chaining approach compared to a forward chaining approach?  

3. The paper breaks down event preconditions into six different classes (item need, location, item state, how, interactions with others, reason). What was the rationale and inspiration behind choosing these specific classes of preconditions? How effective were they in driving coherent story expansion?

4. The paper relies heavily on hand-crafted prompts for precondition and event generation with the language model. To what extent could these prompts be learned or optimized automatically instead? What challenges would this entail? 

5. How does the proposed method of using causal links between inferred events compare to other approaches that use commonsense reasoning to establish relations between events, such as in C2PO? What are the tradeoffs?

6. The evaluation uses GPT-3's ability to answer enablement questions as a proxy for measuring the coherence of generated plotlines. What are other ways the coherence could be evaluated either automatically or with human judges? What are the pros and cons?

7. The paper does not model negative effects of events that could negate preconditions. To what extent could this limit the logical soundness of the resulting plots compared to classical planning methods? When would this matter?

8. How does the proposed approach relate to other hierarchical story generation methods? What unique advantages does explicitly modeling preconditions and effects provide over sketch-then-expand approaches? 

9. The paper argues the approach inherits some interpretability due to the causal link representation. What further analysis could be done to evaluate or improve the interpretability of this method?

10. What future directions could this work enable in terms of unifying neural and symbolic approaches for story generation? What are some concrete next steps for this line of hybrid story generation research?
