# [Policy Adaptation from Foundation Model Feedback](https://arxiv.org/abs/2212.07398)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to adapt a trained language conditioned policy to new tasks and environments, including unseen objects, compositional instructions, and sim-to-real transfer. 

The key hypothesis is that leveraging pre-trained vision-language models to provide feedback can help automatically adapt the policy by re-labeling demonstrations and providing new training data. Specifically, the method proposed in the paper, called Policy Adaptation from Foundation model Feedback (PAFF), hypothesizes that:

- Although a trained policy may fail on new tasks/environments, pre-trained vision-language models can still recognize visual concepts and provide useful feedback.

- By letting the policy "play" and generate demonstrations on the new task, then relabeling those demos using the foundation model, new training data can be obtained to adapt the policy.

- This play-relabel process with a pre-trained vision-language model as teacher can enable automatic policy adaptation without human intervention.

The paper then conducts experiments on compositional generalization, out-of-distribution environments, and sim-to-real transfer to evaluate whether the proposed PAFF method improves adaptation compared to baselines without feedback. The results generally validate the hypothesis, showing benefits across the different settings.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Policy Adaptation from Foundation Model Feedback (PAFF) for adapting trained policies to new tasks and environments. The key ideas are:

- Let the trained policy "play" in the new task by executing randomly generated instructions and record the demonstrations, even though they may be wrong. 

- Use a fine-tuned vision-language foundation model (e.g. CLIP) to relabel the demonstrations by retrieving the correct instruction text given the visual observations.

- Fine-tune the policy on the relabeled demonstrations to adapt it to the new task/environment. 

The main benefits are:

- The adaptation process is fully automatic, without needing human labeling effort.

- It allows adapting policies to compositional tasks, unseen objects/environments, and from simulation to real world. 

- Experiments across different tasks show large improvements in adaptation performance over baseline methods.

So in summary, the key contribution is an automatic play-relabel process leveraging foundation models to adapt policies to significant distribution shifts at test time.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a method called Policy Adaptation from Foundation Model Feedback (PAFF) that leverages pre-trained vision-language models to provide feedback for adapting robotic manipulation policies to new tasks and environments without human supervision.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in language conditioned robotics:

- This paper focuses on policy adaptation and generalization to new tasks/environments using feedback from pre-trained vision-language models. Much prior work has focused just on training policies on a fixed set of tasks, objects, and environments. The idea of leveraging feedback from foundation models to enable adaptation is novel.

- For language-conditioned manipulation, this paper builds on recent work like CLIPort and CALVIN that also uses pre-trained vision-language models like CLIP. The main difference is this paper goes further to use the models not just for encoding, but also for providing feedback to improve generalization.

- The proposed play and relabel pipeline is similar in spirit to some prior test-time adaptation methods in robotics. But utilizing automatic relabelling from the foundation model instead of self-supervision or human annotations is a key difference.

- The extensive evaluations on compositional generalization, out-of-distribution tasks, and sim-to-real transfer cover challenging settings for language-conditioned policies. The results demonstrate broad improvements over strong baselines.

- Compared to concurrent work on foundation models for robot learning, this paper has a different focus on leveraging the models for on-policy adaptation rather than just pre-training representations. The play and relabel approach seems complementary.

Overall, this paper makes important contributions in improving the generalization capabilities of language-conditioned policies by exploiting feedback from pre-trained models. The play and relabel pipeline is simple yet effective for automatic adaptation without human involvement. The results significantly advance the state-of-the-art across a range of challenging language-conditioned robotics benchmarks.
