# [Policy Adaptation from Foundation Model Feedback](https://arxiv.org/abs/2212.07398)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to adapt a trained language conditioned policy to new tasks and environments, including unseen objects, compositional instructions, and sim-to-real transfer. 

The key hypothesis is that leveraging pre-trained vision-language models to provide feedback can help automatically adapt the policy by re-labeling demonstrations and providing new training data. Specifically, the method proposed in the paper, called Policy Adaptation from Foundation model Feedback (PAFF), hypothesizes that:

- Although a trained policy may fail on new tasks/environments, pre-trained vision-language models can still recognize visual concepts and provide useful feedback.

- By letting the policy "play" and generate demonstrations on the new task, then relabeling those demos using the foundation model, new training data can be obtained to adapt the policy.

- This play-relabel process with a pre-trained vision-language model as teacher can enable automatic policy adaptation without human intervention.

The paper then conducts experiments on compositional generalization, out-of-distribution environments, and sim-to-real transfer to evaluate whether the proposed PAFF method improves adaptation compared to baselines without feedback. The results generally validate the hypothesis, showing benefits across the different settings.
