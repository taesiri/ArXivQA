# [Policy Adaptation from Foundation Model Feedback](https://arxiv.org/abs/2212.07398)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to adapt a trained language conditioned policy to new tasks and environments, including unseen objects, compositional instructions, and sim-to-real transfer. 

The key hypothesis is that leveraging pre-trained vision-language models to provide feedback can help automatically adapt the policy by re-labeling demonstrations and providing new training data. Specifically, the method proposed in the paper, called Policy Adaptation from Foundation model Feedback (PAFF), hypothesizes that:

- Although a trained policy may fail on new tasks/environments, pre-trained vision-language models can still recognize visual concepts and provide useful feedback.

- By letting the policy "play" and generate demonstrations on the new task, then relabeling those demos using the foundation model, new training data can be obtained to adapt the policy.

- This play-relabel process with a pre-trained vision-language model as teacher can enable automatic policy adaptation without human intervention.

The paper then conducts experiments on compositional generalization, out-of-distribution environments, and sim-to-real transfer to evaluate whether the proposed PAFF method improves adaptation compared to baselines without feedback. The results generally validate the hypothesis, showing benefits across the different settings.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Policy Adaptation from Foundation Model Feedback (PAFF) for adapting trained policies to new tasks and environments. The key ideas are:

- Let the trained policy "play" in the new task by executing randomly generated instructions and record the demonstrations, even though they may be wrong. 

- Use a fine-tuned vision-language foundation model (e.g. CLIP) to relabel the demonstrations by retrieving the correct instruction text given the visual observations.

- Fine-tune the policy on the relabeled demonstrations to adapt it to the new task/environment. 

The main benefits are:

- The adaptation process is fully automatic, without needing human labeling effort.

- It allows adapting policies to compositional tasks, unseen objects/environments, and from simulation to real world. 

- Experiments across different tasks show large improvements in adaptation performance over baseline methods.

So in summary, the key contribution is an automatic play-relabel process leveraging foundation models to adapt policies to significant distribution shifts at test time.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a method called Policy Adaptation from Foundation Model Feedback (PAFF) that leverages pre-trained vision-language models to provide feedback for adapting robotic manipulation policies to new tasks and environments without human supervision.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in language conditioned robotics:

- This paper focuses on policy adaptation and generalization to new tasks/environments using feedback from pre-trained vision-language models. Much prior work has focused just on training policies on a fixed set of tasks, objects, and environments. The idea of leveraging feedback from foundation models to enable adaptation is novel.

- For language-conditioned manipulation, this paper builds on recent work like CLIPort and CALVIN that also uses pre-trained vision-language models like CLIP. The main difference is this paper goes further to use the models not just for encoding, but also for providing feedback to improve generalization.

- The proposed play and relabel pipeline is similar in spirit to some prior test-time adaptation methods in robotics. But utilizing automatic relabelling from the foundation model instead of self-supervision or human annotations is a key difference.

- The extensive evaluations on compositional generalization, out-of-distribution tasks, and sim-to-real transfer cover challenging settings for language-conditioned policies. The results demonstrate broad improvements over strong baselines.

- Compared to concurrent work on foundation models for robot learning, this paper has a different focus on leveraging the models for on-policy adaptation rather than just pre-training representations. The play and relabel approach seems complementary.

Overall, this paper makes important contributions in improving the generalization capabilities of language-conditioned policies by exploiting feedback from pre-trained models. The play and relabel pipeline is simple yet effective for automatic adaptation without human involvement. The results significantly advance the state-of-the-art across a range of challenging language-conditioned robotics benchmarks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the main future research directions suggested by the authors are:

1. Exploring different methods for temporal reasoning when fine-tuning the vision-language foundation model to label recorded demonstrations. The authors experimented with different mechanisms like 2D/3D joint attention and spatio-temporal adapters, but there is room for further exploration here. 

2. Applying PAFF to other types of policies beyond just language-conditioned policies, such as goal-conditioned or reinforcement learning policies. The authors focused on language-conditioned policies in this work but their method could potentially benefit other policy learning paradigms as well.

3. Evaluating PAFF on more complex and diverse tasks, environments and platforms. The authors conducted a broad range of experiments but there is still room to assess the method on even more challenging settings.

4. Combining PAFF with other policy adaptation and transfer techniques like domain randomization and meta-learning. Integrating PAFF into existing learning paradigms could further improve generalization capabilities.

5. Exploring different ways to generate the language instructions for the policy to "play" with, beyond just random generation. This could potentially make the play phase more efficient and effective.

6. Studying how to better leverage the feedback and representations from the foundation models beyond just re-labeling demonstrations. There may be other ways to take advantage of their capabilities.

7. Reducing the amount of experience needed for adaptation and analyzing the sample efficiency gains from PAFF in more depth.

So in summary, the main suggested directions are around exploring improvements to the components of PAFF, applying it to new settings and policies, and better understanding its benefits. The authors have opened up an interesting new research avenue with promising results so far.


## Summarize the paper in one paragraph.

 The paper proposes a method called Policy Adaptation from Foundation model Feedback (PAFF) for adapting language conditioned manipulation policies to new tasks and environments. The key idea is to leverage vision-language foundation models to provide feedback for collecting new demonstration data to fine-tune the policy. Specifically, when deploying a trained policy to a new task, the method first lets the policy "play" by following random instructions and records the demonstrations. Although the executions may be wrong, a fine-tuned foundation model can relabel the demonstrations by retrieving accurate instructions given the visual observations. This provides new reliable demonstration-instruction pairs to fine-tune the policy for the new task or environment. The method is evaluated on compositional generalization, out-of-distribution generalization, and sim-to-real transfer. Results show it significantly improves adaptation performance over baselines in all cases. The main contribution is using foundation models to automatically collect reliable data to adapt policies to new distributions.
