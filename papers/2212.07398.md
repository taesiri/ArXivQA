# [Policy Adaptation from Foundation Model Feedback](https://arxiv.org/abs/2212.07398)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to adapt a trained language conditioned policy to new tasks and environments, including unseen objects, compositional instructions, and sim-to-real transfer. 

The key hypothesis is that leveraging pre-trained vision-language models to provide feedback can help automatically adapt the policy by re-labeling demonstrations and providing new training data. Specifically, the method proposed in the paper, called Policy Adaptation from Foundation model Feedback (PAFF), hypothesizes that:

- Although a trained policy may fail on new tasks/environments, pre-trained vision-language models can still recognize visual concepts and provide useful feedback.

- By letting the policy "play" and generate demonstrations on the new task, then relabeling those demos using the foundation model, new training data can be obtained to adapt the policy.

- This play-relabel process with a pre-trained vision-language model as teacher can enable automatic policy adaptation without human intervention.

The paper then conducts experiments on compositional generalization, out-of-distribution environments, and sim-to-real transfer to evaluate whether the proposed PAFF method improves adaptation compared to baselines without feedback. The results generally validate the hypothesis, showing benefits across the different settings.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Policy Adaptation from Foundation Model Feedback (PAFF) for adapting trained policies to new tasks and environments. The key ideas are:

- Let the trained policy "play" in the new task by executing randomly generated instructions and record the demonstrations, even though they may be wrong. 

- Use a fine-tuned vision-language foundation model (e.g. CLIP) to relabel the demonstrations by retrieving the correct instruction text given the visual observations.

- Fine-tune the policy on the relabeled demonstrations to adapt it to the new task/environment. 

The main benefits are:

- The adaptation process is fully automatic, without needing human labeling effort.

- It allows adapting policies to compositional tasks, unseen objects/environments, and from simulation to real world. 

- Experiments across different tasks show large improvements in adaptation performance over baseline methods.

So in summary, the key contribution is an automatic play-relabel process leveraging foundation models to adapt policies to significant distribution shifts at test time.
