# [Toward Verifiable and Reproducible Human Evaluation for Text-to-Image   Generation](https://arxiv.org/abs/2304.01816)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper are:1. Current practices of human evaluation for text-to-image generation are problematic and lack reliability and transparency. The authors survey recent papers and find inconsistencies in evaluation protocols, omitted experimental details, and questionable practices like small sample sizes. 2. Automatic evaluation metrics like FID and CLIPScore are insufficient to represent human perception of image quality and text-image alignment. The authors hypothesize that these metrics are inconsistent with human evaluation.3. A standardized and well-defined human evaluation protocol is needed to facilitate reliable and reproducible evaluation of text-to-image models. The authors propose and validate such a protocol using crowdsourcing.4. The proposed human evaluation protocol can reveal limitations of current text-to-image models that are not exposed by automatic metrics. The authors hypothesize that their protocol will show discrepancies between human ratings and automatic measures.5. Sharing code and data resources from standardized human evaluation will benefit the research community. The authors aim to provide implementations, reporting templates, and human ratings to facilitate adoption of better practices.In summary, the core goals are to demonstrate issues with current human evaluation practices, propose and validate a better standardized protocol, show its ability to reveal limitations of automatic metrics, and provide resources to facilitate improved evaluation in future text-to-image generation research.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- They survey 37 recent text-to-image generation papers and find that current practices of human evaluation have issues with reliability and transparency. For example, evaluation protocols vary significantly between papers, important details like the number of ratings per sample are often omitted, and annotation quality is rarely assessed.- They propose a standardized human evaluation protocol for text-to-image generation to address these issues. The protocol uses crowdsourcing and is designed to be simple and produce interpretable results.- They evaluate several state-of-the-art generative models with their protocol and analyze the collected ratings. The results reveal limitations of commonly used automatic measures like FID and CLIPScore.- They provide recommendations for conducting reliable human evaluations, like reporting details of the experimental configuration and understanding crowdworker behavior. - They make several resources available including an implementation of their protocol, a template for reporting results, and human ratings for several datasets. This is aimed at facilitating reproducible and transparent human evaluation.In summary, the main contribution is developing a standardized protocol to enable more rigorous human evaluation for text-to-image generation, along with an empirical analysis that demonstrates the need for such protocols and resources to support their adoption. The paper aims to improve the verifiability and reproducibility of human evaluation in this field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a standardized human evaluation protocol for text-to-image generation to enable reliable and reproducible human evaluations, empirically validates the protocol, and provides recommendations and resources to facilitate adoption.


## How does this paper compare to other research in the same field?

Here are some key points about how this paper compares to other research in text-to-image generation:- It provides a comprehensive review of human evaluation practices in recent text-to-image papers, identifying issues with reliability and transparency. Many papers rely solely on automatic measures or use poorly described human evaluation protocols. The review is quite thorough in documenting the current state of human evaluation.- The paper proposes a standardized human evaluation protocol using crowdsourcing to facilitate better reproducibility and verifiability. The protocol is designed based on principles of simplicity and interpretability. Details like rating formats, evaluation criteria, qualifications, instructions etc. are carefully considered.- The proposed protocol is validated through pilot experiments and data collection. Different aspects like the impact of question wording, qualification requirements, number of prompts and raters are analyzed. This provides useful insights into protocol design choices.- Several existing text-to-image models are evaluated on datasets using the standardized protocol. Misalignment between automatic metrics like FID and human judgments is demonstrated. The collected human annotations and analysis provide a valuable benchmark and resource.- The work focuses specifically on the evaluation of text-to-image models using natural language descriptions. Other modalities like artwork generation may need different criteria. The scope is narrower compared to general image generation evaluation.- In contrast to many existing papers, this work places emphasis on proper experimental design, transparent reporting and analysis of human evaluation. The code, data and reporting template are open-sourced to facilitate adoption.Overall, the systematic treatment of human evaluation is a key distinguishing factor compared to most prior text-to-image papers. The standardized protocol and analysis help advance best practices for reliable and reproducible human evaluation.
