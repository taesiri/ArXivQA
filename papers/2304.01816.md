# [Toward Verifiable and Reproducible Human Evaluation for Text-to-Image   Generation](https://arxiv.org/abs/2304.01816)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper are:1. Current practices of human evaluation for text-to-image generation are problematic and lack reliability and transparency. The authors survey recent papers and find inconsistencies in evaluation protocols, omitted experimental details, and questionable practices like small sample sizes. 2. Automatic evaluation metrics like FID and CLIPScore are insufficient to represent human perception of image quality and text-image alignment. The authors hypothesize that these metrics are inconsistent with human evaluation.3. A standardized and well-defined human evaluation protocol is needed to facilitate reliable and reproducible evaluation of text-to-image models. The authors propose and validate such a protocol using crowdsourcing.4. The proposed human evaluation protocol can reveal limitations of current text-to-image models that are not exposed by automatic metrics. The authors hypothesize that their protocol will show discrepancies between human ratings and automatic measures.5. Sharing code and data resources from standardized human evaluation will benefit the research community. The authors aim to provide implementations, reporting templates, and human ratings to facilitate adoption of better practices.In summary, the core goals are to demonstrate issues with current human evaluation practices, propose and validate a better standardized protocol, show its ability to reveal limitations of automatic metrics, and provide resources to facilitate improved evaluation in future text-to-image generation research.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- They survey 37 recent text-to-image generation papers and find that current practices of human evaluation have issues with reliability and transparency. For example, evaluation protocols vary significantly between papers, important details like the number of ratings per sample are often omitted, and annotation quality is rarely assessed.- They propose a standardized human evaluation protocol for text-to-image generation to address these issues. The protocol uses crowdsourcing and is designed to be simple and produce interpretable results.- They evaluate several state-of-the-art generative models with their protocol and analyze the collected ratings. The results reveal limitations of commonly used automatic measures like FID and CLIPScore.- They provide recommendations for conducting reliable human evaluations, like reporting details of the experimental configuration and understanding crowdworker behavior. - They make several resources available including an implementation of their protocol, a template for reporting results, and human ratings for several datasets. This is aimed at facilitating reproducible and transparent human evaluation.In summary, the main contribution is developing a standardized protocol to enable more rigorous human evaluation for text-to-image generation, along with an empirical analysis that demonstrates the need for such protocols and resources to support their adoption. The paper aims to improve the verifiability and reproducibility of human evaluation in this field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a standardized human evaluation protocol for text-to-image generation to enable reliable and reproducible human evaluations, empirically validates the protocol, and provides recommendations and resources to facilitate adoption.
