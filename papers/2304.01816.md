# [Toward Verifiable and Reproducible Human Evaluation for Text-to-Image   Generation](https://arxiv.org/abs/2304.01816)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper are:1. Current practices of human evaluation for text-to-image generation are problematic and lack reliability and transparency. The authors survey recent papers and find inconsistencies in evaluation protocols, omitted experimental details, and questionable practices like small sample sizes. 2. Automatic evaluation metrics like FID and CLIPScore are insufficient to represent human perception of image quality and text-image alignment. The authors hypothesize that these metrics are inconsistent with human evaluation.3. A standardized and well-defined human evaluation protocol is needed to facilitate reliable and reproducible evaluation of text-to-image models. The authors propose and validate such a protocol using crowdsourcing.4. The proposed human evaluation protocol can reveal limitations of current text-to-image models that are not exposed by automatic metrics. The authors hypothesize that their protocol will show discrepancies between human ratings and automatic measures.5. Sharing code and data resources from standardized human evaluation will benefit the research community. The authors aim to provide implementations, reporting templates, and human ratings to facilitate adoption of better practices.In summary, the core goals are to demonstrate issues with current human evaluation practices, propose and validate a better standardized protocol, show its ability to reveal limitations of automatic metrics, and provide resources to facilitate improved evaluation in future text-to-image generation research.
