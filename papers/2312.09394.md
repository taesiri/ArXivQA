# [HiER: Highlight Experience Replay and Easy2Hard Curriculum Learning for   Boosting Off-Policy Reinforcement Learning Agents](https://arxiv.org/abs/2312.09394)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
Reinforcement learning (RL) methods have achieved great success in games and simulations, but face significant challenges when applied to robotics. Key issues are the continuous state/action spaces and sparse rewards, which make exploration very difficult. Improving RL performance in robotics through better data collection and exploitation methods is an important area of research.

Proposed Solutions:
The paper proposes three methods to facilitate RL training in robotic environments:

1. Highlight Experience Replay (HiER): Stores the most "useful" transitions in a separate replay buffer and uses them more frequently for training. Useful transitions are identified by a threshold on the total episode reward.

2. Easy-to-Hard Initial State Entropy (E2H-ISE): Gradually increases the entropy/variance of the initial state distribution over training, starting from low-entropy (easy) states and moving towards higher-entropy (harder) states.

3. HiER+: Combination of the above two methods.

Contributions:

1. Novel HiER method to identify and prioritize useful experiences, acting as an "automatic demonstration generator". 

2. E2H-ISE curriculum method that controls task difficulty through the initial state distribution entropy. Fundamentally different from prior CL approaches.

3. HiER+ framework integrating both data collection and exploitation for curriculum learning in RL.

4. Significantly outperforms SOTA on robotic manipulation benchmarks, achieving 1.0, 0.83 and 0.69 success rates on push, slide and pick-and-place tasks.

5. Analysis of different versions of HiER and E2H-ISE provides insights into their impact.

In summary, the paper makes substantial contributions around facilitating robotic RL through principled data control, with very positive results demonstrated on complex manipulation tasks. The ideas proposed seem generally applicable to boost various off-policy RL algorithms.
