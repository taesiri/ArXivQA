# [Positive Label Is All You Need for Multi-Label Classification](https://arxiv.org/abs/2306.16016)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can we improve multi-label image classification by discarding all negative labels and training only on positive and unlabeled data?The key hypothesis is that by removing all negative labels, which are more numerous and more likely to contain noise, the model can learn more effectively from the remaining clean positive labels and unlabeled data in a positive-unlabeled learning framework. In particular, the paper proposes:- Extending positive-unlabeled (PU) learning to multi-label classification (MLC) by discarding all negative labels and using only positive and unlabeled data for training.- Introducing an adaptive re-balance factor and adaptive temperature coefficient in the PU loss function to deal with the severe class imbalance and over-smoothing of probabilities when adapting PU learning to MLC.- A local-global convolution module to capture both local and global dependencies in images to further improve performance.The central hypothesis is that by using only positive and unlabeled data in this PU learning framework tailored to MLC, the model can learn more robustly and achieve better performance compared to traditional MLC trained on positive and noisy negative labels, especially in the multi-label classification with partial labels (MLC-PL) setting.
