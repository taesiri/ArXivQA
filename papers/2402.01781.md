# [When Benchmarks are Targets: Revealing the Sensitivity of Large Language   Model Leaderboards](https://arxiv.org/abs/2402.01781)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Existing leaderboards for large language models based on multiple choice question (MCQ) benchmarks are not robust - small perturbations to the MCQs lead to large changes in model rankings. 
- This is problematic as practitioners rely on benchmarks to select the best models, which can be very costly decisions.

- The paper systematically studies perturbations across 3 categories:
   1) Answer choice format/order 
   2) Prompt/scoring format
   3) In-context knowledge

Key Findings:

- Model rankings break down under many minor perturbations, like shuffling answer order or using alternative scoring methods. Some models' rankings change more drastically.

- All tested models exhibit bias to answer order, symbols, and scoring method to varying degrees. This demonstrates a core weakness in MCQ evaluation.

- Models readily use knowledge provided in-context to "cheat", even when it is incorrect, showing potential benchmark vulnerability.  

- Some perturbations like changing uninformative context do not affect rankings much.

Recommendations:

- Hybrid scoring balances performance and bias over pure symbol scoring.

- Trivial/out-of-domain examples reduce but don't eliminate sensitivity vs pure zero-shot.

- Overall the paper shows critical instability on existing MCQ benchmarks and charts a path to more robust evaluations.

Main Contributions:

- Quantifies and explains sensitivity of benchmark rankings for model selection through minor perturbations
- Shows LLMs exhibit multiple forms of bias on MCQs 
- Identifies minor changes that do/don't affect stability 
- Provides best practice recommendations towards more robust MCQ-based benchmarks


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper demonstrates that current large language model leaderboards based on multiple choice benchmarks lack robustness, with model rankings changing significantly due to minor perturbations like shuffling answer choices or using different scoring methods.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Demonstrating that existing model rankings on popular language model benchmarks like MMLU break down under slight perturbations, particularly for medium to small sized models. For example, model ranks can shift by up to 8 positions under minor changes to the benchmark format or scoring method.

2) Providing an analysis to explain this phenomenon by conducting experiments over three broad categories of benchmark perturbations - answer choice format/ordering, prompt/scoring modifications, and in-context knowledge manipulation. The analysis identifies sources of bias and instability in the models and benchmarks.

3) Making several best-practice recommendations, such as using a "hybrid" scoring method for answer choices instead of the commonly used "symbol" scoring, to help make evaluations more robust.

4) Highlighting the dangers of relying too heavily on simple multiple choice benchmarks for model evaluation and selection, and charting a path towards more robust evaluation schemes. Overall, the study reveals instability in existing leaderboards and MCQ-based benchmarks for comparing language models.

Does this summary accurately capture the main contributions of the paper? Let me know if you need any clarification or have additional questions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's abstract and contents, here are some of the key terms and keywords associated with this paper:

- Large language models (LLMs)
- Leaderboards
- Benchmarks
- Multiple choice questions (MCQs) 
- Selection bias
- Position bias
- Token bias
- Scoring bias  
- Prompt modifications
- Answer choice formatting
- Knowledge manipulation
- Robustness
- Stability

The paper examines the sensitivity and robustness of LLM leaderboard rankings based on popular MCQ benchmarks like MMLU. It studies how minor perturbations to the benchmarks, such as changes to answer order, symbols, scoring methods, prompts etc. can significantly change model rankings. The paper also analyzes different forms of bias exhibited by LLMs when evaluated using MCQs. Overall, the paper demonstrates the lack of stability in current LLM leaderboards and benchmarks, using extensive experiments over different categories of modifications.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. This paper discusses perturbations to MCQ benchmarks that cause instability in model rankings. What types of perturbations were studied and what were some examples of each? How drastic were the ranking changes observed?

2. The paper categorizes sources of bias when evaluating models on MCQs. Can you describe the different categories of bias identified? Provide examples of experiments that were designed to isolate each one.  

3. What was the motivation behind designing experiments to replace the default A/B/C/D answer choice symbols? What two alternative symbol sets were used and what was discovered about model bias toward symbols?

4. The paper analyzes different scoring methods for MCQs including symbol scoring, cloze scoring, and hybrid scoring. Can you explain how each method works? What are the tradeoffs between accuracy and bias for each?

5. When analyzing model rankings, the paper uses Kendall's tau distance to quantify disagreement between two rankings. Explain how this metric works. What threshold was used to determine if ranking changes were significant?

6. The paper experimented with different ways of manipulating the few-shot examples provided to models. Describe three of these experiments. Did any lead to major ranking changes?

7. The experiments reveal that models can readily "cheat" or over-rely on information provided in few-shot examples, even when incorrect. Why is this concerning for benchmark integrity? Suggest ways this could potentially be mitigated.  

8. Can you describe some of the minor prompt modifications that were shown to have little effect on accuracy and rankings? Why were these experiments still useful to conduct?

9. What limitations does the paper identify regarding explaining the underlying causes of model bias? What approaches does it suggest may help better determine the causes in future work?

10. The paper is ultimately unable to propose perturbations that lead to fully robust benchmark rankings. In your opinion, what are the most promising directions that should be explored to work toward more reliable evaluations?
