# [Cleaner Pretraining Corpus Curation with Neural Web Scraping](https://arxiv.org/abs/2402.14652)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Cleaner Pretraining Corpus Curation with Neural Web Scraping":

Problem:
- Large language models (LLMs) require massive amounts of high-quality text data for pretraining, but acquiring such data is challenging. 
- Web pages contain abundant information, but also noise like ads and boilerplate content. Rule-based web scrapers have limitations in handling complex and non-standardized web page layouts.
- There is a need for better web scraping methods to extract high-quality primary content from web pages to build corpora for LLM pretraining.

Proposed Solution:
- The authors propose NeuScraper, a neural web scraper for extracting primary content from web pages. 
- It converts web pages to textual sequences using the DOM structure and feeds them to a shallow neural network to predict which parts are primary content.
- The model uses an XLM-Roberta encoder to get node representations and a transformer model to encode contextual information.

Main Contributions:
- NeuScraper achieves over 20% better performance in extracting primary content compared to state-of-the-art baseline scrapers.
- LLMs pretrained on NeuScraper data achieve better performance on downstream tasks compared to scrapers like trafilatura.
- NeuScraper reduces the perplexity of target corpora by over 4 points, indicating its extracted data is of higher quality.  
- It has high inference speed with GPU support and demonstrates generalizability in handling diverse web page layouts.
- NeuScraper facilitates acquisition of less noisy and better quality data at scale for LLM pretraining.

In summary, NeuScraper is a fast, effective and generalizable neural web scraper that can extract higher quality primary content from web pages to build corpora for pretraining large language models.
