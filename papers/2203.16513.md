# [PromptDet: Towards Open-vocabulary Detection using Uncurated Images](https://arxiv.org/abs/2203.16513)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we expand the vocabulary of an object detector to detect novel object categories without requiring additional manual annotations? The key hypothesis is that by leveraging large-scale pre-trained visual-language models like CLIP, aligning the visual and textual latent spaces through regional prompt learning, and iteratively self-training on uncurated web images, an object detector can learn to detect novel objects without needing any human-provided bounding box annotations for those new classes.In summary, the paper aims to show that open-vocabulary object detection is possible without expensive manual labeling, by making use of the knowledge in CLIP and web images. The core hypothesis is that aligning the detector's visual features with the CLIP text encoder through regional prompt learning and self-training will enable detecting novel objects without annotations.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a two-stage open-vocabulary object detector, where class-agnostic object proposals are classified using a text encoder from a pre-trained visual-language model. This allows the detector to recognize novel object categories beyond its training categories.2. It introduces a "regional prompt learning" method to align the textual embedding space of the text encoder with regional visual object features. This helps pair the visual and textual latent spaces.3. It presents a self-training framework to iteratively update prompt vectors and source candidate images from the web to improve alignment. This allows scaling up to detect more objects using uncurated web images. 4. The proposed detector "PromptDet" outperforms previous methods on LVIS and MS-COCO datasets using fewer additional training images and no manual annotations.In summary, the key contribution is a scalable pipeline to expand an object detector's vocabulary to novel categories without any manual annotation, by leveraging pre-trained visual-language models and uncurated web images. The proposed regional prompt learning and self-training framework are important to enable this.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an open-vocabulary object detector called PromptDet that can detect novel object categories without any manual annotations by leveraging a large corpus of uncurated web images and a self-training framework with regional prompt learning to align the vision and language spaces.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in open-vocabulary object detection:- This paper proposes a novel method called PromptDet that uses a frozen CLIP text encoder as an "off-the-shelf" classifier for detecting novel object categories. Other recent works like ViLD and Detic also leverage CLIP, but they require distilling knowledge from the CLIP visual encoder into the detector backbone which is computationally expensive.- A core contribution is the idea of "regional prompt learning" to align the CLIP text embedding space with region-based visual features from the detector. This is a new technique not used in prior work.- The self-training framework using uncurated web images is more scalable than prior approaches. For example, Detic uses curated ImageNet21K images requiring lots of manual annotation effort. - PromptDet shows much better performance on LVIS using fewer training images and epochs compared to state-of-the-art like ViLD. It also outperforms Detic on COCO with less computation.- Overall, PromptDet provides a simpler and more effective approach for open-vocabulary detection. The regional prompt learning and self-training framework seem to be key innovations over prior arts. The results are very competitive, especially considering the lower training costs.In summary, this paper pushes the state-of-the-art in open-vocabulary detection further by introducing some novel techniques like prompt learning and unsupervised self-training that prove to be very effective. The experimental results demonstrate the efficacy of PromptDet over existing methods.
