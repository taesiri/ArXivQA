# [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text   Documents](https://arxiv.org/abs/2306.16527)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to create a high-quality, large-scale, open dataset of multimodal documents to support training more capable multimodal AI models. Specifically, the key aspects the paper focuses on are:- How to extract and filter a massive amount of web documents to create a dataset of multimodal documents that interleave images and long-form text.- Documenting the dataset creation process in detail, including collection, filtering, and responsible data practices. - Analyzing the resulting dataset to shed light on its properties and content.- Demonstrating the viability of the dataset by training multimodal models on it and evaluating their performance.The overall goal is to provide the community with a new open benchmark dataset that can enable further progress in multimodal AI, akin to how other large datasets like ImageNet or SQuAD have driven vision and language research. The paper highlights the need for such a dataset, provides a methodology for its construction, analyzes its characteristics, and shows its utility through trained models.


## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to create an open, large-scale, high-quality dataset of multimodal documents to support training advanced multimodal models. Specifically, the authors aim to:- Collect a large dataset of multimodal documents by extracting text and images from Common Crawl web crawl data.- Apply careful filtering and processing to the raw HTML data to extract high-quality multimodal documents while removing undesirable content. - Release the resulting dataset openly to support research and development of multimodal models.- Demonstrate the viability of the dataset by training multimodal models on it and evaluating performance on standard benchmarks.The key hypothesis is that a thoughtfully constructed dataset of multimodal documents, orders of magnitude larger than previous options, will enable more capable multimodal models. The paper details the methodology to create such a dataset and provides experiments validating its usefulness.In summary, the core research question is how to build an open, large-scale dataset of multimodal documents to advance multimodal AI capabilities, which the authors address through the design and release of the OBELICS dataset.
