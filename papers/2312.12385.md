# [Input Compression with Positional Consistency for Efficient Training and   Inference of Transformer Neural Networks](https://arxiv.org/abs/2312.12385)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformers have become very popular and achieve state-of-the-art results across various modalities like text, images, audio and video. However, they have large computational requirements for both training and inference. 
- Transformers are also prone to overfitting, especially when training data is limited.

Proposed Solution:
- The paper proposes Input Compression with Positional Consistency (ICPC), a new data augmentation technique that applies varying levels of compression to each input sample during training. 
- This leads to smaller input sequence lengths being processed by the Transformer, resulting in faster training. It also acts as an effective data augmentation method to reduce overfitting.
- To enable compressed inputs to be accurately processed by the Transformer, a consistency-aware position selection method is introduced. This selects positions for the compressed input embeddings while preserving consistency with the original uncompressed input.

Key Contributions:
- Input compression methods specific to text, images, video and audio that reduce the number of embedding vectors needing to be processed.
- A consistency-aware position selection technique that allows compressed inputs to be handled without architectural changes.
- Demonstrating ICPC's ability to simultaneously improve accuracy (by up to 1%) and accelerate training & inference (by up to 2.9x and 2.6x) over 9 tasks across modalities.
- Showing ICPC enables variable-effort inference, where samples are initially processed at high compression levels, then re-evaluated at lower compression only if predictions have low confidence.
- Proposing a hardware-aware test-time augmentation approach using ICPC that further improves accuracy by 1.4 absolute points on average.

In summary, the key innovation is data augmentation through input compression with positional consistency, which brings efficiency and accuracy benefits to Transformers across modalities.
