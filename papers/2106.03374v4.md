# [RegMix: Data Mixing Augmentation for Regression](https://arxiv.org/abs/2106.03374v4)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we improve data augmentation techniques for regression tasks? 

The authors note that existing data augmentation methods like Mixup have shown success for classification tasks, but have limitations when applied directly to regression problems. They hypothesize that taking linear interpolations between arbitrary example pairs, as Mixup does, can produce labels that are increasingly incorrect for regression tasks where the label space is continuous. 

To address this, they propose a new method called RegMix that limits the distance between examples that are mixed, under the assumption that mixing more nearby points will produce more accurate labels. Their key research question seems to be whether selectively limiting the mixing distances per example can improve performance of data augmentation for regression.

The paper introduces the RegMix algorithm and framework to learn optimal mixing policies, and empirically evaluates whether this approach improves over Mixup and other baselines on both synthetic and real regression datasets. The hypothesis appears to be that RegMix will better augment regression data and improve model performance compared to prior augmentation techniques by more selectively determining which examples to mix based on distances.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution appears to be proposing a data augmentation framework called RegMix for regression tasks. The key ideas are:

- Existing data augmentation techniques like Mixup have limitations when applied to regression problems, because mixing arbitrary example pairs via linear interpolation can result in incorrect labels. 

- The paper proposes limiting the distance between examples that are mixed, based on the observation that mixing more distant examples tends to increase label error.

- RegMix learns a kNN mixing policy for each example, specifying how many nearest neighbors it should mix with. This is formulated as a reinforcement learning problem with the goal of minimizing the regression loss on a validation set.

- Experiments on synthetic and real-world datasets demonstrate RegMix outperforms baselines like standard Mixup and global mixing policies. The framework is able to effectively determine which examples to mix and how many neighbors to use.

In summary, the main contribution appears to be proposing RegMix, a data augmentation framework tailored for regression that determines optimal mixing policies per example by limiting the distance between mixed examples. This is shown to improve model performance compared to prior data augmentation techniques designed for classification tasks.
