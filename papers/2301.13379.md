# Faithful Chain-of-Thought Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we design a prompting framework that provides both high accuracy and faithfulness/interpretability for large language models on complex reasoning tasks?The key points are:- Chain-of-Thought (CoT) prompting has been shown to boost language models' performance on complex reasoning tasks recently. - However, existing CoT methods lack faithfulness - the reasoning chain generated does not necessarily reflect how the model actually derives the final answer.- The authors propose a new prompting framework, Faithful CoT, that decomposes reasoning into two stages: Translation and Problem Solving. - In Translation, a language model converts the query into a symbolic reasoning chain interleaving natural language and a symbolic language. - In Problem Solving, the reasoning chain is executed by a deterministic solver to derive the final answer.- This approach provides faithfulness by construction, as the answer is the result of executing the reasoning chain.- Experiments show Faithful CoT outperforms CoT prompting on most datasets, while also providing interpretability into the model's reasoning process.In summary, the central hypothesis is that by decomposing reasoning into translation and problem solving stages, they can create a prompting framework that achieves both high accuracy and faithfulness for language models on complex reasoning tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Faithful Chain-of-Thought (Faithful CoT), a novel prompting framework that provides interpretable and faithful reasoning for language models on complex tasks. Specifically:- It proposes a 2-stage pipeline consisting of Translation and Problem Solving. In Translation, a language model converts a natural language query into a reasoning chain interleaving natural language and symbolic language (e.g. Python code). In Problem Solving, the reasoning chain is executed by a deterministic solver to derive the final answer.- This approach decomposes reasoning into natural language understanding and formal logic execution. By using a solver to deterministically derive the answer, the reasoning chain provides a faithful explanation of how the model arrives at the answer. - The authors demonstrate the efficacy of Faithful CoT on diverse reasoning tasks spanning math word problems, multi-hop QA, planning, and logical inference. It outperforms standard prompting and previous CoT methods on most datasets, while also being interpretable and faithful.- The reasoning chain provides opportunities for interaction, allowing users to understand and potentially fix errors by modifying the natural language components. In summary, the key innovation is proposing a prompting framework that makes language models more accurate, interpretable and faithful on complex reasoning tasks, by integrating natural language prompting with symbolic program execution.
