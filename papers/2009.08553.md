# [Generation-Augmented Retrieval for Open-domain Question Answering](https://arxiv.org/abs/2009.08553)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called Generation-Augmented Retrieval (GAR) for open-domain question answering. The central hypothesis is that augmenting queries (questions) by generating additional relevant contexts through a pre-trained language model, without requiring external resources or supervision, can significantly improve the retrieval of relevant passages and consequently improve question answering performance. 

Specifically, the key research questions addressed are:

1) Can generating heuristic query contexts like answers, relevant sentences, and passage titles using a pre-trained language model improve passage retrieval for open-domain QA without external supervision?

2) Can a simple sparse retrieval method like BM25, augmented with GAR, match or exceed state-of-the-art dense retrieval methods like DPR that use the original queries?

3) Does generating and fusing diverse query contexts consistently improve retrieval accuracy compared to using individual contexts? 

4) Can GAR improve end-to-end QA performance over strong baselines when paired with both extractive and generative reader models?

In summary, the central hypothesis is that text generation can be used to automatically expand queries with relevant contexts extracted from pre-trained language models, without external supervision, leading to improved passage retrieval and question answering. The paper aims to validate this through empirical comparisons across diverse contexts and reader models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Generation-Augmented Retrieval (GAR) for open-domain question answering. GAR augments queries with relevant contexts generated by pre-trained language models, without requiring external supervision or reinforcement learning. Key contributions are:

- Proposing to augment queries with generated contexts like answers, relevant sentences, and passage titles. This expands queries with useful information without needing external resources. 

- Showing GAR with sparse retrieval (BM25) achieves comparable or better performance than state-of-the-art dense methods on NQ and TriviaQA.

- Demonstrating generating and fusing diverse contexts consistently improves retrieval accuracy. 

- Achieving state-of-the-art results on NQ and TriviaQA by combining GAR with existing methods like DPR.

- Showing GAR is efficient as it uses sparse retrieval and avoids expensive training like reinforcement learning.

In summary, the main contribution is proposing an effective and efficient query augmentation approach for open-domain QA that expands queries by extracting relevant information from pre-trained language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes Generation-Augmented Retrieval, which expands queries for open-domain question answering by appending relevant contexts generated by pre-trained language models, achieving state-of-the-art performance with simple sparse retrieval methods.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on Generation-Augmented Retrieval (GAR) for open-domain question answering compares to other related work:

- Most prior work on query reformulation/expansion for retrieval requires external resources like paraphrase datasets or conversational context. GAR instead expands queries by generating relevant contexts from pre-trained language models, without needing additional supervision.

- Existing query expansion techniques like pseudorelevance feedback expand queries with terms from initially retrieved passages. GAR instead extracts query-relevant knowledge directly from PLMs. 

- Other query reformulation studies use reinforcement learning to optimize reformulations based on downstream retrieval metrics. This can be slow and inefficient. GAR uses simpler sequence-to-sequence learning to generate contexts.

- For open-domain QA retrieval specifically, GAR outperforms both sparse methods (BM25) and previous state-of-the-art dense methods (REALM, DPR). It's also more lightweight and efficient than dense methods.

- When combined with BM25 retrieval, GAR achieves new SOTA results on two benchmark open-domain QA datasets under extractive QA setting. It also outperforms others under generative setup when using the same generative reader.

- GAR demonstrates generating diverse query contexts and fusing their results is beneficial compared to a single reformulation, unlike most prior work.

Overall, GAR introduces a novel query augmentation approach specialized for open-domain QA, without external resources. It pushes state-of-the-art for retrieval and end-to-end QA, despite using simple BM25 retrieval. The idea of expanding queries by extracting knowledge from PLMs could also benefit other retrieval tasks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions the authors suggest are:

- Exploring ways to further improve the query context generation, such as through multi-task learning or sampling multiple contexts instead of just greedy decoding. This could potentially enhance the diversity and quality of the generated contexts.

- Studying methods to jointly or iteratively optimize the query context generator and retriever modules, so the generator can be more aware of what contexts are useful for retrieval.

- Using more advanced fusion techniques to combine retrieval results from different generation-augmented queries, based on rank and score.

- Applying Generation-Augmented Retrieval (GAR) to other tasks beyond open-domain QA that involve text matching, such as conversation, search, etc. The key would be designing good generation targets as contexts for those tasks.

- Combining GAR with dense representations instead of just sparse retrieval. The authors suggest GAR could provide a method of handling semantics before retrieval, while dense methods handle it during retrieval.

- Additional hyperparameter tuning and experimentation to further improve performance.

So in summary, the main directions are improving the query generation, more joint training of generator and retriever, applying GAR to new domains, combining it with dense methods, and further optimization of the overall approach.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes Generation-Augmented Retrieval (GAR) for open-domain question answering, which augments queries with relevant contexts generated by pre-trained language models without external supervision. GAR generates contexts like answers, sentences containing answers, and titles of relevant passages as augmentation to the original question. Experiments on Natural Questions and TriviaQA datasets show GAR with BM25 achieves comparable or better performance than state-of-the-art dense retrieval methods like DPR, demonstrating the effectiveness of augmenting queries with generated contexts. GAR also achieves new state-of-the-art results on the datasets when combined with extractive or generative reading models. The paper demonstrates the potential of improving retrieval through query augmentation with unsupervised text generation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a method called Generation-Augmented Retrieval (GAR) for open-domain question answering. GAR augments queries (questions) by generating relevant contexts through text generation using pre-trained language models, without requiring external supervision or resources. Specifically, it generates contexts such as the answer, a sentence containing the answer, and the title of a Wikipedia page related to the question. These generated contexts are appended to the original question to create an expanded, generation-augmented query. 

Experiments on two QA datasets - Natural Questions and TriviaQA - demonstrate four key advantages of GAR: (1) It significantly improves retrieval accuracy over using just the original query with BM25. (2) GAR with BM25 achieves comparable or better accuracy than state-of-the-art dense retrieval methods like DPR. (3) GAR is complementary to dense methods like DPR, and combining them yields further improvements. (4) For end-to-end QA, GAR with an extractive reader achieves new SOTA results on the datasets. It also outperforms other retrieval methods when using the same generative reader. The simplicity and efficiency of GAR are additional benefits over dense methods. The paper concludes by discussing potential improvements and applicability of GAR to other text matching tasks.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes Generation-Augmented Retrieval (GAR) for open-domain question answering. The key idea is to augment queries (questions) with relevant contexts generated by pre-trained language models, without requiring external supervision or reinforcement learning. 

Specifically, the authors generate three types of query contexts using the question as input: the answer, the sentence containing the answer, and the title of a relevant passage. These are appended to the original question to form an expanded query, which is then used for retrieval over a passage collection using BM25. By generating query contexts likely to appear in relevant passages, GAR helps improve lexical overlap and retrieve more passages containing the answer. 

The authors show GAR significantly outperforms BM25 and query expansion baselines. Remarkably, GAR with BM25 achieves comparable or better accuracy than state-of-the-art dense methods like DPR. Furthermore, combining GAR with DPR obtains even better results, creating new SOTA for extractive QA. For generative QA, GAR also outperforms retrieval baselines when the same generative reader is used. The method is efficient as it avoids dense embeddings and reinforcement learning.


## What problem or question is the paper addressing?

 The paper is proposing a new method called Generation-Augmented Retrieval (GAR) for open-domain question answering. The main problem it aims to address is improving the effectiveness and efficiency of the retrieval stage in open-domain QA. 

Specifically, it notes two limitations of existing methods:

1) Sparse retrieval methods like BM25 rely solely on lexical overlap and fail to retrieve semantically relevant passages without word overlap.

2) Dense retrieval methods like DPR can capture semantics but are more computationally expensive in training and inference.

To address these issues, GAR augments the original query (question) with relevant contexts generated by pre-trained language models, without needing external resources or reinforcement learning. This allows sparse methods like BM25 to achieve better performance by expanding the query semantics. GAR is also efficient since it still uses sparse retrieval after augmenting the query.

Overall, the key question/problem is how to improve semantic matching in sparse retrieval for open-domain QA efficiently. GAR proposes query augmentation with generated text as a novel approach to address this.
