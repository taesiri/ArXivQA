# [Seeing a Rose in Five Thousand Ways](https://arxiv.org/abs/2212.04965)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to recover the intrinsic properties (geometry, texture, material) of an object category from a single image containing a few instances of that object. The key hypothesis is that by modeling the intrinsic object properties separately from extrinsic factors like pose and lighting, a generative model can learn to capture the distribution of shapes, textures, and materials of an object category from very limited observations in a single image.

The paper proposes a method to learn such a generative model of object intrinsics, which enables synthesizing new object instances and rendering them under novel views and lighting. The model represents shape with a neural SDF, texture with a neural albedo field, and material as a shininess parameter. It renders images using a differentiable physical renderer with sampled poses. The model components are trained adversarially to match the distribution of rendered images to that of the observed instances. 

The central research questions are:
(1) Can intrinsic object properties like shape and albedo be disentangled and learned from limited observations in a single image? 
(2) Can a generative model exploit such disentanglement of intrinsics and extrinsics to learn from limited data?
(3) Does modeling object intrinsics enable better generalization for reconstruction and synthesis compared to methods without such inductive biases?

The experiments aim to demonstrate the model's ability to capture object intrinsics and synthesize novel views and instances better than previous methods, thereby providing evidence for the hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a generative model that learns to recover the object intrinsics (distributions of geometry, texture, and material properties) of an object category from a single image containing multiple instances of that object type. 

Specifically, the key contributions are:

1. Proposing the problem of recovering complete object intrinsics, including geometry, texture, and materials, from a single image with instance masks.

2. Designing a generative framework with inductive biases based on disentangling object intrinsics and extrinsics. This allows the model to learn from very limited observations in a single image.

3. Demonstrating through experiments that the model can successfully learn object intrinsics for a diverse range of objects, each from just a single image. 

4. Showing that the learned object intrinsics enable superior performance on downstream tasks like shape reconstruction, image synthesis, novel view synthesis, and relighting.

In summary, the main contribution is developing a generative model that can recover complete object intrinsics from a single image, which significantly advances the capability of learning 3D generative models from very limited data.
