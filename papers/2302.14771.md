# [Generic-to-Specific Distillation of Masked Autoencoders](https://arxiv.org/abs/2302.14771)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to transfer both task-agnostic and task-specific knowledge from large vision transformer (ViT) models pre-trained with masked autoencoders to lightweight ViT models, in order to improve the performance of the lightweight models. The key hypotheses are:1) Large ViT models pre-trained with masked autoencoders contain both task-agnostic knowledge (general visual representations that can transfer across tasks) and task-specific knowledge (representations tuned for a particular task). 2) Transferring only the task-specific knowledge via standard knowledge distillation limits the performance of lightweight models, as they miss out on the beneficial task-agnostic knowledge.3) A two-stage distillation approach called "generic-to-specific distillation" (G2SD) can effectively transfer both types of knowledge - first the task-agnostic knowledge via a reconstruction-based distillation, then the task-specific knowledge via prediction-based distillation.4) This two-stage knowledge transfer will allow lightweight ViT models to achieve significantly better performance on downstream tasks compared to only using task-specific distillation.In summary, the central hypothesis is that G2SD enables more effective transfer of knowledge from large to small ViTs, unlocking stronger performance from lightweight models. The experiments aim to validate whether the proposed approach actually improves results across tasks like classification, detection and segmentation.


## What is the main contribution of this paper?

The main contribution of this paper appears to be proposing a two-stage knowledge distillation method called "generic-to-specific distillation" (G2SD) to transfer knowledge from large pre-trained masked autoencoder models to small vision transformer (ViT) models. Specifically, the key contributions seem to be:- Proposing a generic distillation stage to transfer task-agnostic knowledge from the teacher's decoder features to the student model. This helps the student model learn good general purpose representations.- Proposing a task-specific distillation stage to transfer task-specific knowledge and boost student performance on downstream tasks like classification, detection and segmentation. - Showing strong empirical results with G2SD, where a ViT-Small student achieves over 98% of its ViT-Base teacher's performance on ImageNet classification, COCO detection/segmentation and ADE20K segmentation.- Designing a simple but effective distillation strategy in the generic stage by aligning the student's predictions with the teacher's latent features at both visible and masked patches.- Demonstrating that G2SD transfers better occlusion invariance and representation similarity compared to single-stage distillation methods.So in summary, the key contribution is proposing and demonstrating the effectiveness of the two-stage G2SD distillation approach to transfer knowledge from large masked autoencoders to small ViTs. This provides a strong baseline for knowledge distillation in computer vision.
