# [How BPE Affects Memorization in Transformers](https://arxiv.org/abs/2110.02782v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How does the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) affect the ability and tendency of standard Transformer models to memorize training data?

The authors hypothesize that using a large subword vocabulary size will increase both the capacity and preference of Transformers to memorize training data, even when controlling for the number of learned parameters.

To summarize, the key research questions are:

1) How does BPE vocabulary size affect memorization capacity in Transformers? 

2) How does BPE vocabulary size affect memorization preference in Transformers when generalization is possible?

3) How does controlling for the number of learned parameters impact these effects?

The authors conduct experiments on fitting random labels, membership inference attacks, and training data recovery to analyze memorization from different angles. Their main finding is that larger BPE vocabularies consistently enable stronger memorization across tasks and architectures. After ruling out alternative explanations, they attribute this effect primarily to the reduction in sequence length enabled by larger vocabularies.
