# [ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for   Complicated Chart Reasoning](https://arxiv.org/abs/2402.12185)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-modal large language models (MLLMs) have shown promising progress on general vision-language tasks, but their ability to perform complex reasoning on chart data is still limited. Evaluating and improving MLLMs' chart understanding abilities remains an open challenge.

Proposed Solution:
- The paper introduces two main contributions:
    1) ChartX - A comprehensive multi-modal chart evaluation benchmark covering:
        - 18 chart types 
        - 22 topics/domains
        - 7 tasks (structural extraction, classification, QA, description, summarization, redrawing)
        - High quality data with multiple modalities
    2) ChartVLM - An interpretable chart-focused vision-language model featuring: 
        - Cascaded decoder mechanism to incorporate intermediate chart representations and enhance reasoning interpretability
        - Instruction adapter to dynamically select suitable decoders for different user-specified tasks
        - State-of-the-art performance on ChartX compared to existing MLLMs

Key Outcomes:
- Extensive experiments demonstrate ChartVLM's superior performance over both general-purpose and specialized MLLMs on the diverse ChartX benchmark
- Analysis shows improved reasoning ability from explicitly incorporating intermediate perception outputs 
- ChartX provides a rigorous way to benchmark chart understanding abilities of future MLLMs

The paper makes key contributions towards evaluating and enhancing MLLMs' capacity for complex chart-centric reasoning through the release of ChartX and ChartVLM model/code.
