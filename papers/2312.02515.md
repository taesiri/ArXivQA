# [ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a   Single GPU](https://arxiv.org/abs/2312.02515)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Fine-tuning large language models (LLMs) like LLaMA and ChatGLM is computationally expensive, requiring updating all parameters. Methods like LoRA reduce costs by only updating low-rank matrices. 
- However, current LoRA systems like Alpaca-LoRA support only single-job fine-tuning, lacking efficiency when training multiple jobs.

Proposed Solution - ASPEN:
- Presents ASPEN, a high-throughput framework to efficiently fine-tune multiple LoRA jobs on a single GPU by:

1. Multi-LoRA Trainer: Enables weight sharing across jobs using a BatchFusion technique which fuses inputs into a single batch to reduce kernel launch overheads. Saves 53% GPU memory compared to no sharing.

2. Adaptive Job Scheduler: Collects metrics like memory usage to schedule jobs optimally - improves throughput by 17% over methods like PEFT. Incorporates early stopping prediction and memory estimation models to reduce latency while preventing out-of-memory errors.

Key Contributions:  
- BatchFusion method to efficiently share pre-trained weights across concurrent LoRA fine-tuning jobs.
- Adaptive scheduling algorithm that reduces job turnaround time by 24%, end-to-end latency by 12%, and increases throughput by 17%, while preventing out-of-memory issues.
- Overall, ASPEN saves 53% GPU memory and boosts throughput by 17% over state-of-the-art methods when fine-tuning models like LLaMA and ChatGLM on a single GPU.
