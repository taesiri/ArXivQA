# [ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a   Single GPU](https://arxiv.org/abs/2312.02515)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces ASPEN, a high-throughput framework for fine-tuning large language models (LLMs) using the parameter-efficient Low-Rank Adaptation (LoRA) method. ASPEN allows efficient training of multiple LoRA jobs concurrently on a single GPU by sharing pre-trained weights through a novel BatchFusion technique and adaptive scheduling of jobs and resources. Specifically, BatchFusion fuses data from multiple jobs into one batch before each training iteration, enabling parallel training and reducing overhead. An adaptive job scheduler then prioritizes jobs and estimates resource usage to maximize throughput and prevent out-of-memory errors, while a profiler collects metrics to refine scheduling. Experiments highlight ASPEN's ability to boost throughput by 17% and reduce memory usage by 53% compared to existing methods when fine-tuning various LLMs. The scheduler reduces job turnaround time by 24% and end-to-end latency by 12% while preventing out-of-memory issues, with minimal impact on model performance. Overall, ASPEN advances efficient LLM fine-tuning in resource-constrained environments.
