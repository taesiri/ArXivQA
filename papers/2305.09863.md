# [Explaining black box text modules in natural language with language   models](https://arxiv.org/abs/2305.09863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we automatically generate natural language explanations for black box text modules, with only access to their inputs and outputs?The key points are:- The paper proposes a method called "Summarize and Score" (SASC) to generate natural language explanations for "text modules". - A "text module" is defined as any function that maps text to a continuous scalar value, like a neuron in a pretrained language model or a model predicting brain activity.- The explanations should describe what input text causes the largest positive responses from the module, without needing access to the module's internal structure or weights.- The method is evaluated on synthetic modules with known explanations, modules extracted from BERT, and modules predicting fMRI voxel responses. - For the synthetic modules, SASC is able to recover ground truth explanations with high accuracy under different noise conditions.- For BERT modules, SASC generates explanations comparable in quality to human-provided ones. The explained modules are often relevant for downstream tasks.- For fMRI voxel modules, SASC generates explanations pertaining to social concepts more than the BERT modules.So in summary, the key research question is whether natural language explanations can be automatically generated for black box text modules, with evaluations on synthetic, BERT, and fMRI modules suggesting this approach can work.
