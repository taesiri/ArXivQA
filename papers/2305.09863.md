# [Explaining black box text modules in natural language with language   models](https://arxiv.org/abs/2305.09863)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we automatically generate natural language explanations for black box text modules, with only access to their inputs and outputs?The key points are:- The paper proposes a method called "Summarize and Score" (SASC) to generate natural language explanations for "text modules". - A "text module" is defined as any function that maps text to a continuous scalar value, like a neuron in a pretrained language model or a model predicting brain activity.- The explanations should describe what input text causes the largest positive responses from the module, without needing access to the module's internal structure or weights.- The method is evaluated on synthetic modules with known explanations, modules extracted from BERT, and modules predicting fMRI voxel responses. - For the synthetic modules, SASC is able to recover ground truth explanations with high accuracy under different noise conditions.- For BERT modules, SASC generates explanations comparable in quality to human-provided ones. The explained modules are often relevant for downstream tasks.- For fMRI voxel modules, SASC generates explanations pertaining to social concepts more than the BERT modules.So in summary, the key research question is whether natural language explanations can be automatically generated for black box text modules, with evaluations on synthetic, BERT, and fMRI modules suggesting this approach can work.


## What is the main contribution of this paper?

The main contribution of this paper is presenting MPrompt, a method for automatically generating natural language explanations for black box text modules. The key ideas are:- Text modules are functions that map text to a continuous scalar value, such as neurons within a large language model. The method requires only blackbox access to the module's inputs and outputs.- MPrompt generates explanation candidates by extracting and summarizing the ngrams that elicit the largest positive responses from the module. It uses a large language model to perform the summarization. - MPrompt evaluates each candidate explanation by generating synthetic text based on the explanation and testing the module's response. The explanation with the largest difference in response between related and unrelated text is selected.- An explanation score rates how reliably the explanation describes the module.The method is evaluated by recovering ground truth explanations for synthetic modules, explaining internal modules in BERT, and explaining fMRI voxel responses to language. Overall, MPrompt provides a way to automatically generate natural language explanations for black box text modules without human involvement.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a method called SASC to automatically generate natural language explanations for text modules, which are functions that map text to a scalar value, and applies it to modules in BERT and fMRI voxel responses to gain insights about their behavior.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on interpreting and explaining neural network models:- The paper introduces a new method, Summarize and Score (SASC), for generating natural language explanations of any module that maps text to a continuous value. This builds on prior work on interpreting neural network representations and predictions, but focuses specifically on modules that process language.- SASC requires only blackbox access to the module, with no need to access internal weights or representations. Many prior interpretation methods rely on analyzing internal network components. The blackbox approach here allows explaining modules without access to internals.- The paper shows applications of SASC to synthetic modules, modules inside BERT, and fMRI voxel responses. Applying interpretation methods to fMRI data is novel and enables new neuroscience applications. - For synthetic modules, SASC recovers ground truth explanations fairly accurately under different noise conditions. Prior work on synthetic tests often uses simpler linear models, so testing on more complex blackbox modules is useful.- The fMRI results give new fine-grained semantic hypotheses about voxel selectivity, going beyond the broad semantic categories provided by prior encoding model analyses. This demonstrates the value of module-level vs network-level interpretation.- The BERT module explanations find relevant concepts for downstream tasks. Connecting representations to predictions is a key goal in interpretability. The analysis here provides supporting evidence that SASC identifies meaningful explanations.- The paper does not require human-annotated explanations for evaluation. Using automatically generated synthetic explanations and task relevance avoids issues with human label bias/noise.Overall, introducing a new blackbox explanation method and showing applications to fMRI data are significant contributions relative to prior work. The analyses demonstrate ways SASC can provide new insights into complex blackbox modules across different domains.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Testing the generalizability of SASC to a wider range of text modules beyond BERT and fMRI. The authors suggest the method could potentially be used to explain other kinds of black box text modules, but more experiments are needed.- Improving the faithfulness of the generated explanations by better capturing the full behavior of modules rather than just the top responses. The authors mention extending SASC to explain more aspects of a module's responses.- Explaining circuits of interacting modules rather than individual modules in isolation. The authors propose building on ideas like Conmy et al. 2023 to explain relationships between modules. - Applying SASC in more scientific domains like social science, medicine, etc. The authors suggest the method could be useful for interpretability in a variety of scientific pipelines.- Using the fMRI explanations to design follow-up experiments that test the proposed selectivity of different brain regions. The authors suggest the voxel explanations could serve as testable hypotheses.- Developing metrics beyond synthetic data scoring to better evaluate explanation faithfulness. The authors note reliance on synthetic data generation as a limitation.- Studying the environmental implications of the added computational costs of explanation methods like SASC. The authors suggest the increased transparency may enable model optimization that reduces overall footprint.In summary, the main suggested directions are enhancing the flexibility and faithfulness of SASC, applying it to new domains like science and medicine, and leveraging the explanations it provides to enables further experiments and analyses.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces Summarize and Score (\methodlongunderlineds, \method), a method for automatically generating natural language explanations of black box text modules. A text module is defined as any function that maps text to a continuous scalar value, such as a neuron within a large language model (LLM) or a model predicting brain activity. \method requires no access to the module's internals. It first generates explanation candidates by extracting and summarizing the ngrams that elicit the largest responses from the module. It then evaluates each candidate by generating synthetic text data based on the explanation and testing the module's response. The explanation with the largest difference in response between related and unrelated text is selected. \method is evaluated on synthetic modules, modules within BERT, and modules predicting fMRI voxel responses. It often recovers ground truth explanations on synthetic data and generates plausible explanations for BERT modules and fMRI voxels. The method enables interpreting black box modules like those in LLMs and the brain in a natural language format without human involvement.
