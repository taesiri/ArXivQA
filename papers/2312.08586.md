# [Estimating calibration error under label shift without labels](https://arxiv.org/abs/2312.08586)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from this paper:

This paper addresses the challenging problem of evaluating a machine learning model's calibration when there is a distribution shift between the training data (source) and the deployment data (target), specifically label shift, and when there are no labels available for the target data. Label shift refers to changes in the marginal label distribution between source and target, while the conditional input-label distributions remain unchanged. To tackle this, the authors propose the first consistent and asymptotically unbiased estimator for calibration error under label shift without target labels. Their method relies on importance re-weighting the labeled source data to account for the label distribution shift. They derive the variance of this estimator, present an algorithm to calculate it, and compare it to a Monte Carlo estimate. The authors demonstrate the effectiveness of their proposed calibration error estimator under label shift on diverse datasets, models, weight estimators, and degrees of shift. The experiments on medical imaging, language, and wildlife classification datasets confirm that their estimator can reliably assess model calibration on unlabeled target data under varying conditions. A key advantage is integrating state-of-the-art weight estimators from the domain adaptation literature. The estimator could be improved further as these methods enhance. Overall, this is an important contribution towards evaluating model reliability and trustworthiness when deploying machine learning systems in the real world.
