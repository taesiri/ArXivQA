# [Estimating calibration error under label shift without labels](https://arxiv.org/abs/2312.08586)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from this paper:

This paper addresses the challenging problem of evaluating a machine learning model's calibration when there is a distribution shift between the training data (source) and the deployment data (target), specifically label shift, and when there are no labels available for the target data. Label shift refers to changes in the marginal label distribution between source and target, while the conditional input-label distributions remain unchanged. To tackle this, the authors propose the first consistent and asymptotically unbiased estimator for calibration error under label shift without target labels. Their method relies on importance re-weighting the labeled source data to account for the label distribution shift. They derive the variance of this estimator, present an algorithm to calculate it, and compare it to a Monte Carlo estimate. The authors demonstrate the effectiveness of their proposed calibration error estimator under label shift on diverse datasets, models, weight estimators, and degrees of shift. The experiments on medical imaging, language, and wildlife classification datasets confirm that their estimator can reliably assess model calibration on unlabeled target data under varying conditions. A key advantage is integrating state-of-the-art weight estimators from the domain adaptation literature. The estimator could be improved further as these methods enhance. Overall, this is an important contribution towards evaluating model reliability and trustworthiness when deploying machine learning systems in the real world.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reliable uncertainty estimation and model calibration are critical for machine learning systems, especially in safety-critical applications. 
- Calibration error (CE) measures the alignment between a model's predicted probabilities and its actual accuracy.
- Existing CE estimators assume access to labels from the target distribution. However, this assumption is often violated due to dataset shift between training (source) and deployment (target) distributions.
- The paper focuses on label shift - changes in marginal label distribution between source and target, while the conditional input distribution given a label remains unchanged. Label shift commonly occurs in real-world scenarios.
- Estimating CE under label shift without target labels poses a key challenge that has not been addressed before.

Proposed Solution:
- The paper proposes the first consistent and asymptotically unbiased estimator of CE under label shift without requiring labels from the shifted target distribution. 
- The estimator employs importance re-weighting to account for the label shift when estimating CE on the source distribution. It relies on state-of-the-art unsupervised domain adaptation methods to obtain importance weights.
- The variance of the proposed estimator is formally derived, providing insights into its reliability. This applies both when labels are available (source domain) and under label shift (target domain).

Main Contributions:
- Proposes the first method to estimate CE under label shift without target labels, enabling assessing model calibration in this challenging yet practical scenario (ยง3)  
- Demonstrates the effectiveness of the estimator on diverse datasets, models, types and intensities of label shift (ยง4.1, ยง4.2)
- Ablates the influence of sample size, ratio of source/target samples, etc. showing the approach is reliable across different conditions (ยง4.3)
- Derives variance of the estimator, and empirically validates it aligns with Monte Carlo estimates (Appendix)

The proposed method facilitates assessing model calibration under dataset shifts commonly encountered in practice, without needing expensive annotations. The estimator's favorable statistical guarantees make it suitable for safety-critical applications.
