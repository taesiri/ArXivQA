# [Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image](https://arxiv.org/abs/2307.10984)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn a zero-shot, single view, metric depth model that can generalize to unseen cameras and scenes?The key points are:- Existing monocular depth estimation methods either learn metric depth specific to a dataset/camera model, or learn affine-invariant depth that lacks metric information. - The paper aims to learn a model that can estimate metric depth in a zero-shot manner, generalizing to new scenes and camera models not seen during training.- To achieve this, the paper proposes:1) A canonical camera space transformation to resolve metric ambiguity issues from different cameras during training. 2) Training with a large and diverse dataset of 8M images from 11 datasets covering different scenes and 10K+ camera models.3) A random proposal normalization loss to improve depth accuracy.4) Evaluating on multiple unseen datasets to demonstrate zero-shot generalization ability.In summary, the central hypothesis is that through the proposed canonical camera space transformation and large-scale diverse training, the model can learn to estimate metric depth in a zero-shot manner for new scenes and camera models. The experiments aim to demonstrate this generalization ability.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a canonical camera space transformation method to solve the metric depth ambiguity caused by varying camera parameters during training. This enables learning a robust metric depth model from large-scale mixed training data. 2. It introduces a random proposal normalization loss to further improve depth accuracy by emphasizing local contrasts.3. The trained metric depth model achieves state-of-the-art performance on 7 zero-shot benchmarks and enables high-quality metric 3D reconstruction from single images. 4. The predicted metric depths can benefit downstream applications like monocular SLAM by reducing scale drift.In summary, the key contribution is proposing solutions to train a single view metric depth model from large diverse data that generalizes zero-shot across scenes and camera models. This enables accurate metric 3D reconstruction from single images in the wild. The predicted metric depths also improve downstream tasks like SLAM.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points of the paper:The paper proposes a canonical camera space transformation method to enable training monocular depth estimation models on large-scale mixed datasets with many different camera models, allowing the models to achieve robust zero-shot metric depth prediction on new scenes and camera models not seen during training.


## How does this paper compare to other research in the same field?

This paper presents a novel method for learning a zero-shot, single view, metric depth estimation model from monocular images. Here are some key comparisons to other related works:- Most prior works on monocular depth estimation focus on either metric depth trained on a single dataset/camera model, or affine-invariant depth trained on mixed datasets. This paper proposes a method to learn a metric depth model from large-scale mixed training data, achieving both generalization and metric accuracy.- Compared to metric depth methods like Adabins and NeWCRFs, this paper's model shows better generalization to unseen datasets/cameras without finetuning. It also demonstrates robustness on a diverse set of indoor and outdoor scenes.- Compared to affine-invariant methods like MiDaS and LeReS, this model can recover metric 3D structure rather than up to an unknown scale. This enables metrology applications and improves downstream tasks like SLAM.- The proposed canonical camera space transformation is a simple yet effective way to handle varying cameras in mixed training. It avoids directly encoding camera models like CamConv.- The random proposal normalization loss improves over standard losses by emphasizing local scale-invariant geometry.- By training on a large 8M image dataset, this model achieves state-of-the-art performance on depth generalization benchmarks, outperforming recent methods like HDN and DPT.- The experiments demonstrate practical applications like SLAM improvement and in-the-wild metrology that benefit from the learned metric depths.Overall, the key novelty is the ability to learn a single image metric depth model with strong generalization. This is enabled by the canonical space training strategy and large-scale data. The experiments show accurate metric 3D reconstruction on diverse real images.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Developing better methods to solve the metric ambiguity issues in monocular depth estimation when training on diverse camera models. The authors propose a canonical camera space transformation, but suggest this could be further improved. - Exploring different network architectures and self-supervised methods to take advantage of the large-scale mixed training data. The authors show improved results by using ConvNeXt, but there may be room for bigger gains.- Applying the metric depth prediction model to more downstream tasks beyond SLAM and 3D reconstruction. For example, using it for occlusion-aware profscene completion, novel view synthesis, etc.- Improving the speed and efficiency of the model to make it more practical for real-time applications like autonomous vehicles. The current model involves resizing images which is computationally expensive.- Testing the generalization abilities on an even more diverse set of internet images captured completely in the wild. The Flickr images used for metrology were still somewhat constrained.- Developing techniques to estimate camera intrinsics directly from the images themselves, rather than relying on metadata. This could expand applicability.- Exploring the use of predicted metric depths as extra training signal for multi-view stereo and SLAM systems to improve their performance. - Applying the method to video inputs for temporally consistent metric depth estimation.So in summary, the main directions are improving the camera invariance, scaling up with more advanced networks and data, testing on more challenging wild image sets, and integrating the metric depths to benefit various applications.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method for learning to predict metric 3D depth from a single image in a zero-shot transferable manner. The key ideas are 1) proposing a canonical camera space transformation module to handle the ambiguity in metric depth prediction caused by varying camera intrinsics, enabling large-scale mixed data training, 2) a random proposal normalization loss to enhance local depth contrast and accuracy, and 3) training on a large dataset of 8 million images from 11 diverse datasets covering thousands of camera models. Experiments demonstrate state-of-the-art performance on multiple depth prediction benchmarks. A key advantage is the ability to predict metric 3D depth on random internet images and improve downstream tasks like monocular SLAM mapping quality. The method achieves top performance in the 2nd Monocular Depth Estimation Challenge. In summary, the paper presents an effective approach to learn a robust and accurate zero-shot metric depth prediction model from single images through canonical space training and losses tailored for metric depth learning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a method to learn a zero-shot, single view, metric depth model from a single image. The key ideas are using a canonical camera space transformation during training to resolve ambiguity issues from varying camera models, and training on a large and diverse dataset of 8 million images to achieve strong generalization. The canonical camera space transforms all training images to appear as if captured by the same camera model. This is done by either rescaling the input images or the ground truth depth labels. The transformation resolves supervision and appearance conflicts caused by different camera intrinsics. After this transformation, the model is trained on a large 11-dataset training set covering diverse indoor and outdoor scenes and thousands of camera models. The model learns to implicitly understand camera models and bridge image appearance to real-world scale. At inference, a de-canonical transformation recovers the original metric scale. Further improvements come from a proposed random proposal normalization loss to enhance local depth contrasts. Evaluations show state-of-the-art performance on 7 zero-shot benchmarks. The method enables accurate metric 3D reconstruction from internet images. It also improves monocular SLAM systems by resolving scale drift.
