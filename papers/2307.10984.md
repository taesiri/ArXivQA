# [Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image](https://arxiv.org/abs/2307.10984)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn a zero-shot, single view, metric depth model that can generalize to unseen cameras and scenes?The key points are:- Existing monocular depth estimation methods either learn metric depth specific to a dataset/camera model, or learn affine-invariant depth that lacks metric information. - The paper aims to learn a model that can estimate metric depth in a zero-shot manner, generalizing to new scenes and camera models not seen during training.- To achieve this, the paper proposes:1) A canonical camera space transformation to resolve metric ambiguity issues from different cameras during training. 2) Training with a large and diverse dataset of 8M images from 11 datasets covering different scenes and 10K+ camera models.3) A random proposal normalization loss to improve depth accuracy.4) Evaluating on multiple unseen datasets to demonstrate zero-shot generalization ability.In summary, the central hypothesis is that through the proposed canonical camera space transformation and large-scale diverse training, the model can learn to estimate metric depth in a zero-shot manner for new scenes and camera models. The experiments aim to demonstrate this generalization ability.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a canonical camera space transformation method to solve the metric depth ambiguity caused by varying camera parameters during training. This enables learning a robust metric depth model from large-scale mixed training data. 2. It introduces a random proposal normalization loss to further improve depth accuracy by emphasizing local contrasts.3. The trained metric depth model achieves state-of-the-art performance on 7 zero-shot benchmarks and enables high-quality metric 3D reconstruction from single images. 4. The predicted metric depths can benefit downstream applications like monocular SLAM by reducing scale drift.In summary, the key contribution is proposing solutions to train a single view metric depth model from large diverse data that generalizes zero-shot across scenes and camera models. This enables accurate metric 3D reconstruction from single images in the wild. The predicted metric depths also improve downstream tasks like SLAM.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points of the paper:The paper proposes a canonical camera space transformation method to enable training monocular depth estimation models on large-scale mixed datasets with many different camera models, allowing the models to achieve robust zero-shot metric depth prediction on new scenes and camera models not seen during training.
