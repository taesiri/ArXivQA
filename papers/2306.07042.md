# [Transformers learn through gradual rank increase](https://arxiv.org/abs/2306.07042)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:Do transformers exhibit incremental learning dynamics, where the difference between trained and initial weights progressively increases in rank?The authors aim to investigate whether transformers, which achieve state-of-the-art performance in many domains, demonstrate an incremental learning behavior when trained with small weight initializations. Specifically, they hypothesize that the rank of the difference between the trained weights and initial weights will gradually increase over the course of training. This would suggest that transformers learn in a stagewise manner, with increasingly complex functions learned over time.The authors provide theoretical analysis on a simplified transformer model to support the hypothesis. They also conduct experiments on vision transformers trained on image datasets that seem to exhibit the incremental rank increase phenomenon, even without the simplifying assumptions made in the theory.In summary, the central research question is whether transformers exhibit incremental learning dynamics and a bias towards low-rank weight updates during training. The authors aim to demonstrate this both theoretically for a simplified setting, and empirically for vision transformers.
