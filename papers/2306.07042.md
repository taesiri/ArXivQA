# [Transformers learn through gradual rank increase](https://arxiv.org/abs/2306.07042)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:Do transformers exhibit incremental learning dynamics, where the difference between trained and initial weights progressively increases in rank?The authors aim to investigate whether transformers, which achieve state-of-the-art performance in many domains, demonstrate an incremental learning behavior when trained with small weight initializations. Specifically, they hypothesize that the rank of the difference between the trained weights and initial weights will gradually increase over the course of training. This would suggest that transformers learn in a stagewise manner, with increasingly complex functions learned over time.The authors provide theoretical analysis on a simplified transformer model to support the hypothesis. They also conduct experiments on vision transformers trained on image datasets that seem to exhibit the incremental rank increase phenomenon, even without the simplifying assumptions made in the theory.In summary, the central research question is whether transformers exhibit incremental learning dynamics and a bias towards low-rank weight updates during training. The authors aim to demonstrate this both theoretically for a simplified setting, and empirically for vision transformers.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It identifies incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank over the course of training. - It provides a theoretical analysis of these dynamics for a simplified transformer architecture with diagonal weight matrices and small initialization, proving that the rank increases by at most 1 in each stage.- It conducts experiments on vision transformers that support the theory and also demonstrate incremental rank growth even without the simplifying assumptions, showing the phenomenon can occur in practice.In summary, the key contribution is using theory and experiments to demonstrate and characterize the incremental low-rank learning dynamics in transformer models. This sheds light on the training process and implicit biases of transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on skimming the paper, here is a one sentence summary:The paper provides theoretical and experimental results suggesting that transformers exhibit incremental learning dynamics during training, where the difference between the trained weights and random initial weights progressively increases in rank over the course of training.
