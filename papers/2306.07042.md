# [Transformers learn through gradual rank increase](https://arxiv.org/abs/2306.07042)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:Do transformers exhibit incremental learning dynamics, where the difference between trained and initial weights progressively increases in rank?The authors aim to investigate whether transformers, which achieve state-of-the-art performance in many domains, demonstrate an incremental learning behavior when trained with small weight initializations. Specifically, they hypothesize that the rank of the difference between the trained weights and initial weights will gradually increase over the course of training. This would suggest that transformers learn in a stagewise manner, with increasingly complex functions learned over time.The authors provide theoretical analysis on a simplified transformer model to support the hypothesis. They also conduct experiments on vision transformers trained on image datasets that seem to exhibit the incremental rank increase phenomenon, even without the simplifying assumptions made in the theory.In summary, the central research question is whether transformers exhibit incremental learning dynamics and a bias towards low-rank weight updates during training. The authors aim to demonstrate this both theoretically for a simplified setting, and empirically for vision transformers.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:- It identifies incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank over the course of training. - It provides a theoretical analysis of these dynamics for a simplified transformer architecture with diagonal weight matrices and small initialization, proving that the rank increases by at most 1 in each stage.- It conducts experiments on vision transformers that support the theory and also demonstrate incremental rank growth even without the simplifying assumptions, showing the phenomenon can occur in practice.In summary, the key contribution is using theory and experiments to demonstrate and characterize the incremental low-rank learning dynamics in transformer models. This sheds light on the training process and implicit biases of transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on skimming the paper, here is a one sentence summary:The paper provides theoretical and experimental results suggesting that transformers exhibit incremental learning dynamics during training, where the difference between the trained weights and random initial weights progressively increases in rank over the course of training.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of transformers and their training dynamics:- The key contribution is showing empirically and theoretically that transformers exhibit incremental learning dynamics when trained with small initialization. This builds on prior work that showed incremental learning in simple linear networks, but is novel in extending the theory and experiments to the transformer architecture.- The theoretical analysis makes simplifying assumptions like diagonal weight matrices and small initialization in order to make the analysis tractable. These assumptions deviate from practice, so an open direction is removing them. The empirical results on vision transformers suggest the dynamics can still emerge in practice without the assumptions.- The authors connect their findings to the LoRA method, which fine-tunes models by training low-rank perturbations to pretrained weights. An exciting avenue for future work based on this is using the theory of incremental learning to explain and improve algorithms like LoRA. - Prior work has shown low-rank bias can emerge in nonlinear networks when the target function depends on a low-dimensional subspace. This paper lends more evidence for the prevalence of low-rank bias, by showing it arises dynamically in transformers. An open direction is to characterize the implicit bias induced in the function space.- Overall, the paper makes both theoretical and empirical contributions towards understanding the training dynamics of transformers. It relates incremental learning in transformers to a body of literature on deep linear networks. The dynamics are empirically shown to hold more broadly than the theory suggests. Exciting future directions are removing the simplifying assumptions made, and better understanding the implicit regularization induced.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Examining the connection between the incremental learning dynamics they observe and the LoRA algorithm for model compression. They suggest it may be possible to explain and improve LoRA by better understanding the incremental dynamics of large transformers.- Developing a theoretical understanding of the implicit bias in function space of transformers whose weights are a low-rank perturbation of random initialization. The authors show empirically that the difference between trained and initialized weights is low-rank, but do not provide a theoretical analysis of how this impacts the function space. - Extending the theoretical results beyond the simplifying assumptions made, such as small initialization, diagonal weight matrices, and gradient flow training. The experimental results indicate the dynamics may hold more broadly, so an interesting direction is to develop theory without these assumptions.- Considering different architectures beyond the transformer, and different training objectives beyond standard supervised learning. The incremental dynamics may manifest in other models and tasks.- Providing a more fine-grained characterization of the dynamics, for example better understanding how the rank increases in each stage, or how the stages relate to properties of the data distribution.Overall, the main suggested directions aim to build on the initial theory and experiments to develop a more complete understanding of incremental learning dynamics in large models like transformers. Both extending the theory and connecting to practice seem like fruitful avenues for future work.


## Summarize the paper in one paragraph.

 Here is a summary of the key points in the paper:The paper studies the training dynamics of transformers under the simplifying assumptions of small weight initialization and diagonal weight matrices in the attention heads. It shows theoretically and experimentally that transformers exhibit "incremental learning" dynamics, where the weights are initialized near zero, remain close to zero for some time, and then rapidly change and converge to new small values. This happens in stages, with the number of large weights increasing by at most one per stage. The theoretical analysis makes assumptions of diagonal weight matrices and studies the gradient flow dynamics, showing the evolution is governed by an algorithm that increments the sparsity in stages. Experiments on toy transformers verify the theoretical assumptions and results. Experiments on vision transformers without the simplifying assumptions also exhibit incremental learning, suggesting the theory may extend more broadly. Overall, the paper provides new theoretical and empirical evidence that transformers can learn incrementally when initialized small.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:This paper studies the training dynamics of transformers under the simplifying assumptions of small weight initialization and diagonal weight matrices. The authors prove that with these assumptions, transformers exhibit an incremental learning behavior, where the weights remain close to saddle points for long periods, with abrupt transitions between saddle points. Specifically, they show that the difference between the trained weights and initialized weights has a rank that increases by at most one during each transition between saddle points. This indicates that the model complexity increases gradually during training. While the theoretical results rely on simplifying assumptions, experiments on vision transformers demonstrate that similar dynamics occur without these assumptions, even when training with standard techniques like Adam optimization and larger initialization scales. The empirical results show the difference between trained and initialized weights has low rank, and the rank increases gradually, indicating incremental learning still arises. Overall, this work provides both theory and experiments suggesting transformers have an intrinsic bias towards incremental learning when trained with gradient-based methods.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to analyze the training dynamics of transformer models. The key idea is to consider a simplified setting where the attention matrices are restricted to be diagonal. In this setting, the authors prove that gradient descent training proceeds in stages, where in each stage the rank of the attention matrices increases by at most 1. Specifically, they show that the dynamics can be captured by an algorithm that greedily activates 1 more diagonal entry of the attention matrices in each stage. The proof relies on analyzing the gradient flow and showing that:(1) The weights remain close to a saddle point with fixed support for a long time O(log(1/init_scale)). (2) Then there is a short nonlinear phase O(1) where the support increases by 1 and the weights converge to a new saddle point.This result provides a theoretical explanation for the phenomenon of incremental learning in transformers under simplifying assumptions on the architecture and initialization. The authors also conduct experiments showing incremental rank growth arises in practice.


## What problem or question is the paper addressing?

 Based on the introduction, this paper appears to be addressing the following main question:Are there incremental learning dynamics when training a transformer architecture? In other words, do the weights of a transformer exhibit a form of gradual, stepped increase in complexity as the model trains, rather than changing in a continuous, unstructured way?The motivation seems to be that past theoretical work has shown this kind of incremental learning behaviour in simple linear networks, but it has been difficult to analyze for complex nonlinear architectures like transformers. The authors aim to provide both theory and experiments to demonstrate that transformers do in fact learn in an incremental, stepped way when trained with gradient-based methods and small weight initialization.Specifically, the main contributions seem to be:1) Providing a theoretical result showing incremental rank growth in a simplified transformer model with diagonal weight matrices and very small initialization. 2) Conducting experiments on full vision transformers that support the theory and also indicate that the incremental dynamics can occur even without the simplifying assumptions.So in summary, this paper aims to understand and characterize the training dynamics in transformers through the lens of incremental rank growth of the weights. The combination of theory and experiments on simplified and full models provides evidence that transformers exhibit a gradual, stepped learning behaviour.
