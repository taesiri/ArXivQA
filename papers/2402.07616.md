# [Anchor-based Large Language Models](https://arxiv.org/abs/2402.07616)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Large language models (LLMs) require substantial GPU memory, which increases with longer input text lengths. This limits their practical usage in scenarios with constrained computational resources. 

- Caching the keys/values of historical tokens avoids redundant computation but takes up GPU memory during inference. As sequence length grows in tasks like question answering, more memory is needed to cache the increasing number of tokens.

Proposed Solution:
- Introduce Anchor-based LLMs (AnLLMs) with an anchor-based self-attention network (AnSAN) and inference strategy to compress sequence information into an "anchor token".

- AnSAN uses specialized attention masks to force models to aggregate information into the last token during training. This token serves as the anchor encapsulating the overall meaning.

- During inference, only anchor token keys/values are cached while non-anchor tokens are discarded after each sub-sequence. This significantly reduces the memory overhead.

Main Contributions:
- Demonstrate an effective method to reduce GPU memory consumption during LLM inference by over 90% with minimal accuracy loss.

- Achieve up to 3.5x faster inference compared to non-caching approaches by reusing anchor token caches instead of recomputing for all tokens.

- Introduce the novel concepts of anchor tokens and AnSAN for continually pre-training models to automatically learn sequence compression abilities. 

- Show promising results across various question answering datasets and analysis revealing the compatibility of anchor-based attention with full attention.

In summary, the paper makes significant strides in optimizing large language models for efficiency gains without substantially sacrificing performance, enabled by the new idea of anchor tokens. The feasibility of deploying LLMs on resource-limited devices seems closer than before.
