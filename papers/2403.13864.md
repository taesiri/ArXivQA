# [Optimal Transport for Fairness: Archival Data Repair using Small   Research Data Sets](https://arxiv.org/abs/2403.13864)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
With increasing regulations like the AI Act requiring fairness in AI systems, there is a need for algorithms that can repair unfairness in training data. Typically only a small "research" dataset is labeled with protected attributes (S) while large volumes of unlabeled "archival" data exist. The paper addresses the problem of how to repair the unlabeled archival data to remove dependence between features (X) and protected attributes (S), for each value of unprotected attributes (U).

Proposed Solution:
The paper proposes a novel optimal transport (OT) based approach. Fairness is defined based on conditional independence between X and S given U, for each value of U. The method involves first designing OT plans using the small, labeled research dataset to identify "fair" target distributions that are close to the original conditionals but independent of S. These plans map data points to the fair targets. A key contribution is interpolating the supports which significantly reduces computational complexity and allows off-sample repair. The designed plans are then applied to sequentially repair new unlabeled archival data points as they arrive, without needing to redesign the repair each time.

Main Contributions:
- Definition of fairness based on conditional independence given unprotected attributes
- OT-based repair method using interpolated supports for efficiency 
- Repair plans designed on small labeled dataset and applied to unlabeled archival data (off-sample repair)
- Experimental validation on simulated data and Adult benchmark dataset
- Analysis of performance vs amount of labeled data and resolution of supports

In summary, the paper presents an efficient and practical data repair technique to remove unfair dependence applicable to large volumes of unlabeled archival data. It is designed to be computationally efficient for deployment in real systems.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes an optimal transport-based method to repair unfairness in large volumes of streaming archival data by designing repair plans on a small set of labelled research data and applying them in a computationally efficient way to the unlabelled archival data.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel method for off-sample, optimal transport (OT)-based data repair. Specifically:

- They present a distributional repair method suitable for repairing off-sample data points (i.e. data not used to design the repair) by interpolating and transporting distributions rather than individual points. 

- They demonstrate that this method can achieve comparable performance to state-of-the-art on-sample repair methods in both simulated and real datasets. 

- They show that effective repairs can be designed using only a small set of labelled "research data", allowing the repair to then be efficiently applied at scale to large volumes of unlabelled "archival data".

- They analyze the operating conditions and performance of the method, studying the influence of the amount of research data and the resolution of the interpolated supports used in the repair.

In summary, the key contribution is a computationally efficient OT-based data repair method that can generalize to repair new off-sample data, while only requiring a small sample of labelled data to design the repair. This has important implications for being able to address fairness at scale in real-world deployed systems.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Optimal transport
- Data repair
- Conditional independence 
- Fairness 
- Barycentric repair target
- Distributional repair
- Off-sample repair
- Archival data
- Research data
- Conditional dependence metrics
- Kernel density estimation
- Interpolation
- Operating conditions
- Stationarity 

The paper proposes a novel method for "off-sample" data repair to achieve fairness, where only a small labeled "research" dataset is used to design optimal transport plans to remove dependence between protected attributes and features. These plans are then applied to repair unlabeled "archival" data by interpolating and transporting the empirical distributions. Key aspects examined are the conditional independence definition of fairness, the barycentric repair target, distributional vs geometric repair approaches, and performance under different sample sizes and support resolutions. The method aims to enable efficient, scalable fairness repair of torrents of archival data.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper defines fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). Why is this conditional definition preferred over an unconditional definition? What are the implications of using this conditional definition?

2. The paper proposes a distributional repair method suitable for off-sample points. How does this method work and how is it different from the geometric repair method discussed? What are the advantages of the proposed distributional repair? 

3. The paper discusses operating conditions like research data size ($n_R$) and resolution of interpolated pmfs ($n_Q$). How do these parameters impact the performance of the proposed method? What guidelines are provided in the paper for setting these parameters?

4. The proposed method performs repairs stratified by feature. What is the motivation behind this? How does this impact computational complexity during the repair design and application stages?

5. The paper assumes the protected attributes ($s|u$ labels) of the archive data are known or can be accurately estimated. What provisions are there in the AI Act regarding gathering of sensitive attribute data? How can the method be extended for unlabeled archive data?

6. The paper adopts the Wasserstein barycenter as the repair target following previous works. What are other possible repair target choices? What would be the tradeoffs with using alternative repair targets?

7. What are the key assumptions made by the paper regarding stationarity of data, representativeness of research data etc? How do violations of these assumptions impact performance of the proposed method?

8. How exactly is the interpolation of empirical pmfs done in the paper? What role does the kernel bandwidth parameter play here? How sensitive is the method to this parameter?

9. The paper demonstrates comparable performance to state-of-the-art on-sample repairs. What are some ways the individual fairness of repairs can be further improved? 

10. The paper shows significant computational savings by using reduced-state interpolated supports. What is the impact of increased resolution ($n_Q$) on data damage and individual fairness? How can this tradeoff be analyzed?
