# [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference   Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large vision-language models (LVLMs) like LLaVA have become popular for multimodal tasks involving images/videos and language. However, LVLMs are computationally expensive as they transform images into hundreds of tokens to process jointly with text. 
- The paper analyzes the attention patterns in LLaMA and finds that image tokens receive much lower attention compared to text tokens in deeper layers, suggesting inefficient usage of image tokens. For example, in LLaVA 1.5's deep layers, image tokens receive only 0.21% of the attention that prompt tokens receive.

Proposed Solution: 
- The paper proposes FastV, a method to dynamically prune redundant image tokens during inference to reduce computations. 
- FastV ranks image tokens by their averaged attention score at layer K. The bottom R% tokens are pruned for subsequent layers, removing them from attention and feedforward computations.

Main Contributions:
- Identify and analyze the inefficient visual attention phenomenon in popular LVLMs like LLaVA.
- Propose FastV, a versatile inference optimization method to significantly reduce FLOPS without sacrificing performance by dynamically pruning image tokens.
- Validate FastV's effectiveness across models like LLaVA and QwenVL on tasks including captioning, VQA, video QA and embodied reasoning. For example, FastV reduces LLaVA-1.5-13B's FLOPS by 45% with minimal performance drop.

In summary, the paper provides useful insights into LVLMs' inefficient image token usage and introduces FastV as an effective plug-and-play inference accelerator for real-world LVLM deployment.
