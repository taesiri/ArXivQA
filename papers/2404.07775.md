# [Discourse-Aware In-Context Learning for Temporal Expression   Normalization](https://arxiv.org/abs/2404.07775)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Temporal expression (TE) normalization is an important task for information extraction pipelines. It involves identifying temporal expressions in text and normalizing them into a standard format like TimeML.
- Traditional rule-based systems are limited in their applicability to new domains and languages. Machine learning approaches require large amounts of labeled data.
- Recent large language models (LLMs) have shown strong capabilities for downstream NLP tasks through in-context learning with just a few examples, without needing explicit training. However, their potential for TE normalization has not been explored.

Proposed Solution:
- The paper proposes a discourse-aware in-context learning approach to leverage recent LLMs for TE normalization without any training. 
- It uses the GPT-3.5-turbo model and explores various sample selection strategies to construct informative prompts.
- The prompt design incorporates task descriptions, document context through a temporal window, and few-shot examples to teach the model. 
- Different strategies are explored to retrieve the most relevant training samples as few-shot examples for a given test input.

Main Contributions:
- First work to demonstrate the feasibility of large language models for temporal expression normalization through in-context learning.
- Exploration of various sample selection strategies and prompt designs tailored for the TE normalization task.
- Competitive performance to specialized normalization models, especially in non-standard domains.
- Analysis providing insights into domain adaptation capabilities, number of examples, context lengths etc.
- Demonstration of multilingual normalization by selecting examples across languages.
- Error analysis revealing remaining challenges w.r.t implicit and relative expressions.

In summary, the paper shows that large language models can learn to perform temporal normalization without any training by carefully designing the prompt and selecting informative examples. This opens up their applicability to settings with limited resources.
