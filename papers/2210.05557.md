# [OPERA: Omni-Supervised Representation Learning with Hierarchical   Supervisions](https://arxiv.org/abs/2210.05557)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How to train a better model with both self-supervised and fully supervised signals?

The key points are:

- The pretrain-finetune paradigm facilitates the success of self-supervised learning (SSL), which achieves better transferability than fully supervised learning (FSL). 

- Given the availability of massive labeled data, it is worth exploring whether combining SSL signals with FSL signals can further improve transferability.

- The paper proposes Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA) as a solution to effectively combine SSL and FSL.

So in summary, the main research question is how to combine self-supervised and fully supervised signals in an effective framework like OPERA to train better models that have improved transferability over models trained with only SSL or FSL alone. The paper aims to demonstrate the advantages of the proposed OPERA framework.
