# [Scaling Laws for Neural Machine Translation](https://arxiv.org/abs/2109.07740v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research questions addressed in this paper are:

1) Does the encoder-decoder architecture for NMT share the same scaling law function as language models? The authors find that a univariate law based only on total number of parameters is insufficient, and propose a bivariate law treating encoder and decoder parameters separately.

2) How does the naturalness (original vs translated text) of source/target data affect scaling behavior? The authors find target-original data benefits more from scaling than source-original data.

3) Do improvements in cross-entropy from scaling translate to improvements in generation quality? The authors find cross-entropy improvements correlate with BLEU/BLEURT gains on target-original data, but this correlation breaks down on source-original data.

In summary, the key hypotheses tested are:

- Scaling laws for NMT are different than language models and require treating encoder/decoder parameters separately.

- Scaling behavior depends heavily on dataset composition in terms of original vs translated text.

- Improvements in cross-entropy with scaling lead to better generation quality on target-original but not source-original data.

The experiments and analyses appear designed to validate these hypotheses about how scaling laws differ for encoder-decoder NMT models compared to language models, and the importance of dataset composition.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a new bivariate scaling law that can accurately model the relationship between model size (encoder and decoder parameters) and cross-entropy loss for encoder-decoder NMT models. 

2. Demonstrating that the scaling behavior depends significantly on the composition of the training and test data, specifically whether the source or target side contains more "natural" text.

3. Showing that improvements in cross-entropy loss translate to gains in BLEU/BLEURT scores on target-original test sets, but this correlation breaks down on source-original test sets. 

4. Providing extensive empirical analysis of scaling laws on multiple language pairs, using both encoder/decoder scaling as well as symmetric scaling. The proposed scaling laws are shown to accurately predict performance.

5. Raising concerns about the effect of synthetic or machine-translated data on model scaling. The findings indicate training on target-original (backtranslated) data leads to saturation, while source-original training data yields models that quickly overfit.

In summary, this paper provides new insights into scaling laws for NMT by proposing a more accurate bivariate scaling law and highlighting the significant impact of dataset composition on scaling behavior and model quality. The empirical analysis validates predictable scaling for NMT encoder-decoder models.
