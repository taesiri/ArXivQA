# [Sparse Backpropagation for MoE Training](https://arxiv.org/abs/2310.00811)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we train Mixture-of-Experts (MoE) models effectively when the routing function used for sparse computation is non-differentiable?

The key points are:

- MoE models use a discrete routing function to assign different parts of the model selectively to each input. This allows for sparse computation and exceptional scalability. 

- However, the routing function is non-differentiable, which poses challenges for training MoE models via backpropagation. Existing methods like straight-through estimators require dense computation, undermining the purpose of sparsity.

- Typical MoE training neglects certain gradient terms to allow sparse routing, but this may result in suboptimal training.

- This paper proposes SparseMixer, which provides reliable gradient approximations for MoE routing that maintain sparsity. It is inspired by numerical ODE methods.

- Experiments on machine translation and pre-training tasks show SparseMixer accelerates training convergence and improves performance compared to prior MoE training approaches.

So in summary, the core research problem is providing an effective way to train MoE models that maintains the sparsity benefits of routing, which prior methods failed to achieve. SparseMixer proposes a solution based on insights from numerical analysis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Introducing SparseMixer, a novel method to bridge the gap between sparse expert routing in Mixture-of-Experts (MoE) models and dense backpropagation required for training neural networks. 

- Drawing inspiration from numerical ODE solvers, SparseMixer provides reliable gradient approximations for expert routing in MoE models, without needing to activate all experts. This maintains the computational sparsity and efficiency of MoE models.

- Specifically, SparseMixer leverages the midpoint method, a second-order ODE solver, to get precise gradient estimates with negligible computational overhead. 

- Applying SparseMixer to Switch Transformer architecture for machine translation and language model pretraining. Results show SparseMixer speeds up training convergence by up to 2x and allows MoE models to consistently outperform dense models after training.

- Analysis providing intuitions about why existing straight-through gradient estimators are incompatible with sparse MoE models. Comparisons to ablations validate the importance of SparseMixer's designs.

In summary, the main contribution appears to be the proposal of SparseMixer to enable precise and sparse gradients for training MoE models, which improves convergence and performance over prior MoE training techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper: The paper proposes SparseMixer, a novel gradient estimation method that enables reliable backpropagation and training of sparsely activated Mixture-of-Expert models without sacrificing scalability.
