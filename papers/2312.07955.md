# [Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking](https://arxiv.org/abs/2312.07955)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method called PoisonCAM to defend against backdoor attacks on self-supervised learning (SSL) models. The key idea is to accurately detect and remove poisoned samples from the training data so that the resulting SSL model is free from the injected backdoor trigger. Specifically, the authors introduce a "Cluster Activation Masking" technique to precisely locate candidate backdoor trigger patches in images. This works by analyzing changes in cluster assignments when random image regions are masked out. Based on the detected triggers, a poison classifier is trained to identify and filter poisonous samples from the dataset. Experiments on ImageNet-100 and STL-10 show that PoisonCAM significantly outperforms prior arts in defending against backdoor attacks, while preserving model accuracy on clean data. For example, it improves trigger detection accuracy from 3% to 96% and boosts poisoned data accuracy by 4.7%, compared to the state-of-the-art defender. By effectively "erasing" the backdoor from training data, PoisonCAM facilitates learning a robust and trustworthy SSL model.
