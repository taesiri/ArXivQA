# [Flashback: Understanding and Mitigating Forgetting in Federated Learning](https://arxiv.org/abs/2402.05558)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates the problem of forgetting in federated learning (FL). Under severe data heterogeneity between clients, FL algorithms like FedAvg suffer from forgetting - where knowledge gained at one round is lost in later rounds. This leads to unstable training and slow convergence. The paper analyzes where forgetting happens in FL - at the local client updates and at the server-side aggregation. It also proposes a new metric to measure round-to-round forgetting.

Proposed Solution: 
The paper proposes Flashback, an FL algorithm that uses dynamic knowledge distillation to mitigate forgetting. At the client side, Flashback distills knowledge from the global model into each local model to prevent them from forgetting global knowledge during local training. It uses the label counts of models as a proxy for their knowledge levels and adapts the distillation loss accordingly. At the server side, Flashback aggregates models as usual, then further distills knowledge from past models into the new global model, again using dynamic distillation based on label counts. 

Contributions:
- Analyzes forgetting issue in FL, showing it happens during client updates and server aggregation
- Proposes metric to measure round-wise forgetting 
- Introduces Flashback algorithm with dynamic distillation to mitigate forgetting
- Shows Flashback achieves faster convergence and fewer rounds to target accuracy
- Demonstrates Flashback works well even with small public dataset for server distillation

In summary, the paper provides useful insights into forgetting in FL and presents an effective solution called Flashback that leverages dynamic knowledge distillation to address this problem. Flashback mitigates forgetting, leading to faster and more stable federated learning.
