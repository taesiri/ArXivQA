# [Exploring Continual Learning of Compositional Generalization in NLI](https://arxiv.org/abs/2403.04400)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing work on compositional generalization in natural language inference (NLI) uses offline training, unlike how humans continuously acquire knowledge. 
- This paper introduces a new task called Continual Compositional Generalization (C2Gen) NLI to evaluate models' ability to perform compositional generalization in a continual learning scenario.

Proposed Solution:
- Construct a compositional NLI dataset and split it into sequential stages to simulate continual learning. 
- Propose two tasks: compositional inference (CI) to test unseen compositional inferences, and primitive recognition (P) to test recognition of constituent inferences.
- Show models fail on C2Gen due to catastrophic forgetting. Benchmark continual learning methods like experience replay and knowledge distillation to mitigate forgetting.
- Analyze impact of learning order, task dependencies and difficulties. Show importance of ordering easy inference types before complex ones.

Main Contributions:  
- Introduce novel C2Gen NLI task to evaluate compositional generalization in continual learning scenario.  
- Construct compositional NLI dataset and tasks for evaluating C2Gen ability.
- Show forgetting is a key challenge for C2Gen and benchmark continual learning methods to combat it. 
- Demonstrate ordering primitives and inference types by dependencies and difficulties enhances compositional generalization in continual learning.
- Provide insights into designing better continual and compositional learning algorithms using principles identified in analyses.

In summary, this paper proposes a new challenging task to evaluate compositional generalization in NLI in continual learning settings, provides analysis into the key issues, and offers insights that can guide future research into algorithm design for enhancing compositional and continual learning abilities.
