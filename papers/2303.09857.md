# [Dual-path Adaptation from Image to Video Transformers](https://arxiv.org/abs/2303.09857)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to efficiently transfer the representation power of pretrained image models like ViT and Swin Transformers to the video domain for video understanding tasks. 

Specifically, the authors aim to address two key questions:

1) Is it possible to transfer the parameters of an image foundation model to the video domain? 

2) Can the transferred model perform comparably to carefully designed video models that explicitly model the spatiotemporal nature of videos?

The authors argue that directly fine-tuning image models on video data does not work well since it ignores the temporal context. On the other hand, specialized video transformers require complex architectures and many parameters to model temporal dynamics. 

To address this, the paper proposes a novel dual-path adaptation method called DualPath that separates spatial and temporal adaptation paths, where lightweight adapter modules are employed to tune each path. This allows efficiently leveraging the spatial modeling of image models and adding temporal modeling capabilities.

The central hypothesis is that the proposed dual-path approach can transfer image models to video effectively, achieving comparable or better performance than specialized video models, while being extremely efficient in terms of computational cost and number of parameters.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposes a novel adapter-based dual-path parameter efficient tuning method called DualPath for video understanding. It consists of two distinct paths - a spatial path and a temporal path - to adapt image transformers for video action recognition with minimal trainable parameters.

2. The spatial path is designed to encode spatial contexts from individual frames by sparsely sampling frames and using parallel adapters. The temporal path captures temporal dynamics by constructing a grid-like frameset from multiple frames and using serial adapters. 

3. Introduces a simple yet effective grid-like frameset transform technique to enable temporal modeling in image transformers, by aggregating multiple low-resolution frames into a unified structure.

4. Implements and compares with several baselines adapting recent PETL approaches like VPT, AdaptFormer, ProTuning etc. for image-to-video transfer.

5. Demonstrates state-of-the-art performance on multiple action recognition benchmarks using CLIP-pretrained vision transformers, with significantly lower computational costs compared to prior video models and baselines.

In summary, the key novelty is the proposed DualPath approach that incorporates a dual-path design with grid-like frameset prompting into image transformers via lightweight adapters, for efficient and effective video understanding. The results show strong transferability of image models to video through this technique.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel dual-path adapter-based approach called DualPath to efficiently transfer image transformers pretrained on large image datasets to video action recognition tasks using only a small number of additional trainable parameters.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in video action recognition:

- It builds on recent work in parameter-efficient transfer learning (PETL) for adapting large pretrained image models like ViT to video tasks. However, most prior PETL methods for video have still relied on some temporal modeling like 3D convolutions. This paper proposes a novel dual-path design to enable efficient spatiotemporal adaptation of ViT to video without 3D convolutions.

- The dual-path approach is inspired by prior two-stream architectures in CNNs, but this paper is the first to develop a two-stream design for vision transformers while maintaining efficiency. The spatial and temporal paths with lightweight adapters allow separate adaptation for spatial and temporal context.

- A key novelty is the grid-like frameset input to the temporal path, which aggregates multiple frames into a unified representation to mimic the self-attention mechanism's ability to model global relationships. This differs from prior work like inflating image models or using 3D convolutions for temporal modeling.

- Compared to recent methods like VideoPrompt and X-CLIP that require additional text encoder branches, this method accomplishes efficient spatiotemporal modeling using only the pretrained image model. It also does not have the computational cost scaling with temporal resolution like those methods.

- Extensive experiments demonstrate superior efficiency and performance compared to recent PETL baselines. Notably, it attains comparable or better accuracy to supervised video transformer models with orders of magnitude fewer parameters and FLOPs.

In summary, this paper introduces an innovative dual-path adapter-based approach to efficiently adapt image transformers to video with separate spatial and temporal adaptation. The grid-like frameset is a simple but effective technique to provide temporal modeling while maintaining efficiency. The experiments demonstrate promising results compared to prior art.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Expanding cross-domain transfer learning. The authors propose image-to-video transfer learning in this work, but suggest exploring other cross-domain transfers like vision-language models for video tasks or transferring 2D knowledge to 3D tasks.

- Leveraging large foundation vision-language models. The authors note that large pretrained vision-language models are now available and it could be interesting to utilize them for video understanding tasks.

- Improving spatial modeling from images to video. The authors were able to effectively transfer spatial context from images to video, but suggest further improvements could be made in spatial modeling for videos using techniques like the ones proposed in this paper.

- More complex temporal modeling. While the grid-like frameset technique worked well, the authors suggest exploring more complex temporal modeling techniques building on top of the methods proposed here.

- Pretraining video models more efficiently. The authors motivate their work by noting inefficient pretraining of video models. They suggest their methods could provide a foundation for more efficient video pretraining.

- Exploring prompt-based tuning. This work used adapter-based tuning, but prompt-based tuning could be another interesting direction.

In summary, the main directions are extending the cross-domain transfer learning explored here, leveraging large vision-language models, improving spatial and temporal modeling for videos, and doing more efficient video pretraining and tuning. The authors position their work as a starting point to enable future video understanding research in these areas.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes an efficient method for transferring image transformers, such as ViT and Swin, to video understanding tasks with minimal trainable parameters. The proposed DualPath method consists of separate spatial and temporal adaptation paths, where lightweight bottleneck adapters are employed in each transformer block. The spatial path is designed to encode spatial context from sparse frame inputs, while the temporal path transforms consecutive frames into a grid-like structure to capture temporal relationships. This allows leveraging the spatial modeling of image transformers and adding temporal modeling with minimal parameters. The method is compared to baselines adapted from recent parameter-efficient tuning approaches and demonstrates superior performance on action recognition benchmarks with extremely low compute. The dual-path design enables efficient spatiotemporal modeling and effective transfer of image transformers to video while maintaining low training and inference costs.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a novel dual-path adaptation method called DualPath to efficiently transfer image foundation models like ViT and Swin Transformers for video understanding. Previous adaptation methods tried to model both spatial and temporal contexts using a single unified module, but struggled to fully leverage the representation power of image transformers. The authors argue that using a dual-path architecture, commonly used in CNN video models, can help mitigate this. Their proposed DualPath method has separate spatial and temporal adaptation paths, each using lightweight bottleneck adapters in the transformer blocks. The spatial path adapts to the spatial context from individual frames, while the temporal path transforms multiple frames into a "grid-like" frameset to model relationships between frames over time. Especially the grid frameset design allows maintaining computational efficiency similar to single-stream architectures.

The authors compare DualPath to recent parameter-efficient tuning methods adapted for video and show it achieves comparable or better performance on action recognition benchmarks, with far fewer trainable parameters and computations than standard video transformers. For example, with a ViT-B/16 backbone DualPath attains 85.4% top-1 accuracy on Kinetics-400 using only 10M trainable parameters and 710 GFLOPs, compared to full video transformer models requiring hundreds of millions of parameters and 10x more computations. The results demonstrate image foundation models can be effectively transferred to video tasks through efficient dual-path adaptation.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel dual-path parameter efficient tuning method for video understanding called DualPath, which efficiently transfers the representation power of pretrained image transformers like ViT and Swin to the video domain with minimal trainable parameters. DualPath consists of two distinct paths - a spatial path and a temporal path. The spatial path uses sparsely sampled frames to encode spatial contexts from individual frames via adapters in each transformer block. The temporal path transforms consecutive frames into a grid-like frameset input to precisely imitate the capability of transformers to model relationships between tokens. It uses adapters in each block to encode temporal dynamics over frames. With this dual-path design, DualPath can perform spatiotemporal modeling for video understanding by tuning only a small number of additional parameters in the adapters while freezing the pretrained image model. Experiments on action recognition benchmarks demonstrate DualPath's effectiveness and efficiency compared to baselines and prior video models.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper aims to efficiently transfer the representation power of pretrained image models like ViT and Swin Transformers for the task of video understanding. 

- Prior work has adapted image models to video using methods like prompt tuning or adapters, but they treat spatial and temporal modeling jointly. This makes it hard to fully leverage the spatial modeling of image models. 

- The paper proposes a novel dual-path adaptation approach called DualPath, with separate spatial and temporal adaptation paths. This allows better transfer of image models' spatial modeling to video.

- For temporal modeling, they propose a "grid-like frameset" input that aggregates multiple frames into a unified representation. This helps the model learn relationships between frames like ViT learns between image patches.

- They implement several baselines using recent prompt/adapter methods and compare to them. Experiments on action recognition datasets show DualPath achieves better performance and efficiency than baselines.

- DualPath attains strong results compared to prior supervised video models, while using far fewer parameters and computations. This demonstrates effective adaptation of image models for video understanding.

In summary, the key innovation is the dual-path design and grid-like frameset to separately adapt the spatial and temporal modeling of image transformers for efficient video understanding.
