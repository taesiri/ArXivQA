# [Dual-path Adaptation from Image to Video Transformers](https://arxiv.org/abs/2303.09857)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to efficiently transfer the representation power of pretrained image models like ViT and Swin Transformers to the video domain for video understanding tasks. 

Specifically, the authors aim to address two key questions:

1) Is it possible to transfer the parameters of an image foundation model to the video domain? 

2) Can the transferred model perform comparably to carefully designed video models that explicitly model the spatiotemporal nature of videos?

The authors argue that directly fine-tuning image models on video data does not work well since it ignores the temporal context. On the other hand, specialized video transformers require complex architectures and many parameters to model temporal dynamics. 

To address this, the paper proposes a novel dual-path adaptation method called DualPath that separates spatial and temporal adaptation paths, where lightweight adapter modules are employed to tune each path. This allows efficiently leveraging the spatial modeling of image models and adding temporal modeling capabilities.

The central hypothesis is that the proposed dual-path approach can transfer image models to video effectively, achieving comparable or better performance than specialized video models, while being extremely efficient in terms of computational cost and number of parameters.
