# [Dual-path Adaptation from Image to Video Transformers](https://arxiv.org/abs/2303.09857)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to efficiently transfer the representation power of pretrained image models like ViT and Swin Transformers to the video domain for video understanding tasks. 

Specifically, the authors aim to address two key questions:

1) Is it possible to transfer the parameters of an image foundation model to the video domain? 

2) Can the transferred model perform comparably to carefully designed video models that explicitly model the spatiotemporal nature of videos?

The authors argue that directly fine-tuning image models on video data does not work well since it ignores the temporal context. On the other hand, specialized video transformers require complex architectures and many parameters to model temporal dynamics. 

To address this, the paper proposes a novel dual-path adaptation method called DualPath that separates spatial and temporal adaptation paths, where lightweight adapter modules are employed to tune each path. This allows efficiently leveraging the spatial modeling of image models and adding temporal modeling capabilities.

The central hypothesis is that the proposed dual-path approach can transfer image models to video effectively, achieving comparable or better performance than specialized video models, while being extremely efficient in terms of computational cost and number of parameters.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposes a novel adapter-based dual-path parameter efficient tuning method called DualPath for video understanding. It consists of two distinct paths - a spatial path and a temporal path - to adapt image transformers for video action recognition with minimal trainable parameters.

2. The spatial path is designed to encode spatial contexts from individual frames by sparsely sampling frames and using parallel adapters. The temporal path captures temporal dynamics by constructing a grid-like frameset from multiple frames and using serial adapters. 

3. Introduces a simple yet effective grid-like frameset transform technique to enable temporal modeling in image transformers, by aggregating multiple low-resolution frames into a unified structure.

4. Implements and compares with several baselines adapting recent PETL approaches like VPT, AdaptFormer, ProTuning etc. for image-to-video transfer.

5. Demonstrates state-of-the-art performance on multiple action recognition benchmarks using CLIP-pretrained vision transformers, with significantly lower computational costs compared to prior video models and baselines.

In summary, the key novelty is the proposed DualPath approach that incorporates a dual-path design with grid-like frameset prompting into image transformers via lightweight adapters, for efficient and effective video understanding. The results show strong transferability of image models to video through this technique.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel dual-path adapter-based approach called DualPath to efficiently transfer image transformers pretrained on large image datasets to video action recognition tasks using only a small number of additional trainable parameters.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in video action recognition:

- It builds on recent work in parameter-efficient transfer learning (PETL) for adapting large pretrained image models like ViT to video tasks. However, most prior PETL methods for video have still relied on some temporal modeling like 3D convolutions. This paper proposes a novel dual-path design to enable efficient spatiotemporal adaptation of ViT to video without 3D convolutions.

- The dual-path approach is inspired by prior two-stream architectures in CNNs, but this paper is the first to develop a two-stream design for vision transformers while maintaining efficiency. The spatial and temporal paths with lightweight adapters allow separate adaptation for spatial and temporal context.

- A key novelty is the grid-like frameset input to the temporal path, which aggregates multiple frames into a unified representation to mimic the self-attention mechanism's ability to model global relationships. This differs from prior work like inflating image models or using 3D convolutions for temporal modeling.

- Compared to recent methods like VideoPrompt and X-CLIP that require additional text encoder branches, this method accomplishes efficient spatiotemporal modeling using only the pretrained image model. It also does not have the computational cost scaling with temporal resolution like those methods.

- Extensive experiments demonstrate superior efficiency and performance compared to recent PETL baselines. Notably, it attains comparable or better accuracy to supervised video transformer models with orders of magnitude fewer parameters and FLOPs.

In summary, this paper introduces an innovative dual-path adapter-based approach to efficiently adapt image transformers to video with separate spatial and temporal adaptation. The grid-like frameset is a simple but effective technique to provide temporal modeling while maintaining efficiency. The experiments demonstrate promising results compared to prior art.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Expanding cross-domain transfer learning. The authors propose image-to-video transfer learning in this work, but suggest exploring other cross-domain transfers like vision-language models for video tasks or transferring 2D knowledge to 3D tasks.

- Leveraging large foundation vision-language models. The authors note that large pretrained vision-language models are now available and it could be interesting to utilize them for video understanding tasks.

- Improving spatial modeling from images to video. The authors were able to effectively transfer spatial context from images to video, but suggest further improvements could be made in spatial modeling for videos using techniques like the ones proposed in this paper.

- More complex temporal modeling. While the grid-like frameset technique worked well, the authors suggest exploring more complex temporal modeling techniques building on top of the methods proposed here.

- Pretraining video models more efficiently. The authors motivate their work by noting inefficient pretraining of video models. They suggest their methods could provide a foundation for more efficient video pretraining.

- Exploring prompt-based tuning. This work used adapter-based tuning, but prompt-based tuning could be another interesting direction.

In summary, the main directions are extending the cross-domain transfer learning explored here, leveraging large vision-language models, improving spatial and temporal modeling for videos, and doing more efficient video pretraining and tuning. The authors position their work as a starting point to enable future video understanding research in these areas.
