# [Dual-path Adaptation from Image to Video Transformers](https://arxiv.org/abs/2303.09857)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to efficiently transfer the representation power of pretrained image models like ViT and Swin Transformers to the video domain for video understanding tasks. 

Specifically, the authors aim to address two key questions:

1) Is it possible to transfer the parameters of an image foundation model to the video domain? 

2) Can the transferred model perform comparably to carefully designed video models that explicitly model the spatiotemporal nature of videos?

The authors argue that directly fine-tuning image models on video data does not work well since it ignores the temporal context. On the other hand, specialized video transformers require complex architectures and many parameters to model temporal dynamics. 

To address this, the paper proposes a novel dual-path adaptation method called DualPath that separates spatial and temporal adaptation paths, where lightweight adapter modules are employed to tune each path. This allows efficiently leveraging the spatial modeling of image models and adding temporal modeling capabilities.

The central hypothesis is that the proposed dual-path approach can transfer image models to video effectively, achieving comparable or better performance than specialized video models, while being extremely efficient in terms of computational cost and number of parameters.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposes a novel adapter-based dual-path parameter efficient tuning method called DualPath for video understanding. It consists of two distinct paths - a spatial path and a temporal path - to adapt image transformers for video action recognition with minimal trainable parameters.

2. The spatial path is designed to encode spatial contexts from individual frames by sparsely sampling frames and using parallel adapters. The temporal path captures temporal dynamics by constructing a grid-like frameset from multiple frames and using serial adapters. 

3. Introduces a simple yet effective grid-like frameset transform technique to enable temporal modeling in image transformers, by aggregating multiple low-resolution frames into a unified structure.

4. Implements and compares with several baselines adapting recent PETL approaches like VPT, AdaptFormer, ProTuning etc. for image-to-video transfer.

5. Demonstrates state-of-the-art performance on multiple action recognition benchmarks using CLIP-pretrained vision transformers, with significantly lower computational costs compared to prior video models and baselines.

In summary, the key novelty is the proposed DualPath approach that incorporates a dual-path design with grid-like frameset prompting into image transformers via lightweight adapters, for efficient and effective video understanding. The results show strong transferability of image models to video through this technique.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel dual-path adapter-based approach called DualPath to efficiently transfer image transformers pretrained on large image datasets to video action recognition tasks using only a small number of additional trainable parameters.
