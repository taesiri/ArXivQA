# [SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2309.07084)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve LiDAR-camera fusion for 3D object detection by introducing supervised learning to the fusion process?The key hypothesis is that by providing supervision on the fused LiDAR-camera features, the fusion process can be improved to extract more robust and higher quality features, which in turn improves 3D detection performance. Specifically, the paper proposes:1) A supervised training strategy called SupFusion that introduces auxiliary feature-level supervision to the LiDAR-camera fusion process using high-quality LiDAR features generated from an assistant model.2) A deep fusion module that better fuses LiDAR and camera features under the supervision of the high-quality LiDAR features. 3) A polar sampling method to enhance LiDAR data to generate more complete point clouds to facilitate extracting high-quality LiDAR features.Through experiments based on different LiDAR-camera detectors, the paper shows supervised fusion can consistently improve 3D detection accuracy, demonstrating the effectiveness of the proposed techniques.In summary, the core research question is how to improve LiDAR-camera fusion with supervision, and the key hypothesis is supervision on the fused features can enhance the fusion process and in turn boost 3D detection performance. The proposed SupFusion strategy and deep fusion module aim to address this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel supervised fusion training strategy called SupFusion for LiDAR-camera 3D object detection. This introduces auxiliary feature-level supervision to help guide the 3D/2D feature extraction and fusion process.2. A new data enhancement method called Polar Sampling that densifies sparse objects in the LiDAR data. This is used to generate higher quality features from an assistant model to provide supervision.3. A simple but effective deep fusion module that fuses LiDAR and camera features through stacked MLP blocks.4. Demonstrating consistent improvements of around 2% mAP on the KITTI benchmark by applying SupFusion and the deep fusion module to various LiDAR-camera detectors.In summary, the key ideas are using a supervised training approach with auxiliary feature-level supervision, densifying the LiDAR data to help generate better supervision, and proposing an effective deep fusion module. The combination of these techniques leads to noticeable gains in detection performance across different base detectors. The proposed methods aim to improve the learning and effectiveness of LiDAR-camera fusion for 3D object detection.
