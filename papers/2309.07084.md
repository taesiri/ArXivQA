# [SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2309.07084)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve LiDAR-camera fusion for 3D object detection by introducing supervised learning to the fusion process?The key hypothesis is that by providing supervision on the fused LiDAR-camera features, the fusion process can be improved to extract more robust and higher quality features, which in turn improves 3D detection performance. Specifically, the paper proposes:1) A supervised training strategy called SupFusion that introduces auxiliary feature-level supervision to the LiDAR-camera fusion process using high-quality LiDAR features generated from an assistant model.2) A deep fusion module that better fuses LiDAR and camera features under the supervision of the high-quality LiDAR features. 3) A polar sampling method to enhance LiDAR data to generate more complete point clouds to facilitate extracting high-quality LiDAR features.Through experiments based on different LiDAR-camera detectors, the paper shows supervised fusion can consistently improve 3D detection accuracy, demonstrating the effectiveness of the proposed techniques.In summary, the core research question is how to improve LiDAR-camera fusion with supervision, and the key hypothesis is supervision on the fused features can enhance the fusion process and in turn boost 3D detection performance. The proposed SupFusion strategy and deep fusion module aim to address this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel supervised fusion training strategy called SupFusion for LiDAR-camera 3D object detection. This introduces auxiliary feature-level supervision to help guide the 3D/2D feature extraction and fusion process.2. A new data enhancement method called Polar Sampling that densifies sparse objects in the LiDAR data. This is used to generate higher quality features from an assistant model to provide supervision.3. A simple but effective deep fusion module that fuses LiDAR and camera features through stacked MLP blocks.4. Demonstrating consistent improvements of around 2% mAP on the KITTI benchmark by applying SupFusion and the deep fusion module to various LiDAR-camera detectors.In summary, the key ideas are using a supervised training approach with auxiliary feature-level supervision, densifying the LiDAR data to help generate better supervision, and proposing an effective deep fusion module. The combination of these techniques leads to noticeable gains in detection performance across different base detectors. The proposed methods aim to improve the learning and effectiveness of LiDAR-camera fusion for 3D object detection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel supervised training strategy called SupFusion for LiDAR-camera fusion-based 3D object detection, which introduces auxiliary feature-level supervision using high-quality features from an assistant model trained on enhanced LiDAR data to improve the robustness and accuracy of the fusion model.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of LiDAR-camera fusion for 3D object detection:- The key contribution of this paper is proposing a supervised training strategy called SupFusion that introduces auxiliary feature-level supervision to improve the fusion process and boost detection accuracy. This is a novel approach as most prior work has focused on developing new network architectures or fusion techniques, without adding supervision to the fusion process itself.- For fusion techniques, this paper proposes a simple but effective deep fusion module with stacked MLP blocks, which helps continuously improve performance. This compares well to other learnable fusion methods like Transformer-based or attention-based fusion. The simplicity of the module makes it easy to integrate into various detectors.- For supervision, the use of a pretrained assistant model on enhanced LiDAR data to generate high-quality features for supervision is unique. Other works have used knowledge distillation but not in this way for LiDAR-camera fusion. The polar sampling method to enhance LiDAR data is also novel.- In terms of performance, the consistent 1-2% mAP improvements across multiple detectors on KITTI and nuScenes benchmarks are very compelling. This shows the broad applicability and effectiveness of the proposed SupFusion strategy.- The method is flexible and easy to integrate into existing detectors, as evidenced by the experiments on multiple architectures like SECOND, PointPillars, etc. This compares well to prior work that often introduces specialized or complex modules restrictive to certain detectors.In summary, the simplicity yet effectiveness of the proposed supervised training strategy is the key differentiator of this work from prior art. The gains are achieved without fundamentally changing the detector architectures. The results demonstrate this is a promising direction for further improving LiDAR-camera fusion in 3D detection.
