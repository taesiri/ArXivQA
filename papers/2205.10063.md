# [Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision   Transformers with Locality](https://arxiv.org/abs/2205.10063)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is:

How to enable efficient Masked Autoencoder (MAE) pre-training for Pyramid-based Vision Transformers with locality constraints? 

The key challenges are:

1) MAE's asymmetric encoder-decoder design relies on global self-attention in vanilla ViT, but Pyramid ViTs have locality constraints (e.g. shifted windows).

2) Random masking in MAE is incompatible with Pyramid ViTs' local windows. 

3) Existing methods like SimMIM use inefficient masking strategies for Pyramid ViTs.

To address this, the paper proposes a Uniform Masking (UM) strategy with two components:

1) Uniform Sampling (US): Sample patches uniformly from 2x2 grids to get sparse but compatible inputs. 

2) Secondary Masking (SM): Further randomly mask some sampled patches to increase task difficulty.

This enables efficient MAE pre-training for Pyramid ViTs like Swin and PVT, with competitive accuracy but much faster speed and lower memory than SimMIM.

In summary, the core research question is how to enable efficient MAE-style pre-training for popular Pyramid ViTs, which the proposed UM strategy tries to address.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a Uniform Masking (UM) strategy to enable Masked Autoencoder (MAE) pre-training for Pyramid Vision Transformers. UM contains Uniform Sampling (US) and Secondary Masking (SM). 

2. It shows that the proposed UM-MAE method can significantly improve pre-training efficiency (2x faster and uses 2x less GPU memory) compared to existing masked image modeling methods like SimMIM, while maintaining competitive fine-tuning performance.

3. It reveals and discusses empirical differences in behaviors between vanilla Vision Transformers (ViT) and Pyramid Vision Transformers under masked image modeling frameworks. Key findings are:

- Pyramid ViTs rely more on intermediate fine-tuning compared to vanilla ViT when transferring to dense prediction tasks. 

- Layerwise learning rate decay is crucial for vanilla ViT but harmful for Pyramid ViTs in fine-tuning.

Overall, the main contribution is proposing an effective and efficient strategy to enable MAE-style pre-training for popular Pyramid Vision Transformers, with extensive empirical analysis and insights on their behaviors. The improved efficiency and competitive accuracy make the method practical.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a method called Uniform Masking to enable efficient Masked Autoencoder (MAE) pre-training for Pyramid-based Vision Transformers, achieving competitive performance while significantly improving training speed and reducing memory requirements.
