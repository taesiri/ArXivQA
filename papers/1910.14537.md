# [Attention Is All You Need for Chinese Word Segmentation](https://arxiv.org/abs/1910.14537)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design an effective Chinese word segmentation (CWS) model that is fast and accurate using only simple components. The key hypotheses are:1) With a powerful enough encoder, a greedy decoder using only unigram features can achieve strong CWS performance compared to more complex models. 2) An attention-only model with the proposed Gaussian-masked Directional Transformer encoder and a biaffine scorer can provide state-of-the-art representations for CWS using greedy decoding.In particular, the paper proposes a new CWS model consisting of the Gaussian-masked Directional Transformer encoder and a biaffine scorer, using only greedy decoding and unigram features. The encoder is designed to capture localness, position and directional information well for CWS. This simple but effective model achieves state-of-the-art or comparable CWS performance on benchmark datasets while being very fast. The central hypothesis is that with a strong enough encoder, complex decoders and feature representations are unnecessary for accurate CWS.
