# [Attention Is All You Need for Chinese Word Segmentation](https://arxiv.org/abs/1910.14537)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design an effective Chinese word segmentation (CWS) model that is fast and accurate using only simple components. The key hypotheses are:1) With a powerful enough encoder, a greedy decoder using only unigram features can achieve strong CWS performance compared to more complex models. 2) An attention-only model with the proposed Gaussian-masked Directional Transformer encoder and a biaffine scorer can provide state-of-the-art representations for CWS using greedy decoding.In particular, the paper proposes a new CWS model consisting of the Gaussian-masked Directional Transformer encoder and a biaffine scorer, using only greedy decoding and unigram features. The encoder is designed to capture localness, position and directional information well for CWS. This simple but effective model achieves state-of-the-art or comparable CWS performance on benchmark datasets while being very fast. The central hypothesis is that with a strong enough encoder, complex decoders and feature representations are unnecessary for accurate CWS.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a new Chinese word segmentation model consisting only of attention mechanisms and a greedy decoder, aiming to strengthen the encoder to produce good representations for segmentation. - It introduces a new variant of the Transformer encoder called Gaussian-masked Directional (GD) Transformer. This uses Gaussian-masked directional multi-head attention to better capture local and directional information important for CWS. - It shows that with a strong enough encoder, the model only needs unigram features and greedy decoding to achieve state-of-the-art performance on CWS. Previous works relied on more complex decoders and feature representations.- The model achieves new state-of-the-art results on standard CWS datasets while also being extremely fast for segmentation, thanks to the simple greedy decoder.In summary, the main contribution is proposing a new way to build an efficient and accurate neural CWS model by focusing on strengthening the encoder and using simple decoding, rather than complex decoders or feature engineering. The GD-Transformer is introduced to improve encoding of key information for the task.
