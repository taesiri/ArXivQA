# [MLPs Compass: What is learned when MLPs are combined with PLMs?](https://arxiv.org/abs/2401.01667)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT have shown strong capabilities in capturing semantic information. However, it remains unclear what additional information simple MLP components can provide when combined with PLMs. 

- Recent works show MLPs can effectively capture structural features, even outperforming graph neural networks. But it is still an open question what is learned when MLPs are fused with powerful PLMs.

Methodology:
- The authors propose a simple yet effective probing framework with MLP components based on BERT's structure. 

- They introduce 10 probing tasks across 3 linguistic levels (surface, syntactic, semantic) to quantify the information gained from adding MLPs to PLMs.

- The framework keeps BERT weights frozen during probing and introduces MLPs between BERT representations and probe classifiers to attribute performance differences to the MLPs.

Results:
- Experiments show MLPs enhance PLMs' comprehension of linguistic structure, without needing to add structural bias.

- MLPs boost capability to capture surface, syntactic and semantic information. The enhancements are most consistent in higher layers of BERT.

- MLPs are most skilled at improving capture of syntactic and semantic information compared to surface tasks.

Conclusion:
- The paper provides interpretable insights into how basic MLPs can enhance PLMs' linguistic understanding, especially for syntactic and semantic structures.

- This sheds light on why adding MLPs improves PLM performance on tasks emphasizing certain language structures.

In summary, the key contribution is an analysis quantifying and explaining what additional linguistic information MLPs provide when combined with powerful PLMs like BERT, using a simple yet effective probing methodology.
