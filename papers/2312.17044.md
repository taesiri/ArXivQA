# [Length Extrapolation of Transformers: A Survey from the Perspective of   Position Encoding](https://arxiv.org/abs/2312.17044)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding":

Problem:
- Transformers have become the dominant architecture for many NLP tasks due to their superior ability to model complex dependencies in sequences. However, they suffer from a predefined context length limit (usually 512 or 1024 tokens) due to their quadratic complexity, which makes it difficult to apply them to long sequences. 
- Length extrapolation, i.e. training on short contexts while being able to inference on longer ones, seems to be the most feasible way to apply Transformers to long sequences without expensive training costs. However, Transformers fail to extrapolate well to longer contexts beyond those seen during training.

Proposed Solution:
- The paper comprehensively reviews methods for enhancing the length extrapolation of Transformers from the perspective of position encodings (PEs), which play a vital role in equipping Transformers with notion of order and have proven critical for extrapolation.
- The methods are organized based on whether they are absolute PEs (APE) or relative PEs (RPE), as well as if they were proposed before or after the rise of large language models (LLMs).
- For APEs, methods like sinusoidal PE, SHAPE, CAPE, complex PE, FLOATER are introduced, which leverage ideas like randomness and continuity to enhance extrapolation.
- For RPEs, methods like RPE bias, ALiBi, KERPLE, RoPE are elaborated, which are dominant recently due to inherent benefits for extrapolation.
- Specific sections are devoted to introductions of popular methods in the LLM era, including position interpolation methods and randomized PEs.

Main Contributions:
- Provides formal formulations and comparisons of different extrapolatable PEs from the birth of Transformer until now.
- Organizes scattered research efforts towards length extrapolation of Transformers in a unified perspective and notation, enabling easy understanding and selection between methods.  
- Summarizes mainstream PEs adopted by recent influential LLMs and dedicates separate sections to emerging methods and frontiers unique to the LLM era.
- Reveals limitations of existing work and discusses multiple open challenges to stimulate future research towards enhanced extrapolation of Transformers and LLMs.
