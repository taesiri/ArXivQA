# [FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment   Act Flows](https://arxiv.org/abs/2202.06633)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is exploring the potential of using dialog act information, specifically segment-level dialog acts called "segment acts", for evaluating open-domain dialog systems. The authors point out that hardly any previous dialogue evaluation methods explicitly model dialog acts, even though dialog acts convey important information about the function or intent of each utterance. To incorporate dialog act information into evaluation, the authors propose "segment acts" which extend dialog acts to the segment level within utterances, allowing for finer-grained modeling of utterance function.To enable use of segment acts, the authors create a new dataset called ActDial with segment act annotations on two existing dialog datasets. They then propose a new consensus-based dialog evaluation framework called FlowEval which utilizes segment act flows (sequences of segment acts). The key hypothesis seems to be that explicit modeling of segment act flows can improve open-domain dialog evaluation by providing useful complementary information compared to methods focused solely on semantic meaning. The authors test this via experiments on benchmark dialog datasets.In summary, the central research question is whether segment act flows can be effectively modeled to improve open-domain dialog evaluation, providing complementary benefits to existing semantic meaning-focused methods. The key hypothesis is that FlowEval, through its use of segment acts, can achieve state-of-the-art or comparable correlation with human judgments of dialog quality.


## What is the main contribution of this paper?

 This paper proposes FlowEval, a new framework for evaluating dialogue systems. The key contributions are:1. They propose modeling dialogue flows using "segment acts" - extending dialog acts to the segment level within an utterance. This allows capturing finer-grained communicative intent compared to regular dialog acts. 2. They collect a large dataset of segment act annotations on two dialogue datasets - ConvAI2 and DailyDialog. This segment act dataset enables modeling segment act flows.3. They propose a consensus-based dialogue evaluation framework called FlowEval that can assess dialogues without requiring reference responses. It works by retrieving pseudo-references from a dataset based on segment act and content features, then evaluating the dialogue using consensus of the pseudo-references. 4. Experiments show FlowEval reaches state-of-the-art or comparable performance to existing methods on three dialogue datasets. It also provides complementary information to existing semantic similarity metrics.5. This is the first work to adapt consensus-based evaluation from image captioning to open-domain dialog. The proposed framework allows integrating various kinds of features for retrieval and assessment.Overall, the key novelty is using fine-grained segment acts and a consensus-based framework to evaluate dialog without needing true references. This provides a new direction for improving open-domain dialogue evaluation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes FlowEval, a new dialogue evaluation framework that uses segment act flows and a consensus-based approach to better correlate with human judgments of dialogue quality.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in dialogue evaluation:- It proposes using segment-level dialogue acts ("segment acts") for evaluation. Most prior work uses utterance-level dialogue acts, which are more coarse-grained. Modeling dialogue flows with segment acts provides more fine-grained functional information about each utterance.- It develops a new consensus-based framework for open-domain dialogue evaluation. This allows incorporating various features like segment acts into the assessment, without requiring reference responses. Most prior automatic evaluation metrics rely on comparing to ground truth responses.- The consensus-based method retrieves pseudo-references and evaluates using their consensus. This is a novel way to make traditional reference-based metrics usable without true references. - Experiments show combining the proposed segment act features with existing methods like BERTscore and DynaEval improves correlation with human judgement. This demonstrates segment acts provide complementary information to lexical/semantic metrics.- A new segment act dataset is collected and released to enable modeling segment act flows. This adds a valuable resource for the community.Overall, the key novelties are using segment acts rather than utterance acts, proposing a consensus framework to avoid needing true references, and showing segment acts improve existing methods. This points towards better utilizing dialogue structure for evaluation.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing better datasets for open-domain dialogue evaluation. The authors point out limitations in their segment act dataset, including relatively small scale and distribution skewed towards certain acts like questions and information. They suggest improving the dataset in future work.- Further exploration of the consensus-based evaluation framework. The authors propose this as a novel framework but say more analysis is needed to determine concrete patterns of when it succeeds or fails. They suggest investigating new perspectives and features that could work with the consensus approach.- Combining evaluation metrics in more sophisticated ways. The authors show simple averaging of metrics can improve performance, but more research is needed on optimal combination methods.- Incorporating other types of features beyond just segment acts. The consensus framework is compatible with many features like sentiment, engagement, etc. Exploring these could further improve evaluation.- Developing more reference-free evaluation methods. The consensus framework provides a path to making reference-based metrics usable without true references. More research in this direction could lead to revival of traditional metrics.- Analysis to determine more concrete failure patterns of automatic metrics. The authors show sample failure cases but more analysis is needed to find systematic weaknesses.In summary, the main suggestions are around improving datasets, further exploration of the consensus framework, combining complementary metrics, and developing more reference-free evaluation methods. Robustness of metrics across domains is also highlighted as an open challenge.


## Summarize the paper in one paragraph.

 The paper proposes FlowEval, a consensus-based dialogue evaluation framework that utilizes segment act flows for open-domain dialogues. The key ideas are:1) Extend dialog acts to the segment level, called segment acts, to capture finer-grained functional information about utterances. A sequence of segment acts is called a segment act flow.2) Crowdsource a large dataset of segment act annotations on two popular open-domain dialogue datasets - ConvAI2 and DailyDialog. This forms a new dataset called ActDial.3) Propose FlowEval, the first consensus-based framework for open-domain dialogue evaluation. It works by first obtaining segment act flows for a dialogue, then using segment act and content features to retrieve pseudo-references, and finally evaluating the dialogue by its consensus with the pseudo-references.  4) Experiments on 3 datasets show FlowEval reaches best or comparable performance against strong baselines. Analysis indicates segment acts provide complementary information to semantic-focused metrics. The consensus mechanism is also shown to be effective with a reasonably sized retrieval set.In summary, the key novelty is using segment act flows and a consensus-based framework to enable better automatic open-domain dialogue evaluation. The work provides new insights on how to approach this challenging task.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes FlowEval, a new framework for evaluating open-domain dialogues using segment act flows. Segment acts extend the concept of dialog acts from the utterance level to the segment level, allowing finer-grained modeling of an utterance's communicative function. The authors crowdsource a large dataset of segment act annotations on two dialogue datasets, ConvAI2 and DailyDialog, and name this dataset ActDial. To utilize segment act flows for evaluation, they propose a consensus-based framework called FlowEval. For a dialogue to be evaluated, FlowEval first obtains the segment act flow, then retrieves similar pseudo-reference dialogues from a dataset based on segment act and content features. Finally, it assesses the dialogue using metrics comparing it to the retrieved pseudo-references. Experiments on three dialogue datasets show that FlowEval achieves strong correlation with human judgement, outperforming or matching state-of-the-art methods. Additional analysis provides evidence that segment act flows capture complementary information to semantic similarity metrics. The consensus framework also shows promise for adapting traditional reference-based metrics like BLEU and BERTScore to work without reference dialogues.In summary, this paper makes three main contributions. First, it proposes modeling segment-level acts, which provide finer-grained functional information than utterance-level dialog acts. Second, it develops the first consensus-based framework for open-domain dialogue evaluation, which retrieves pseudo-references allowing reference-free assessment. Third, experiments demonstrate strong performance and the ability of FlowEval's segment act modeling to complement existing semantic similarity metrics. The paper provides promising evidence that segment acts and consensus-based approaches could enable better automatic evaluation of open-domain dialogues.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a consensus-based dialogue evaluation framework called FlowEval that utilizes segment act flows for open-domain dialog evaluation. The key contributions are:1. They propose the concept of segment acts, which extends dialog acts from the utterance level to the segment level within an utterance. This allows capturing finer-grained conversational functions. They crowdsource a large dataset of segment act annotations called ActDial. 2. They develop a consensus-based framework for dialog evaluation that does not require reference dialogues. Given a test dialogue, it first predicts the segment acts using a classifier. It then retrieves nearest neighbour pseudo-references from a pool of human dialogues based on segment act and content features extracted by ActBERT and RoBERTa respectively. Finally, it assesses the test dialogue by comparing it to the pseudo-references using metrics like BLEU and BERTScore.3. Experiments on 3 dialog datasets show FlowEval achieves high correlation with human judgments, outperforming strong baselines. Using both segment act and content features helps provide complementary information. The consensus framework also enables making reference-based metrics like BERTScore reference-free.In summary, the key idea is to model finer-grained dialog functions through segment acts, and perform reference-free evaluation through a consensus framework over retrieved nearest neighbours as pseudo-references. This shows promising results compared to previous methods.


## What problem or question is the paper addressing?

 The paper proposes a new framework called FlowEval for evaluating open-domain dialog systems. Here are the key points about the problem and solution:- Open-domain dialog evaluation remains challenging due to the one-to-many nature and lack of reliable references. Existing methods focus on semantic meaning but don't explicitly model dialog functions/acts. - The paper explores using segment-level dialog acts, called "segment acts", as extra information for evaluation. Segment acts extend utterance-level dialog acts to the finer granularity of utterance segments.- It proposes FlowEval, the first consensus-based dialog evaluation framework. FlowEval does not require reference responses. It extracts segment act and content features, retrieves nearest neighbor pseudo-references, and assesses the dialog against the consensus of pseudo-references.- The paper crowdsources a large dataset of segment act annotations on two dialog datasets to enable segment act modeling.- Experiments show FlowEval achieves best or comparable correlation with human scores against strong baselines. It also provides complementary information beyond semantic-focused metrics. The consensus mechanism is shown to be promising.In summary, the key problem is improving open-domain dialog evaluation, especially without relying on unavailable true reference responses. The paper explores segment acts and a consensus-based approach as a potential solution direction.
