# FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment   Act Flows

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is exploring the potential of using dialog act information, specifically segment-level dialog acts called "segment acts", for evaluating open-domain dialog systems. The authors point out that hardly any previous dialogue evaluation methods explicitly model dialog acts, even though dialog acts convey important information about the function or intent of each utterance. To incorporate dialog act information into evaluation, the authors propose "segment acts" which extend dialog acts to the segment level within utterances, allowing for finer-grained modeling of utterance function.To enable use of segment acts, the authors create a new dataset called ActDial with segment act annotations on two existing dialog datasets. They then propose a new consensus-based dialog evaluation framework called FlowEval which utilizes segment act flows (sequences of segment acts). The key hypothesis seems to be that explicit modeling of segment act flows can improve open-domain dialog evaluation by providing useful complementary information compared to methods focused solely on semantic meaning. The authors test this via experiments on benchmark dialog datasets.In summary, the central research question is whether segment act flows can be effectively modeled to improve open-domain dialog evaluation, providing complementary benefits to existing semantic meaning-focused methods. The key hypothesis is that FlowEval, through its use of segment acts, can achieve state-of-the-art or comparable correlation with human judgments of dialog quality.


## What is the main contribution of this paper?

This paper proposes FlowEval, a new framework for evaluating dialogue systems. The key contributions are:1. They propose modeling dialogue flows using "segment acts" - extending dialog acts to the segment level within an utterance. This allows capturing finer-grained communicative intent compared to regular dialog acts. 2. They collect a large dataset of segment act annotations on two dialogue datasets - ConvAI2 and DailyDialog. This segment act dataset enables modeling segment act flows.3. They propose a consensus-based dialogue evaluation framework called FlowEval that can assess dialogues without requiring reference responses. It works by retrieving pseudo-references from a dataset based on segment act and content features, then evaluating the dialogue using consensus of the pseudo-references. 4. Experiments show FlowEval reaches state-of-the-art or comparable performance to existing methods on three dialogue datasets. It also provides complementary information to existing semantic similarity metrics.5. This is the first work to adapt consensus-based evaluation from image captioning to open-domain dialog. The proposed framework allows integrating various kinds of features for retrieval and assessment.Overall, the key novelty is using fine-grained segment acts and a consensus-based framework to evaluate dialog without needing true references. This provides a new direction for improving open-domain dialogue evaluation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes FlowEval, a new dialogue evaluation framework that uses segment act flows and a consensus-based approach to better correlate with human judgments of dialogue quality.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in dialogue evaluation:- It proposes using segment-level dialogue acts ("segment acts") for evaluation. Most prior work uses utterance-level dialogue acts, which are more coarse-grained. Modeling dialogue flows with segment acts provides more fine-grained functional information about each utterance.- It develops a new consensus-based framework for open-domain dialogue evaluation. This allows incorporating various features like segment acts into the assessment, without requiring reference responses. Most prior automatic evaluation metrics rely on comparing to ground truth responses.- The consensus-based method retrieves pseudo-references and evaluates using their consensus. This is a novel way to make traditional reference-based metrics usable without true references. - Experiments show combining the proposed segment act features with existing methods like BERTscore and DynaEval improves correlation with human judgement. This demonstrates segment acts provide complementary information to lexical/semantic metrics.- A new segment act dataset is collected and released to enable modeling segment act flows. This adds a valuable resource for the community.Overall, the key novelties are using segment acts rather than utterance acts, proposing a consensus framework to avoid needing true references, and showing segment acts improve existing methods. This points towards better utilizing dialogue structure for evaluation.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing better datasets for open-domain dialogue evaluation. The authors point out limitations in their segment act dataset, including relatively small scale and distribution skewed towards certain acts like questions and information. They suggest improving the dataset in future work.- Further exploration of the consensus-based evaluation framework. The authors propose this as a novel framework but say more analysis is needed to determine concrete patterns of when it succeeds or fails. They suggest investigating new perspectives and features that could work with the consensus approach.- Combining evaluation metrics in more sophisticated ways. The authors show simple averaging of metrics can improve performance, but more research is needed on optimal combination methods.- Incorporating other types of features beyond just segment acts. The consensus framework is compatible with many features like sentiment, engagement, etc. Exploring these could further improve evaluation.- Developing more reference-free evaluation methods. The consensus framework provides a path to making reference-based metrics usable without true references. More research in this direction could lead to revival of traditional metrics.- Analysis to determine more concrete failure patterns of automatic metrics. The authors show sample failure cases but more analysis is needed to find systematic weaknesses.In summary, the main suggestions are around improving datasets, further exploration of the consensus framework, combining complementary metrics, and developing more reference-free evaluation methods. Robustness of metrics across domains is also highlighted as an open challenge.


## Summarize the paper in one paragraph.

The paper proposes FlowEval, a consensus-based dialogue evaluation framework that utilizes segment act flows for open-domain dialogues. The key ideas are:1) Extend dialog acts to the segment level, called segment acts, to capture finer-grained functional information about utterances. A sequence of segment acts is called a segment act flow.2) Crowdsource a large dataset of segment act annotations on two popular open-domain dialogue datasets - ConvAI2 and DailyDialog. This forms a new dataset called ActDial.3) Propose FlowEval, the first consensus-based framework for open-domain dialogue evaluation. It works by first obtaining segment act flows for a dialogue, then using segment act and content features to retrieve pseudo-references, and finally evaluating the dialogue by its consensus with the pseudo-references.  4) Experiments on 3 datasets show FlowEval reaches best or comparable performance against strong baselines. Analysis indicates segment acts provide complementary information to semantic-focused metrics. The consensus mechanism is also shown to be effective with a reasonably sized retrieval set.In summary, the key novelty is using segment act flows and a consensus-based framework to enable better automatic open-domain dialogue evaluation. The work provides new insights on how to approach this challenging task.
