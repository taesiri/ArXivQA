# [Hyperspherical embedding for novel class classification](https://arxiv.org/abs/2102.03243)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is:How can the normalized softmax loss function be leveraged for novel class classification in the open set problem, and does it offer advantages over common metric learning approaches that require pairwise training?The paper proposes using the normalized softmax loss (NSL) for novel class classification by adding a new neuron to the output layer of a trained network and inferring the weights between that neuron and the penultimate layer. This allows new classes to be classified without retraining the full network. The central hypothesis seems to be that the NSL approach will enable more effective and scalable novel class classification compared to metric learning approaches like triplet loss and contrastive loss that require computationally expensive pairwise training. The paper hypotomizes the NSL will achieve better accuracy by exploiting the cosine similarity properties it enforces on the latent space.In summary, the key research question is whether the proposed NSL approach can outperform current metric learning techniques on novel class classification, while also being more efficient and scalable by avoiding pairwise training. The experiments aim to validate if the NSL approach lives up to that potential.
