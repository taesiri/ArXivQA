# [Occ$^2$Net: Robust Image Matching Based on 3D Occupancy Estimation for   Occluded Regions](https://arxiv.org/abs/2308.16160)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform robust image matching for occlusion scenarios where parts of the scene are visible in one image but occluded in the other. 

Specifically, the paper proposes a novel occlusion-aware image matching method called Occ^2Net that can match not only visible points between two images, but also between visible points and occluded points. This allows it to establish correspondences even when there is significant occlusion.

The key hypothesis is that by modeling the 3D occupancy of the scene and the occlusion relationships between objects, the network can infer the matching location of points in occluded regions that are not directly visible.

So in summary, the paper focuses on the problem of occlusion in image matching, and hypothesizes that occlusion relations can be modeled via 3D occupancy estimation to enable matching of visible to occluded points.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes Occ^2Net, a novel occlusion-aware image matching algorithm that uses 3D occupancy to model the occlusion relations between objects and infer the location of matching points in occluded regions. 

- It combines an Occupancy Estimation (OE) module with an Occlusion-Aware (OA) module to enable visible-occluded matching using a coarse-to-fine structure with occupancy estimation.

- It demonstrates state-of-the-art pose estimation accuracy on both the real-world dataset ScanNet and the simulated dataset TartanAir, showing the effectiveness of matching occluded points.

In summary, the key innovation is using 3D occupancy modeling and estimation to achieve robust image matching performance in the presence of occlusions, which is a common challenge in many vision applications like SLAM. By matching occluded points in addition to visible points, the method improves accuracy over previous approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from this paper:

The paper proposes Occ^2Net, a novel image matching method that uses 3D occupancy estimation to model occlusion relations between objects and infer matches in occluded regions to achieve robust pose estimation, outperforming state-of-the-art methods on real and simulated indoor/outdoor datasets.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on occlusion-aware image matching:

- Most prior work in image matching focuses on matching between visible points, while this paper proposes matching occluded points to visible points. Matching occluded points is a relatively new idea that hasn't been explored much before. 

- This paper introduces a novel network architecture Occ^2Net that combines coarse-to-fine matching with 3D occupancy estimation to enable visible-occluded matching. The framework of using coarse attention-based matching plus fine matching with 3D reasoning is novel.

- For 3D reasoning, this paper uses a learned implicit 3D representation through occupancy estimation. Other recent works like NeuS (Wang et al. ICCV'21) and IBRNet (Chen et al. CVPR'21) also explore learned 3D representations for novel view synthesis across large baselines. This paper tailors the idea for the different goal of occlusion-aware matching.

- Compared to classic feature-based methods like SIFT/SuperGlue, this work follows recent trends in end-to-end dense matching like LoFTR and transformers. The competitive results validate these are promising directions. 

- For evaluation, this paper uses standard pose estimation benchmarks on ScanNet and TartanAir. The experiments focus on occlusion cases and demonstrate superior robustness compared to other state-of-the-art matching methods.

In summary, this paper pushes forward occlusion-aware dense image matching through a novel architecture leveraging ideas like attention, implicit 3D reasoning, and coarse-to-fine matching. The problem formulation and methodology seems innovative compared to related work in dense matching or novel view synthesis.
