# [Efficient Computation Sharing for Multi-Task Visual Scene Understanding](https://arxiv.org/abs/2303.09663)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform multiple visual scene understanding tasks efficiently from a single input image or video stream. Specifically, the paper aims to reduce the computational redundancy that arises when using separate models for each task. 

The key hypothesis is that an efficient multi-task learning approach can be developed that balances accuracy and efficiency by:

1) Leveraging individually-trained single-task models to maintain performance.

2) Sharing parameters and activations across tasks using a novel computation sharing scheme to reduce redundancy.

3) Exploiting sparsity in the temporal domain for video inputs to further reduce computation. 

The overall goal is to reduce the computational and memory costs of multi-task learning for visual scene understanding without sacrificing accuracy. The paper presents a method that shares weights and activations across tasks and time using regularization techniques to induce sparsity. This is hypothesized to significantly reduce model complexity while achieving comparable or improved accuracy over prior multi-task learning techniques.

In summary, the central research question is how to develop an efficient yet accurate approach for multi-task visual scene understanding. The key hypothesis is that substantial computation and memory savings can be attained through a sparse parameter and activation sharing scheme across tasks and time while maintaining accuracy.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel computation and parameter sharing scheme for transformer-based multi-task learning to improve efficiency when performing multiple dense prediction visual tasks on the same input image or video. 

Specifically, the key ideas and contributions are:

- Divide tasks into a base task and sub-tasks, train them independently using single-task learning to maintain performance.

- Propose a parameter sharing scheme to learn sparse delta weights for sub-tasks initialized from base task to reduce redundancy. 

- Introduce a computation sharing scheme to also reuse activations from base task for sub-tasks by learning sparse delta activations. This allows replacing dense matrix multiplications with sparse versions to reduce computation.

- Extend the computation sharing scheme to the temporal domain for video inputs by exploiting redundancies across frames. 

- Develop a strategy to combine task and temporal domain activation sharing for maximum computation savings.

- Demonstrate improved efficiency over multi-task learning baselines with comparable or better accuracy on PASCAL Context and NYUD-v2 datasets for multiple dense prediction tasks.

In summary, the key contribution is developing a computation and parameter sharing approach that balances accuracy and efficiency for concurrent execution of multiple visual perception tasks on images and videos using transformer models.
