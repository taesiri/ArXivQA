# [Efficient Computation Sharing for Multi-Task Visual Scene Understanding](https://arxiv.org/abs/2303.09663)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform multiple visual scene understanding tasks efficiently from a single input image or video stream. Specifically, the paper aims to reduce the computational redundancy that arises when using separate models for each task. 

The key hypothesis is that an efficient multi-task learning approach can be developed that balances accuracy and efficiency by:

1) Leveraging individually-trained single-task models to maintain performance.

2) Sharing parameters and activations across tasks using a novel computation sharing scheme to reduce redundancy.

3) Exploiting sparsity in the temporal domain for video inputs to further reduce computation. 

The overall goal is to reduce the computational and memory costs of multi-task learning for visual scene understanding without sacrificing accuracy. The paper presents a method that shares weights and activations across tasks and time using regularization techniques to induce sparsity. This is hypothesized to significantly reduce model complexity while achieving comparable or improved accuracy over prior multi-task learning techniques.

In summary, the central research question is how to develop an efficient yet accurate approach for multi-task visual scene understanding. The key hypothesis is that substantial computation and memory savings can be attained through a sparse parameter and activation sharing scheme across tasks and time while maintaining accuracy.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel computation and parameter sharing scheme for transformer-based multi-task learning to improve efficiency when performing multiple dense prediction visual tasks on the same input image or video. 

Specifically, the key ideas and contributions are:

- Divide tasks into a base task and sub-tasks, train them independently using single-task learning to maintain performance.

- Propose a parameter sharing scheme to learn sparse delta weights for sub-tasks initialized from base task to reduce redundancy. 

- Introduce a computation sharing scheme to also reuse activations from base task for sub-tasks by learning sparse delta activations. This allows replacing dense matrix multiplications with sparse versions to reduce computation.

- Extend the computation sharing scheme to the temporal domain for video inputs by exploiting redundancies across frames. 

- Develop a strategy to combine task and temporal domain activation sharing for maximum computation savings.

- Demonstrate improved efficiency over multi-task learning baselines with comparable or better accuracy on PASCAL Context and NYUD-v2 datasets for multiple dense prediction tasks.

In summary, the key contribution is developing a computation and parameter sharing approach that balances accuracy and efficiency for concurrent execution of multiple visual perception tasks on images and videos using transformer models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper presents an efficient multi-task learning approach that shares computations and parameters across tasks and over time for dense visual scene understanding using transformer models.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to related work in multi-task learning for visual scene understanding:

- The paper builds off recent work in using transformer models like ViT and Swin for multi-task dense prediction problems. It cites several recent papers that have explored transformer-based multi-task learning, including InvPT, which serves as a strong baseline in the experiments.

- Compared to typical multi-task learning approaches that jointly optimize all tasks in a single model, this paper takes a different approach of training independent single-task models and introducing a novel computation/parameter sharing scheme. This is motivated by limitations of joint multi-task learning like task imbalance. 

- For parameter sharing, the paper adapts recent techniques like Diff-Pruning from the transfer learning literature to enable weight sharing between the base and sub-tasks models. This differs from standard multi-task learning that would fully share parameters.

- The main novelty is extending these parameter sharing ideas to also share activations/computation between tasks and temporal frames. This is achieved by learning sparse delta activation matrices.

- Most prior work has focused on architectural designs for improving multi-task interaction. This paper focuses more on the computational aspects to balance accuracy and efficiency.

- Experiments show this approach can outperform joint multi-task learning baselines like InvPT in terms of accuracy and efficiency. The FLOPs and parameters are reduced substantially compared to independent single-task models.

- The temporal extension to video frames is also novel, as most prior work has focused on single image multi-task learning. This further reduces computation by sharing across time.

In summary, the key distinction of this work is moving from joint multi-task learning to an efficient single-task based approach with intelligent sharing of parameters and computation. The results demonstrate the potential of this direction to get the best of both worlds - maintaining accuracy while improving efficiency.
