# [Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text   Aligned Latent Representation](https://arxiv.org/abs/2306.17115)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How can we generate high-quality and diverse 3D shapes that better conform to given 2D image or text conditional inputs? The key challenges are:1) The significant distribution gap between 3D shapes and 2D images/texts makes it difficult to directly learn a probabilistic mapping function from images/texts to 3D shapes.2) Different 3D objects have very different and complex topology structures which are hard to process into a neural network friendly format. 3) The lack of large-scale aligned 3D-2D data exacerbates the difficulty in learning cross-modal conditional generative models.To address these challenges, the central hypothesis of this paper is:Representing 3D shapes in an aligned shape-image-text latent space can help bridge the domain gap across modalities and facilitate learning better conditional generative models from images/texts to 3D shapes.The proposed approach involves:1) Learning a Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) to represent 3D shapes in a latent space aligned with image and text embeddings. 2) Leveraging this alignment to train an Aligned Shape Latent Diffusion Model (ASLDM) that maps from images/texts to the aligned latent space to generate 3D shapes.By aligning the representations and adopting a diffusive generative process, the paper aims to generate higher quality and more diverse 3D shapes that conform better to the conditional inputs.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: How can we generate high-quality and diverse 3D shapes that semantically conform to given 2D image or text conditional inputs?The key challenges are:1) 3D shapes have diverse topologies that are difficult to process into a neural network friendly representation. 2) There is a significant distribution gap between 3D shapes and 2D images/text, making it difficult to learn a direct mapping from images/text to 3D shapes.The core ideas proposed in the paper to address these challenges are:1) Represent 3D shapes with neural fields (occupancy or SDF) using a topology-free structure like latent codes to make them more amenable to neural networks.2) Learn an aligned latent space between 3D shapes, 2D images, and text using contrastive learning. This helps bridge the distribution gap.3) Develop a two-stage generative model:- Stage 1 (SITA-VAE): Learn a shape-image-text aligned variational autoencoder to represent 3D shapes in the aligned latent space.- Stage 2 (ASLDM): Learn a probabilistic mapping from images/text to the aligned latent space using a diffusion model to generate high quality and diverse 3D shapes conforming to the image/text input.So in summary, the central hypothesis is that aligning the representations of 3D shapes, images, and text can help bridge the domain gap and enable generating 3D shapes conditioned on images/text inputs in a consistent semantically meaningful way.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. A new 3D shape representation that aligns 3D shapes with images and texts in a common latent space. This is achieved through a Shape-Image-Text Aligned Variational Autoencoder (SITA-VAE) that uses contrastive learning to align the shape, image, and text embeddings. 2. A method for generating 3D shapes conditioned on images or text using the aligned latent space. This involves an Aligned Shape Latent Diffusion Model (ASLDM) that learns to map from images/text to the aligned shape latent space.3. Strong performance on 3D shape reconstruction, image/text-conditional shape generation, shape classification, and retrieval benchmarks. The proposed aligned space facilitates computing similarities between 3D shapes and conditional inputs for evaluation.4. Ablation studies validating the benefits of the aligned latent space for training the generative model, using CLIP as the vision-language model in SITA-VAE, and using learnable query embeddings.In summary, the key novelty is representing 3D shapes in an aligned latent space with images and texts to enhance the semantics of the shape representation. This facilitates learning conditional generative models from images/texts to 3D shapes by closing the domain gap. The alignment-before-generation approach leads to higher quality and more diverse results.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The introduction of a novel shape-image-text-aligned representation for 3D shapes. This representation aligns 3D shapes, 2D images, and text in a shared embedding space using contrastive learning. 2. A two-stage generative model for conditional 3D shape generation:- Stage 1 is a Shape-Image-Text-Aligned Variational Autoencoder (SITA-VAE) which encodes 3D shapes into the aligned space and reconstructs them using a transformer decoder. - Stage 2 is an Aligned Shape Latent Diffusion Model (ASLDM) which learns to map from images/text to the aligned 3D shape latent space for high-quality shape generation.3. Demonstrating that aligning the spaces and learning the generative model in two stages leads to higher quality and more diverse conditional 3D shape generation results that better match the visual/textual inputs, compared to prior approaches.4. Introducing new evaluation metrics for measuring similarity between generated 3D shapes and conditioning inputs using the aligned embedding spaces.In summary, the key novelties are the shape-image-text aligned representation, the two-stage generative modeling approach leveraging this representation, and showing how this can improve conditional 3D shape generation performance and evaluation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel framework for cross-modal 3D shape generation that represents 3D shapes in an aligned space with images and texts, enabling learning of better conditional generative models from images/texts to 3D shapes for higher quality and more diverse results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a novel alignment-before-generation approach for cross-modal 3D shape generation, representing 3D shapes in an aligned space with images and text via contrastive learning to enable generating high-quality 3D shapes consistent with visual or textual inputs.
