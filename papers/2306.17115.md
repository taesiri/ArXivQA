# [Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text   Aligned Latent Representation](https://arxiv.org/abs/2306.17115)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How can we generate high-quality and diverse 3D shapes that better conform to given 2D image or text conditional inputs? The key challenges are:1) The significant distribution gap between 3D shapes and 2D images/texts makes it difficult to directly learn a probabilistic mapping function from images/texts to 3D shapes.2) Different 3D objects have very different and complex topology structures which are hard to process into a neural network friendly format. 3) The lack of large-scale aligned 3D-2D data exacerbates the difficulty in learning cross-modal conditional generative models.To address these challenges, the central hypothesis of this paper is:Representing 3D shapes in an aligned shape-image-text latent space can help bridge the domain gap across modalities and facilitate learning better conditional generative models from images/texts to 3D shapes.The proposed approach involves:1) Learning a Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) to represent 3D shapes in a latent space aligned with image and text embeddings. 2) Leveraging this alignment to train an Aligned Shape Latent Diffusion Model (ASLDM) that maps from images/texts to the aligned latent space to generate 3D shapes.By aligning the representations and adopting a diffusive generative process, the paper aims to generate higher quality and more diverse 3D shapes that conform better to the conditional inputs.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: How can we generate high-quality and diverse 3D shapes that semantically conform to given 2D image or text conditional inputs?The key challenges are:1) 3D shapes have diverse topologies that are difficult to process into a neural network friendly representation. 2) There is a significant distribution gap between 3D shapes and 2D images/text, making it difficult to learn a direct mapping from images/text to 3D shapes.The core ideas proposed in the paper to address these challenges are:1) Represent 3D shapes with neural fields (occupancy or SDF) using a topology-free structure like latent codes to make them more amenable to neural networks.2) Learn an aligned latent space between 3D shapes, 2D images, and text using contrastive learning. This helps bridge the distribution gap.3) Develop a two-stage generative model:- Stage 1 (SITA-VAE): Learn a shape-image-text aligned variational autoencoder to represent 3D shapes in the aligned latent space.- Stage 2 (ASLDM): Learn a probabilistic mapping from images/text to the aligned latent space using a diffusion model to generate high quality and diverse 3D shapes conforming to the image/text input.So in summary, the central hypothesis is that aligning the representations of 3D shapes, images, and text can help bridge the domain gap and enable generating 3D shapes conditioned on images/text inputs in a consistent semantically meaningful way.
