# [Dual Aggregation Transformer for Image Super-Resolution](https://arxiv.org/abs/2308.03364)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper addresses is how to design an effective transformer architecture for image super-resolution that can aggregate both spatial and channel features. The key points are:- Existing CNN and transformer models for image super-resolution have limitations in capturing global context in both spatial and channel dimensions. - The authors propose a new dual aggregation transformer (DAT) model that can aggregate features across spatial and channel dimensions in an inter-block and intra-block manner.- Specifically, DAT alternately applies spatial window self-attention and channel-wise self-attention in consecutive blocks to achieve inter-block spatial and channel feature aggregation. - DAT also uses an adaptive interaction module (AIM) and spatial-gate feedforward network (SGFN) to realize intra-block feature aggregation.- Extensive experiments show DAT achieves state-of-the-art performance for image super-resolution, demonstrating the benefits of feature aggregation in both dimensions.In summary, the central hypothesis is that aggregating both spatial and channel features can boost image super-resolution performance, which is validated through the proposed DAT model.


## What is the main contribution of this paper?

This paper proposes a new Transformer model called Dual Aggregation Transformer (DAT) for image super-resolution. The main contributions are:1. DAT aggregates spatial and channel features in an inter-block and intra-block dual manner to obtain powerful feature representation abilities. 2. DAT alternately applies spatial window and channel-wise self-attention in successive Transformer blocks to achieve inter-block feature aggregation between the spatial and channel dimensions.3. Two new components are proposed for intra-block feature aggregation:- Adaptive Interaction Module (AIM) strengthens the modeling ability of self-attention from corresponding dimensions (spatial or channel). - Spatial-Gate Feed-Forward Network (SGFN) complements the feed-forward network with additional non-linear spatial information.4. Extensive experiments show DAT achieves state-of-the-art performance for image super-resolution while maintaining lower complexity and model size compared to previous methods.In summary, the key contribution is a new Transformer architecture that aggregates spatial and channel features across blocks and within blocks to obtain powerful representations for image super-resolution. The proposed components enable effective inter-block and intra-block feature aggregation.
