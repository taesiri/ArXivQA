# [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from the paper:

This paper explores adversarial attacks that target the activations within language models by modifying a small subset of model activations to precisely control a significant number of subsequent token predictions. Through empirical analysis on models ranging from 33M to 2.8B parameters, the authors demonstrate an approximate linear scaling law relating the number of controlled activation tokens $a$ to the maximum number of target tokens $t_{\text{max}}$ that can be dictated as $t_{\text{max}}=\kappa a$, where $\kappa$ is a model-specific attack multiplier constant. Remarkably, the ratio of input control bits to output influence bits, defined as attack resistance $\chi$, remains fairly consistent between 16-25 across models. Compared to token substitution attacks, activation attacks enable exponentially stronger control, however the attack strength normalized by dimensionality is comparable. This dimensional analysis provides evidence that adversarial vulnerability arises fundamentally from the mismatch between input and output space dimensions. Practical implications are highlighted for retrieval models directly encoding retrieved text as activations and multi-modal models concatenating modalities onto activations. Overall, the identified activation attack vulnerability underscores a critical security weakness in language models that warrants further research into defensive strategies.
