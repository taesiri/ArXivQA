# [Knowledge Distillation for 6D Pose Estimation by Aligning Distributions   of Local Predictions](https://arxiv.org/abs/2205.14971)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively transfer knowledge from a large, deep teacher network to a compact student network for 6D object pose estimation. Specifically, the key hypothesis is that aligning the distributions of local predictions (e.g. keypoints or dense codes) from the teacher and student networks can facilitate training a compact yet accurate student model. 

The authors argue that standard prediction-to-prediction distillation struggles when applied to 6D pose estimation, as the student's local predictions tend to be much less accurate than the teacher's. Therefore, they propose a novel distillation approach based on optimal transport to align the teacher and student distributions in a flexible manner, which helps train better students.

In summary, the paper introduces a new knowledge distillation method tailored for 6D pose estimation that is driven by the hypothesis that aligning local prediction distributions is more effective than enforcing prediction-to-prediction matching for this task. The experiments aim to validate whether this proposed distillation strategy can improve compact student networks compared to baselines.
