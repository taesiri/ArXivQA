# [Efficient Document Re-Ranking for Transformers by Precomputing Term   Representations](https://arxiv.org/abs/2004.14255)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the query-time efficiency of transformer-based neural ranking models like BERT to make them more practical for real-time ranking scenarios?The authors propose an approach called PreTTR (Precomputing Transformer Term Representations) to address this question. The key ideas are:- Precompute partial document representations at index time without the query, up to some intermediate layer l of the transformer network. - At query time, compute query representations up to layer l, then combine with precomputed document representations to compute final ranking scores.- Use a compression technique to reduce the storage overhead of storing the precomputed document representations.So in summary, the central hypothesis is that by precomputing part of the document representations and storing them, the query-time latency of transformer models can be substantially reduced while maintaining effectiveness. The PreTTR approach is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called PreTTR (Precomputing Transformer Term Representations) to improve the efficiency and reduce the query latency of transformer-based neural ranking models like BERT. The key ideas are:- Precompute part of the document representations at index time without the query, and merge them with the query representation at query time to compute the final ranking score. This avoids recomputing the document representations at query time.- Propose a compression technique to reduce the storage overhead of storing the precomputed document representations. This involves training a compression network between transformer layers to minimize the difference in attention scores. - Conduct comprehensive experiments on TREC WebTrack 2012 and Robust 2004 datasets. The results show PreTTR can accelerate document ranking by up to 42x on WebTrack with minimal impact on ranking accuracy. The compression technique can reduce the storage overhead by up to 97.5%.In summary, the main contribution is developing and evaluating PreTTR to improve the efficiency of transformer neural rankers so they can be used in practical search settings where query latency is critical. The paper proposes and tests techniques to reduce both computation time and storage requirements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient method called PreTTR for using transformer networks like BERT for document ranking by precomputing part of the document representations at index time and merging them with query representations at query time to reduce computation.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in efficient neural ranking:- The main focus is on improving the efficiency of transformer-based neural ranking models like BERT. Most prior work has focused more on CNN or interaction-based neural rankers. Transformers are currently state-of-the-art but very slow for ranking.- The proposed PreTTR method precomputes part of the document representations during indexing to speed up query time. This builds on previous work using two-stage ranking, but applies it specifically to optimize transformer efficiency.- They introduce a compression technique to reduce the storage overhead of PreTTR. Most prior work has not focused much on compressing neural ranker models or representations.- Experiments show up to 42x faster query processing versus vanilla BERT, with minimal impact on ranking accuracy. Most prior work has not quantified execution time to this extent.- The improvements come purely from model optimization, unlike other work that uses specialized hardware, distillation, etc. But those could still be applied here.- Overall, this paper makes one of the first attempts at comprehensive transformer efficiency for neural IR. The precomputation and compression methods set it apart from prior work on efficient neural ranking.In summary, this paper makes important contributions around optimizing transformers for fast and accurate neural ranking. The precomputation method is novel and the results emphasize query latency more than prior neural IR efficiency papers.
