# [Efficient Document Re-Ranking for Transformers by Precomputing Term   Representations](https://arxiv.org/abs/2004.14255)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the query-time efficiency of transformer-based neural ranking models like BERT to make them more practical for real-time ranking scenarios?The authors propose an approach called PreTTR (Precomputing Transformer Term Representations) to address this question. The key ideas are:- Precompute partial document representations at index time without the query, up to some intermediate layer l of the transformer network. - At query time, compute query representations up to layer l, then combine with precomputed document representations to compute final ranking scores.- Use a compression technique to reduce the storage overhead of storing the precomputed document representations.So in summary, the central hypothesis is that by precomputing part of the document representations and storing them, the query-time latency of transformer models can be substantially reduced while maintaining effectiveness. The PreTTR approach is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called PreTTR (Precomputing Transformer Term Representations) to improve the efficiency and reduce the query latency of transformer-based neural ranking models like BERT. The key ideas are:- Precompute part of the document representations at index time without the query, and merge them with the query representation at query time to compute the final ranking score. This avoids recomputing the document representations at query time.- Propose a compression technique to reduce the storage overhead of storing the precomputed document representations. This involves training a compression network between transformer layers to minimize the difference in attention scores. - Conduct comprehensive experiments on TREC WebTrack 2012 and Robust 2004 datasets. The results show PreTTR can accelerate document ranking by up to 42x on WebTrack with minimal impact on ranking accuracy. The compression technique can reduce the storage overhead by up to 97.5%.In summary, the main contribution is developing and evaluating PreTTR to improve the efficiency of transformer neural rankers so they can be used in practical search settings where query latency is critical. The paper proposes and tests techniques to reduce both computation time and storage requirements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient method called PreTTR for using transformer networks like BERT for document ranking by precomputing part of the document representations at index time and merging them with query representations at query time to reduce computation.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in efficient neural ranking:- The main focus is on improving the efficiency of transformer-based neural ranking models like BERT. Most prior work has focused more on CNN or interaction-based neural rankers. Transformers are currently state-of-the-art but very slow for ranking.- The proposed PreTTR method precomputes part of the document representations during indexing to speed up query time. This builds on previous work using two-stage ranking, but applies it specifically to optimize transformer efficiency.- They introduce a compression technique to reduce the storage overhead of PreTTR. Most prior work has not focused much on compressing neural ranker models or representations.- Experiments show up to 42x faster query processing versus vanilla BERT, with minimal impact on ranking accuracy. Most prior work has not quantified execution time to this extent.- The improvements come purely from model optimization, unlike other work that uses specialized hardware, distillation, etc. But those could still be applied here.- Overall, this paper makes one of the first attempts at comprehensive transformer efficiency for neural IR. The precomputation and compression methods set it apart from prior work on efficient neural ranking.In summary, this paper makes important contributions around optimizing transformers for fast and accurate neural ranking. The precomputation method is novel and the results emphasize query latency more than prior neural IR efficiency papers.


## What future research directions do the authors suggest?

The authors suggest the following future research directions:- Explore applying more advanced techniques like LSH hashing to improve the efficiency of transformer models like CEDR that require computation/storage of additional term representations beyond just the [CLS] token. - Investigate the importance of the feed-forward step in each transformer layer, since the paper shows comparable performance can be achieved with a compressed representation. This could lead to simplifying or removing this step.- Apply additional compression techniques like kernel density estimation-based quantization to further reduce the storage requirements.- Evaluate the approach on other transformer architectures beyond BERT, RoBERTa, and DistilBERT.- Explore the tradeoffs in effectiveness vs efficiency in other search domains beyond web and news search.- Combine this precomputation approach with other orthogonal techniques like knowledge distillation to create even more efficient transformer models.- Study the differences in how well this approach works for different types of queries besides just keyword queries, like natural language queries.- Analyze the differences in how well this approach works for datasets with different characteristics in terms of query length, document length, etc.In summary, the main future direction is exploring ways to further improve the efficiency of transformer models for ranking while maintaining effectiveness, either by enhancing this precomputation approach or combining it with other techniques. The authors also suggest evaluating how well this approach generalizes across domains, transformer architectures, and query types.


## Summarize the paper in one paragraph.

This paper proposes PreTTR, a method to improve the efficiency of transformer-based neural ranking models like BERT for ad-hoc retrieval. The key idea is to precompute part of the document representations at index time without the query, and merge them with query representations at query time to compute the final ranking score. This avoids recomputing the full document representations at query time. To reduce the storage overhead of storing the precomputed representations, they propose a compression technique to minimize the difference in attention scores before and after compression. Their experiments on TREC WebTrack 2012 and Robust 2004 show PreTTR can accelerate document ranking by up to 42x on WebTrack with comparable effectiveness to the full model, while reducing storage needs by up to 97.5% via compression. Overall, PreTTR moves transformer networks towards practicality for real-time ranking by improving efficiency with minimal impact on effectiveness.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper presents a method called PreTTR (Precomputing Transformer Term Representations) to improve the efficiency of transformer-based neural ranking models like BERT. The key idea is to precompute part of the document representations during indexing before the query is known. Specifically, the first l layers of the transformer network are executed on documents at index time to build contextualized term representations. These representations are stored and then combined with query term representations at query time to compute the final ranking score. This avoids executing the full transformer network on documents at query time, greatly reducing latency. The paper also proposes a compression technique to reduce the storage overhead of the precomputed representations. A compression network is trained to minimize the difference in attention scores between the original and compressed representations. Experiments on TREC WebTrack 2012 and Robust 2004 show PreTTR can accelerate document ranking by up to 42x on WebTrack with comparable precision, while the compression can reduce the storage overhead by up to 97.5%. The method is shown to generalize to other transformer architectures like RoBERTa and DistilBERT. Overall, PreTTR moves transformer ranking models towards practicality by reducing query latency while maintaining effectiveness.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a method called PreTTR (Precomputing Transformer Term Representations) to improve the efficiency of transformer-based neural ranking models like BERT. The key idea is to precompute part of the document representations at indexing time without the query, and then merge them with query representations at query time to compute the final ranking score. Specifically, during training, an attention mask is applied to the first l layers of the transformer network to prevent interactions between query and document terms. At indexing time, documents are encoded through these first l layers and the resulting term representations are stored. At query time, the query is encoded through the first l layers. Then the stored document representations and query representations are combined and passed through the remaining layers to produce the final ranking score. This avoids recomputing the full document representations at query time and leads to large speedups. The paper also proposes a compression technique to reduce the storage overhead of the precomputed representations.


## What problem or question is the paper addressing?

Based on my reading of the paper, the key problem the authors are trying to address is the poor query-time latency of using deep transformer networks like BERT for document ranking. Specifically, they point out that while transformers like BERT have led to substantial gains in ranking effectiveness, they are very computationally expensive to run, making them impractical in real-time search systems. The main question they are trying to answer is: how can we speed up the query-time processing of transformer-based ranking models, while maintaining their effectiveness gains?Some more details on the problem and questions:- Transformer networks like BERT are huge (billions of parameters) and slow to run, taking around 2 seconds to re-rank 100 documents. This makes them impractical for real-time ranking.- Prior work has mainly focused on ranking accuracy gains from transformers, ignoring efficiency. - The authors want to improve the efficiency of using transformers like BERT for document ranking, so they can be deployed in practical search systems.- Specifically, they aim to reduce the query-time latency when using transformers to re-rank a set of initially retrieved documents.- Their main research questions are around quantifying the efficiency gains and tradeoffs with accuracy from their proposed approach.So in summary, the key problem is improving the speed of transformer-based neural ranking models, to make them viable in real production systems, while maintaining their substantial accuracy improvements.
