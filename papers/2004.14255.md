# [Efficient Document Re-Ranking for Transformers by Precomputing Term   Representations](https://arxiv.org/abs/2004.14255)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the query-time efficiency of transformer-based neural ranking models like BERT to make them more practical for real-time ranking scenarios?The authors propose an approach called PreTTR (Precomputing Transformer Term Representations) to address this question. The key ideas are:- Precompute partial document representations at index time without the query, up to some intermediate layer l of the transformer network. - At query time, compute query representations up to layer l, then combine with precomputed document representations to compute final ranking scores.- Use a compression technique to reduce the storage overhead of storing the precomputed document representations.So in summary, the central hypothesis is that by precomputing part of the document representations and storing them, the query-time latency of transformer models can be substantially reduced while maintaining effectiveness. The PreTTR approach is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called PreTTR (Precomputing Transformer Term Representations) to improve the efficiency and reduce the query latency of transformer-based neural ranking models like BERT. The key ideas are:- Precompute part of the document representations at index time without the query, and merge them with the query representation at query time to compute the final ranking score. This avoids recomputing the document representations at query time.- Propose a compression technique to reduce the storage overhead of storing the precomputed document representations. This involves training a compression network between transformer layers to minimize the difference in attention scores. - Conduct comprehensive experiments on TREC WebTrack 2012 and Robust 2004 datasets. The results show PreTTR can accelerate document ranking by up to 42x on WebTrack with minimal impact on ranking accuracy. The compression technique can reduce the storage overhead by up to 97.5%.In summary, the main contribution is developing and evaluating PreTTR to improve the efficiency of transformer neural rankers so they can be used in practical search settings where query latency is critical. The paper proposes and tests techniques to reduce both computation time and storage requirements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient method called PreTTR for using transformer networks like BERT for document ranking by precomputing part of the document representations at index time and merging them with query representations at query time to reduce computation.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in efficient neural ranking:- The main focus is on improving the efficiency of transformer-based neural ranking models like BERT. Most prior work has focused more on CNN or interaction-based neural rankers. Transformers are currently state-of-the-art but very slow for ranking.- The proposed PreTTR method precomputes part of the document representations during indexing to speed up query time. This builds on previous work using two-stage ranking, but applies it specifically to optimize transformer efficiency.- They introduce a compression technique to reduce the storage overhead of PreTTR. Most prior work has not focused much on compressing neural ranker models or representations.- Experiments show up to 42x faster query processing versus vanilla BERT, with minimal impact on ranking accuracy. Most prior work has not quantified execution time to this extent.- The improvements come purely from model optimization, unlike other work that uses specialized hardware, distillation, etc. But those could still be applied here.- Overall, this paper makes one of the first attempts at comprehensive transformer efficiency for neural IR. The precomputation and compression methods set it apart from prior work on efficient neural ranking.In summary, this paper makes important contributions around optimizing transformers for fast and accurate neural ranking. The precomputation method is novel and the results emphasize query latency more than prior neural IR efficiency papers.


## What future research directions do the authors suggest?

The authors suggest the following future research directions:- Explore applying more advanced techniques like LSH hashing to improve the efficiency of transformer models like CEDR that require computation/storage of additional term representations beyond just the [CLS] token. - Investigate the importance of the feed-forward step in each transformer layer, since the paper shows comparable performance can be achieved with a compressed representation. This could lead to simplifying or removing this step.- Apply additional compression techniques like kernel density estimation-based quantization to further reduce the storage requirements.- Evaluate the approach on other transformer architectures beyond BERT, RoBERTa, and DistilBERT.- Explore the tradeoffs in effectiveness vs efficiency in other search domains beyond web and news search.- Combine this precomputation approach with other orthogonal techniques like knowledge distillation to create even more efficient transformer models.- Study the differences in how well this approach works for different types of queries besides just keyword queries, like natural language queries.- Analyze the differences in how well this approach works for datasets with different characteristics in terms of query length, document length, etc.In summary, the main future direction is exploring ways to further improve the efficiency of transformer models for ranking while maintaining effectiveness, either by enhancing this precomputation approach or combining it with other techniques. The authors also suggest evaluating how well this approach generalizes across domains, transformer architectures, and query types.


## Summarize the paper in one paragraph.

This paper proposes PreTTR, a method to improve the efficiency of transformer-based neural ranking models like BERT for ad-hoc retrieval. The key idea is to precompute part of the document representations at index time without the query, and merge them with query representations at query time to compute the final ranking score. This avoids recomputing the full document representations at query time. To reduce the storage overhead of storing the precomputed representations, they propose a compression technique to minimize the difference in attention scores before and after compression. Their experiments on TREC WebTrack 2012 and Robust 2004 show PreTTR can accelerate document ranking by up to 42x on WebTrack with comparable effectiveness to the full model, while reducing storage needs by up to 97.5% via compression. Overall, PreTTR moves transformer networks towards practicality for real-time ranking by improving efficiency with minimal impact on effectiveness.
