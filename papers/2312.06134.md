# [Order Matters in the Presence of Dataset Imbalance for Multilingual   Learning](https://arxiv.org/abs/2312.06134)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a simple yet effective pre-training joint fine-tuning method to improve the performance of low-resource tasks in the presence of high-resource tasks during multi-task learning. The key idea is to first pre-train the model on high-resource tasks, reset the optimizer state and learning rate schedule, and then fine-tune the model on a mixture of high and low-resource tasks. This allows the high-resource tasks to be trained for longer without overfitting the low-resource tasks. The separate fine-tuning phase also enables higher sampling rates for low-resource tasks, improving their data efficiency. Through extensive experiments on neural machine translation and multilingual language modeling, the authors demonstrate that this method pushes past the Pareto frontier achievable by standard static sampling methods. The performance gains come from better initialization via transfer learning from the high-resource pre-training task as well as the more aggressive fine-tuning phase. The method is simple to implement, robust to choices of pre-training lengths, and introduces little overhead for hyperparameter tuning.
