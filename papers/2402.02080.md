# [Translation Errors Significantly Impact Low-Resource Languages in   Cross-Lingual Learning](https://arxiv.org/abs/2402.02080)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Popular cross-lingual benchmarks like XNLI use human translations of English data into multiple languages as test sets. However, there are concerns on whether these translations consistently preserve semantic relationships from the original English data, especially for low-resource languages.

- This paper finds that translation inconsistencies leading to label errors do exist in XNLI's human translations, and they disproportionately impact low-resource languages like Hindi, Urdu and Swahili. 

Methodology:
- Compare XNLI performance of models on human translations versus machine translations of original English data. Larger gaps indicate translation errors that alter semantics.

- Human evaluation by re-annotating subset of XNLI sentences in Hindi/Urdu to check agreement with original English labels. 

- Attention analysis to compare overlap in attention heads between English sentences and corresponding human/machine translated sentences.

Key Findings:
- XNLI performance gaps between human and machine translations were much higher for low-resource languages, suggesting label inconsistencies.

- In human evaluation, newly annotated Hindi/Urdu labels only matched original English labels 60-66% of the time, a big drop from 90% for English.

- Attention overlap was higher between English sentences and their machine translations compared to human translations.

Main Contributions:
- Highlighted problem of translation errors in XNLI disproportionately impacting low-resource languages, proposed method to identify them.

- Showed these errors persist in multiple train/test settings.

- Human evaluation and attention analysis provided further evidence on inadequacies of human translations for languages like Hindi and Urdu.

Overall, the paper makes a strong case for acquiring high-quality translations and ensuring label consistency for low-resource languages in cross-lingual datasets.


## Summarize the paper in one sentence.

 This paper finds that translation inconsistencies exist in multilingual benchmarks like XNLI and disproportionately impact low-resource languages, leading to incorrect estimates of cross-lingual transfer performance.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Highlighting the problem of translation errors in the XNLI benchmark that disproportionately affect low-resource languages. The authors show there are larger performance gaps when evaluating on human translations versus machine translations for low-resource languages compared to high-resource languages.

2) Proposing a practical way to identify low-quality human translations by comparing performance with machine translations derived from the original English sentences. Large performance gaps are indicative of translation inconsistencies. 

3) Demonstrating that translation errors persist even when using translated/backtranslated training data, and that they lead to overestimated cross-lingual transfer gaps especially for low-resource languages.

4) Manually re-annotating subsets of the XNLI dev/test sets for Hindi and Urdu and finding poor agreement with the original English labels, indicating translation errors.

In summary, the main contribution is showing that translation quality issues disproportionately impact low-resource languages in cross-lingual benchmarks like XNLI, and providing methods to identify such languages along with analysis on real datasets.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Cross-lingual language understanding
- Multilingual benchmarks
- XNLI (Cross-Lingual Natural Language Inference)
- Translation errors
- Low-resource languages (Hindi, Urdu, Swahili)
- High-resource languages (Spanish, German, French) 
- Zero-shot evaluation
- Machine translation
- Backtranslation
- Cross-lingual transfer gap
- Annotation inconsistencies
- Attention analysis

The paper studies translation errors and inconsistencies in multilingual datasets like XNLI, and shows that these issues disproportionately impact low-resource languages. It proposes methods to identify such errors, including comparing performance on human vs machine translations of the test sets, and manually re-annotating subsets of the data. The key goal is to accurately characterize the cross-lingual transfer gap for multilingual models, which can be mis estimated due to translation errors affecting low-resource languages.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes comparing performance on human-translated test sets versus machine-translated test sets as a way to identify translation errors. What are some limitations of using this method? For example, could there be systematic differences between human and machine translations that cause performance gaps even without direct translation errors?

2. The paper shows performance gaps between human and machine translations being higher for low-resource languages like Swahili, Urdu, and Turkish. What explanations could there be for why low-resource languages seem more susceptible to translation inconsistencies? 

3. For the manual re-annotation of Hindi and Urdu data, the paper reports relatively low agreement rates with the original English labels (66.5% for Hindi and 60% for Urdu). What could be some reasons for the lower annotation agreement compared to high-resource languages like English and French?

4. Could the lower annotation agreement rates for Hindi and Urdu indicate cultural/linguistic differences in how textual entailment is interpreted instead of translation errors? How could the authors further analyze this?

5. The paper relies on back-translations to English when comparing human and machine translated test performance. What biases could back-translation introduce that affect the validity of using performance gaps to identify translation errors?  

6. What other automatic metrics besides accuracy could the authors have used to compare consistency between human and machine translations? For example, BLEU, METEOR, BERTScore etc. How would that change their analysis?

7. For the attention analysis between English and Hindi/Urdu translations, what other alignment techniques could have been used instead of relying on attention scores? How do those change the word alignment picture? 

8. The paper shows training on back-translated or machine-translated data helps models generalize better to both human and machine test sets. Does this eliminate the need for human translations when creating multilingual datasets? What is still lacking?

9. How feasible is the proposed method of using machine translation performance gaps to check human translation quality when creating new multilingual benchmarks? What practical challenges need to be addressed?

10. The paper relies a lot on XNLI and focuses less on MLQA and PAWS-X. Would the authors' conclusions change if more tasks were analyzed in depth beyond XNLI? What differences could emerge?
