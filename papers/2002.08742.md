# [Disentangled Speech Embeddings using Cross-modal Self-supervision](https://arxiv.org/abs/2002.08742)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper develops a self-supervised method to learn speaker recognition embeddings from speech without needing any labeled training data. It does this by exploiting the natural co-occurrence of faces and voices in videos.- The method aims to explicitly disentangle representations of speech into two factors: linguistic content and speaker identity. It assumes faces and voices from the same video contain the same identity but different content, while faces and voices from different videos contain different identities and content.- The method uses a two-stream convolutional neural network architecture with a shared feature extraction trunk. It imposes constraints using several losses to learn representations of content and identity, as well as disentangle them.- The method is evaluated on speaker recognition using the VoxCeleb benchmark. It shows improvements over previous self-supervised methods, and can outperform fully supervised training when limited labeled data is available.In summary, the key hypothesis is that speaker identity and linguistic content can be disentangled through self-supervision by exploiting co-occurrence of faces and voices in unlabeled video data, leading to more robust speaker recognition. The method aims to demonstrate this hypothesis empirically.
