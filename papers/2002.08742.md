# [Disentangled Speech Embeddings using Cross-modal Self-supervision](https://arxiv.org/abs/2002.08742)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper develops a self-supervised method to learn speaker recognition embeddings from speech without needing any labeled training data. It does this by exploiting the natural co-occurrence of faces and voices in videos.- The method aims to explicitly disentangle representations of speech into two factors: linguistic content and speaker identity. It assumes faces and voices from the same video contain the same identity but different content, while faces and voices from different videos contain different identities and content.- The method uses a two-stream convolutional neural network architecture with a shared feature extraction trunk. It imposes constraints using several losses to learn representations of content and identity, as well as disentangle them.- The method is evaluated on speaker recognition using the VoxCeleb benchmark. It shows improvements over previous self-supervised methods, and can outperform fully supervised training when limited labeled data is available.In summary, the key hypothesis is that speaker identity and linguistic content can be disentangled through self-supervision by exploiting co-occurrence of faces and voices in unlabeled video data, leading to more robust speaker recognition. The method aims to demonstrate this hypothesis empirically.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a novel self-supervised method to learn speaker recognition embeddings from speech without needing any labels, by exploiting the natural co-occurrence of faces and audio in videos. 2. Showing that sharing a trunk architecture for learning representations of both content and speaker identity, along with adding an explicit disentanglement objective, improves performance and disentangles the factors of content and identity.3. Evaluating the self-supervised embeddings on the VoxCeleb1 speaker recognition benchmark and showing they can outperform fully supervised methods when limited labeled data is available.4. Introducing constraints based on the correspondence between faces and audio within and across face tracks in videos to learn representations of content and identity in a self-supervised manner.5. Presenting a framework to train a model end-to-end using these constraints, with specific loss functions to implement the content, identity and disentanglement constraints.In summary, the key contribution is a novel self-supervised approach to learn disentangled and robust speaker identity embeddings from unlabeled video data, without needing manually annotated data. The method is shown to achieve competitive performance on speaker recognition compared to fully supervised techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a self-supervised method to learn speaker identity representations from speech by exploiting the natural co-occurrence of faces and voices in videos, with a disentanglement loss to explicitly separate content and identity factors, achieving improved generalization and competitive performance to supervised methods on speaker recognition tasks.
