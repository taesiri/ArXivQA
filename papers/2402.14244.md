# [MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback   and Dynamic Distance Constraint](https://arxiv.org/abs/2402.14244)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Hierarchical reinforcement learning (HRL) methods aim to tackle complex tasks by dividing them into more manageable subgoals. However, current HRL methods struggle with two key issues: (1) generating meaningful and instructive subgoals to guide policy learning, and (2) efficiently achieving the generated subgoals using the low-level policy. Manual subgoal design is expensive and does not scale, while automatically generating good subgoals requires extensive exploration. Furthermore, using sole low-level policies to complete subgoals suffers from sparse rewards, lacking exploration, or training instability.

Proposed Solution: 
The authors propose MENTOR, a HRL framework incorporating human feedback and dynamic distance constraints (DDC) to address the limitations above. 

Key Components:
1) Human Feedback: Unlike humans, MENTOR can incorporate human subgoal preferences to guide high-level policy learning via pairwise comparisons. This directional guidance helps generate better subgoals.

2) Exploration-Exploitation Decoupling (EED): The low-level policy is split into separate exploration and base policies that share experiences. The exploration policy drives novel state discovery while the base policy focuses on subgoal achievement using this data.

3) Dynamic Distance Constraints (DDC): A distance measure tracks subgoal difficulty to constrain the high-level's subgoal space, ensuring subgoals match the low-level policy's evolving capabilities. The range expands as the low-level improves. 

Main Contributions:
- Demonstrates using limited human feedback to significantly boost HRL performance in sparse reward settings
- Proposes EED to stabilize low-level training by decoupling exploration and exploitation 
- Introduces DDC to dynamically coordinate high/low-level policies by adjusting subgoal difficulties

The proposed MENTOR framework outperforms state-of-the-art baselines across various benchmark environments. Ablation studies validate the positive impacts of its key components.


## Summarize the paper in one sentence.

 The paper proposes MENTOR, a hierarchical reinforcement learning framework that incorporates human feedback to guide high-level subgoal selection and uses dynamic distance constraints and exploration-exploitation decoupling to enable efficient subgoal achievement.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) Proposing MENTOR, a general hierarchical reinforcement learning framework that incorporates human feedback to guide high-level policy learning in finding better subgoals, and uses exploration-exploitation decoupling (EED) at the low-level to stabilize training. 

2) Introducing the Dynamic Distance Constraint (DDC) mechanism to dynamically adjust the space of optional subgoals based on low-level policy capabilities, so that the high-level policy generates subgoals well matched to the low-level learning process from easy to hard.

3) Demonstrating through extensive experiments that MENTOR can effectively leverage a small amount of human feedback to achieve significant improvement in completing complex tasks with sparse rewards across various domains, outperforming existing baselines.

In summary, the key innovations of this paper are using human feedback to guide high-level policy, decoupling exploration and exploitation at the low-level policy, and dynamically constraining the subgoal space to match high-level and low-level capabilities. Together these methods allow MENTOR to efficiently solve challenging reinforcement learning problems with sparse rewards.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Hierarchical reinforcement learning (HRL)
- Human feedback
- Dynamic distance constraint (DDC)
- Exploration-exploitation decoupling (EED)
- Subgoals
- Sparse rewards
- Goal-conditioned reinforcement learning (GCRL) 
- Reinforcement learning from human feedback (RLHF)
- Curiosity-driven exploration
- Hindsight relabelling

The paper proposes a hierarchical reinforcement learning framework called MENTOR that incorporates human feedback to guide high-level policy learning in selecting better subgoals. It also uses dynamic distance constraints to align subgoal difficulty with agent capabilities and exploration-exploitation decoupling to stabilize training. Key capabilities of the framework highlighted include efficiently using human feedback, coordinating the hierarchical levels, and enabling rapid, stable learning on tasks with sparse rewards.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) How does the high-level policy in MENTOR select subgoals? What mechanism is used to incorporate human feedback into this process? 

2) Explain the Exploration-Exploitation Decoupling (EED) approach used in the low-level policy. Why is this decoupling important for stabilizing training?

3) What is the Dynamic Distance Constraint (DDC) and how does it align subgoal difficulty with the capabilities of the low-level policy? Walk through the mathematical formulation.  

4) Discuss the differences in how the reward model is trained compared to prior work like PEBBLE. How does near-policy sampling focus the training?

5) Analyze the effects of negative sampling in the distance model training. Why is this important for avoiding unrealistic subgoal proposals?

6) Compare and contrast the maximum entropy and RND approaches for encouraging exploration at the high-level policy. What are the tradeoffs? 

7) Explain the dual optimization problem for the high-level policy and balancing coefficient alpha. Why is auto-adjustment of alpha preferred?

8) What are the key benefits of incorporating human feedback specifically for hierarchical RL as done in MENTOR? How does it mitigate issues in both HRL and RLHF?

9) Discuss the results of the batch query and frequency experiment. What can we conclude about the amount/frequency of human feedback needed?

10) Analyze the heatmaps with and without DDC during training. How does DDC change the guidance provided and coordinate the levels?
