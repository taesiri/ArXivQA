# [MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback   and Dynamic Distance Constraint](https://arxiv.org/abs/2402.14244)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Hierarchical reinforcement learning (HRL) methods aim to tackle complex tasks by dividing them into more manageable subgoals. However, current HRL methods struggle with two key issues: (1) generating meaningful and instructive subgoals to guide policy learning, and (2) efficiently achieving the generated subgoals using the low-level policy. Manual subgoal design is expensive and does not scale, while automatically generating good subgoals requires extensive exploration. Furthermore, using sole low-level policies to complete subgoals suffers from sparse rewards, lacking exploration, or training instability.

Proposed Solution: 
The authors propose MENTOR, a HRL framework incorporating human feedback and dynamic distance constraints (DDC) to address the limitations above. 

Key Components:
1) Human Feedback: Unlike humans, MENTOR can incorporate human subgoal preferences to guide high-level policy learning via pairwise comparisons. This directional guidance helps generate better subgoals.

2) Exploration-Exploitation Decoupling (EED): The low-level policy is split into separate exploration and base policies that share experiences. The exploration policy drives novel state discovery while the base policy focuses on subgoal achievement using this data.

3) Dynamic Distance Constraints (DDC): A distance measure tracks subgoal difficulty to constrain the high-level's subgoal space, ensuring subgoals match the low-level policy's evolving capabilities. The range expands as the low-level improves. 

Main Contributions:
- Demonstrates using limited human feedback to significantly boost HRL performance in sparse reward settings
- Proposes EED to stabilize low-level training by decoupling exploration and exploitation 
- Introduces DDC to dynamically coordinate high/low-level policies by adjusting subgoal difficulties

The proposed MENTOR framework outperforms state-of-the-art baselines across various benchmark environments. Ablation studies validate the positive impacts of its key components.
