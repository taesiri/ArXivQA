# [Correctness of Automatic Differentiation via Diffeologies and   Categorical Gluing](https://arxiv.org/abs/2001.02209)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is how to give a formal semantic account of automatic differentiation (AD), particularly for higher-order functions, that can be used to prove the correctness of AD algorithms. Specifically, the paper aims to:- Provide a denotational semantics for languages used in differentiable programming that can interpret higher-order functions, using diffeological spaces. - Use this semantics to prove the correctness of a simple forward-mode AD translation, by making a connection to logical relations proofs.- Show how the semantic proof can be understood categorically, in terms of a "gluing" construction that builds new models.- Extend the correctness proof to variant and inductive types like lists, not just base types like reals. - Demonstrate the generality of their semantic approach by also proving correctness of a reverse-mode AD algorithm.The key challenge is that while differential geometry provides notions of derivatives for first-order functions, the derivatives of higher-order functions are not canonically defined. The paper proposes a way to interpret types as diffeological spaces in a way that provides semantics for AD at higher types.In summary, the central research contribution is using categories of diffeological spaces and categorical gluing constructions to create models that can formally interpret AD algorithms and support proofs of their semantic correctness, for languages with features like higher-order functions and algebraic data types.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of the paper is to provide a semantic analysis of automatic differentiation using denotational semantics and diffeological spaces. Specifically:- It gives a denotational semantics for a higher order typed language with real numbers, products, functions, variants, and inductive types like lists. The semantics interprets types as diffeological spaces and terms as smooth functions between these spaces.- It defines a forward mode automatic differentiation transformation on the language as a syntactic macro. This macro transforms both types and terms.- It shows the correctness of this automatic differentiation transformation with respect to the denotational semantics. The key idea is to interpret types not just as spaces but as triples (X, X', S) where X' serves as the "tangent bundle" and S relates curves in X to tangent curves in X'. This allows defining derivatives at higher types.- It provides a categorical analysis of the correctness argument using gluing/logical relations. This also leads to a proof of correctness for first order types based on manifolds and tangent bundles.- It shows the approach is flexible by also giving a semantics for a continuation-based reverse mode AD algorithm.So in summary, the main contribution is using denotational semantics in diffeological spaces to model differentiation and prove the correctness of syntactic AD transformations, including at higher types, while retaining compositionality. The categorical perspective provides a deeper understanding of how this works.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading, here is a one sentence summary of the key points of the paper:The paper presents semantic correctness proofs for automatic differentiation (AD) on a higher order language with algebraic data types, using diffeological spaces and categorical gluing to model differentiation and show that a forward-mode AD algorithm correctly computes derivatives.


## How does this paper compare to other research in the same field?

Here are some key points of comparison between this paper and other related work on formalizing and verifying automatic differentiation:- The paper provides a denotational semantics for a higher-order functional programming language with automatic differentiation. This is in contrast to much work that focuses only on first-order languages or computation graphs. Providing semantics for higher-order AD is an open challenge.- The semantics is based on diffeological spaces, which allow modeling smooth higher-order functions while still having a close connection to traditional differential geometry. Other categorical models of differentiation like synthetic differential geometry or differential Î»-calculi don't have this tight connection.- The main correctness theorem connects syntactic forward-mode AD to taking derivatives of semantic interpretations. This is proved via an elegant "gluing" construction relating syntax, semantics, and differentiation. Other correctness proofs tend to be more syntactic in nature.- The approach is shown to generalize beyond forward mode, e.g. to a continuation-passing style reverse mode algorithm. Most other formal verification has focused specifically on reverse mode.- Inductive and coinductive data types like lists and trees are supported in the language and semantics. These pose challenges for differentiation and are not considered in much related work.- The semantics is more abstract and mathematical than operational models relating eval and differentiation directly. But the proofs seem less automated/formalized than in some of those syntactic accounts.- There is no treatment of effects like non-termination or probabilistic functions. Nor algorithms like gradient descent. Other work connects AD more directly to these practical concerns.So in summary, this paper provides a quite broad denotational semantics for AD in a higher-order functional language, developed in a categorical style. It relates syntax and semantics through some elegantly structured proofs. The treatment of higher-order AD, inductive types, and the diffeological spaces semantics seems relatively novel. But it's arguably quite far from practical implementation concerns.
