# [Probing Pretrained Language Models with Hierarchy Properties](https://arxiv.org/abs/2312.09670)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating hierarchical knowledge in pretrained language models (PLMs) is typically done through task-dependent approaches based on performance on downstream tasks requiring some hierarchical reasoning. This risks conflating a model's understanding of a task vs its understanding of hierarchy itself. 
- Existing evaluations also focus only on direct hypernymy relations and miss more complex taxonomic relations like ancestors and siblings.  
- There is a need for more comprehensive, task-agnostic evaluation methods to probe a model's intrinsic understanding of hierarchy.

Methodology:
- Identify 6 hierarchy properties based on edge distances in taxonomies, capturing relations like parent, ancestor, sibling, etc. 
- Formulate properties as inequalities over triples of concepts (ternaries).
- Design probe datasets encoding these ternary-based properties. 
- Use probes in a zero-shot setup to evaluate encoding of properties in PLMs.
- Also fine-tune PLMs on probes to inject hierarchy knowledge.

Experiments and Results:
- Evaluation reveals PLMs struggle with some relations like siblings and ancestors.  
- Fine-tuning PLMs with probes improves performance on properties, enhancing hierarchy knowledge.
- Downstream evaluation on tasks like taxonomy reconstruction shows transferred knowledge from enhanced PLMs can improve performance.  
- But on reading comprehension, enhanced models struggle, showing possible catastrophic forgetting.

Main Contributions:
- Novel set of task-agnostic hierarchy properties for comprehensive PLM evaluation
- Ternary-based probe methodology to test intrinsic encoding of properties
- Technique to inject hierarchical knowledge into PLMs via fine-tuning on probes
- Analysis showing potential but also limitations of transferring injected knowledge to downstream tasks


## Summarize the paper in one sentence.

 This paper proposes a task-agnostic methodology to evaluate and enhance pretrained language models' understanding of taxonomic hierarchy through intrinsic properties, finding that models struggle with certain hierarchical relations and that injecting properties improves performance on hierarchy probes but transfers only moderately to downstream tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a task-agnostic methodology for probing the capability of pretrained language models (PLMs) to capture hierarchy. Specifically:

1) They identify a set of intrinsic hierarchy properties from taxonomies and construct probes encoding these properties to evaluate PLMs' understanding of hierarchy. 

2) They show that PLMs struggle to capture certain hierarchical relationships like siblings and ancestors. The properties can be injected into PLMs to enhance their representations regardless of specific downstream tasks.

3) They evaluate the hierarchy-enhanced PLMs on downstream tasks like hypernym discovery, taxonomy reconstruction, and reading comprehension. Results show the enhanced representations are moderately transferable in a sequential fine-tuning approach, but struggle in some cases like reading comprehension due to catastrophic forgetting.

Overall, the key contribution is the task-agnostic probing methodology to evaluate and enhance PLMs' capability to represent hierarchical knowledge, which provides insights into designing more interpretable and effective PLMs. The properties and methodology could be extended beyond hierarchy as well.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Pretrained Language Models (PLMs)
- Hierarchy properties
- Task-agnostic evaluation
- Probes
- Ternaries
- Hypernym discovery
- Taxonomy reconstruction 
- Reading comprehension
- Transfer learning
- Catastrophic forgetting

The paper proposes a task-agnostic methodology to evaluate how well pretrained language models capture hierarchical relations between concepts. It defines a set of hierarchy properties and designs "probes" in the form of ternaries (triplets of entities) that encode these properties. These probes are used to evaluate and teach hierarchy to PLMs in a task-agnostic way. The hierarchy-enhanced PLMs are then evaluated on downstream tasks like hypernym discovery, taxonomy reconstruction, and reading comprehension to analyze the transferability of the learned hierarchical knowledge. Key findings include that PLMs struggle to capture certain hierarchical relations, the hierarchy properties can be injected into PLMs to improve their representations, but the transfer to downstream tasks is only moderately successful due to issues like catastrophic forgetting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the proposed task-agnostic methodology for probing PLMs differ from existing task-dependent evaluation approaches for hierarchical knowledge? What are the key advantages and disadvantages?

2. The hierarchy properties in Table 1 seem intuitive but how were they formally validated? Were any formal taxonomy properties or metrics used to justify them? 

3. The ternary representation using 3 nodes seems interesting. What was the motivation behind using 3 nodes instead of binary node pairs? Were other n-ary representations explored?

4. What distance metrics and similarity measures were experimented with to represent the hierarchy properties as inequalities? Why was the shortest path distance chosen as the final metric?

5. The LMScorer prompt scoring method performs worse than vector similarity methods in Table 2. Why does a language model struggle to score factual sentences involving taxonomic relations?

6. Fig 2 shows promising improvements from fine-tuning but what is the risk of overfitting to the probes? Were techniques like early stopping used to prevent memorization?

7. For RQ3, a sequential fine-tuning approach is used. Why not adopt more advanced transfer learning techniques like auxiliary losses or adapter modules to retain hierarchy knowledge?

8. The performance gains on downstream tasks are modest. Is there a theoretical analysis of why the hierarchy knowledge does not fully transfer? Is more research needed?

9. How sensitive are the results to the choice of datasets used for probing and downstream tasks? Would gains be more pronounced on specialized hierarchical datasets?

10. The hypernym discovery and taxonomy reconstruction tasks have an hierarchical nature unlike the reading comprehension task. Would a different downstream task selection better demonstrate transferability?
