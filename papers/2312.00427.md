# [From Mutual Information to Expected Dynamics: New Generalization Bounds   for Heavy-Tailed SGD](https://arxiv.org/abs/2312.00427)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper studies generalization bounds for machine learning models obtained from stochastic gradient descent algorithms with heavy-tailed gradient noise. The authors relate the learning dynamics to stochastic differential equations driven by stable Lévy processes. They derive bounds on the generalization gap that depend on the fractal properties of these dynamics, measured by the Hausdorff dimension which relates to the tail index α of the noise distribution. A key contribution is providing bounds without relying on mutual information terms between the dataset and the model trajectory, which are difficult to compute. Instead, they introduce a geometric decoupling term by comparing the empirical dynamics (depending on empirical risk) to an expected dynamics (depending on population risk). This term is further bounded using techniques from heavy-tailed process theory. The authors also propose a PAC-Bayes framework to potentially tighten the bounds, where the geometric decoupling term still plays a crucial role. Overall, the paper provides new generalization error bounds for heavy-tailed learning algorithms based on the geometric properties of the dynamics rather than statistical dependence.
