# [Revisiting Knowledge Distillation for Autoregressive Language Models](https://arxiv.org/abs/2402.11890)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Knowledge distillation (KD) is commonly used to compress large autoregressive language models (LMs) into smaller student models for efficient inference. 
- However, the paper finds a counter-intuitive phenomenon that larger teacher models can dramatically result in poorer student performance, especially when there is a large gap between teacher and student model capabilities.

Analyses & Findings:
- The paper reformulates the classic KD loss as two components: (1) target-oriented KD (TKD) focusing on target token, and (2) diversity-oriented KD (DKD) encouraging diverse knowledge distillation on non-targets. 
- These two components are tied by an uncertainty coefficient (UnC) that reflects the token difficulty.
- Through analyses, the paper finds: (1) UnC measures token learning difficulty; (2) DKD contributes more but is suppressed more for larger teachers; (3) TKD plays different roles for easy vs hard tokens.

Proposed Solution: Adaptive Teaching KD (ATKD)
- The key insight is that different tokens have different optimal teaching modes, which existing KD neglects, causing performance degradation.
- ATKD reduces rote learning and makes teaching more flexible for different tokens:
  - For easy-to-learn tokens, skip target-oriented teaching (TKD) 
  - For hard tokens, use both TKD and enhanced DKD (removing suppression)
- Overall, ATKD allows more adaptive and diverse knowledge transfer per token characteristics.

Contributions:
- Reveals limitation of existing KD in neglecting diverse token teaching needs, especially for large teacher-student gaps.
- Proposes simple yet effective ATKD method to adaptively teach different tokens based on uncertainty.
- Achieves consistent and significant gains over strong KD baselines across models, alleviating poorer students from large teachers.
