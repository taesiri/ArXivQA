# [Cross-lingual Transfer or Machine Translation? On Data Augmentation for   Monolingual Semantic Textual Similarity](https://arxiv.org/abs/2403.05257)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Monolingual semantic textual similarity (STS) is important for natural language understanding but suffers from lack of labeled data in non-English languages.  
- Prior work has used English NLI datasets to train multilingual models via cross-lingual transfer or machine translation, but these techniques have not been directly compared for monolingual STS.

Proposed Solution:
- Compare cross-lingual transfer and machine translation for augmenting data for monolingual STS in Japanese and Korean using unsupervised SimCSE as the training framework.
- Evaluate on multiple STS datasets per language and analyze impact of training data domain (NLI vs Wikipedia).
- Also compare to state-of-the-art multilingual sentence encoder LaBSE.  

Key Findings:
- Cross-lingual transfer and machine translation achieve comparable performance depending on domain of training data.
- Surprisingly, Wikipedia data outperforms NLI data for monolingual STS when used in cross-lingual transfer framework.
- Cross-lingual transfer of Wikipedia data achieves best performance, outperforming or comparable to LaBSE.

Main Contributions:
- First direct comparison of cross-lingual transfer and machine translation for monolingual STS.
- Demonstrates Wikipedia data can outperform NLI data for this task. 
- Shows cross-lingual transfer of Wikipedia data exceeds performance of state-of-the-art multilingual encoder.
- Provides recipe for improving non-English STS via cross-lingual transfer learning using large-scale Wikipedia data.

In summary, the key novelty is in demonstrating the effectiveness of Wikipedia data over expected NLI data for monolingual STS via cross-lingual transfer. The results provide a simple framework for improving multilingual models for this important NLU task.


## Summarize the paper in one sentence.

 This paper compares cross-lingual transfer and machine translation for data augmentation on monolingual semantic textual similarity, finding that cross-lingual transfer of Wikipedia data achieves the best performance comparable to a state-of-the-art multilingual model.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper empirically compares two data augmentation techniques - cross-lingual transfer and machine translation - for the task of monolingual semantic textual similarity (STS) in Japanese and Korean. It finds that:

1) The performance of cross-lingual transfer and machine translation depends on the data domain used. Cross-lingual transfer of Wikipedia data outperforms machine translation of natural language inference (NLI) data. 

2) In contrast to prior work, Wikipedia data can be an effective alternative to NLI data as unlabeled training data for monolingual STS.

3) The combination of cross-lingual transfer and Wikipedia data achieves state-of-the-art performance on monolingual STS, outperforming or being comparable to a supervised multilingual model LaBSE.

In summary, the key contribution is demonstrating the effectiveness of cross-lingual transfer of Wikipedia data for monolingual STS, outlining a recipe for training better sentence embeddings in multiple languages.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it are:

- Sentence embeddings
- Cross-lingual transfer
- Machine translation 
- Data augmentation
- Monolingual semantic textual similarity (STS)
- Natural language inference (NLI)
- Multilingual models
- Unsupervised learning
- SimCSE
- XLM-R
- LaBSE
- Japanese
- Korean

The paper compares cross-lingual transfer and machine translation as data augmentation techniques for the task of monolingual semantic textual similarity. It uses sentence embeddings from models like XLM-R and LaBSE, trained in an unsupervised way using SimCSE on datasets like SNLI and MNLI. The models are evaluated on Japanese and Korean monolingual STS benchmarks. So the key terms reflect this focus on cross-lingual transfer, machine translation, sentence embeddings, unsupervised learning, and evaluation in Japanese and Korean.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. What were the two data augmentation techniques compared in the paper for monolingual semantic textual similarity (STS)? Explain the difference between cross-lingual transfer and machine translation. 

2. What languages were chosen as the test bed for comparing the two data augmentation techniques and why? What makes these languages challenging for the techniques?

3. What was the setup used in the experiments? What model architecture and training procedure were followed? What evaluation metrics were used? 

4. What were the main training datasets used in English, Japanese and Korean? Explain the difference between the NLI and Wikipedia domains and how they were combined.  

5. What were the main evaluation datasets used for monolingual STS in Japanese and Korean? How many datasets were used for each language?

6. What were the key findings from the controlled experiment comparing cross-lingual transfer and machine translation using the NLI domain? Which technique worked better?

7. How did the results change when the Wikipedia domain was used instead of NLI? What do these new findings suggest about the choice of training data?

8. What was the best performing combination found in the experiments? How did it compare against the LaBSE baseline? What explanations are provided?

9. What additional experiments were performed using native Japanese Wikipedia data? What new research questions emerge from these findings?

10. What are the limitations of the current study in terms of language coverage? How can the methodology be extended to truly low-resource languages in the future?
