# [Injecting Wiktionary to improve token-level contextual representations   using contrastive learning](https://arxiv.org/abs/2402.07817)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pretrained language models (PLMs) provide contextual token embeddings that are too specific to each occurrence, making tokens from same word sense too distant in the vector space. This is problematic for lexical semantics tasks. 
- Prior work has used self-supervised contrastive learning (CL) to bring closer embeddings of augmented examples, but no work has investigated using lexicons to supervise the CL.

Proposed Solution:
- Fine-tune a PLM (BERT) using supervised CL, with examples from the English Wiktionary as training data. Each example sentence illustrates a word sense and contains the target word.
- Adapt the loss function to handle multiple positive examples per class, using the other examples of the same word sense as positives.
- Additionally test dimensionality reduction of embeddings before CL, using PCA with whitening.

Main Contributions:
- Show CL fine-tuning improves results on Word-in-Context (WiC) task, establishing new state-of-the-art on WiC test set. Gains generalize over test sets of different sense inventories.
- Outperform prior work using self-supervised CL, showing value of lexicon supervision.
- Observe modest improvements on frame induction task.
- Approach can be extended to many languages having large Wiktionaries.
- Analyze impact of dimensionality reduction which further improves WiC accuracy.

In summary, the paper presents a novel method to inject lexicon information into contextual token embeddings using CL, demonstrating its benefits for lexical semantics. The approach leverages supervision from Wiktionary to outperform prior self-supervised methods.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes fine-tuning contextual word representations of pretrained language models using contrastive learning on examples from the English Wiktionary, showing improvements on the Word-in-Context task and to some extent on the frame induction task.


## What is the main contribution of this paper?

 The main contribution of this paper is a new approach for fine-tuning token-level representations of pretrained language models (PLMs) using contrastive learning with examples from the English Wiktionary. Specifically:

- They adapt a supervised contrastive loss to leverage the example sentences and associated word senses in the Wiktionary as a form of supervision. This allows injecting lexical semantic information into the PLM token representations.

- They show the effectiveness of their fine-tuning method on the Word-in-Context (WiC) task, establishing a new state-of-the-art result on the WiC test set in an unsupervised setting. They also obtain substantial gains on two new WiC test sets.

- They additionally demonstrate modest improvements from their fine-tuned representations on the more downstream task of semantic frame induction. 

- Although they experiment on English, they argue their method is adaptable to other languages with large Wiktionaries available. It provides a simple way to obtain token embeddings better suited for lexical semantics.

In summary, the main contribution is using contrastive learning and Wiktionary supervision to inject lexical semantics into contextual token representations from PLMs, and showing the benefits for lexical semantics tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Contrastive learning (CL) - The paper proposes using contrastive learning to fine-tune contextual word representations. CL involves bringing similar representations closer while pushing dissimilar ones apart.

- Wiktionary - The paper leverages example sentences from Wiktionary as a source of supervision for contrastive learning. 

- Word-in-Context (WiC) task - Used for intrinsic evaluation of the fine-tuned representations. Involves predicting if two usages of a word have the same meaning.

- Frame induction - Used as an extrinsic evaluation task. Involves clustering verbal occurrences into semantic frames. 

- Pretrained language models (PLMs) - The representations that are fine-tuned are from a PLM, specifically BERT-base-uncased.

- Token-level representations - The focus is on improving context-sensitive token-level representations from the PLM using contrastive learning on Wiktionary examples.

- Unsupervised setting - For WiC evaluation, an unsupervised setting is used where the WiC training data is not utilized.

- State-of-the-art (SotA) - The fine-tuned models achieve new SotA results on the WiC test set in an unsupervised setting.

- Multilingual - Although experiments use English data, the method could be applied to other languages with large Wiktionaries.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using contrastive learning with examples from the English Wiktionary to fine-tune token-level representations of PLMs. Why is using a lexicon like Wiktionary beneficial compared to using automatically augmented examples as in previous work? What are the potential limitations of using Wiktionary?

2. The paper adapts the multiple positive contrastive loss proposed by Khosla et al. (2020) to the setting with lexicon examples. What is the intuition behind this loss function and how does it differ from losses used in prior contrastive learning work? 

3. The paper evaluates both intrinsic (Word-in-Context task) and extrinsic (frame induction) benefits of the fine-tuning method. Why is it important to test on both types of evaluations? What do the different results on these two evaluations suggest?

4. For the Word-in-Context (WiC) evaluation, the paper tests on three different datasets with different sense inventories. Why is it useful to have these different test sets? What do the consistent improvements across them demonstrate about the fine-tuning method?

5. The paper establishes a new state-of-the-art on the WiC benchmark in the unsupervised setting. How does this result compare to prior supervised approaches? What implications does this have?

6. While the fine-tuning brings substantial gains on the WiC task, improvements are more modest for frame induction. What explanations are provided in the paper for why this may be the case?

7. The paper experiments with PCA dimensionality reduction of the token embeddings. What impact does this have on the WiC and frame induction results? What are potential benefits of using PCA?

8. The paper only experiments on English data but claims the method can be extended to other languages. What properties of the method make this extension feasible? What steps would need to be taken to apply it to a new language?

9. The training data used focuses only on verbal lemmas. How could the coverage be expanded to other parts of speech? Would this likely improve or potentially hurt the fine-tuning?

10. The paper mentions promising future work directions such as creating positive pairs between different lemmas using Wiktionary. What is the intuition behind this idea? What challenges do you foresee in implementing it effectively?
