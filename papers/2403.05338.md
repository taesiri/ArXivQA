# [Explaining Pre-Trained Language Models with Attribution Scores: An   Analysis in Low-Resource Settings](https://arxiv.org/abs/2403.05338)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Explainability of machine learning models is important, especially when deploying them in real-world applications. Two main challenges are the need to explain model predictions and the need to adapt models to low-resource scenarios.
- Prior work has mostly studied model explanations for fine-tuned models, not for prompt-based models which work well in low-resource settings. 
- It is unclear how plausible and faithful explanations from prompt-based models are, especially compared to fine-tuned models. It is also unclear which explanation method works best to explain prompt-based models.

Methods:
- Extract explanations (attention scores, integrated gradients, Shapley values) from prompt-based models and fine-tuned models trained on sentiment analysis and natural language inference tasks.
- Evaluate explanations w.r.t. plausibility (human perspective) and faithfulness (model perspective), considering different training set sizes.
- Compare a large language model (decoder-only) to encoder-based prompt model.
- Use statistical significance testing to analyze differences.

Key Findings:
- Prompt-based models generate more plausible explanations than fine-tuned models in low-resource settings.  
- Shapley Value Sampling consistently outperforms attention and integrated gradients in terms of plausibility and faithfulness.
- Findings transfer from encoder-based prompt models to decoder-based language models.

Main Contributions:
- First analysis and comparison of explanations from prompt-based versus fine-tuned models
- Analysis along different training set sizes 
- Showing the superiority of Shapley Value Sampling for explaining prompt-based models
- Demonstrating that prompting works better than fine-tuning not only for model performance but also for model explainability
