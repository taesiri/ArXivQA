# [Weakly-Supervised HOI Detection from Interaction Labels Only and   Language/Vision-Language Priors](https://arxiv.org/abs/2303.05546)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper tackles the problem of weakly-supervised human-object interaction (HOI) detection, using only image-level interaction labels as supervision (e.g. "ride"). To make learning possible under this weak supervision, the authors exploit the implicit grounding capability of a vision-language model (CLIP) to prune unlikely interacting human/object proposals from the images. They also restrict the output space to only natural interactions by querying a large language model on whether a predicted <interaction, object> pair is plausible. Additionally, they formulate an auxiliary weakly-supervised preposition prediction task to improve the model's spatial reasoning ability. Their method outperforms prior work on weakly-supervised HOI detection by a large margin on the V-COCO dataset. For the first time, they also demonstrate training an HOI detector on image-caption pairs scraped from the web, without any HOI annotations, by extracting labels from captions. Through extensive experiments and ablations, they verify the effectiveness of their contributions over the weakly-supervised baseline.


## Summarize the paper in one sentence.

 This paper proposes a weakly-supervised approach for human-object interaction detection that utilizes image-level interaction labels, prunes non-interacting proposals with a vision-language model, restricts the output space to plausible interactions with a language model, and improves spatial reasoning via a novel preposition prediction task.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this paper are:

1) Formulating a weakly-supervised HOI (human-object interaction) detection setting where supervision comes only from image-level interaction labels (e.g. "ride", "eat"). This is the weakest supervision used for HOI detection in the literature.

2) Using a vision-language model (VLM) to exploit its implicit grounding capability and prune non-interacting human and object proposals.

3) Using a large language model (LLM) to verify if a given <interaction, object> pair is plausible, in order to restrict the model's output space to only natural interactions. 

4) Formulating a weakly-supervised preposition prediction task to improve the model's spatial reasoning capability.

5) Training an HOI detection model on image-caption pairs scraped from the web, extracting image-level interaction labels from the captions. This is the first work to train an HOI detector without any HOI-specific annotation.

In summary, the main contribution is formulating and tackling the HOI detection problem under very weak supervision, using vision-language and language models to aid the learning, as well as training on freely available image-caption data instead of costly HOI annotations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Human-object interaction (HOI) detection
- Weakly-supervised learning
- Multiple instance learning (MIL)
- Vision-language models (VLM)
- Large language models (LLM) 
- Proposal pruning
- Plausible interaction suppression
- Preposition prediction
- Image-level interaction labels
- HICO-DET dataset
- V-COCO dataset
- Conceptual Captions dataset

The paper tackles the problem of HOI detection using only weak image-level supervision in the form of interaction labels (e.g. "ride", "eat"). It formulates the problem as an MIL task and utilizes VLMs and LLMs in novel ways to prune proposals, restrict the output space, and improve spatial reasoning. The method is evaluated on standard HOI detection datasets HICO-DET and V-COCO, as well as a subset of the Conceptual Captions dataset. Key terms like "weakly-supervised", "multiple instance learning", "vision-language models", and "proposal pruning" capture the core ideas and technical contributions of the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using a vision-language model to prune non-interacting human and object proposals. What is the intuition behind using a vision-language model for this task instead of just relying on the object detector scores? How exactly does the vision-language model help identify interacting proposals?

2. The paper queries a large language model to restrict the output space to only plausible human-object interactions. What are the two approaches explored to query the language model? What are the tradeoffs between these two approaches? 

3. The paper formulates a weakly-supervised preposition prediction task. Why is this beneficial for the overall HOI detection task? What kind of spatial reasoning capability does this provide to the model? 

4. The paper extracts interaction labels from image captions using a simple technique based on POS tagging and synonym matching. What are the limitations of this technique? How could this process be improved to handle more complex captions?  

5. The paper benchmarks performance on HICO-DET and V-COCO datasets. Why does the performance difference between the proposed method and baselines reduce on HICO-DET compared to V-COCO? What causes this?

6. Could the proposed weakly-supervised method work for video HOI detection? What modifications would be needed? What additional cues could be exploited from the video setting?

7. The paper relies on an off-the-shelf object detector. How much does performance depend on the quality and diversity of object proposals? What enhancements could make the method more robust to missed object proposals?  

8. What other vision-language models beyond CLIP could be explored for pruning proposals? What properties should an ideal VLM have for this task?

9. For the language model, the paper uses RoBERTa. How sensitive are the results to the choice of language model? What factors determine what language model would work best?

10. The paper demonstrates learning an HOI model from image-caption pairs scraped from the web. What other weakly labeled or unlabeled multimodal data could this approach exploit? How can we scale up the training data?
