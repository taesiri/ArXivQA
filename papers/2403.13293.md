# [Building Optimal Neural Architectures using Interpretable Knowledge](https://arxiv.org/abs/2403.13293)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Neural Architecture Search (NAS) is a costly process due to the exponential size of the search space and the high cost of evaluating each architecture. Recent methods like weight-sharing networks reduce evaluation cost but have high upfront training costs and are dataset-specific. As models grow more complex, NAS is becoming infeasible for many organizations. 

Solution - AutoBuild:
The paper proposes AutoBuild, a method to automatically build high-quality neural architectures by learning to quantify and rank the importance of architecture building blocks (operations, sequences, subgraphs). It applies a ranking loss to a graph neural network predictor to align embedding norms of architecture modules with ground-truth performance. This allows interpretable importance scores to be assigned to modules to construct architectures directly or reduce the search space for more efficient NAS.

Key Contributions:

- Proposes a hop-level ranking loss to learn a Magnitude Ranked Embedding Space where GNN node/subgraph embeddings are aligned with architecture performance. Allows importance scores to be assigned to modules.

- Introduces mechanisms to compare and select architecture modules of different sizes using distribution shift statistics computed from the dataset.

- Leverages a Feature Embedding MLP to decompose and quantify the importance of individual operation-level features like kernel size.

- Evaluates AutoBuild on macro search spaces for image classification, segmentation and text-to-image tasks. Shows AutoBuild can construct high-performing architectures using insights learned from only thousands of samples, instead of tens/hundreds of thousands like NAS methods.

- Demonstrates AutoBuild can effectively reduce NAS search spaces by 1-9 orders of magnitude and attain superior accuracy-latency Pareto frontiers for mobile vision models compared to NAS baselines.

In summary, AutoBuild provides an interpretable, data-driven solution to learn how to build neural architectures rather than rely on expensive search. It has broad applicability and outperforms NAS baselines across multiple domains.
