# [Solving Label Variation in Scientific Information Extraction via   Multi-Task Learning](https://arxiv.org/abs/2312.15751)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There are two popular datasets for scientific information extraction (ScientificIE) - SemEval-2018 and SciERC. These datasets have 307 overlapped abstracts but differ significantly in their annotations, leading to label variations/inconsistencies.  
- Traditional models struggle to effectively handle these label variations. This poses challenges for training accurate ScientificIE systems.

Proposed Solution:
- The paper proposes a multi-task learning approach to handle label variations. 
- It extends the SpERT model to have multiple heads, allowing it to learn from multiple annotation perspectives jointly.
- It also generates "soft labels" from multi-level human agreements to capture annotation ambiguities. These are used alongside the hard labels to make the model more robust.

Key Contributions:
- Demonstrates that multi-task learning can mitigate the impact of label noise and improve performance on both named entity recognition and relation extraction.
- Soft labels enhance model robustness, especially for ambiguous instances.
- Analysis shows label variations can reduce data requirements and provide richer information.
- The proposed model achieves state-of-the-art on the SciERC benchmark and outperforms baseline models, highlighting the utility of modeling label variations.
- Overall, shows the importance of releasing and modeling variation labels to handle ambiguity and enhance ScientificIE systems.

In summary, the paper tackles label inconsistencies in ScientificIE via multi-task learning and soft labeling to achieve improved performance and robustness. The analysis provides insights into the value of capturing annotator disagreements.
