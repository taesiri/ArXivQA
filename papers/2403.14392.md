# [A Bag of Tricks for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2403.14392)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper tackles the problem of few-shot class-incremental learning (FSCIL). FSCIL involves continuously adapting a model to learn new classes, with only a few labelled samples per class available during training. This is a challenging problem as models need to balance stability (retaining knowledge of old classes) and adaptability (effectively learning new classes). Most prior methods are unable to achieve this balance, performing well on either old or new classes but not both.

Proposed Solution:  
The paper presents a "bag of tricks" framework that combines 8 complementary techniques to simultaneously enhance stability, adaptability and overall performance of models for FSCIL. The tricks are organized into - (i) stability tricks: improves separation between old class embeddings to minimize interference when learning new classes (e.g. supervised contrastive loss, pre-assigning prototypes); (ii) adaptability tricks: provides model ability to effectively learn new classes (e.g. incremental fine-tuning, subnet tuning); (iii) training tricks: boosts overall performance without compromising stability or adaptability (e.g. larger encoder, additional pre-training, incorporating auxiliary self-supervised task).

Main Contributions:
- Proposes a new framework that combines 8 complementary tricks that have never been explored together to push state-of-the-art in FSCIL.
- Tricks are systematically categorized based on their impact towards improving stability, adaptability and overall performance.  
- Conducts detailed analysis on the behaviour of different tricks and demonstrates their effectiveness.
- Outperforms prior state-of-the-art methods on CIFAR-100, CUB-200 and miniImageNet datasets, establishing new benchmark for future research.
- Evaluates framework over different shot sizes and demonstrates consistent gains, especially in extreme low-shot regimes of 1-shot and 2-shot learning.
