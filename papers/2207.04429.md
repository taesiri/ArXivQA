# LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,
  Vision, and Action

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we develop a robotic navigation system that can follow free-form natural language instructions over long horizons, by combining pre-trained models of language, vision, and robotic control, without requiring any fine-tuning or human annotations on navigation datasets?The key hypothesis is that by intelligently combining and interfacing large pre-trained models for language (GPT-3), vision-language grounding (CLIP), and visual navigation (ViNG), the resulting system can follow unconstrained natural language commands to navigate complex real-world environments, without needing any labeled navigation data. The authors aim to show that these powerful pre-trained models can enable instruction following without task-specific fine-tuning, by essentially using them as modular components and connecting them through probabilistic inference and graph search. The approach aims to leverage the impressive few-shot generalization capabilities of models like GPT-3 and CLIP to construct a natural language interface for robotic control.So in summary, the paper explores whether independent pre-training plus smart interfaces between modules is sufficient for navigational instruction following, removing the need for navigation-specific training.


## What is the main contribution of this paper?

This paper presents LM-Nav, a system for robotic navigation from natural language instructions. The main contribution is showing that an embodied instruction following system can be built by combining three large pre-trained models, without requiring any fine-tuning or labeled navigation data:- A visual navigation model (ViNG) that creates a topological map from observations and can navigate to goals. - A vision-language model (CLIP) that grounds textual descriptions in visual observations.- A large language model (GPT-3) that parses instructions into landmarks.These models are combined through a probabilistic graph search formulation to follow free-form instructions over long horizons. The key insight is that modern pre-trained models can enable complex robotic tasks by serving as interfaces between modalities, without task-specific fine-tuning.The main result is an instantiation of this idea on a mobile robot, where LM-Nav can follow natural language commands to navigate outdoors over hundreds of meters, choosing appropriate landmarks and handling mistakes. This demonstrates the potential of using pre-trained models to decompose and solve robotic control problems, without expensive annotation.In summary, the key contribution is showing how off-the-shelf pre-trained models can be combined for instruction following on a real robot, proposing a general paradigm for utilizing such models in robotic systems without fine-tuning.
