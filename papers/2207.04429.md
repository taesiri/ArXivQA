# LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,   Vision, and Action

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we develop a robotic navigation system that can follow free-form natural language instructions over long horizons, by combining pre-trained models of language, vision, and robotic control, without requiring any fine-tuning or human annotations on navigation datasets?The key hypothesis is that by intelligently combining and interfacing large pre-trained models for language (GPT-3), vision-language grounding (CLIP), and visual navigation (ViNG), the resulting system can follow unconstrained natural language commands to navigate complex real-world environments, without needing any labeled navigation data. The authors aim to show that these powerful pre-trained models can enable instruction following without task-specific fine-tuning, by essentially using them as modular components and connecting them through probabilistic inference and graph search. The approach aims to leverage the impressive few-shot generalization capabilities of models like GPT-3 and CLIP to construct a natural language interface for robotic control.So in summary, the paper explores whether independent pre-training plus smart interfaces between modules is sufficient for navigational instruction following, removing the need for navigation-specific training.


## What is the main contribution of this paper?

This paper presents LM-Nav, a system for robotic navigation from natural language instructions. The main contribution is showing that an embodied instruction following system can be built by combining three large pre-trained models, without requiring any fine-tuning or labeled navigation data:- A visual navigation model (ViNG) that creates a topological map from observations and can navigate to goals. - A vision-language model (CLIP) that grounds textual descriptions in visual observations.- A large language model (GPT-3) that parses instructions into landmarks.These models are combined through a probabilistic graph search formulation to follow free-form instructions over long horizons. The key insight is that modern pre-trained models can enable complex robotic tasks by serving as interfaces between modalities, without task-specific fine-tuning.The main result is an instantiation of this idea on a mobile robot, where LM-Nav can follow natural language commands to navigate outdoors over hundreds of meters, choosing appropriate landmarks and handling mistakes. This demonstrates the potential of using pre-trained models to decompose and solve robotic control problems, without expensive annotation.In summary, the key contribution is showing how off-the-shelf pre-trained models can be combined for instruction following on a real robot, proposing a general paradigm for utilizing such models in robotic systems without fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents LM-Nav, a robotic navigation system that combines pre-trained models for language, vision, and control to follow free-form natural language instructions in real-world environments, without requiring any fine-tuning or human-annotated data.


## How does this paper compare to other research in the same field?

Based on my reading, this paper presents a novel approach to robotic navigation and instruction following by combining three large pre-trained models (a visual navigation model, a vision-language model, and a language model) without requiring any fine-tuning or annotated data from the target environment. Here is a brief comparison to related works:- Compared to prior work on vision-and-language navigation (VLN) that requires large datasets of annotated trajectories, this method does not rely on any environment-specific annotations. It can follow free-form instructions in new environments using just raw onboard observations.- Compared to other methods that use pre-trained vision-language models like CLIP for instruction following, this combines it with a self-supervised navigation model (VIN) to handle the navigation and control. It does not assume the base agent can already follow textual commands.- Compared to end-to-end VLN methods, the modular design based on pre-trained components provides more interpretability and enables analyzing the impact of each module. But it relies on landmarks and does not use language beyond landmarks.- Compared to classical semantic parsing approaches that map instructions to formal languages, this learns the parsing implicitly using the pre-trained LLM rather than an explicit grammar.- Compared to other self-supervised navigation approaches, the addition of the LLM and VLM allow handling natural language commands, while retaining the generality of self-supervised training.So in summary, the key novelty seems to be the combination of modern pre-trained vision, language and robotic models to follow free-form instructions on a real robot without environment-specific annotations. The results demonstrate promising generalization capabilities.


## What future research directions do the authors suggest?

The authors suggest some future research directions in the Discussion section:- Grounding verbs and other nuanced commands in instructions. Currently, LM-Nav only focuses on the landmark nouns and ignores verbs or other details like "drive slowly". Enabling the grounding of verbs and commands like "go straight" would make the system more capable.- Developing a more general "large navigation model" that can work across different robot platforms, not just the Clearpath Jackal used in this work. The language and vision models (LLM and VLM) are quite general, but the visual navigation model (VNM) is specific to the Jackal. Creating a more generic VNM would improve the applicability of the system. - Exploring how modern pre-trained models can enable effective decomposition of robotic control into modules like language understanding, vision, and navigation. The authors hope this work provides a step towards such modularization, where each module is pre-trained separately on large datasets, then combined to solve tasks without environment-specific training. Further pursuing this direction could enable more generalizable robotic systems.- Leveraging improvements in self-supervised goal-conditioned policies to directly benefit instruction following systems like LM-Nav. Since LM-Nav utilizes a self-supervised navigation policy, progress in that area could integrate well.In summary, the main future directions are: enabling grounding of more detailed commands, developing more generic navigation models, using pre-trained models to decompose robotic control, and benefiting from advances in self-supervised robot learning.
