# LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,   Vision, and Action

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we develop a robotic navigation system that can follow free-form natural language instructions over long horizons, by combining pre-trained models of language, vision, and robotic control, without requiring any fine-tuning or human annotations on navigation datasets?The key hypothesis is that by intelligently combining and interfacing large pre-trained models for language (GPT-3), vision-language grounding (CLIP), and visual navigation (ViNG), the resulting system can follow unconstrained natural language commands to navigate complex real-world environments, without needing any labeled navigation data. The authors aim to show that these powerful pre-trained models can enable instruction following without task-specific fine-tuning, by essentially using them as modular components and connecting them through probabilistic inference and graph search. The approach aims to leverage the impressive few-shot generalization capabilities of models like GPT-3 and CLIP to construct a natural language interface for robotic control.So in summary, the paper explores whether independent pre-training plus smart interfaces between modules is sufficient for navigational instruction following, removing the need for navigation-specific training.


## What is the main contribution of this paper?

This paper presents LM-Nav, a system for robotic navigation from natural language instructions. The main contribution is showing that an embodied instruction following system can be built by combining three large pre-trained models, without requiring any fine-tuning or labeled navigation data:- A visual navigation model (ViNG) that creates a topological map from observations and can navigate to goals. - A vision-language model (CLIP) that grounds textual descriptions in visual observations.- A large language model (GPT-3) that parses instructions into landmarks.These models are combined through a probabilistic graph search formulation to follow free-form instructions over long horizons. The key insight is that modern pre-trained models can enable complex robotic tasks by serving as interfaces between modalities, without task-specific fine-tuning.The main result is an instantiation of this idea on a mobile robot, where LM-Nav can follow natural language commands to navigate outdoors over hundreds of meters, choosing appropriate landmarks and handling mistakes. This demonstrates the potential of using pre-trained models to decompose and solve robotic control problems, without expensive annotation.In summary, the key contribution is showing how off-the-shelf pre-trained models can be combined for instruction following on a real robot, proposing a general paradigm for utilizing such models in robotic systems without fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents LM-Nav, a robotic navigation system that combines pre-trained models for language, vision, and control to follow free-form natural language instructions in real-world environments, without requiring any fine-tuning or human-annotated data.


## How does this paper compare to other research in the same field?

Based on my reading, this paper presents a novel approach to robotic navigation and instruction following by combining three large pre-trained models (a visual navigation model, a vision-language model, and a language model) without requiring any fine-tuning or annotated data from the target environment. Here is a brief comparison to related works:- Compared to prior work on vision-and-language navigation (VLN) that requires large datasets of annotated trajectories, this method does not rely on any environment-specific annotations. It can follow free-form instructions in new environments using just raw onboard observations.- Compared to other methods that use pre-trained vision-language models like CLIP for instruction following, this combines it with a self-supervised navigation model (VIN) to handle the navigation and control. It does not assume the base agent can already follow textual commands.- Compared to end-to-end VLN methods, the modular design based on pre-trained components provides more interpretability and enables analyzing the impact of each module. But it relies on landmarks and does not use language beyond landmarks.- Compared to classical semantic parsing approaches that map instructions to formal languages, this learns the parsing implicitly using the pre-trained LLM rather than an explicit grammar.- Compared to other self-supervised navigation approaches, the addition of the LLM and VLM allow handling natural language commands, while retaining the generality of self-supervised training.So in summary, the key novelty seems to be the combination of modern pre-trained vision, language and robotic models to follow free-form instructions on a real robot without environment-specific annotations. The results demonstrate promising generalization capabilities.
