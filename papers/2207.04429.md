# LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,
  Vision, and Action

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we develop a robotic navigation system that can follow free-form natural language instructions over long horizons, by combining pre-trained models of language, vision, and robotic control, without requiring any fine-tuning or human annotations on navigation datasets?The key hypothesis is that by intelligently combining and interfacing large pre-trained models for language (GPT-3), vision-language grounding (CLIP), and visual navigation (ViNG), the resulting system can follow unconstrained natural language commands to navigate complex real-world environments, without needing any labeled navigation data. The authors aim to show that these powerful pre-trained models can enable instruction following without task-specific fine-tuning, by essentially using them as modular components and connecting them through probabilistic inference and graph search. The approach aims to leverage the impressive few-shot generalization capabilities of models like GPT-3 and CLIP to construct a natural language interface for robotic control.So in summary, the paper explores whether independent pre-training plus smart interfaces between modules is sufficient for navigational instruction following, removing the need for navigation-specific training.
