# [VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval](https://arxiv.org/abs/2211.12764)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we efficiently adapt large pre-trained vision-language models like CLIP to downstream video-based tasks like text-video retrieval, without significant fine-tuning that may cause overfitting or forgetting of pre-trained knowledge?

The key hypotheses appear to be:

1) By freezing the backbone model and only tuning small additional prompt parameters, we can efficiently adapt the model while retaining most of the pre-trained knowledge. 

2) Designing video-specific prompt solutions that incorporate positional, contextual, and functional information can further improve adaptation and performance on text-video retrieval compared to just using regular visual prompts.

3) Combinations of text and video prompt tuning can match or exceed the performance of full fine-tuning of the entire model, but with much lower training overhead.

So in summary, the central research direction is efficient and effective adaptation of large pre-trained vision-language models to video domains via prompt tuning, with a focus on the text-video retrieval task. The key hypotheses relate to the benefits of collaborative text and video prompt design for model tuning.
