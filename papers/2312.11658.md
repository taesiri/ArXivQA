# [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) for code are being increasingly used for software engineering tasks like code generation and completion. These models are trained on large amounts of code data scraped from the internet. There is a concern that these models may be memorizing and later emitting private data or code snippets from their training data, leading to privacy violations or potential copyright issues. 

However, there has been little empirical analysis quantifying the extent of memorization in LLMs trained on code. Prior work has mostly focused on natural language models. It is important to analyze memorization in code models given additional considerations around licensing and exposure of credentials/API keys in code.

Proposed Solution:
This paper proposes a novel framework to measure memorization in LLMs using a "data extraction security game". The framework involves constructing a dataset of extractable code snippets that exist in the model's training data, prompting the model with a prefix from those snippets, and checking if the model can generate the original suffix from its training data. 

The rate of exact match between generated and original suffixes estimates the memorization rate. Using this framework, the authors construct a benchmark Python code dataset and test memorization in a variety of code LLMs and compare with natural language models.

Key Contributions:

- Framework to quantify memorization rates in code LLMs using "data extraction security game"
- Evaluation of memorization in 10 code LLMs of varying sizes
- Code models memorize less than natural language models, but memorization still exists and increases with model size 
- Information-dense data like dictionaries are memorized at higher rates
- Different model architectures memorize different samples
- Even after fine-tuning, models can still emit pre-training data

The paper urges more comprehensive analysis of memorization in code models and development of safeguards, given the privacy and legal risks of leaked training data. The study methodology and framework introduced enables such analysis.
