# [Visually Guided Self Supervised Learning of Speech Representations](https://arxiv.org/abs/2001.04316)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn useful speech representations in a self-supervised manner using visual supervision. Specifically, the authors propose a framework for learning audio representations guided by the visual modality in the context of audiovisual speech. The key hypothesis is that by generating video of speech from audio, the audio encoder network will be driven to learn speech representations that capture information relevant for producing realistic facial movements and lip synchronization. These visually-guided speech representations can then be evaluated on downstream speech tasks like emotion recognition and speech recognition without requiring any manual annotation.In summary, the main research question is: Can visual supervision be exploited to learn good self-supervised speech representations? The key hypothesis is that an audio encoder trained to generate video of speech will capture useful speech information like emotions and phonemes needed to produce proper facial expressions and lip movements.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for learning audio representations guided by the visual modality in the context of audiovisual speech. Specifically:- They employ a generative audio-to-video training scheme where they animate a still image corresponding to a given audio clip and optimize the generated video to match the real video. - Through this process, the audio encoder network learns useful speech representations that they evaluate on emotion recognition and speech recognition tasks. - They achieve state-of-the-art results for emotion recognition and competitive results for speech recognition using their self-supervised audio features.- This demonstrates the potential of using visual supervision, i.e. generating facial expressions and lip movements from audio, as a way to learn useful audio representations in a self-supervised manner. In summary, the key contribution is showing that visual modality can guide the learning of audio representations for speech in a cross-modal self-supervised approach, which has not been explored before. The results show this is a promising direction for unsupervised speech representation learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a framework for learning speech representations in an unsupervised manner by using visual information from faces as supervision. Specifically, it generates videos of facial movements from audio and optimizes the generated video to match real videos, which helps the audio encoder learn useful speech features that can be applied to emotion recognition and speech recognition tasks.
