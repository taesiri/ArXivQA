# [Visually Guided Self Supervised Learning of Speech Representations](https://arxiv.org/abs/2001.04316)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn useful speech representations in a self-supervised manner using visual supervision. Specifically, the authors propose a framework for learning audio representations guided by the visual modality in the context of audiovisual speech. The key hypothesis is that by generating video of speech from audio, the audio encoder network will be driven to learn speech representations that capture information relevant for producing realistic facial movements and lip synchronization. These visually-guided speech representations can then be evaluated on downstream speech tasks like emotion recognition and speech recognition without requiring any manual annotation.In summary, the main research question is: Can visual supervision be exploited to learn good self-supervised speech representations? The key hypothesis is that an audio encoder trained to generate video of speech will capture useful speech information like emotions and phonemes needed to produce proper facial expressions and lip movements.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for learning audio representations guided by the visual modality in the context of audiovisual speech. Specifically:- They employ a generative audio-to-video training scheme where they animate a still image corresponding to a given audio clip and optimize the generated video to match the real video. - Through this process, the audio encoder network learns useful speech representations that they evaluate on emotion recognition and speech recognition tasks. - They achieve state-of-the-art results for emotion recognition and competitive results for speech recognition using their self-supervised audio features.- This demonstrates the potential of using visual supervision, i.e. generating facial expressions and lip movements from audio, as a way to learn useful audio representations in a self-supervised manner. In summary, the key contribution is showing that visual modality can guide the learning of audio representations for speech in a cross-modal self-supervised approach, which has not been explored before. The results show this is a promising direction for unsupervised speech representation learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a framework for learning speech representations in an unsupervised manner by using visual information from faces as supervision. Specifically, it generates videos of facial movements from audio and optimizes the generated video to match real videos, which helps the audio encoder learn useful speech features that can be applied to emotion recognition and speech recognition tasks.


## How does this paper compare to other research in the same field?

Here are some key things I noticed about how this paper compares to other research in self-supervised speech representation learning:- The main novelty of this paper is using visual information from facial movements during speech to guide the learning of audio representations. Most prior self-supervised speech work focuses just on the audio modality alone. Using cross-modal supervision from vision is a new approach.- For emotion recognition, this method achieves state-of-the-art results on CREMA-D and Ravdess datasets, outperforming other audio-only self-supervised methods like CPC, APC, and PASE. This demonstrates the benefits of the visual guidance.- For speech recognition, the results are competitive but not state-of-the-art compared to other self-supervised methods like PASE. This suggests there is room for improvement in learning speech representations tailored for ASR.- The generative framework of animating faces conditioned on audio is similar to other cross-modal generation works, but the application to self-supervised representation learning is novel.- The approach can leverage large amounts of unlabeled audiovisual speech data, which is a strength over supervised methods reliant on labeled data.- The features learned could likely be further improved by using even larger and more diverse datasets for pretraining, as the results generalized decently from different pretrain/test splits.In summary, the key novel contribution is using visual supervision from facial movements to guide audio representation learning in a generative framework. This shows promising results on emotion recognition but is currently still behind state of the art methods for speech recognition. The approach opens up an interesting new direction for exploiting cross-modal self-supervision.
