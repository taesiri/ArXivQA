# [Training and inference of large language models using 8-bit floating   point](https://arxiv.org/abs/2309.17224)

## What is the central research question or hypothesis that this paper addresses?

 From my reading, the central research question addressed in this paper is:

How can we effectively leverage 8-bit floating point (FP8) formats for both training and inference of large deep learning models like GPT and Llama, while preventing degradation in accuracy that can occur due to the reduced dynamic range of FP8 compared to higher precision formats?

The key contributions aimed at addressing this question seem to be:

1. Presenting a methodology to select per-tensor scaling biases for the linear layers in large language models like GPT and Llama when using FP8. This helps shift the representable range to prevent underflow or overflow.

2. Showing this methodology allows training and inference with FP8 for GPT and Llama models ranging from 111M to 70B parameters without accuracy degradation compared to FP16.

3. Providing insights into how the scaling biases evolve during training and how they vary across layers, which helps explain why constant per-tensor scales may not be robust.

4. Detailing how the methodology can be used for post-training quantization of models to FP8 for inference.

5. Discussing the tradeoffs between dynamically computing scaling biases (FP8-AMAX) versus using constant scales (FP8-CSCALE).

So in summary, the main research focus is on effectively using FP8 for large models by properly selecting per-tensor scaling biases during training and inference.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a methodology to select the per-tensor scaling biases when quantizing the linear layers of large language models to 8-bit floating point (FP8). The paper focuses on two main aspects:

1) Inference with FP8: The authors detail a methodology to quantize a pre-trained model from FP16/FP32 to FP8 without accuracy loss, using a technique called post-training quantization (PTQ). This involves computing scaling biases for the weights and activations of each layer to prevent underflow or overflow when casting to the more limited FP8 range.

2) Training with FP8: The authors show how their methodology of computing per-tensor scaling biases can also enable training large transformer models with FP8, by dynamically updating the scales during training to account for the changing distributions of weights, activations and gradients. 

The key novel aspects are:

- Providing specific methodology and pseudocode for computing the FP8 scaling biases, which has been lacking in previous FP8 work.

- Demonstrating this methodology scales to very large language models up to 70B parameters for both inference and training in FP8.

- Analyzing how the scaling biases evolve during training and what insights this provides about the FP8 quantization dynamics.

Overall, the paper focuses on the practical details to make FP8 work effectively for large language models, providing guidance for practitioners aiming to leverage FP8's efficiency benefits. The scaling methodology and insights around model size scaling are the main contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper presents a methodology to select per-tensor scaling factors when quantizing large language models to 8-bit floating point (FP8) for training and inference. The key points are:

- They propose specific techniques to compute scaling biases for weights, activations, and gradients in FP8 linear layers of models like GPT and Llama. 

- The methodology dynamically updates these per-tensor scales during training to prevent underflow/overflow from the reduced FP8 range.

- Experiments show this methodology enables inference and training convergence for models up to 70 billion parameters without accuracy loss compared to FP16/32.

- Analysis of the evolving scaling factors provides insights into FP8 quantization behavior and why per-tensor scales are needed.

In summary, the paper develops a methodology for per-tensor scaling to leverage FP8 quantization in large language models, demonstrating techniques to match full-precision accuracy.
