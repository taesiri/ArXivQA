# [Training and inference of large language models using 8-bit floating   point](https://arxiv.org/abs/2309.17224)

## What is the central research question or hypothesis that this paper addresses?

 From my reading, the central research question addressed in this paper is:

How can we effectively leverage 8-bit floating point (FP8) formats for both training and inference of large deep learning models like GPT and Llama, while preventing degradation in accuracy that can occur due to the reduced dynamic range of FP8 compared to higher precision formats?

The key contributions aimed at addressing this question seem to be:

1. Presenting a methodology to select per-tensor scaling biases for the linear layers in large language models like GPT and Llama when using FP8. This helps shift the representable range to prevent underflow or overflow.

2. Showing this methodology allows training and inference with FP8 for GPT and Llama models ranging from 111M to 70B parameters without accuracy degradation compared to FP16.

3. Providing insights into how the scaling biases evolve during training and how they vary across layers, which helps explain why constant per-tensor scales may not be robust.

4. Detailing how the methodology can be used for post-training quantization of models to FP8 for inference.

5. Discussing the tradeoffs between dynamically computing scaling biases (FP8-AMAX) versus using constant scales (FP8-CSCALE).

So in summary, the main research focus is on effectively using FP8 for large models by properly selecting per-tensor scaling biases during training and inference.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a methodology to select the per-tensor scaling biases when quantizing the linear layers of large language models to 8-bit floating point (FP8). The paper focuses on two main aspects:

1) Inference with FP8: The authors detail a methodology to quantize a pre-trained model from FP16/FP32 to FP8 without accuracy loss, using a technique called post-training quantization (PTQ). This involves computing scaling biases for the weights and activations of each layer to prevent underflow or overflow when casting to the more limited FP8 range.

2) Training with FP8: The authors show how their methodology of computing per-tensor scaling biases can also enable training large transformer models with FP8, by dynamically updating the scales during training to account for the changing distributions of weights, activations and gradients. 

The key novel aspects are:

- Providing specific methodology and pseudocode for computing the FP8 scaling biases, which has been lacking in previous FP8 work.

- Demonstrating this methodology scales to very large language models up to 70B parameters for both inference and training in FP8.

- Analyzing how the scaling biases evolve during training and what insights this provides about the FP8 quantization dynamics.

Overall, the paper focuses on the practical details to make FP8 work effectively for large language models, providing guidance for practitioners aiming to leverage FP8's efficiency benefits. The scaling methodology and insights around model size scaling are the main contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper presents a methodology to select per-tensor scaling factors when quantizing large language models to 8-bit floating point (FP8) for training and inference. The key points are:

- They propose specific techniques to compute scaling biases for weights, activations, and gradients in FP8 linear layers of models like GPT and Llama. 

- The methodology dynamically updates these per-tensor scales during training to prevent underflow/overflow from the reduced FP8 range.

- Experiments show this methodology enables inference and training convergence for models up to 70 billion parameters without accuracy loss compared to FP16/32.

- Analysis of the evolving scaling factors provides insights into FP8 quantization behavior and why per-tensor scales are needed.

In summary, the paper develops a methodology for per-tensor scaling to leverage FP8 quantization in large language models, demonstrating techniques to match full-precision accuracy.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to related work:

- The paper presents a methodology for selecting per-tensor scaling factors when quantizing large language models to 8-bit floating point (FP8). This addresses an important practical issue when moving to lower precision, as FP8 has a reduced dynamic range compared to higher precision formats like FP16 or FP32. 

- Prior work on FP8 quantization like Noune et al. (2022) and Micikevicius et al. (2022) introduced the idea of using per-tensor scaling biases but did not provide much detail on how to select these in practice. Other works have relied on sweeps or heuristics. This paper provides an explicit methodology and tests it across a range of model sizes.

- For inference, the paper shows the proposed methodology enables post-training quantization to FP8 without accuracy degradation. This is a useful result as post-training quantization is typically easier than quantization-aware training. Prior INT8 inference work has required more complex schemes to maintain accuracy.

- For training, the paper demonstrates FP8 fine-tuning is possible using the proposed scaling methodology. Prior work has considered FP8 training infeasible and focused on mixed precision approaches. Showing FP8 training is possible could open up further efficiency gains.

- The analysis of how scaling factors evolve during training provides novel insights into the dynamics of lower precision training. For example, the authors show scaling factors for weights and activations are relatively stable, while gradients require more frequent adjustment.

- The experiments focus on transformer models like GPT and Llama. Testing the methodology on CNNs and other architectures would be an interesting extension, as the tensor value distributions may differ.

Overall, the scaling methodology and experiments help address the practical challenge of leveraging FP8 in large models. The results demonstrate accuracy can be maintained during inference and training, complementing existing literature on FP8 formats and mixed precision approaches.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Exploring the application of FP8 quantization to other layers in transformer architectures besides the linear layers, such as the dot-product attention layers. The authors only focused on quantizing the linear layers in this work.

- Applying FP8 quantization to models beyond transformers, such as graph neural networks and convolutional neural networks for computer vision. The work here focused specifically on large language models like GPT and Llama.

- Evaluating whether FP8 can be used more broadly during training instead of only for matrix multiplications. Currently FP8 is used in a mixed precision approach where values are accumulated in higher precision.

- Investigating techniques to further mitigate the impact of emergent outliers in very large models, building on recent work that has aimed to encourage models to produce fewer outliers.

- Comparing FP8 to INT8 quantization in more model architectures, to better understand the tradeoffs between the two formats in different contexts beyond transformers.

- Exploring optimized implementations of the scaling methodology presented here, to maximize throughput and efficiency especially on hardware with limited SRAM.

- Standardizing the FP8 format itself through contributions to the ongoing IEEE working group, including determining default scaling biases.

So in summary, the authors point to several areas of further study around model architecture support, training techniques, hardware efficiency, format standardization, and mitigating outliers at very large scale. The overall goal is to advance FP8 as an efficient 8-bit format for both inference and training across a wide range of model types.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a methodology to select per-tensor scaling factors when quantizing the linear layers of large transformer models to 8-bit floating point (FP8) formats. FP8 provides computational efficiency gains for training and inference but its reduced dynamic range compared to higher precision formats like FP16 can lead to accuracy degradation if values underflow or overflow the FP8 range. The authors propose computing scaling biases dynamically via the absolute maximum of tensor values, which are applied before casting weights, activations and gradients to FP8. They showcase their methodology by quantizing and fine-tuning GPT models from 111M to 13B parameters and Llama models up to 70B parameters using FP8. The scaling biases are shown to adapt appropriately during training to match reference accuracies obtained with FP16. Plots of the per-tensor scaling bias evolution provide insights into the behavior of different layers. Overall, the work provides a practical methodology and analysis to leverage FP8 quantization for large language models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents a methodology to select appropriate scaling factors (called scaling biases) when quantizing the linear layers of large transformer models to 8-bit floating point (FP8) formats. Using too few bits can reduce the range of representable values, causing underflow or overflow which degrades model accuracy. The authors propose computing a per-tensor scaling bias by shifting the exponent range based on the maximum absolute value of that tensor. This dynamically matches the value range to the FP8 format's reduced dynamic range. 

The authors showcase their methodology by quantizing several sizes of GPT and Llama models to FP8, from 111 million to 70 billion parameters. They are able to match baseline FP16 accuracy during both inference and fine-tuning by adapting the scaling biases. Plots of how the biases evolve over training provide insights into the FP8 quantization dynamics. The work provides a methodology to leverage FP8's efficiency gains while preventing accuracy degradation for large language models.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents a methodology to select appropriate per-tensor scaling biases when quantizing the linear layers of large transformer models to 8-bit floating point (FP8) for efficient training and inference. The key idea is to dynamically compute scaling biases before casting weight, activation, and gradient tensors from higher precision like FP16 down to the more limited dynamic range of FP8. These per-tensor scaling biases shift the value distributions to prevent underflow or overflow when casting to FP8. For training, the scaling biases are recomputed periodically to account for distribution shifts during optimization. For inference, they are computed once via post-training quantization. The authors test this methodology by quantizing decoder-only transformer architectures like GPT and Llama to FP8, demonstrating full convergence during fine-tuning and no accuracy degradation during inference for model sizes ranging from 111M to 70B parameters. The paper provides useful practical details like pseudocode and scaling bias evolution plots that are missing from previous FP8 work, facilitating adoption by practitioners.
