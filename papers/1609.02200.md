# [Discrete Variational Autoencoders](https://arxiv.org/abs/1609.02200)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can discrete latent variables be effectively incorporated into the variational autoencoder framework for probabilistic modeling?

The key points are:

- Discrete latent variables (such as those in a restricted Boltzmann machine) are useful for capturing discrete classes or modes in the data. 

- However, it is difficult to train models with discrete latent variables using the variational autoencoder framework, since backpropagation through discrete variables is generally not possible.

- The authors propose a method to train variational autoencoders with discrete latent variables by:

1) Introducing continuous latent variables that are transformations of the discrete variables. This allows backpropagation through the continuous variables.

2) Using a hierarchical approximating posterior over the discrete variables to capture multimodality. 

3) Adding layers of continuous latent variables below the discrete variables to model continuous deformations.

4) Deriving low-variance gradient estimates for training.

So in summary, the central hypothesis is that by introducing continuous variables and hierarchical approximations, along with proper gradient estimates, discrete latent variable models can be effectively trained within the variational autoencoder framework.


## What is the main contribution of this paper?

 The main contribution of this paper is a method to train probabilistic models with discrete latent variables using the variational autoencoder (VAE) framework, including backpropagation through the discrete latent variables. 

Specifically, the key ideas are:

- They introduce continuous latent variables that are deterministically coupled with the discrete latent variables. This allows backpropagation through the continuous variables while retaining a discrete latent representation.

- They use a hierarchical approximating posterior over the discrete latent variables to capture strong correlations, avoiding a mean-field approximation.

- They add layers of continuous latent variables below the discrete variables to build powerful nonlinear hierarchical priors and posteriors. 

- They apply this to a model with a restricted Boltzmann machine (RBM) over the discrete latent variables, allowing the RBM to capture multimodal discrete structure.

The resulting "discrete variational autoencoder" combines the benefits of VAEs and RBMs to achieve state-of-the-art performance on permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets. Overall, the paper contributes a method to effectively optimize probabilistic models with both discrete and continuous latent variables using backpropagation and the VAE framework.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a novel class of probabilistic models called discrete variational autoencoders that combine discrete and continuous latent variables and can be trained efficiently using variational inference, achieving state-of-the-art performance on image datasets like MNIST, Omniglot, and Caltech-101 Silhouettes.
