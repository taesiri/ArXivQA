# [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the table, the key points seem to be:- The paper is evaluating different summarization models on metrics like BERTScore, ROUGE, and correlation with length.- The models being compared include a 175B RL model, a 175B model trained only on the first subtree, and a smaller 6B model. There are also some extractive baselines.- The central research question seems to be: How well can these recursive summarization models summarize entire books, as measured by automatic metrics compared to baselines and human performance?- The 175B RL model achieves the best BERTScore, while the 175B first subtree model achieves the best ROUGE-2 and competitive BERTScore. - All the recursive summarization models outperform the non-oracle baselines on BERTScore, suggesting they are producing more abstractive and accurate summaries.- The 175B models approach the performance of an extractive oracle on ROUGE, while significantly exceeding it on BERTScore.- The recursive models do not yet match human performance on these metrics, but are making significant progress towards that goal based on the metrics.In summary, the central hypothesis is that recursive summarization models can produce high-quality abstractive summaries of entire books, which the results seem to support based on automatic metrics. The key research questions are how the models compare to baselines and human performance.


## What is the main contribution of this paper?

The main contribution of this paper seems to be presenting progress on using human feedback and recursive task decomposition for abstractive summarization of long documents. Specifically:- They apply task decomposition and human feedback to the challenging problem of abstractive book summarization. They decompose the long book summarization task into summarizing smaller chunks, and recursively summarize the chunk summaries.- They collect a large volume of human demonstrations and comparisons to train a model (fine-tuning GPT-3) to summarize at each level of the decomposition through behavioral cloning and reinforcement learning. - Their resulting model can generate complete book summaries that contain important events and some abstraction, approaching human quality on a small fraction of books.- The model summaries enable a QA model to achieve competitive performance on the NarrativeQA benchmark for answering questions about books.- They achieve state-of-the-art results on the BookSum dataset for long document summarization.- Their analysis shows that reinforcement learning on human comparisons leads to better summarization ability compared to just behavioral cloning, once the policy quality reaches a certain level.In summary, the key contribution is showing that recursive decomposition and human feedback can scale to training models on very long and difficult summarization tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper: The paper investigates recursive task decomposition for abstractive summarization of long documents, using human feedback to train models on subtasks in order to generate full document summaries.


## How does this paper compare to other research in the same field?

Based on my review, here is how I see this paper comparing to other research in the field:- The paper introduces a novel approach for abstractive summarization of long documents like books by combining recursive task decomposition with reinforcement learning from human preferences. This is a fairly unique contribution compared to most prior work on summarization which focuses on short texts or extractive methods. The idea of task decomposition has been explored conceptually in some prior theoretical AI safety papers, but this work provides one of the first concrete applications to a difficult real-world task.- The recursive summarization approach achieves new state-of-the-art results on the BookSum benchmark for long document summarization. It outperforms previous neural abstractive methods by 3-4 ROUGE points and also beats an extractive oracle on the BERTScore metric. This demonstrates the value of the proposed techniques compared to other summarization methods.- For the reinforcement learning component, the gains over the BC baseline echo similar findings from past work showing the benefits of RL for text summarization. However, the analysis of RL's improved sample efficiency over BC with high-quality demonstrations is quite novel. This helps make the case for RL's practicality in real-world settings.- The qualitative analysis of limitations reveals open challenges shared across most long document summarization work, like difficulty tracking entities and coreference. The conjecture that fiction may be inherently harder to summarize than non-fiction seems plausible but preliminary.- The NarrativeQA evaluation demonstrates the broader value of the summaries for question answering, but is hard to directly compare due to using a much larger QA model. The zero-shot recursive QA results are intriguing but also hard to contextualize without comparison to other methods.Overall, I find this paper to make solid incremental progress over comparable past work, while also introducing some new techniques and analysis to the field. The limitations show open research questions remain, but the approach appears promising. Let me know if you would like me to expand on any part of this comparison.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigating better and more principled curricula for training. They suggest ideas like generating the summary trees on-the-fly during training, and training the reward model online. They also suggest exploring the impact of different episode lengths during RL training.- Improving the techniques for fine-tuning models using human feedback. The authors suggest exploring more efficient ways to collect comparison data instead of binary comparisons. They also suggest trying other human feedback optimization methods like expert iteration.- Learning a task decomposition model rather than using a fixed decomposition. The authors suggest this could be feasible for hard real-world tasks, but it remains an open question.- Further exploring what kinds of tasks are fundamentally limited by task decomposition, since it requires tasks to be separable. - Investigating other ways ML models could assist humans in specifying preferences and giving feedback for high-level tasks beyond summarization.- Training an end-to-end model that maps from a full book to a summary, using distillation over the recursive model.- Addressing the limitations of their current book summarization model, such as improving coherence and abstraction in the summaries.So in summary, the main future directions are improving the training techniques like curriculum and human feedback methods, learning task decomposition, applying the approach to new tasks, addressing model limitations, and investigating the limits of task decomposability.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper examines the relationship between summary length, BERTScore, ROUGE metrics, and human ratings on book summaries generated by different models. They train recursive summarization models with behavioral cloning and reinforcement learning on summaries of increasing length, from paragraphs to chapter summaries to full book summaries. A key finding is that their 175B parameter RL model achieves the highest BERTScore of 0.182 on the BookSum dataset, outperforming extractive methods and large pretrained models. However, BERTScore has a slight negative correlation with length, so they run an adjustment to control for this. Even after adjustment, their model still achieves the top BERTScore. Their model also beats baselines on ROUGE metrics, approaching the performance of an extractive oracle. When evaluated by humans, their model summaries occasionally match human quality, with over 5% rated 6/7, although on average they remain worse than human summaries. They also show their summaries contain enough salient information to do competitively on the NarrativeQA question answering benchmark.
