# [Masked Video Distillation: Rethinking Masked Feature Modeling for   Self-supervised Video Representation Learning](https://arxiv.org/abs/2212.04500)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that using pretrained image and video models as teachers to provide high-level features for continued masked feature prediction can learn better video representations compared to reconstructing low-level features like pixels. 

Specifically, the key questions/hypotheses explored are:

- Can using features from pretrained image models (image teachers) and video models (video teachers) as targets for masked feature prediction improve video representation learning compared to reconstructing pixels?

- Do students taught by image teachers learn different representations compared to students taught by video teachers? I.e. do image teachers transfer stronger spatial representations while video teachers transfer stronger temporal representations?

- Can combining image and video teachers in a co-teaching framework improve performance on different types of downstream video tasks by leveraging the complementary strengths of the different teachers?

So in summary, the main hypothesis is that masked video modeling using high-level feature targets from pretrained image and video models can improve video representation learning, and combining image and video teachers captures both strong spatial and temporal representations. The experiments aim to demonstrate the effectiveness of this masked video distillation approach.
