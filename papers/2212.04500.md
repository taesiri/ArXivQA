# [Masked Video Distillation: Rethinking Masked Feature Modeling for   Self-supervised Video Representation Learning](https://arxiv.org/abs/2212.04500)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that using pretrained image and video models as teachers to provide high-level features for continued masked feature prediction can learn better video representations compared to reconstructing low-level features like pixels. 

Specifically, the key questions/hypotheses explored are:

- Can using features from pretrained image models (image teachers) and video models (video teachers) as targets for masked feature prediction improve video representation learning compared to reconstructing pixels?

- Do students taught by image teachers learn different representations compared to students taught by video teachers? I.e. do image teachers transfer stronger spatial representations while video teachers transfer stronger temporal representations?

- Can combining image and video teachers in a co-teaching framework improve performance on different types of downstream video tasks by leveraging the complementary strengths of the different teachers?

So in summary, the main hypothesis is that masked video modeling using high-level feature targets from pretrained image and video models can improve video representation learning, and combining image and video teachers captures both strong spatial and temporal representations. The experiments aim to demonstrate the effectiveness of this masked video distillation approach.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing masked video distillation (MVD), a simple yet effective two-stage masked feature modeling framework for video representation learning. In MVD, image or video models pretrained with masked modeling are used as teachers to provide high-level feature targets for continued masked feature prediction.

2. Observing that students taught by video teachers perform better on temporally-heavy video tasks, while image teachers transfer stronger spatial representations for spatially-heavy video tasks.

3. Designing a spatial-temporal co-teaching strategy for MVD to leverage the advantages of both image and video teachers. Specifically, the student model reconstructs features from both teachers using two separate decoders.

4. Demonstrating strong performance of MVD on multiple standard video recognition benchmarks, surpassing state-of-the-art methods by clear margins. For example, MVD achieves 86.4% and 76.7% top-1 accuracy on Kinetics-400 and Something-Something-v2 with ViT-Large, outperforming VideoMAE by 1.2% and 2.4% respectively.

In summary, the key contribution is proposing the masked video distillation framework and spatial-temporal co-teaching strategy, which lead to state-of-the-art video representation learning. The observation about different teachers is also an important finding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a two-stage masked video modeling approach called Masked Video Distillation (MVD) where models pretrained with masked image/video modeling are used as teachers to provide high-level feature targets, and finds that image teachers transfer better spatial representations while video teachers provide stronger temporal modeling for the student models.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other related research in self-supervised video representation learning:

- The main contribution of this paper is proposing masked video distillation (MVD), which uses features from pretrained masked image modeling (MIM) and masked video modeling (MVM) models as targets for continued masked feature prediction. This is a novel approach compared to most prior work like VideoMAE and ST-MAE that reconstructs low-level features like raw pixels. 

- The authors find that using image teachers provides benefits for spatial transfer while video teachers help more for temporal modeling. To get the advantages of both, they propose a spatial-temporal co-teaching strategy in MVD. This is a new idea not explored in other masked video modeling papers.

- This work shows strong performance of MVD, surpassing VideoMAE significantly on multiple datasets. The improvements are more notable for smaller models, but gains are still observed even on large models like ViT-L. Most prior work has focused on pixel reconstruction rather than masked feature distillation.

- For computational efficiency, the authors show MVD can achieve better accuracy-efficiency trade-off compared to longer VideoMAE pretraining. This analysis of efficiency is lacking in many existing video SSL papers.

- The visualizations and analysis of how features from different teachers impact student models provide useful insights. Such in-depth analysis and understanding of what the models learn is not present in many existing works.

In summary, the masked video distillation idea, co-teaching strategy, superior performance over strong baselines, and detailed experimental analysis help advance research in self-supervised video representation learning. The distillation approach and findings are novel compared to most prior arts in this field.
