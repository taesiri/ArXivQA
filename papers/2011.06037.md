# [Unsupervised Video Representation Learning by Bidirectional Feature   Prediction](https://arxiv.org/abs/2011.06037)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the key research question this paper seeks to address is:How can we improve self-supervised video representation learning by incorporating both past and future frame predictions, compared to only using future frame prediction as in prior work? The central hypothesis is that utilizing both past and future frame predictions as complementary supervisory signals will encourage the model to better explore the temporal structure of videos and learn richer video representations. Specifically, the authors argue that the supervisory signal from unobserved past frames provides a complementary signal to the commonly used future frame prediction.The key ideas and contributions are:- Proposing a novel pretext task of bidirectional (past and future) feature prediction for self-supervised video representation learning. - Showing how to effectively incorporate past and future frame predictions jointly in a contrastive learning framework. This allows using swapped past/future order as temporal hard negatives.- Demonstrating through experiments that the proposed bidirectional prediction method outperforms unidirectional future prediction baselines on action recognition benchmarks.In summary, the paper introduces a new approach for self-supervised video representation learning that exploits past and future frame predictions bidirectionally rather than just future prediction unidirectionally. The central hypothesis is that this will enable learning more powerful video representations by better capturing temporal structure.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel method for self-supervised video representation learning through bidirectional (past and future) feature prediction. 2. It shows how to jointly incorporate past and future prediction signals in a contrastive learning framework, using the wrong order of future/past frames as "temporal hard negatives".3. It provides extensive empirical evaluation showing the proposed method outperforms unidirectional (future-only) feature prediction on downstream action recognition tasks. Specifically, the key ideas are:- Using both past and future frames provides complementary supervision signals to learn video representations.- Simply combining losses from independent future and past prediction is not very effective.- Jointly predicting future and past allows creating hard negatives by swapping their order, forcing the model to encode temporal structure.- This bidirectional prediction method gives improved transfer learning performance on action recognition compared to predicting future only.So in summary, the main contribution is proposing and demonstrating the benefits of a novel self-supervised pretext task for videos based on joint future and past feature prediction with temporal hard negatives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a new self-supervised method for video representation learning that trains a model to jointly predict unseen past and future features from a video clip, using the incorrect order of future/past frames as hard negatives to encourage learning of temporal structure.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of self-supervised video representation learning:- The main novel contribution is incorporating both past and future feature prediction in a bidirectional manner. Most prior works have focused only on future prediction as the pretext task. - The proposed method trains the model using an InfoNCE-based contrastive loss function. This follows the trend in recent self-supervised representation learning literature of using contrastive learning frameworks.- For the base network architecture, the paper uses a 2D3D ResNet18 backbone which is a fairly standard choice. This allows for direct comparison to other methods using similar backbone networks.- The evaluations follow common practices - finetuning on downstream action recognition datasets like UCF101 and HMDB51. Layer-wise evaluation on a large dataset like Kinetics-400 provides more insight into the learned representations.- Compared to state-of-the-art self-supervised methods, the proposed approach achieves strong results, especially on HMDB51 which is known to be challenging. The gains over unidirectional future prediction demonstrate the benefits of bidirectional prediction.- Some recent concurrent works have incorporated additional cues like optical flow or trained over larger datasets. But under similar training settings, this paper shows competitive performance to leading approaches.- Overall, the paper makes a nice incremental contribution on top of prior work by introducing the bidirectional prediction concept and providing thorough experiments/analysis. The results validate the proposed ideas empirically.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring different ways to incorporate past and future signals beyond the joint prediction approach proposed in this paper. The authors mention that utilizing both past and future is non-trivial and requires careful design choices. There may be other effective ways to leverage both signals that are worth exploring.- Evaluating the proposed approach on larger-scale datasets. The experiments in the paper are mostly on smaller datasets like UCF101. Scaling up the experiments to larger video datasets like Kinetics could provide more insights.- Combining the proposed bidirectional prediction approach with other pretext tasks like temporal order verification, speed prediction etc. The authors suggest the temporal structure of videos enables many possible pretext tasks that could potentially be combined.- Incorporating additional modalities like audio along with RGB frames. The paper focuses on RGB-only video representation learning, but audio and other signals could provide complementary self-supervision.- Exploring different network architectures and self-supervised losses for video representation learning. The InfoNCE loss and 2D3D-ResNet architecture are effective but other options could be examined.- Applying the learned self-supervised representations to various downstream tasks beyond action recognition. General video understanding requires strong representations applicable to many tasks.- Comparing to fully supervised representation learning on large labeled video datasets. Closing the gap with supervised learning is an important end goal.So in summary, the authors point to many promising research avenues related to loss formulations, architectures, modalities, and tasks that can build on their bidirectional prediction approach. Advancing self-supervised video representation learning remains an open problem with much room for innovation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper introduces a novel method for self-supervised video representation learning via feature prediction. In contrast to previous methods focusing on future feature prediction, the authors argue that using features from unobserved past frames as supervision provides a complementary signal to future frames for exploring the temporal structure of videos. Their approach trains a model to distinguish between future and past features using an InfoNCE loss, where the joint encoding of future and past allows for comprehensive temporal hard negatives by swapping. Empirical results on action recognition show their bidirectional approach outperforms independent future or past prediction, as it enriches representations by encouraging similarity of present with future/past and distinguishability between future and past.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces a novel method for self-supervised video representation learning via feature prediction. In contrast to previous methods focusing on future feature prediction, this work argues that a supervisory signal from unobserved past frames is complementary to one originating from future frames. The rationale is to encourage the network to explore temporal structure by distinguishing between future and past observations. The model is trained in a contrastive learning framework, where joint encoding of future and past provides temporal hard negatives via swapping. Empirically, utilizing both future and past signals enriches learned representations for action recognition, outperforming independent prediction of just future or past. The method involves predicting unseen past and future video features using a variant of noise contrastive estimation called InfoNCE. Video clips are divided into past, present, and future partitions. Positive and negative pairs are constructed from joint past/future representations and trained with InfoNCE loss. Rather than naively combining losses from independent future and past prediction, the model takes past/future pairs as input to allow temporal hard negatives of wrong future/past order. Distinguishing correct orders requires encoding temporal structure to detect matching sequences. Experiments show the proposed bidirectional prediction outperforms unidirectional prediction on action recognition benchmarks.
