# [VertexSerum: Poisoning Graph Neural Networks for Link Inference](https://arxiv.org/abs/2308.01469)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can an attacker amplify the leakage of private link information from graph neural networks through data poisoning? 

The key hypothesis is that an adversary can significantly improve the effectiveness of stealing private link information from graph neural networks by carefully poisoning a small portion of the training graph data.

Specifically, the paper proposes a novel data poisoning attack called VertexSerum that modifies node features to amplify the connectivity leakage from graph neural networks. The attack focuses on increasing the leakage of link information between nodes belonging to the same class. 

The paper hypothesizes that by optimizing the poisoning using projected gradient descent to promote similarity of model outputs on linked nodes and dissimilarity on unlinked nodes, the attack can greatly improve link inference, especially for node pairs from the same class.

In summary, the central research question is how data poisoning can be used to increase link information leakage from graph neural networks, with the key hypothesis that an optimized poisoning attack like VertexSerum can significantly improve the success of link inference attacks. The novelty lies in using data poisoning specifically to target link privacy in graph neural networks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes VertexSerum, a novel graph poisoning attack to amplify the leakage of private link information in graph neural networks (GNNs). The attack injects adversarial noise into a small portion of the training graph to make the GNN model produce more similar outputs for linked nodes.

2. It introduces a new evaluation metric, intra-class AUC, that focuses on evaluating link inference attacks on node pairs from the same class. This overcomes biases in prior work that do not differentiate between inter-class and intra-class node pairs.

3. It constructs a self-attention based network to train the link detector, which captures dependencies between features better than MLPs used in prior work. It also proposes a pre-training strategy to overcome overfitting on the limited training data.

4. It demonstrates the attack effectiveness on various real-world datasets and GNN structures, improving AUC scores by 9.8% on average compared to prior art. It also shows the attack works in black-box and online learning settings.

5. It analyzes the stealthiness of the attack by evaluating the homophily noticeability and victim model accuracy after poisoning. Results show the attack increases model privacy leakage without significantly affecting GNN performance.

In summary, the key innovation is a poisoning attack that specifically targets and amplifies link information leakage in GNNs, evaluated comprehensively on multiple datasets and settings. The work reveals vulnerabilities of GNNs to such attacks and the need for more robust defenses.
