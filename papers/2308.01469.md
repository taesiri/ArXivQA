# [VertexSerum: Poisoning Graph Neural Networks for Link Inference](https://arxiv.org/abs/2308.01469)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can an attacker amplify the leakage of private link information from graph neural networks through data poisoning? 

The key hypothesis is that an adversary can significantly improve the effectiveness of stealing private link information from graph neural networks by carefully poisoning a small portion of the training graph data.

Specifically, the paper proposes a novel data poisoning attack called VertexSerum that modifies node features to amplify the connectivity leakage from graph neural networks. The attack focuses on increasing the leakage of link information between nodes belonging to the same class. 

The paper hypothesizes that by optimizing the poisoning using projected gradient descent to promote similarity of model outputs on linked nodes and dissimilarity on unlinked nodes, the attack can greatly improve link inference, especially for node pairs from the same class.

In summary, the central research question is how data poisoning can be used to increase link information leakage from graph neural networks, with the key hypothesis that an optimized poisoning attack like VertexSerum can significantly improve the success of link inference attacks. The novelty lies in using data poisoning specifically to target link privacy in graph neural networks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes VertexSerum, a novel graph poisoning attack to amplify the leakage of private link information in graph neural networks (GNNs). The attack injects adversarial noise into a small portion of the training graph to make the GNN model produce more similar outputs for linked nodes.

2. It introduces a new evaluation metric, intra-class AUC, that focuses on evaluating link inference attacks on node pairs from the same class. This overcomes biases in prior work that do not differentiate between inter-class and intra-class node pairs.

3. It constructs a self-attention based network to train the link detector, which captures dependencies between features better than MLPs used in prior work. It also proposes a pre-training strategy to overcome overfitting on the limited training data.

4. It demonstrates the attack effectiveness on various real-world datasets and GNN structures, improving AUC scores by 9.8% on average compared to prior art. It also shows the attack works in black-box and online learning settings.

5. It analyzes the stealthiness of the attack by evaluating the homophily noticeability and victim model accuracy after poisoning. Results show the attack increases model privacy leakage without significantly affecting GNN performance.

In summary, the key innovation is a poisoning attack that specifically targets and amplifies link information leakage in GNNs, evaluated comprehensively on multiple datasets and settings. The work reveals vulnerabilities of GNNs to such attacks and the need for more robust defenses.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes VertexSerum, a novel graph poisoning attack to significantly amplify the leakage of private link information in graph neural networks by modifying node features, along with a self-attention-based link detector that infers adjacent nodes more accurately; experiments demonstrate VertexSerum's superiority over state-of-the-art link inference attacks across different datasets and GNN structures.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other related research on graph neural networks and privacy attacks:

- It proposes a new type of privacy attack on GNNs - a data poisoning attack called VertexSerum that amplifies link information leakage from GNN models. Prior work has studied membership inference and model stealing attacks, but this is the first work to study a poisoning attack for link privacy leakage on GNNs.

- It identifies limitations in prior link inference attacks, showing they perform poorly on intra-class node pairs. The paper introduces a new metric, intra-class AUC, to address this issue. 

- It utilizes a self-attention based link detector which outperforms prior work using MLP models. The self-attention mechanism better captures dependencies in the similarity features.

- Experiments demonstrate VertexSerum significantly outperforms state-of-the-art link inference attacks, improving AUC by 9.8% on average across datasets. 

- The attack is evaluated in practical settings like online training and black-box scenarios where the model architecture is unknown. This demonstrates broader applicability compared to prior attacks evaluated only in offline or grey-box settings.

- The paper provides useful insights into vulnerabilities of GNNs to such privacy attacks, and suggests potential defenses like using differential privacy or avoiding oversmoothing.

In summary, this paper makes novel contributions in both attack methodology and evaluation compared to prior work. The findings pose new privacy challenges for GNNs that call for more research into building robust models.
