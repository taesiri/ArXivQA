# [VL-Adapter: Parameter-Efficient Transfer Learning for   Vision-and-Language Tasks](https://arxiv.org/abs/2112.06825)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently fine-tune large pre-trained language models like VL-BART and VL-T5 for downstream vision-and-language (V&L) tasks using adapter-based techniques. Specifically, the paper benchmarks different adapter methods like Adapter, Hyperformer, and Compacter against full fine-tuning of the entire model as well as against prompt tuning on a diverse set of V&L tasks. It aims to match the performance of full fine-tuning while only updating a small fraction of the total parameters.The key hypothesis is that adapter-based techniques can allow for parameter-efficient transfer learning to new V&L tasks without sacrificing much performance compared to full fine-tuning of large pre-trained models. The paper thoroughly tests this hypothesis across image-text datasets like VQA, GQA, NLVR2, COCO as well as video-text datasets like TVQA, How2QA, TVC, and YC2C.In summary, the central research question is how to efficiently adapt pre-trained V&L models to new tasks using adapter methods, and the key hypothesis is that adapter-based training can match full fine-tuning performance while only updating a small fraction of parameters. The paper does extensive experiments and analysis to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The first work benchmarking different types of parameter-efficient training techniques (Adapter, Hyperformer and Compacter) for diverse challenging downstream image-text and video-text tasks. 2. Empirical demonstration of adapters reaching the performance of full fine-tuning while updating only 3.39-4.18% of the parameters.3. Comprehensive analysis on the design of freezing CLIP, impact of different architectural components, weight-sharing techniques, task-specific prompts, and vision-language pretraining.In summary, the paper proposes using adapter-based methods to efficiently fine-tune generative vision-and-language (V&L) models on downstream tasks. Through experiments on image-text and video-text datasets, the authors show that a simple adapter module with weight sharing (updating only 3-4% of parameters) can match the performance of full model fine-tuning. The paper also provides ablation studies and analysis to understand the contribution of different components.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using adapter modules, which are small networks inserted into a pre-trained language model, to efficiently fine-tune the model for vision-and-language tasks by updating only a small fraction of parameters; experiments on diverse image and video understanding datasets show that with careful training, a shared-weight adapter module with only 3-4% of trainable parameters can match the performance of full fine-tuning the entire model.
