# [Robust Multi-Task Learning with Excess Risks](https://arxiv.org/abs/2402.02009)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Robust Multi-Task Learning with Excess Risks":

Problem:
- Multi-task learning (MTL) aims to train a single model to perform well on multiple related tasks. A key challenge is balancing between different tasks.
- Existing adaptive weighting methods assign weights based on task losses. However, they are vulnerable to label noise, which can result in noisy tasks getting high weights and clean tasks getting insufficient training.

Proposed Solution: 
- The paper proposes a new MTL algorithm called ExcessMTL that assigns weights based on excess risks instead of raw losses.
- Excess risk measures the gap between the risk of current model and optimal model. It filters out the irreducible Bayes error that depends on label noise level.
- Excess risks are estimated efficiently via Taylor expansion. Tasks with higher excess risks get higher weights.

Key Contributions:
- Proposes a novel and robust task weighting scheme using excess risks that is resilient to label noise. Clean tasks maintain performance even with extreme noise in other tasks.
- Provides theoretical analysis on convergence rate and Pareto stationarity.
- Empirically demonstrates superior performance over baselines on benchmark datasets with label noise injection. Maintains strong performance on clean datasets as well.
- Conceptually compares with other methods and explains why excess risk is more robust than raw loss for determining task difficulty.

In summary, the paper presents a principled and effective method for multi-task learning that is robust to label noise through a task weighting scheme based on excess risks. Both theoretical and empirical results demonstrate its reliability and effectiveness.
