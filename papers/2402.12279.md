# [Key ingredients for effective zero-shot cross-lingual knowledge transfer   in generative tasks](https://arxiv.org/abs/2402.12279)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Zero-shot cross-lingual generation implies finetuning a multilingual pretrained language model (mPLM) on a text generation task in one language (source) and applying it to generate text in other unseen languages (target). 
- Prior work faced issues like generating text in the wrong language or producing irrelevant/incoherent outputs. Several potential solutions were proposed but their efficacy remained unclear.

Methods:
- Compared 6 adaptation methods for mPLMs: full finetuning, prompt tuning, adapters, freezing decoder & embeddings, mixing target language data, using multiple source languages.
- Studied impact of tuning learning rate (LR) and intermediate tuning on mT5, mBART and NLLB-200 models.
- Evaluated on summarization (XL-Sum dataset) and question answering (XQuAD dataset) tasks.

Key Findings:
- Carefully tuning LR alleviates generating in wrong language for full finetuning, bringing bigger gains than other methods.  
- Intermediate tuning substantially improves performance in most cases.
- With proper LR and intermediate tuning, full finetuning acts as a very strong baseline. Other methods bring modest gains. Freezing decoder & embeddings works best for mBART. Using multiple source languages works best for mT5.
- mT5 and mBART perform similarly. NLLB-200 is competitive in summarization but lags in QA.
- Best zero-shot models match or even outperform translate-train approach.

Main Contributions:
- Showed the critical impact of hyperparameters (LR, intermediate tuning) for cross-lingual transfer in generative tasks. 
- Demonstrated full finetuning as a strong baseline with careful tuning. Assessed efficacy of other proposed adaptation methods.
- Analyzed suitability of different mPLMs and compared performance against data translation.
- Showed that zero-shot cross-lingual generation can match performance of data translation baseline.
