# [Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur](https://arxiv.org/abs/2304.12652)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How can we synthesize high-fidelity and view-consistent novel view images of large-scale real-world scenes from images captured in the wild that contain inevitable artifacts like motion blur? 

The key points are:

- Synthesizing novel views of large-scale real-world scenes is challenging due to limitations of neural 3D representations and unavoidable artifacts in the image data.

- The paper proposes a hybrid neural rendering approach that combines the strengths of image-based rendering (for high fidelity) and neural 3D representations (for consistency).

- The paper also proposes efficient blur simulation and detection strategies to provide blur-free supervisory signals to optimize the hybrid rendering model when trained on real-world blurry data.

- Experiments on real and synthetic indoor scene datasets demonstrate the proposed method outperforms prior neural rendering techniques, especially point-based methods, for novel view synthesis.

In summary, the paper addresses the problem of rendering high-quality, consistent novel views of real-world scenes by using a hybrid neural rendering approach and handling inevitable image artifacts like blur through simulation and detection techniques.
