# ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced   MiniGPT-4

## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Proposing a new multimodal model called ArtGPT-4 for artistic vision-language understanding. The key innovation is incorporating tailored linear layers and activation functions into the Vicuna language model to enhance its ability to understand images.2. Demonstrating a training methodology to optimize ArtGPT-4's performance in just 2 hours on a single GPU, using only around 200GB of image-text data. This makes the model highly efficient to train compared to other large multimodal models.3. Introducing novel benchmarks for evaluating vision-language models across several criteria: image depiction, sentiment analysis, content recognition, and multi-round dialog. This provides a more comprehensive way to assess these models. 4. Evaluating ArtGPT-4 using the proposed benchmarks and showing it outperforms the original MiniGPT-4 model and reaches near human-level performance. The model scored over 1 point higher than state-of-the-art on a 6 point scale.5. Releasing the code and pre-trained ArtGPT-4 model publicly to facilitate research and applications in multimodal AI.In summary, the key novelty is in model architecture, training efficiency, evaluation methodology and benchmarks, and strong empirical results demonstrating improvements over prior arts in artistic vision-language tasks. The end-to-end contribution is a compelling multimodal model with state-of-the-art performance.
