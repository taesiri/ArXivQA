# [Evolving Knowledge Distillation with Large Language Models and Active   Learning](https://arxiv.org/abs/2403.06414)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) achieve great performance on NLP tasks but have high computational cost during inference. 
- Knowledge distillation (KD) can transfer knowledge from large models into smaller ones, but most work uses LLMs just for text generation, not fully utilizing their capabilities.  
- Prior KD studies also conduct the process in a static, offline manner without considering changes in student model's status and weaknesses over time.

Proposed Solution:
- The paper proposes EvoKD - Evolving Knowledge Distillation with LLMs and Active Learning.  
- It interactively enhances LLM-based data generation by analyzing weaknesses of the student model and tailoring samples accordingly.  
- The batch distillation process is repetitive, with the LLM prompted to create both easy and challenging sentences based on observed weakness patterns.
- The student model's past performance serves as input to the LLM to identify more beneficial knowledge over iterations.

Main Contributions:
- Introduces concept of Evolving Knowledge Distillation using dynamic teaching strategies adapted to the student model.
- Proposes the EvoKD framework that leverages LLM's potential for comprehending tasks and acquiring valuable knowledge.  
- Experiments show EvoKD outperforms baselines on text classification and NER tasks, achieving 90% of full-shot performance with just 1-shot on some text classification datasets.

In summary, the key innovation is the evolving and interactive nature of knowledge distillation, where the LLM takes an active teaching role based on dynamic weakness analysis of the student model. This allows more informative knowledge transfer, outperforming static distillation approaches.
