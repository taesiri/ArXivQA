# [(Ir)rationality and Cognitive Biases in Large Language Models](https://arxiv.org/abs/2402.09193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper seeks to evaluate whether large language models (LLMs) display rational reasoning or if they exhibit biases and heuristics similar to humans. Specifically, the goal is to assess the rationality of LLMs using tasks from cognitive psychology that were designed to highlight irrational reasoning in humans. The paper also aims to provide a methodology to assess and compare the capabilities of different LLMs with respect to rationality.

Methods:
The authors evaluated 7 LLMs - GPT-3.5, GPT-4, Google's Bard, Anthropic's Claude 2, and 3 versions of Meta's Llama 2. They prompted the models with 12 tasks from cognitive psychology literature that were originally designed to reveal biases in human reasoning. The tasks include the Wason selection task, AIDS test problem, hospital birth problem, Monty Hall problem and others. Facilitated versions were also included. Each task was prompted 10 times and responses categorized as correct/incorrect and human-like/not human-like.

Results: 
The models displayed high inconsistency in their responses to the same tasks. While the tasks reveal irrational reasoning in humans, the LLMs exhibited a different kind of irrationality - their incorrect responses generally did not display stereotypical human biases and they were inconsistent across multiple runs. GPT-4 had the best performance overall. Llama 2 models refused to answer many questions. Performance was lower on tasks requiring mathematical calculations compared to logical reasoning questions.  

Conclusions:
The paper demonstrates that LLMs do not fail at these reasoning tasks in the same way humans do. Their errors are different from standard human biases. The high inconsistency in responses also reveals limitations in critical reasoning abilities. The paper contributes methodologically by showing how tasks from cognitive psychology can be used to assess rational capabilities of LLMs and compare model performance. The approach could facilitate developing benchmarks to evaluate model rationality.


## Summarize the paper in one sentence.

 The paper evaluates the rational reasoning abilities of seven large language models using cognitive psychology tasks, finding that while the models display some irrationality, their incorrect responses generally do not mimic human cognitive biases but instead show inconsistencies and logical errors.


## What is the main contribution of this paper?

 The main contribution of this paper is to provide a methodology to assess and compare the rational reasoning abilities of different large language models. The paper evaluates seven language models (GPT-3.5, GPT-4, Bard, Claude 2, and three versions of Llama 2) using cognitive psychology tasks originally designed to test human reasoning. 

The key findings are:

- LLMs display a different type of irrationality compared to humans. Their responses are often inconsistent, giving correct and incorrect answers to the same prompt. When incorrect, the errors tend to not reflect human cognitive biases but instead show illogical reasoning. 

- LLMs do not necessarily perform better on facilitated versions of classic reasoning problems, unlike human subjects. This suggests they may have already been exposed to some problems during training.

- Mathematical tasks proved more challenging for LLMs than non-mathematical ones. Even models that gave logically correct reasoning struggled with basic calculations at times.

The main methodological contribution is providing a way to compare rational capabilities across different LLMs using established human benchmarks. While not proposing a definitive set of tests, the paper demonstrates how we can evaluate model performance on reasoning tasks. This allows identifying limitations in areas like logical consistency as AI systems continue to advance. Overall, it introduces an approach to assess model rationality grounded in the cognitive psychology literature.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms associated with it are:

- Large language models (LLMs)
- Rationality 
- Irrationality
- Cognitive biases
- Heuristics 
- Reasoning
- Inconsistency
- Performance evaluation
- Machine psychology
- Species-fair comparisons
- GPT-3.5
- GPT-4
- LaMDA
- Claude 2
- LLAMA 2
- Confirmation bias
- Inverse/conditional probability fallacy  
- Insensitivity to sample size
- Gambler's fallacy
- Endowment effect
- Conjunction fallacy  
- Representativeness heuristic
- Cognitive Reflection Test (CRT)

The paper evaluates the rational and irrational reasoning capabilities of several LLMs using tests designed to assess human reasoning and decision making. It analyzes whether the models display correct logical reasoning or exhibit biases seen in humans. A key finding is that LLMs demonstrate inconsistency in their responses, revealing a distinct kind of irrationality compared to humans. The paper proposes a methodology to assess and compare rational abilities across different LLMs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes treating large language models as participants in psychological experiments to evaluate their rational reasoning abilities. What are some of the key advantages and disadvantages of this methodology compared to other approaches for evaluating reasoning in AI systems?

2. The paper evaluates model performance across two dimensions - correct vs incorrect responses, and human-like vs non-human-like errors. What other frameworks could be used to categorize the responses and evaluate reasoning systematically? How could multiple categorization schemes be combined to get further insight?

3. The results show significant inconsistency across responses from the same models on identical tasks. What could be some reasons for this inconsistency? How can consistency be improved in future iterations of such models? 

4. For the facilitated versions of classic reasoning tasks, the paper finds that model performance does not always improve compared to the non-facilitated versions, unlike in humans. What modifications could be made to the facilitation methodology to make the tasks clearer for LLM reasoning?

5. Mathematical vs non-mathematical tasks showed a performance gap across models. Is this gap seen consistently across different types of mathematical reasoning? What specifically makes mathematical reasoning more challenging for current LLMs?

6. The paper identifies both similarities and differences between human and LLM failures in reasoning. What are some key next steps to bring LLM reasoning closer to correct, human-like reasoning? Can insights from cognitive science research help?

7. The paper studies mostly single-turn, zero-shot reasoning by the models without any feedback or fine-tuning. How could an experimental framework incorporate iterative reasoning, feedback and learning to evaluate rationality development over time?

8. Some models provide non-answers or refuse to answer certain sensitive questions. How can evaluation metrics account for such refused responses? What is a principled way for models to refuse unsafe questions?

9. The paper studies a limited set of known tasks designed to trigger specific human reasoning errors. What methodologies could generate new reasoning task sets to minimize bias from models having seen the tasks previously? 

10. How can benchmarks developed from this methodology account for variation in human rationality and calibration of what kinds of errors are "acceptably rational"? What data could better inform developing such a calibrated benchmark?
