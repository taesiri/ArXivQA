# [(Ir)rationality and Cognitive Biases in Large Language Models](https://arxiv.org/abs/2402.09193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper seeks to evaluate whether large language models (LLMs) display rational reasoning or if they exhibit biases and heuristics similar to humans. Specifically, the goal is to assess the rationality of LLMs using tasks from cognitive psychology that were designed to highlight irrational reasoning in humans. The paper also aims to provide a methodology to assess and compare the capabilities of different LLMs with respect to rationality.

Methods:
The authors evaluated 7 LLMs - GPT-3.5, GPT-4, Google's Bard, Anthropic's Claude 2, and 3 versions of Meta's Llama 2. They prompted the models with 12 tasks from cognitive psychology literature that were originally designed to reveal biases in human reasoning. The tasks include the Wason selection task, AIDS test problem, hospital birth problem, Monty Hall problem and others. Facilitated versions were also included. Each task was prompted 10 times and responses categorized as correct/incorrect and human-like/not human-like.

Results: 
The models displayed high inconsistency in their responses to the same tasks. While the tasks reveal irrational reasoning in humans, the LLMs exhibited a different kind of irrationality - their incorrect responses generally did not display stereotypical human biases and they were inconsistent across multiple runs. GPT-4 had the best performance overall. Llama 2 models refused to answer many questions. Performance was lower on tasks requiring mathematical calculations compared to logical reasoning questions.  

Conclusions:
The paper demonstrates that LLMs do not fail at these reasoning tasks in the same way humans do. Their errors are different from standard human biases. The high inconsistency in responses also reveals limitations in critical reasoning abilities. The paper contributes methodologically by showing how tasks from cognitive psychology can be used to assess rational capabilities of LLMs and compare model performance. The approach could facilitate developing benchmarks to evaluate model rationality.
