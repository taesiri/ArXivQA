# [SUN Team's Contribution to ABAW 2024 Competition: Audio-visual   Valence-Arousal Estimation and Expression Recognition](https://arxiv.org/abs/2403.12609)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Emotion recognition in unconstrained "in-the-wild" settings is challenging but important for enhancing human-computer interaction. 
- While multimodal systems have high performance on lab data, achieving ecological validity on non lab-controlled data remains difficult.

Proposed Solution:
- The authors explore audiovisual deep learning methods, specifically CNN and transformer architectures fine-tuned on pre-trained models. 
- For video, EfficientNet and ViT models are used as feature extractors. These static models are first pre-trained on emotion datasets, fine-tuned on AffWild2, then frozen for temporal modeling.
- For audio, the PDEM model is fine-tuned. GRU layers are added on top for temporal modeling.
- Late fusion schemes like Dirichlet-based Random Weighted Fusion and Random Forests are used to combine predictions from audio and video models.

Main Contributions:
- Pre-training and fine-tuning strategies for adapting CNN and transformer models for in-the-wild emotion recognition
- Comparison of different temporal modeling techniques including RNN, Transformer and statistical functionals
- Analysis of various fusion methods to effectively leverage complementary audiovisual information
- Results validating the proposed methods on the AffWild2 dataset for both categorical emotion recognition and valence-arousal regression

The key ideas are leveraging pre-trained models as feature extractors, experimenting with different temporal aggregation strategies, and fusing audio and video effectively to push the state-of-the-art for in-the-wild emotion recognition. The paper provides useful insights and analysis to advance multimodal affective computing research.
