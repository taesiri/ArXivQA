# [Binarized Neural Architecture Search](https://arxiv.org/abs/1911.10862v2)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an efficient neural architecture search method for finding optimal binarized neural network architectures. 

The key points are:

- The authors propose a binarized neural architecture search (BNAS) method to search for hardware-efficient binarized neural networks. This is more challenging than standard neural architecture search (NAS) due to the difficulties in optimizing binarized networks. 

- To improve search efficiency, they introduce channel sampling and operation space reduction strategies into the differentiable NAS framework. Specifically, they use a performance-based strategy to progressively abandon less promising operations during the search.

- They implement BNAS based on two binarization methods - XNOR networks and projection convolutional neural networks (PCNNs). 

- Experiments on CIFAR-10 and ImageNet show their BNAS method can find binarized architectures that achieve accuracies comparable to NAS-found architectures, but with much higher compression rates and 40% faster search speed.

In summary, the key hypothesis is that incorporating channel sampling and operation space reduction strategies into differentiable NAS can enable efficient search for high-performance binarized neural architectures. The results validate this hypothesis and demonstrate the effectiveness of their proposed BNAS method.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing BNAS, the first binarized neural architecture search algorithm, to search for optimal architectures of binarized neural networks. 

2. Introducing channel sampling and operation space reduction into differentiable NAS to significantly reduce the search cost.

3. Using a performance-based strategy to progressively abandon less promising operations during the search, further speeding up the search process by 40% compared to prior work.

4. Achieving state-of-the-art results using BNAS-discovered binarized architectures on CIFAR-10 and ImageNet, with comparable accuracy to full precision networks but highly compressed models.

5. Demonstrating the generalization of BNAS by validating it with two different binarized neural network optimization methods, XNOR nets and PCNN.

In summary, the key contribution is developing an efficient neural architecture search framework specialized for binarized networks, called BNAS. It can discover highly compressed yet accurate models, reducing the memory and computational costs of neural networks. The proposed techniques like operation space reduction and the performance-based search strategy significantly improve the efficiency of architecture search.
