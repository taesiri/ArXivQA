# [ScribFormer: Transformer Makes CNN Work Better for Scribble-based   Medical Image Segmentation](https://arxiv.org/abs/2402.02029)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Medical image segmentation plays a crucial role in disease diagnosis and treatment planning. However, most deep learning methods rely on large datasets with dense pixel-level annotations, which are expensive and time-consuming to obtain.  
- Weakly-supervised learning with scribble annotations can reduce annotation burden, but CNN-based methods struggle to capture global shape information from limited supervision signals, often resulting in mis-segmented regions.

Proposed Solution:
- The paper proposes ScribFormer, a novel CNN-Transformer hybrid network for scribble-supervised medical image segmentation.
- It consists of three branches - a CNN branch, a Transformer branch, and an attention-guided Class Activation Map (ACAM) branch.
- The CNN branch captures local features while the Transformer branch encodes global representations to overcome CNN's limitations. Both outputs are compared to scribble annotations.
- The predictions are also dynamically mixed to supervise each other as pseudo labels.
- The ACAM branch generates attention maps from multiple CNN layers and refines them via an ACAM-consistency loss. This expands the supervised region.

Main Contributions:
- First Transformer-based solution for scribble supervision, hybridizing CNN and Transformer to leverage both global context and local details.
- ACAM branch with attention modulation and consistency loss to expand activation maps and supervision signals.
- Comprehensive experiments showing state-of-the-art performance over previous scribble-supervised methods on ACDC, MSCMR and HeartUII datasets.
- Outperforms most fully-supervised methods despite using only scribble annotations, demonstrating effectiveness.

In summary, the paper makes CNN-Transformer hybridization feasible for scribble supervision via a well-designed architecture with complementary components that reinforce each other. Both quantitative and qualitative results validate the high segmentation accuracy achieved by ScribFormer.
