# [SMPConv: Self-moving Point Representations for Continuous Convolution](https://arxiv.org/abs/2304.02330)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It proposes an alternative approach to building continuous convolution kernels using self-moving point representations and interpolation, instead of using multilayer perceptrons (MLPs) like most prior work. - The goal is to create a more computationally efficient and higher performing continuous convolution, while avoiding the drawbacks of MLP-based approaches such as high computational cost, complex hyperparameter tuning, and limited descriptive power of the filters.- The self-moving point representation allows weight parameters to freely move, and interpolation is used to implement continuous functions for the kernels. This results in lightweight and flexible representations.- Experiments across different tasks (1D sequence, 2D image, large-scale ImageNet) demonstrate improved performance and efficiency over MLP-based continuous convolutions and other baselines.- The method provides benefits such as handling irregularly sampled data, modeling long-term dependencies, constructing large kernels efficiently, and improved performance as a drop-in replacement for standard convolution.In summary, the key hypothesis is that self-moving point representations with interpolation can build better performing and more efficient continuous convolutional kernels compared to prevailing MLP-based approaches. The experiments seem to validate this hypothesis across different tasks and datasets.
