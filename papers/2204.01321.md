# [PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking   Models](https://arxiv.org/abs/2204.01321)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How to effectively attack neural ranking models under the practical decision-based black-box attack setting, where the attacker has no direct access to the model and can only query it to get ranking decisions.The authors propose a novel attack method called PRADA to address this question. The key ideas are:1) Learn a surrogate model to imitate the target neural ranking model, based on pseudo relevance feedback. 2) Identify important words in the document that influence rankings the most.3) Perturb the embedding of important words to promote document ranking.4) Iteratively replace important words with synonyms to generate adversarial examples.The paper focuses on attacking neural ranking models like BERT in a black-box manner, which is a realistic but challenging setting. The proposed PRADA method aims to craft adversarial examples that can mislead neural ranking models while preserving semantics and fluency. Evaluations on benchmark datasets demonstrate the effectiveness of PRADA.In summary, the central hypothesis is that it's possible to launch effective yet inconspicuous adversarial attacks against neural ranking models under practical black-box conditions, by careful surrogate modeling, word importance analysis, embedding perturbation and semantic-preserving replacement. The PRADA method provides an initial solution.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Introducing a new Word Substitution Ranking Attack (WSRA) task against neural ranking models (NRMs). The goal is to promote a target document's rank by replacing important words with synonyms. 2. Proposing a novel PRADA (Pseudo Relevance-based ADversarial ranking Attack) method to generate adversarial examples under the challenging decision-based black-box attack setting. PRADA learns a surrogate model using pseudo relevance feedback, identifies important words, perturbs embeddings, and iteratively replaces words.3. Conducting experiments on two web search datasets (MS MARCO document and passage ranking) to demonstrate PRADA's effectiveness. The results show it can achieve high attack success rates with small indiscernible perturbations compared to baselines.4. Analyzing PRADA's ability to evade spam detection, the impact of different components, performance across rank positions, and the number of perturbed words.5. Providing insights into vulnerabilities of neural ranking models, which could help facilitate development of more robust models and countermeasures before deployment.In summary, the key contribution appears to be introducing a new adversarial attack task for ranking, and proposing a novel method PRADA that can effectively attack neural ranking models in a challenging black-box setting. The experiments and analyses provide useful insights.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points from the paper:The paper proposes a new adversarial attack task called Word Substitution Ranking Attack (WSRA) against neural ranking models, focusing on a challenging decision-based black-box attack setting, and introduces a novel attack method called PRADA that can successfully fool neural ranking models by replacing important words in a document with synonyms while preserving semantics.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work on adversarial attacks against neural ranking models:- It focuses on a new task, the Word Substitution Ranking Attack (WSRA), which aims to promote a target document's ranking by replacing words with synonyms. This is a novel adversarial attack problem formulation compared to prior work.- It tackles the challenging decision-based black-box attack setting, where the attacker can only query the target model to get ranked lists. Most prior adversarial attack research in IR has focused on white-box or score-based attacks.- The proposed PRADA method is innovative in using pseudo-relevance feedback to train a surrogate model for guiding the attack. Other black-box attack methods often rely on heuristics or randomization.- Experiments are conducted on two large-scale web search datasets (MS MARCO document/passage ranking), using BERT neural ranking models. This is a more realistic setting than some prior adversarial IR papers that used smaller datasets or simpler learning-to-rank models.- The evaluation is fairly comprehensive, including automatic metrics like success rate, perturbation percentage, semantic similarity, and human evaluations of quality. Many prior attack papers have focused just on success rate.- Compared to related work on adversarial attacks for text classification, the constraints and goals are different for the ranking problem, requiring new techniques tailored for promoting documents in a list.Overall, this paper makes solid contributions to the nascent field of adversarial information retrieval, proposing a new task formulation and novel attack method for a realistic threat model. The experiments demonstrate promising results, outperforming heuristic attacks. More research is still needed to develop robust defenses and attacks against ever-improving neural ranking models.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Developing more advanced attack objectives to jointly consider long text and short text for promoting robust neural ranking models. The current PRADA method performs worse on short texts like passages compared to long documents.- Conducting attacks against real-world search engines using PRADA to demonstrate its practical applicability. The current work focuses on attacking research benchmark datasets.- Studying adversarial defense methods specifically for neural ranking models to identify vulnerabilities and develop more robust models before deployment. The authors hope this adversarial attack analysis could provide useful insights for future defense research. - Going further to explore stronger black-box attack methods against neural ranking models. The decision-based black-box setting tackled in this paper is quite challenging, but even more advanced attacks could be developed.- Analyzing the transferability of adversarial examples across different neural ranking architectures. The authors utilize this transferability property to train a surrogate model for the attacks, but more analysis can be done.In summary, the main future directions are: 1) improving attacks for short texts, 2) evaluating against real-world systems, 3) developing defense methods, 4) designing more advanced attacks, and 5) studying transferability across models. The authors aim to promote research on robust neural ranking through this adversarial attack analysis.
