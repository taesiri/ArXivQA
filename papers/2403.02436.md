# [How does Architecture Influence the Base Capabilities of Pre-trained   Language Models? A Case Study Based on FFN-Wider Transformer Models](https://arxiv.org/abs/2403.02436)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models like BERT and GPT have shown strong "base capabilities" beyond just language modeling, like transfer learning and few-shot learning. However, the impact of model architecture changes on these base capabilities is not well understood. 
- The paper focuses on studying Transformer models with wider Feedforward Network (FFN) layers. It is observed that widening the FFN layers leads to reduced performance on various downstream tasks, indicating a drop in base capabilities compared to vanilla Transformers.

Proposed Explanation:
- The Multi-Headed Self-Attention (MHA) layer in Transformers serves as a "combination function", expressing language combinability. The FFN layer serves more as a per-position "transformation function".  
- The actual contribution ratio of these layers to pre-training is key. Widening FFN reduces relative contribution of MHA, which hurts base capabilities.

Solution: 
- Propose a "Combination Adjustable Architecture" to control relative contribution ratios of MHA and FFN. Gradually reducing FFN contribution shows improvement in base capabilities, confirming the hypothesis.
- Determine an optimal "Combination Enhanced Architecture" that improves base capabilities of widened FFN Transformers close to vanilla levels.

Key Contributions:
- Identify and analyze issue of base capability degradation when widening Feedforward layers in Transformers
- Explain underlying reason to be the reduced relative contribution of Multi-Headed Self-Attention to pre-training
- Confirm explanation through designed architectures that can adjust relative contribution ratios
- Propose a Combination Enhanced Architecture to improve base capabilities of widened FFN Transformers
- Show applicability of findings even for other architectures like Mixture-of-Experts Transformers

The paper provides useful insights into how architectural changes can affect essential inductive biases and base capabilities of pre-trained language models. The analysis and solution approach could serve as a guideline for better model architecture design.


## Summarize the paper in one sentence.

 This paper analyzes how architectural changes in transformers, specifically widening the feedforward network layers, degrade base capabilities like out-of-distribution language modeling and transfer learning, proposes an explanation involving reduced contribution from the multi-head attention combination function, validates this through controlled experiments, and provides an enhancement method to mitigate the capability decline.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is:

1) It analyzes and explains why Feedforward-Wider Transformers exhibit a decline in base capabilities compared to vanilla Transformers, even when pre-training performance is similar. The key reason identified is that wider Feedforward Networks reduce the actual contribution ratio of the Multi-Head Attention module (which captures language combinability) during pre-training.

2) It proposes a Combination Enhanced Architecture to address the decline in base capabilities of Feedforward-Wider Transformers. By bifurcating the wider Feedforward Network and relocating part of it within the MHA module, the actual contribution ratio of the combination function (MHA) is increased, leading to improved base capabilities.

3) The analysis and proposed architecture are shown to be effective not just for Feedforward-Wider Transformers but also applicable to other architectures like Mixture-of-Experts Transformers. This demonstrates their value in providing useful insights and guidance for architecture design, analysis and improvement in general.

In summary, the key contribution is explaining and addressing the decline in base capabilities for certain Transformer architectures by identifying and modifying a key influencing factor - the actual contribution ratio of the combination function during pre-training.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Base capabilities - The paper focuses on exploring the base capabilities of pre-trained language models, including out-of-distribution language modeling, transfer learning, and few-shot learning. 

- Architecture - The paper examines how architectural choices in transformers, specifically using wider feedforward networks (FFN-Wider Transformers), impact model base capabilities.

- Combination function - The multi-head attention (MHA) layer in transformers is analyzed as a "combination function" that expresses combinability of language.

- Contribution ratio - The paper hypothesizes that the contribution ratio of the MHA layer during pre-training is a key factor influencing base capabilities. Experiments manipulate contribution ratios.

- Inductive bias - The impact of architectural inductive biases on base capabilities is a focus. Models with same pre-training performance but different architectures are compared.

- FFN-Wider Transformers - Transformers with wider than normal feedforward network layers. The paper aims to explain and address their reduced base capabilities.

- Combination Enhanced Architecture (CEA) - A proposed architectural variant that bifurcates the wider feedforward network to enhance contribution of the MHA combination function.

So in summary, the key focus is on how architectural choices impact the base capabilities of transformers through their inductive biases, specifically by altering combination vs. transformation function contribution ratios.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a Combination Adjustable Architecture (CAA) to intervene in the contribution ratios of the multi-head attention (MHA) layer and feedforward network (FFN) layer. What is the rationale behind adjusting these contribution ratios? How does it test the hypothesis about actual contribution ratios affecting base capabilities?

2. The CAA bifurcates a wider FFN into an Outer-FFN and Inner-FFN. What is the purpose of this bifurcation? Why is the Inner-FFN designed to serve only the combination function? 

3. The paper finds optimal width ratios for the Outer-FFN in CAA when applied to BERT and GPT models. What factors determine these optimal ratios? How would you select the optimal ratio for other architecture models? 

4. How exactly does the direct pathway designed in the MHA ensure that the Inner-FFN cannot bypass the combination function? What would be the consequences of not having this direct pathway?

5. The paper tests CAA extensively on BERT and GPT models. Do you think CAA can generalize to other architecture models? What adaptations would be needed to apply it to models like T5 or BART?

6. When testing CAA on MoE models, comprehensive improvements in base capabilities are not observed. What could be the reasons? Does it indicate limitations of the method or scope for improvements?

7. The paper focuses on language modeling as the pre-training objective. How do you think using other pre-training objectives like masked language modeling (MLM) could affect the conclusions?

8. The method targets base capabilities like out-of-distribution language modeling, transfer learning, and few-shot learning. Do you think it could also improve capabilities like following instructions? Why or why not?  

9. The paper analyzes contribution ratios using mutual information and token prediction. What other analysis methods could provide additional insights into the effect of CAA?

10. The conclusion states that contribution ratio of the MHA layer is likely a universal factor affecting base capabilities. Do you agree? What further experiments could conclusively establish the universality?
