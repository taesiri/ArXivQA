# [How does Architecture Influence the Base Capabilities of Pre-trained   Language Models? A Case Study Based on FFN-Wider Transformer Models](https://arxiv.org/abs/2403.02436)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models like BERT and GPT have shown strong "base capabilities" beyond just language modeling, like transfer learning and few-shot learning. However, the impact of model architecture changes on these base capabilities is not well understood. 
- The paper focuses on studying Transformer models with wider Feedforward Network (FFN) layers. It is observed that widening the FFN layers leads to reduced performance on various downstream tasks, indicating a drop in base capabilities compared to vanilla Transformers.

Proposed Explanation:
- The Multi-Headed Self-Attention (MHA) layer in Transformers serves as a "combination function", expressing language combinability. The FFN layer serves more as a per-position "transformation function".  
- The actual contribution ratio of these layers to pre-training is key. Widening FFN reduces relative contribution of MHA, which hurts base capabilities.

Solution: 
- Propose a "Combination Adjustable Architecture" to control relative contribution ratios of MHA and FFN. Gradually reducing FFN contribution shows improvement in base capabilities, confirming the hypothesis.
- Determine an optimal "Combination Enhanced Architecture" that improves base capabilities of widened FFN Transformers close to vanilla levels.

Key Contributions:
- Identify and analyze issue of base capability degradation when widening Feedforward layers in Transformers
- Explain underlying reason to be the reduced relative contribution of Multi-Headed Self-Attention to pre-training
- Confirm explanation through designed architectures that can adjust relative contribution ratios
- Propose a Combination Enhanced Architecture to improve base capabilities of widened FFN Transformers
- Show applicability of findings even for other architectures like Mixture-of-Experts Transformers

The paper provides useful insights into how architectural changes can affect essential inductive biases and base capabilities of pre-trained language models. The analysis and solution approach could serve as a guideline for better model architecture design.
