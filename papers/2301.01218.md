# [Tracing the Origin of Adversarial Attack for Forensic Investigation and   Deterrence](https://arxiv.org/abs/2301.01218)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How to trace the origin of adversarial examples generated from one of many similar deep learning models in a buyer-seller setting?

Specifically, the paper proposes techniques to determine which model an adversarial example was created from, given a set of candidate models trained on the same task. This could aid in forensic analysis and provide deterrence against attacks in scenarios where models are distributed to buyers. 

The key ideas are:

- Model separation: Modify training to make each distributed model uniquely susceptible to adversarial examples, so attacks leave distinct fingerprints. This is done via a parallel network structure with a noise-sensitive "tracer".

- Origin tracing: Given adversarial examples and candidate models, identify the likely source model by analyzing the tracer outputs and finding the one with the most distinct logit differences for the true and target labels.

So in summary, the main research contribution is a framework for tracing adversarial examples to their source model by intentionally creating detectable differences during training, then exploiting those differences to identify the origin. This provides a kind of forensic traceability against adversarial attacks in distributed model settings.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Introducing the concept of traceability for adversarial attacks in the buyers-seller setting, where the seller wants to be able to trace which buyer generated adversarial examples. This concept could aid forensic investigation of attacks and deter future attacks.

2. Proposing a framework with two stages - model separation and origin tracing - to achieve traceability. 

3. For model separation, proposing a parallel network structure with a unique tracer paired with the classifier and a noise-sensitive training loss to inject distinct characteristics into each model copy.

4. For origin tracing, proposing a logits-based tracing mechanism that compares the output logits of the tracer models on the adversarial examples to identify the likely source model. 

5. Conducting experiments on multiple datasets and network architectures which show the proposed framework can effectively trace the origin of adversarial examples. Tracing accuracy over 97% is achieved on some tasks.

In summary, the main contribution appears to be introducing the concept of traceability for adversarial attacks in the buyers-seller setting, and proposing a concrete framework to achieve traceability by inducing unique characteristics during model separation and exploiting those characteristics during tracing. The experimental results demonstrate the promise of the proposed techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a framework to trace the origin of adversarial examples in a buyers-seller setting by pairing unique tracers with classifiers during model separation and exploiting tracer output logits during origin tracing.


## How does this paper compare to other research in the same field?

 Based on my reading of the anonymous paper submission, here is a summary of how it compares to related work:

The paper focuses on the problem of tracing the origin of adversarial attacks, specifically in a buyers-seller setting where multiple copies of a model are distributed. This provides a novel forensic perspective on defending against adversarial attacks, as most prior work has focused on making models more robust to attacks or detecting adversarial examples. The idea of tracing attacks back to their source model for investigation and deterrence is unique.

The proposed framework has two main components: model separation and origin tracing. For model separation, the use of a parallel network structure with a tracer paired with the classifier is different from prior work on watermarking or fingerprinting models. The noise-sensitive loss function used to train the tracers also seems to be a new technique. 

For origin tracing, the use of the tracers' output logits as distinguishing features for models is different from prior forensic techniques like modeling gradient distributions. Analyzing logits for tracing does not appear to be explored before.

Overall, the focus on traceability and the forensic aspect of defending against adversarial attacks makes this work quite distinct from prior literature. The two-stage framework with parallel tracer networks and logits-based tracing are novel techniques proposed to achieve traceability. The results demonstrating high tracing accuracy support the promise of the approach.

In summary, I believe this paper makes a valuable contribution by opening up the new problem area of tracing adversarial attacks forensically. The framework and techniques seem innovative compared to related work in adversarial defense and watermarking. The empirical results also lend credibility to the ideas proposed. The approach could inspire more work at the intersection of security, forensics and adversarial learning.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the main future research directions suggested by the authors are:

1. Finding better features to express the model characteristics and improve the tracing accuracy. The authors currently use the output logits for tracing, but suggest exploring other internal features of the network that could provide better discrimination between models. 

2. Studying adaptive attacks that could potentially bypass the proposed tracing mechanism, and ways to make the system more robust against such attacks. For example, the authors discuss that attacking the main classifier C and bypassing the tracer T could be a strategy, but may come at the "cost" of degrading attack quality. They suggest further research into utilizing such costs to evade adaptive attacks.

3. Extending the approach to other application scenarios beyond the buyers-seller setting. The current work focuses on tracing attacks in a multi-model distribution scenario, but the ideas could potentially be applied more broadly. Exploring traceability in other contexts is suggested as a research direction.

4. Improving the training methodology to generate more diverse and unique models to enhance traceability. The current approach relies on random initialization and noise-sensitive training, but more advanced generative techniques could be explored.

5. Analyzing theoretical bounds on tracing accuracy and transferability rates for the proposed framework. Deriving formal guarantees on the limits of traceability could be an interesting direction.

In summary, the main suggestions are to explore improvements to the tracing mechanism, study adaptive attacks and defenses, generalize the approach to other applications, improve the model generation process, and conduct theoretical analysis - in order to further advance the traceability research.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

This paper proposes a framework for tracing the origin of adversarial attacks in a buyers-seller setting where a seller distributes multiple copies of a machine learning model to different buyers. The framework has two stages: model separation and origin tracing. In the model separation stage, they propose a parallel network structure that pairs a unique "tracer" model with the original classifier. The tracer model is trained to be sensitive to noise so that it will be easier to attack, trapping adversarial attacks to induce unique features. For origin tracing, they feed the adversarial examples into the tracer models and analyze the output logits, exploiting differences induced by the unique tracers to identify the likely source model. Experiments on different datasets and network architectures show the approach can effectively trace adversarial examples to their origin with high accuracy. The paper demonstrates a new aspect of defending against adversarial attacks by enabling forensic tracing to the source, which can aid investigation and provide deterrence.
