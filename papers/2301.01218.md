# [Tracing the Origin of Adversarial Attack for Forensic Investigation and   Deterrence](https://arxiv.org/abs/2301.01218)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How to trace the origin of adversarial examples generated from one of many similar deep learning models in a buyer-seller setting?Specifically, the paper proposes techniques to determine which model an adversarial example was created from, given a set of candidate models trained on the same task. This could aid in forensic analysis and provide deterrence against attacks in scenarios where models are distributed to buyers. The key ideas are:- Model separation: Modify training to make each distributed model uniquely susceptible to adversarial examples, so attacks leave distinct fingerprints. This is done via a parallel network structure with a noise-sensitive "tracer".- Origin tracing: Given adversarial examples and candidate models, identify the likely source model by analyzing the tracer outputs and finding the one with the most distinct logit differences for the true and target labels.So in summary, the main research contribution is a framework for tracing adversarial examples to their source model by intentionally creating detectable differences during training, then exploiting those differences to identify the origin. This provides a kind of forensic traceability against adversarial attacks in distributed model settings.
