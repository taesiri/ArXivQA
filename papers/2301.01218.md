# [Tracing the Origin of Adversarial Attack for Forensic Investigation and   Deterrence](https://arxiv.org/abs/2301.01218)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How to trace the origin of adversarial examples generated from one of many similar deep learning models in a buyer-seller setting?

Specifically, the paper proposes techniques to determine which model an adversarial example was created from, given a set of candidate models trained on the same task. This could aid in forensic analysis and provide deterrence against attacks in scenarios where models are distributed to buyers. 

The key ideas are:

- Model separation: Modify training to make each distributed model uniquely susceptible to adversarial examples, so attacks leave distinct fingerprints. This is done via a parallel network structure with a noise-sensitive "tracer".

- Origin tracing: Given adversarial examples and candidate models, identify the likely source model by analyzing the tracer outputs and finding the one with the most distinct logit differences for the true and target labels.

So in summary, the main research contribution is a framework for tracing adversarial examples to their source model by intentionally creating detectable differences during training, then exploiting those differences to identify the origin. This provides a kind of forensic traceability against adversarial attacks in distributed model settings.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Introducing the concept of traceability for adversarial attacks in the buyers-seller setting, where the seller wants to be able to trace which buyer generated adversarial examples. This concept could aid forensic investigation of attacks and deter future attacks.

2. Proposing a framework with two stages - model separation and origin tracing - to achieve traceability. 

3. For model separation, proposing a parallel network structure with a unique tracer paired with the classifier and a noise-sensitive training loss to inject distinct characteristics into each model copy.

4. For origin tracing, proposing a logits-based tracing mechanism that compares the output logits of the tracer models on the adversarial examples to identify the likely source model. 

5. Conducting experiments on multiple datasets and network architectures which show the proposed framework can effectively trace the origin of adversarial examples. Tracing accuracy over 97% is achieved on some tasks.

In summary, the main contribution appears to be introducing the concept of traceability for adversarial attacks in the buyers-seller setting, and proposing a concrete framework to achieve traceability by inducing unique characteristics during model separation and exploiting those characteristics during tracing. The experimental results demonstrate the promise of the proposed techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a framework to trace the origin of adversarial examples in a buyers-seller setting by pairing unique tracers with classifiers during model separation and exploiting tracer output logits during origin tracing.


## How does this paper compare to other research in the same field?

 Based on my reading of the anonymous paper submission, here is a summary of how it compares to related work:

The paper focuses on the problem of tracing the origin of adversarial attacks, specifically in a buyers-seller setting where multiple copies of a model are distributed. This provides a novel forensic perspective on defending against adversarial attacks, as most prior work has focused on making models more robust to attacks or detecting adversarial examples. The idea of tracing attacks back to their source model for investigation and deterrence is unique.

The proposed framework has two main components: model separation and origin tracing. For model separation, the use of a parallel network structure with a tracer paired with the classifier is different from prior work on watermarking or fingerprinting models. The noise-sensitive loss function used to train the tracers also seems to be a new technique. 

For origin tracing, the use of the tracers' output logits as distinguishing features for models is different from prior forensic techniques like modeling gradient distributions. Analyzing logits for tracing does not appear to be explored before.

Overall, the focus on traceability and the forensic aspect of defending against adversarial attacks makes this work quite distinct from prior literature. The two-stage framework with parallel tracer networks and logits-based tracing are novel techniques proposed to achieve traceability. The results demonstrating high tracing accuracy support the promise of the approach.

In summary, I believe this paper makes a valuable contribution by opening up the new problem area of tracing adversarial attacks forensically. The framework and techniques seem innovative compared to related work in adversarial defense and watermarking. The empirical results also lend credibility to the ideas proposed. The approach could inspire more work at the intersection of security, forensics and adversarial learning.
