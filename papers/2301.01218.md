# [Tracing the Origin of Adversarial Attack for Forensic Investigation and   Deterrence](https://arxiv.org/abs/2301.01218)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How to trace the origin of adversarial examples generated from one of many similar deep learning models in a buyer-seller setting?

Specifically, the paper proposes techniques to determine which model an adversarial example was created from, given a set of candidate models trained on the same task. This could aid in forensic analysis and provide deterrence against attacks in scenarios where models are distributed to buyers. 

The key ideas are:

- Model separation: Modify training to make each distributed model uniquely susceptible to adversarial examples, so attacks leave distinct fingerprints. This is done via a parallel network structure with a noise-sensitive "tracer".

- Origin tracing: Given adversarial examples and candidate models, identify the likely source model by analyzing the tracer outputs and finding the one with the most distinct logit differences for the true and target labels.

So in summary, the main research contribution is a framework for tracing adversarial examples to their source model by intentionally creating detectable differences during training, then exploiting those differences to identify the origin. This provides a kind of forensic traceability against adversarial attacks in distributed model settings.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Introducing the concept of traceability for adversarial attacks in the buyers-seller setting, where the seller wants to be able to trace which buyer generated adversarial examples. This concept could aid forensic investigation of attacks and deter future attacks.

2. Proposing a framework with two stages - model separation and origin tracing - to achieve traceability. 

3. For model separation, proposing a parallel network structure with a unique tracer paired with the classifier and a noise-sensitive training loss to inject distinct characteristics into each model copy.

4. For origin tracing, proposing a logits-based tracing mechanism that compares the output logits of the tracer models on the adversarial examples to identify the likely source model. 

5. Conducting experiments on multiple datasets and network architectures which show the proposed framework can effectively trace the origin of adversarial examples. Tracing accuracy over 97% is achieved on some tasks.

In summary, the main contribution appears to be introducing the concept of traceability for adversarial attacks in the buyers-seller setting, and proposing a concrete framework to achieve traceability by inducing unique characteristics during model separation and exploiting those characteristics during tracing. The experimental results demonstrate the promise of the proposed techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a framework to trace the origin of adversarial examples in a buyers-seller setting by pairing unique tracers with classifiers during model separation and exploiting tracer output logits during origin tracing.
