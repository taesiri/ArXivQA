# [FedD2S: Personalized Data-Free Federated Knowledge Distillation](https://arxiv.org/abs/2402.10846)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Federated learning (FL) faces challenges from non-IID (non-independent and identically distributed) data across clients, leading to model drift. This limits the personalization performance of a global model compared to models trained locally on each client's data.

- Existing personalized FL (pFL) methods have limitations in handling heterogeneity in data/model architectures across clients and rely on public datasets for knowledge transfer, which is impractical.

Proposed Solution:
- The paper proposes FedD2S, a novel data-free pFL approach using knowledge distillation, which incorporates a deep-to-shallow layer dropping mechanism.

- The key idea is to gradually exclude deeper layers in the local models from participating in federation. Deeper layers capture more personalized knowledge, which can negatively impact personalization if shared across clients.

- A mutual knowledge distillation strategy is employed with two phases: (1) clients-to-server: Partial knowledge from clients' models is transferred to global model on server (2) server-to-clients: Ensemble knowledge from global model is transferred back to enhance clients' models.

Main Contributions:

- Introduces adaptive deep-to-shallow layer dropping to prevent adverse knowledge sharing while retaining useful correlations to accelerate convergence.

- Employs head models to transform intermediate features to soft labels for more effective knowledge transfer between local and global models. 

- Achieves state-of-the-art personalization performance across diverse datasets without relying on public datasets, with faster convergence and improved fairness.

- Provides analysis on impact of key hyperparameters like layer dropping rate, elucidating the interactions between model configuration, data heterogeneity and performance.

In summary, the paper makes notable contributions in advancing personalized federated learning to handle practical challenges of systems heterogeneity and statistical data distributions by strategic layer-based knowledge sharing.
