# [ViTs for SITS: Vision Transformers for Satellite Image Time Series](https://arxiv.org/abs/2301.04944)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can vision transformer architectures be adapted and optimized for satellite image time series (SITS) processing tasks like crop type classification and semantic segmentation?

The authors propose a new model called Temporo-Spatial Vision Transformer (TSViT) specifically designed for SITS data. The key ideas and hypotheses tested in the paper are:

- A temporal-then-spatial factorization of SITS inputs is more suitable than spatial-then-temporal, common in video processing models. This captures temporal patterns first before modeling spatial interactions. 

- Using multiple learnable class tokens instead of a single token enables better separation of class-specific evidence. 

- Employing acquisition-time specific temporal position encodings helps account for irregular sampling in SITS.

- The proposed TSViT architecture outperforms previous convolutional and recurrent models on SITS classification and segmentation tasks.

So in summary, the main hypothesis is that a vision transformer tailored for SITS with the proposed modifications will achieve superior performance compared to prior arts on satellite image analysis tasks involving temporal modeling. The experiments aim to validate this hypothesis.
