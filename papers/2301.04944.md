# [ViTs for SITS: Vision Transformers for Satellite Image Time Series](https://arxiv.org/abs/2301.04944)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can vision transformer architectures be adapted and optimized for satellite image time series (SITS) processing tasks like crop type classification and semantic segmentation?

The authors propose a new model called Temporo-Spatial Vision Transformer (TSViT) specifically designed for SITS data. The key ideas and hypotheses tested in the paper are:

- A temporal-then-spatial factorization of SITS inputs is more suitable than spatial-then-temporal, common in video processing models. This captures temporal patterns first before modeling spatial interactions. 

- Using multiple learnable class tokens instead of a single token enables better separation of class-specific evidence. 

- Employing acquisition-time specific temporal position encodings helps account for irregular sampling in SITS.

- The proposed TSViT architecture outperforms previous convolutional and recurrent models on SITS classification and segmentation tasks.

So in summary, the main hypothesis is that a vision transformer tailored for SITS with the proposed modifications will achieve superior performance compared to prior arts on satellite image analysis tasks involving temporal modeling. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 This paper proposes a new deep learning architecture called Temporo-Spatial Vision Transformer (TSViT) for processing satellite image time series (SITS). The main contributions are:

- TSViT is the first fully attentional model for SITS, meaning it uses only attention mechanisms and no convolutional layers. This allows it to model long-range dependencies in time and space. 

- It employs a novel temporo-spatial factorization of inputs, processing temporal dependencies first. The paper argues this is more suitable for SITS than the typical spatial-temporal factorization used in video models.

- It introduces several modifications to the Vision Transformer architecture to make it more suitable for SITS:
  - Learned acquisition-time specific temporal position encodings to handle irregular sampling.
  - Multiple class tokens to enhance class separation. 
  - Constrained interactions between class tokens in the spatial encoder.

- Achieves new state-of-the-art results on three public SITS datasets for semantic segmentation and classification, outperforming previous convolutional and recurrent networks by a significant margin.

In summary, the main contribution is proposing the first fully attentional architecture designed specifically for SITS processing, with several innovations to handle the unique properties of satellite image timeseries. Experiments demonstrate superior performance over prior arts.
