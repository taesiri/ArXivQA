# [Fair Normalizing Flows](https://arxiv.org/abs/2106.05937)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is:How can we learn fair data representations that provably guarantee sensitive attributes cannot be recovered, while maintaining high utility for downstream prediction tasks?The key ideas and contributions in addressing this question are:- Proposing Fair Normalizing Flows (FNF), which uses normalizing flow models as encoders. This allows exact likelihood computation in the latent space.- Leveraging the exact likelihoods to minimize the statistical distance between latent representations of different sensitive groups. This provides an upper bound on the maximum unfairness of any downstream predictor.- Theoretical analysis showing how density estimation can be used to obtain guarantees even when the true densities are unknown.- Experimental evaluation on several real-world datasets demonstrating FNF can substantially increase fairness guarantees without significantly sacrificing accuracy.So in summary, the main research contribution is proposing a novel fair representation learning approach called Fair Normalizing Flows, which uniquely provides rigorous theoretical guarantees on the fairness of learned representations, while maintaining high utility. The theoretical analysis and experimental results support this.
