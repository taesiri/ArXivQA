# [AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods](https://arxiv.org/abs/2402.11215)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper proposes adaptive batch size schemes called \AdAdaGrad and \AdAdaGradNorm for adaptive gradient methods like \AdaGrad and \AdaGradNorm. The key idea is to incrementally increase batch sizes during training based on statistics of the batch gradients, while model updates are done using \AdaGrad or \AdaGradNorm.

Motivation:
- Batch size is a critical hyperparameter but its selection is mostly heuristic in deep learning. 
- Existing adaptive batch size methods focus on SGD, but adaptive gradient methods like \AdaGrad dominate deep learning practice.
- Intricate relationship between batch size and learning rate is not well understood for adaptive methods.

Contributions:
- Propose principled adaptive batch size schemes \AdAdaGrad and \AdAdaGradNorm derived from adaptive sampling methods.
- Establish high probability convergence rate for finding stationary point of smooth nonconvex functions using \AdAdaGradNorm and \AdAdaGrad.
- Relax standard smoothness assumption using generalized smoothness condition.
- Demonstrate improved performance over SGD and baseline adaptive methods on image classification tasks.

The adaptive batch size is determined based on statistics of per-sample gradients to ensure batch gradient is a good descent direction. Specifically, the norm and inner product tests from adaptive sampling literature are used.

If test conditions are violated, batch size is increased using precise formulae. Under reasonable assumptions, high probability convergence rates are proven for \AdAdaGradNorm (O(1/K)) and \AdAdaGrad (O(1/sqrt(K))) to find stationary points of smooth nonconvex functions. Generalized smoothness relaxes standard assumption.

Experiments on MNIST and CIFAR-10 classification using logistic regression and CNNs show faster convergence over SGD. Adaptivity of progressive batch size strategies is evident from batch size plots. Results showcase potential of such schemes for adaptive gradient methods in large-scale nonconvex optimization.
