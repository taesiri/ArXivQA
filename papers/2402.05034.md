# [How BERT Speaks Shakespearean English? Evaluating Historical Bias in   Contextual Language Models](https://arxiv.org/abs/2402.05034)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper explores historical bias in contextual language models (CLMs) like BERT when applied to analyze historical texts. Specifically, it looks at the adequacy of different CLMs with respect to Early Modern English (EME) from Shakespeare's time and Modern English (ME). Assessing the suitability of CLMs for historical text analysis is important but lacks systematic methods.  

Method:
The authors propose a method to quantify the historical bias and domain adequacy of CLMs. They define metrics to assign temporal valence scores to test sentences and predicted tokens indicating their proximity to EME or ME. Using these scores, they calculate a model's bias and domain adequacy for a sentence. The metrics measure how well a model's predictions match the expected language variety.

Experiments: 
The method is tested on BERT Base, MacBERTh (trained on 1500-1950 texts), and English HLM (BERT tuned on 19th century+ texts). 60 test sentences are created with 20 EME-specific, 20 ME-specific and 20 generic sentences. Model predictions and probabilities are collected and assigned temporal valence scores. Bias and domain adequacy are calculated.

Results: 
MacBERTh shows highest EME alignment and BERT Base shows highest ME alignment. English HLM tends neutral for marked sentences. For generic sentences, MacBERTh and English HLM remain consistent while BERT Base tends toward ME. The metrics successfully capture model biases and domain adequacy.

Contributions:
The key contributions are - (1) a method to systematically evaluate historical bias and domain adequacy of CLMs with respect to language varieties and periods, and (2) demonstration of its use to analyze suitability of models for historical text analysis. The framework is flexible - metrics can be customized for different language varieties.

In summary, the paper presents a novel analysis to assess and quantify the historical bias in CLMs to determine their reliability for tasks like historical text analysis.


## Summarize the paper in one sentence.

 This paper proposes a method and measure to evaluate the adequacy of contextual language models in representing historical varieties of English by analyzing their predictions on masked sentences marked for Early Modern or Modern English.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a method and metrics to evaluate the historical bias and domain adequacy of contextual language models with respect to different varieties of English over time. 

Specifically, the key contributions are:

1) Defining a temporal valence task to assess how well a language model aligns with early modern vs modern English. This involves creating test sentences marked with a temporal valence score and having models predict masked words to see if their predictions match the expected linguistic variety. 

2) Defining two metrics - bias score β and domain adequacy score δ - to quantify how much a model's predictions match the temporal valence of the test sentences. This measures the model's historical bias.

3) Demonstrating the method on Bert Base, MacBERTh, and English HLM models using a purpose-built test set of 60 sentences. The results show MacBERTh is most adequate for early modern English, while Bert Base fits modern English better.  

4) Proposing this method as a way for digital humanities scholars to evaluate the suitability of language models for analyzing historical texts. The metrics help reveal the linguistic biases of different models.

In summary, the key contribution is the temporal valence evaluation framework and associated metrics to systematically measure language model biases across different historical varieties of English.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Contextual language models (CLMs)
- Early Modern English (EME) 
- Modern English (ME)
- Bias in language models
- Diachronic bias
- Domain adequacy
- Temporal valence
- Fill-in-the-blank task
- BERT models (Bert-Base-Uncased, MacBERTh, Bert-Base-Historic-English)
- Test set creation
- Sentence temporal valence score
- Token-in-sentence temporal valence score
- Weighted bias score

The paper explores analyzing historical bias in contextual language models by comparing their performance on Early Modern vs Modern English. It proposes metrics like "temporal valence" and "domain adequacy" to quantify this. The method involves a fill-in-the-blank test with a set of masked sentences labeled as EME-specific, ME-specific or generic. The models' predictions are scored on a bipolar scale between the two language varieties to measure adequacy. The key BERT models tested are BERT Base, MacBERTh, and English HLM. So in summary, the key terms cover contextual language models, historical English language varieties, bias measurement, and the models and evaluation approach used.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a temporal valence task to evaluate the historical bias of contextual language models. Could you explain in more detail how the sentence temporal valence score ($\rho$) and token-in-sentence temporal valence score ($\sigma$) are defined and assigned? What criteria are used?

2. The bias score ($\beta$) and domain adequacy score ($\delta$) seem to provide complementary insights into model performance. How would you characterize the differences between what these two metrics tell you? When would you use one versus the other?  

3. The test set of sentences is central for evaluating model historical bias. What considerations went into designing the test set of 60 sentences? How were sentences selected and annotated to capture important linguistic differences between Early Modern and Modern English?

4. The paper evaluates three BERT-based models with different training. What motivated the choice of these three models? What hypotheses did you have about how they would differ in handling the test sentences? 

5. The distribution graphs highlight noticeable differences between the three models' bias and adequacy scores. For the neutral test sentences, what explains why the English HLM model tends to diverge from the other two?

6. The conclusion hints that purely semantic/syntactic test sentences have limitations. What are some examples of culture-oriented test sentences that could better probe a model's social knowledge? How feasible is it to construct such culturally-grounded tests?

7. The bias score is interpreted relative to the dichotomy of language varieties in the study. How could this framework incorporate more fine-grained historical nuance beyond a binary early/modern distinction? Would the scores need to be adapted?

8. How sensitive are the results to the annotation of the token temporal valence scores ($\sigma$)? What steps were taken to ensure consistency in assigning these scores? How could the robustness of annotations be improved in future work?

9. The study focuses specifically on diachronic bias in English language models. How could this evaluation approach be adapted to probe historical biases in language models for other languages? What components would need customization?

10. The conclusion proposes connections between analyzing model preferences and inferring cultural patterns. But what cautions need to be kept in mind before drawing sociocultural inferences? What are the limitations of interpreting temporal biases in purely cultural terms?
