# Building a Role Specified Open-Domain Dialogue System Leveraging   Large-Scale Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we build an open-domain dialogue system that can satisfy specified role constraints while still maintaining natural conversation abilities? In particular, the authors aim to study methods for imposing persona, style, safety and system policy constraints on open-domain chatbots, so they behave appropriately for a given role. This is challenging since it requires collecting suitable training data and developing models that can meet the role specifications without sacrificing performance on general dialogue metrics.To address this, the key ideas proposed in the paper are:1) A scalable data collection framework leveraging large language models to generate synthetic dialogue data satisfying arbitrary role specifications.2) Comparing various model architectures, including classifier-based filtering, response selection, and conditioned response generation, in terms of adhering to role specifications.3) Demonstrating the effectiveness of the proposed data collection and modeling methods through experiments on a new Korean dialogue dataset built for the role of a caring chatbot for senior citizens.In summary, the central hypothesis is that by combining large-scale LM-generated training data with specialized model architectures, we can develop open-domain chatbots that satisfy complex role constraints while still conversing naturally. The paper aims to explore methods for achieving this goal.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for building role specified open-domain dialogue systems. The key points are:- They propose a method to efficiently generate a dataset for training dialogue systems that satisfy particular role specifications, by leveraging large language models and human filtering. - They compare various model architectures like out-of-bounds detection, response selection, and response generation for building role-satisfying dialogue agents.- They demonstrate the effectiveness of their data collection framework and modeling approaches through experiments. The models are able to satisfy role constraints while maintaining good conversational abilities.- They release the first Korean dialogue dataset for this task, collected using their proposed framework.In summary, the main contribution is providing an end-to-end framework and analysis on how to build open-domain conversational agents that stick to predefined roles, using large language models and human-in-the-loop workflows for efficient data collection and model training. The released dataset also contributes to research on this task.
