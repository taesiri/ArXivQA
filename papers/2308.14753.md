# [Efficient Discovery and Effective Evaluation of Visual Perceptual   Similarity: A Benchmark and Beyond](https://arxiv.org/abs/2308.14753)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper focuses on the problem of visual similarity discovery (VSD) - retrieving images of different objects that are visually similar to a query image. - The authors argue that existing methods for VSD are often evaluated based on identification tasks, such as retrieving other images of the same object. They posit that this is a limited evaluation approach, and that faithful evaluation of VSD methods requires expert annotations of visual similarity.- The paper proposes an efficient methodology called EDS for collecting expert annotations of visual similarity at scale. EDS uses vision models to identify candidate similar pairs, which are then verified by experts. - Using EDS, the authors curate a new benchmark dataset of over 110K expert-annotated image pairs for evaluating VSD in the fashion domain. To the best of their knowledge, this is the first large-scale VSD benchmark in fashion.- The paper analyzes limitations and biases of EDS, and proposes evaluation metrics like ROC-AUC that aim to mitigate them and enable fair comparison of models.- Extensive experiments are conducted comparing various pretrained and finetuned models on the new benchmark. The limitations of supervised training for VSD are also analyzed.In summary, the central hypothesis is that faithful evaluation of VSD requires expert annotation of visual similarities. The key contribution is a large-scale expert-annotated fashion VSD benchmark, created efficiently using the proposed EDS approach, that enables proper evaluation of VSD models.


## What is the main contribution of this paper?

The main contribution of this paper seems to be the introduction of the first large-scale fashion visual similarity benchmark dataset, consisting of over 110,000 expert-annotated image pairs. Some key points:- The paper argues that evaluating visual similarity discovery (VSD) methods based on identification tasks is limited, and that faithful evaluation must rely on expert annotations.- To enable efficient collection of expert annotations, the authors propose a new method called Efficient Discovery of Similarities (EDS). Using EDS, they were able to label a large dataset of over 110K image pairs.- This benchmark dataset is presented as the first large-scale dataset for evaluating VSD models in the fashion domain. The authors hope it will expedite further research in VSD.- Besides introducing the dataset, the paper also proposes new metrics like ROC-AUC and PR-AUC to enable fair evaluation of models that were not involved in generating the ground truth labels.- The limitations of supervised training for VSD are also analyzed, suggesting that currently pretrained models perform better than supervised finetuning on this task.In summary, the main contribution is the large-scale benchmark dataset for evaluating visual similarity in fashion, enabled by a proposed efficient annotation method. The paper also provides analysis and insights into evaluating and training VSD models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces the first large-scale fashion visual similarity benchmark dataset consisting of over 110K expert-annotated image pairs, proposes an efficient labeling procedure and evaluation metrics to enable research in visual similarity discovery, and demonstrates limitations of using identification tasks as a proxy for evaluating visual similarity methods.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on visual perceptual similarity:- It focuses specifically on evaluating methods for visual similarity discovery (VSD) rather than just identification tasks. The authors argue that most prior work evaluates VSD methods by how well they identify different images of the same objects, which is limited. This paper stresses the need for datasets with true expert annotations of visual similarity.- The paper proposes a new efficient method (EDS) for collecting human expert annotations of visual similarity. This allows creating a large-scale benchmark dataset (over 110K image pairs) more feasibly compared to naive labeling approaches.- The authors examine limitations of their EDS dataset collection method, like bias towards the initial models used. They propose evaluation metrics like ROC-AUC that help mitigate these issues and enable fair comparison of other models. - The paper introduces the first large-scale expert-annotated visual similarity benchmark dataset in the fashion domain. Prior fashion retrieval datasets focus on identity, not visual similarity.- The benchmark dataset enables analyzing how well different baseline models, including pretrained and finetuned versions, can capture visual similarity rather than just identity. The paper shows supervised finetuning on identity actually hurts similarity performance.- The paper demonstrates that high performance on identification doesn't translate to good performance on visual similarity discovery. This highlights that they are distinct tasks requiring specialized methods and evaluation.Overall, this paper makes both methodological and empirical contributions for evaluating visual similarity in a more robust way. The proposed techniques could extend beyond fashion to other visual domains where similarity matters. The novel benchmark dataset enables more rigorous analysis of model performance on this challenging perceptual task.
