# [Efficient Discovery and Effective Evaluation of Visual Perceptual   Similarity: A Benchmark and Beyond](https://arxiv.org/abs/2308.14753)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper focuses on the problem of visual similarity discovery (VSD) - retrieving images of different objects that are visually similar to a query image. - The authors argue that existing methods for VSD are often evaluated based on identification tasks, such as retrieving other images of the same object. They posit that this is a limited evaluation approach, and that faithful evaluation of VSD methods requires expert annotations of visual similarity.- The paper proposes an efficient methodology called EDS for collecting expert annotations of visual similarity at scale. EDS uses vision models to identify candidate similar pairs, which are then verified by experts. - Using EDS, the authors curate a new benchmark dataset of over 110K expert-annotated image pairs for evaluating VSD in the fashion domain. To the best of their knowledge, this is the first large-scale VSD benchmark in fashion.- The paper analyzes limitations and biases of EDS, and proposes evaluation metrics like ROC-AUC that aim to mitigate them and enable fair comparison of models.- Extensive experiments are conducted comparing various pretrained and finetuned models on the new benchmark. The limitations of supervised training for VSD are also analyzed.In summary, the central hypothesis is that faithful evaluation of VSD requires expert annotation of visual similarities. The key contribution is a large-scale expert-annotated fashion VSD benchmark, created efficiently using the proposed EDS approach, that enables proper evaluation of VSD models.


## What is the main contribution of this paper?

The main contribution of this paper seems to be the introduction of the first large-scale fashion visual similarity benchmark dataset, consisting of over 110,000 expert-annotated image pairs. Some key points:- The paper argues that evaluating visual similarity discovery (VSD) methods based on identification tasks is limited, and that faithful evaluation must rely on expert annotations.- To enable efficient collection of expert annotations, the authors propose a new method called Efficient Discovery of Similarities (EDS). Using EDS, they were able to label a large dataset of over 110K image pairs.- This benchmark dataset is presented as the first large-scale dataset for evaluating VSD models in the fashion domain. The authors hope it will expedite further research in VSD.- Besides introducing the dataset, the paper also proposes new metrics like ROC-AUC and PR-AUC to enable fair evaluation of models that were not involved in generating the ground truth labels.- The limitations of supervised training for VSD are also analyzed, suggesting that currently pretrained models perform better than supervised finetuning on this task.In summary, the main contribution is the large-scale benchmark dataset for evaluating visual similarity in fashion, enabled by a proposed efficient annotation method. The paper also provides analysis and insights into evaluating and training VSD models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces the first large-scale fashion visual similarity benchmark dataset consisting of over 110K expert-annotated image pairs, proposes an efficient labeling procedure and evaluation metrics to enable research in visual similarity discovery, and demonstrates limitations of using identification tasks as a proxy for evaluating visual similarity methods.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on visual perceptual similarity:- It focuses specifically on evaluating methods for visual similarity discovery (VSD) rather than just identification tasks. The authors argue that most prior work evaluates VSD methods by how well they identify different images of the same objects, which is limited. This paper stresses the need for datasets with true expert annotations of visual similarity.- The paper proposes a new efficient method (EDS) for collecting human expert annotations of visual similarity. This allows creating a large-scale benchmark dataset (over 110K image pairs) more feasibly compared to naive labeling approaches.- The authors examine limitations of their EDS dataset collection method, like bias towards the initial models used. They propose evaluation metrics like ROC-AUC that help mitigate these issues and enable fair comparison of other models. - The paper introduces the first large-scale expert-annotated visual similarity benchmark dataset in the fashion domain. Prior fashion retrieval datasets focus on identity, not visual similarity.- The benchmark dataset enables analyzing how well different baseline models, including pretrained and finetuned versions, can capture visual similarity rather than just identity. The paper shows supervised finetuning on identity actually hurts similarity performance.- The paper demonstrates that high performance on identification doesn't translate to good performance on visual similarity discovery. This highlights that they are distinct tasks requiring specialized methods and evaluation.Overall, this paper makes both methodological and empirical contributions for evaluating visual similarity in a more robust way. The proposed techniques could extend beyond fashion to other visual domains where similarity matters. The novel benchmark dataset enables more rigorous analysis of model performance on this challenging perceptual task.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing training schemes to improve the level of similarity captured by visual similarity discovery methods. The authors found that finetuning with identity or category labels did not consistently improve performance on their benchmark. They suggest more research is needed to find supervised or self-supervised training objectives that can better capture visual similarity.- Applying the proposed efficient labeling method and evaluation metrics to other domains beyond fashion, such as natural language processing or audio analytics. The methodologies presented in the paper could potentially be used for efficiently collecting human annotations and evaluating similarity in other perceptual domains.- Mitigating the bias caused by the model-based sampling strategy used to select candidate pairs for annotation. The authors propose AUC metrics to help alleviate this, but suggest more work could be done, perhaps by combining model-based sampling with some random sampling.- Studying what visual features and metrics best align with human judgments of similarity. The paper focuses on benchmarking existing methods, but doesn't deeply analyze what makes something visually similar. Future work could try to gain insight into human perception.- Scaling up the benchmark dataset to cover more fashion categories, types of images, etc. The current dataset focuses on women's clothing, so expanding to more products could better support real-world applications.- Developing improved loss functions and model architectures specialized for visual similarity, rather than simply leveraging existing recognition models. The gap between identification and similarity highlights this area for improvement.In summary, the authors point to opportunities for better models, training methods, datasets, evaluation techniques, and human perception studies related to visual similarity discovery. Their work provides a foundation to support many exciting avenues for future research in this area.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces the first large-scale benchmark dataset for evaluating visual similarity discovery (VSD) models in the fashion domain. VSD is important for applications like search and recommendations but is challenging to evaluate due to the subjectivity of similarity. Most VSD methods are evaluated by proxy using identification tasks, but the authors argue this is limited and does not reflect true visual similarity. They propose a novel efficient procedure called EDS for collecting reliable similarity labels from fashion experts on over 110K image pairs. EDS uses vision models to identify candidate similar pairs for human review. The paper discusses limitations of biased evaluation sets and proposes robust AUC metrics to address them. Using the benchmark dataset, experiments compare various pretrained and supervised models on closed-catalog and wild image discovery. Key findings are that supervised finetuning underperforms pretrained models on similarity, demonstrating the difficulty of learning visual similarity from proxy tasks. The benchmark and analysis provide insights to advance further research on visual similarity and discovery.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper introduces the first large-scale benchmark dataset for evaluating visual similarity discovery (VSD) models in the fashion domain. VSD is the task of retrieving images of different objects that are visually similar given a query image. The authors argue that evaluating VSD methods based on identification tasks, which retrieve different images of the same object, is limited. Faithful evaluation of VSD requires expert annotations of visual similarity. However, collecting expert labels at scale is prohibitively expensive using naive labeling approaches. To address this, the authors develop an efficient labeling procedure called Efficient Discovery of Similarities (EDS) that uses vision models to find candidate pairs likely to be judged similar by experts. This allows focusing expensive expert effort on promising candidates only. Using EDS, the authors collect a benchmark of over 110K expert-labeled fashion image pairs. They propose area under ROC curve metrics that mitigate biases in evaluation. Extensive experiments validate the benchmark and show limitations of supervised training for VSD. The benchmark and insights on evaluation and training will facilitate future VSD research.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces the Efficient Discovery of Similarities (EDS) method for efficiently collecting feedback from human domain experts to label a large dataset of image pairs for evaluating visual similarity discovery (VSD) models. EDS utilizes a set of VSD models to generate a set of image pairs suspected to be visually similar. These suspects are reviewed by experts to generate ground truth labels. EDS assumes the models can serve as a proxy similarity measure, so their top-ranked images are likely to contain more true positives than random sampling. This allows efficient discovery of positives compared to brute force labeling of all pairs or random sampling. The resulting labeled dataset is biased towards the models used, so the paper proposes using AUC metrics like ROC-AUC that measure relative rank of positives vs negatives to enable fair evaluation of any model, even those not used to generate the labels. Overall, EDS and the proposed evaluation metrics allow efficient large-scale labeling and mitigation of biases, enabling creation of the first benchmark for evaluating VSD models in the fashion domain.
