# [WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search   Results with Citations](https://arxiv.org/abs/2403.01774)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Enhancing attribution in large language models (LLMs) is important for reducing risks like hallucination and improving credibility. One approach is enabling LLMs to cite external sources supporting their generations. 
- However, existing datasets and evaluation methods for training and assessing the attribution abilities of LLMs have limitations:
    - Most datasets lack high-quality citation annotations
    - Evaluation methods cannot thoroughly assess attribution - they don't differentiate between groundedness errors and citation errors, and cannot handle sentences drawing partial support from multiple sources

Proposed Solution:
- Present WebCiteS, a Chinese dataset with 7k human-annotated summaries with citations, derived from real-world user queries and web search results
    - Adopt a human-LLM collaborative annotation pipeline to ensure quality
- Propose a comprehensive evaluation framework that distinguishes groundedness and citation quality, and incorporates a claim-split model to detect partial support
- Develop an automatic evaluator aligned with human citations  

Main Contributions:
- Formulate the task of attributed query-focused summarization (AQFS)  
- Construct the WebCiteS dataset which offers real-world queries, search results as documents, and high-quality human annotations of summaries and citations
- Put forward detailed evaluation metrics and optimized automatic evaluator for assessing the attribution abilities of LLMs  
- Conduct comprehensive experiments on WebCiteS using both open-source and proprietary LLMs, revealing remaining challenges LLMs face in attribution

The key significance is developing a valuable benchmark to facilitate research towards enhancing and evaluating the attribution capacities of LLMs. Findings also showcase the difficulty LLMs have with accurate citations even if grounded, and declining performance as context length increases.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents WebCiteS, a new Chinese dataset for attributed query-focused summarization derived from real-world search queries and results, along with a comprehensive evaluation framework to assess both summarization utility and attribution quality in language models.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It formulates the task of attributed query-focused summarization (AQFS) which aims to generate a summary from documents retrieved for a query, with in-line citations to make the summary attributable. 

2. It presents WebCiteS, a high-quality Chinese dataset for the AQFS task, consisting of 7k samples derived from real-world user queries and search results. The dataset has human-annotated summaries and citations.

3. It proposes a comprehensive evaluation framework for AQFS that assesses both summarization utility and attribution quality. The framework includes automatic metrics to evaluate groundedness and citation accuracy at a more fine-grained level.

4. It provides an analysis of various open-source and proprietary language models on the AQFS task using WebCiteS, revealing the remaining challenges they face in contextual grounding and accurate citation. The analysis also shows the impact of longer context and more fine-grained documents on model performance.

In summary, the key contributions are: (1) task formulation, (2) new dataset, (3) evaluation framework, and (4) model analysis. The paper underscores the need for continued efforts to enhance attribution in large language models.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Attributed query-focused summarization (AQFS) - The task formulated in the paper where models generate a summary from documents retrieved for a query, with in-line citations to make the summary attributable.

- WebCiteS - The Chinese dataset presented in the paper, built from real-world user queries and search results. It has human-annotated summaries with citations.

- Groundedness - One aspect of evaluating attribution, referring to whether the generated text is contextually supported by the input documents.  

- Citation quality - The other aspect of evaluating attribution, regarding the accuracy and coverage of citations in making the generated text attributable.

- Claim-split model - A model introduced in the paper's evaluation framework to decompose sentences into sub-claims, enabling fine-grained verification of attribution. 

- Long-context setting - An experiment setting that provides models with full web page content, instead of just snippets, posing a harder summarization and attribution test.

- Few-shot prompting (FSP) - One of the methods used in the experiments to elicit summaries from language models, utilizing prompt instruction and demonstration. 

- Supervised fine-tuning (SFT) - The other experiment method that fine-tunes models on the AQFS dataset to optimize their performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new task formulation called "attributed query-focused summarization" (AQFS). Can you elaborate more on why this task formulation is useful compared to existing formulations for query-focused summarization? What are the key differences?

2. The paper uses a "human-LLM collaborative annotation pipeline" to create the WebCiteS dataset. Can you explain the motivation and advantages of this semi-automated approach compared to fully manual or fully automated methods? 

3. The paper argues that existing evaluation methods for attribution are insufficient. Can you discuss 2-3 specific limitations with current methods, and explain how the proposed comprehensive evaluation framework aims to address those limitations?

4. One key component proposed is the automatic claim-split model for extracting sub-claims from sentences. What considerations went into the training and selection of this model? What are some weaknesses or limitations of this approach?

5. The paper finds that groundedness errors and citation errors often co-exist across models. What might be some reasons that even models which ground sentences contextually still struggle with accurate citations?

6. Supervised fine-tuning is found to simultaneously improve both groundedness and citation quality. Can you discuss possible reasons why this joint improvement occurs with fine-tuning? 

7. Why does model performance degrade on full web page content compared to shorter snippets? What specific challenges arise from longer context that may impact attribution?

8. The paper tests impact of document granularity and finds it also reduces attribution performance. What factors related to more fine-grained documents make citation more difficult?  

9. Can you suggest some possible methods or directions to address the attribution errors observed, especially in the long context experiments? What are some promising ways forward?

10. One limitation mentioned is reliability issues with the automatic evaluator components. Can you suggest some future work to improve reliability of the claim-split model and NLI model used in the evaluator?
