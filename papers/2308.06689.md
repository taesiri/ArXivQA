# [Estimator Meets Equilibrium Perspective: A Rectified Straight Through   Estimator for Binary Neural Networks Training](https://arxiv.org/abs/2308.06689)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we better balance the tradeoff between estimating error and gradient stability in training binary neural networks? The key points are:- Binary neural networks (BNNs) aim to binarize weights and/or activations to 1 bit during training. This is typically done by using a straight-through estimator (STE) to approximate the gradient of the non-differentiable sign function. - However, STE causes an inconsistency between the forward pass (sign function) and backward pass (identity function), leading to estimating error. Other estimators have been proposed to reduce this error, but often cause unstable/divergent gradients.- The authors propose a new perspective - viewing BNN training as an equilibrium between estimating error and gradient stability. Reducing one can hurt the other. - They design indicators to quantitatively measure estimating error and gradient instability. This allows them to demonstrate the equilibrium phenomenon.- They propose a new estimator, Rectified STE (ReSTE), based on a power function. This is designed to flexibly balance estimating error and gradient stability.- Experiments show ReSTE achieves state-of-the-art performance on CIFAR-10 and ImageNet without any extra modules/losses. The indicators also demonstrate it can adjust the equilibrium.In summary, the central hypothesis is that explicitly considering the equilibrium between estimating error and gradient stability, via new metrics and a flexible estimator, can improve BNN training performance. The results seem to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new perspective on binary neural network (BNN) training, viewing it as an equilibrium between estimating error and gradient stability. - Designing two quantitative indicators to measure estimating error and gradient instability, in order to demonstrate the equilibrium phenomenon.- Proposing a new estimator called Rectified Straight Through Estimator (ReSTE) based on a power function, which is designed to flexibly balance estimating error and gradient stability.- Demonstrating through experiments on CIFAR-10 and ImageNet that ReSTE outperforms prior BNN training methods, without needing any auxiliary modules or losses.- Using the proposed indicators to show that ReSTE can adjust the degree of equilibrium and find a suitable balance between small estimating error and stable gradients.In summary, the key ideas are framing BNN training as an equilibrium problem, quantifying that via proposed indicators, and introducing ReSTE as a way to flexibly control the tradeoff. Experiments validate its effectiveness versus prior approaches.
