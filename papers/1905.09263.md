# [FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/abs/1905.09263)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we develop a text-to-speech (TTS) system that generates mel-spectrograms from text quickly and in parallel (non-autoregressively), while also improving the robustness and controllability of the synthesized speech output?

The key hypotheses/claims appear to be:

- Using a feed-forward network based on self-attention and 1D convolutions can generate mel-spectrograms in parallel, avoiding the slow sequential generation of autoregressive TTS models.

- Explicitly modeling alignments between phonemes and mel-spectrograms with a duration predictor can improve robustness by avoiding word skipping/repeating issues faced by autoregressive attention-based TTS models. 

- A length regulator module can control speed and prosody by expanding the phoneme sequence according to predicted durations.

So in summary, the central research direction seems to be developing a non-autoregressive TTS approach called FastSpeech that is faster, more robust, and more controllable than previous autoregressive neural TTS models. The core hypotheses are around the architectural design choices and modeling phoneme/mel alignment enabling these advantages over autoregressive models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FastSpeech, a parallel text to speech model that can generate mel-spectrograms from text much faster than autoregressive models while achieving similar speech quality. The key ideas are:

- Using a feed-forward network structure based on Transformer and convolution to generate mel-spectrograms in parallel. This greatly speeds up inference compared to autoregressive models like Tacotron 2. 

- Incorporating a duration predictor to expand the phoneme sequence to match the length of the target mel-spectrogram for parallel generation. The duration predictor ensures alignment between phonemes and mel-spectrograms.

- Adding a length regulator that can adjust phoneme durations to control voice speed and prosody. This makes the synthesized speech more controllable.

- Leveraging sequence-level knowledge distillation to improve speech quality by transferring knowledge from an autoregressive teacher model.

The results show FastSpeech can match the quality of Transformer TTS while speeding up mel-spectrogram generation by 270x and overall speech synthesis by 38x. It also largely eliminates word skipping/repeating issues and enables smooth voice speed control. So in summary, FastSpeech significantly improves speed, robustness and controllability compared to previous autoregressive TTS models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes FastSpeech, a fast, robust and controllable neural text-to-speech model that generates mel-spectrograms in parallel rather than autoregressively, achieving up to 270x faster mel-spectrogram generation and 38x faster end-to-end speech synthesis compared to autoregressive models, while nearly matching the voice quality and effectively eliminating issues like word skipping/repeating.
