# [FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/abs/1905.09263)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we develop a text-to-speech (TTS) system that generates mel-spectrograms from text quickly and in parallel (non-autoregressively), while also improving the robustness and controllability of the synthesized speech output?

The key hypotheses/claims appear to be:

- Using a feed-forward network based on self-attention and 1D convolutions can generate mel-spectrograms in parallel, avoiding the slow sequential generation of autoregressive TTS models.

- Explicitly modeling alignments between phonemes and mel-spectrograms with a duration predictor can improve robustness by avoiding word skipping/repeating issues faced by autoregressive attention-based TTS models. 

- A length regulator module can control speed and prosody by expanding the phoneme sequence according to predicted durations.

So in summary, the central research direction seems to be developing a non-autoregressive TTS approach called FastSpeech that is faster, more robust, and more controllable than previous autoregressive neural TTS models. The core hypotheses are around the architectural design choices and modeling phoneme/mel alignment enabling these advantages over autoregressive models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FastSpeech, a parallel text to speech model that can generate mel-spectrograms from text much faster than autoregressive models while achieving similar speech quality. The key ideas are:

- Using a feed-forward network structure based on Transformer and convolution to generate mel-spectrograms in parallel. This greatly speeds up inference compared to autoregressive models like Tacotron 2. 

- Incorporating a duration predictor to expand the phoneme sequence to match the length of the target mel-spectrogram for parallel generation. The duration predictor ensures alignment between phonemes and mel-spectrograms.

- Adding a length regulator that can adjust phoneme durations to control voice speed and prosody. This makes the synthesized speech more controllable.

- Leveraging sequence-level knowledge distillation to improve speech quality by transferring knowledge from an autoregressive teacher model.

The results show FastSpeech can match the quality of Transformer TTS while speeding up mel-spectrogram generation by 270x and overall speech synthesis by 38x. It also largely eliminates word skipping/repeating issues and enables smooth voice speed control. So in summary, FastSpeech significantly improves speed, robustness and controllability compared to previous autoregressive TTS models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes FastSpeech, a fast, robust and controllable neural text-to-speech model that generates mel-spectrograms in parallel rather than autoregressively, achieving up to 270x faster mel-spectrogram generation and 38x faster end-to-end speech synthesis compared to autoregressive models, while nearly matching the voice quality and effectively eliminating issues like word skipping/repeating.


## How does this paper compare to other research in the same field?

 Here are some comparisons of this paper to other research in text-to-speech synthesis:

- This paper proposes FastSpeech, a non-autoregressive model for parallel mel-spectrogram generation. Other recent work like Parallel WaveNet and ClariNet also generate audio in parallel, but still rely on autoregressive models for mel-spectrogram generation. FastSpeech is novel in entirely removing the autoregressive component.

- For parallel mel-spectrogram generation, this paper extracts alignments from an autoregressive teacher model. Another concurrent work Peng et al. 2019 generates mel-spectrograms using a non-autoregressive Transformer instead of extracting alignments. The FastSpeech approach seems simpler and achieves faster inference speedup.

- A key focus of FastSpeech is improving robustness and controllability compared to autoregressive models like Tacotron 2 and Transformer TTS. It shows advantages in avoiding word repetitions/skipping and enabling adjustible speech speed. Other work has not emphasized these aspects as much.

- FastSpeech adopts a streamlined feedforward Transformer architecture without encoder-decoder attention. Most other sequence-to-sequence TTS models use encoder-decoder structures with attention. The feedforward design likely contributes to FastSpeech's faster inference speed.

- This paper shows FastSpeech almost matches autoregressive Transformer TTS in terms of speech quality, while greatly improving speed. Other attempts at non-autoregressive TTS have struggled to match the quality of autoregressive models. The knowledge distillation approach here seems effective.

Overall, FastSpeech pushes parallel sequence generation into TTS instead of just the vocoder, while improving controllability and robustness. The alignments and feedforward architecture seem like the key innovations compared to prior art. This paper shows non-autoregressive TTS can match autoregressive quality if designed properly.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Continue to improve the quality of the synthesized speech produced by FastSpeech, especially for more complex datasets and multiple speakers. They mention trying to apply FastSpeech to multi-speaker and low-resource settings.

- Train FastSpeech jointly with a parallel neural vocoder like WaveGlow to make the system fully end-to-end and parallel. Right now they are using a separately trained WaveGlow model as the vocoder. Training the two models jointly could improve quality and sync between the mel spectrograms and final audio.

- Explore ways to improve prosody control beyond just inserting breaks between words. The length regulator allows some basic prosody modifications but more advanced control of pitch, emphasis, etc. could make the synthesized speech sound more natural.

- Apply FastSpeech to other sequence generation tasks like machine translation where parallel generation could also be beneficial. The model architecture may be adaptable to other domains.

- Continue to improve the alignment and duration prediction components that enable the parallel generation in FastSpeech. Better alignments and duration predictions could improve quality and training stability.

So in summary, the main directions relate to improving the speech quality, extending to joint training with vocoders and other tasks, and improving the underlying alignment and duration prediction modules that make FastSpeech work. The parallel generation approach seems promising for both speed and quality improvements in text-to-speech and related sequence generation tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a new model called FastSpeech for text-to-speech (TTS) synthesis. FastSpeech differs from prior neural TTS models in that it generates mel-spectrograms in parallel rather than autoregressively. It uses a feed-forward Transformer network consisting of self-attention and 1D convolution blocks. To handle the length mismatch between the phoneme and spectrogram sequences, the model includes a length regulator that expands the phoneme sequence based on predicted phoneme durations from a duration predictor module. Experiments show that FastSpeech achieves similar audio quality to autoregressive Transformer TTS, while speeding up mel spectrogram generation by 270x and overall speech synthesis by 38x. It also improves robustness by avoiding issues like word skipping/repeating. The model can adjust voice speed and prosody by modifying the phoneme durations. Key advantages are much faster inference speed, better robustness, and controllable synthesis compared to prior neural TTS techniques.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new neural text-to-speech (TTS) model called FastSpeech that can generate mel-spectrograms in parallel. Most existing neural TTS systems generate mel-spectrograms autoregressively, which leads to slow inference speed as well as issues with robustness and lack of control. FastSpeech consists of a feed-forward network based on Transformer self-attention and 1D convolutions. It includes a length regulator to upsample phoneme sequences to match mel-spectrogram length based on predicted phoneme duration. This allows FastSpeech to generate mel-spectrograms in parallel, greatly speeding up synthesis. Experiments on LJSpeech show FastSpeech matches autoregressive Transformer TTS in quality but with 270x faster mel-spectrogram generation and 38x faster end-to-end speech synthesis. FastSpeech also eliminates word skipping/repeating issues in difficult cases and enables smooth voice speed control.

In more detail, FastSpeech contains several main components. The feed-forward Transformer converts phonemes to mel-spectrograms in parallel using self-attention and 1D convolutions. The duration predictor uses a convolutional network to predict phoneme durations which are used by the length regulator to expand the phoneme sequence. Sequence-level knowledge distillation helps transfer knowledge from an autoregressive teacher model. On LJSpeech, FastSpeech achieves quality close to Transformer TTS but with much faster inference and better robustness. It can adjust voice speed from 0.5-1.5x smoothly and add breaks between words to control prosody. Overall, FastSpeech provides fast, robust, and controllable neural TTS compared to previous autoregressive approaches.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel feed-forward network called FastSpeech for parallel mel-spectrogram generation in text-to-speech (TTS). FastSpeech consists of feed-forward Transformer blocks, a length regulator, and a duration predictor. It first extracts monotonic attention alignments from an autoregressive teacher model to predict phoneme durations. The predicted durations are used by the length regulator to expand the phoneme sequence to match the length of the target mel-spectrogram for parallel generation. This allows FastSpeech to generate mel-spectrograms non-autoregressively in parallel, overcoming issues like slow inference speed, speech robustness, and controllability in previous autoregressive TTS models. Experiments show FastSpeech achieves 270x speedup in mel-spectrogram generation and 38x in end-to-end speech synthesis compared to Transformer TTS, while nearly matching its voice quality. It also eliminates word skipping/repeating and allows smooth control of voice speed.
