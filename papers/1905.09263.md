# [FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/abs/1905.09263)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we develop a text-to-speech (TTS) system that generates mel-spectrograms from text quickly and in parallel (non-autoregressively), while also improving the robustness and controllability of the synthesized speech output?

The key hypotheses/claims appear to be:

- Using a feed-forward network based on self-attention and 1D convolutions can generate mel-spectrograms in parallel, avoiding the slow sequential generation of autoregressive TTS models.

- Explicitly modeling alignments between phonemes and mel-spectrograms with a duration predictor can improve robustness by avoiding word skipping/repeating issues faced by autoregressive attention-based TTS models. 

- A length regulator module can control speed and prosody by expanding the phoneme sequence according to predicted durations.

So in summary, the central research direction seems to be developing a non-autoregressive TTS approach called FastSpeech that is faster, more robust, and more controllable than previous autoregressive neural TTS models. The core hypotheses are around the architectural design choices and modeling phoneme/mel alignment enabling these advantages over autoregressive models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing FastSpeech, a parallel text to speech model that can generate mel-spectrograms from text much faster than autoregressive models while achieving similar speech quality. The key ideas are:

- Using a feed-forward network structure based on Transformer and convolution to generate mel-spectrograms in parallel. This greatly speeds up inference compared to autoregressive models like Tacotron 2. 

- Incorporating a duration predictor to expand the phoneme sequence to match the length of the target mel-spectrogram for parallel generation. The duration predictor ensures alignment between phonemes and mel-spectrograms.

- Adding a length regulator that can adjust phoneme durations to control voice speed and prosody. This makes the synthesized speech more controllable.

- Leveraging sequence-level knowledge distillation to improve speech quality by transferring knowledge from an autoregressive teacher model.

The results show FastSpeech can match the quality of Transformer TTS while speeding up mel-spectrogram generation by 270x and overall speech synthesis by 38x. It also largely eliminates word skipping/repeating issues and enables smooth voice speed control. So in summary, FastSpeech significantly improves speed, robustness and controllability compared to previous autoregressive TTS models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes FastSpeech, a fast, robust and controllable neural text-to-speech model that generates mel-spectrograms in parallel rather than autoregressively, achieving up to 270x faster mel-spectrogram generation and 38x faster end-to-end speech synthesis compared to autoregressive models, while nearly matching the voice quality and effectively eliminating issues like word skipping/repeating.


## How does this paper compare to other research in the same field?

 Here are some comparisons of this paper to other research in text-to-speech synthesis:

- This paper proposes FastSpeech, a non-autoregressive model for parallel mel-spectrogram generation. Other recent work like Parallel WaveNet and ClariNet also generate audio in parallel, but still rely on autoregressive models for mel-spectrogram generation. FastSpeech is novel in entirely removing the autoregressive component.

- For parallel mel-spectrogram generation, this paper extracts alignments from an autoregressive teacher model. Another concurrent work Peng et al. 2019 generates mel-spectrograms using a non-autoregressive Transformer instead of extracting alignments. The FastSpeech approach seems simpler and achieves faster inference speedup.

- A key focus of FastSpeech is improving robustness and controllability compared to autoregressive models like Tacotron 2 and Transformer TTS. It shows advantages in avoiding word repetitions/skipping and enabling adjustible speech speed. Other work has not emphasized these aspects as much.

- FastSpeech adopts a streamlined feedforward Transformer architecture without encoder-decoder attention. Most other sequence-to-sequence TTS models use encoder-decoder structures with attention. The feedforward design likely contributes to FastSpeech's faster inference speed.

- This paper shows FastSpeech almost matches autoregressive Transformer TTS in terms of speech quality, while greatly improving speed. Other attempts at non-autoregressive TTS have struggled to match the quality of autoregressive models. The knowledge distillation approach here seems effective.

Overall, FastSpeech pushes parallel sequence generation into TTS instead of just the vocoder, while improving controllability and robustness. The alignments and feedforward architecture seem like the key innovations compared to prior art. This paper shows non-autoregressive TTS can match autoregressive quality if designed properly.
