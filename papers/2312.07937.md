# [BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics](https://arxiv.org/abs/2312.07937)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new method called BOTH2Hands for generating realistic two-hand motions from both text prompts describing desired hand gestures and implicit body dynamics. To enable research in this novel hybrid text-body to hands generation task, the authors contribute a large-scale multi-modal dataset called BOTH57M. This dataset contains accurate motion tracking data of 57.4 million frames capturing intricate hand and body motions of 8 hours of activities, along with rich textual annotations describing motions at both the body and detailed finger level. BOTH2Hands first warms up two separate diffusion models conditioning on body dynamics and text prompts respectively. It then blends the outputs using a cross-attention transformer to generate hand motions aligning with the multi-modal inputs. Experiments demonstrate BOTH2Hands generates more convincing hand motions than state-of-the-art methods. Tests also validate the richness of BOTH57M for two-hand generation compared to existing datasets. The released dataset, code and models will support further research directions in multi-modal controllable hand motion generation and analysis.
