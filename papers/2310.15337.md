# [Moral Foundations of Large Language Models](https://arxiv.org/abs/2310.15337)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions and hypotheses appear to be:

1) Do the moral foundations exhibited by large language models (LLMs) like GPT-3 and PaLM demonstrate a cultural and/or political bias? The hypothesis is that due to the attributes of the training data, LLMs may have acquired a consistent set of moral foundations constituting a particular cultural or political bias. 

2) Do LLMs remain consistent with their exhibited moral foundations across different conversation contexts? The hypothesis is that if the model shows high variability in its moral foundations depending on the prompt, then the judgments may be highly context-specific rather than displaying a consistent bias.

3) Can the moral reasoning of LLMs be reliably changed in predictable ways by deliberately crafting prompts? The hypothesis is that it is possible to condition LLMs to exhibit a particular moral stance or bias through targeted prompting.

4) Do different moral foundations lead to measurably different behaviors in downstream tasks? The hypothesis is that prompting an LLM to exhibit certain moral foundations will affect its behavior on unrelated downstream tasks.

In summary, the central questions focus on assessing whether LLMs display consistent moral biases, whether these can be deliberately manipulated, and whether biased moral foundations affect downstream task performance. The key hypothesis is that moral biases are present in LLMs, are consistent, can be altered, and influence unrelated behaviors.
