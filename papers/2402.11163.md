# [KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning   over Knowledge Graph](https://arxiv.org/abs/2402.11163)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have limited reasoning abilities for complex tasks like multi-hop reasoning over knowledge graphs (KGs). 
- Existing methods that combine LLMs and KGs have predefined interaction strategies that lack flexibility to handle complex reasoning tasks.
- Many existing methods rely on large proprietary LLMs, limiting wider adoption.

Proposed Solution - KG-Agent Framework
- Proposes an autonomous agent framework called KG-Agent that enables smaller 7B LLM to actively make decisions for reasoning over KGs.
- Integrates the LLM with a multifunctional toolbox, KG-based executor and knowledge memory.
- Develops an iteration mechanism for tool selection and memory update to support autonomous reasoning over KGs.  
- Leverages existing KGQA datasets to synthesize code-based instructions to fine-tune the base LLM.

Main Contributions:
- Extends LLM's capacity to handle structured KG data via multifunctional toolbox.
- Synthesizes code-based instruction data from KGQA datasets to fine-tune smaller 7B LLM.  
- Proposes autonomous iteration mechanism based on tool selection and memory update for reasoning over KGs.

Results:
- With only 10K instruction samples, KG-Agent outperforms competitive methods on in-domain KGQA datasets while using a much smaller LLM.
- Zero-shot KG-Agent outperforms full-data supervised models on out-of-domain open-domain QA datasets.
- Demonstrates strong performance on domain-specific KGs, indicating generalization ability.

In summary, the paper presents KG-Agent, a novel autonomous agent framework that enables complex reasoning over KGs by a relatively small 7B LLM via instruction tuning and iterative interaction with KG toolbox and knowledge memory.
