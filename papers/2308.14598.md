# [S-TREK: Sequential Translation and Rotation Equivariant Keypoints for   local feature extraction](https://arxiv.org/abs/2308.14598)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we design a deep learning-based local feature extractor that is robust to in-plane image rotations?The key hypothesis appears to be:By combining a rotation equivariant keypoint detector with a lightweight descriptor network trained in a "detect then describe" manner, we can obtain a local feature extractor that achieves state-of-the-art performance, especially on images with significant in-plane rotations.In particular, the paper proposes a new method called S-TREK that has the following key components:- A keypoint detector based on rotation equivariant convolutions, making the detections robust to in-plane rotations by design.- A training framework inspired by reinforcement learning to directly optimize repeatability of keypoints.- A sequential sampling strategy to extract probabilistic keypoints during training.- A separate lightweight descriptor network trained only on detected keypoints.The central hypothesis is that by designing the detector to be rotation equivariant, optimizing it directly for repeatability, and decoupling it from the descriptor learning, S-TREK can outperform prior feature extraction methods, especially in the presence of in-plane rotations. The experiments then aim to validate this hypothesis.In summary, the key research question is how to develop a feature extractor robust to in-plane rotations, with the central hypothesis being that the proposed S-TREK approach can achieve this. The paper aims to demonstrate and evaluate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new local feature extractor called S-TREK, which combines:- A deep keypoint detector that is translation and rotation equivariant, trained with a reinforcement learning inspired framework to directly maximize repeatability. - A lightweight deep network for extracting descriptors at the detected keypoints.The key aspects of S-TREK are:- The keypoint detector uses rotation equivariant convolutions to achieve robustness to in-plane rotations. It is trained with a novel sequential sampling procedure and reward formulation to maximize repeatability.- The descriptor network is trained in a "detect, then describe" manner after the detector is trained, focusing only on the detected keypoint locations.- The overall pipeline follows a detect, then describe paradigm with specialized networks for each task. This allows designing the keypoint detector with small receptive field and the descriptor with very large receptive field.- Extensive experiments show S-TREK achieves state-of-the-art performance in terms of repeatability under rotations and matching performance for pose estimation, especially for images with large in-plane rotation differences.In summary, the main contribution is proposing a novel local feature extractor with a rotation equivariant keypoint detector trained directly for repeatability and a detect-then-describe pipeline tailored for accurate feature matching and pose estimation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper introduces S-TREK, a new method for extracting repeatable and discriminative image features that combines a rotation equivariant keypoint detector trained with a reinforcement learning approach and a lightweight descriptor network.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of local feature extraction and matching:- The main contribution of this paper is proposing a new method (S-TREK) that combines a deep keypoint detector that is translation and rotation equivariant with a lightweight deep descriptor extractor. This is a novel approach compared to prior work.- For the keypoint detector, the use of rotation equivariant convolutions is similar to the REKD method. However, S-TREK trains the detector with a new sequential sampling procedure inspired by reinforcement learning which is different from prior training strategies like proxy losses on heatmaps.- For descriptor learning, S-TREK follows a "detect, then describe" approach using separate detector and descriptor networks. This is in contrast to recent joint detector-descriptor methods like SuperPoint, D2-Net, R2D2, etc. that share a backbone. Separating them allows optimizing the architectures.- The descriptor learning uses a standard triplet loss, similar to many prior learned descriptors (HardNet, SOSNet, etc). But a key difference is it trains on exact detected keypoints rather than dense sampling.- For evaluation, the paper shows S-TREK achieves state-of-the-art results for keypoint repeatability on HPatches and the Image Matching Benchmark, outperforming recent learned methods like SuperPoint, R2D2, DISK.- S-TREK also demonstrates particularly strong performance on rotated versions of standard benchmarks. This highlights the benefits of its rotation equivariance.Overall, I would summarize that S-TREK brings together some existing ideas like rotation equivariance and triplet loss descriptor learning, but combines them in a novel way and achieves improved results compared to prior art, especially for robustness to rotation. The reinforcement learning-based keypoint detector training also represents a new contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the future research directions suggested by the authors include:- Incorporating scale equivariance into the network architecture. The current S-TREK architecture is translation and rotation equivariant, but does not handle changes in scale. Adding scale equivariance could further reduce the number of learnable parameters and the amount of training data/computation required.- Exploring different training frameworks like meta-learning to improve generalization across datasets. The current training is done on a single dataset. Using meta-learning approaches could improve performance when applying the method to new datasets. - Investigating the integration of S-TREK features into full 3D reconstruction pipelines. The current evaluation focuses on repeatability metrics and sparse feature matching. Validating the impact on full SfM/SLAM systems could be an interesting direction.- Combining ideas from S-TREK with recent learning-based matching methods. For instance, using S-TREK keypoints with learned matching networks like SuperGlue could be a promising hybrid approach.- Extending S-TREK to video sequences and exploring keypoint tracking applications. The current method is designed for image pairs, but video tracking could be an interesting use case.In summary, the authors suggest directions like adding more equivariance, using meta-learning, integration into 3D pipelines, combining with learned matching, and extending to video to further improve upon their S-TREK method. The core ideas have potential for further development in future works.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper introduces S-TREK, a novel local feature extractor that combines a deep keypoint detector with a lightweight deep descriptor extractor. The keypoint detector uses rotation-equivariant convolutions to achieve robustness to in-plane rotations. It is trained with a reinforcement learning inspired approach to directly maximize repeatability. The descriptors are trained in a "detect, then describe" manner, only at locations where keypoints are detected. Experiments show S-TREK achieves state-of-the-art performance on multiple benchmarks, especially for cases with large in-plane rotations. The detector excels at repeatability while the full S-TREK pipeline provides accurate matching and camera pose estimation. The method's equivariance and training procedure allow it to reliably find stable, repeatable keypoints.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a novel local feature extractor method called S-TREK that combines a deep keypoint detector with a lightweight deep descriptor extractor. The keypoint detector uses rotation equivariant convolutional layers to make the detections robust to in-plane rotations. The detector is trained with a framework inspired by reinforcement learning to directly maximize repeatability. Keypoints are sampled sequentially from the detection heatmap to avoid sampling the same point multiple times. The descriptor extractor has a U-Net architecture and is trained separately after the detector using a triplet loss, evaluating the loss only on the detected keypoints.  The method is evaluated on benchmark datasets like HPatches and Image Matching Benchmark. Experiments show the detector achieves state-of-the-art repeatability scores, even under significant in-plane rotation. When combined with the learned lightweight descriptors, S-TREK provides features suitable for recovering accurate camera poses. The method often outperforms other state-of-the-art techniques, especially in cases with large in-plane rotations. The equivariant architecture and specialized training procedure allow S-TREK to find repeatable and distinctive keypoints.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new local feature extractor called S-TREK. The key contributions are:- It uses a deep keypoint detector network that is designed to be translation and rotation equivariant. This makes the detections robust to in-plane image rotations. - The detector is trained with a reinforcement learning inspired approach, where keypoints are sampled from the heatmap probabilistically. This allows optimizing a repeatability-based reward directly, overcoming the non-differentiability of the keypoint selection process.- A sequential sampling strategy is proposed that avoids repeatedly selecting the same keypoint and adapts the number of keypoints automatically based on the input image.- The descriptor network has a lightweight U-Net architecture and is trained separately after the detector, only at the keypoint locations identified by the already trained detector.- Experiments show S-TREK achieves state-of-the-art results in terms of repeatability on multiple benchmarks. It also provides accurate pose estimates, especially for images with large in-plane rotations where other methods struggle.In summary, the main contribution is a full local feature pipeline designed to handle rotation changes. This is achieved through equivariant convolutions in the detector and a novel reinforcement learning based training approach.


## What problem or question is the paper addressing?

 The paper is addressing the problem of designing robust local feature extractors for image matching. Specifically, it proposes a new method called S-TREK that improves performance on two fronts:1. Keypoint Detection: The paper proposes a new keypoint detector that is translation and rotation equivariant, making the detections robust to in-plane rotations. This is achieved using specialized rotation equivariant convolutional layers. 2. Keypoint Descriptors: The paper trains lightweight keypoint descriptors using a "detect then describe" approach, where descriptors are learned after detector training. This allows optimizing the two components separately.The key ideas/contributions can be summarized as:- Proposes S-TREK, a novel local feature extractor combining a rotation equivariant keypoint detector with a lightweight descriptor network.- Introduces a reinforcement learning inspired training framework to directly optimize repeatability of the keypoints. Uses a new sequential sampling strategy during training.- Evaluates performance extensively on standard benchmarks like HPatches and Image Matching Benchmark. Shows S-TREK achieves state-of-the-art results, especially under viewpoint/rotation changes.- Follows a "detect then describe" paradigm to learn the detector and descriptor separately. This allows designing specialized architectures for each task.So in summary, the paper focuses on improving local feature extraction, particularly keypoint detection and description, in order to achieve robust image matching across viewpoint changes. The proposed S-TREK method combines ideas like rotation equivariance, sequential sampling, and separate detector/descriptor training to advance the state-of-the-art.
