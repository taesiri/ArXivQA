# [Neural HMMs are all you need (for high-quality attention-free TTS)](https://arxiv.org/abs/2108.13320)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to combine classical HMM-based speech synthesis with modern neural sequence-to-sequence TTS to get the benefits of both approaches. Specifically, the authors propose replacing the attention mechanism in Tacotron 2 with a neural hidden Markov model in order to obtain a fully probabilistic model that learns monotonic alignments and avoids attention failures like skipping and babbling. The hypothesis is that this neural HMM approach can achieve naturalness on par with Tacotron 2 while being simpler, faster to train, more data efficient, and avoiding attention failures.The key ideas and contributions are:- Proposing to replace the Tacotron 2 attention with a neural HMM to get a fully probabilistic model with monotonic alignment.- Describing how to integrate neural network-based acoustic modeling with classical HMM concepts like flat start initialization, numerical stability, and deterministic output generation. - Achieving comparable quality to Tacotron 2 with a smaller and simpler model that learns faster, needs less data, and does not babble or skip phonemes.- Introducing duration modeling based on transition probabilities and median-based generation, allowing control of speaking rate.- Demonstrating state-of-the-art results for an HMM-based TTS system, on par with a strong neural baseline.So in summary, the main contribution is showing how neural HMMs can match or exceed the quality of attention-based models for TTS by combining classical and modern techniques. The experiments support the hypothesis that this approach provides notable advantages over standard sequence-to-sequence architectures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a neural text-to-speech (TTS) synthesis method based on hidden Markov models (HMMs) rather than attention. The key ideas are:- Replacing the attention mechanism in Tacotron 2 with a neural HMM, which enforces monotonic alignments and allows joint modelling of durations and acoustics.- Combining innovations from classical HMM-based TTS and modern neural TTS, such as using the encoder from Tacotron 2 and integrating autoregression into the HMM. - Showing that this HMM-based neural TTS approach achieves comparable naturalness to Tacotron 2 on the LJ Speech dataset, while learning faster, needing less data, and avoiding attention failures.- Demonstrating that the neural HMM allows easy control over speaking rate during synthesis.In summary, the paper proposes an attention-free, fully probabilistic neural TTS method based on neural HMMs. Experiments show it matches Tacotron 2 quality while providing advantages such as monotonic alignment, fast and low-data learning, avoidance of attention failures, and controllable speaking rate.
