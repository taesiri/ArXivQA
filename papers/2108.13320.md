# [Neural HMMs are all you need (for high-quality attention-free TTS)](https://arxiv.org/abs/2108.13320)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to combine classical HMM-based speech synthesis with modern neural sequence-to-sequence TTS to get the benefits of both approaches. Specifically, the authors propose replacing the attention mechanism in Tacotron 2 with a neural hidden Markov model in order to obtain a fully probabilistic model that learns monotonic alignments and avoids attention failures like skipping and babbling. The hypothesis is that this neural HMM approach can achieve naturalness on par with Tacotron 2 while being simpler, faster to train, more data efficient, and avoiding attention failures.The key ideas and contributions are:- Proposing to replace the Tacotron 2 attention with a neural HMM to get a fully probabilistic model with monotonic alignment.- Describing how to integrate neural network-based acoustic modeling with classical HMM concepts like flat start initialization, numerical stability, and deterministic output generation. - Achieving comparable quality to Tacotron 2 with a smaller and simpler model that learns faster, needs less data, and does not babble or skip phonemes.- Introducing duration modeling based on transition probabilities and median-based generation, allowing control of speaking rate.- Demonstrating state-of-the-art results for an HMM-based TTS system, on par with a strong neural baseline.So in summary, the main contribution is showing how neural HMMs can match or exceed the quality of attention-based models for TTS by combining classical and modern techniques. The experiments support the hypothesis that this approach provides notable advantages over standard sequence-to-sequence architectures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a neural text-to-speech (TTS) synthesis method based on hidden Markov models (HMMs) rather than attention. The key ideas are:- Replacing the attention mechanism in Tacotron 2 with a neural HMM, which enforces monotonic alignments and allows joint modelling of durations and acoustics.- Combining innovations from classical HMM-based TTS and modern neural TTS, such as using the encoder from Tacotron 2 and integrating autoregression into the HMM. - Showing that this HMM-based neural TTS approach achieves comparable naturalness to Tacotron 2 on the LJ Speech dataset, while learning faster, needing less data, and avoiding attention failures.- Demonstrating that the neural HMM allows easy control over speaking rate during synthesis.In summary, the paper proposes an attention-free, fully probabilistic neural TTS method based on neural HMMs. Experiments show it matches Tacotron 2 quality while providing advantages such as monotonic alignment, fast and low-data learning, avoidance of attention failures, and controllable speaking rate.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes combining neural seq2seq TTS with ideas from HMM-based SPSS to create a smaller, simpler system that learns faster, needs less data, is fully probabilistic, avoids attention failures, and achieves comparable or better quality.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other research in neural TTS:- This paper proposes using neural hidden Markov models (HMMs) instead of attention for sequence-to-sequence modeling in neural TTS. Most prior work in neural TTS uses attention, which can sometimes fail and cause babbling. Replacing attention with HMMs enforces monotonic alignment.- The proposed model is fully probabilistic and trained by maximizing the exact log likelihood using the forward algorithm. Many other neural TTS models only optimize a lower bound on the likelihood or use auxiliary losses.- The paper shows the neural HMM model matches the quality of Tacotron 2, while learning faster, needing less data, and being simpler. Most prior HMM or duration modeling approaches for neural TTS have not quite matched standard attentional models in quality.- The neural HMM allows easy control of speaking rate at synthesis time by modifying state duration thresholds. Other neural TTS models with explicit duration modeling don't always permit easy rate control.- The model combines insights from both classical statistical parametric speech synthesis and modern neural TTS. Many other proposals focus more narrowly on either extending HMMs with neural components or integrating explicit duration modeling into neural TTS.- Concurrent work has also proposed neural HMMs and semi-Markov models for TTS. However, this paper shows better results and a simpler exact likelihood training procedure compared to those methods.In summary, the key novelty is the proposed architecture and training process combining neural networks and HMMs to get an interpretable, controllable model matching state-of-the-art neural TTS quality. The results validate the potential of this probabilistic modeling approach.


## What future research directions do the authors suggest?

The authors suggest several potential directions for future work:- Using stronger network architectures, such as transformers. They note that transformers have shown good results in other neural sequence modeling tasks.- Adding an enhancement network after the neural HMM decoder, similar to the post-net in Tacotron 2. This could help further improve the quality and naturalness of the synthesized speech.- Exploring the use of more powerful output distribution families, such as normalizing flows, to replace the Gaussian outputs used in this work. This could enable higher-quality sampling from the models.- Combining neural HMMs with other powerful generative models like normalizing flows, either by using flows as the output distribution or as a probabilistic post-net. This could further improve quality.- Investigating the use of Viterbi training to reduce memory usage during training, although this would come at the expense of optimizing a lower bound on the log-likelihood rather than the exact value.- Overall, the authors suggest enhancements like more powerful architectures, separately trained post-processing networks, and integrating neural HMMs with other advances in deep generative modeling as promising future work to build on the neural HMM approach presented in this paper. The goal is to further improve the quality and flexibility of the resulting speech synthesis systems.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes combining classical HMM-based speech synthesis with modern neural sequence-to-sequence approaches to get the benefits of both paradigms. Specifically, the authors replace the attention mechanism in Tacotron 2 with a neural network-parameterized hidden Markov model (HMM) to obtain a fully probabilistic model that learns monotonic alignments and can be trained by maximizing the exact data likelihood. Compared to Tacotron 2, the proposed neural HMM approach is smaller, simpler, learns faster with less data, achieves comparable naturalness, does not suffer from attention failures, and allows easy control of speaking rate. The neural HMM system is implemented by modifying the Tacotron 2 architecture to remove dependencies violating the Markov property. Experiments on the LJ Speech dataset show the neural HMM system matches the naturalness of Tacotron 2 without the post-net while learning coherent speech 15x faster. The proposed combination of neural networks and HMMs brings together innovations from both classical and modern TTS to advance the state of the art.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel text-to-speech synthesis method that combines neural networks with hidden Markov models (HMMs). The key idea is to replace the attention mechanism in neural sequence-to-sequence models like Tacotron 2 with a hidden Markov model that acts as a probabilistic alignment model. The neural network parameterizes the HMM emission probabilities and transition probabilities. This results in a fully probabilistic model that learns monotonic alignments and avoids attention failures like skipping and repetition. Experiments compare the proposed neural HMM synthesis models to a Tacotron 2 baseline on the LJ Speech dataset. A two-state neural HMM model achieves comparable naturalness to Tacotron 2 in a listening test, while learning faster, needing less data, and having fewer parameters. The neural HMM model also allows easy control of speaking rate at synthesis time. Overall, the neural HMM approach combines the benefits of statistical parametric speech synthesis using HMMs and recent neural TTS methods to create an attention-free, probabilistic model that generates high-quality and robust speech output. The neural HMM architecture provides a promising new direction for sequence-to-sequence text-to-speech.
