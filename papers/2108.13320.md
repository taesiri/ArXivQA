# [Neural HMMs are all you need (for high-quality attention-free TTS)](https://arxiv.org/abs/2108.13320)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to combine classical HMM-based speech synthesis with modern neural sequence-to-sequence TTS to get the benefits of both approaches. Specifically, the authors propose replacing the attention mechanism in Tacotron 2 with a neural hidden Markov model in order to obtain a fully probabilistic model that learns monotonic alignments and avoids attention failures like skipping and babbling. The hypothesis is that this neural HMM approach can achieve naturalness on par with Tacotron 2 while being simpler, faster to train, more data efficient, and avoiding attention failures.The key ideas and contributions are:- Proposing to replace the Tacotron 2 attention with a neural HMM to get a fully probabilistic model with monotonic alignment.- Describing how to integrate neural network-based acoustic modeling with classical HMM concepts like flat start initialization, numerical stability, and deterministic output generation. - Achieving comparable quality to Tacotron 2 with a smaller and simpler model that learns faster, needs less data, and does not babble or skip phonemes.- Introducing duration modeling based on transition probabilities and median-based generation, allowing control of speaking rate.- Demonstrating state-of-the-art results for an HMM-based TTS system, on par with a strong neural baseline.So in summary, the main contribution is showing how neural HMMs can match or exceed the quality of attention-based models for TTS by combining classical and modern techniques. The experiments support the hypothesis that this approach provides notable advantages over standard sequence-to-sequence architectures.
