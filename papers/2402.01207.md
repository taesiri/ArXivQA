# [Efficient Causal Graph Discovery Using Large Language Models](https://arxiv.org/abs/2402.01207)

## Summarize the paper in one sentence.

 This paper proposes a novel framework for efficient full causal graph discovery using large language models with a breadth-first search approach that achieves linear query complexity and state-of-the-art performance.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel framework for full causal graph discovery that leverages large language models (LLMs) more efficiently than prior methods. Specifically:

- It uses a breadth-first search approach to query the LLM that reduces the number of queries from quadratic to linear in the number of variables, making it scalable to larger graphs. 

- It achieves state-of-the-art or competitive performance on three real-world causal graphs of varying sizes, including a very large 221 node graph that other methods failed on.

- It shows how observational data can easily be incorporated when available to further improve performance. 

- Compared to traditional statistical methods, it eliminates the need for observational data for training while still achieving strong performance.

- Compared to prior LLM-based methods that rely on pairwise queries, it is much more efficient and applicable to larger graphs.

So in summary, the main contribution is an LLM-based causal graph discovery framework that is more efficient, flexible, and effective than existing methods.


## What are the keywords or key terms associated with this paper?

 Based on my review, the key terms associated with this paper appear to be:

LLM, large language models, causality, machine learning, causal graph discovery, breadth-first search, efficiency, pairwise queries, observational data

The paper proposes a novel framework that leverages large language models (LLMs) for efficient full causal graph discovery. Key aspects include:

- Uses a breadth-first search approach to construct the causal graph which reduces the number of queries from quadratic to linear
- Incorporates observational data when available to improve performance 
- Achieves state-of-the-art results on real-world causal graphs of varying sizes
- Showcases the potential of LLMs for causal reasoning and causal graph discovery across domains

The key terms cover the main topics and contributions around using LLMs, specifically large language models, for efficient causal graph discovery.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using a breadth-first search approach for causal graph discovery with LLMs. Why is BFS more efficient than a pairwise querying approach? What is the time complexity of BFS versus pairwise queries? 

2. The framework has three main stages - initialization, expansion, and insertion. Walk through each of these stages in detail and explain their purpose. How do these tie together to construct the full causal graph?

3. The paper shows that incorporating observational statistics into the prompts can improve performance. Why might this be the case? What are some ways the statistics could provide additional signal to the LLM? How might the statistics also add noise or be unhelpful?

4. The experimental results showcase superior performance on the Neuropathic Pain dataset compared to baseline methods. Why do you think the other methods struggle on larger graphs? What specifically allows the proposed BFS approach to scale better?

5. The paper mentions using advanced prompting strategies like Tree of Thoughts. Explain this strategy and how it could potentially improve the performance of causal graph discovery. What are its limitations?

6. Besides Tree of Thoughts, propose two other advanced prompting strategies not mentioned in the paper that could further improve the querying and reasoning process. Explain your proposals. 

7. The performance of the method relies heavily on the quality of the underlying LLM. How might performance differ when using an LLM with less capable reasoning skills? Propose experiments to test this.

8. The method is evaluated only on three datasets. What other datasets could be used for more rigorous testing? What kinds of datasets might be especially challenging for this approach?

9. The paper focuses only on causal graph discovery. How could the ideas be extended to perform causal effect estimation or counterfactual prediction on the discovered graph?

10. The impact statement mentions potential positive impacts in medicine and biology. Propose three new applications in other domains along with experiments to demonstrate the value of the method in those domains.
