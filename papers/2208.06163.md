# [Dropout is NOT All You Need to Prevent Gradient Leakage](https://arxiv.org/abs/2208.06163)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be whether dropout can reliably protect against gradient leakage and reconstruction of private training data via iterative gradient inversion attacks in federated learning. The central hypothesis appears to be that while dropout may seem to provide protection by introducing stochasticity during training, it does not reliably prevent reconstruction of private data if the attacker can approximate the specific realization of the stochastic client model.In particular, the authors hypothesize that an attacker could jointly optimize the reconstructed data and dropout masks applied during training to approximate the client's model realization. This proposed "Dropout Inversion Attack" could bypass the protection seemingly offered by dropout.The paper then conducts a systematic evaluation to test this hypothesis and demonstrate that the proposed attack can successfully reconstruct private training data even when dropout is used, across various model architectures and datasets.In summary, the central question is whether dropout alone is sufficient to prevent gradient leakage, and the hypothesis is that it is not if the attacker can approximate the stochastic client model realization. The Dropout Inversion Attack is proposed and evaluated to test this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Systematically showing that the use of dropout seems to prevent gradient leakage from iterative gradient inversion attacks on neural networks. Prior work had suggested dropout could provide some protection, but this had not been thoroughly analyzed.- Formulating a new "Dropout Inversion Attack" (DIA) that can successfully reconstruct client training data from gradients of dropout-protected models. This attack jointly optimizes for the client data and dropout masks used during training.- Performing an extensive evaluation of the proposed DIA attack on different model architectures (MLPs, CNNs, vision transformers) and image datasets. The experiments demonstrate that DIA can effectively bypass the protection seemingly provided by dropout.- Arguing that changes to model architectures alone like dropout cannot be assumed to reliably protect models from gradient leakage. The paper recommends combining dropout with other complementary defense mechanisms.In summary, the key contribution is proposing and evaluating the DIA attack, which reveals the vulnerability of using dropout alone as a defense against gradient inversion attacks for federated learning. The experiments systematically demonstrate this vulnerability across models and datasets.
