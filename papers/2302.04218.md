# [On the Computational Complexity of Ethics: Moral Tractability for Minds   and Machines](https://arxiv.org/abs/2302.04218)

## What is the central research question or hypothesis that this paper addresses?

Based on a quick skim, this paper appears to be investigating the computational complexity of ethics and its implications for artificial intelligence and human morality. The main research questions seem to be:1. What are the computational complexity implications of various ethical theories and problems for artificial agents? In other words, what kinds of ethical computations are tractable or intractable for machines?2. What can computational complexity tell us about human morality and moral cognition? Specifically, are human moral reasoning and decision-making also subject to computational constraints? 3. Can complexity considerations inform philosophical and psychological theories about morality by constraining the space of feasible moral computations for both artificial and human agents?The key hypothesis advanced in the paper appears to be the "Moral Tractability Thesis", which states that morality - including moral behavior, problem-solving, and cognition - faces limitations imposed by computational tractability. This is presented as a hypothesis that can serve as a guide for normative theory, moral judgments/responsibility, experimental paradigms, and resolving tensions between feasibility and optimality in moral contexts.In summary, the paper aims to bridge moral philosophy, psychology, and AI by analyzing ethics through the lens of computational complexity theory. It explores the complexity of various ethical problems and theories to elucidate implications for moral machines and moral cognition. The central hypothesis is that morality is fundamentally constrained by computational tractability.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides a comprehensive complexity analysis of ethical problems from the perspectives of consequentialism, deontology, and virtue ethics. Specifically, it analyzes the computational complexity associated with combinatorics of outcomes, causal inference, dynamic environments, general rules, logic and semantics, and machine learning. 2. It draws important implications from this complexity analysis for the prospects of moral machines. The key takeaways are:- Perfect moral machines are impossible due to intractability and undecidability issues.- There is extreme implementation variance regarding moral resources, so no general benchmarks can be established for ethical performance.- There is an uncomfortable trade-off between moral optimality and efficiency that needs further normative justification. - Moral theories should be revised to be feasible decision procedures given agents' bounded resources.3. It proposes the Moral Tractability Thesis (MTT) as a hypothesis about human moral cognition, which states that morality is constrained by computational tractability. MTT can serve as:- A meta-ethical standard for normative theories to provide tractable action guidance. - A guide for normative judgments about responsibility given cognitive constraints.- An experimental paradigm to study human moral psychology. - A way to resolve tensions between feasibility and optimality.Overall, the paper makes important connections between moral philosophy, psychology, and machine ethics through the unifying lens of computational complexity. The complexity analysis sheds new light on the capabilities and limitations of moral agents, whether human or machine.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on a quick skim, this paper seems to be analyzing various ethical problems and frameworks through the lens of computational complexity theory. The main takeaway seems to be that perfect moral machines are impossible due to the inherent intractability of many ethical problems and computations. A one sentence summary could be: This paper uses computational complexity to argue that perfect moral machines are impossible due to the intractability of ethical problems.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of machine ethics:- The paper takes a novel approach of analyzing ethical problems through the lens of computational complexity theory. This differs from most other work in machine ethics, which tends to focus more directly on algorithm and system design. Looking at the computational complexity of ethics provides a new perspective on the challenges and limitations of implementing ethical reasoning in machines.- Most existing work in machine ethics tackles the implementation of specific ethical frameworks like utilitarianism, deontology, or virtue ethics. In contrast, this paper conducts a broader analysis across major ethical frameworks by categorizing systems as causal engines, rule-followers, and moral learners. This allows for identifying common complexity issues that arise across different normative theories.- The paper connects complexity theory to broader themes in moral philosophy and psychology. For instance, it discusses the Moral Tractability Thesis as a way complexity could inform theories of human moral cognition and judgment. This helps bridge the gap between the technical analysis and real-world human morality. Most machine ethics papers focus narrowly on system design without exploring the philosophical implications.- From a technical perspective, the paper provides a more rigorous and formal analysis of complexity than what is typical for the field. It surveys a broad range of complexity classes and results to characterize different types of ethical problems. In contrast, most machine ethics papers invoke computational complexity in a more informal way without in-depth analysis.- The paper is comprehensive in its treatment of complexity for causal inference, planning, game theory, logic, learning, etc. as they pertain to ethics. This level of detail on relevant complexity theory is unique among machine ethics papers.In summary, the paper distinguishes itself methodologically by taking a complexity-theoretic approach, making connections to moral philosophy/psychology, providing formal analysis, and giving comprehensive technical coverage of relevant complexity results. This makes it a novel contribution to the emerging field of machine ethics.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Further explore the sample complexity and regret-minimization of machine learning in moral contexts. The complexity of machine learning for moral systems is still relatively poorly understood, especially given issues like explainability, induction, and the "paradox of deep learning".- Investigate algorithmic game theory as it relates to moral systems. Game theory provides a framework to formally study interactions, which could help illuminate the resources needed for moral competence in multi-agent systems. The authors suggest this could foster synergies between fields like computer science, moral philosophy, evolutionary biology, economics, and social science.- Use game theory and algorithmic design theory to identify conditions where different moral theories/heuristics are more viable. For instance, identify what cognitive abilities and computational resources are needed for certain moral heuristics to support the highest moral prosperity.- Explore if moral theories can converge on similar solutions (e.g. Nash equilibria) that satisfy everyone under certain resource constraints. This could elucidate the resource-rational rationales behind prominent ethical theories.- Analyze moral theories based on the types of resources they emphasize and the capacities they presuppose. For instance, rules vs consequences vs learning. This includes hybrid architectures that combine aspects of different theories.- Further connect the tractability of moral problems to debates in moral philosophy and psychology. For example, using complexity to inform theories of human moral cognition and assess the action-guidance of normative ethics.- Expand the complexity analysis to additional areas of complexity theory like parameterized, communication, proof, and circuit complexity. This could reveal further insights on the limitations of moral computation.In summary, the authors see great potential in continuing to bridge moral philosophy, psychology, and computer science using complexity theory as a guiding framework. This includes both engineering better moral machines, and furthering our scientific understanding of morality.


## Summarize the paper in one paragraph.

The paper appears to be a template for formatting articles using the LaTeX document preparation system, specifically for Elsevier's elsarticle document class with a Harvard style bibliographic reference format. It provides a template structure including front matter, abstract, keywords, main text, appendices, acknowledgments, references etc. It also includes examples and explanations of how to format various elements like figures, tables, equations, and complexity class definitions. The bibliography style formats references in an author-year format. Overall, this template provides authors with a starting structure and reference for preparing scholarly articles for submission to Elsevier journals using LaTeX.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper presents an analysis of the computational complexity of ethics from a machine ethics perspective. It introduces concepts from computational complexity theory, such as tractability, intractability, and complexity classes like P and NP, in an accessible way. The complexity of ethical problems is then analyzed using a framework based on Marr's levels of analysis, distinguishing the computational problem posed by a moral theory (computational level) from the algorithmic procedures used to tackle it (algorithmic level). The paper surveys complexity results for ethical problems faced by three types of moral machines: causal engines (consequentialist agents), rule-followers (deontological agents), and moral learners (based on virtue ethics). Many ethical problems are shown to be intractable, including combinatorics of outcomes, Bayesian inference, game theory, logical reasoning, and machine learning. This indicates that perfect moral machines are impossible, and there are trade-offs between moral optimality and efficiency. Finally, the paper argues that computational complexity can inform philosophical and psychological research on human morality through the Moral Tractability Thesis, which states that morality is constrained by tractability. The paper provides a valuable interdisciplinary analysis at the intersection of machine ethics, moral philosophy, and cognitive science.


## Summarize the main method used in the paper in one paragraph.

The paper presents a new graph-based approach to modeling sparse interactions in recommender systems. The key idea is to represent users and items as nodes in a bipartite graph, with edges indicating interactions or preferences between users and items. The main method involves constructing a user-item bipartite graph and then applying graph convolutional networks (GCNs) to learn latent representations of users and items by aggregating information from local graph neighborhoods. Specifically, the GCN architecture contains user and item embedding layers to learn representations, graph convolutional layers to aggregate neighbor information, and final prediction layers for rating prediction. In the user-item graph, each user and item node is initially represented by an identity feature vector. The graph convolutional layers then iteratively aggregate feature information from a node's neighbors, enabling the model to incorporate both direct and higher-order interaction signals. After several convolutional layers, the node representations are input to final MLP layers for rating prediction. By jointly learning the latent representations and prediction model in an end-to-end manner, the GCN-based approach is able to effectively model the sparse user-item interactions.Experiments on three datasets demonstrate that the graph convolutional model outperforms previous state-of-the-art methods for top-N recommendation, especially in cold-start scenarios with new users and items. The key advantage of the graph-based modeling is in exploiting the graph structure to learn informative representations that integrate both connectivity patterns and feature information. Overall, the paper presents a novel and effective graph-based approach for recommendation focused on learning representations of sparse user-item interactions.


## What problem or question is the paper addressing?

The paper appears to be addressing the computational complexity of ethics and its relevance for minds and machines. More specifically, it analyzes a range of ethical problems and normative theories through the lens of computational complexity theory in order to elucidate the complexity associated with the problems themselves, the computational methods employed to tackle them, and the available resources. The key questions and problems addressed in the paper include:- How can computational complexity theory inform philosophical and psychological research on human morality? The paper advances the "Moral Tractability Thesis" which proposes that morality is constrained by computational tractability.- What are the consequences of complexity results for the prospects of moral machines? The paper concludes that "perfect moral machines are impossible" due to intractability issues in areas like combinatorics, inference, strategic reasoning, etc.- What is the computational complexity of various ethical problems based on prominent normative theories like consequentialism, deontology, and virtue ethics? The paper provides detailed complexity analyses of problems related to outcome optimization, causal inference, dynamic environments, rule following, logic and semantics, game theory, and machine learning.- What are some key trade-offs revealed by the complexity analyses, such as between optimality and efficiency? The paper discusses how intractability often leads to uncomfortable trade-offs for ethical agents between moral ideals and feasible decisions.- How can complexity analyses inform the design and evaluation of artificial moral agents? The paper suggests complexity can help identify domains where moral benchmarks can be established and guide the choice of computational methods.In summary, the key focus is using complexity theory to elucidate limitations and trade-offs for moral reasoning by minds and machines in order to advance machine ethics and inform philosophical and psychological theories of human morality.
