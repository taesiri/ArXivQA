# [The Curious Robot: Learning Visual Representations via Physical   Interactions](https://arxiv.org/abs/1604.01360)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can physical interactions with the world provide useful supervisory signals to train visual representations, comparable or superior to using just passive image observations? The key hypothesis is that by having a robot perform various physical interactions and tasks involving objects, such as grasping, pushing, and tactile sensing, the data collected from those interactions can be used to train a convolutional neural network (ConvNet) to learn good visual representations. The authors argue that physical interaction is important for visual representation learning in biological agents, but most current computer vision systems just passively observe images/videos from the web rather than interacting with the world. This paper explores whether physical interaction data can also teach ConvNets useful visual representations.To test this, they collect a dataset of over 130K datapoints from a Baxter robot interacting with objects via grasping, pushing, tactile sensing, and observing different viewpoints. This data is used to train a shared ConvNet architecture. The quality of the learned visual representations is then evaluated by analyzing neuron activations, nearest neighbor retrieval, image classification, and instance retrieval. The results suggest that the physical interaction data does provide a useful supervisory signal for training visual representations.
