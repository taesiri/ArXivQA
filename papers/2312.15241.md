# [Measuring Value Alignment](https://arxiv.org/abs/2312.15241)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- As AI systems are deployed in critical domains like healthcare and transportation, ensuring they align with human values is extremely important. 
- However, defining and quantifying "value alignment" remains an open challenge. 

Proposed Solution:
- The paper introduces a novel formalism to quantify the degree of alignment between AI systems and human values. 
- The formalism models AI decision-making as a Markov Decision Process (MDP).
- Human values are formalized as "desirable goals" that associate preferences with states. Norms represent rules that govern behavior.
- A new metric called "degree of alignment" is proposed to measure how much applying a norm changes preferences over state transitions. This captures how well the norm aligns with human values.

Main Contributions:
- Provides precise definitions of values, norms, preferences and transitions based on MDPs.
- Defines a mathematical equation for calculating degree of alignment between a norm and value.
- Enables comparing different norms based on their quantitative alignment with human values.
- Allows analyzing tradeoffs between competing values imposed by different norms.
- Offers a way to move from informal notions of value alignment to a technical framework.

The formalism makes simplifying assumptions of modeling the world as an MDP and deterministic state transitions. Limitations around computational complexity and accounting for uncertainty are discussed. Overall, this work provides an initial foundation to reason rigorously about value alignment in AI systems.
