# [DeepCalliFont: Few-shot Chinese Calligraphy Font Synthesis by   Integrating Dual-modality Generative Models](https://arxiv.org/abs/2312.10314)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Few-shot font generation, especially for Chinese calligraphy fonts, is challenging. Existing methods utilize prior knowledge of Chinese characters, such as consistent components and stroke order across different fonts. However, calligraphy fonts often do not satisfy such consistency assumptions due to connected strokes and stroke order changes. Therefore, existing methods fail to synthesize high-quality calligraphy fonts.

Proposed Solution:
This paper proposes DeepCalliFont, a dual-modality generative model that integrates glyph images and writing trajectories for few-shot Chinese calligraphy font synthesis. The key ideas are:

1) Use writing trajectories instead of prior knowledge about components and stroke order, since trajectories have better consistency with glyphs, even for calligraphy styles. 

2) Adopt a coordinated representation to map image and sequence features into the same space via distillation and restoration.

3) Ensure modality consistency via an Image Feature Recombination (IFR) module and differentiable rasterization loss. IFR recombines image features under the guidance of sequence features. Rasterization loss constrains the generated trajectories to match the glyph images.  

4) Compensate for lack of dual-modality data by pre-training the image and sequence branches separately using abundant uni-modality data.

Main Contributions:

1) Analyze the conflicts between Chinese character prior knowledge and calligraphy font aesthetic designs.

2) Propose DeepCalliFont integrating dual-modality generative models with feature recombination and differentiable rasterization loss for consistent and high-quality calligraphy font synthesis.

3) Design a pre-training strategy to exploit available uni-modality data.  

4) Experiments show DeepCalliFont synthesizes higher-quality calligraphy fonts than state-of-the-arts. It also performs competitively on regular fonts.

In summary, this paper tackles the challenging problem of few-shot calligraphy font synthesis by effectively integrating dual modality generative models - a novel and promising direction for font generation.
