# [Improving Passage Retrieval with Zero-Shot Question Generation](https://arxiv.org/abs/2204.07496)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:How can we improve passage retrieval for open-domain question answering in an unsupervised manner using pre-trained language models?Specifically, the authors propose an unsupervised passage re-ranking method called UPR (Unsupervised Passage Re-ranking) that leverages pre-trained language models to re-score retrieved passages by estimating the likelihood of generating the question from the passage. The goal is to boost the ranking of relevant passages without requiring any labeled data or task-specific tuning.The key hypotheses tested in the paper through experiments are:- UPR can effectively improve retrieval accuracy over strong unsupervised and supervised baseline retriever models across several QA datasets.- UPR provides consistent gains when applied on top of different types of retrievers like sparse keyword-based methods or dense methods.- The performance improvements from UPR transfer to end-to-end open-domain QA, where using re-ranked passages during inference directly improves answer generation accuracy.- UPR works well even for keyword-centric datasets where dense retrievers tend to struggle.So in summary, the paper focuses on investigating unsupervised passage re-ranking for open-domain QA using pre-trained language models, with the goal of improving retrieval and end-task performance without needing any labeled data.


## What is the main contribution of this paper?

This paper proposes a simple and effective re-ranking method for improving passage retrieval in open-domain question answering. The key contribution is an unsupervised passage re-ranker named UPR (Unsupervised Passage Re-ranker) which re-scores retrieved passages using a pre-trained language model to estimate the probability of generating the input question conditioned on each passage. Some of the main benefits and results of UPR highlighted in the paper are:- It can be applied on top of any retrieval method like neural or keyword-based without needing any task-specific training data or fine-tuning. This makes it very generalizable.- It provides rich cross-attention between query and passage tokens which helps better estimate relevance than just using dense retrievers. - When re-ranking passages from unsupervised retrievers like Contriever and BM25, UPR provides gains of 6-18% absolute in top-20 retrieval accuracy across several QA datasets.- UPR helps improve supervised dense retrievers like DPR by up to 12% in top-20 accuracy.- A fully unsupervised pipeline of retriever + UPR outperforms supervised models like DPR, showing the strength of this approach.- Using re-ranked passages with a pretrained reader leads to SOTA results on open-domain QA with gains of up to 3 EM points, without needing to retrain reader or do end-to-end training.So in summary, the key contribution is presenting UPR, an unsupervised and task-agnostic passage re-ranker that provides significant gains over strong baseline retrievers and achieves new SOTA results when integrated into an open-domain QA pipeline.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper proposes an unsupervised passage reranking method using pretrained language models for open-domain question answering that improves retrieval accuracy and end task performance without requiring finetuning.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- The key contribution is proposing an unsupervised passage re-ranking approach called UPR for improving open-domain question answering. Most prior work on re-ranking uses some supervision, either via fine-tuning on relevance labels or joint training with reader models. In contrast, UPR is fully unsupervised.- The idea of scoring passages by the likelihood of generating the question is inspired by past work on using query likelihood with count-based language models. However, UPR uses a pre-trained transformer which allows for more expressive modeling.- Recent work like REALM and FiD also leverage pre-trained language models for retrieval and reading comprehension. But they require end-to-end training on question-answering data. UPR avoids any finetuning and can be applied on top of any retriever.- Compared to pure sparse retrievers like BM25 or dense retrievers like DPR, UPR incorporates token-level cross-attention between questions and passages. So it provides a middle ground between these two extremes.- The overall pipeline of retriever + UPR re-ranker + reader is similar to recent open-domain QA systems. The novelty is showing the effectiveness of adding an unsupervised re-ranker in this framework.- While UPR uses a standard pre-trained language model, the results highlight that scaling up model size and using instruction tuning (T0 model) improves re-ranking accuracy, similar to findings in other generative tasks.- The consistently strong results across diverse datasets and retrievers demonstrate the general usefulness of UPR for improving existing systems, without needing in-domain data.In summary, the paper makes a simple but impactful contribution of a zero-shot re-ranker that outperforms past supervised approaches. The unsupervised nature and model-agnostic design are advantageous compared to prior work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some future research directions the authors suggest:- Applying UPR to other retrieval tasks such as improving source-code retrieval based on textual queries. This could help explore the generalization capabilities of the approach.- Tuning the instruction prompts used in UPR according to the nature of the retrieval task. For example, using different prompts for retrieving similar sentences versus question-answering passages. This could help improve performance.- Experimenting with larger instruction-tuned language models as the re-ranker. As models continue to scale up, their performance as re-rankers may also improve. - Making UPR more scalable by using model distillation to transfer re-ranking abilities to dual encoders. This could help improve computational efficiency during inference.- Evaluating the benefits of using domain-specific language models that are finetuned on in-domain text. This could provide further gains for domain-specific retrieval tasks.- Comparing UPR against other methods like supervised re-rankers finetuned with relevance annotations. This analysis could reveal relative strengths and weaknesses.- Validating UPR on a wider range of retrieval benchmarks and tasks to better understand where it works well or struggles.Overall, the paper suggests several interesting future work directions around scaling up the approach, improving prompt engineering, evaluating on more tasks, and doing in-depth comparative analysis. Testing UPR's limits through rigorous experimentation seems like a key next step.
