# [CBA: Improving Online Continual Learning via Continual Bias Adaptor](https://arxiv.org/abs/2308.06925)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a method called Continual Bias Adaptor (CBA) to address the problem of catastrophic forgetting in online continual learning. The key research questions/hypotheses addressed in this paper are:- How to deal with the catastrophic distribution shifts caused by non-stationary data streams in online continual learning? The paper argues that both the label distribution and feature distribution can shift catastrophically over time, leading to severe forgetting of past knowledge. - Can we model and adapt to the posterior distribution shift directly to alleviate catastrophic forgetting? The paper hypothesizes that explicitly capturing the posterior distribution changes can help consolidate knowledge learned from previous tasks.- Can a lightweight module be designed to assimilate the posterior distribution shift during training, and removed during testing for efficiency? The paper proposes a Continual Bias Adaptor (CBA) module to dynamically augment the classifier to adapt to distribution changes, without overhead during testing.- Can CBA effectively improve existing rehearsal-based methods for online continual learning? The paper demonstrates consistent gains by applying CBA to various rehearsal-based methods like experience replay.- How does bi-level optimization of CBA aid in learning a stable classifier and alleviating catastrophic shifts? Theoretical analysis provides insights into how CBA's training strategy enables gradient alignment to prevent catastrophic forgetting.In summary, the key hypothesis is that explicitly modeling and adapting to posterior distribution shifts can help mitigate catastrophic forgetting in online continual learning. The CBA module and associated training procedure are proposed to achieve this goal in an efficient and robust manner.


## What is the main contribution of this paper?

This paper proposes a new approach called Continual Bias Adaptor (CBA) for online continual learning. The key contributions are:- It proposes a bi-level learning framework to model the posterior distribution shift in an online manner during continual learning. This allows adapting to catastrophic distribution changes and consolidating knowledge from previous tasks. - It introduces a CBA module that can plug into most rehearsal-based methods during training and be removed at test time, introducing no extra computation cost or memory overhead.- It provides theoretical analysis to show CBA establishes gradient alignment between training and memory buffer data, explaining why it alleviates catastrophic forgetting.- It demonstrates strong empirical performance by applying CBA to 4 rehearsal baselines on 3 benchmarks, consistently improving their accuracy and forgetting measures under online continual learning settings.In summary, the main contribution is proposing the CBA approach to explicitly model and adapt to distribution shifts in online continual learning, both theoretically and empirically showing it helps consolidate knowledge across tasks. The bi-level learning and removable module allow flexibility to apply CBA to existing rehearsal methods.
