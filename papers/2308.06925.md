# [CBA: Improving Online Continual Learning via Continual Bias Adaptor](https://arxiv.org/abs/2308.06925)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a method called Continual Bias Adaptor (CBA) to address the problem of catastrophic forgetting in online continual learning. The key research questions/hypotheses addressed in this paper are:- How to deal with the catastrophic distribution shifts caused by non-stationary data streams in online continual learning? The paper argues that both the label distribution and feature distribution can shift catastrophically over time, leading to severe forgetting of past knowledge. - Can we model and adapt to the posterior distribution shift directly to alleviate catastrophic forgetting? The paper hypothesizes that explicitly capturing the posterior distribution changes can help consolidate knowledge learned from previous tasks.- Can a lightweight module be designed to assimilate the posterior distribution shift during training, and removed during testing for efficiency? The paper proposes a Continual Bias Adaptor (CBA) module to dynamically augment the classifier to adapt to distribution changes, without overhead during testing.- Can CBA effectively improve existing rehearsal-based methods for online continual learning? The paper demonstrates consistent gains by applying CBA to various rehearsal-based methods like experience replay.- How does bi-level optimization of CBA aid in learning a stable classifier and alleviating catastrophic shifts? Theoretical analysis provides insights into how CBA's training strategy enables gradient alignment to prevent catastrophic forgetting.In summary, the key hypothesis is that explicitly modeling and adapting to posterior distribution shifts can help mitigate catastrophic forgetting in online continual learning. The CBA module and associated training procedure are proposed to achieve this goal in an efficient and robust manner.


## What is the main contribution of this paper?

This paper proposes a new approach called Continual Bias Adaptor (CBA) for online continual learning. The key contributions are:- It proposes a bi-level learning framework to model the posterior distribution shift in an online manner during continual learning. This allows adapting to catastrophic distribution changes and consolidating knowledge from previous tasks. - It introduces a CBA module that can plug into most rehearsal-based methods during training and be removed at test time, introducing no extra computation cost or memory overhead.- It provides theoretical analysis to show CBA establishes gradient alignment between training and memory buffer data, explaining why it alleviates catastrophic forgetting.- It demonstrates strong empirical performance by applying CBA to 4 rehearsal baselines on 3 benchmarks, consistently improving their accuracy and forgetting measures under online continual learning settings.In summary, the main contribution is proposing the CBA approach to explicitly model and adapt to distribution shifts in online continual learning, both theoretically and empirically showing it helps consolidate knowledge across tasks. The bi-level learning and removable module allow flexibility to apply CBA to existing rehearsal methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Continual Bias Adaptor module that augments the classifier network to adapt to catastrophic distribution shifts during online continual learning training, allowing the classifier to consolidate knowledge across tasks, and this module can be removed during testing so it adds no computational overhead.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of continual learning:- This paper focuses on tackling catastrophic forgetting in the online continual learning setting. Most prior work has focused more on the offline setting where models can train on batches from previous tasks multiple times. The online setting is more challenging and realistic since models only see each example once.- The key contribution is proposing a Continual Bias Adaptor (CBA) module to dynamically adapt the classifier to distribution shifts during training. This is a unique approach compared to prior methods that typically try to replay old examples, regularize model updates, or separate task-specific modules. Modeling the posterior directly seems more principled.- The CBA module allows the classifier to learn a stable consolidation of knowledge across tasks, while absorbing catastrophic distribution shifts into the adaptor parameters. This is a clever bi-level optimization strategy.- They demonstrate broad improvements by plugging CBA into several strong replay-based baselines like Experience Replay, DER++, RAR, and CLSER. Showing consistent gains across different algorithms is a strong result.- The theoretical analysis provides justification for why CBA helps align gradients and reduce forgetting. Connecting to prior gradient alignment work like GEM helps explain the mechanism.- Overall, modeling distribution shift explicitly via the posterior is novel compared to prior bias correction layers or simpler replay strategies. The bi-level optimization and ability to plug into existing methods are also strengths. The gains across multiple benchmarks substantiate the benefits. I think it's an interesting and valuable contribution to continual learning.In summary, the CBA module and bi-level optimization approach seem unique compared to related methods. The results demonstrate clear improvements in a challenging online setting across diverse algorithms and benchmarks. The theoretical insights also help explain the success. I find it to be a solid incremental contribution over prior art in this domain.


## What future research directions do the authors suggest?

Based on my review, the main future research directions suggested by the authors are:- Exploring more advanced architectures for the Continual Bias Adaptor (CBA) module. The authors used a simple MLP to parameterize CBA in this work. They suggest exploring more sophisticated architectures like attention networks or transformers to further enhance the adaptation capability of CBA.- Extending the proposed method to other continual learning settings such as domain incremental learning and task incremental learning. The current work focused on class incremental learning. Applying the idea of modeling posterior distribution shifts using CBA to other CL settings could be an interesting direction. - Theoretical analysis of the proposed method. The authors provided some initial theoretical analysis of why the proposed bi-level optimization provides gradient alignment. More in-depth theoretical analysis of the properties and behavior of the CBA module could provide better insights.- Applications to more complex real-world continual learning problems. The experiments were on image classification datasets. Testing the approach on more complex domains like natural language processing, speech recognition etc. would be valuable.- Combining rehearsal and regularization-based continual learning methods with the proposed approach. The current work augmented rehearsal-based methods with CBA. Exploring regularization methods like EWC, SI etc. jointly with CBA could further boost performance.- Long-term continual learning with hundreds or thousands of tasks. The experiments were limited to 10-20 tasks. Scaling up to much longer task sequences would pose additional challenges.In summary, the key future directions are around developing more advanced CBA architectures, combining CBA with other CL approaches, testing on more complex and larger-scale problems, and providing more theoretical analysis. Advancing along these lines can further unlock the potential of the proposed idea of dynamically adapting to posterior distribution shifts.
