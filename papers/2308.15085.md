# [Learning to Upsample by Learning to Sample](https://arxiv.org/abs/2308.15085)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design an ultra-lightweight and effective dynamic upsampler for dense prediction tasks?The key points are:- The paper proposes a new upsampling method called DySample, which is based on dynamic point sampling rather than dynamic kernels like previous methods. - DySample aims to achieve strong upsampling performance while being extremely efficient in terms of model complexity, latency, and memory.- The authors start with a naive sampling-based upsampler design and incrementally improve it through controlling the initial sampling position, adjusting the offset scope, and dividing the process into independent groups. - Extensive experiments on semantic segmentation, object detection, instance segmentation, panoptic segmentation, and depth estimation show DySample consistently outperforms previous upsamplers while adding negligible overhead.In summary, the central hypothesis is that dynamic upsampling can be effectively achieved through simple and efficient point sampling rather than more complex kernel-based approaches. DySample is proposed to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes DySample, a new dynamic upsampling operator for dense prediction tasks. DySample formulates upsampling as a point sampling process rather than the commonly used kernel-based approach. - It provides a step-by-step analysis on how to improve a naive sampling-based upsampler design into the proposed DySample. This includes controlling the initial sampling position, adjusting the moving scope of offsets, and dividing the upsampling into independent groups.- DySample is shown to achieve superior performance compared to other upsampling operators like nearest neighbor, bilinear, deconvolution, and recent dynamic upsamplers like CARAFE, FADE, SAPA on various tasks. - Importantly, DySample has very low computational complexity and latency compared to other dynamic upsamplers. It does not need any custom CUDA implementation and has negligible parameters, FLOPs, memory footprint and latency overhead compared to bilinear upsampling.So in summary, the key contributions are proposing the dynamic point sampling view for upsampling, the step-wise design of DySample, and demonstrating its effectiveness and efficiency across multiple dense prediction tasks like semantic/instance/panoptic segmentation and depth estimation.
