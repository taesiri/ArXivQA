# [UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL   Models](https://arxiv.org/abs/2402.08898)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Non-autoregressive automatic speech recognition (NASR) models have gained attention due to their fast inference speed resulting from parallel decoding. 
- However, existing NASR models have limitations:
    - Encoder-only models like CTC lack ability to model dependencies between output tokens.
    - Encoder-decoder models can model token dependencies better but don't efficiently leverage speech foundation models which are based on encoder-only architecture.

Proposed Solution:
- The paper proposes a new NASR model called UniEnc-CASSNAT which only uses a single encoder network.
- The encoder goes through two forward passes to act like both the encoder and decoder of CASS-NAT:
    - First pass generates frame-level representations and token-level acoustic embeddings (TAEs).
    - Second pass concatenates TAEs with frame representations as input to model token dependencies.
- Multi-pass CTC training and iterative decoding are introduced to improve accuracy of TAEs.

Main Contributions:
- Proposes encoder-only UniEnc-CASSNAT NASR that achieves comparable accuracy to CASS-NAT but with fewer parameters.
- Integrates advantages of CTC and CASS-NAT - leverages speech foundation models through encoder-only structure while modeling token dependencies.
- Introduces techniques like multi-pass CTC and iterative decoding to further refine accuracy.
- Achieves state-of-the-art NASR performance on Librispeech and MyST datasets.
- Provides efficient and accurate NASR solution for deployment.

In summary, the paper proposes a novel NASR model UniEnc-CASSNAT that only uses a single encoder to match speech foundation model architecture while accurately modeling token dependencies by going through two forward passes. Additional training and decoding techniques further boost accuracy. State-of-the-art results are demonstrated on benchmark datasets.
