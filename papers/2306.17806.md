# [Stay on topic with Classifier-Free Guidance](https://arxiv.org/abs/2306.17806)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, it seems the central hypothesis of this paper is that Classifier-Free Guidance (CFG), a technique originally proposed for improving text-to-image generation, can also be an effective method for improving prompt-adherence in language models. Specifically, the authors hypothesize that using CFG during inference will result in generations from language models that better align with the given prompt, across a variety of prompting approaches like basic prompting, chain-of-thought prompting, text-to-text generation, and chatbot prompts. They test this hypothesis through a series of experiments on standard NLP benchmarks, reasoning tasks, code generation, machine translation, and chatbot dialogue. The key research question appears to be whether CFG can reliably improve prompt-adherence and performance across these diverse language generation tasks compared to vanilla sampling.


## What is the main contribution of this paper?

Based on my reading of the abstract, it seems the main contributions of this paper are:1. Adapting Classifier-Free Guidance (CFG), a technique originally developed for text-to-image generation, to improve prompt-adherence in autoregressive language models. 2. Demonstrating the effectiveness of CFG across a range of NLP benchmarks and prompting techniques, including basic prompting, chain-of-thought prompting, text-to-text generation, and chatbot prompting.3. Achieving state-of-the-art performance on the LAMBADA benchmark using LLaMA-7B with CFG, outperforming even much larger models like PaLM-540B. 4. Showing that a model using CFG can achieve comparable performance to a vanilla model 2x its size, allowing for smaller models that require less training compute and VRAM.5. Providing analysis and visualizations to explain how CFG reduces sampling entropy and gives more weight to prompt-relevant tokens.So in summary, the main contributions seem to be introducing CFG for language models, showing its broad effectiveness on various NLP tasks, achieving a new SOTA, and analyzing how it impacts model behavior. The potential for smaller models with CFG is also notable.
