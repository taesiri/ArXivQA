# [RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing   using Residual Policy Learning](https://arxiv.org/abs/2403.07129)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Autonomous racing is important for developing self-driving car technology, but most methods rely on pre-mapped racing lines rather than being able to dynamically navigate unknown environments. 
- Mapless online path planning using only local sensor observations is more flexible and robust but poses challenges for safely overtaking opponents at high speeds due to the limited planning horizon.

Proposed Solution:
- The paper introduces RaceMOP, a novel mapless online path planning method for multi-agent autonomous racing of F1TENTH vehicles.
- RaceMOP combines an artificial potential field (APF) method as a base policy for local navigation and collision avoidance, with residual policy learning to enable longer-horizon planning for overtaking maneuvers.  
- The residual policy is represented as a deep neural network that takes past sensor observations as input and outputs a correction to the APF policy actions. This allows reasoning about future states for more optimal maneuvers.

Main Contributions:
- Demonstration that the proposed RaceMOP method outperforms existing mapless planners in the ability to safely overtake opponents in simulated races, generalizing to unknown tracks.
- Introduction of a new technique for fusing the learned residual policy with the base APF policy using a truncated Gaussian distribution that prevents bias.
- Detailed analyses showing RaceMOP's capability for long-horizon decision-making and interpretation of the learned robust overtaking behaviors in various scenarios.
- Provision of open-source implementation to facilitate further research.

In summary, the paper makes important contributions towards mapless online planning for high-speed racing scenarios by combining classical and learning-based methods in a novel way. The results showcase safer and more optimal planning than reactive approaches reliant only on immediate sensor inputs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces RaceMOP, a novel mapless online path planning method for multi-agent autonomous racing that fuses an artificial potential field planner with a learned residual policy to enable robust overtaking maneuvers relying solely on local observations from onboard sensors.


## What is the main contribution of this paper?

 Based on my reading of the paper abstract and introduction, the main contribution of this paper is:

The proposal of RaceMOP, a novel mapless online path planning method for multi-agent autonomous racing of F1TENTH cars. RaceMOP combines an artificial potential field (APF) method as a base policy with residual policy learning to enable long-horizon planning capabilities using only local observations from onboard sensors. The key innovations include:

1) A new approach for multi-agent racing that operates without relying on a map or predefined racing lines, using only local observations to overtake other vehicles at high speeds. 

2) A novel method for policy fusion in the residual policy learning framework, fusing the APF policy directly in probability space to avoid bias.

3) Demonstrated superior performance over existing mapless planners in handling and generalization capabilities across different simulated racetracks.

In summary, the main contribution is the proposal and evaluation of the RaceMOP method for mapless online path planning in multi-agent autonomous racing scenarios.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Mapless online path planning: The paper introduces a method for path planning that does not rely on pre-mapped data of the environment. It uses only local observations.

- Multi-agent autonomous racing: The method is designed for racing scenarios with multiple autonomous vehicles that must overtake each other.

- F1TENTH cars: The experiments are done using a simulator of F1TENTH scale autonomous racing cars.

- Artificial potential fields (APF): The base path planning module uses an APF method to avoid collisions.

- Residual policy learning (RPL): The paper improves the APF planner by combining it with a learned residual policy using deep reinforcement learning. 

- Policy fusion: A novel method is introduced to fuse the APF policy and learned residual policy directly in probability space using a truncated Gaussian.

- Long-horizon reasoning: The method demonstrates an ability to reason about overtaking maneuvers over a longer time horizon using only local observations.

- Generalization: The method generalizes to unknown racetrack configurations not seen during training.

- Deep neural network: The residual policy is represented by a DNN which takes past sensor observations as input for temporal reasoning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions that the proposed method called RaceMOP operates without a map and relies solely on local observations. How exactly does it plan collision-free trajectories and overtake other vehicles at high speeds without global map information or predictions of other vehicles' future states?

2. One key aspect seems to be the combination of an artificial potential fields (APF) planner with residual policy learning (RPL). Can you explain the rationale behind this hybrid approach and why APF and RPL complement each other? 

3. The paper states that the APF planner uses only a subset of LiDAR points closest to the ego vehicle's edge points. What is the motivation behind this and how does it lead to better overtaking maneuvers compared to modeling the vehicle size as a point mass or circle?

4. How exactly is the goal point for path planning determined from the LiDAR data in the APF planner? Does this goal point reasoning capability allow for longer planning horizons?

5. What is the architecture of the neural network used for the residual policy? What input representation is used and does it capture temporal/historical information to enable reasoning over longer time scales?

6. A key contribution seems to be the specific policy fusion method using a truncated Gaussian distribution. Why is this proposed over alternative fusion techniques? How does it help reduce bias and ensure safe bounded actions?

7. The reward function has several weighted components related to progress, action smoothness, proximity to obstacles etc. What is the rationale behind each of these terms and how do they shape the learned policy?

8. How many training environments and steps were used? What domain randomization techniques were employed during training to improve sim-to-real transfer? 

9. The results show great generalization to 4 unseen test tracks. What specific behaviors lead to robust overtaking on these new tracks e.g. inside vs outside overtakes, high speed maneuvers etc?

10. What are limitations of the current approach when faced with more complex opponent behaviors instead of non-reactive vehicles? How can the method be extended to handle such interactive scenarios?
