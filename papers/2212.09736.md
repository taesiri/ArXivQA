# Don't Generate, Discriminate: A Proposal for Grounding Language Models   to Real-World Environments

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is proposing a new framework called Pangu for grounding language models to real-world environments and tasks. The central hypothesis is that using language models for discrimination of candidate plans proposed by a symbolic agent is more effective than using language models generatively to directly produce plans, for grounded language understanding tasks like querying knowledge bases.The key claims made are:- Directly generating plans with language models is challenging because it requires models to have intimate knowledge of the planning language and environment. It also lacks fine-grained control.- Using LMs discriminatively to evaluate plausibility of symbolic agent-proposed plans allows disentangling LMs from having to ensure grammaticality, faithfulness, and controllability.- The Pangu framework pairs a symbolic agent that searches environments to propose valid plans with a neural LM that scores candidate plans to guide the search.- Pangu provides flexibility in using different types of LMs like BERT, T5, and Codex in a plug-and-play fashion.- Pangu achieves new SOTA results on KBQA datasets and enables few-shot learning, validating the effectiveness of using LMs discriminatively instead of generatively.The central hypothesis is that discrimination is a better use of LMs than generation for grounded language understanding. The Pangu framework and KBQA experiments aim to validate this claim.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new framework called Pangu for grounded language understanding. The key ideas are:- Pangu disentangles the responsibilities of language models (LMs) and lets LMs focus on what they are good at - evaluating the plausibility of utterance-plan pairs. This is in contrast to prior work that relies on LMs' generative abilities and makes LMs directly generate plans/programs.- Pangu consists of a symbolic agent that explores the environment to propose candidate plans, which are guaranteed to be grammatical and faithful by design. The LM then evaluates and scores the candidate plans to guide the agent's search process.- By separating the responsibilities, Pangu provides flexibility in using different types of LMs (encoder-only, encoder-decoder, decoder-only) in a plug-and-play fashion. It also enables few-shot learning by prompting large LMs with just a few demonstrations.- Experiments on knowledge base question answering, which features a massive grounded environment, show that Pangu achieves new state-of-the-art results with different LMs. It also enables few-shot learning on this task using Codex.In summary, the key contribution is proposing a new neuro-symbolic framework Pangu that capitalizes on LMs' discriminative abilities rather than generative abilities for grounded language understanding. This allows flexibility, sample efficiency, and strong empirical results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new framework called Pangu for grounded language understanding that combines a symbolic agent to explore environments and propose valid plans with a neural language model that evaluates and guides the search process.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other related work in grounded language understanding and knowledge base question answering:- Compared to generation-based approaches like sequence-to-sequence models, this paper proposes using language models for discrimination instead of generation. This allows leveraging the discriminative strengths of LMs while avoiding some of the challenges of generating directly in a complex environment like a knowledge base.- For knowledge base question answering, this approach shares similarities with some prior bottom-up semantic parsing methods. However, it differs in allowing partial observations initially and using adaptive search with an LM scorer rather than enumerating and scoring all candidates upfront.- While large language models have been applied recently in few-shot settings, this work is novel in using an LM to score candidate plans from a symbolic agent. This provides a simpler interface for the LM compared to describing the full environment.- The framework is generic for grounded language understanding across different environments and planning languages. The experiments on knowledge bases validate the benefits over both generation-based KBQA models and limited bottom-up parsers.- The results demonstrate flexibility in using different encoder, encoder-decoder, and decoder-only LMs. This provides uniformity lacking in prior work that typically combines specialized modules.- Enabling few-shot knowledge base question answering with large LMs is novel. The sample efficiency is far greater than fine-tuning methods.In summary, the key innovations are in proposing discriminative use of LMs for grounded understanding, the adaptive search approach tailored to complex environments, and showing the flexibility, effectiveness, and sample efficiency benefits across both standard and large LMs. This opens promising research directions in neuro-symbolic systems and grounded language learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, the authors suggest the following future research directions:- Improving the efficiency of their approach. The iterative scoring of candidate plans by the language model makes their method computationally expensive. They suggest exploring algorithms that can find the top-K candidates in better than O(N) complexity, where N is the number of candidate plans.- Enhancing few-shot learning with large language models (LLMs). While their method shows promising initial results for few-shot knowledge base QA using Codex, there is significant room for improvement, especially on more complex questions. They suggest exploring better prompt designs, retrieval methods, and scoring functions tailored for few-shot learning.- Training with weak supervision instead of full logical forms. The current approach assumes access to gold logical forms during training, but these can be expensive to collect for some environments. Using rewards from the environment in response to the agent's decisions is suggested as a direction to reduce supervision.- Exploring controllability for improved robustness. Their method allows easy control over the search process, but they did not experiment much with constraints for security, safety, etc. Applying it to tasks like text-to-SQL where this control is useful is suggested. - Extending to other grounded language understanding tasks. The general framework they propose is applicable to many tasks, like instruction following, API learning, etc. Experimenting in more environments is suggested.In summary, the main future directions are improving efficiency, few-shot learning, weak supervision, controllability, and broader applications of their neuro-symbolic framework for grounded language understanding. The flexibility of their approach enables many possibilities for extension.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new framework called Pangu for grounded language understanding that leverages the strengths of both symbolic agents and neural language models. The key idea is to have a symbolic agent interact with the target environment (e.g. a knowledge base) to incrementally propose candidate plans, while the language model evaluates and scores these plans to guide the search process. This allows the agent to ensure the plans are valid, while the language model focuses on assessing semantic plausibility. The authors demonstrate the effectiveness of this approach on the challenging task of complex question answering over knowledge bases, where Pangu with just a BERT-base model achieves new state-of-the-art results. The framework also enables few-shot learning by prompting large models like Codex. The results highlight Pangu's effectiveness in leveraging language models for discrimination rather than generation, providing a flexible way to ground language models in real-world environments.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new framework called Pangu for grounded language understanding. The key idea is to leverage the strengths of both symbolic agents and neural language models. The symbolic agent explores the environment to propose candidate plans that are guaranteed to be valid, while the language model evaluates the plausibility of the candidate plans to guide the search process. This allows for more complex plans to be constructed through an iterative search process, while avoiding some of the challenges with using language models alone such as ensuring grammaticality and faithfulness of the generated plans.  The framework is evaluated on the task of knowledge base question answering. Experiments show that it achieves state-of-the-art results on standard datasets using just a BERT-base model, and performance further improves with larger transformers. The method also enables few-shot learning by prompting large language models like Codex, outperforming prior methods using only 10 examples on one dataset. The results demonstrate the effectiveness and flexibility of the proposed framework for grounded language understanding. By combining symbolic search with neural scoring, the strengths of both neural and symbolic methods can be leveraged.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework called Pangu that consists of a symbolic agent interacting with an environment to propose candidate plans, and a neural language model that evaluates the plausibility of each candidate plan. The symbolic agent incrementally constructs candidate plans by exploring the environment, guaranteeing they are valid. The language model scores each candidate plan based on how well it matches the intent of the original input utterance. This process continues, with the language model guiding the search by scoring candidates, until no better scoring plan can be found. By separating plan generation using symbolic methods and plan evaluation using neural models, Pangu aims to leverage the strengths of both approaches. The language model is freed from ensuring plan validity and can focus on plausibility, while the symbolic agent handles the complex search through the environment.
