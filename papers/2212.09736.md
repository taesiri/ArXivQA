# Don't Generate, Discriminate: A Proposal for Grounding Language Models   to Real-World Environments

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is proposing a new framework called Pangu for grounding language models to real-world environments and tasks. The central hypothesis is that using language models for discrimination of candidate plans proposed by a symbolic agent is more effective than using language models generatively to directly produce plans, for grounded language understanding tasks like querying knowledge bases.The key claims made are:- Directly generating plans with language models is challenging because it requires models to have intimate knowledge of the planning language and environment. It also lacks fine-grained control.- Using LMs discriminatively to evaluate plausibility of symbolic agent-proposed plans allows disentangling LMs from having to ensure grammaticality, faithfulness, and controllability.- The Pangu framework pairs a symbolic agent that searches environments to propose valid plans with a neural LM that scores candidate plans to guide the search.- Pangu provides flexibility in using different types of LMs like BERT, T5, and Codex in a plug-and-play fashion.- Pangu achieves new SOTA results on KBQA datasets and enables few-shot learning, validating the effectiveness of using LMs discriminatively instead of generatively.The central hypothesis is that discrimination is a better use of LMs than generation for grounded language understanding. The Pangu framework and KBQA experiments aim to validate this claim.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new framework called Pangu for grounded language understanding. The key ideas are:- Pangu disentangles the responsibilities of language models (LMs) and lets LMs focus on what they are good at - evaluating the plausibility of utterance-plan pairs. This is in contrast to prior work that relies on LMs' generative abilities and makes LMs directly generate plans/programs.- Pangu consists of a symbolic agent that explores the environment to propose candidate plans, which are guaranteed to be grammatical and faithful by design. The LM then evaluates and scores the candidate plans to guide the agent's search process.- By separating the responsibilities, Pangu provides flexibility in using different types of LMs (encoder-only, encoder-decoder, decoder-only) in a plug-and-play fashion. It also enables few-shot learning by prompting large LMs with just a few demonstrations.- Experiments on knowledge base question answering, which features a massive grounded environment, show that Pangu achieves new state-of-the-art results with different LMs. It also enables few-shot learning on this task using Codex.In summary, the key contribution is proposing a new neuro-symbolic framework Pangu that capitalizes on LMs' discriminative abilities rather than generative abilities for grounded language understanding. This allows flexibility, sample efficiency, and strong empirical results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new framework called Pangu for grounded language understanding that combines a symbolic agent to explore environments and propose valid plans with a neural language model that evaluates and guides the search process.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other related work in grounded language understanding and knowledge base question answering:- Compared to generation-based approaches like sequence-to-sequence models, this paper proposes using language models for discrimination instead of generation. This allows leveraging the discriminative strengths of LMs while avoiding some of the challenges of generating directly in a complex environment like a knowledge base.- For knowledge base question answering, this approach shares similarities with some prior bottom-up semantic parsing methods. However, it differs in allowing partial observations initially and using adaptive search with an LM scorer rather than enumerating and scoring all candidates upfront.- While large language models have been applied recently in few-shot settings, this work is novel in using an LM to score candidate plans from a symbolic agent. This provides a simpler interface for the LM compared to describing the full environment.- The framework is generic for grounded language understanding across different environments and planning languages. The experiments on knowledge bases validate the benefits over both generation-based KBQA models and limited bottom-up parsers.- The results demonstrate flexibility in using different encoder, encoder-decoder, and decoder-only LMs. This provides uniformity lacking in prior work that typically combines specialized modules.- Enabling few-shot knowledge base question answering with large LMs is novel. The sample efficiency is far greater than fine-tuning methods.In summary, the key innovations are in proposing discriminative use of LMs for grounded understanding, the adaptive search approach tailored to complex environments, and showing the flexibility, effectiveness, and sample efficiency benefits across both standard and large LMs. This opens promising research directions in neuro-symbolic systems and grounded language learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, the authors suggest the following future research directions:- Improving the efficiency of their approach. The iterative scoring of candidate plans by the language model makes their method computationally expensive. They suggest exploring algorithms that can find the top-K candidates in better than O(N) complexity, where N is the number of candidate plans.- Enhancing few-shot learning with large language models (LLMs). While their method shows promising initial results for few-shot knowledge base QA using Codex, there is significant room for improvement, especially on more complex questions. They suggest exploring better prompt designs, retrieval methods, and scoring functions tailored for few-shot learning.- Training with weak supervision instead of full logical forms. The current approach assumes access to gold logical forms during training, but these can be expensive to collect for some environments. Using rewards from the environment in response to the agent's decisions is suggested as a direction to reduce supervision.- Exploring controllability for improved robustness. Their method allows easy control over the search process, but they did not experiment much with constraints for security, safety, etc. Applying it to tasks like text-to-SQL where this control is useful is suggested. - Extending to other grounded language understanding tasks. The general framework they propose is applicable to many tasks, like instruction following, API learning, etc. Experimenting in more environments is suggested.In summary, the main future directions are improving efficiency, few-shot learning, weak supervision, controllability, and broader applications of their neuro-symbolic framework for grounded language understanding. The flexibility of their approach enables many possibilities for extension.
