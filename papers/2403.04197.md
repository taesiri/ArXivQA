# [Large Language Models are In-Context Molecule Learners](https://arxiv.org/abs/2403.04197)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) show great capabilities in biochemical tasks like molecule-caption translation. However, previous methods require extra pre-training data, suffer weak alignment between molecules and text, or impose high demands on model scale.

Proposed Solution (In-Context Molecule Adaptation - ICMA):  
- A new paradigm to adapt LLMs for molecule-caption translation via in-context learning from examples, without extra training data or modifications to model structure.

Key Stages:
1) Cross-Modal Retrieval: Retrieve informative molecule-caption examples using caption and graph retrieval algorithms.
2) Post-Retrieval Re-Ranking: Refine and prioritize examples using proposed sequence reversal and random walk strategies. 
3) In-Context Molecule Tuning: Enable LLMs to learn molecule-text alignment from context examples and adapt parameters for the translation task.

Main Contributions:
- Propose ICMA to unlock in-context molecule learning capabilities of LLMs using only a few examples.
- Implement through cross-modal retrieval, re-ranking and in-context tuning to significantly enhance context informativeness.  
- Experiments show ICMA helps LLMs achieve state-of-the-art performance on molecule-caption translation without extra training data or model modifications. 
- Study demonstrates LLMs have inherent capabilities as in-context molecule learners when provided with informative examples.
- Provides framework to deploy advanced billion-parameter LLMs for scientific tasks.
