# [Agent Attention: On the Integration of Softmax and Linear Attention](https://arxiv.org/abs/2312.08874)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a novel attention mechanism called Agent Attention that aims to strike a favorable balance between computational efficiency and representation power. It introduces additional "agent" tokens A into the conventional attention triplet of queries Q, keys K and values V. First, the agent tokens aggregate global information from the keys and values via a softmax attention operation. Then, another softmax attention distributes this information from the agent tokens back to the queries. As the number of agents can be much smaller than the queries, this reduces the quadratic complexity of regular softmax attention to linear, while retaining modeling of global context. Interestingly, agent attention is shown to be equivalent to a generalized form of linear attention, thereby integrating the benefits of both softmax and linear attention. When applied to vision transformers across diverse tasks like image classification, detection, segmentation and generation, agent attention improves performance, especially for high-resolution scenarios. For instance, simply inserting it into Stable Diffusion accelerates image generation by 1.8x and enhances quality, without any training. Thus, by elegantly combining highly expressive softmax attention with efficient linear attention, agent attention provides an effective attention mechanism for vision transformers.
