# [A Negative Result on Gradient Matching for Selective Backprop](https://arxiv.org/abs/2312.05021)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates an improved selection mechanism for Selective Backpropagation, an approach to accelerate deep neural network training by restricting the backward pass to a subset of the minibatch. The authors propose using a gradient matching strategy to choose a small, possibly weighted, subset of examples from the minibatch that best preserves the mean gradient over the full batch. They efficiently compute an approximation of the last layer gradients and use a sparse approximation algorithm to find the best-matching subset. Experiments across various models and datasets show that this gradient matching approach consistently outperforms the original loss-based selection strategy, especially in the presence of label noise which biases loss-based sampling. However, both selective backpropagation strategies fail to consistently beat a simple random subsampling baseline equivalent to using a smaller batch size. While the gradient matching obtains better gradient estimates, this does not directly translate to faster convergence. The paper provides an extensive empirical evaluation of selective backpropagation, including previously absent baselines, and highlights remaining challenges and open questions around effectively accelerating deep learning optimization.
