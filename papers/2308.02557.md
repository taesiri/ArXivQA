# [Attention-free Spikformer: Mixing Spike Sequences with Simple Linear   Transforms](https://arxiv.org/abs/2308.02557)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on Spikformer, a spiking neural network (SNN) architecture that applies the transformer model to SNNs. Spikformer introduces a Spiking Self-Attention (SSA) module to mix sparse visual features using spike sequences. However, SSA has a quadratic time complexity of O(N^2), limiting efficiency. The paper investigates whether simpler sequence mixing mechanisms can substitute SSA while retaining performance.

Method: 
The paper proposes replacing SSA with an unparameterized Linear Transform (LT) layer based on Fourier Transform (FT) or Wavelet Transform (WT). Unlike SSA, LT does not require learnable parameters and reduces complexity to O(NlogN). LT alternates between frequency and time domains to extract features.

Main Contributions:

1) Demonstrates that simple LTs like FT and WT are effective for extracting sparse visual features in SNNs, suggesting SSA may not be essential for Spikformer's strong performance.

2) Introduces Spikformer variant with LT layer (FT or WT) substituting SSA, enhancing efficiency. Comprehensively analyzes complexity.  

3) Achieves 1) higher accuracy on neuromorphic datasets, 2) comparable accuracy on static datasets, 3) 4-26% memory savings, 4) 29-51% faster training, 5) 61-70% faster inference compared to Spikformer with SSA.

In summary, the paper shows a simple unparameterized LT can substitute the more complex SSA in Spikformer without sacrificing accuracy while enhancing efficiency. This demonstrates the surprising effectiveness of simplicity for mixing spike sequences in SNNs. The introduced Spikformer variant with LT advances efficiency of SNN transformers.
