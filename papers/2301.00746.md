# [NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory](https://arxiv.org/abs/2301.00746)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is:

How can we leverage narrations to improve natural language query localization in long egocentric videos? 

More specifically, the key hypothesis is:

Timestamped video narrations can be converted into natural language queries with temporal response windows and used to augment training data for query localization models, in order to significantly improve natural language query performance.

The authors propose using narrations as queries (NaQ) to expand the usually limited supervision available for training query localization models in episodic memory architectures. Their hypothesis is that narrations provide localizable descriptive information and can benefit query localization when used to generate training data. The paper aims to validate this hypothesis and demonstrate the effectiveness of the proposed NaQ data augmentation strategy.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of a data augmentation strategy called Narrations-as-Queries (NaQ) to improve natural language video querying models. Specifically, the paper shows how to automatically generate training data for the natural language querying task by converting video narrations and their timestamps into query-response pairs. This allows them to expand the limited annotated querying data by 80x. The paper demonstrates that simply augmenting existing state-of-the-art models with this additional NaQ data leads to significant performance improvements on the Ego4D episodic memory benchmark, with relative gains ranging from 30% to over 100%. The paper also analyzes the benefits of NaQ, showing advantages for queries about long-tail objects, zero-shot querying, and across different query types. Overall, the key contribution is a simple yet highly effective data augmentation technique that leverages narrations to supervise episodic memory models by treating them as natural language queries.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper introduces Narrations-as-Queries (NaQ), a data augmentation strategy that leverages narrations to generate pseudo training data for the episodic memory task of localizing natural language queries in long videos. The key idea is to convert timestamped narrations into query-response pairs to expand the limited supervision available for training query localization models. Experiments on Ego4D show NaQ substantially improves multiple state-of-the-art methods and achieves the best results to date.

In one sentence: The paper proposes using narrations as pseudo training data to improve natural language video localization via data augmentation.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on Narrations-as-Queries (NaQ) compares to other related work in video-language grounding and episodic memory:

- It addresses a key limitation of prior work on natural language queries (NLQ) for episodic memory, which is the lack of large-scale supervision. By leveraging narrations as queries, NaQ provides a way to massively expand the training data.

- Compared to other video-language pretraining methods like EgoVLP, VideoBERT, etc., NaQ is complementary. It focuses specifically on improving the query localization module rather than just the video/text encoders. The results show NaQ benefits even methods that use large-scale pretraining like EgoVLP.

- Unlike some prior work that relies on weakly aligned or automatically generated text for pretraining, NaQ leverages narrations that are human-annotated and grounded in the video contents. This likely leads to more effective training.

- Compared to methods that reduce annotation costs for language grounding via point annotations, NaQ relies on narrations that are not task-specific. The narrations can benefit multiple downstream tasks beyond just NLQ.

- NaQ demonstrates unique capabilities like improving on long-tail objects, and enabling zero-shot/few-shot NLQ. This has not been shown in prior work to my knowledge.

- It achieves state-of-the-art results on the challenging Ego4D NLQ benchmark, outperforming all winners from the CVPR and ECCV 2022 challenges. This demonstrates the effectiveness of the idea.

In summary, NaQ offers a simple yet powerful way to inject multimodal supervision into the training of query localization models by converting an existing rich source of data (narrations) into a form usable for NLQ. The strength of the idea lies in its simplicity, generality, and dramatic empirical gains.
