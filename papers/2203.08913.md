# [Memorizing Transformers](https://arxiv.org/abs/2203.08913)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can language models be extended with the ability to memorize the internal representations of past inputs in order to acquire new knowledge immediately, rather than needing to be trained or finetuned?The paper proposes that this can be achieved by augmenting language models with an approximate kNN lookup into a non-differentiable external memory of recent (key, value) pairs. The hypothesis is that this will improve language modeling performance across various benchmarks and tasks by allowing the model to memorize and retrieve relevant information that it has seen before. Specifically, the paper hypothesizes that:1) Adding external memory will improve language modeling perplexity across a variety of long-form text datasets like books, code, math papers etc. 2) Increasing the size of the external memory will provide continued gains in perplexity.3) Models will be able to generalize to and make use of larger memory sizes at test time than they were trained on. 4) The model will use the memory in the intended way, for example by looking up definitions of functions or lemmas that were defined earlier in a document.The experiments and results aim to validate these hypotheses by evaluating language modeling perplexity with different memory sizes on several datasets, studying generalization to larger memories, and doing an analysis of retrieval patterns.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contribution of this paper seems to be presenting a method to extend language models to attend to much longer context by using approximate k-nearest neighbors (kNN) lookup into a large external memory. The key ideas are:- Using kNN retrieval to access long-range context rather than typical attention mechanisms. This allows efficiently retrieving information from a much longer context than fits in local attention.- Making the external memory non-differentiable, so keys and values can be reused from prior steps rather than recomputed, enabling large memory sizes.- Showing steady perplexity gains on various language modeling benchmarks as memory size is increased up to 262K tokens.- Demonstrating the model can make use of the memory, for example to look up mathematical lemma definitions in a formal proof corpus.So in summary, the main contribution appears to be presenting an efficient method to incorporate non-differentiable long-range memory into transformers to improve language modeling, and demonstrating its benefits on various textual domains. The method seems simple but effective for providing transformers a long-term memory capability.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here are a few thoughts on how it compares to other research in natural language processing and deep learning:- The use of transformers and self-attention has become quite common in NLP research in recent years. So in that sense, this paper builds on a popular architecture that has shown a lot of success for many language tasks. However, the idea of augmenting transformers with an external memory using kNN lookups seems relatively novel.- There has been a lot of work on efficiently scaling transformers to longer context lengths, through methods like sparse attention, local attention windows, hierarchical attention, etc. This paper takes a different approach of using an approximate kNN lookup to retrieve from a non-differentiable memory. The idea of using retrieval for long-range memory seems less explored than some other approaches.- The idea of memorization and rapidly acquiring facts at test time, without updating model weights, is an intriguing concept. This seems connected to ideas in the literature about meta-learning and few-shot learning, but focused on language specifically. Being able to rapidly ingest facts from provided context seems important for language understanding.- Evaluating on datasets spanning different domains like books, math papers, code, and formal proofs is a nice contribution, showing the generality of the approach. Many papers focus evaluation on just one type of language data.- Analyzing what facts the model retrieves from memory during inference provides some interesting insights into how the approach works in practice. The idea of looking up definitions of mathematical concepts seems particularly compelling as a use case.- Compared to methods that require pretraining on massive corpora, this approach seems more feasible to try out and adapt, while still giving significant perplexity improvements. The gains are noteworthy compared to just training a standard transformer baseline.Overall, I find the idea of a memorizing transformer quite interesting and this seems like a worthwhile exploration, building on existing ideas like neural caches and nearest neighbors retrieval in some novel ways. The results look promising and it will be interesting to see if this approach gets more uptake in future NLP research.


## What future research directions do the authors suggest?

Based on my review of the paper, some key future research directions suggested by the authors include:- Exploring even larger memory sizes beyond 262K tokens. The authors showed steady improvements in perplexity up to memory sizes of 262K tokens, but note that further gains may be possible with even larger memories on datasets with longer documents.- Leveraging the capability to memorize large knowledge bases or code repositories. The authors mention that the simplicity of their approach could allow scaling to huge external memories, and suggest exploring how to best exploit this capability.- Improving training stability with large memories. The authors faced some instability training models from scratch with large memories, likely due to distributional shift. Methods to improve training stability could help unlock the full potential of large memories.- Exploring different architectures and mechanisms for integrating external memory. The authors used a simple gating mechanism to combine local and long-range attention, but other architectures could further enhance the benefits of external memory.- Applying this approach to other domains and tasks beyond language modeling. The authors focus on language tasks, but suggest the approach could be useful in other domains like vision as well.- Enhancing the model's ability to retrieve and make use of information in the memory. The authors conduct some analysis of retrieval patterns, but more work could be done to improve the precision of retrieval and how retrieved memories are utilized.In summary, the authors propose a number of promising future directions to build on their approach and results, including scaling to even larger memories, improving training, exploring different architectures, applying the approach to new domains, and enhancing retrieval mechanisms. Advances in these areas could further unleash the benefits of memory-augmented transformers.
