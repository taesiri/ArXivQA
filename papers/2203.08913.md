# [Memorizing Transformers](https://arxiv.org/abs/2203.08913)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can language models be extended with the ability to memorize the internal representations of past inputs in order to acquire new knowledge immediately, rather than needing to be trained or finetuned?The paper proposes that this can be achieved by augmenting language models with an approximate kNN lookup into a non-differentiable external memory of recent (key, value) pairs. The hypothesis is that this will improve language modeling performance across various benchmarks and tasks by allowing the model to memorize and retrieve relevant information that it has seen before. Specifically, the paper hypothesizes that:1) Adding external memory will improve language modeling perplexity across a variety of long-form text datasets like books, code, math papers etc. 2) Increasing the size of the external memory will provide continued gains in perplexity.3) Models will be able to generalize to and make use of larger memory sizes at test time than they were trained on. 4) The model will use the memory in the intended way, for example by looking up definitions of functions or lemmas that were defined earlier in a document.The experiments and results aim to validate these hypotheses by evaluating language modeling perplexity with different memory sizes on several datasets, studying generalization to larger memories, and doing an analysis of retrieval patterns.
