# Natural Language Reasoning, A Survey

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we leverage pre-trained language models (PLMs) to perform effective natural language reasoning (NLR)?Specifically, the paper aims to:- Provide a clear definition and taxonomy of natural language reasoning in NLP- Discuss the potentials, challenges, and requirements of using PLMs for NLR- Review the development and empirical evidence showing PLMs' capabilities for NLR- Categorize and compare different methodologies for building NLR systems with PLMs - Survey NLR benchmarks and tasks, analyzing to what extent they require reasoning- Identify limitations of current research and discuss future directionsThe key hypothesis seems to be that PLMs, especially large language models, have significant potential for natural language reasoning due to their abilities in language understanding, capturing implicit knowledge, few-shot learning, and chain-of-thought reasoning. The paper aims to provide a comprehensive overview of the progress, methodologies, tasks, and open questions around this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this survey paper are:1. It provides a distinct definition for natural language reasoning (NLR) in NLP, discussing what tasks require reasoning and introducing a taxonomy of reasoning types. 2. It conducts a comprehensive literature review of NLR methods based on pre-trained language models (PLMs), covering major benchmarks like logical reasoning, natural language inference, multi-hop QA, and commonsense reasoning. The paper categorizes methodologies into end-to-end reasoning, forward reasoning, and backward reasoning. 3. The paper identifies and highlights backward reasoning as a powerful yet under-explored paradigm for multi-step reasoning compared to the more popular forward reasoning. 4. The paper introduces defeasible reasoning (non-deductive reasoning) and discusses its differences from deductive reasoning. It suggests defeasible reasoning as one of the most important future directions for NLR.5. The survey focuses on reasoning with unstructured natural language text using PLMs, excluding neuro-symbolic techniques and mathematical reasoning.In summary, this survey makes conceptual and practical contributions to formalize and advance natural language reasoning research in NLP, especially using pre-trained language models. The highlights include providing a distinct definition, comprehensive taxonomy and literature review, identifying backward reasoning, and suggesting defeasible reasoning as a key future direction.
