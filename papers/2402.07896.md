# [Suppressing Pink Elephants with Direct Principle Feedback](https://arxiv.org/abs/2402.07896)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Existing language models often fail to follow instructions to not discuss certain topics or entities, an issue the authors term the "Pink Elephant Problem." For example, when prompted not to discuss "American universities," a model meant for discussing British universities may still incorrectly mention American ones.  
- Current methods to control model behaviors require knowing ahead of time the specific topics/entities to avoid and fine-tuning separately for each one. This does not allow controlling models dynamically at inference time on new, unforeseen prohibited topics.

Proposed Solution:
- The authors introduce Direct Principle Feedback (DPF), a simplified form of Constitutional AI, which uses critiques and revisions of model outputs as natural language feedback for preference learning.  
- They apply DPF to train a model on synthetic data to avoid mentioning a Pink Elephant entity specified at inference time, enabling dynamic control without separate fine-tuning.

Contributions:
- Formalization of the Pink Elephant Problem in LLMs.
- DPF - a simplified RLAIF method using critiques and revisions for preference learning.  
- Technique to generate diverse synthetic training data with unwanted and corrected behaviors.
- Models trained with DPF match performance of GPT-4 in avoiding dynamically-specified Pink Elephants, outperforming baselines.
- Shared code, models and preference dataset generation approaches that could aid the community.

In summary, the authors make LLMs dynamically controllable at inference time to avoid unspecified prohibited topics, using a simplified form of Constitutional AI and high-quality synthetic training data. Their models outperform baselines in suppressing dynamically-chosen "Pink Elephants."
