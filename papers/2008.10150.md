# [Contrastive learning, multi-view redundancy, and linear models](https://arxiv.org/abs/2008.10150)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can contrastive learning methods produce useful representations for downstream prediction tasks under an assumption of redundancy between the views?Specifically, the paper analyzes two contrastive learning approaches:1) The landmark embedding method, where the contrastive function is evaluated on landmark points to produce an embedding. 2) Direct embedding methods, where the contrastive learning objective is optimized over embedding functions directly. The key analysis studies linear predictors on top of the learned representations and shows they can match the performance of the Bayes optimal predictor under the redundancy assumption. The dimensionality and optimization error of the representations are also analyzed.Overall, the paper provides a theoretical justification for why contrastive learning methods are effective in practice for learning representations useful in downstream tasks, when the views contain redundant information. The analysis helps explain when and why contrastive learning works.
