# [Benchmarking Neural Network Training Algorithms](https://arxiv.org/abs/2306.07179)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question/hypothesis of this paper seems to be: How can we construct rigorous, fair, and relevant benchmarks to systematically compare neural network training algorithms and measure improvements in training speed due to algorithmic advances?The authors argue that the deep learning community currently lacks good benchmarks and methodologies for empirically evaluating and comparing training algorithms. As a result, it is difficult to determine the state-of-the-art training algorithm or reliably identify improvements. To address this, the paper introduces a new time-to-result benchmark called the MLCommons AlgoPerf: Training Algorithms benchmark. The goal of this benchmark is to evaluate the training speed of algorithms across multiple workloads on fixed hardware. The benchmark aims to produce convincing measurements, provide insights into the causes of measured improvements, and focus on conditions relevant to real-world practice.The key components of the benchmark include:- Precisely defining and measuring training speed based on time to reach target error rates.- Using a diverse set of workloads covering different data modalities and model architectures.- Accounting for sensitivity to workload details through fixed and randomized workloads. - Requiring explicit tuning of hyperparameters under a controlled protocol.- Producing an aggregated benchmark score across workloads to incentive generally useful algorithms.Through detailed experiments, the authors demonstrate the need for such a benchmark and highlight challenges in training algorithm comparisons. They also provide baseline results using popular algorithms like AdamW and NAdamW to showcase the feasibility of the benchmark and provide a provisional state-of-the-art.Overall, the central hypothesis is that the proposed benchmark methodology will enable more rigorous empirical comparisons of training algorithms and drive further progress in this area. The paper aims to highlight the need for and introduce an initial instantiation of such a benchmark.
