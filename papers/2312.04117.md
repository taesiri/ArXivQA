# [Instance Tracking in 3D Scenes from Egocentric Videos](https://arxiv.org/abs/2312.04117)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces a new benchmark task and dataset for studying the problem of instance tracking in 3D scenes from egocentric videos. The dataset features RGB-D video captured from a HoloLens2 device along with per-frame camera poses. Instances to track are enrolled either on-the-fly based on user interaction (single-view online enrollment) or using a pre-specified collection of photos capturing the object from multiple views (multi-view pre-enrollment). The benchmark evaluates performance using precision/recall metrics on predicted 3D locations as well as geometric errors. The authors re-purpose several single object trackers to establish baseline performance. They also present a simple but surprisingly effective method combining segment proposals from SAM with feature embeddings from DINOv2 that significantly outperforms the baselines. The improved performance from formulating the task in an allocentric 3D world coordinate frame suggests egocentric 3D sensing provides an easier setup for tracking than 2D images alone. The newly collected dataset and exploration of baseline methods lay groundwork for future research into building assistive agents that can track objects manipulated by users over extended tasks.
