# [Certified Minimax Unlearning with Generalization Rates and Deletion   Capacity](https://arxiv.org/abs/2312.10336)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of machine unlearning for minimax models. Minimax models are machine learning models that optimize two sets of variables - the primal variables and the dual variables. Examples include generative adversarial networks, robust learning models, and reinforcement learning models. Most prior work on machine unlearning has focused on standard statistical learning (STL) models that optimize only a single set of parameters. However, unlearning for minimax models is not well studied. 

Proposed Solution: 
The paper develops a new algorithm for certified minimax unlearning. Certified unlearning provides theoretical guarantees that the unlearning process sufficiently removes the influence of deleted data. The proposed algorithm has the following key aspects:

1) It introduces a minimax unlearning step based on the total Hessian matrix and complete Newton update. This accounts for the interdependence between the primal and dual variables. 

2) It injects calibrated Gaussian noise, inspired by differential privacy, to achieve the rigorous $(\epsilon, \delta)$-certified unlearning guarantee. This requires carefully bounding the sensitivity between the unlearning variables and retraining-from-scratch variables.

3) It further proposes a more efficient extension based on proximal infinitesimal jackknife to avoid recomputing the total Hessian. This also allows handling online/successive unlearning requests.


Main Contributions:

1) Develops the first certified machine unlearning algorithm tailored to minimax models with theoretical guarantees.

2) Provides generalization rates for the after-unlearning models in terms of population primal-dual risks.

3) Derives deletion capacity results that formally quantify the maximum number of samples that can be removed while maintaining good generalization. The deletion capacity matches state-of-the-art results for STL unlearning.

4) Extends the algorithm and analysis to handle more general loss functions like convex-concave losses.

5) Proposes a more efficient unlearning algorithm variation that avoids repeatedly recomputing the total Hessian.

In summary, the paper addresses the open problem of machine unlearning for minimax models, with rigorous theoretical guarantees on privacy, generalization, and deletion capacity. The results match or improve state-of-the-art guarantees for standard statistical learning models.
