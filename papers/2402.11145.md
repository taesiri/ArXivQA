# [Supporting Experts with a Multimodal Machine-Learning-Based Tool for   Human Behavior Analysis of Conversational Videos](https://arxiv.org/abs/2402.11145)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Experts in conversational analysis (e.g. coaches, clinicians) need to search for key scenes in videos based on participants' multimodal behaviors (verbal, non-verbal cues). This is time-consuming, subjective and lacks tools.

Proposed Solution: 
- The authors develop "Providence", a visual programming tool for multimodal scene search in conversations. 

- It allows combining machine learning features (gaze, speech, etc) to create customizable queries through a block diagram. 

- The tool provides transparency through visualizations to interpret which signals contributed to the detected scenes.

- Queries are sharable as Python code for reusability across videos. 

Contributions:

- Identified design considerations (customizability, transparency, reusability) through interviews with 8 experts.

- Implementation of Providence prototype with multimodal feature extraction and visual programming frontend.

- User study (N=12) showed reduced cognitive load, positive usability and satisfaction with detected scenes.

- In-the-wild deployment generated diverse meaningful queries, transformed experts' workflow through objectivity and knowledge sharing.

In summary, the paper demonstrates an expert-AI collaborative tool, Providence, that facilitates multimodal scene search in conversations via an intuitive visual programming approach. Evaluations exhibit its effectiveness in reducing experts' effort and subjectivity. The shareable queries contribute to collective knowledge accumulation in this emerging field.
