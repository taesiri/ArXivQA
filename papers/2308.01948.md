# [A Multidimensional Analysis of Social Biases in Vision Transformers](https://arxiv.org/abs/2308.01948)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper investigates is: 

What factors contribute to the emergence of social biases in the embedding spaces of Vision Transformers (ViTs)?

The authors note that previous work has shown image models like SimCLR and iGPT encode social biases like racism and sexism. However, little research has examined what causes these biases to arise in the first place. 

To address this question, the paper analyzes how different choices in model training impact social biases in ViTs, specifically investigating:

- Training data: Does augmenting the training data with counterfactual images reduce biases?

- Training objectives: Do discriminative vs. generative self-supervised objectives lead to more or less biased models? 

- Model architecture: How do model scale, input resolution, and other architectural choices affect social biases?

The goal is to gain a better understanding of the root factors causing social biases in ViT embeddings, in order to make informed choices when developing and selecting models for downstream applications. The authors find that no single factor fully explains or eliminates biases, but choices in training data, objectives, and architecture can mitigate biases to some degree.

In summary, the central research question is: What factors contribute to social biases in ViT embeddings? The paper investigates this through experiments analyzing training data, objectives, and model architecture.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Investigating specific factors that contribute to the emergence of social biases in Vision Transformers (ViTs), including training data, model architecture, and training objectives. 

2. Evaluating the impact of counterfactual data augmentation using diffusion-based image editing on mitigating biases. The authors find it can reduce but not eliminate biases, and its effectiveness is model-specific.

3. Assessing the effect of training objectives, finding that ViTs trained with discriminative objectives exhibit fewer biases than those trained with generative objectives. 

4. Observing that ViTs trained on the same data with different objectives can have opposite social biases, suggesting biases arise from more than just data.

5. Analyzing model architecture factors, showing social biases decrease as model size and input resolution increase, but no systematic effect of patch size.

6. Overall, providing insights into factors contributing to biases in ViTs, and suggesting model design choices could substantially improve fairness. The authors recommend model developers/users consider these trade-offs in selecting the most appropriate model.

In summary, the key contribution is a multidimensional analysis that reveals how various factors related to training data, objectives, and model architecture impact the emergence of social biases in Vision Transformers. The findings can inform efforts to mitigate biases.
