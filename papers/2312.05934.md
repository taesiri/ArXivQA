# [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/abs/2312.05934)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) like GPT-3 contain a lot of factual knowledge, but this knowledge has limitations: it is static, non-specific, and relies heavily on the pre-training data characteristics. 
- There is a need to enhance LLMs' knowledge in specialized domains and keep it updated. This poses a challenge on how to best inject new factual information into pretrained models.

Proposed Solutions Compared
- The paper compares two main approaches for knowledge injection:
   1) Fine-tuning (FT): further train the LLM on a domain-specific dataset
   2) Retrieval augmented generation (RAG): retrieve relevant contexts from a knowledge source and add them to the LLM's input at inference time

Key Contributions 
- Created tasks and datasets covering both previously seen (anatomy, astronomy) and entirely new (recent events) factual knowledge to test injection capabilities
- Showed RAG outperforms fine-tuning consistently, for both existing and new knowledge
- Fine-tuning struggles to learn new factual information; exposing models to numerous variations of the same fact during training helps mitigate this
- Analysis of factors impacting performance of both methods
- Formulation of knowledge in LLMs based on accuracy of factual question answering

In summary, this paper demonstrates retrieval augmented generation as a superior approach over fine-tuning for knowledge injection in large language models, through extensive empirical evaluation on diverse factual tasks. The findings also reveal some core challenges LLMs face in acquiring new knowledge.
