# [Towards Efficient Use of Multi-Scale Features in Transformer-Based   Object Detectors](https://arxiv.org/abs/2208.11356)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to enable efficient use of multi-scale features in Transformer-based object detectors. 

The key hypothesis is that sparsely sampling multi-scale features from only a few crucial locations guided by prior detection predictions can significantly improve detection performance without introducing much computational overhead.

Specifically, the two core hypotheses are:

1) Rearranging the Transformer encoder-decoder pipeline to allow iterative update of encoded features can lay the foundation for efficient multi-scale feature exploitation based on intermediate detection predictions. 

2) A sparse sampling strategy that identifies a few promising regions based on prior detections, searches keypoints within each region, and samples adaptive scale features around these keypoints, can provide highly beneficial yet sparse multi-scale features to boost detection with minimal costs.

In summary, the central goal is to develop a generic and efficient paradigm for exploiting multi-scale features in Transformer-based detectors, which is achieved by the two key hypotheses above involving iterative feature update and sparse feature sampling.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new method called Iterative Multi-scale Feature Aggregation (IMFA) for efficiently utilizing multi-scale features in Transformer-based object detectors. The key ideas are:

1. Rearranging the Transformer encoder-decoder pipeline into multiple stacked detection stages, so that encoded image features can be iteratively updated along with refined detection predictions. This allows exploiting multi-scale features under guidance of intermediate predictions. 

2. Proposing a sparse sampling strategy to extract only the most crucial multi-scale features from a few keypoints in promising regions indicated by prior detections. This avoids the prohibitive cost of using dense multi-scale features.

3. Integrating the above two ideas into a simple yet effective paradigm called IMFA. It boosts detection accuracy of Transformer-based detectors significantly with marginal computational overhead.

In summary, the main contribution is proposing IMFA as an efficient and generic approach to incorporate multi-scale features in Transformer-based object detectors. Experiments show it consistently improves various detectors with minimal extra cost.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Iterative Multi-scale Feature Aggregation (IMFA) - a novel paradigm that enables efficient use of multi-scale features in Transformer-based object detectors by sampling and aggregating sparse yet highly informative multi-scale features under the guidance of prior detection predictions, boosting detection accuracy with minimal extra computation.
