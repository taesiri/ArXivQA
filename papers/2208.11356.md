# [Towards Efficient Use of Multi-Scale Features in Transformer-Based   Object Detectors](https://arxiv.org/abs/2208.11356)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to enable efficient use of multi-scale features in Transformer-based object detectors. 

The key hypothesis is that sparsely sampling multi-scale features from only a few crucial locations guided by prior detection predictions can significantly improve detection performance without introducing much computational overhead.

Specifically, the two core hypotheses are:

1) Rearranging the Transformer encoder-decoder pipeline to allow iterative update of encoded features can lay the foundation for efficient multi-scale feature exploitation based on intermediate detection predictions. 

2) A sparse sampling strategy that identifies a few promising regions based on prior detections, searches keypoints within each region, and samples adaptive scale features around these keypoints, can provide highly beneficial yet sparse multi-scale features to boost detection with minimal costs.

In summary, the central goal is to develop a generic and efficient paradigm for exploiting multi-scale features in Transformer-based detectors, which is achieved by the two key hypotheses above involving iterative feature update and sparse feature sampling.
