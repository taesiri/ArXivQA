# DoReMi: Grounding Language Model by Detecting and Recovering from
  Plan-Execution Misalignment

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we enable immediate detection and recovery from misalignments between high-level plans generated by language models and low-level skill executions in embodied agents? The key hypothesis appears to be: By leveraging language models to generate constraints that indicate plan-execution alignment, and using a visual question answering (VQA) model to continuously monitor these constraints, we can enable timely re-planning and recovery when misalignments occur during long-horizon robotic tasks.In summary, this paper focuses on detecting and recovering from mismatches between the high-level plans created by language models and the actual low-level execution in robotic systems. The core proposal is to use language models and VQA to quickly identify these plan-execution misalignments and trigger re-planning for more effective task completion.
