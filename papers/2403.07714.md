# [StableToolBench: Towards Stable Large-Scale Benchmarking on Tool   Learning of Large Language Models](https://arxiv.org/abs/2403.07714)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models":

Problem:
- Existing tool learning benchmarks rely on either small-scale handcrafted tools or large-scale unstable real online APIs. The instability of real APIs leads to irreproducible and incomparable benchmark results over time.

Proposed Solution: 
- Introduce StableToolBench, a new benchmark with two main components:
  1) Virtual API Server: Contains a caching system to store API call responses and API simulators using LLMs to simulate unavailable real APIs. This ensures APIs are always accessible.
  2) Stable Evaluation System: Judges solvable tasks and uses metrics like Solvable Pass Rate (SoPR) with GPT-4 as the automatic evaluator to reduce randomness.

Main Contributions:
- Significantly enhances stability of model performance evaluation despite API failures. Experiments show consistent scores even with 50% API failure rate.
- Virtual API system maintains diversity and realism comparable to real APIs based on human evaluation and embedding visualization. 
- Caching system greatly contributes to stability with over 90% cache hit rate for in-domain methods.
- Replacing GPT-3.5 evaluator with GPT-4 yields over 70% accuracy, much higher than GPT-3.5's sub-70% accuracy.

In summary, StableToolBench enables reliable assessment of models' tool usage abilities over time through its stable virtual API server and evaluation system. The benchmark demonstrates enhanced reproducibility and comparability while maintaining diversity and scope.
