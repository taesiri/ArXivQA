# [How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 demonstrate remarkable multilingual capabilities, but how they process and handle multiple languages is not well understood. 

Proposed Solution:
- The paper proposes a framework depicting LLMs' multilingual workflow:
  1) In the first layers, LLMs understand the input query and convert linguistic features into a unified representation.  
  2) In intermediate layers, LLMs solve tasks by thinking in English, using self-attention for reasoning and feedforward layers to incorporate multilingual knowledge.
  3) In final layers, LLMs generate responses aligned with the original query language.

- To validate this framework, the paper introduces a Parallel Language-specific Neuron Detection (PLND) method to identify neurons activated for each language.

- Extensive ablation studies are conducted by selectively deactivating neuron groups to analyze their impact on various multilingual tasks.

Key Findings:
- Language-specific neurons exist, occupying only 0.13% of neurons, but deactivating them severely hinders multilingual capabilities.

- Deactivating first-layer neurons only affects non-English performance, confirming the language translation role of early layers.  

- Reasoning relies more on self-attention while knowledge incorporation leverages feedforward structures.

- Multilingual performance can be enhanced by fine-tuning small subsets of language-specific neurons.

Main Contributions:
- Comprehensive framework explaining how LLMs process multilingual input and output.

- Novel PLND technique to detect language-specific neurons without labels.

- Extensive ablation studies validating the proposed multilingual workflow of LLMs. 

- Demonstrating that fine-tuning tiny subsets of neurons can markedly improve multilingual abilities.
