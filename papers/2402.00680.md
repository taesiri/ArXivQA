# [LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video   Compression](https://arxiv.org/abs/2402.00680)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing learned video compression models use flow nets or deformable convolutions to estimate motion information. However, these methods have limited receptive fields so they can only capture local contexts and motion. They fail to capture global contexts and large motions between frames, which limits their performance.

Proposed Solution:
The paper proposes a joint local and global motion compensation (LGMC) module to address this problem. The key ideas are:

1) Use a flow net for local motion compensation to capture local contexts and motions. 

2) Employ cross attention in the feature domain to capture global contexts and motions. This allows modeling long-range dependencies between frames without using additional bits. 

3) Divide the softmax operation in attention into two independent softmaxes to reduce the complexity from quadratic to linear. This makes it efficient for high resolutions.

4) Integrate local and global motion compensation together in a joint LGMC module.

The LGMC module is integrated into the DCVC-TCM architecture to obtain a learned video codec called LVC-LGMC.

Main Contributions:

1) First work to employ cross attention for motion compensation to capture global context and motion without extra bits.

2) Propose an efficient attention mechanism with linear complexity for high resolution video coding.

3) Joint integration of local and global motion compensation in a unified architecture.

4) LVC-LGMC codec outperforms prior arts and baseline DCVC-TCM significantly, with over 10% BD-rate reduction on test datasets. This demonstrates the benefits of joint local and global motion modeling.

In summary, the paper presents an effective way to capture both local and global contexts in video frames using attention and flow, which leads to improved learned video compression performance. The core ideas of efficient attention and joint motion compensation are the main contributions.


## Summarize the paper in one sentence.

 This paper proposes a joint local and global motion compensation module for learned video compression that combines flow-based local motion compensation with an efficient cross-attention mechanism for global motion compensation to improve rate-distortion performance.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a joint local and global motion compensation (LGMC) module for learned video coding. Specifically:

1) It proposes to use cross attention to capture global contexts for global motion compensation in learned video coding. This is the first attempt to use attention for motion compensation.

2) It employs flow-based motion compensation for local contexts. By jointly using attention and flow, the model can capture both local and global redundancies between video frames.

3) It proposes an efficient attention mechanism with linear complexity by dividing the softmax operation, allowing it to scale to high resolution videos.

4) It incorporates the proposed LGMC module into the DCVC-TCM model to create the LVC-LGMC model. Experiments show LVC-LGMC achieves significant bitrate savings (over 10% BD-rate reduction) compared to DCVC-TCM and other learned video coding methods.

In summary, the key innovation is using attention for global motion compensation and jointly using it with traditional flow-based local motion compensation to improve learned video coding performance.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords or key terms associated with this paper are:

- Motion estimation
- Neural video compression 
- Local motion compensation
- Global motion compensation  
- Flow net
- Deformable convolutional networks (DCN)
- Cross attention
- Learned video compression 
- Rate-distortion performance
- Conditional coding
- Contextual coding

The paper proposes a joint local and global motion compensation (LGMC) module for learned video compression. It uses flow nets for local motion compensation and employs cross attention in the feature domain for global motion compensation to capture long-range dependencies. The model is evaluated on standard test sets and shows improved rate-distortion performance compared to baseline models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a joint local and global motion compensation (LGMC) module. What are the limitations of only using local motion compensation that LGMC aims to address? 

2. The paper mentions that global contexts such as large-scale motions and global correlations among frames are ignored in previous methods. How does the proposed LGMC module capture such global contexts?

3. The LGMC module uses both flow-based motion compensation and attention-based motion compensation. What are the roles of each? Why use both instead of just one?

4. The attention mechanism in LGMC aims to achieve global motion compensation. Explain the attention formulation used and how it computes similarities between frames without extra bits.  

5. The vanilla attention formulation has quadratic complexity which does not scale to high resolutions. How does the paper modify the attention to reduce the complexity to linear?

6. Explain how the local and global motion information are combined in the overall compression framework of LVC-LGMC. How does the decoder utilize both cues?

7. Analyze the bit allocation results in Figure 5. Why does LGMC require fewer bits for motion representation compared to the baseline DCVC-TCM?

8. The ablation study shows that removing global context at encoder or decoder both hurt performance. Analyze these results - why is global context useful at both encoder and decoder?

9. The method improves upon conditional coding frameworks like DCVC. What are other conditional coding methods that could integrate the proposed LGMC module?

10. The LGMC module brings rate-distortion improvements but also increases complexity. Discuss this trade-off and how it could be optimized further.
