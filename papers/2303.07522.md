# Audio Visual Language Maps for Robot Navigation

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to build a 3D spatial map representation that integrates multimodal information from audio, visual, and language cues to enable a robot to navigate to goals specified through multimodal prompts (e.g. images, audio clips, or natural language referring to visual or audio concepts). The key hypothesis is that fusing features from pre-trained vision-language and audio-language models into a shared 3D voxel map will allow effective cross-modal reasoning and goal disambiguation for navigation. For example, the robot could resolve ambiguities and identify the correct table to navigate to based on a prompt like "go to the table where you heard a microwave sound earlier".The paper proposes Audio-Visual-Language Maps (AVLMaps) as the unified map representation and introduces methods for constructing these maps from raw sensor data and querying landmark locations using multimodal prompts. Experiments demonstrate AVLMaps' ability to achieve higher accuracy in disambiguating goals compared to alternatives, and showcase navigation capabilities in simulation and the real world.In summary, the central research contribution is a new type of multimodal semantic map representation that integrates audio, visual and language information to achieve more robust goal-oriented robot navigation from open-vocabulary multimodal prompts.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation that integrates information from audio, visual, and language modalities. - Showing how AVLMaps can be constructed by fusing features from pre-trained visual-language models like CLIP and audio-language models like AudioCLIP into a shared 3D voxel grid.- Demonstrating how AVLMaps enable cross-modal spatial reasoning and goal disambiguation. For example, using audio cues to identify the correct table out of multiple tables, or using object semantics to identify the source of a sound.- Introducing a method to interface AVLMaps with large language models to achieve zero-shot multimodal goal navigation from free-form language commands referring to visual, audio, or spatial concepts.- Conducting simulations and real-world experiments that validate the ability of AVLMaps to enable more reliable goal disambiguation and multimodal navigation compared to baseline methods.In summary, the main contribution is proposing AVLMaps as a way to effectively integrate multimodal sensory information in a spatial map representation to achieve improved cross-modal reasoning and navigation capabilities. The key novelty is the inclusion of audio alongside vision and language.
