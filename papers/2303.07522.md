# Audio Visual Language Maps for Robot Navigation

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to build a 3D spatial map representation that integrates multimodal information from audio, visual, and language cues to enable a robot to navigate to goals specified through multimodal prompts (e.g. images, audio clips, or natural language referring to visual or audio concepts). The key hypothesis is that fusing features from pre-trained vision-language and audio-language models into a shared 3D voxel map will allow effective cross-modal reasoning and goal disambiguation for navigation. For example, the robot could resolve ambiguities and identify the correct table to navigate to based on a prompt like "go to the table where you heard a microwave sound earlier".The paper proposes Audio-Visual-Language Maps (AVLMaps) as the unified map representation and introduces methods for constructing these maps from raw sensor data and querying landmark locations using multimodal prompts. Experiments demonstrate AVLMaps' ability to achieve higher accuracy in disambiguating goals compared to alternatives, and showcase navigation capabilities in simulation and the real world.In summary, the central research contribution is a new type of multimodal semantic map representation that integrates audio, visual and language information to achieve more robust goal-oriented robot navigation from open-vocabulary multimodal prompts.
