# [Neural Fields meet Explicit Geometric Representation for Inverse   Rendering of Urban Scenes](https://arxiv.org/abs/2304.03266)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform high-quality inverse rendering of large outdoor urban scenes to recover intrinsic scene properties like geometry, spatially-varying materials and lighting from images. 

The key ideas and contributions are:

- Proposing a novel hybrid rendering pipeline that combines neural fields and explicit mesh representations to efficiently render primary and secondary rays respectively. This enables scaling inverse rendering to large scenes.

- Modeling the scene properties like geometry, materials, lighting using a neural intrinsic field and HDR sky dome to enable applications like relighting and AR.

- Achieving state-of-the-art performance on outdoor scene relighting benchmark and high-quality results on challenging single-illumination captures from driving datasets.

In summary, the main hypothesis is that combining the benefits of neural fields and explicit geometry can enable high-quality intrinsic decomposition and inverse rendering of large outdoor urban scenes from images. The experiments validate this on various datasets and downstream applications.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposes a novel hybrid rendering pipeline called FEGR that combines neural fields and explicit geometric representations for inverse rendering of large urban scenes.

- Represents the scene intrinsics (geometry, materials, lighting) using a neural field, which enables modeling high-resolution details. 

- Renders primary rays through volumetric rendering of the neural field to get a G-buffer.

- Extracts a mesh from the underlying signed distance field and uses it to render secondary rays for modeling global illumination effects like shadows.

- Models HDR lighting and materials, making the representation suitable for relighting and virtual object insertion.

- Achieves state-of-the-art performance on novel view synthesis and relighting on the NeRF-OSR dataset. Also shows results on a challenging autonomous driving dataset captured under single illumination.

- Enables downstream applications like photorealistic relighting and virtual object insertion with accurate shadows.

In summary, the key contribution is a hybrid neural rendering approach that combines the benefits of neural fields and explicit geometry to achieve high-quality inverse rendering of large outdoor scenes for graphics applications. The hybrid rendering and intrinsic scene decomposition enable photorealistic view synthesis and editing.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel neural inverse rendering framework called FEGR that combines neural fields and explicit geometric representations to reconstruct urban outdoor scenes from posed camera images, enabling high quality novel view synthesis, relighting, and virtual object insertion.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of inverse rendering and neural scene representations:

- This paper introduces a novel hybrid rendering approach that combines neural fields and explicit geometric representations. Most prior work relies solely on either a neural field (NeRF-based methods) or an explicit mesh. The hybrid approach allows the method to leverage the strengths of both representations - high resolution details from the neural field and efficient secondary ray rendering from the mesh.

- For neural field methods, this paper is most related to NeRF-OSR and other efforts on inverse rendering outdoor scenes. Compared to NeRF-OSR, the proposed method achieves better decomposition of lighting and materials, enabling high-quality relighting. The hybrid rendering also allows modeling of sharper cast shadows compared to using a learned visibility network like in NeRF-OSR.

- Compared to explicit mesh methods like Nvdiffrec, this work scales to much larger outdoor environments by using a neural field as the core scene representation. The mesh is only used for efficiency rather than being the sole representation. This allows the method to handle complex urban geometry.

- A key advantage of this method is the ability to model HDR lighting and materials. This is crucial for applications like AR/VR where lighting needs to be estimated accurately for realism. Many past methods only recover low frequency environment maps or point lights.

- The experiments demonstrate state-of-the-art performance on outdoor scene relighting using the NeRF-OSR benchmark. The method also produces compelling results on challenging urban driving datasets with complex materials, geometry and lighting.

In summary, the hybrid rendering approach and HDR modeling appear to be the key novelties that allow this method to advance the state-of-the-art in neural inverse rendering and modeling of large outdoor environments. The results are a step towards high-fidelity digital twins of real world scenes.
