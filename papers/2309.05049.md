# [Multi-view Self-supervised Disentanglement for General Image Denoising](https://arxiv.org/abs/2309.05049)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that self-supervised multi-view learning can effectively disentangle scene content from corruption artifacts for image restoration. Specifically, the paper proposes that by using multiple views (different corrupted versions) of the same underlying image scene, the model can identify invariant features related to scene content and separate out features related to corruption artifacts. This allows the model to restore corrupted images without requiring ground truth clean data for supervision.The key research questions addressed are:1) Can a self-supervised model learn to effectively disentangle content and corruption features using only pairs of differently corrupted versions of the same image? 2) Will this type of multi-view self-supervised learning allow the model to generalize well to different types of corruption, including synthetic noise and real-world noise?3) How does the performance of this approach compare to supervised methods and other self-supervised techniques on image denoising tasks?The central hypothesis is that multi-view self-supervision provides a powerful inductive bias for disentangling content and artifacts. By comparing multiple views of the same content, the model can identify invariant features related to the content. The paper aims to demonstrate the effectiveness of this technique for self-supervised image restoration.In summary, the key hypothesis is multi-view self-supervision for disentanglement, and the key questions relate to the efficacy of this approach for generalization across corruption types and comparison to other techniques. The experiments aim to validate whether this hypothesis holds true.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a new self-supervised learning method called MeD (Multi-view Self-supervised Disentanglement) for image denoising. The key idea is to leverage multiple corrupted views of the same image and disentangle the latent clean image features from the corruptions. 2. It introduces a training scheme that operates on pairs of corrupted images of the same scene and aims to extract their common latent representation. This allows the model to be trained purely on noisy data without clean ground truth images.3. It presents a disentangled representation learning framework with specialized encoders and decoders. These include a shared content encoder, an auxiliary noise encoder, a cross disentanglement decoder, and a Bernoulli manifold mixture decoder. The framework is designed to meet certain properties like independence, consistency and composability between the latent spaces.4. The method demonstrates state-of-the-art performance on unseen synthetic noise types like speckle, Poisson etc. when trained only on Gaussian noise. This highlights its ability to generalize to unfamiliar noise distributions.5. Experiments on real-world datasets like SIDD and PolyU show the approach outperforms existing self-supervised and supervised methods by 2-3 dB in PSNR, without any real-world training data.6. The model can be easily adapted to other image restoration tasks like super-resolution and inpainting by replacing the corruption pool with a more generic one during training.In summary, the key novelty lies in the problem formulation, the multi-view training process and the disentangled latent space learning to extract robust image representations from purely noisy observations. The method shows remarkable generalization ability and performance on complex real-world image denoising.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new self-supervised learning approach called MeD that disentangles scene and noise features from multiple corrupted views of the same image to effectively denoise images, even unmatched real-world noise, outperforming prior self-supervised and supervised methods.


## How does this paper compare to other research in the same field?

Here is a brief analysis comparing this paper to other research in the field of image denoising:- This paper proposes a new self-supervised learning framework called MeD (Multi-view Self-supervised Disentanglement) for image denoising. The key idea is to leverage multiple corrupted views of the same image to disentangle the shared scene representation from the noise. This is a novel approach compared to most prior work that relies on noisy/clean image pairs or uses only a single corrupted image.- The proposed method does not require clean target images for training like supervised methods (e.g. DnCNN). It also does not need unpaired clean images like some other self-supervised approaches (e.g. LIR). MeD is trained using only the corrupted data itself. This makes it more practical for real-world scenarios.- MeD demonstrates superior performance to prior self-supervised methods like N2N, N2S, R2R, especially on complex real noise. It also outperforms supervised methods by 3dB on real datasets, showing its effectiveness.- Unlike methods like DIP and S2S that use a single corrupted image, MeD leverages multiple views which provides more information to resolve the ill-posed inverse problem and learn robust representations.- The idea of using multiple views has been explored before in video denoising, but not for static image denoising. MeD is the first to use it for self-supervised learning on still images.- MeD also shows good generalizability by training on only synthetic noise but performing well on real unseen noise types. Methods like LIR suffer more from domain shift issues.- The disentanglement idea is also novel, decomposing the latent space into separate content and noise spaces. This helps isolate the invariant scene representation.In summary, MeD pushes the boundaries of self-supervised image denoising by using multi-view observations and representation disentanglement. The results demonstrate state-of-the-art performance compared to other approaches.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Exploring different network architectures for the scene encoder and decoder modules. The authors used a simple modified Swin Transformer network, but suggest exploring more advanced/specialized architectures for image restoration tasks.- Investigating the impact of using more than 2 views during training. The authors showed some initial results using up to 4 views, but suggest further exploration of how increasing views impacts performance and computational cost.- Applying the multi-view self-supervised disentanglement idea to other low-level vision tasks beyond denoising, such as super-resolution, deblurring etc. The authors showed some preliminary results on super-resolution and inpainting but suggest more in-depth study. - Evaluating the method on more real-world noise datasets, beyond SIDD, PolyU etc. used in the paper. The authors suggest testing on more diverse real-world noise types.- Combining the proposed approach with existing regularization techniques like total variation that encourage smoothness in images. This could potentially further improve denoising performance.- Exploring ways to reduce the computational overhead of the multi-view training, for example through smarter sampling of views rather than using all pairs exhaustively.- Investigating the use of learned priors along with self-supervision to further improve overall performance and robustness.In summary, the main future directions are around architecture exploration, applying the core idea to other tasks, testing on more real-world data, combining with other techniques like regularization or learned priors, and reducing computational overhead during training. The self-supervised multi-view disentanglement concept shows a lot of promise based on the initial results.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new self-supervised learning method called Multi-view Self-supervised Disentanglement (MeD) for image denoising. The key idea is to learn to disentangle the latent clean image from the noise corruption by using multiple corrupted views of the same image as input. Specifically, MeD takes two corrupted versions of an image and encodes them into separate scene and noise representations. It then performs cross-feature combination and feature mixing to explicitly disentangle the shared scene representation from the noise. This allows it to learn robust scene features without access to clean ground truth images. Experiments on Gaussian noise, unseen noise types, real-world noise, and other tasks demonstrate that MeD outperforms prior self-supervised methods and competes with supervised techniques. A key advantage is its ability to generalize well to complex real-world noise. The self-supervised paradigm also makes it more practical for real applications. Overall, the paper presents a novel way to perform image restoration without clean data by learning to disentangle latent representations from multiple corrupted views.\begin{table}	\caption{ Quantitative result obtained from the application of various methods trained on a general Noise Pool to real noise datasets.}	\label{tab:real}	% \small	\centering	% \setlength{\tabcolsep}{4pt}	%\renewcommand{\arraystretch}{0.98} \resizebox{\columnwidth}{!}{    \begin{tabular}{l|cccc}     \toprule   Method & PolyU \cite{xu2018real} & SIDD \cite{abdelhamed2018high} & CC \cite{nam2016holistic} & Average \\    \midrule         N2C \cite{liu2021Swin}& 35.89/  0.9652 & 30.37/  0.6028  &   \underline{37.89}/ 0.9408 & 34.72/ 0.8363 \\     DBD$_4$  \cite{godard2018deep}& 35.69/  0.9571 & 30.23/ 0.6173 & 37.74/  0.9357 & 34.55/ 0.8367 \\     N2N \cite{lehtinen2018noise2noise}&  36.22/  0.9679	&  \underline{32.82/ 0.7297} & 37.39/  0.9570 & \underline{35.48/ 0.8849}  \\     N2S \cite{batson2019noise2self} & \underline{36.41/  0.9721}  &	30.98/ 0.6018 & 37.58/  \underline{0.9622}	& 34.99/ 0.8454 \\     R2R  \cite{pang2021recorrupted}& 34.58/  0.8890 &	29.64/ 0.5708   &  35.35/  0.8478 & 33.19/ 0.7692 \\     LIR \cite{du2020learning}&  34.81/  0.7278  & 28.76/  0.5296  & 35.50/  0.8403 & 33.02/ 0.6992 \\     \midrule     MeD (ours) & \textbf{38.65/ 0.9855} & \textbf{35.81/ 0.8278} & \textbf{40.08/  0.9745 }     & \textbf{38.18/ 0.9293}      \\ \bottomrule    \end{tabular}%    }% \hspace{-0.7cm}\end{table}The paper proposes a new self-supervised learning method for image denoising called Multi-view Self-supervised Disentanglement (MeD). The key idea is to learn to disentangle the latent clean image from the noise corruption by using multiple corrupted views of the same image as input. MeD takes two corrupted versions of an image and encodes them into separate scene and noise representations. It then performs cross-feature combination and feature mixing to explicitly disentangle the shared scene representation from the noise. This allows it to learn robust scene features without access to clean ground truth images. Experiments on Gaussian noise, unseen noise types, real-world noise, and other tasks demonstrate that MeD outperforms prior self-supervised methods and competes with supervised techniques. A key advantage is its ability to generalize well to complex real-world noise. The self-supervised paradigm also makes it more practical for real applications. Overall, the paper presents a novel way to perform image restoration without clean data by learning to disentangle latent representations from multiple corrupted views.In summary, the key contributions are:- A new self-supervised formulation for image denoising using only noisy data - A multi-view training schema with explicit disentanglement of scene and noise latent representations- Cross-feature combination and feature mixing to learn robust shared scene features- State-of-the-art performance on Gaussian noise, unseen noise types, and real-world noise- Ability to generalize well to complex real-world scenarios without clean data- Demonstrated potential for other image restoration tasks like super-resolution and inpainting- A more practical paradigm for image restoration that does not rely on expensive paired ground truth data\begin{table*}[t]\vspace{-2mm}	\caption{Performance comparison of single-view approaches and Ours training on Gaussian noise and testing on various noise types. } % \vspace{0.2cm}	\small	\centering	% \resizebox{\linewidth}{!}{	% \setlength{\tabcolsep}{4pt}	%\renewcommand{\arraystretch}{0.98}  \resizebox{1\textwidth}{!}{    \begin{tabular}{l|ccccc|c}     \toprule   Noise Type & DIP~\cite{ulyanov2018deep} & NAC~\cite{xu2020noisy} & S2S~\cite{quan2020self2self} & IDR~\cite{zhang2022idr}  & Restormer~\cite{zamir2022restormer} & MeD (Ours)\\    \midrule         Gaussian, $\hat \sigma \in[25, 75]$  & 25.62/ 0.7017 & 27.13/ 0.7391  & 27.71/ 0.7622 & {28.52/ 0.8061} &  \textbf{29.10/ 0.8250} & 28.45/ 0.8057 \\     Speckle, $ \hat v\in [25, 50]$ & 30.14/ 0.8574  & 31.55/ 0.8859  & 31.83/ 0.8980 &28.62/ 0.8763 &   30.12/ 0.8557 & \textbf{33.48/ 0.9115}\\      S\&P, $\hat r \in [0.3, 0.5]$  & 28.62/ 0.7957  &  29.89/ 0.8741 & 30.57/ 0.9053 & 27.26/ 0.7544 &  23.09/ 0.6381 & \textbf{30.84/ 0.9135}\\     \midrule     Average & 28.13/ 0.7849 & 29.52/ 0.8330 & 30.04/ 0.8552 & 28.13/ 0.8123 &  27.44/ 0.7729 & \textbf{30.92/ 0.8770} \\     % SIDD &&&&&& have not enough time for doing it\\      \bottomrule    \end{tabular}%    }    \hspace{-0.3cm}	\label{tab:unseenmore}\end{table*}\vspace{-3mm}The paper proposes a new self-supervised learning method for image denoising called Multi-view Self-supervised Disentanglement (MeD). Here are some key details:- Uses only noisy images as input, does not require clean ground truth data.- Takes multiple corrupted views of the same image. - Disentangles scene and noise representations into separate latent spaces.- Performs cross-feature combination and feature mixing to explicitly separate shared scene features from noise.- Can learn robust scene representations without access to clean data.- Outperforms prior self-supervised methods on Gaussian noise, unseen noise types, and real-world noise.- Generalizes very well to complex real-world scenarios.- Also demonstrates potential for super-resolution, inpainting etc.- Provides a more practical paradigm for image restoration without expensive paired data.- Overall presents a novel way to do image restoration using only noisy data, by learning to disentangle latent representations from multiple views.So in summary, it proposes a new self-supervised formulation for image denoising that only uses multiple corrupted views of an image to disentangle the latent clean image from the noise in an unsupervised manner. The key novelty is the ability to perform restoration without clean ground truth data.\begin{table*}[t]\vspace{-2mm}	\caption{Performance comparison of single-view approaches and Ours training on Gaussian noise and testing on various noise types. } % \vspace{0.2cm}	\small	\centering	% \resizebox{\linewidth}{!}{	% \setlength{\tabcolsep}{4pt}	%\renewcommand{\arraystretch}{0.98}  \resizebox{1\textwidth}{!}{    \begin{tabular}{l|ccccc|c}     \toprule   Noise Type & DIP~\cite{ulyanov2018deep} & NAC~\cite{xu2020noisy} & S2S~\cite{quan2020self2self} & IDR~\cite{zhang2022idr}  & Restormer~\cite{zamir2022restormer} & MeD (Ours)\\    \midrule         Gaussian, $\hat \sigma \in[25, 75]$  & 25.62/ 0.7017 & 27.13/ 0.7391  & 27.71/ 0.7622 & {28.52/ 0.8061} &  \textbf{29.10/ 0.8250} & 28.45/ 0.8057 \\     Speckle, $ \hat v\in [25, 50]$ & 30.14/ 0.8574  & 31.55/ 0.8859  & 31.83/ 0.8980 &28.62/ 0.8763 &   30.12/ 0.8557 & \textbf{33.48/ 0.9115}\\      S\&P, $\hat r \in [0.3, 0.5]$  & 28.62/ 0.7957  &  29.89/ 0.8741 & 30.57/ 0.9053 & 27.26/ 0.7544 &  23.09/ 0.6381 & \textbf{30.84/ 0.9135}\\     \midrule     Average & 28.13/ 0.7849 & 29.52/ 0.8330 & 30.04/ 0.8552 & 28.13/ 0.8123 &  27.44/ 0.7729 & \textbf{30.92/ 0.8770} \\     % SIDD &&&&&& have not enough time for doing it\\      \bottomrule    \end{tabular}%    }    \hspace{-0.3cm}	\label{tab:unseenmore}\end{table*}\vspace{-3mm}The key points about the paper:- Proposes a new self-supervised learning method for image denoising called MeD- Uses multiple corrupted views of the same image as input- Disentangles scene and noise representations into separate latent spaces - Performs cross-feature combination and mixing to separate shared scene features- Can learn robust representations without clean ground truth data- Outperforms prior self-supervised methods on Gaussian noise, unseen noise, and real noise- Generalizes very well to complex real-world noise types- Also shows potential for super-resolution, inpainting etc- Provides a practical paradigm without need for expensive paired data- Main novelty is doing restoration with only noisy data by disentangling latent representationsIn summary, it presents a new way to perform self-supervised image restoration using multiple corrupted views to disentangle the latent clean image. The key advantage is not needing clean data.\begin{table*}%[!htbp]\caption{Quantitative result of generalisation performance experiment on CBSD68 \cite{martin2001database}. All methods use Gaussian $\sigma=25$ for pre-trained methods and then Gaussian $\sigma\in [5, 50]$ for fine-turning. The better result in each method is highlighted in \textit{italics}.} % \label{tab:unseen}	% \small	\centering	\resizebox{\linewidth}{!}{	% \setlength{\tabcolsep}{4pt}	%\renewcommand{\arraystretch}{0.98} % \scalebox{1}{    \begin{tabular}{l|cc|cc|cc|c}     \toprule         Fine-tuning Method & \multicolumn{2}{c|}{N2C \cite{liu2021Swin}}& \multicolumn{2}{c|}{N2N \cite{lehtinen2018noise2noise} } &     % \multicolumn{2}{c|}{N2S \cite{batson2019noise2self}}&     \multicolumn{2}{c|}{LIR \cite{du2020learning}} & MeD \\    Pretraining Method & N2C & MeD & N2N & MeD & LIR & MeD & MeD \\      \midrule      Gaussian, $\hat \sigma \in [15, 75]$ & 29.20/     0.7797 & \underline{\textit{29.53/ 0.8081}} &29.04/   0.7642& \textit{29.21/     0.7890 } &26.42/   0.6640  &\textit{27.25/   0.7036}& \textbf{29.60/   0.8101}\\       % Gaussian, $\hat \sigma =15$ & 33.47/  0.9032 & \underline{\textbf{33.69/  0.9177}} & 33.45/  0.8923  & \underline{33.57/  0.9002} &      % % \underline{31.73/  0.86} & 31.09/  0.85 &       % 30.85/   0.8471 & \underline{31.27/   0.8685} & 33.69/  0.9066  \\          % Gaussian, $\hat \sigma =25$ &30.87/  0.8512 & \underline{31.02/  0.8625} &  30.77/  0.8491 & \underline{30.93/  0.8655} &      % % \underline{30.02/  0.82} & 29.47/  0.81 &      % 28.92/   0.8082 & \underline{29.22 /   0.8113} & \textbf{31.28/  0.8772}  \\          % Gaussian, $\hat \sigma =50$ & 27.41/  0.7417 & \underline{27.68/  0.7662} & 27.15/  0.7253  & \underline{27.26/  0.7549} &      % % \underline{27.11/  0.72} & 26.73/  0.69 &      % 24.53/   0.5957 & \underline{24.98/   0.6454} & \textbf{27.81/  0.7680}  \\      % Gaussian, $\hat \sigma =75$ & 25.05/  0.6226 & \underline{25.72/  0.6860} & 24.80/  0.5902  & \underline{25.08/  0.6355} &      % % 24.78/  0.58 & \underline{24.96/  0.59} &      % 21.37/   0.4049 & \underline{23.52/   0.4894} & \textbf{25.61/  0.6865}  \\          Local Var Gaussian  &  35.62/  0.9308 & \underline{\textit{35.85/  0.9439}} & 35.66/  0.9256 & \textit{35.73/  0.9310} &       % \underline{33.08/  0.89} & 32.21/  0.89  &     29.26/   0.8170  & \textit{30.51/   0.8387} & \textbf{35.91/  0.9762} \\          Poisson Noise  & 40.49/  0.9736 & \underline{\textit{42.80/  0.9776}} & 41.35/  0.9736  & \textit{42.27/  0.9813} &      % 35.26/  0.93 & \underline{36.04/  0.95} &      31.23/   0.8672 & \textit{33.47/   0.8932} & \textbf{45.05/  0.9826} \\          Speckle, $ \hat v\in [25, 50]$ & 33.36/  0.9004 & \underline{\textit{33.40/  0.9044}} & 33.32/ \textit{0.8931}   &  \textit{33.33}/  0.8907 &       % 32.86/  0.89 & \underline{32.03/  0.88} &       28.28/  0.7713  & \textit{29.82/   0.8229}  & \textbf{33.48/  0.9115}  \\
