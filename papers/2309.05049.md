# [Multi-view Self-supervised Disentanglement for General Image Denoising](https://arxiv.org/abs/2309.05049)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that self-supervised multi-view learning can effectively disentangle scene content from corruption artifacts for image restoration. Specifically, the paper proposes that by using multiple views (different corrupted versions) of the same underlying image scene, the model can identify invariant features related to scene content and separate out features related to corruption artifacts. This allows the model to restore corrupted images without requiring ground truth clean data for supervision.

The key research questions addressed are:

1) Can a self-supervised model learn to effectively disentangle content and corruption features using only pairs of differently corrupted versions of the same image? 

2) Will this type of multi-view self-supervised learning allow the model to generalize well to different types of corruption, including synthetic noise and real-world noise?

3) How does the performance of this approach compare to supervised methods and other self-supervised techniques on image denoising tasks?

The central hypothesis is that multi-view self-supervision provides a powerful inductive bias for disentangling content and artifacts. By comparing multiple views of the same content, the model can identify invariant features related to the content. The paper aims to demonstrate the effectiveness of this technique for self-supervised image restoration.

In summary, the key hypothesis is multi-view self-supervision for disentanglement, and the key questions relate to the efficacy of this approach for generalization across corruption types and comparison to other techniques. The experiments aim to validate whether this hypothesis holds true.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a new self-supervised learning method called MeD (Multi-view Self-supervised Disentanglement) for image denoising. The key idea is to leverage multiple corrupted views of the same image and disentangle the latent clean image features from the corruptions. 

2. It introduces a training scheme that operates on pairs of corrupted images of the same scene and aims to extract their common latent representation. This allows the model to be trained purely on noisy data without clean ground truth images.

3. It presents a disentangled representation learning framework with specialized encoders and decoders. These include a shared content encoder, an auxiliary noise encoder, a cross disentanglement decoder, and a Bernoulli manifold mixture decoder. The framework is designed to meet certain properties like independence, consistency and composability between the latent spaces.

4. The method demonstrates state-of-the-art performance on unseen synthetic noise types like speckle, Poisson etc. when trained only on Gaussian noise. This highlights its ability to generalize to unfamiliar noise distributions.

5. Experiments on real-world datasets like SIDD and PolyU show the approach outperforms existing self-supervised and supervised methods by 2-3 dB in PSNR, without any real-world training data.

6. The model can be easily adapted to other image restoration tasks like super-resolution and inpainting by replacing the corruption pool with a more generic one during training.

In summary, the key novelty lies in the problem formulation, the multi-view training process and the disentangled latent space learning to extract robust image representations from purely noisy observations. The method shows remarkable generalization ability and performance on complex real-world image denoising.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new self-supervised learning approach called MeD that disentangles scene and noise features from multiple corrupted views of the same image to effectively denoise images, even unmatched real-world noise, outperforming prior self-supervised and supervised methods.


## How does this paper compare to other research in the same field?

 Here is a brief analysis comparing this paper to other research in the field of image denoising:

- This paper proposes a new self-supervised learning framework called MeD (Multi-view Self-supervised Disentanglement) for image denoising. The key idea is to leverage multiple corrupted views of the same image to disentangle the shared scene representation from the noise. This is a novel approach compared to most prior work that relies on noisy/clean image pairs or uses only a single corrupted image.

- The proposed method does not require clean target images for training like supervised methods (e.g. DnCNN). It also does not need unpaired clean images like some other self-supervised approaches (e.g. LIR). MeD is trained using only the corrupted data itself. This makes it more practical for real-world scenarios.

- MeD demonstrates superior performance to prior self-supervised methods like N2N, N2S, R2R, especially on complex real noise. It also outperforms supervised methods by 3dB on real datasets, showing its effectiveness.

- Unlike methods like DIP and S2S that use a single corrupted image, MeD leverages multiple views which provides more information to resolve the ill-posed inverse problem and learn robust representations.

- The idea of using multiple views has been explored before in video denoising, but not for static image denoising. MeD is the first to use it for self-supervised learning on still images.

- MeD also shows good generalizability by training on only synthetic noise but performing well on real unseen noise types. Methods like LIR suffer more from domain shift issues.

- The disentanglement idea is also novel, decomposing the latent space into separate content and noise spaces. This helps isolate the invariant scene representation.

In summary, MeD pushes the boundaries of self-supervised image denoising by using multi-view observations and representation disentanglement. The results demonstrate state-of-the-art performance compared to other approaches.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Exploring different network architectures for the scene encoder and decoder modules. The authors used a simple modified Swin Transformer network, but suggest exploring more advanced/specialized architectures for image restoration tasks.

- Investigating the impact of using more than 2 views during training. The authors showed some initial results using up to 4 views, but suggest further exploration of how increasing views impacts performance and computational cost.

- Applying the multi-view self-supervised disentanglement idea to other low-level vision tasks beyond denoising, such as super-resolution, deblurring etc. The authors showed some preliminary results on super-resolution and inpainting but suggest more in-depth study. 

- Evaluating the method on more real-world noise datasets, beyond SIDD, PolyU etc. used in the paper. The authors suggest testing on more diverse real-world noise types.

- Combining the proposed approach with existing regularization techniques like total variation that encourage smoothness in images. This could potentially further improve denoising performance.

- Exploring ways to reduce the computational overhead of the multi-view training, for example through smarter sampling of views rather than using all pairs exhaustively.

- Investigating the use of learned priors along with self-supervision to further improve overall performance and robustness.

In summary, the main future directions are around architecture exploration, applying the core idea to other tasks, testing on more real-world data, combining with other techniques like regularization or learned priors, and reducing computational overhead during training. The self-supervised multi-view disentanglement concept shows a lot of promise based on the initial results.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new self-supervised learning method called Multi-view Self-supervised Disentanglement (MeD) for image denoising. The key idea is to learn to disentangle the latent clean image from the noise corruption by using multiple corrupted views of the same image as input. Specifically, MeD takes two corrupted versions of an image and encodes them into separate scene and noise representations. It then performs cross-feature combination and feature mixing to explicitly disentangle the shared scene representation from the noise. This allows it to learn robust scene features without access to clean ground truth images. Experiments on Gaussian noise, unseen noise types, real-world noise, and other tasks demonstrate that MeD outperforms prior self-supervised methods and competes with supervised techniques. A key advantage is its ability to generalize well to complex real-world noise. The self-supervised paradigm also makes it more practical for real applications. Overall, the paper presents a novel way to perform image restoration without clean data by learning to disentangle latent representations from multiple corrupted views.

\begin{table}
	\caption{ Quantitative result obtained from the application of various methods trained on a general Noise Pool to real noise datasets.}
	\label{tab:real}
	% \small
	\centering
	% \setlength{\tabcolsep}{4pt}
	%\renewcommand{\arraystretch}{0.98}
 \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cccc}
     \toprule
   Method & PolyU \cite{xu2018real} & SIDD \cite{abdelhamed2018high} & CC \cite{nam2016holistic} & Average \\
    \midrule
    
     N2C \cite{liu2021Swin}& 35.89/  0.9652 & 30.37/  0.6028  &   \underline{37.89}/ 0.9408 & 34.72/ 0.8363 \\
     DBD$_4$  \cite{godard2018deep}& 35.69/  0.9571 & 30.23/ 0.6173 & 37.74/  0.9357 & 34.55/ 0.8367 \\
     N2N \cite{lehtinen2018noise2noise}&  36.22/  0.9679	&  \underline{32.82/ 0.7297} & 37.39/  0.9570 & \underline{35.48/ 0.8849}  \\
     N2S \cite{batson2019noise2self} & \underline{36.41/  0.9721}  &	30.98/ 0.6018 & 37.58/  \underline{0.9622}	& 34.99/ 0.8454 \\
     R2R  \cite{pang2021recorrupted}& 34.58/  0.8890 &	29.64/ 0.5708   &  35.35/  0.8478 & 33.19/ 0.7692 \\
     LIR \cite{du2020learning}&  34.81/  0.7278  & 28.76/  0.5296  & 35.50/  0.8403 & 33.02/ 0.6992 \\
     \midrule
     MeD (ours) & \textbf{38.65/ 0.9855} & \textbf{35.81/ 0.8278} & \textbf{40.08/  0.9745 }     & \textbf{38.18/ 0.9293}
 
     \\ \bottomrule
    \end{tabular}%
    }
% \hspace{-0.7cm}
\end{table}

The paper proposes a new self-supervised learning method for image denoising called Multi-view Self-supervised Disentanglement (MeD). The key idea is to learn to disentangle the latent clean image from the noise corruption by using multiple corrupted views of the same image as input. MeD takes two corrupted versions of an image and encodes them into separate scene and noise representations. It then performs cross-feature combination and feature mixing to explicitly disentangle the shared scene representation from the noise. This allows it to learn robust scene features without access to clean ground truth images. Experiments on Gaussian noise, unseen noise types, real-world noise, and other tasks demonstrate that MeD outperforms prior self-supervised methods and competes with supervised techniques. A key advantage is its ability to generalize well to complex real-world noise. The self-supervised paradigm also makes it more practical for real applications. Overall, the paper presents a novel way to perform image restoration without clean data by learning to disentangle latent representations from multiple corrupted views.

In summary, the key contributions are:

- A new self-supervised formulation for image denoising using only noisy data 

- A multi-view training schema with explicit disentanglement of scene and noise latent representations

- Cross-feature combination and feature mixing to learn robust shared scene features

- State-of-the-art performance on Gaussian noise, unseen noise types, and real-world noise

- Ability to generalize well to complex real-world scenarios without clean data

- Demonstrated potential for other image restoration tasks like super-resolution and inpainting

- A more practical paradigm for image restoration that does not rely on expensive paired ground truth data

\begin{table*}[t]
\vspace{-2mm}
	\caption{Performance comparison of single-view approaches and Ours training on Gaussian noise and testing on various noise types. }
 % \vspace{0.2cm}
	\small
	\centering
	% \resizebox{\linewidth}{!}{
	% \setlength{\tabcolsep}{4pt}
	%\renewcommand{\arraystretch}{0.98}
 
 \resizebox{1\textwidth}{!}{
    \begin{tabular}{l|ccccc|c}
     \toprule
   Noise Type & DIP~\cite{ulyanov2018deep} & NAC~\cite{xu2020noisy} & S2S~\cite{quan2020self2self} & IDR~\cite{zhang2022idr}  & Restormer~\cite{zamir2022restormer} & MeD (Ours)\\
    \midrule
    
     Gaussian, $\hat \sigma \in[25, 75]$  & 25.62/ 0.7017 & 27.13/ 0.7391  & 27.71/ 0.7622 & {28.52/ 0.8061} &  \textbf{29.10/ 0.8250} & 28.45/ 0.8057 \\
     Speckle, $ \hat v\in [25, 50]$ & 30.14/ 0.8574  & 31.55/ 0.8859  & 31.83/ 0.8980 &28.62/ 0.8763 &   30.12/ 0.8557 & \textbf{33.48/ 0.9115}\\ 
     S\&P, $\hat r \in [0.3, 0.5]$  & 28.62/ 0.7957  &  29.89/ 0.8741 & 30.57/ 0.9053 & 27.26/ 0.7544 &  23.09/ 0.6381 & \textbf{30.84/ 0.9135}\\
     \midrule
     Average & 28.13/ 0.7849 & 29.52/ 0.8330 & 30.04/ 0.8552 & 28.13/ 0.8123 &  27.44/ 0.7729 & \textbf{30.92/ 0.8770} \\
     % SIDD &&&&&& have not enough time for doing it\\
 
     \bottomrule
    \end{tabular}%
    }
    
\hspace{-0.3cm}
	\label{tab:unseenmore}
\end{table*}
\vspace{-3mm}

The paper proposes a new self-supervised learning method for image denoising called Multi-view Self-supervised Disentanglement (MeD). Here are some key details:

- Uses only noisy images as input, does not require clean ground truth data.

- Takes multiple corrupted views of the same image. 

- Disentangles scene and noise representations into separate latent spaces.

- Performs cross-feature combination and feature mixing to explicitly separate shared scene features from noise.

- Can learn robust scene representations without access to clean data.

- Outperforms prior self-supervised methods on Gaussian noise, unseen noise types, and real-world noise.

- Generalizes very well to complex real-world scenarios.

- Also demonstrates potential for super-resolution, inpainting etc.

- Provides a more practical paradigm for image restoration without expensive paired data.

- Overall presents a novel way to do image restoration using only noisy data, by learning to disentangle latent representations from multiple views.

So in summary, it proposes a new self-supervised formulation for image denoising that only uses multiple corrupted views of an image to disentangle the latent clean image from the noise in an unsupervised manner. The key novelty is the ability to perform restoration without clean ground truth data.

\begin{table*}[t]
\vspace{-2mm}
	\caption{Performance comparison of single-view approaches and Ours training on Gaussian noise and testing on various noise types. }
 % \vspace{0.2cm}
	\small
	\centering
	% \resizebox{\linewidth}{!}{
	% \setlength{\tabcolsep}{4pt}
	%\renewcommand{\arraystretch}{0.98}
 
 \resizebox{1\textwidth}{!}{
    \begin{tabular}{l|ccccc|c}
     \toprule
   Noise Type & DIP~\cite{ulyanov2018deep} & NAC~\cite{xu2020noisy} & S2S~\cite{quan2020self2self} & IDR~\cite{zhang2022idr}  & Restormer~\cite{zamir2022restormer} & MeD (Ours)\\
    \midrule
    
     Gaussian, $\hat \sigma \in[25, 75]$  & 25.62/ 0.7017 & 27.13/ 0.7391  & 27.71/ 0.7622 & {28.52/ 0.8061} &  \textbf{29.10/ 0.8250} & 28.45/ 0.8057 \\
     Speckle, $ \hat v\in [25, 50]$ & 30.14/ 0.8574  & 31.55/ 0.8859  & 31.83/ 0.8980 &28.62/ 0.8763 &   30.12/ 0.8557 & \textbf{33.48/ 0.9115}\\ 
     S\&P, $\hat r \in [0.3, 0.5]$  & 28.62/ 0.7957  &  29.89/ 0.8741 & 30.57/ 0.9053 & 27.26/ 0.7544 &  23.09/ 0.6381 & \textbf{30.84/ 0.9135}\\
     \midrule
     Average & 28.13/ 0.7849 & 29.52/ 0.8330 & 30.04/ 0.8552 & 28.13/ 0.8123 &  27.44/ 0.7729 & \textbf{30.92/ 0.8770} \\
     % SIDD &&&&&& have not enough time for doing it\\
 
     \bottomrule
    \end{tabular}%
    }
    
\hspace{-0.3cm}
	\label{tab:unseenmore}
\end{table*}
\vspace{-3mm}

The key points about the paper:

- Proposes a new self-supervised learning method for image denoising called MeD

- Uses multiple corrupted views of the same image as input

- Disentangles scene and noise representations into separate latent spaces 

- Performs cross-feature combination and mixing to separate shared scene features

- Can learn robust representations without clean ground truth data

- Outperforms prior self-supervised methods on Gaussian noise, unseen noise, and real noise

- Generalizes very well to complex real-world noise types

- Also shows potential for super-resolution, inpainting etc

- Provides a practical paradigm without need for expensive paired data

- Main novelty is doing restoration with only noisy data by disentangling latent representations

In summary, it presents a new way to perform self-supervised image restoration using multiple corrupted views to disentangle the latent clean image. The key advantage is not needing clean data.

\begin{table*}%[!htbp]
\caption{Quantitative result of generalisation performance experiment on CBSD68 \cite{martin2001database}. All methods use Gaussian $\sigma=25$ for pre-trained methods and then Gaussian $\sigma\in [5, 50]$ for fine-turning. The better result in each method is highlighted in \textit{italics}.} % 
\label{tab:unseen}
	% \small
	\centering
	\resizebox{\linewidth}{!}{
	% \setlength{\tabcolsep}{4pt}
	%\renewcommand{\arraystretch}{0.98}
 % \scalebox{1}{
    \begin{tabular}{l|cc|cc|cc|c}
     \toprule
     
    Fine-tuning Method & \multicolumn{2}{c|}{N2C \cite{liu2021Swin}}& \multicolumn{2}{c|}{N2N \cite{lehtinen2018noise2noise} } & 
    % \multicolumn{2}{c|}{N2S \cite{batson2019noise2self}}& 
    \multicolumn{2}{c|}{LIR \cite{du2020learning}} & MeD \\
    Pretraining Method & N2C & MeD & N2N & MeD & LIR & MeD & MeD \\
      \midrule
      Gaussian, $\hat \sigma \in [15, 75]$ & 29.20/     0.7797 & \underline{\textit{29.53/ 0.8081}} &29.04/   0.7642& \textit{29.21/     0.7890 } &26.42/ 
  0.6640  &\textit{27.25/   0.7036}& \textbf{29.60/   0.8101}\\
  
     % Gaussian, $\hat \sigma =15$ & 33.47/  0.9032 & \underline{\textbf{33.69/  0.9177}} & 33.45/  0.8923  & \underline{33.57/  0.9002} & 
     % % \underline{31.73/  0.86} & 31.09/  0.85 &  
     % 30.85/   0.8471 & \underline{31.27/   0.8685} & 33.69/  0.9066  \\
     
     % Gaussian, $\hat \sigma =25$ &30.87/  0.8512 & \underline{31.02/  0.8625} &  30.77/  0.8491 & \underline{30.93/  0.8655} & 
     % % \underline{30.02/  0.82} & 29.47/  0.81 & 
     % 28.92/   0.8082 & \underline{29.22 /   0.8113} & \textbf{31.28/  0.8772}  \\
     
     % Gaussian, $\hat \sigma =50$ & 27.41/  0.7417 & \underline{27.68/  0.7662} & 27.15/  0.7253  & \underline{27.26/  0.7549} & 
     % % \underline{27.11/  0.72} & 26.73/  0.69 & 
     % 24.53/   0.5957 & \underline{24.98/   0.6454} & \textbf{27.81/  0.7680}  \\
 
     % Gaussian, $\hat \sigma =75$ & 25.05/  0.6226 & \underline{25.72/  0.6860} & 24.80/  0.5902  & \underline{25.08/  0.6355} & 
     % % 24.78/  0.58 & \underline{24.96/  0.59} & 
     % 21.37/   0.4049 & \underline{23.52/   0.4894} & \textbf{25.61/  0.6865}  \\
     
     Local Var Gaussian  &  35.62/  0.9308 & \underline{\textit{35.85/  0.9439}} & 35.66/  0.9256 & \textit{35.73/  0.9310} &  
     % \underline{33.08/  0.89} & 32.21/  0.89  &
     29.26/   0.8170  & \textit{30.51/   0.8387} & \textbf{35.91/  0.9762} \\
     
     Poisson Noise  & 40.49/  0.9736 & \underline{\textit{42.80/  0.9776}} & 41.35/  0.9736  & \textit{42.27/  0.9813} & 
     % 35.26/  0.93 & \underline{36.04/  0.95} & 
     31.23/   0.8672 & \textit{33.47/   0.8932} & \textbf{45.05/  0.9826} \\
     
     Speckle, $ \hat v\in [25, 50]$ & 33.36/  0.9004 & \underline{\textit{33.40/  0.9044}} & 33.32/ \textit{0.8931}   &  \textit{33.33}/  0.8907 &  
     % 32.86/  0.89 & \underline{32.03/  0.88} & 
      28.28/  0.7713  & \textit{29.82/   0.8229}  & \textbf{33.48/  0.9115}  \\
     
    


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new self-supervised learning method called Multi-view Self-supervised Disentanglement (MeD) for image denoising. The key idea is to leverage multiple corrupted views of the same image to learn a shared latent representation of the underlying clean image, without requiring clean ground truth images during training. 

The method takes two corrupted versions of an image as input and encodes them into separate scene and noise latent representations using encoder networks. A cross-disentanglement decoder is then used to reconstruct one corrupted image from the scene code of one view and noise code of the other view. This forces the disentanglement of scene and noise features. Additionally, a Bernoulli manifold mixture technique is used during training to enforce consistency between scene codes from different views. Experiments show the model achieves state-of-the-art performance on unseen synthetic noise types and real-world image denoising benchmarks. The model also demonstrates strong generalization ability when adapted to other image restoration tasks like super-resolution and inpainting. A key advantage is the ability to train on only noisy data, removing the need for expensive paired supervised data.

In summary, the paper presents a novel self-supervised multi-view learning approach for image denoising that achieves impressive results by disentangling scene and noise features. A major contribution is the ability to train using only unpaired noisy data, increasing feasibility for real-world applications.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new self-supervised learning approach called Multi-view Self-supervised Disentanglement (MeD) for image denoising. The key idea is to learn a shared latent representation of the underlying clean image by using multiple corrupted views of the same static scene as input. Specifically, MeD takes two corrupted versions of an image as input and disentangles them into separate latent representations for the shared content (scene) and unique distortions (noise) using an encoder-decoder architecture. It enforces consistency between the latent content representations from each view and uses a cross-reconstruction loss to explicitly disentangle the content and distortion representations. A Bernoulli manifold mixture loss is also used to further enforce consistency of the content latent space. The disentangled content representation is then decoded to reconstruct the denoised image. A key advantage of MeD is that it only requires corrupted image pairs during training and does not need ground truth clean images. Experiments show it outperforms previous self-supervised methods and even supervised methods on some tasks, especially for real-world noise removal.

\begin{table*}[!htbp]
% \footnotesize

% \hspace{-0.40cm}

\centering
\hspace{0.1cm}

\begin{tabular}{c@{\extracolsep{0em}}
c@{\extracolsep{0.05em}}
c@{\extracolsep{0.05em}}c}
\centering
\large
		\includegraphics[width=0.241 \textwidth]{imgs/sr/2/editedimg_002_SRF_2_HR.png}~
		&\includegraphics[width=0.241 \textwidth]{imgs/sr/2/editedRCAN_data1_noisereal_metric34.89_0.95.jpg}~  
        & \includegraphics[width=0.241 \textwidth]{imgs/sr/2/editedDASR_img_002_SRF_2_HR.png_psnr_34.42_ssim_0.9364.jpg}~
		&\includegraphics[width=0.241 \textwidth]{imgs/sr/2/editedmedia_data1_noisereal_metric36.66_0.97.jpg}~   \\
     
	Set5 ``Bird" \cite{bevilacqua2012low}~  & RCAN \cite{zhang2018image}~ &  DASR \cite{wang2021unsupervised}~ & MeD (Ours)  \\
   
          PSNR/SSIM ~ & 34.89/ 0.9512 ~ & 34.42/ 0.9364~ & \textbf{36.66/ 0.9747}  \\

\end{tabular}

	\caption{Visual comparison of image super-resolution (×3) methods on Set5 ``Bird" \cite{bevilacqua2012low} images. }
	
	% \vspace{-0.1cm}
 \label{fig:sr1}
\end{figure*}



\begin{figure*}[!htpb]
% \footnotesize
% \captionsetup{font=small}
% \hspace{-0.20cm}
\newcommand\M{\includegraphics[width=0.241 \textwidth]}

\centering

\begin{tabular}{c@{\extracolsep{0em}}c@{\extracolsep{0em}}c@{\extracolsep{0em}}c}
\centering
\large

		\includegraphics[width=0.241 \textwidth]{imgs/sr/3/editedimg_003_SRF_2_HR.png}~
		&\includegraphics[width=0.241 \textwidth]{imgs/sr/3/editedRCAN_img_30.91_ssim_0.9459.jpg}~  
       &\includegraphics[width=0.241 \textwidth]{imgs/sr/3/editedDASR_data2_noisereal_metric30.82_0.95.jpg}~
		&\includegraphics[width=0.241 \textwidth]{imgs/sr/3/editedmedia_data2_noisereal_metric31.12_0.96_media_swinv2_sr.jpg}~   \\
         Set 5 ``Butterfly" \cite{bevilacqua2012low}  &  RCAN \cite{zhang2018image} &  DASR \cite{wang2021unsupervised} & MeD (Ours)   \\
         PSNR/SSIM & 30.91/ 0.9459 & 30.82/ 0.9527 & \textbf{31.12/ 0.9636}\\

	\end{tabular}
    \vspace{-0.0cm}
	\caption{Visual comparison of image super-resolution (×4) methods on Set5 ``Butterfly" \cite{bevilacqua2012low} images. 
	}
	
	% \vspace{-0.1cm}
 \label{fig:sr2}
\end{figure*}






%%%    !t
\begin{figure*}[!htpb]
% \footnotesize
% \captionsetup{font=small}
% \hspace{-0.20cm}
\newcommand\M{\includegraphics[width=0.241 \textwidth]}
\centering
% 
\begin{tabular}{c@{\extracolsep{0em}}c@{\extracolsep{0em}}c@{\extracolsep{0em}}c}
\centering
% \Large

		\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedParrots.jpeg}~
		&\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedDIP_data1_metric31.94_0.94.jpg}~  
        &\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedParrot_0.7_33.91_0.9294.png}~
		&\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedmedia_Parrots.tif_psnr_34.01142104723582_ssim_0.9507.png}~   \\
         Set 11 ``Parrots" \cite{bevilacqua2012low}  &  DIP \cite{ulyanov2018deep} &  S2S \cite{quan2020self2self} & MeD (Ours)  \\
         PSNR/SSIM & 31.94/ 0.9479 & 33.91/ 0.9224 & \textbf{34.01/ 0.9507}\\


        \includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedcameraman.jpeg}~
		&\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedDIP-5000_30.97_0.9778.png}~  
        &\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editeds2s_Camera_0.5_33.37_0.9355.png}~
		&\includegraphics[width=0.241 \textwidth]{imgs/inpaint/editedmedia_cameraman.tif_psnr_34.999074402872445_ssim_0.9478926441759973.png}~   \\
  
         Set 11 ``Cameraman" \cite{bevilacqua2012low}  & DIP \cite{ulyanov2018deep} &  S2S \cite{quan2020self2self} & MeD (Ours)  \\
         PSNR/SSIM & 30.97/ 0.9778 & 33.37/ 0.9355 & \textbf{34.99/ 0.9478}\\
        
		
	\end{tabular}
    % \vspace{-0.0cm}
	\caption{Visual comparison of image Inpainting methods on Set11 \cite{kulkarni2016reconnet} images. 
	}
	
	% \vspace{-0.1cm}
 \label{fig:im}
\end{figure*}


\section{Additional Qualitative Results}
\label{result}
The following figures show the denoising comparison on both synthetic noise removal (Figure~\ref{fig:synthfirst} -- Figure~\ref{fig:synthlast}) and denoising real noise data (Figure~\ref{fig:realfirst} -- Figure~\ref{fig:reallast}). 




%%%%%%%%%%%%% -- more supp images


\begin{figure*}[h!]
    \centering  
    %  左下右上  , trim=10 0 150 200,clip
    \includegraphics[width=\textwidth ]{imgs/noise_pool/noiselocal_val_gaussian25_15.jpg}
    % \includegraphics[height=0.9\textheight ]{figures/real/real_50.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=25$) + local variance Gaussian noise.} 
 \label{fig:synthfirst}
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/noise_pool/noiselocal_val_gaussian25_3.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=25$) + local variance Gaussian noise.} 
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{imgs/noise_pool/noiselocal_val_gaussian25_22.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=25$) + local variance Gaussian noise.} 
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{imgs/noise_pool/noiselocal_val_gaussian50_0.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=50$) + local variance Gaussian noise.} 
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{imgs/noise_pool/noiselocal_val_gaussian50_7.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=50$) + local variance Gaussian noise.} 
\end{figure*}
\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/noise_pool/noiselocal_val_gaussian50_17.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=50$) + local variance Gaussian noise.} 
\end{figure*}
\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/noise_pool/noiselocal_val_gaussian75_8.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=75$) + local variance Gaussian noise.} 
\end{figure*}
\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/noise_pool/noiselocal_val_gaussian75_9.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=75$) + local variance Gaussian noise.} 
\end{figure*}
\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{imgs/noise_pool/noiselocal_val_gaussian75_19.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with Gaussian ($\sigma=75$) + local variance Gaussian noise.} 
\end{figure*}
\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{imgs/noise_pool/noise_pool_noiselocal_val_poisson.jpg}
	\caption{Visual comparison of image denoising methods on Kodak \cite{franzen1999kodak} images with local variance Gaussian + Poisson noise.} 
 \label{fig:synthlast}
\end{figure*}




% \begin{figure*}[!hbtp]
%     \centering  
%     %  左下右上  , trim=10 0 150 200,clip
%     \includegraphics[width=\textwidth ]{figures/real/real_50.jpg}
%     % \includegraphics[height=0.9\textheight ]{figures/real/real_50.jpg}
% 	\caption{Visual comparison of image denoising methods on real noisy image dataset SIDD \cite{SIDD_2018_CVPR} example images with real noise.} 
%  \label{fig:realfirst}
% \end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/real_78.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset SIDD \cite{SIDD_2018_CVPR} example images with real noise.} 
 \label{fig:realfirst}
\end{figure*}

% \begin{figure*}[!hbtp]
%     \centering
%     \includegraphics[width=\textwidth, trim=10 0 150 200,clip]{figures/real/real_85.jpg}
% \end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/real_97.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset SIDD \cite{SIDD_2018_CVPR} example images with real noise.} 
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/real_151.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset SIDD \cite{SIDD_2018_CVPR} example images with real noise.} 
\end{figure*}

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/real_159.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset SIDD \cite{SIDD_2018_CVPR} example images with real noise.} 
 
\end{figure*}


\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/PolyU_data2.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset PolyU \cite{xu2018real} example images with real noise.} 

 
\end{figure*} 

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/PolyU_data6.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset PolyU \cite{xu2018real} example images with real noise.} 

 
\end{figure*} 

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/PolyU_data13.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset PolyU \cite{xu2018real} example images with real noise.} 

 
\end{figure*} 

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/real/PolyU_data7.jpg}
	\caption{Visual comparison of image denoising methods on real noisy image dataset PolyU \cite{xu2018real} example images with real noise.} 

 \label{fig:reallast}
\end{figure*} 







% \end{appendices}

% % {\small
% % 	\bibliographystyle{../ieee_fullname}
% % 	\bibliography{../main}
% % }


% \end{document}

The paper proposes a novel self-supervised learning method for image denoising called Multi-view Self-supervised Disentanglement (MeD). The key idea is to learn a shared latent representation of the underlying clean image from multiple corrupted views of the same static scene. 

Specifically, MeD takes two corrupted versions of an image as input and disentangles them into separate latent representations for the shared content (scene) and unique distortions (noise). It uses an encoder-decoder architecture with a shared content encoder, separate noise encoders, and cross decoders. 

Several losses are used to enforce disentanglement and consistency of the latent spaces:

1) Scene and noise reconstruction losses to reconstruct the inputs.

2) Cross-reconstruction loss to reconstruct inputs from crossed latent codes, enforcing disentanglement. 

3) Bernoulli manifold mixture loss to enforce consistency of shared content latent space.

The disentangled content latent is decoded to reconstruct the denoised image


## What problem or question is the paper addressing?

 This paper is addressing the problem of image denoising, specifically focusing on developing a method that can generalize well to handle complex, unseen noise distributions. The key questions it aims to tackle are:

1. How can we develop an image denoising method that does not rely on paired clean/noisy training data and can work in a purely self-supervised manner using only noisy data? 

2. How can we improve the generalization ability of denoising methods to handle unfamiliar, complex noise distributions beyond what they are trained on?

3. Can we achieve competitive or even superior performance compared to existing supervised denoising methods without access to clean ground truth images?

4. Can we effectively disentangle the latent clean image content from the noise corruption in a learned feature space in a completely self-supervised manner?

To address these questions, the paper proposes a new self-supervised learning framework called Multi-view Self-supervised Disentanglement (MeD). The key ideas are:

- Utilize multiple corrupted views of the same image during training rather than a single view. This provides more signals about the underlying clean image.

- Perform feature disentanglement by explicitly separating content and noise features in the latent space. The content features are encouraged to be consistent across views while noise is separated. 

- Use a self-supervised loss to train with only noisy images, without requiring clean data.

The proposed MeD method is shown to outperform prior self-supervised methods and even competitive supervised techniques on unseen complex noise distributions. It demonstrates a new way to effectively learn to denoise images in a practical self-supervised manner.

In summary, the paper aims to push the boundaries of self-supervised image denoising to handle complex real-world noise in a robust way without relying on clean training data. The core innovation is the multi-view disentanglement framework.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts covered in this paper include:

- Image denoising - The main focus of the paper is on image denoising, which is the process of removing noise from corrupted images to recover clean images.

- Self-supervised learning - The proposed method adopts a self-supervised learning approach for image denoising, where noisy images are used for training without requiring paired clean images. 

- Disentanglement - A core idea in the paper is disentangling the underlying scene representation from the noise component in the feature space.

- Multi-view learning - The method uses multiple corrupted views of the same scene image during training to exploit consistency across views.

- Generalization - A major emphasis is on generalization ability to handle complex real-world noise and unseen noise distributions, without overfitting to the training noise types.

- Noise modeling - Different noise models like AWGN, Poisson, Speckle etc. are used to synthesize noisy training data. Robustness to these noise types is evaluated.

- Benchmarking - Performance is benchmarked extensively on standard datasets like CBSD68, SIDD, and real-world noisy images.

- Ablation studies - Ablation experiments analyze impact of different loss terms and hyperparameters.

So in summary, the key terms cover self-supervised multi-view disentangled representation learning, generalization to unseen and real noise, benchmarking on public datasets, and ablation studies.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of a research paper:

1. What is the research question or problem being addressed in this paper? 

2. What are the key goals or objectives of this research?

3. What methods were used to conduct the research and collect/analyze data? 

4. What were the major findings or results of the research?

5. What conclusions did the authors draw based on the results?

6. What are the limitations or weaknesses of the research methods and analysis?

7. How do the findings relate to or build upon previous research in this field? 

8. What are the major implications or significance of the research findings?

9. What recommendations do the authors make for future research based on their findings?

10. How well did the authors articulate their arguments and communicate the research effectively?

Asking these types of questions will help summarize the key information, contributions, and implications of a research paper. The questions cover the research problem, goals, methods, findings, conclusions, limitations, connections to prior work, significance, future directions, and overall communication quality. Answering these questions provides the basis for crafting a comprehensive summary of the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a new self-supervised learning framework called MeD. What is the key intuition behind using a multi-view approach for image denoising in a self-supervised manner? How does this differ from prior work?

2. The paper aims to disentangle the latent clean image features from the noise features. Explain the proposed approach for achieving this disentanglement in detail. What are the different components and objectives? 

3. The method assumes certain properties about the latent space, like independence, consistency and composability. Elaborate on these properties and how the proposed framework is designed to satisfy them. 

4. One of the key components of MeD is the cross-disentanglement decoder R. Explain its purpose and how it enforces the disentanglement of latent codes z and u. What is the significance of using features from different views here?

5. The Bernoulli Manifold Mixture (BMM) is used to enhance interchangeability between scene features from different views. Provide an intuitive explanation of how BMM works and how it helps achieve this goal.

6. Compare and contrast the proposed multi-view self-supervised approach with single image based methods like DIP and Noise2Noise. What are the limitations of single image methods that MeD aims to address?

7. The paper demonstrates superior performance on real image denoising tasks without any real training data. Analyze the possible reasons why MeD generalizes well to real noise distributions despite being trained only on synthetic data.

8. What design choices were made in MeD to avoid trivial solutions and mode collapse issues? E.g. using paired views, injecting noise, cross-disentanglement etc.

9. The method seems to benefit more from a diverse noise pool compared to other techniques. Provide possible explanations for why this might be the case.

10. The framework can be expanded to incorporate more views. Discuss the tradeoffs between using more views vs computational efficiency. How does the performance vary with number of views?
