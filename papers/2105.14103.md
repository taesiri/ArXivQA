# [An Attention Free Transformer](https://arxiv.org/abs/2105.14103)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:Can a Transformer model achieve competitive performance without relying on dot product attention, which has quadratic complexity? The paper proposes a new module called Attention Free Transformer (AFT) that replaces dot product attention with a more efficient operation while maintaining the benefits of global connectivity. The key hypotheses tested are:1) AFT can achieve comparable or better performance to standard Transformers on various tasks including image modeling, language modeling, and image classification.2) AFT is much more computationally and memory efficient compared to standard Transformers and other efficient Transformer variants.3) Design choices like factorization and localization of the position biases in AFT lead to improved performance and efficiency.4) AFT demonstrates interesting properties like the ability to handle variable sequence lengths and compatibility with standard Transformers.In summary, the central hypothesis is that AFT can match or exceed the performance of dot product attention Transformers while being far more efficient, through the use of a novel attention-free operation. The paper provides extensive experiments to validate this hypothesis across different tasks and model configurations.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Proposing Attention Free Transformer (AFT), a new efficient variant of Transformers that eliminates the need for dot product self attention. - Introducing two model variants AFT-local and AFT-conv that incorporate notions of locality and spatial weight sharing while maintaining global connectivity.- Demonstrating through experiments on image autoregressive modeling, language modeling, and image classification that AFT can match or exceed the performance of standard Transformers and other efficient variants, while providing improved efficiency in terms of computational complexity and memory usage.In summary, the paper proposes a novel attention-free mechanism to replace the expensive dot product attention in Transformers. The experiments across diverse tasks highlight AFT's competitive performance and efficiency gains compared to prior work. The overall contribution is a new family of efficient Transformer models that retain global connectivity without relying on attention.
