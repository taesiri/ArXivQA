# [Backpropagation Path Search On Adversarial Transferability](https://arxiv.org/abs/2308.07625)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the transferability of adversarial attacks against black-box models by searching for optimal backpropagation paths? The key hypotheses are:1) Modifying the backpropagation path (e.g. adding skip connections) during adversarial attack generation can reduce overfitting to the surrogate model and improve transferability.2) Searching the space of possible backpropagation paths using Bayesian optimization can find better paths than heuristic or random search.3) Evaluating paths by their one-step attack success rate against a validation model is an effective approximation for finding the most transferable paths. To test these hypotheses, the authors propose:- SkipConv to add skip connections to convolution layers - A DAG search space combining skip connections for convolutions, activations, and residuals- Using Bayesian optimization to search this space based on one-step attack success against a validation modelThe overall framework is called PAS (backPropagation pAth Search) and aims to automate finding optimal backpropagation modifications to improve adversarial transferability in black-box attacks. The effectiveness of PAS is evaluated on ImageNet subsets against normally trained and robust models.In summary, the central research question is how to automatically search for optimal backpropagation paths to improve adversarial transferability, with the main hypotheses relating to using skip connections, Bayesian optimization, and one-step approximation. PAS is proposed as a framework to address this question.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes SkipConv, a method to adjust the backpropagation path of convolution modules by structural reparameterization. This allows modifying the gradient flow through convolutions to improve adversarial transferability. 2. It formulates the search for optimal backpropagation paths as a transferable backpropagation path search problem. It proposes a framework called PAS that uses Bayesian Optimization to search for optimal paths in a DAG-based search space.3. It constructs the search space by combining backpropagation paths of various modules like convolution, activation, and residuals. The paths are controlled using decay factors to explore different gradient flows.4. It uses a one-step approximation during the search to efficiently evaluate path transferability and reduce overhead.5. Experiments show PAS significantly improves attack success rates against both normally trained and defense models compared to prior art. The searched paths are highly transferable and generalize across different model architectures.In summary, the key novelty is formulating backpropagation path search as an optimization problem, constructing a flexible DAG-based search space, and efficiently searching for optimal paths using Bayesian Optimization and one-step approximation. This allows automatically finding highly transferable paths rather than manual heuristic design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called BackPropagation pAth Search (PAS) to improve the transferability of adversarial examples by searching for optimal backpropagation paths in neural networks using Bayesian optimization and structural reparameterization of modules like convolution.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on adversarial transferability:- Focus on backpropagation path: This paper focuses specifically on modifying the backpropagation path to improve transferability, while many other works have explored different techniques like data/model augmentation, momentum, etc. The key novelty is optimizing the backpropagation path itself.- Structural reparameterization of convolution: Most prior work has focused on modifying backpropagation through residual connections or activation functions. This paper proposes a new method, SkipConv, to reparameterize convolutional layers to enable modifying their backpropagation as well. - Unified DAG search framework: The paper frames finding a good backpropagation path as a search problem on a DAG, and uses Bayesian optimization to do this search. This provides a unified way to optimize the backpropagation path that subsumes prior heuristic techniques.- One-step approximation for evaluation: To make the search tractable, the paper uses a one-step attack success rate as the evaluation metric rather than doing full iterative attacks. This approximation enables efficient search with little degradation in final transferability.- Comprehensive experiments: The paper does extensive experiments across different models, datasets, and defense techniques. The consistent and sizable improvements demonstrate the general applicability of the approach.In summary, the key novelty is in reformulating and solving the problem of optimizing backpropagation for transferability as a DAG search problem. This provides a more principled and unified approach compared to prior heuristic techniques, leading to stronger results, especially against defense models. The paper convincingly validates the efficacy of this approach across diverse scenarios.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring structural reparameterization for Transformer modules and using PAS to improve adversarial transferability between CNNs and Transformers. The current work focuses on CNN architectures, but the authors suggest extending the ideas to Transformers as well.- Finding better paths by using more advanced search algorithms and more effective search objectives. The current PAS framework uses Bayesian Optimization, but more sophisticated AutoML techniques could potentially find even better backpropagation paths. The search objective could also be improved beyond just using one-step attack success rate. - Expanding the backpropagation DAG with new proposed modules and augmentations. The current DAG combines skip connections for convolution, activation, and residual modules. Adding new types of skip connections or other structural modifications to the DAG could further improve the diversity and transferability.- Applying the ideas of PAS more broadly to additional domains beyond image classifiers. The framework could potentially be used to improve adversarial transferability in other areas like natural language processing.- Reducing the extra overhead required for the search process. Currently, PAS introduces additional computational overhead to search for the backpropagation path. Reducing this cost would make the approach more practical.In summary, the main future directions are: exploring other architectures like Transformers, improving the search algorithm and objective, expanding the DAG with new structural modifications, applying PAS more broadly, and reducing the computational overhead. The PAS framework provides a lot of flexibility for exploring improvements in transfer-based adversarial attacks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a method called Backpropagation Path Search (PAS) to improve the transferability of adversarial examples against black-box models. PAS focuses on adjusting the backpropagation path used when crafting adversarial examples against a surrogate model to make them less overfitted and more transferable. It introduces a technique called SkipConv that decomposes convolution layers into a skip connection and residual connection to allow modifying backpropagation. Then PAS formulates a search space of possible backpropagation paths through a model as a directed acyclic graph (DAG) combining skip connections for convolution, activation, and residual layers. It uses Bayesian optimization to search this DAG to find optimal paths that maximize transferability as evaluated by attack success rate against a validation model. Experiments show PAS substantially improves transferability of attacks against both normally trained and defense models compared to prior state-of-the-art methods. The searched paths focus gradients on more transferable features related to the target class. PAS provides a unified framework for automatically searching over backpropagation modifications to boost transferability of adversarial attacks.
