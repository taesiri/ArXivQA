# [Backpropagation Path Search On Adversarial Transferability](https://arxiv.org/abs/2308.07625)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we improve the transferability of adversarial attacks against black-box models by searching for optimal backpropagation paths? 

The key hypotheses are:

1) Modifying the backpropagation path (e.g. adding skip connections) during adversarial attack generation can reduce overfitting to the surrogate model and improve transferability.

2) Searching the space of possible backpropagation paths using Bayesian optimization can find better paths than heuristic or random search.

3) Evaluating paths by their one-step attack success rate against a validation model is an effective approximation for finding the most transferable paths. 

To test these hypotheses, the authors propose:

- SkipConv to add skip connections to convolution layers 

- A DAG search space combining skip connections for convolutions, activations, and residuals

- Using Bayesian optimization to search this space based on one-step attack success against a validation model

The overall framework is called PAS (backPropagation pAth Search) and aims to automate finding optimal backpropagation modifications to improve adversarial transferability in black-box attacks. The effectiveness of PAS is evaluated on ImageNet subsets against normally trained and robust models.

In summary, the central research question is how to automatically search for optimal backpropagation paths to improve adversarial transferability, with the main hypotheses relating to using skip connections, Bayesian optimization, and one-step approximation. PAS is proposed as a framework to address this question.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes SkipConv, a method to adjust the backpropagation path of convolution modules by structural reparameterization. This allows modifying the gradient flow through convolutions to improve adversarial transferability. 

2. It formulates the search for optimal backpropagation paths as a transferable backpropagation path search problem. It proposes a framework called PAS that uses Bayesian Optimization to search for optimal paths in a DAG-based search space.

3. It constructs the search space by combining backpropagation paths of various modules like convolution, activation, and residuals. The paths are controlled using decay factors to explore different gradient flows.

4. It uses a one-step approximation during the search to efficiently evaluate path transferability and reduce overhead.

5. Experiments show PAS significantly improves attack success rates against both normally trained and defense models compared to prior art. The searched paths are highly transferable and generalize across different model architectures.

In summary, the key novelty is formulating backpropagation path search as an optimization problem, constructing a flexible DAG-based search space, and efficiently searching for optimal paths using Bayesian Optimization and one-step approximation. This allows automatically finding highly transferable paths rather than manual heuristic design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new method called BackPropagation pAth Search (PAS) to improve the transferability of adversarial examples by searching for optimal backpropagation paths in neural networks using Bayesian optimization and structural reparameterization of modules like convolution.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on adversarial transferability:

- Focus on backpropagation path: This paper focuses specifically on modifying the backpropagation path to improve transferability, while many other works have explored different techniques like data/model augmentation, momentum, etc. The key novelty is optimizing the backpropagation path itself.

- Structural reparameterization of convolution: Most prior work has focused on modifying backpropagation through residual connections or activation functions. This paper proposes a new method, SkipConv, to reparameterize convolutional layers to enable modifying their backpropagation as well. 

- Unified DAG search framework: The paper frames finding a good backpropagation path as a search problem on a DAG, and uses Bayesian optimization to do this search. This provides a unified way to optimize the backpropagation path that subsumes prior heuristic techniques.

- One-step approximation for evaluation: To make the search tractable, the paper uses a one-step attack success rate as the evaluation metric rather than doing full iterative attacks. This approximation enables efficient search with little degradation in final transferability.

- Comprehensive experiments: The paper does extensive experiments across different models, datasets, and defense techniques. The consistent and sizable improvements demonstrate the general applicability of the approach.

In summary, the key novelty is in reformulating and solving the problem of optimizing backpropagation for transferability as a DAG search problem. This provides a more principled and unified approach compared to prior heuristic techniques, leading to stronger results, especially against defense models. The paper convincingly validates the efficacy of this approach across diverse scenarios.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Exploring structural reparameterization for Transformer modules and using PAS to improve adversarial transferability between CNNs and Transformers. The current work focuses on CNN architectures, but the authors suggest extending the ideas to Transformers as well.

- Finding better paths by using more advanced search algorithms and more effective search objectives. The current PAS framework uses Bayesian Optimization, but more sophisticated AutoML techniques could potentially find even better backpropagation paths. The search objective could also be improved beyond just using one-step attack success rate. 

- Expanding the backpropagation DAG with new proposed modules and augmentations. The current DAG combines skip connections for convolution, activation, and residual modules. Adding new types of skip connections or other structural modifications to the DAG could further improve the diversity and transferability.

- Applying the ideas of PAS more broadly to additional domains beyond image classifiers. The framework could potentially be used to improve adversarial transferability in other areas like natural language processing.

- Reducing the extra overhead required for the search process. Currently, PAS introduces additional computational overhead to search for the backpropagation path. Reducing this cost would make the approach more practical.

In summary, the main future directions are: exploring other architectures like Transformers, improving the search algorithm and objective, expanding the DAG with new structural modifications, applying PAS more broadly, and reducing the computational overhead. The PAS framework provides a lot of flexibility for exploring improvements in transfer-based adversarial attacks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a method called Backpropagation Path Search (PAS) to improve the transferability of adversarial examples against black-box models. PAS focuses on adjusting the backpropagation path used when crafting adversarial examples against a surrogate model to make them less overfitted and more transferable. It introduces a technique called SkipConv that decomposes convolution layers into a skip connection and residual connection to allow modifying backpropagation. Then PAS formulates a search space of possible backpropagation paths through a model as a directed acyclic graph (DAG) combining skip connections for convolution, activation, and residual layers. It uses Bayesian optimization to search this DAG to find optimal paths that maximize transferability as evaluated by attack success rate against a validation model. Experiments show PAS substantially improves transferability of attacks against both normally trained and defense models compared to prior state-of-the-art methods. The searched paths focus gradients on more transferable features related to the target class. PAS provides a unified framework for automatically searching over backpropagation modifications to boost transferability of adversarial attacks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a method called backPropagation pAth Search (PAS) to improve the transferability of adversarial examples against black-box models. Adversarial attacks often rely on crafting examples against surrogate models and transferring them to the target model. However, existing approaches suffer from two issues: 1) they do not fully explore adjusting the backpropagation through convolutional layers which are important for feature extraction, and 2) they use heuristic methods to modify the backpropagation path which leads to suboptimal results. 

To address these issues, the authors first propose SkipConv which decomposes convolutional kernels into a skip kernel and residual kernel. This allows modifying backpropagation through convolutional layers while keeping the forward pass unchanged. Second, they formulate a search space over possible backpropagation paths through the network as a directed acyclic graph. Bayesian optimization is used to efficiently search for the optimal path that maximizes transferability. Experiments demonstrate PAS significantly outperforms prior methods, improving success rates against normally trained models by 5-20% and by even larger margins against defense models. The results show the efficacy of directly searching over backpropagation paths to find portions that generalize across models.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method called backPropagation pAth Search (PAS) to improve the adversarial transferability of black-box attacks. The key ideas are:

1. Propose SkipConv to modify the backpropagation path through convolution layers by decomposing the convolution kernel into a skip kernel and a residual kernel. This allows controlling the amount of gradient flow through the convolution layer. 

2. Construct a directed acyclic graph (DAG) that combines the backpropagation paths through convolution, activation, and residual modules as the search space. 

3. Use Bayesian Optimization to search the DAG for the optimal backpropagation path that maximizes adversarial transferability. The path is evaluated using a one-step attack success rate on a validation dataset.

4. Craft adversarial examples using the searched optimal path against the surrogate model, which transfers better to unseen victim models compared to heuristically designed paths.

In summary, PAS searches for the optimal backpropagation path through a DAG that combines skip connections in different modules to maximize adversarial transferability from a surrogate model to black-box victim models. The core novelty is the formulation as a search problem and decomposing convolution for path modification.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper focuses on transfer-based adversarial attacks, where adversarial examples are crafted against a surrogate model and then transferred to attack black-box victim models. 

- Existing structure-based attackers modify the backpropagation graph to reduce overfitting to the surrogate model. However, they have two main limitations:
    1) They do not adjust the backpropagation of convolution modules, missing important shared features.
    2) They heuristically modify the graph in predefined ways, which may not be optimal.

- To address these limitations, the paper proposes Backpropagation Path Search (PAS):

    1) They introduce SkipConv to decompose convolution kernels into a skip connection part and residual part, enabling modifying convolution backpropagation.
    
    2) They construct a search space of possible backpropagation DAGs combining skip connections in various modules. 
    
    3) They use Bayesian optimization to search this space for the optimal graph based on transferability estimated by a one-step attack.
    
- Experiments show PAS significantly improves transferability against normally trained and defense models compared to prior heuristic structure-based attackers.

In summary, the key contribution is using a principled search strategy to find optimal backpropagation graphs for transferable adversarial attacks, outperforming prior heuristic approaches. The core problems addressed are improving transferability by modifying backpropagation and doing so in an automated, non-heuristic way.
