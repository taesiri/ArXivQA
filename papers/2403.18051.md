# [Supervisory Prompt Training](https://arxiv.org/abs/2403.18051)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Performance of large language models (LLMs) like GPT-4 relies heavily on the quality of prompts, which are often manually engineered in a costly and non-scalable way. 
- Existing prompts are also brittle, not working well when LLMs change even slightly.

Proposed Solution: 
- The authors propose a new method called Supervisory Prompt Training (SPT) to automatically generate highly effective prompts using two LLMs working together. 
- One LLM called the generator performs a task while prompted. The other LLM called the corrector provides feedback on mistakes and generates improved prompts for the generator.
- The key innovation is that both LLMs collaboratively and continuously improve their prompts over time. 
- The authors also introduce "impact scores" to quantify sentence-level effectiveness of prompts.

Contributions:
- SPT allows prompts to be automatically optimized to enhance LLM performance and reduce hallucinations.
- Both the generator and corrector LLMs improve their prompting abilities over time.
- Impact scores give a metric to determine which sentences in prompts are most useful.
- SPT achieved major boosts in accuracy over baseline models, demonstrating the method's ability to reduce hallucinations. For example, GPT-4's accuracy on the GSM8K benchmark increased from 65.8% to 94.1%, a 28.3% boost.
- SPT advances the field by offering an efficient, scalable way to improve LLMs without model fine-tuning.

In summary, the paper introduces a novel dual-LLM framework called SPT that collaboratively and iteratively enhances prompts to bolster performance and mitigate hallucinations in LLMs. The method has shown significant accuracy gains over baseline models.
