# [Principled Weight Initialisation for Input-Convex Neural Networks](https://arxiv.org/abs/2312.12474)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Input-convex neural networks (ICNNs) are neural networks that guarantee convexity in the input-output mapping. This convexity is useful for tasks like energy-based modeling, optimal transport, and learning invariances.
- However, training of ICNNs tends to be slow due to poor initialization strategies that do not account for the non-negative weights in ICNNs. Standard initialization methods assume centered weight distributions, which is not possible with non-negative weights.

Proposed Solution:
- The authors generalize signal propagation theory to remove the assumption that weights are sampled from a centered distribution. This allows analyzing the effects of non-negative weights on activation statistics.
- Using the generalized theory, the authors derive a principled initialization strategy for ICNNs that stabilizes the propagation of mean, variance and correlation of activations throughout the layers.
- The initialization samples non-negative weights from a log-normal distribution with carefully tuned parameters and biases that center the activations.

Main Contributions:
- Generalization of standard signal propagation theory to non-centered weight distributions
- Principled initialization method for ICNNs based on the generalized theory
- Empirical demonstration that the proposed initialization accelerates ICNN training and improves generalization
- Finding that with proper initialization, ICNNs can be trained without skip-connections, simplifying architectures
- Application of ICNNs to controlled latent space exploration in drug discovery

In summary, the paper proposes a theoretically grounded initialization strategy to enable more effective training of input-convex neural networks. Experiments confirm faster convergence and state-of-the-art performance across multiple tasks.
