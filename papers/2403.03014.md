# [The Case for Evaluating Multimodal Translation Models on Text Datasets](https://arxiv.org/abs/2403.03014)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multimodal machine translation (MMT) models are typically evaluated only on the Multi30k dataset, which consists of short image captions. This evaluation methodology is inadequate because:
  1) It does not measure whether models actually use visual information
  2) It does not test translation of complex sentences
  3) Most captions are unambiguous and do not require visual context

Proposed Solution:
- Evaluate MMT models on three test sets:
  1) CoMMuTE: Measures use of visual context for disambiguation
  2) WMT News Task: Contains complex sentences from news data
  3) Multi30k: Tests performance on real MMT task
- This evaluates both use of visual information and translation ability

Contributions:
- Proposes more comprehensive MMT evaluation using CoMMuTE, WMT and Multi30k
- Evaluates state-of-the-art MMT models on this framework 
- Shows that while these models perform well on Multi30k, they struggle on WMT complex sentences
- Suggests MMT models should have high text-only translation performance before multimodal training

In summary, this paper argues that evaluating MMT models solely on Multi30k is inadequate. It proposes a more comprehensive evaluation approach using three test sets. Experiments show current MMT models fail to translate complex sentences. The results demonstrate the need to enforce text-only performance and better model training procedures for advanced MMT.
