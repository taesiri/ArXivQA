# [Megatron-LM: Training Multi-Billion Parameter Language Models Using   Model Parallelism](https://arxiv.org/abs/1909.08053)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

1) Can simple model parallelism techniques enable training of transformer-based language models with billions of parameters, using existing PyTorch frameworks without custom compilers or libraries?

2) How does model scaling impact accuracy on downstream NLP tasks for both left-to-right transformers like GPT-2 and bidirectional transformers like BERT? 

3) Can careful rearrangement of layer normalization in BERT-style transformers enable continued accuracy improvements with increased model size, overcoming limitations noted in prior work?

4) Can scaled up versions of GPT-2 and BERT establish new state-of-the-art results on benchmark NLP datasets?

The authors aim to demonstrate that with their proposed simple model parallel approach, they can efficiently train very large transformer models on existing frameworks. They also want to empirically show that bigger models lead to better accuracy on downstream tasks, for both GPT-2 and BERT architectures. A key hypothesis is that modifying layer normalization in BERT can enable this increased performance with model scaling. The overarching goal is to push state-of-the-art results on NLP benchmarks by training the largest GPT-2 and BERT models to date.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can large-scale transformer language models be efficiently trained using model parallelism, and do larger models result in improved performance on downstream natural language tasks?

Specifically, the authors aim to:

- Implement an efficient intra-layer model parallel approach that enables training transformer models with billions of parameters, using only minimal modifications to existing PyTorch code.

- Empirically analyze the scaling efficiency of their approach up to 512 GPUs. 

- Study the effect of scaling up model size on the accuracy of downstream tasks, for both GPT-2 and BERT-style models.

- Show that properly rearranging layer normalization in BERT-style models is critical for good accuracy as model size increases.

- Demonstrate state-of-the-art results using their largest GPT-2 (8.3 billion parameters) and BERT (3.9 billion parameters) models on several language modeling datasets and tasks.

So in summary, the central hypothesis is that larger transformer language models, trained efficiently via model parallelism, can achieve improved performance on language tasks. The paper aims to demonstrate this through empirical analysis and state-of-the-art results using their proposed training method.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a simple and efficient intra-layer model parallel approach to train large transformer models with billions of parameters. The key points are:

- They implement model parallelism in PyTorch with just a few modifications, without needing a new framework or compiler. This is done by exploiting the inherent parallelism in the multi-head attention and feedforward layers of transformers. 

- They achieve up to 15.1 PFLOPS sustained on 512 GPUs with up to 8-way model parallelism, demonstrating excellent scaling efficiency.

- They show careful rearrangement of layer normalization in BERT-like models is critical for good accuracy as model size increases. 

- They train GPT-2 models up to 8.3 billion parameters and BERT models up to 3.9 billion parameters, achieving state-of-the-art results on WikiText103, LAMBADA, and RACE datasets.

- They demonstrate that increasing model size leads to better performance on downstream tasks for both GPT-2 and BERT models.

In summary, the main contribution is presenting an efficient and easy-to-implement approach to model parallelism that enables training much larger transformer models, leading to improved accuracy, and achieving state-of-the-art results. The simplicity of the modifications is a key advantage over other model parallel approaches.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

- They implement a simple and efficient model parallel approach for training large transformer models by making only a few targeted modifications to an existing PyTorch transformer implementation. This allows training models with billions of parameters without needing a custom compiler or framework.

- They perform an in-depth analysis of their model and data parallel techniques and demonstrate good scaling efficiency up to 512 GPUs. 

- They show that rearranging the layer normalization and residual connections in BERT-style models is critical for improving performance as model size increases. 

- They demonstrate that scaling up both GPT-2 (up to 8.3B parameters) and BERT (up to 3.9B parameters) transformer models leads to improved performance on downstream tasks.

- Their largest GPT-2 model achieves state-of-the-art results on the WikiText-103, LAMBADA, and RACE datasets. Their largest BERT model achieves state-of-the-art on the RACE dataset.

- They open source their implementation to enable further research on model parallel transformers.

In summary, the main contributions are around their efficient model parallel approach, scaling analysis, improvements to training large BERT models, and demonstrating state-of-the-art results on several benchmarks using their largest GPT-2 and BERT models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a simple and efficient approach to training large transformer models up to 8.3 billion parameters on 512 GPUs using model parallelism in PyTorch, and shows state-of-the-art results on several language modeling tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper presents a simple and efficient intra-layer model parallel approach for training large transformer models with billions of parameters, enabling state-of-the-art results on several language modeling benchmarks.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in language modeling and large-scale model training:

- Scale of Models Trained: This paper trains extremely large language models, up to 8.3 billion parameters. This is significantly larger than previous work like BERT and GPT-2, which were in the hundreds of millions of parameters. The scale demonstrates the capability of their proposed model parallel approach.

- Model Parallelism Approach: The model parallel approach in this paper is simple and requires minimal code changes compared to existing frameworks like Mesh TensorFlow. It shows model parallelism can be efficient without complex compilation and execution frameworks. However, other work has shown more optimized pipelining approaches. 

- Scaling Efficiency: The paper demonstrates good scaling efficiency up to 512 GPUs. Other work has shown higher efficiency but for smaller model sizes. This paper pushes parallel efficiency to unprecedented model sizes.

- Model Architectures: This paper trains both auto-regressive (GPT-style) and auto-encoding (BERT-style) transformer models. Most prior work focused on one or the other. The techniques apply to both broad model families.

- Rearranging BERT Architecture: An important finding is that rearranging BERT's layers improves scaling. This was not explored in detail before and is important for training huge BERT models.

- State-of-the-Art Results: The large models achieve new state-of-the-art results on several language modeling datasets, demonstrating the value of scale. However, other techniques like sparse attention also improve results.

Overall, this paper makes solid contributions in training giant language models using simple but efficient model parallelism. The scale and efficiency results are impressive. However, there is still much room for innovation in frameworks, optimizations, and alternate approaches to large-scale modeling.


## How does this paper compare to other research in the same field?

 Here are a few key things to compare and contrast about this paper with other research in model parallelism for large language models:

- The approach taken is simple and efficient, only requiring a few modifications to existing PyTorch transformer code rather than a whole new compiler or framework like some other works. This makes it more accessible.

- It focuses on intra-layer model parallelism specifically, whereas pipeline model parallelism as in GPipe is a separate approach. The authors position their method as complementary to pipeline techniques.

- The scaling results they demonstrate up to 8.3 billion parameters on 512 GPUs are quite impressive. Other works may not have achieved the same level of strong and weak scaling efficiency. 

- They provide a thorough empirical analysis of how model performance improves with size, including adjusting BERT to continue improving with size. Other works like ALBERT found degradement beyond a point.

- The model sizes trained and the resulting SOTA results achieved set new benchmarks compared to previous bests, especially in the bi-directional BERT-style model.

- The code and training pipelines are open sourced, allowing replication and extension by others. Other model parallel works may not have released code.

So in summary, this paper pushes forward model parallelism for transformers in terms of efficiency, scaling, model sizes achieved, model quality improvements, and practical accessibility of the methods. The empirical analyses and new SOTA results help validate the usefulness of the approaches proposed.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Continuing to scale up pretraining by training even larger language models. The authors mention going up to 16 billion or more parameters would require hybrid intra-layer and inter-layer model parallelism.

- Pretraining different model architectures besides transformers, like XLNet and T5.

- Evaluating performance of large pretrained models on more difficult and diverse downstream tasks beyond GLUE - e.g. question answering, summarization, conversation.

- Using knowledge distillation to train smaller student models that can mimic the capabilities of the very large teacher models.

- Improving efficiency and reducing memory footprint of optimizers to enable larger batch sizes and model sizes.

- Exploring model parallel training of other model families besides transformers, like RNNs.

- Combining model parallelism with pipeline model parallelism techniques like in GPipe to further increase scale.

So in summary, the main directions are around continuing to scale model size, evaluating on more tasks, using distillation, improving optimization and efficiency, and expanding beyond just transformers. The authors see model parallelism as enabling continued progress in pretraining by overcoming memory limitations.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Continuing to scale up pretraining by training even larger language models. They mention that going beyond 16 billion parameters would likely require a hybrid of intra-layer and inter-layer model parallelism.

- Pretraining different model families like XLNet and T5. The authors focus on BERT and GPT-2 style models in this work.

- Evaluating performance of large pretrained models on more diverse and difficult downstream tasks beyond the ones studied in the paper, such as generative question answering, summarization, and conversation modeling.

- Using knowledge distillation to train smaller student models that can mimic the capabilities of the very large pretrained teacher models. This could help serve models on edge devices with limited compute. 

- Studying different techniques like parameter sharing that can help scale up models without drastically increasing compute requirements.

- Exploring model parallel approaches that go beyond the intra-layer parallelism studied here, such as pipelined model parallelism.

- Improving efficiency and memory usage of optimizers to support larger batch sizes and models.

So in summary, the main directions mentioned are 1) continuing to scale model size, 2) evaluating on more tasks, 3) distillation 4) alternate model parallelism approaches 5) efficiency improvements to enable larger models. The authors lay out several interesting avenues to build on their work on scalable model parallel transformer pretraining.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents Megatron-LM, a method for training large transformer-based language models using model parallelism. The key ideas are 1) implementing efficient intra-layer model parallelism in PyTorch by splitting tensor operations like GEMMs and attention across GPUs, 2) rearranging layer normalization in BERT to enable stable training of large models, 3) training transformer models up to 8.3 billion parameters using up to 512 GPUs with up to 76% scaling efficiency, 4) showing that larger GPT-2 and BERT models achieve state-of-the-art results on several language tasks including WikiText-103, LAMBADA, and RACE. The technical contributions include model parallelism techniques for attention and other operations, scaling results on up to 512 GPUs, and state-of-the-art natural language processing results. Overall, this paper pushes the frontier of large transformer model training using an efficient and simple model parallel approach in PyTorch.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents techniques for training large transformer language models using model parallelism. The authors implemented a simple and efficient intra-layer model parallel approach in PyTorch that enables training models with billions of parameters by splitting the layers across GPUs. They empirically analyze the scaling efficiency of their approach and achieve up to 76% weak scaling efficiency on 512 GPUs compared to a strong single GPU baseline. The authors show that careful rearrangement of layer normalization in BERT-style models is important for good performance as model size increases. They train GPT-2 models up to 8.3 billion parameters and BERT models up to 3.9 billion parameters and evaluate them on several datasets, achieving state-of-the-art results on WikiText-103, LAMBADA, and RACE. Overall, this work demonstrates highly efficient model parallel training of transformer language models at unprecedented scale, and shows continued accuracy improvements as model size increases. The code has been open-sourced to enable future research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents Megatron-LM, a method for training large transformer-based language models using model parallelism. The key ideas are:

1) They implement a simple and efficient model parallel approach in PyTorch by making only a few targeted modifications to existing PyTorch transformer code. This splits the model across GPUs without needing a custom framework or compiler. 

2) They empirically demonstrate excellent weak scaling results up to 8.3 billion parameters across 512 GPUs. This includes sustaining 15.1 PetaFLOPs across the full application with 76% scaling efficiency compared to a strong single GPU baseline.

3) They show that careful rearrangement of layer normalization in BERT-style models allows them to scale much larger compared to prior work. 

4) They train GPT-2 models up to 8.3 billion parameters and BERT models up to 3.9 billion parameters and achieve state-of-the-art results on WikiText-103, LAMBADA, and RACE datasets, showing the value of larger models.

In summary, the key contributions are implementing efficient model parallelism in PyTorch, empirically demonstrating scaling to over 15 PetaFLOPs on 512 GPUs, identifying fixes to scale BERT-style models larger, and achieving new SOTA results through scaling model sizes. The techniques presented enable training models that were previously infeasible, unlocking further progress in natural language processing.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes Megatron-LM, a method for training large transformer language models with billions of parameters using model parallelism. The key ideas are:

1) They implement efficient intra-layer model parallelism for transformer models by splitting matrix multiplications across GPUs and minimizing communication. This only requires adding a few communication primitives to native PyTorch code. 

2) They studied scaling up to 8.3 billion parameters on 512 GPUs and achieved up to 15.1 PetaFLOPs sustained performance, which is 76% scaling efficiency compared to a strong single GPU baseline.

3) They showed careful rearrangement of layer normalization in BERT-style models is critical to prevent degradation with larger sizes. 

4) They demonstrated state-of-the-art results by scaling up GPT-2 to 8.3B parameters and BERT to 3.9B parameters. The larger GPT-2 model achieved new SOTA perplexity on WikiText-103 and accuracy on the LAMBADA dataset. The 3.9B BERT model achieved SOTA accuracy on the RACE benchmark.

In summary, the work shows intra-layer model parallelism can efficiently scale transformer models to billions of parameters using only minor code changes. This enables training significantly larger language models that advance state-of-the-art across multiple NLP tasks. The techniques could be useful for scaling up other large transformer models in the future.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a simple and efficient intra-layer model parallel approach to train large transformer-based language models in PyTorch. The key idea is to split the layers of the transformer across multiple GPUs, while minimizing the communication needed between GPUs. Specifically, they carefully partition the matrix multiplications in the self-attention and feedforward layers across GPUs to fuse groups of operations and reduce synchronization. This enables training models with billions of parameters using only a few extra all-reduce communication operations in the forward and backward passes. For example, they train an 8.3 billion parameter GPT-2 model using 8-way model parallelism on 512 GPUs. The approach uses native PyTorch and standard NCCL primitives, with no custom compilers or frameworks needed. They show strong scaling results up to 512 GPUs for both model parallel and model+data parallel training regimes.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper presents an intra-layer model parallel approach to train large transformer-based language models in PyTorch. The key idea is to split the layers of the transformer model across multiple GPUs to overcome memory constraints. This is done by carefully partitioning the matrix multiplications and operations in each transformer layer. For example, in the MLP block, the first GEMM is parallelized column-wise to allow independent GeLU nonlinearities without synchronization. The second GEMM takes the output of the parallel GeLUs directly. Similarly, in the self-attention block, the QKV projections are parallelized to distribute the multi-head attention. This approach only requires adding a few all-reduce communication operations to synchronize gradients while keeping most computation local. It allows scaling up models like GPT-2 and BERT to billions of parameters across hundreds of GPUs with minimal code changes. The method is shown to achieve up to 15 PetaFLOPS on 512 GPUs with 76% weak scaling efficiency for an 8.3 billion parameter GPT-2 model compared to a single GPU baseline. It is demonstrated that these large models improve performance on language modeling datasets like WikiText-103 and LAMBADA.


## What problem or question is the paper addressing?

 The paper is addressing the challenges of training very large transformer language models, and presents techniques to scale these models up to billions of parameters using model parallelism. 

Specifically, the key problems/questions addressed in the paper are:

- How to train transformer models that are too large to fit on a single GPU due to memory constraints. The paper presents an efficient intra-layer model parallel approach to distribute the model across multiple GPUs.

- Demonstrating strong scaling results for the model parallel approach, up to 8.3 billion parameters distributed across 512 GPUs. The paper analyzes scaling efficiency and shows up to 76% efficiency compared to a single GPU baseline.

- Studying the effect of model scaling on downstream task performance for both GPT-2 and BERT models. The paper shows that carefully scaling up model size leads to accuracy improvements on several NLP datasets. 

- Addressing model degradation issues that occur when naively scaling up BERT. The paper identifies issues with layer normalization placement and proposes a modification to enable training larger BERT models.

- Achieving state-of-the-art results on several benchmarks including WikiText-103, LAMBADA, and RACE using the largest GPT-2 and BERT models trained with the presented techniques.

So in summary, the key focus is on using model parallelism to scale up training of transformer language models, studying how this impacts model accuracy, and achieving new SOTA results through scaling model size.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper are:

- Model parallelism - The paper introduces a simple and efficient approach to model parallelism that enables training large transformer models by splitting them across multiple GPUs.

- Scalability - A major focus is demonstrating the scalability of the model parallel approach, training models up to 8.3 billion parameters across 512 GPUs.

- Efficiency - The paper analyzes scaling efficiency, demonstrating up to 76% scaling efficiency compared to a single GPU baseline.

- Transformer models - The techniques are applied to transformer-based language models like GPT-2 and BERT.

- BERT - Experiments demonstrate that modifying the layer normalization in BERT enables continued accuracy improvements as model size increases. 

- State-of-the-art results - Large models trained with the approach achieve SOTA results on datasets like WikiText-103, LAMBADA, and RACE.

- PyTorch - The model parallel approach is implemented efficiently in PyTorch, without needing custom frameworks or compilers.

- Language modeling - Pretraining large language models is a major focus, using techniques like GPT-2 and BERT.

- Downstream tasks - Models are evaluated on downstream NLP tasks like question answering and language inference.

- Model sizes - Models ranging from 355M parameters up to 8.3B parameters are trained and analyzed.

So in summary, the key terms cover model parallelism, transformer models, scalability, efficiency, PyTorch implementation, large language models, state-of-the-art results, and downstream task evaluation.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the title and general focus of the paper?

2. Who are the authors and what are their affiliations? 

3. What problem is the paper trying to solve? What gap does it aim to fill?

4. What are the key contributions or main findings of the research? 

5. What methods did the researchers use to conduct their study or experiments?

6. What were the main results or findings from the research?

7. Did the authors discuss any limitations or weaknesses of their work?

8. How does this research compare to prior work in the same field? How does it advance the state-of-the-art?

9. What conclusions or implications did the authors draw based on their results? 

10. Did the authors suggest any directions for future work? What open questions remain?

Asking these types of questions should help summarize the key information and contributions of the research paper in a comprehensive way. The questions cover the paper's background, goals, methods, findings, limitations, and implications to help understand it holistically.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a simple and efficient model parallel approach for training large transformer models in PyTorch. Can you elaborate on how the modifications enable training models with billions of parameters? What are the key ideas that make this approach efficient?

2. The paper shows excellent scaling results up to 8.3 billion parameters on 512 GPUs. What modifications were made to the transformer architecture to enable scaling to such large model sizes? How does the scaling efficiency compare to strong single GPU baselines?

3. The paper demonstrates that rearranging layer normalization and residual connections in BERT-style models is critical for training larger models. Why does the original BERT architecture result in degradation? How does changing the position of layer norm fix these issues?

4. The results show that increasing model size leads to better perplexity and downstream task performance for both GPT-2 and BERT models. Why do you think larger models perform better? What are the benefits and potential downsides of scaling up model size? 

5. The paper establishes new SOTA results on WikiText-103, LAMBADA, and RACE datasets. What novel techniques or training strategies do you think contributed to the improved performance? How could these be further advanced?

6. How does the proposed model parallelism approach compare to pipeline model parallelism techniques like GPipe? What are the tradeoffs between the two methods and when would you use one vs the other?

7. The paper uses a vocabulary size divisible by 128x the number of model parallel workers. Why is this important? How does vocabulary size impact the efficiency of model parallel training?

8. How does the paper handle issues like random number generation and dropout in model parallel regions? Why can this be challenging and what techniques are used to synchronize properly?

9. What hyperparameter configurations are used for the different model sizes? How were these selected and tuned? What effect does batch size have on model parallel scaling?

10. The paper demonstrates strong scaling results for a 1.2B parameter model. How much speedup is achieved by increasing number of GPUs? What are the limits to strong scaling using model parallelism?


## Summarize the paper in one sentence.

 The paper presents Megatron-LM, a method for training large transformer language models up to 8.3 billion parameters using model parallelism in PyTorch.


## Summarize the paper in one paragraphs.

 The paper presents a model parallel approach for training large transformer-based language models. The key ideas are:

- They implement intra-layer model parallelism in PyTorch by splitting the feedforward and attention computations across GPUs. This only requires adding a few communication primitives like all-reduce.

- They combine model parallelism with data parallelism for overall efficiency and scalability. Using this approach they train up to 8.3 billion parameter models on 512 GPUs.

- For GPT-2 models, they show increasing size from 355M to 8.3B parameters improves perplexity on WikiText-103 (from 19 to 10.8) and accuracy on LAMBADA (45% to 66.5%). The 8.3B model achieves state-of-the-art results on both.

- For BERT models, they find rearranging layer normalization is critical for good scaling. They train up to a 3.9B parameter BERT and achieve state-of-the-art accuracy on the RACE benchmark (90.9%).

- Overall, the work shows large transformer models continue to achieve better performance on NLP tasks, and presents an efficient approach to training them using model parallelism in PyTorch. The code is open-sourced.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a simple intra-layer model parallel approach for training large transformer models. How does this approach compare to other model parallelism techniques like pipeline parallelism? What are the trade-offs?

2. The method splits matrices like the key, query, and value matrices in the transformer self-attention computation. What considerations need to be made in terms of matrix dimensions and computational load when deciding how to split these matrices across GPUs?

3. The paper mentions duplicating things like layer normalization parameters and random number generators across GPUs to avoid communication. When might this duplication become problematic in terms of memory usage? How could this be avoided?

4. The method adds only a few communication operations like AllReduce. What are the performance implications of adding too many vs too few communication operations? How can the optimal number of communications be determined?

5. How does the proposed approach handle gradient synchronization during backpropagation? What techniques are used to make this communication efficient?

6. What modifications need to be made to the optimizer when using model parallelism to ensure parameters are updated correctly? How does the method proposed handle this?

7. How does the hybrid model + data parallelism approach coordinate gradient reductions across the different groups? What are the trade-offs vs doing only model or only data parallelism?

8. The method achieves high efficiency like 76% scaling across 512 GPUs. What are the likely bottlenecks limiting further scaling? How could the efficiency be improved further?

9. For the BERT models, rearrangement of layer normalization is critical. Why does this change enable scaling to larger models when the original architecture fails? What theories explain this?

10. The method focuses on transformer models. What considerations would be necessary to apply similar model parallelism approaches to other architectures like CNNs or RNNs? What are the limitations?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper introduces Megatron-LM, a framework for efficiently training large transformer-based language models with billions of parameters using model parallelism in PyTorch. The key technical contributions are: 1) Implementing simple and efficient intra-layer model parallelism in PyTorch by partitioning layers and using collective communication primitives like all-reduce. This approach achieves up to 15.1 PFLOPS sustained on 512 GPUs with up to 76% scaling efficiency. 2) Identifying that rearranging layer normalization and residual connections in BERT-style models is crucial for good scaling behavior. 3) Demonstrating state-of-the-art results by scaling up GPT-2 to 8.3B parameters (new SOTA results on WikiText-103 and LAMBADA) and BERT to 3.9B parameters (new SOTA on RACE dataset). The authors open source the code to allow training even larger pretrained language models in the future. Overall, the work shows the promise of scaling up model size for improving language modeling performance and provides an efficient framework to enable training such large models.
