# [Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations](https://arxiv.org/abs/2403.02051)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper studies the differential privacy guarantees of noisy stochastic gradient descent (SGD) and gradient descent (GD) algorithms, when heavy-tailed noise (e.g. alpha-stable noise) is added to the parameter updates. 

- Prior work has studied the privacy properties of noisy GD/SGD mostly under Gaussian noise. This paper provides the first differential privacy analysis for heavy-tailed SGD/GD.

- The analysis of heavy-tailed perturbations brings additional challenges compared to light-tailed noise, due to the unbounded higher order moments. 

Proposed Solution
- The paper develops a novel analysis based on studying the total variation (TV) distance between the stochastic processes of SGD/GD on a dataset and its neighboring dataset. Controlling this TV distance ensures differential privacy.  

- Several key technical results are proven, including the V-uniform ergodicity of the stochastic processes and bounding the one-step TV distances between the Markov transition kernels.

- These results are combined to obtain bounds on the overall TV distance between the SGD/GD iterates on original and neighboring datasets.

Main Results
- It is shown SGD/GD with alpha-stable noise achieves (0, Õ(1/n)) differential privacy under mild assumptions on the loss function, without needing bounded gradients or projection steps.

- The privacy guarantees match existing results for Gaussian SGD/GD up to logarithmic factors, suggesting heavy-tailed noise can be a viable alternative.

- The impact of heavier tails is small: the bounds get affected only by a constant factor as tail exponent α decreases from 2 to 1.

- Numerical experiments on synthetic data support the theory and analyze the utility under different tail exponents.
