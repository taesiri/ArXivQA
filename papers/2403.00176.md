# [SoD$^2$: Statically Optimizing Dynamic Deep Neural Network](https://arxiv.org/abs/2403.00176)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the challenge of optimizing dynamic deep neural networks (DNNs), where tensor shapes, sizes, and even the operators used can vary depending on the input. Such dynamism makes many existing optimizations like operator fusion and memory optimization difficult. The paper argues that most prior works either use static solutions incurring high overhead or runtime solutions that are expensive.

Solution:
The key idea is to classify DNN operators based on how the output shape relates to the input, leading to 4 types:
1) Input Shape Determined Output 
2) Input Shape Determined Output Shape  
3) Input Shape & Value Determined Output Shape
4) Execution Determined Output

Using this, the paper develops a dataflow analysis called Rank and Dimension Propagation (RDP) to infer shapes of tensors, even symbolically. RDP considers both forward and backward analysis iteratively.  

Then, leveraging RDP, the paper presents several optimizations:
1) Operator fusion 
2) Static execution order planning using graph partitioning and RDP information
3) Runtime memory allocation plan generation starting from peak memory locations 
4) Multi-version code generation for hotspot operators based on common shapes

Contributions:
- Comprehensive DNN operator dynamism classification
- Novel static analysis (RDP) to propagate shapes in dynamic DNNs  
- Set of optimizations like fusion, execution planning, memory planning and multi-version code generation enabled by RDP
- Extensive evaluation on 10 emerging dynamic DNNs showing up to 88% memory savings and 3.9x speedup over state-of-the-art frameworks

The techniques offer promise for optimizing inference across devices from data centers to mobile devices, especially in single-input scenarios. Future work includes handling dynamism in batched inference on GPUs and integration with model compression techniques for large language models.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a comprehensive framework called SoD^2 for optimizing dynamic deep neural networks, which uses a novel static dataflow analysis to propagate rank and dimension information and enable optimizations like operator fusion, execution planning, and multi-version code generation that improve efficiency on mobile devices.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. DNN Operator Classification: The paper classifies 150 common operators used in ONNX into 4 categories based on the degree of dynamism they exhibit and how the output shapes relate to the input shapes. The 4 categories are: Input Shape Determined Output, Input Shape Determined Output Shape, Input Shape & Value Determined Output Shape, and Execution Determined Output.

2. Data-Flow Analysis for Rank and Dimension Propagation (RDP): The paper develops a static analysis framework called RDP to propagate shape and size information through the computational graph of a DNN. RDP employs forward and backward iterative analysis and considers both known and symbolic constants as well as expressions involving them. 

3. Comprehensive Set of Static and Dynamic Optimizations: Using the results of the RDP analysis, the paper presents several optimizations for dynamic DNNs, including fused code generation, execution (order) planning, runtime memory allocation planning, and multi-version code generation.

4. Extensive Evaluation: The proposed techniques are implemented in a system called SoD^2 and evaluated extensively on 10 emerging dynamic DNN models, showing significant reductions in execution latency and memory requirements compared to state-of-the-art DNN execution frameworks.

In summary, the main contribution is a comprehensive framework for optimizing dynamic DNNs, centered around the novel RDP analysis and enabled optimizations. Both memory and execution time improvements are demonstrated through evaluations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Dynamic neural networks
- Operator classification
- Rank and dimension propagation (RDP)
- Dataflow analysis 
- Operator fusion
- Static execution planning
- Dynamic memory planning
- Multi-version code generation
- Mobile devices
- Inference optimization
- Shape dynamism
- Control-flow dynamism

The paper presents a framework called SoD^2 for optimizing dynamic neural networks, which have varying input shapes and dynamic control flow. The key components include classifying operators based on dynamism, using a dataflow analysis technique called RDP to propagate shape information, and leveraging the analysis to enable optimizations like fusion, execution planning, memory planning, and multi-version code generation. The techniques are evaluated on mobile devices for neural network inference. So the keywords cover the proposed techniques as well as the problem context focused on shape and control flow dynamism for mobile inference.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a rank and dimension propagation (RDP) framework for static analysis of dynamic neural networks. Can you explain in more detail how RDP works and how it is different from traditional constant propagation analysis?

2. The paper categorizes DNN operators into 4 types based on their degree of dynamism. Can you provide some examples of operators in each category and explain the significance of this categorization? 

3. Operator fusion is one of the key optimizations enabled by RDP. How does knowing symbolic shapes from RDP help in fusing operators with dynamic shapes? Can you provide a concrete example to illustrate this?

4. The paper claims that choosing an optimal execution order is an NP-complete problem. How does RDP help in guiding the graph partitioning and execution order selection heuristics?

5. Memory allocation planning is another optimization discussed. Why is it harder for dynamic neural networks compared to static ones? How does RDP provide useful insights for memory planning?

6. The multi-version code generation utilizes RDP shape information to reduce the number of versions needed. Can you explain with an example how this works?

7. What are the 4 components of the formal definition of RDP? Can you explain the role of each, especially the transfer functions? 

8. The optimization breakdown analysis shows the individual impact of techniques like operator fusion, execution planning etc. Can you summarize what percentage speedups/memory savings each of those techniques provide?

9. The paper evaluates performance under different scenarios - same execution path, different input sizes, fixed memory budget etc. What key insights do these experiments provide about the approach?

10. How does the operator classification introduced in this paper differ from prior efforts on optimizing static DNNs? What new opportunities does it create?
