# [Bit-wise Training of Neural Network Weights](https://arxiv.org/abs/2202.09571)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Why doesn't gradient descent naturally prune neural network weights and result in sparse networks during training? The authors hypothesize that an important reason is the over-parametrization in terms of the number of bits used to represent the weights. With 32-bit floating point weights, the chance of any given weight being pruned (set exactly to 0) is very low. By training the individual bits that make up each weight, the authors aim to test if sparsity increases on lower bit depths.In summary, the main hypothesis is that training the bits representing the weights will allow gradient descent to more easily prune weights and find sparse networks, revealing the dependence between sparsity and bit depth. The research then explores this hypothesis through bit-wise training experiments across different network architectures and datasets.
