# [GLIMMER: generalized late-interaction memory reranker](https://arxiv.org/abs/2306.10231)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the quality-compute tradeoff for retrieval-augmented language models. In particular, it proposes a model called GLIMMER that aims to achieve higher performance at faster speeds compared to prior methods like LUMEN and FiD. The key ideas in GLIMMER are:1) Incorporating a built-in reranker that reuses the initial layers of the live encoder to select the most relevant passages, improving retrieval quality at low computational cost.2) Sharing the memory and live encoder across tasks and training them with multi-task learning. This enables training the memory encoder and improves the live encoder, especially when it has lower capacity.3) Unifying reranking and memory augmentation into a single end-to-end model, with joint training to optimize both components.The hypothesis is that by exploiting efficient passage reranking and multi-task learning, GLIMMER can outperform prior work like LUMEN and FiD in terms of both quality and computational efficiency on knowledge-intensive NLP tasks. The experiments aim to validate whether this integrated approach effectively improves the quality-compute tradeoff.
