# [SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is whether it is possible to efficiently search over neural network architectures by training an auxiliary model to generate weights conditioned on the architecture. The key hypothesis is that the relative performance of architectures using the suboptimal generated weights from the auxiliary model will correlate with the relative performance when training each architecture normally with optimized weights. If this correlation holds, then the authors can sample many architectures, generate weights for them using the auxiliary model, evaluate their performance, and select the best architecture for further optimization and use in practice. This would allow efficient architecture search without having to do full training runs for each candidate architecture.The paper introduces a method called SMASH that implements this idea. It trains a HyperNet to generate weights for a main model conditioned on the architecture specification. By sampling architectures, generating weights with the HyperNet, and evaluating performance on a validation set, the authors can rank architectures and select promising ones after only a single training run of the HyperNet. The main research question is whether this approach actually works in practice for neural architecture search.In summary, the key hypothesis is that performance with HyperNet-generated weights will correlate with performance with optimized weights across architectures, allowing efficient architecture search. The paper aims to validate this hypothesis and demonstrate the feasibility of the SMASH approach.


## What is the main contribution of this paper?

This paper appears to be a Latex template for NIPS 2017 conference submissions, and does not contain any research content. The template provides formatting instructions and a basic document structure to help authors prepare papers for the conference. The main contribution is simply providing an up-to-date Latex template that follows the NIPS formatting guidelines. Some of the key features include:- Instructions for formatting the title, authors, affiliations, abstract, sections, references, etc. according to NIPS standards. - Inclusion of common packages like inputenc, fontenc, hyperref, url, booktabs, amsmath, etc. that are useful for formatting a paper.- Passing options to the natbib package for customizing citation formatting.- Use of a final option for the nips_2017 package to generate a camera-ready version. - Support for including algorithms and representing them properly.- An example author block demonstrating how to list multiple authors with different affiliations.So in summary, this paper itself does not contain research content, but provides a template to make it easier for authors to prepare submissions that conform to NIPS formatting guidelines. The main contribution is the template itself rather than any novel research work described in the paper.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The idea of using a HyperNet to generate weights for exploring architectures is novel. Most prior work on neural architecture search relies on evolutionary methods, reinforcement learning, or training many models from scratch. Using a HyperNet is a more efficient way to estimate performance across architectures.- However, the architectures explored in this paper are still somewhat hand-designed and limited compared to some other architecture search methods. The method doesn't start from a completely blank slate and build up architectures from scratch. The building blocks like convolutions and connectivity patterns are predefined. Other techniques like evolutionary methods or reinforcement learning search a more open-ended space.- The compute resources required are modest compared to some other architecture search techniques. Methods based on evolution or reinforcement learning often use hundreds or thousands of GPUs across multiple servers. SMASH was developed using a single GPU. However, those other methods can discover highly novel architectures, while SMASH searches over a more constrained space.- The paper validates SMASH on several image classification datasets. Performance is decent but not state-of-the-art. The authors acknowledge SMASH may benefit from expanding the search space and increasing the computational budget. Some other architecture search papers have achieved extremely high performance by searching very large spaces.- An interesting aspect is studying architecture transfer learning, taking an architecture found for one dataset and applying it to another. The CIFAR architecture transferred decently to other datasets, suggesting architectures can generalize.Overall, I'd say this paper introduces a novel and efficient architecture search method using HyperNets, but there is still room for improvement in the architectures discovered and performance achieved. The strength is in the clever idea of using a HyperNet for fast approximate evaluation of many architectures during training.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Improvements to the SMASH method itself, such as using more intelligent sampling strategies during training like Bayesian Optimization or HyperBand, using the generated weights to initialize the full networks, and expanding the search space to explore more architectural options.- Applying SMASH to a wider range of tasks beyond image classification to further validate it, and producing design guidelines to make it more accessible.- Training larger-scale SMASH networks comparable to state-of-the-art hand-designed networks to try to match their performance.- Exploring the memory-bank view of networks more, such as using learned attention for reading/writing, adding true persistent memory, and trying different read/write operations like multiplication. - Varying more elements of the network like the activation functions, order of operations, number of convolutions per layer, and mixing different types of layers and blocks.- Modifying which parts of the network are generated versus fixed/learned parameters, potentially even using some unlearned components like Gabor filters.- Combining SMASH with other techniques like reinforcement learning or Net2Net to further improve it.In summary, the main directions are improving SMASH itself, applying it to more tasks, integrating it with other methods, and leveraging the memory-bank viewpoint to expand the architectural search space in creative ways. The flexibility of the approach opens up many possibilities for future work.


## Summarize the paper in one paragraph.

The paper presents SMASH (One-Shot Model Architecture Search through Hypernetworks), a method for efficiently searching over neural network architectures. The key idea is to train an auxiliary "hypernetwork" model that learns to generate weights for a main network conditioned on the architecture of that network, represented as a binary tensor. By sampling many random architectures and evaluating their validation performance using the hypernetwork-generated weights, the relative ranking of the architectures can be assessed after only a single training run. The authors introduce a flexible scheme for encoding network architectures based on reading and writing to "memory banks" that allows sampling complex topologies like ResNets, DenseNets, and FractalNets. Experiments on image classification datasets like CIFAR and ImageNet32x32 demonstrate that the hypernetwork can learn a decent mapping from architectures to weights, such that the ranking inferred through the hypernetwork correlates with the true ranking when training networks normally. The discovered architectures achieve competitive accuracy compared to hand-designed networks of similar size. Overall, the work presents a way to accelerate neural architecture search by avoiding the need to train each candidate model from scratch.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a technique called SMASH to accelerate neural network architecture search. The key idea is to train an auxiliary model called a HyperNet to generate the weights for a main model with a variable architecture. By sampling different architectures and evaluating their performance using the HyperNet-generated weights on a validation set, the relative performance of architectures can be rapidly compared after only a single training run. This allows efficient architecture search without having to train every candidate model from scratch.The authors introduce a flexible scheme for defining connectivity patterns in the main model, with ResNet, DenseNet, and FractalNet blocks as special cases. The architecture is encoded as a tensor which serves as input to the HyperNet. Experiments on image classification datasets like CIFAR and Imagenet32x32 demonstrate that the validation performance using the HyperNet-generated weights correlates well with the actual performance when training the weights normally. The discovered architectures achieve competitive accuracy compared to hand-designed networks of similar size. This demonstrates the feasibility of using a HyperNet proxy model to accelerate neural architecture search.
