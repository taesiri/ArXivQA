# [Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in   the Age of Artificial Intelligence](https://arxiv.org/abs/2312.01109)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
With the emergence of powerful AI systems like ChatGPT that can generate computer code, there are open questions around their capability to accurately solve programming problems, especially in introductory programming courses. This has implications for relying on such systems to complete assignments and ensure students properly learn programming concepts. 

Proposed Solution:
The authors conducted an experiment to evaluate how well ChatGPT can solve 127 randomly selected programming problems from an automated grading system called Kattis which is widely used in programming courses. The problems spanned various difficulty levels. ChatGPT solutions were submitted to Kattis which would approve or reject them. The accuracy of ChatGPT was quantified based on percentage of solutions approved and error analysis was conducted.

Key Contributions:
- Only 15% of ChatGPT solutions were fully approved, showing limited capability currently to solve wide range of problems 
- ChatGPT solved 19/127 problems overall, mainly at easy difficulty levels
- For rejected solutions, 77% were wrong answers, 15% crashed, 8% exceeded time limits
- ChatGPT performed better on problems that had higher historical solution rates in Kattis system

In summary, the study found ChatGPT has restricted proficiency today in solving programming problems across difficulty levels. This suggests students should use caution in fully relying on ChatGPT for assignments. The results contribute knowledge about generative AI performance that informs teaching practices and academic integrity considerations.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper examines ChatGPT's ability to solve 127 randomly selected programming problems across difficulty levels that were automatically generated and assessed by Kattis, an online judge system, finding that ChatGPT could independently solve only 15% of the problems, mostly at lower difficulty levels, indicating limited capability to pass introductory programming courses currently.


## What is the main contribution of this paper?

 The main contribution of this paper is an empirical evaluation of ChatGPT's capability to solve programming tasks at different difficulty levels that are generated and assessed by Kattis, an automated grading system used in introductory programming courses. Specifically, the paper examines whether ChatGPT can accurately solve 127 randomly selected programming problems from Kattis across varying levels of difficulty. The key findings are:

1) ChatGPT was only able to correctly solve 15% (19 out of 127) of the programming tasks according to Kattis' assessments.

2) Most of the tasks solved were at lower difficulty levels - 10 tasks at level 1, 7 at level 2, and only 2 at level 4. ChatGPT struggled with more complex programming problems.  

3) For the failed solutions, the most common errors were "Wrong Answer" (77% of incorrect solutions) and "Run Time Error" (15%).

4) There were weak-moderate positive correlations between ChatGPT's performance and the percentage of approved submissions and successful submitters for tasks in Kattis.

In conclusion, the paper contributes empirical evidence that ChatGPT currently has limited capability in solving automatically generated coding problems across difficulty levels, especially more complex tasks, as used in introductory programming courses. The findings inform the debate around AI tools in programming education.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, the keywords associated with this paper appear to be:

Programming Education, ChatGPT, Automated Grading, Academic Integrity

These keywords are listed in the paper under the \keywords section:

\keywords{Programming Education, ChatGPT, Automated Grading, Academic Integrity}

So the key terms that capture the main topics and focus of this paper are:

Programming Education
ChatGPT 
Automated Grading
Academic Integrity


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions that coding tasks were randomly selected from Kattis. What specific strategies or procedures were used to ensure true randomness in the selection process? How might the randomness of selection impact the generalizability of the results?

2. Why was Python chosen as the programming language to test ChatGPT's code generation capabilities? How might the performance vary for other common introductory programming languages like Java or C++? 

3. The paper categorizes task difficulty on a 10-point scale. What specific metrics or criteria does Kattis use to quantify task difficulty on this scale? Could the granularity of this scale mask important differences in task complexity?

4. For tasks where ChatGPT generated incorrect solutions, what was the distribution of specific error types (e.g. syntax errors, logical errors, infinite loops)? Does this distribution provide any additional insight?

5. Rather than treating each task independently, could tasks be clustered into groups based on underlying algorithmic techniques required? Might ChatGPT show different performance across task clusters? 

6. The correlation analysis shows some association between ChatGPT performance and human performance metrics from Kattis. However, correlation does not imply causation. What additional analyses could strengthen claims about how ChatGPT performance relates to human performance?

7. The paper reports aggregate performance over 127 tasks. However, does ChatGPT show learning over time as it is exposed to more Kattis programming tasks? Was there any evidence that performance improved over the course of testing?

8. What data normalization or preprocessing steps were applied to the Kattis tasks prior to inputting them to ChatGPT? Could techniques like prompt engineering further optimize ChatGPT performance? 

9. The paper acknowledges that mathematical and graphic-heavy tasks were excluded due to technical limitations. With those limitations addressed, might the performance assessment change across the full space of introductory programming tasks?

10. Beyond aggregate task performance, were any supplemental metrics captured related to code quality, style, or documentation? Are those additional indicators relevant to evaluating ChatGPT's utility for introductory programming education?
