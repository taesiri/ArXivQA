# FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key goals of this paper are:1. To propose a multi-dimensional dialogue-level metric called FineD-Eval that consists of specialized sub-metrics, each targeting a specific fine-grained dialogue quality dimension (e.g. coherence, likability, topic depth). 2. To demonstrate that combining the specialized sub-metrics using ensemble methods or multitask learning leads to better correlation with human judgments across different quality dimensions compared to individual sub-metrics.3. To show that FineD-Eval achieves state-of-the-art performance in terms of correlation with human judgments on several dialogue evaluation benchmarks and is more interpretable than existing metrics.The central hypothesis is that decomposing overall dialogue quality into specialized fine-grained dimensions and then combining models trained on those dimensions will result in more effective automatic evaluation compared to models trained on a single dimension or on overall quality.The key research questions addressed are:- How to identify meaningful and relatively independent quality dimensions for dialogue evaluation?- How to develop specialized sub-metrics targeting different quality dimensions? - How to effectively combine the specialized sub-metrics into a unified holistic metric?- Whether the proposed multi-dimensional metric leads to better correlation with human judgments and interpretability compared to existing metrics?In summary, the paper focuses on developing a more fine-grained, interpretable and effective automatic dialogue evaluation metric through multi-dimensional modeling and specialized sub-metric combination.


## What is the main contribution of this paper?

The main contribution of this paper is proposing FineD-Eval, a multi-dimensional dialogue-level evaluation metric. The key aspects are:- It consists of three specialized sub-metrics, each targeting a specific fine-grained quality dimension (coherence, likability, topic depth). - The sub-metrics are trained with novel self-supervised objectives using strategies like utterance shuffling, contradiction scoring, etc. without reliance on human annotations.- Two approaches are explored to combine the sub-metrics into a unified metric - metric ensemble and multitask learning. - Experiments show the sub-metrics highly correlate with human judgment on their respective target dimensions. Combining them leads to better performance than individual sub-metrics.- FineD-Eval significantly outperforms existing state-of-the-art metrics on three dialogue evaluation benchmarks, achieving around 16% relative improvement on average.- Compared to existing metrics, FineD-Eval provides more fine-grained and interpretable scores along different quality dimensions.In summary, the key contribution is proposing an effective and interpretable multi-dimensional dialogue-level evaluation metric based on specialized sub-metrics and self-supervised learning. The metric ensemble and multitask learning approaches demonstrate viability in combining sub-metrics into a superior holistic metric.
