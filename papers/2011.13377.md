# [How Well Do Self-Supervised Models Transfer?](https://arxiv.org/abs/2011.13377)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How well do self-supervised visual representation learning models transfer to downstream tasks compared to supervised learning? The authors evaluate a range of self-supervised models on their transfer performance to 40 different downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. They compare the transfer performance to a supervised baseline to analyze how well these self-supervised models can replace or even outperform supervised pre-training. The key hypothesis appears to be that the best self-supervised models can surpass supervised pre-training on most downstream tasks.In summary, the central research question is about quantitatively evaluating and comparing the transfer capabilities of different self-supervised learning algorithms to downstream tasks in computer vision. The key hypothesis is that self-supervised pre-training can match or exceed a supervised pre-training baseline.


## What is the main contribution of this paper?

The main contribution of this paper is a large-scale empirical evaluation of the transfer performance of 13 top self-supervised visual representation learning models on 40 downstream tasks. The key findings are:- On most tasks, the best self-supervised models outperform supervised pre-training, confirming the recent trend of self-supervised models surpassing supervised pre-training.- ImageNet top-1 accuracy is highly correlated with transfer performance on many-shot recognition, but less correlated for few-shot recognition, object detection, and dense prediction tasks. - No single self-supervised method dominates across all tasks, suggesting universal pre-training is still an open problem.- Analysis of the learned features suggests top self-supervised models fail to preserve color information as well as supervised models, but induce better classifier calibration and less overfitting.In summary, this paper provides a comprehensive benchmark and analysis of the transfer capabilities of recent self-supervised visual representation learning methods across a diverse set of downstream tasks. The results highlight the promise of self-supervised learning while also identifying key limitations and open challenges.
