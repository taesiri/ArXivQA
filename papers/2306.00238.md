# [Bytes Are All You Need: Transformers Operating Directly On File Bytes](https://arxiv.org/abs/2306.00238)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether transformer models can perform inference directly on raw file bytes, without requiring any decoding or modality-specific preprocessing. The key hypotheses appear to be:- Transformer models with some modifications (e.g. convolutional downsampling, shifted window attention) can handle long sequences of raw file bytes as inputs.- Operating directly on file bytes can enable multi-modal processing without modality-specific components. The same model architecture can handle images, audio, etc stored in various file formats.- Processing file bytes rather than decoded data representations may have applications in privacy-preserving inference, since less information about the raw data is exposed.So in summary, the main research question is whether transformers can effectively perform inference on raw file bytes across multiple modalities, which could have benefits for model generalization and privacy. The key hypotheses relate to transformers' ability to handle long byte sequences and the potential advantages of not decoding inputs for multi-modal processing and privacy.
