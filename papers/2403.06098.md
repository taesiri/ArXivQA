# [VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video   Diffusion Models](https://arxiv.org/abs/2403.06098)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Text-to-video diffusion models like Sora heavily rely on prompts to generate videos, but there is no publicly available prompt dataset to support research in this area. 

Proposed Solution:
- The paper introduces VidProM, the first large-scale dataset with 1.67 million unique text-to-video prompts collected from real users. 
- The dataset contains 6.69 million videos generated from the prompts using 4 state-of-the-art diffusion models. 
- Prompts are embedded using OpenAI's text-embedding-3-large model and have NSFW probabilities assigned.
- A subset named VidProS with over 1 million semantically unique prompts is also provided.

Comparison with Existing Datasets:
- Compared to the image prompt dataset DiffusionDB, VidProM has more unique prompts collected over a longer period, uses a better embedding method, and focuses specifically on video generation which requires more complex prompts.

Key Observations:  
- Analysis shows video prompts exhibit different distributions and semantics compared to image prompts, highlighting the need for a specialized text-to-video prompt dataset.  
- The prompts show user preference for topics like humans, sci-fi, animals indicating scope for better generative models.

Main Contributions:
- The first large-scale text-to-video prompt dataset to accelerate research.  
- In-depth analysis on the differences between image and video prompts.
- Outlines new research directions enabled such as prompt engineering, efficient generation, fake detection and copy detection for videos.

The paper makes the VidProM dataset publicly available to support the text-to-video generation research community.
