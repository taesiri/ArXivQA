# [Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement   Learning Using Unique Experiences](https://arxiv.org/abs/2402.05963)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Off-policy actor-critic reinforcement learning algorithms use experience replay to train the critic and guide the actor. The performance depends on the quality of experiences in the replay buffer. 
- Maintaining independent and identically distributed (IID) samples in the replay buffer is crucial for stability and faster convergence of optimization algorithms used for training.
- For complex systems, replay buffers are very large to ensure IID samples. This poses memory constraints, especially for embedded systems.

Proposed Solution:
- The paper proposes the Frugal Actor-Critic (FAC) algorithm to maintain unique experiences in the replay buffer. 
- It uses matrix decomposition on initial random samples to identify important state dimensions that characterize system dynamics.
- The state space is partitioned into abstract states based on values of the important state dimensions.
- A kernel density estimator estimates reward density for an abstract state. New experiences with low reward density (unique) are added to the replay buffer.
- This reduces replay buffer size while ensuring IID samples for faster convergence.

Main Contributions:
- FAC algorithm to store unique experiences in replay buffer for off-policy actor-critic methods.
- Theoretical guarantees for faster convergence and better IID characteristics compared to general off-policy algorithms.
- Experiments show FAC requires much smaller replay buffers while achieving better rewards and faster convergence on most benchmarks.
- Significantly outperforms prioritization-based method LABER in terms of convergence speed and accumulated rewards.
- The first work addressing replay buffer management for improving off-policy actor-critic algorithm performance.

In summary, the paper makes offline reinforcement learning more practical by reducing replay buffer size without compromising performance. The proposed FAC algorithm and analysis open up new research directions in efficient utilization of replay buffers.
