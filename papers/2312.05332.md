# [Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming   Controllers Inspired by Model Predictive Control](https://arxiv.org/abs/2312.05332)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep reinforcement learning (DRL) controllers like multi-layer perceptrons (MLPs) lack verifiability and performance guarantees despite good empirical performance. 
- Model predictive control (MPC) has stability and safety guarantees but suffers from short-sightedness, lack of robustness, and high computational cost.

Proposed Solution:
- Propose a new class of parameterized controllers inspired by MPC's quadratic programming (QP) structure. 
- The controller resembles an unrolled QP solver, structured like a recurrent neural network, with parameters learned via DRL instead of derived from models.
- Ensures theoretical guarantees akin to MPC thanks to the QP structure, while demonstrating competitive empirical performance and efficiency.

Main Contributions:
- Introduce QP-structured neural network controllers inspired by MPC, with parameters trained by DRL.
- Prove properties like persistent feasibility and asymptotic stability for the learned controllers.
- Empirically show strong performance matching MPC and MLP baselines on control tasks, with added robustness against uncertainties.
- Demonstrate superior computational efficiency over MPC methods.
- Provide promising results on a real-world vehicle drift maneuvering task, indicating potential for nonlinear robotic systems.

In summary, the paper proposes a principled way of combining strengths of MPC and DRL to create performant and verifiable controllers. Both theoretical and empirical evidence validate the effectiveness of this MPC-inspired model-free QP control framework.
