# [DVANet: Disentangling View and Action Features for Multi-View Action   Recognition](https://arxiv.org/abs/2312.05719)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-view action recognition is more challenging than single-view action recognition due to variability in backgrounds, occlusions, and visibility across different viewpoints. 
- Current state-of-the-art methods rely heavily on skeletal/depth data, but many real-world scenarios lack the equipment to capture these modalities.
- RGB-based methods typically underperform compared to skeletal methods for multi-view action recognition.

Proposed Solution:
- A novel transformer decoder architecture to disentangle action features from view features using RGB data only.
- Uses multiple action queries in decoder to capture action information and a single view query to capture view information.
- Additional losses beyond cross-entropy, including two supervised contrastive losses and a query orthogonal loss, to further enforce disentanglement of action and view features.

Contributions:
- Achieves new state-of-the-art for RGB-based multi-view action recognition on 4 datasets, improving by up to 4.8% over previous RGB methods.
- Outperforms previous skeletal-based methods on some datasets, even without using skeletal data.
- Analyses and visualizations demonstrate clear disentanglement of learned action and view representations.
- Approach works effectively even with small amounts of training data.
- Does not require additional modalities like skeletal or depth data.

In summary, the paper proposes a novel RGB-based architecture and training procedure to learn disentangled action and view representations for improved multi-view action recognition, achieving state-of-the-art results. The model works very well while only using RGB data, removing the need for extra modalities.
