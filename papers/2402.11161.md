# [PANDA (Pedantic ANswer-correctness Determination and   Adjudication):Improving Automatic Evaluation for Question Answering and Text   Generation](https://arxiv.org/abs/2402.11161)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Question answering (QA) evaluation typically uses metrics like exact match and ROUGE which are limited for assessing free-form answers from large language models. 
- Lack of clear QA evaluation guidelines and limited training data for learning better automated metrics.
- Expensive QA evaluation methods using large language models do not scale.

Solution:
- Propose guidelines for QA answer correctness adopted from human QA competitions. 
- Introduce PANDA - a small (812 KB), efficient, deterministic classifier for answer correctness.
- PANDA uses tf-idf encoded features of question, reference and candidate answers along with raw token scores like F1. This allows learning optimal score thresholds for different answer types and rules.
- Construct a dataset of 4K+ labelled QA pairs following the guidelines to train PANDA.

Contributions:
- QA evaluation guidelines more suitable for free-form answers.
- PANDA matches performance of large LM estimators at low computational cost.
- Human annotation and analysis on 3 datasets shows PANDA agrees better with humans than exact match and other metrics.
- Analysis reveals PANDA's advantages in specificity, semantic similarity and co-reference resolution.
- There are still challenges in evaluating certain types of irrelevant information.

In summary, the paper bridges ideas from the trivia community to formulate better automated QA evaluation guidelines and methods for modern datasets. The proposed PANDA classifier balances performance and efficiency for answer correctness.
