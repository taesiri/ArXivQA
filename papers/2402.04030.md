# [Reducing the Cost of Quantum Chemical Data By Backpropagating Through   Density Functional Theory](https://arxiv.org/abs/2402.04030)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Density Functional Theory (DFT) can accurately predict quantum properties of molecules, but is very computationally expensive, scaling as O(N_electrons^3).
- Neural networks (NNs) have been trained to efficiently approximate DFT predictions, achieving 1000x speedups. However, generating enough training data is prohibitively expensive. For example, the PCQ dataset took years to generate, yet NNs train on it in just a week.

Proposed Solution:
- Bypass the need for labeled training data by directly using DFT's energy function E(H) as a "loss function" for training NNs. 
- Introduce a Quantum Pre-trained Transformer (QPT) model that outputs a matrix H which is fed into E(H) to compute loss.
- Generate new training data on-the-fly each iteration by sampling new molecules X_i and computing E(H_i) for the QPT's predictions H_i = QPT(X_i).

Contributions:  
- Eliminate the huge cost of precomputing training sets with DFT.
- QPT reaches accuracy comparable to prior supervised methods on MD17, while reducing total training time from 786 hrs to 31 hrs on Uracil dataset.
- Approach allows infinite supply of training data, mitigating overfitting.
- Enables arbitrarily scaling up model size, a first step towards giant molecular foundation models.
- Merges optimization techniques from ML and DFT, using ideas like density mixing and minao initialization.
- Demonstrates a simple Transformer architecture without inductive biases can achieve strong performance.

In summary, the paper presents a method to train NNs to approximate DFT without the prohibitive computational cost of generating training sets. By interleaving ML and DFT optimization, it provides a path towards scalable molecular foundation models.
