# [Beyond Human Data: Scaling Self-Training for Problem-Solving with   Language Models](https://arxiv.org/abs/2312.06585)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from this paper:

This paper explores a simple yet powerful self-training method called Reinforced Self-Training with Expectation Maximization (ReST-EM) to improve large language models (LLMs) without reliance on scarce human-labeled data. ReST-EM alternates between a generate step where the LLM creates training data and filters it via a reward signal, and an improve step where the LLM fine-tunes on this synthetic data. Theoretically, ReST-EM is grounded in applying expectation-maximization for reinforcement learning. Empirically, the authors show ReST-EM leads to substantial gains on mathematical reasoning and code generation tasks using PaLM models, significantly exceeding fine-tuning on human-generated data. Multiple iterations of ReST-EM provide further improvements, though performance eventually saturates then declines due to overfitting. ReST-EM benefits from more model capacity and training data, produces gains in pass@k metrics, exhibits positive transfer, and requires little computation relative to pretraining costs. Limitations include the need for some human-labeled data and reward design. Overall, the simplicity, scalability and effectiveness of ReST-EM underscores the promise of self-training with feedback for reducing reliance on human annotations.
