# [GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via   Game-Theoretic Evaluations](https://arxiv.org/abs/2402.12348)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) are increasingly integrated into critical real-world applications like cybersecurity, decision science, and finance, evaluating their strategic reasoning abilities is crucial. However, existing LLM benchmarks focus more on role-playing games involving intricate details and verbal exchanges which can mask deficiencies in concrete reasoning skills. The paper argues that game-theoretic tasks offer a purer assessment of logic and strategic thinking. 

Methodology:
The paper proposes GTBench, a comprehensive game-theoretic benchmark with 10 tasks spanning complete/incomplete information, dynamic/static, and probabilistic/deterministic games. It enables both LLM-vs-conventional agent and LLM-vs-LLM evaluations. Metrics like Normalized Relative Advantage and regret are used to quantify performance.

Key Contributions:
1) GTBench provides the research community the first game-theoretic LLM benchmark environment supporting diverse gaming scenarios to spur more research.

2) The study reveals distinct LLM behaviors - LLMs fail completely in complete-information deterministic games against MCTS agents but remain competitive in incomplete-information probabilistic games.  

3) Code-pretraining is shown to benefit game-theoretic reasoning with CodeLlama outperforming comparably sized LLMs. However, advanced reasoning schemes do not always help.

4) Open-source LLMs struggle in complex games compared to commercial LLMs, but achieve comparable performance in simple games. Detailed LLM error profiles are also identified.

5) The introduced multi-turn LLM-vs-LLM evaluation paradigm serves as a competitive benchmark even for advanced future LLMs. The benchmark leaderboard interface will promote open research.

In summary, the paper provides an extensive analysis of LLMs' strategic reasoning skills using game-theoretic tasks and offers the community a strong benchmark for driving future progress.
