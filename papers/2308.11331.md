# [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive   Language-Image Pre-training](https://arxiv.org/abs/2308.11331)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:- How can we design a vision-language pre-training model that can automatically grow its architecture as more training data becomes available over time? - Can we develop an efficient online learning algorithm for large-scale contrastive language-image pre-training that adapts the model architecture and parameters as the dataset grows?- Can architectural search and parameter inheritance with momentum allow the model to efficiently learn from new data while retaining previous knowledge?- How does model growth along different dimensions (e.g. number of parameters, transformer blocks, attention heads, etc.) impact performance on downstream vision-language tasks?- Can this approach of continuous automated model growth outperform existing pre-training methods with fixed architectures when trained on increasingly large datasets in an online learning fashion?The key hypotheses appear to be:- An adaptive model that grows its architecture dynamically will be better suited for online learning scenarios where pre-training data continuously expands over time.- Architectural search and parameter inheritance can allow efficient learning on new data while avoiding catastrophic forgetting of previous knowledge. - Model growth along certain dimensions (e.g. image encoder capacity) will be more impactful than others.- The proposed GrowCLIP method will outperform baseline pre-training approaches with fixed model architectures in an online learning setting as the dataset scales up.In summary, the main focus seems to be developing an automated way to grow vision-language models during pre-training to adapt to continuously expanding datasets in an online learning scenario. The key ideas are architectural search and parameter inheritance to efficiently learn from new data at each step.


## What is the main contribution of this paper?

Based on my reading of the abstract and skimming the paper, the main contribution of this paper appears to be proposing a data-driven automatic model growing algorithm called GrowCLIP for large-scale contrastive language-image pre-training that can adapt to continuously growing training data. Specifically, the key ideas proposed are:- A dynamic growth space and growth architecture selection method to select optimal network architectures that adapt to growing training data. This includes proposing a shared encoder to enhance cross-modal fusion.- Parameter inheriting with momentum (PIM) to maintain knowledge from previous models and address issues with local minima when fine-tuning on new data. - A pipeline with supernet training, growth architecture selection, and selected model training to incrementally grow the model architecture and parameters as more training data becomes available.The experiments show GrowCLIP can improve performance on downstream tasks like zero-shot image classification and retrieval compared to baseline methods. The ablation studies also demonstrate the effectiveness of the different components of GrowCLIP.In summary, the main contribution appears to be proposing an automatic and data-driven way to incrementally grow cross-modal models like CLIP as more training data becomes available, while efficiently leveraging previous knowledge and selecting optimal architectures.
