# SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a scalable approach for pre-training robot policies to equip them with a diverse repertoire of skills while minimizing the amount of human annotation effort required?The key hypothesis is that by using two techniques - instruction relabeling via large language models and cross-trajectory skill chaining - the authors' proposed SPRINT method can substantially reduce the human labeling effort needed for pre-training a diverse set of skills, compared to prior pre-training approaches that rely on collecting hundreds of thousands of manual instruction labels. The SPRINT method aims to automatically expand a base set of pre-training tasks using these two core ideas in order to equip robots with a richer skill set. The authors hypothesize this will lead to more efficient finetuning on new, unseen downstream tasks compared to previous pre-training methods.In summary, the central research question tackles how to develop a more scalable pre-training approach that provides a diversity of skills to robots while minimizing costly human annotation effort. The key hypothesis is that SPRINT's automated instruction relabeling and cross-trajectory skill chaining techniques can achieve this goal.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing SPRINT, a scalable approach for pre-training robot policies that aims to equip robots with a diverse repertoire of skills while minimizing the amount of required human annotation effort. The key ideas of SPRINT are:1. Using large language models to automatically aggregate and relabel consecutive language instructions into more complex tasks. For example, aggregating "place mug in coffee machine" and "press brew button" into a higher-level "make coffee" instruction.2. Introducing a cross-trajectory skill chaining objective that stitches together segments from different trajectories via offline RL to form new long-horizon tasks. This allows learning skills that are not directly present in the offline training data. By combining language model-based aggregation and cross-trajectory chaining, SPRINT can automatically expand a base set of pre-training tasks defined by initial human annotations. Experiments show this leads to pre-trained policies that can more efficiently learn challenging new tasks compared to prior pre-training approaches.In summary, the main contribution is proposing a more scalable way to pre-train policies by minimizing human annotation effort, while still equipping robots with a rich skill repertoire to enable effective generalization. The key ideas are using LLMs and cross-trajectory chaining to automatically grow the space of pre-training tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper introduces a new method called SPRINT for scalable pre-training of robot policies to acquire diverse skills while minimizing human annotation effort. Other recent work like Calvin and Interactive Language Learning also use language instructions for pre-training, but require a lot more human labeling effort (hundreds of thousands of instructions). SPRINT reduces this effort through automated instruction relabeling and cross-trajectory skill chaining.- The idea of using language models for automated instruction aggregation is novel. Prior work has looked at generating language instructions, but makes assumptions like access to privileged state information or hand-defined grammars that limit scalability. SPRINT shows language models can help expand pre-training tasks in a scalable, offline way.- For pre-training, SPRINT uses an offline RL method based on Implicit Q-Learning. Other recent offline RL works focus on pre-training for a target task rather than a diverse set of skills. Meta-RL methods do pre-train on varied tasks, but require manual task design rather than leveraging language.- SPRINT demonstrates strong results on challenging, long-horizon household tasks compared to prior pre-training methods. This shows the benefits of SPRINT's automated skill aggregation for acquiring complex, compositional skills.- Experiments are conducted both in a household simulator and real robot environment. Having real robot results is useful for validating that SPRINT transfers to physical systems. Many prior language-conditioned RL works are only simulated.Overall, SPRINT makes good progress on scaling up language-guided pre-training by reducing human annotation needs. The experiments demonstrate these scalable pre-trained policies acquire richer skills and transfer better to unseen tasks compared to prior pre-training approaches. The real robot experiments help validate the applicability of SPRINT's methods.
