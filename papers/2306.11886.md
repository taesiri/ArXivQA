# SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a scalable approach for pre-training robot policies to equip them with a diverse repertoire of skills while minimizing the amount of human annotation effort required?The key hypothesis is that by using two techniques - instruction relabeling via large language models and cross-trajectory skill chaining - the authors' proposed SPRINT method can substantially reduce the human labeling effort needed for pre-training a diverse set of skills, compared to prior pre-training approaches that rely on collecting hundreds of thousands of manual instruction labels. The SPRINT method aims to automatically expand a base set of pre-training tasks using these two core ideas in order to equip robots with a richer skill set. The authors hypothesize this will lead to more efficient finetuning on new, unseen downstream tasks compared to previous pre-training methods.In summary, the central research question tackles how to develop a more scalable pre-training approach that provides a diversity of skills to robots while minimizing costly human annotation effort. The key hypothesis is that SPRINT's automated instruction relabeling and cross-trajectory skill chaining techniques can achieve this goal.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing SPRINT, a scalable approach for pre-training robot policies that aims to equip robots with a diverse repertoire of skills while minimizing the amount of required human annotation effort. The key ideas of SPRINT are:1. Using large language models to automatically aggregate and relabel consecutive language instructions into more complex tasks. For example, aggregating "place mug in coffee machine" and "press brew button" into a higher-level "make coffee" instruction.2. Introducing a cross-trajectory skill chaining objective that stitches together segments from different trajectories via offline RL to form new long-horizon tasks. This allows learning skills that are not directly present in the offline training data. By combining language model-based aggregation and cross-trajectory chaining, SPRINT can automatically expand a base set of pre-training tasks defined by initial human annotations. Experiments show this leads to pre-trained policies that can more efficiently learn challenging new tasks compared to prior pre-training approaches.In summary, the main contribution is proposing a more scalable way to pre-train policies by minimizing human annotation effort, while still equipping robots with a rich skill repertoire to enable effective generalization. The key ideas are using LLMs and cross-trajectory chaining to automatically grow the space of pre-training tasks.
