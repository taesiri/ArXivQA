# [Aligner: One Global Token is Worth Millions of Parameters When Aligning   Large Language Models](https://arxiv.org/abs/2312.05503)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) need to frequently adapt to new behaviors and align to specified values, which requires fine-tuning the entire model. This is often impractical due to the massive size of modern LLMs. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged to address this, but primarily target "form adaptation" tasks like outputting content in new styles.

Proposed Solution:
The paper proposes "Aligner", a highly parameter-efficient PEFT method for form adaptation in LLMs. Aligner introduces a globally shared set of tunable tokens that modify the attention in every layer of the LLM. With just 1 token, Aligner achieves comparable performance to state-of-the-art methods on tasks like instruction following and value alignment, using 800x fewer parameters than methods like LoRA.

Main Contributions:
1) Introduces Aligner, which shows much higher parameter efficiency than existing methods for form adaptation tasks, using as few as 1-10 tokens.
2) Shows strong performance of Aligner on instruction following and value alignment tasks.
3) Provides evidence that LLMs handle "form" and "reasoning" orthogonally, since the global form component in Aligner causes no disadvantage on reasoning tasks.
4) The efficiency and efficacy of Aligner yields insights into LLM mechanisms and enables customization for diverse users.
5) Aligner promises to advance research into safer and more controllable LLM alignment.

In summary, the paper makes significant contributions around a highly parameter-efficient method for tuning LLMs, while also providing valuable insights into how LLMs internally separate form and reasoning.
