# [Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again](https://arxiv.org/abs/2401.17052)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Anomaly detection on tabular data is challenging for deep learning models. Recent research has shown that incorporating non-parametric mechanisms, which leverage relationships between samples, can significantly improve model performance. 
- Reconstruction-based anomaly detection methods that learn to reconstruct normal samples seem well suited to benefit from non-parametric sample-sample relationships. However, most existing methods do not effectively leverage such relationships.

Proposed Solution:
- The paper investigates augmenting reconstruction-based anomaly detection with external retrieval modules to leverage sample-sample dependencies. 
- Several retrieval methods are proposed and evaluated, including nearest neighbor and attention-based modules. These identify relevant training samples to help reconstruct masked features of a target sample.  
- The retrieval representations are aggregated with the target sample's encoded representation before final reconstruction and anomaly scoring.

Main Contributions:
- Provides an extensive evaluation of retrieval-based anomaly detection methods on 31 tabular datasets.
- Empirically demonstrates that augmenting a transformer-based reconstruction approach with an attention-based retrieval module (referred to as transformer+attention-bsim) can significantly improve detection accuracy.
- On average, transformer+attention-bsim improves F1 by 4.3% and AUROC by 1.2% over the vanilla transformer which only captures feature-feature dependencies.
- Analyzes design choices for the retrieval modules such as location, number of neighbors, aggregation weight.
- Overall, shows the benefit of using sample-sample relationships in anomaly detection on tabular data and provides insights into effective retrieval module design.

In summary, the paper demonstrates through comprehensive experiments that non-parametric mechanisms can effectively augment reconstruction-based anomaly detection on tabular data. The transformer+attention-bsim model with an optimized retrieval module provides state-of-the-art performance.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes and evaluates different approaches for augmenting reconstruction-based anomaly detection methods on tabular data with external non-parametric retrieval modules to leverage sample-sample dependencies and shows empirically that an attention-based similarity retrieval module can significantly improve anomaly detection performance.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

1) Proposing an extensive evaluation of retrieval-based methods for anomaly detection on tabular data. 

2) Empirically showing that augmenting existing anomaly detection methods with a retrieval module to leverage sample-sample dependencies can help improve detection performance.

In particular, the paper investigates adding external retrieval modules to a transformer-based reconstruction approach for anomaly detection. Experiments on 31 tabular datasets show that using an attention-based retrieval module (referred to as "attention-bsim") provides the best performance, outperforming the vanilla transformer and other retrieval methods. This suggests sample-sample dependencies captured by the retrieval module can enhance anomaly detection accuracy.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key keywords and terms associated with it:

- Anomaly detection (AD)
- Tabular data
- Deep learning
- Retrieval models/modules 
- Sample-sample dependencies
- Feature-feature dependencies
- Reconstruction-based methods
- Mask reconstruction 
- External retrieval mechanisms
- Attention-based retrieval
- Non-parametric transformers

The paper investigates using external retrieval modules to augment existing anomaly detection methods for tabular data, in order to leverage sample-sample dependencies. It focuses specifically on reconstruction-based AD methods involving mask reconstruction, and tests different attention-based retrieval modules to select relevant samples and help in the reconstruction process. Key goals are improving AD performance by accounting for non-parametric relationships in the data via retrieval, and empirically showing that leveraging both feature-feature and sample-sample dependencies is important for effective AD on tabular data.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using external retrieval modules to augment existing anomaly detection methods. Why is using external retrieval preferable to using internal retrieval modules which have been explored in some previous works? What are the tradeoffs?

2. The paper explores different retrieval module architectures like KNN-based, attention-based etc. Why does the attention-bsim module perform better than other retrieval modules? What properties of this module make it suitable for anomaly detection?

3. The paper uses a mask reconstruction framework for anomaly detection. Why is mask reconstruction well-suited for anomaly detection? What types of anomalies can it effectively identify? How does it compare to other reconstruction objectives?

4. How exactly does the retrieval module help in identifying local and group anomalies? Explain with examples of how the retrieved samples would assist in detecting such anomalies. 

5. The paper fixes the reconstructed feature loss for numerical and categorical features. How should the loss functions be designed to maximize anomaly detection performance? Should the loss functions be tailored for different data types?

6. External retrieval modules add computational overhead. How can efficiency be ensured so these modules can scale to large datasets? What approximations can be made?

7. The paper uses the full training set as candidates during inference where possible. However, for large datasets, only a subset is used. What strategies can be used to cleverly subsample the candidates? How does the choice of candidates impact results?

8. What additional constraints, biases or priors can be incorporated in the retrieval module's design to make it more suited for anomaly detection? How can we prevent trivial solutions?

9. How sensitive is the performance of the proposed method to architectural choices like number of attention heads, hidden dimensions etc? How can we design the architecture for maximum efficacy?

10. The paper evaluates on a standard anomaly detection benchmark. What additional real-world datasets from diverse domains can be used to test generalization capability? What domain gaps need to be addressed?
