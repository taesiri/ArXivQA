# [A Backpack Full of Skills: Egocentric Video Understanding with Diverse   Task Perspectives](https://arxiv.org/abs/2403.03037)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Current approaches in egocentric video understanding focus on creating specialized models for individual tasks like action recognition, anticipation, etc. This misses opportunities for sharing knowledge across tasks.
- Multi-task learning has been explored but suffers from negative transfer between unrelated tasks. Also, it assumes supervision is available for all tasks during training.
- The recently proposed EgoT2 translates features between task-specific models to benefit the primary task. But it requires extensive pretraining, lacks knowledge abstraction, and the primary task must be known apriori.

Proposed Solution: 
- Present a unified architecture with a shared temporal backbone (using graph neural nets) and minimal task-specific heads to enable multi-task learning.
- Propose EgoPack to create task-specific prototypes that summarize the perspective/knowledge gained from each task. These prototypes are stored like a "backpack" for later use.
- When learning a new task, the model interacts with the closest prototypes to enrich its features. This allows transferring perspectives from previous tasks without requiring their supervision.

Key Contributions:
- A unified architecture for multi-task learning of egocentric vision tasks with minimal overhead
- EgoPack method to abstract knowledge into reusable prototypes that aids learning new tasks 
- Outperforms specialized and multi-task baselines by enabling unique synergies and leveraging perspectives from different tasks
- Achieves competitive performance on Ego4D tasks, outperforming state-of-the-art on some tasks, without end-to-end training.

In summary, the paper proposes an effective approach for knowledge sharing across egocentric vision tasks. By creating reusable task prototypes, it enables efficiently learning new skills using perspectives from previously learned tasks.
