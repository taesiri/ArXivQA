# [Unveiling Divergent Inductive Biases of LLMs on Temporal Data](https://arxiv.org/abs/2404.01453)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Large language models (LLMs) face challenges in accurately sequencing events and discerning temporal relationships due to limited context windows. This can introduce biases in models' temporal reasoning abilities and preferences for certain temporal relations over others. 

- There is a research gap exploring the inductive biases of LLMs in comprehending temporal dynamics. This paper investigates the temporal comprehension abilities and biases of GPT-3.5 and GPT-4 models.

Methodology:
- Analyzed two types of temporal event data - implicit (actions inferred from context) and explicit (directly stated in text). 

- Used two prompt formats - QA (question-answering) and TE (textual entailment) to query models on determining temporal relationships.

- Measured inductive bias by examining models' tendencies towards "BEFORE"/"AFTER" relations (QA format) and "TRUE"/"FALSE" labels (TE format).

Key Findings:
- GPT-3.5 favors "AFTER" relation while GPT-4 favors "BEFORE" relation in QA format for both implicit and explicit events.  

- In TE format, GPT-3.5 tends towards "TRUE" while GPT-4 prefers "FALSE" for both implicit and explicit events.

- The consistent yet contradictory patterns between GPT-3.5 and GPT-4 reveal entrenched inductive biases in comprehending temporal data.

Main Contributions:
- First study to systematically analyze inductive biases of LLMs (GPT-3.5 and GPT-4) in temporal reasoning across different prompt types and event data.

- Uncovered specific tendencies of models towards certain temporal relationships, highlighting need to address biases.

- Showed model evolution may introduce new complex biases rather than just mitigating existing ones.

- Findings can guide developing methodologies for effective temporal reasoning in LLMs.
