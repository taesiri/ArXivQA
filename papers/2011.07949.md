# [RSPNet: Relative Speed Perception for Unsupervised Video Representation   Learning](https://arxiv.org/abs/2011.07949)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is:How can we learn effective video representations in an unsupervised manner, especially learning useful features for both motion and appearance?The key points are:- Unsupervised video representation learning is very challenging due to the complex spatio-temporal information in videos and lack of labeled data. - Existing methods using playback speed perception as pretext task suffer from imprecise speed labels. Also they do not explicitly encourage learning appearance features.- This paper proposes to use relative speed perception as pretext task which can provide more consistent supervision. - It also extends instance discrimination from images to videos and uses speed augmentation so the model focuses more on appearance. - By combining the two pretext tasks, the model can learn useful features for both motion and appearance in an unsupervised manner.- Experiments show the learned features achieve excellent performance on downstream action recognition and video retrieval without using any manually annotated data.In summary, the main research question is how to do unsupervised video representation learning, especially learning features for both motion and appearance. The key ideas are exploiting relative speed and extending instance discrimination with speed augmentation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new self-supervised learning method called RSPNet for video representation learning without using any labels. The key ideas are:- Proposing a relative speed perception (RSP) task that identifies the relative playback speed between two clips instead of predicting their specific playback speeds. This provides more precise supervision to learn motion features. - Extending instance discrimination task to video domain with an appearance-focused video instance discrimination (A-VID) task. It encourages models to learn appearance features by distinguishing whether two clips are from the same video. - Combining the RSP and A-VID tasks in a unified framework to learn spatiotemporal features capturing both motion and appearance information from unlabeled videos.- Achieving state-of-the-art performance on downstream action recognition and video retrieval tasks. Remarkably, RSPNet outperforms ImageNet supervised pretraining on UCF101 action recognition by a large margin.In summary, the key contribution is designing effective pretext tasks and combining them in a principled framework for self-supervised video representation learning without any manual labels. The learned features generalize very well to downstream tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a self-supervised video representation learning method called RSPNet that learns motion features through a relative speed perception task and appearance features through an appearance-focused video instance discrimination task on unlabeled videos, achieving strong performance on downstream action recognition without needing annotated data.
