# [RSPNet: Relative Speed Perception for Unsupervised Video Representation   Learning](https://arxiv.org/abs/2011.07949)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is:How can we learn effective video representations in an unsupervised manner, especially learning useful features for both motion and appearance?The key points are:- Unsupervised video representation learning is very challenging due to the complex spatio-temporal information in videos and lack of labeled data. - Existing methods using playback speed perception as pretext task suffer from imprecise speed labels. Also they do not explicitly encourage learning appearance features.- This paper proposes to use relative speed perception as pretext task which can provide more consistent supervision. - It also extends instance discrimination from images to videos and uses speed augmentation so the model focuses more on appearance. - By combining the two pretext tasks, the model can learn useful features for both motion and appearance in an unsupervised manner.- Experiments show the learned features achieve excellent performance on downstream action recognition and video retrieval without using any manually annotated data.In summary, the main research question is how to do unsupervised video representation learning, especially learning features for both motion and appearance. The key ideas are exploiting relative speed and extending instance discrimination with speed augmentation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new self-supervised learning method called RSPNet for video representation learning without using any labels. The key ideas are:- Proposing a relative speed perception (RSP) task that identifies the relative playback speed between two clips instead of predicting their specific playback speeds. This provides more precise supervision to learn motion features. - Extending instance discrimination task to video domain with an appearance-focused video instance discrimination (A-VID) task. It encourages models to learn appearance features by distinguishing whether two clips are from the same video. - Combining the RSP and A-VID tasks in a unified framework to learn spatiotemporal features capturing both motion and appearance information from unlabeled videos.- Achieving state-of-the-art performance on downstream action recognition and video retrieval tasks. Remarkably, RSPNet outperforms ImageNet supervised pretraining on UCF101 action recognition by a large margin.In summary, the key contribution is designing effective pretext tasks and combining them in a principled framework for self-supervised video representation learning without any manual labels. The learned features generalize very well to downstream tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a self-supervised video representation learning method called RSPNet that learns motion features through a relative speed perception task and appearance features through an appearance-focused video instance discrimination task on unlabeled videos, achieving strong performance on downstream action recognition without needing annotated data.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in unsupervised video representation learning:- The main novel contribution is proposing relative speed perception (RSP) as a pretext task. Most prior work has focused on predicting absolute playback speeds. Using relative speed labels is more consistent with motion patterns and provides better supervision.- This paper combines RSP with a video instance discrimination task (A-VID) to learn both motion and appearance features. Other methods tend to focus on either motion (e.g. speed perception) or appearance (e.g. context prediction), but not both. Jointly optimizing RSP and A-VID helps learn more balanced features.- For A-VID, they use a speed augmentation strategy to make models focus on appearance rather than speed cues. This is a simple but effective idea not explored by other video instance discrimination methods. - The two-branch architecture optimizing RSP and A-VID with a shared encoder is quite standard. But the design of the pretext tasks themselves is novel.- Without any labeled data for pre-training, RSPNet achieves 93.7% on UCF-101, outperforming supervised ImageNet pre-training. This demonstrates the effectiveness of their approach for learning useful video representations.- Compared to concurrent self-supervised methods like CoCLR and MemDPC, RSPNet achieves competitive or better results on UCF-101 and HMDB51 with a conceptually simpler framework.In summary, this paper introduces creative pretext tasks tailored for videos and combines motion and appearance learning more effectively than prior work. The results validate their design choices and demonstrate state-of-the-art transfer learning performance.
